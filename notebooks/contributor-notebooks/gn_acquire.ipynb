{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests import get\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "import main\n",
    "import acquire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of urls using a list of topics\n",
    "pages = acquire.get_url_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 unique search topics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://github.com/search?l=JavaScript&q=sports&type=Repositories',\n",
       " 'https://github.com/search?l=JavaScript&q=data+engineering&type=Repositories',\n",
       " 'https://github.com/search?l=JavaScript&q=artificial+intelligence&type=Repositories',\n",
       " 'https://github.com/search?l=JavaScript&q=space+exploration&type=Repositories',\n",
       " 'https://github.com/search?l=JavaScript&q=biology&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=sports&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=data+engineering&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=artificial+intelligence&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=space+exploration&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=biology&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=sports&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=data+engineering&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=artificial+intelligence&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=space+exploration&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=biology&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=sports&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=data+engineering&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=artificial+intelligence&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=space+exploration&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=biology&type=Repositories']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {len(pages)} unique search topics')\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repository_links = get_url_links(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url=''):\n",
    "    '''\n",
    "    This helper function takes in a url and requests and parses HTML\n",
    "    returning a soup object.\n",
    "    '''\n",
    "    headers = {'User-Agent': 'NLP3'} \n",
    "    response = get(url, headers=headers)\n",
    "    sleep(3)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_links(urls=[]):\n",
    "    '''\n",
    "    This function creates a list of 10 urls from a single page\n",
    "    '''\n",
    "    extensions = []\n",
    "\n",
    "    for url in urls:\n",
    "        soup = make_soup(url)\n",
    "        extension = soup.find_all(\"a\", {\"class\":\"v-align-middle\"}, {\"data-hydro-click\":\"url\"})\n",
    "        for count in range(len(extension)):\n",
    "            extensions.append(\"http://github.com/\" + extension[count].get_text())\n",
    "    return extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_repos(repo_urls=[]):\n",
    "    readme_data = []\n",
    "    requests = 0\n",
    "    for repo in repo_urls:\n",
    "        requests += 1\n",
    "        print(requests)\n",
    "        repo_soup = make_soup(repo)\n",
    "        if repo_soup.find('span', class_=\"Progress-item\", itemprop='keywords', attrs='aria-label') == None:\n",
    "            continue\n",
    "        else:\n",
    "            for sentence in repo_soup.findAll(class_=\"markdown-body entry-content container-lg\"):\n",
    "                readme = ''.join(sentence.findAll(text=True))\n",
    "            language = repo_soup.find('span', class_=\"Progress-item\", itemprop='keywords', attrs='aria-label')['aria-label']\n",
    "            watchers = repo_soup.find_all('a', class_=\"social-count\")[0].text.strip()\n",
    "            stars = repo_soup.find_all('a', class_=\"social-count\")[1].text.strip()\n",
    "            forks = repo_soup.find_all('a', class_=\"social-count\")[2].text.strip()\n",
    "            commits = repo_soup.find_all('strong')[3].text\n",
    "            repo_info = {'language' : language,\n",
    "                     'readme': readme,\n",
    "                     'watchers': watchers,\n",
    "                     'stars': stars,\n",
    "                     'forks': forks,\n",
    "                     'commits': commits}\n",
    "            readme_data.append(repo_info)\n",
    "    return readme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `[0]` not found\n"
     ]
    }
   ],
   "source": [
    "page[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page0 = get_url_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'JavaScript 97.4',\n",
       " 'readme': 'nba\\nNode.js client for nba.com API endpoints\\nnpm install nba\\nNOTES:\\nBROWSER USAGE\\nThis package can\\'t be used from the browser because of CORS restrictions imposed by nba.com. Currently the hostnames are hardcoded so this package can\\'t be used with a proxy host but if you want support for this use case please open an issue!\\nBLACKLISTED IP ADDRESSES:\\nIt appears as though the NBA has blacklisted certain blocks of IP addresses, specifically those of cloud hosting providers including AWS. As such, you may hit a situation where an application using this package works fine on your local machine, but doesn\\'t work at all when deployed to a cloud server. Annoyingly, requests from these IPs seem to just hang. More information here and here -- the second issue has a curl command somewhere which will quickly tell you if NBA is accepting requests from your IP. (Incidentally, this is also the same reason the TravisCI build is always \"broken\" but tests all pass locally). There is a simple pass-through server in scripts/proxy that can be used to get around this restriction; you can put the proxy server somewhere that can reach NBA.com (e.g. not on AWS or Heroku or similar) and host your actual application on a cloud provider.\\nNBA API\\nThe stats.nba.com uses a large number of undocumented JSON endpoints to provide the statistics tables and charts displayed on that website. This library provides a JavaScript client for interacting with many of those API endpoints.\\nGetting Started\\nNBA.findPlayer(str) will return an object with a player\\'s name, their ID, and their team information. This method is built into the package.\\nAll methods in the NBA.stats namespace require an object to be passed in as a parameter. The keys to the object are in the docs for the stats namespace here\\nconst NBA = require(\"nba\");\\nconst curry = NBA.findPlayer(\\'Stephen Curry\\');\\nconsole.log(curry);\\n/* logs the following:\\n{\\n  firstName: \\'Stephen\\',\\n  lastName: \\'Curry\\',\\n  playerId: 201939,\\n  teamId: 1610612744,\\n  fullName: \\'Stephen Curry\\',\\n  downcaseName: \\'stephen curry\\'\\n}\\n*/\\nNBA.stats.playerInfo({ PlayerID: curry.playerId }).then(console.log);\\nFor more example API calls, see /test/integration/stats.js\\nStability Warning\\nThis is a client for an unstable and undocumented API. While I try to follow semver for changes to the JavaScript API this library exposes, the underlying HTTP API can (and has) changed without warning. In particular, the NBA has repeatedly deprecated endpoints, or added certain required headers without which requests will fail. Further, this library comes bundled with a (relatively) up-to-date list of current NBA players which is subject to change at any time -- the specific contents of it should not be considered part of this library\\'s API contract.\\nUsability\\nTo put it nicely, the NBA\\'s API endpoints are a little clunky to work with. This library tries to strike a balance between being usable but not making assumptions about how the data will be used. Specifically, the NBA sends data in a concise \"table\" form where the column headers come first then each result is an array of values that need to be matched with the proper header. This library does a simple transformation to zip the header and values arrays into a header-keyed object. Beyond that, it tries to not do too much. This is important to note because sometimes the various \"result sets\" that come back on a single endpoint seem sort of arbitrary. The underlying HTTP API doesn\\'t seem to follow standard REST practices; rather it seems the endpoints are tied directly to the data needed by specific tables and charts displayed on stats.nba.com. This is what I mean by \"clunky\" to work with -- it can be tricky to assemble the data you need for a specific analysis from the various endpoints available.\\nDocumentation\\nstill lots to do here...\\nThere are four primary parts of this library\\n\\nTop-level methods\\nstats namespace — docs\\nsportVu namespace\\nsynergy namespace\\n\\nTransport Layer\\nIn some cases you will want to use a different transport layer to handle HTTP requests. Perhaps you have an HTTP client library you like better than what I used here. Better yet, you want to get stats for the WNBA or the G League. The following code snippet shows how to use the withTransport method to create a new client with your own transport function.\\n// here we are getting stats for the WNBA!\\n\\nconst nba = require(\"nba\");\\nconst getJSON = require(\"nba/src/get-json\");\\n\\n// for the G League, try \"stats.gleague.nba.com\"\\nconst newHost = \"stats.wnba.com\";\\n\\nconst transport = (url, params, options) => {\\n  // simply swap the host and then defer the rest to the built in getJSON function\\n  const fixedURL = url.replace(\"stats.nba.com\", \"stats.wnba.com\");\\n  return getJSON(fixedURL, params, options);\\n};\\n\\n// create a new stats client here with our WNBA transport\\nconst wnbaStats = nba.stats.withTransport(transport);\\n\\n(async () => {\\n  const result = await wnbaStats.playerInfo({ PlayerID: \"1628886\" });\\n  console.log(result);\\n})();\\n\"I don\\'t use Node.js\"\\nPlease take a look at nba-client-template. The relevant part of the repo is a single JSON document from which many programming languages can dynamically generate an API client. The repo contains (sloppy) examples in Ruby and Python. Compiled languages can use code generation techniques to the same effect -- there\\'s a (again, sloppy) example in Go. If you\\'d like me to publish it to a specific registry so you can install it with your language\\'s package manager, please open an issue. Please note, however, that package only includes  the endpoints exposed by this library under the stats namespace -- sportvu and synergy endpoints aren\\'t yet included in it. I also plan to add a command-line interface to this library so that it can be easily driven as a child process by another program.\\n\\n',\n",
       " 'watchers': '49',\n",
       " 'stars': '621',\n",
       " 'forks': '150',\n",
       " 'commits': '294'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_soup = acquire.make_soup(repository_links[1])\n",
    "\n",
    "for sentence in repo_soup.findAll(class_=\"markdown-body entry-content container-lg\"):\n",
    "    readme = ''.join(sentence.findAll(text=True))\n",
    "\n",
    "language = repo_soup.find('span', class_=\"Progress-item\", itemprop='keywords', attrs='aria-label')['aria-label']\n",
    "watchers = repo_soup.find_all('a', class_=\"social-count\")[0].text.strip()\n",
    "stars = repo_soup.find_all('a', class_=\"social-count\")[1].text.strip()\n",
    "forks = repo_soup.find_all('a', class_=\"social-count\")[2].text.strip()\n",
    "repo_info = {'language' : language,\n",
    "         'readme': readme,\n",
    "         'watchers': watchers,\n",
    "         'stars': stars,\n",
    "         'forks': forks}\n",
    "repo_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = acquire.scrape_repos(repository_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saving as a json file\n",
    "# with open('read_me.json', 'w') as file:\n",
    "#     json.dump(data, file)\n",
    "\n",
    "#can also use\n",
    "\n",
    "# df.to_json('articles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/search?l=&o=desc&q=language%3AJavaScript&s=stars&type=Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_pages_test():\n",
    "    '''\n",
    "    This function creates a list of url pages \n",
    "    with specified languages and topics\n",
    "    '''\n",
    "    urls = []\n",
    "    languages = ['JavaScript', 'Python', 'Java', 'C%2B%2B']\n",
    "    pages = list(i for i in range(1,13))\n",
    "    for page in pages:\n",
    "        for language in languages:\n",
    "            urls.append(f\"https://github.com/search?l=&p={page}&q=language%3A{language}&ref=advsearch&type=Repositories\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pages = get_url_pages_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/search?l=&p=12&q=language%3AC%2B%2B&ref=advsearch&type=Repositories'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pages[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/search?l=&p=1&q=language%3AJavaScript&ref=advsearch&type=Repositories'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40): test_pages[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_test8 = []\n",
    "for i in range(8):\n",
    "    pg_test8.append(test_pages[i])\n",
    "\n",
    "len(pg_test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_links = get_url_links(pg_test8)\n",
    "\n",
    "len(test_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_test16 = []\n",
    "for i in range(8,16):\n",
    "    pg_test16.append(test_pages[i])\n",
    "\n",
    "len(pg_test16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links2 = get_url_links(pg_test16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_links2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_test24 = []\n",
    "for i in range(16,24):\n",
    "    pg_test24.append(test_pages[i])\n",
    "\n",
    "len(pg_test24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_links3 = get_url_links(pg_test24)\n",
    "\n",
    "len(test_links3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_test32 = []\n",
    "for i in range(24,32):\n",
    "    pg_test32.append(test_pages[i])\n",
    "\n",
    "len(pg_test32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_links4 = get_url_links(pg_test32)\n",
    "\n",
    "len(test_links4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_test40 = []\n",
    "for i in range(32,40):\n",
    "    pg_test40.append(test_pages[i])\n",
    "\n",
    "len(pg_test40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_links5 = get_url_links(pg_test40)\n",
    "\n",
    "len(test_links5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_test48 = []\n",
    "for i in range(40,48):\n",
    "    pg_test48.append(test_pages[i])\n",
    "\n",
    "len(pg_test48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_links6 = get_url_links(pg_test48)\n",
    "\n",
    "len(test_links6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links.extend(test_links2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links.extend(test_links3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links.extend(test_links4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links.extend(test_links5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links.extend(test_links6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "test_repos = scrape_repos(test_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'language': 'JavaScript 89.8',\n",
       "  'readme': 'Kalei - Style guide\\nThis project aims at making sure your style sheets are fully documented whilst being synchronized with your webpages styles. To do this it actually uses your live stylesheets in so that at anytime you can review how your styleguide looks.\\nMain goals and benefits\\n\\nFully documented CSS - No need to explain the benefits\\nNo dependencies, simply download the repository and run in your browser\\nAutomatic generation of demo UI components\\nEasy access for anyone, designer, developer, manager and users\\nRapid development of projects by allowing developers to find the correct CSS and HTML for any given UI component\\nOpen sourced so that all great ideas can be included\\n\\nGetting started\\n\\nDownload the repository (git clone git://github.com/kaleistyleguide/kaleistyleguide.git)\\nServe it on a HTTP server and it should work!\\nEdit js/config.js to point at your own styles.css\\n\\nAuthor\\nThomas Davis\\nContributors\\nLuke Brooker\\nRichard Barret\\nSam Pospischil\\nInspiration\\nKalei is heavily influenced by the following projects and blog posts.\\nPea.rs\\nKSS\\nStyleDocco\\nRJ Metrics\\nAnchoring Your Design Language in a Live Style Guide\\nTechnologies\\n\\nmarked\\njscssp\\nLESS\\ncssbeautify\\nfixie\\nhighlight\\nbackbone\\nunderscore\\njquery\\n\\nLicense\\nPublic domain: http://unlicense.org/\\n',\n",
       "  'watchers': '39',\n",
       "  'stars': '683',\n",
       "  'forks': '110',\n",
       "  'commits': '133'},\n",
       " {'language': 'JavaScript 84.8',\n",
       "  'readme': 'bui\\n基于jQuery的富客户端控件库\\n\\n文档库地址\\n应用代码\\nAPI代码\\nLicense\\n提交代码流程\\n\\n文件结构\\n\\nassets : css文件，基于bootstrap的css样式，可以自己在此基础上编译出新的版本\\nbuild : js 和 css文件打包好的目录\\nsrc: js 的源文件\\ntest: 单元测试，所有控件的单元测试都在内部，以php的方式提供\\ntools : 文件打包，以及生成文件的工具\\ndocs ： 源文件中未提供，但是可以自己执行 tools/jsduck/run.bat文件，请不要提交此文件夹\\n\\n打包\\n源文件的编译包括：\\n\\n合并js，压缩js\\n编译less生成 css,压缩css\\n复制文件，将所有js合并成一个bui.js\\n执行build.bat文件\\n\\n生成文档：\\n\\n使用jsduck 进行编译文档，tools/jsduck/run.bat\\n配置文件在tools/jsduck/config.json\\n如果不想配置环境，请下载文档API\\n\\n文档地址\\n\\ndpl 地址\\n控件库demo\\n控件库API\\n集成的应用\\n\\n提交问题\\n提问\\n联系我们\\n\\n论坛：http://bbs.builive.com\\n旺旺群号： 778141976\\nQQ群：138692365\\n\\n',\n",
       "  'watchers': '96',\n",
       "  'stars': '659',\n",
       "  'forks': '374',\n",
       "  'commits': '610'},\n",
       " {'language': 'JavaScript 98.2',\n",
       "  'readme': 'JQuery EasyTabs Plugin\\nTabs with(out) style.\\nEasyTabs creates tabs with all the functionality, no unwanted changes\\nto your markup, and no hidden styling.\\nUnlike jQuery UI tabs, which style and arrange your tabs and panels for you, this plugin handles only the functionality of the tabs. By leaving the styling and layout up to you, it is much easier to style and arrange your tabs the way you want.\\nWhat EasyTabs Does:\\n\\nCreates tabs from an unordered list, which link to divs on the page\\nAllows complete customization of appearance, layout, and style via CSS\\nSupports forward- and back-button in browsers\\nTabs are bookmarkable and SEO-friendly\\nTabs can be cycled at a specified interval\\n\\nWhat EasyTabs Does NOT Do:\\n\\nStyle your tabs in any way (though sensible CSS defaults can be found\\nin the demos)\\n\\nShow Your Support\\n\\n\\n\\nShow your support for jQuery EasyTabs, by helping us raise money for the Karmanos Cancer\\nInstitute.\\n\\n\\n\\n\\n\\n\\nDocumentation\\n\\nInstallation\\nStylization\\nConfiguration Options\\nDemos\\n\\nInstallation\\nThe HTML\\nUnlike JQuery UI tabs, the HTML markup for your tabs and content can be arranged however you want. At the minimum, you need a container, an unordered list of links for your tabs, and matching divs for your tabbed content.\\n<div id=\"tab-container\">\\n  <ul>\\n    <li><a href=\"#tab-1-div\">Tab 1</a></li>\\n    <li><a href=\"#that-other-tab\">The Second Tab</a></li>\\n    <li><a href=\"#lastly\">Tab C</a></li>\\n  </ul>\\n  <div id=\"tab-1-div\">\\n    <h2>Heading 1</h2>\\n    <p>This is the content of the first tab.</p>\\n  </div>\\n    <div id=\"that-other-tab\">\\n    <h2>Heading 2</h2>\\n    <p>Stuff from the second tab.</p>\\n  </div>\\n  <div id=\"lastly\">\\n    <h2>Heading 3</h2>\\n    <p>More stuff from the last tab.</p>\\n  </div>\\n</div>\\n\\nThe Javascript\\nTo enable back- and forward-button support for the users\\' browsers, be sure to include either the jQuery HashChange plugin (recommended) or the Address plugin before including the EasyTabs plugin. There is no other configuration required, it will just work!\\n<script src=\"/javascripts/jquery.js\" type=\"text/javascript\"></script> \\n<script src=\"/javascripts/jquery.hashchange.js\" type=\"text/javascript\"></script> \\n<script src=\"/javascripts/jquery.easytabs.js\" type=\"text/javascript\"></script>  \\n\\n<script type=\"text/javascript\"> \\n  $(document).ready(function(){ $(\\'#tab-container\\').easytabs(); });\\n</script> \\n\\nI varied the tab ids and names just to show you how flexible this is. There is no magic going on with this plugin; it\\'s not trying to guess the order of your tabs or what tab is associated with which <div>. Just make the id of the content <div> match the href of the tab link.\\nRequired Markup\\nThe only rules you need to follow are these:\\n\\ncontaining <div> with a unique id\\nthe container <div>\\xa0contains an unordered list <ul>\\xa0of links <a>\\n\\n(UPDATE: As of version 1.1, this is no longer the case. You can now include your tabs anywhere within the container. It can be a <ul>, <ol>, <div>, or anything you want. The default is still a top-level <ul>, so to change it you just specify your selector with the new \"tabs\" option.)\\n\\nthe container div also contains content divs (for the tabbed content), each div has a unique id\\xa0that matches the href property of a link in the unordered list\\n\\nOther than that, go nuts. The order of the elements does NOT matter. Your <ul> could be before or after the content divs (or even between them). You can put non-tabbed content between the elements. It doesn\\'t matter. The most common structure (for inspiration\\'s sake) is something like this:\\ndiv#tab-container ul > ( li > a[href=\"tab-1\"], li > a[href=\"second-tab\"] )\\ndiv#tab-container div#tab-1\\ndiv#tab-container div#second-tab\\n\\n+---------------------------------------------------------------------------+\\n|                              div#tab-container                            |\\n|  +---------------------------------------------------------------------+  |\\n|  |                                  ul                                 |  |\\n|  |  +-----------------------------+    +----------------------------+  |  |\\n|  |  |             li              |    |             li             |  |  |\\n|  |  |  +-----------------------+  |    |  +----------------------+  |  |  |\\n|  |  |  |    a[href=\"tab-1\"]    |  |    |  | a[href=\"second-tab\"] |  |  |  |\\n|  |  |  +-----------------------+  |    |  +----------------------+  |  |  |\\n|  |  +-----------------------------+    +----------------------------+  |  |\\n|  +---------------------------------------------------------------------+  |\\n|                                                                           |\\n|  +---------------------------------------------------------------------+  |\\n|  |                               div#tab-1                             |  |\\n|  +---------------------------------------------------------------------+  |\\n|                                                                           |\\n|  +---------------------------------------------------------------------+  |\\n|  |                             div#second-tab                          |  |\\n|  +---------------------------------------------------------------------+  |\\n|                                                                           |\\n+---------------------------------------------------------------------------+\\n\\n\\nFor stylization, configuration options, and live demos, see the EasyTabs homepage.\\n\\nLinks\\n\\nFull Documentation and Demos\\nUpdates and new features for v1.1.2\\nUpdates and new features for v2.0\\nUpdates and new features for v2.1.2\\nDownload jQuery EasyTabs\\nFork and view source code\\n\\nInfo\\n\\nAuthor: Steve Schwartz\\nCompany: Alfa Jango, LLC\\nLicense: Dual licensed under the MIT and GPL licenses.\\n\\n',\n",
       "  'watchers': '34',\n",
       "  'stars': '564',\n",
       "  'forks': '211',\n",
       "  'commits': '111'},\n",
       " {'language': 'JavaScript 55.9',\n",
       "  'readme': \"\\nVisit the Mimosa Website for all sorts of Mimosa documentation goodness.\\nQuestions?\\n\\n\\nGoogle Group\\nTwitter @mimosajs\\n\\nQuick Start\\nThere are a ton of docs on the site to get you started, but here's a quick start.\\n\\nInstall node.js.  A .10 version.\\nnpm install -g mimosa\\nmimosa new testproject\\nFollow the prompts to choose some assets\\ncd testproject\\nmimosa watch -s\\nhttp://localhost:3000\\n\\nNow you are ready to rock!\\nMaybe you want to start with something other than an empty app?  Maybe you want to start with a preconfigured Ember app for instance?\\nmimosa skel:list\\nThat will get you a list of the skeletons currently available for Mimosa that might help jumpstart your app.\\nLicense\\n(The MIT License)\\nCopyright (c) 2014 David Bashford\\nPermission is hereby granted, free of charge, to any person obtaining\\na copy of this software and associated documentation files (the\\n'Software'), to deal in the Software without restriction, including\\nwithout limitation the rights to use, copy, modify, merge, publish,\\ndistribute, sublicense, and/or sell copies of the Software, and to\\npermit persons to whom the Software is furnished to do so, subject to\\nthe following conditions:\\nThe above copyright notice and this permission notice shall be\\nincluded in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\",\n",
       "  'watchers': '23',\n",
       "  'stars': '532',\n",
       "  'forks': '37',\n",
       "  'commits': '1,636'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Sketch Mate\\nThese plugins will make you best friends with Sketch.\\nPlugin Directory\\nArtboards\\n\\nDuplicate Artboard shift + ⌘ + D\\nFit Artboard\\nFit Artboard Height ctrl + shift + A\\nRemove Artboard ⌘ + ⌫\\nSort Artboards\\n\\nMisc\\n\\nGoto Page\\nReplace Layer ⌘ + ⌥ + R\\nSet Line Height\\nToggle Click Through\\nWrap in Bounding Box\\n\\nSmart Align\\n\\nDistribute Horizontally control + ⌘ + ⌥ + ,\\nDistribute Vertically control + ⌘ + ⌥ + .\\nSmart Align Horizontally ⌘ + ⌥ + ,\\nSmart Align Vertically ⌘ + ⌥ + .\\nSmart Align Bottom control + ⌘ + ↓\\nSmart Align Left control + ⌘ + ←\\nSmart Align Right control + ⌘ + →\\nSmart Align Top control + ⌘ + ↑\\nSpace Horizontally\\nSpace Vertically\\nStretch Height ⌘ + ⌥ + e\\nStretch Width ⌘ + e\\n\\nSmart Move\\n\\nDelete and Pull\\nPull Left shift + ⌘ + ⌥ + ←\\nPull Up shift + ⌘ + ⌥ + ↑\\nPush Down shift + ⌘ + ⌥ + ↓\\nPush Right shift + ⌘ + ⌥ + →\\nSet Increments shift + ⌘ + ⌥ + I\\n\\nSort\\n\\nReverse Layer Order\\nReverse Positions\\nSort Artboards by Name\\nSort Layers ctrl + ⌘ + ⌥ + S\\n\\nInstallation\\nTo install all plugins, download them all first, unzip the archive, and place the folder contents in your Sketch Plugins folder by navigating to Sketch > Plugins > Reveal Plugins Folder…\\nTo install only a selection of plugins, you will first need to place the library file inventory.js in the root of your Sketch Plugins directory. This is very important as all plugins rely on its functionality.\\nYou can then install selected plugins by double-clicking the file, or alternatively, drag and drop the file onto the Sketch app icon. This will automatically copy the plugin to your Sketch Plugins folder.\\nKeyboard Shortcuts\\nMost plugins have a pre-defined keyboard shortcut. You can always change it by editing the shortcut written in parenthesis at the end of the first line of a plugin.\\nFor example, the first line of Duplicate Artboard.sketchplugin:\\n\\n// Duplicates the current artboard right next to it. (shift command d)\\n\\nYou can use modifier keys such as option, command, control, shift\\nArtboards\\nDuplicate Artboard (next to the current artboard)\\nThis improves the built in behavior of artboard duplication in Sketch. If the selected artboard is in the middle of other artboards, all artboards on the right side will be shifted to the right before the artboard is duplicated. Requires any layer of an artboard to be selected.\\nShortcut: shift + ⌘ + D\\n\\nRemove Artboard\\nThis improves the built in behavior of artboard removal in Sketch. If the selected artboard is in the middle of other artboards, all artboards on the right side will be shifted to the left after the artboard has been removed. Requires any layer of an artboard to be selected.\\nShortcut: ⌘ + ⌫ (Backspace)\\n\\nFit Artboard\\nResizes the artboard to fit its layers.\\nFit Artboard Height\\nResizes the artboard to fit the height of its layers.\\nShortcut: shift + ctrl + A\\n\\nSort Artboards\\nSorts selected artboard layers by their horizontal position. Useful when your layer list does not reflect the artboard arrangement on your canvas.\\nText\\nSet Line Height\\nPlugin that allows you to set the line height of a text layer as a multiple of the font size. It’s like using em in CSS. Supports multiple selections.\\nShortcut: ⌘ + L\\n\\nMisc\\nReplace Layer\\nReplaces the selected layer with the content in the clipboard. Basically this plugin does paste in place while removing the original selection.\\nShortcut: ⌥ + cmd + R\\n\\nSmart Align\\nDistribute Horizontally\\nCalls the menu command \"Distribute Horizontally\". Just for shortcut purposes.\\nShortcut: ctrl + ⌘ + ⌥ + ,\\nDistribute Vertically\\nCalls the menu command \"Distribute Vertically\". Just for shortcut purposes.\\nShortcut: ctrl + ⌘ + ⌥ + .\\nSmart Align Horizontally\\nexperimental Aligns the selected layer relative to its parent group.\\nShortcut: ⌘ + ⌥ + ,\\nSmart Align Vertically\\nexperimental Aligns the selected layer relative to its parent group.\\nShortcut: ⌘ + ⌥ + .\\nSpace Horizontal\\nDistributes the selected elements horizontally, with the same distante beetween them. If only one layer is selected, the layer will be moved by the spacing that has been input.\\n\\nSpace Vertical\\nDistributes the selected elements vertically, with the same distante beetween them. If only one layer is selected, the layer will be moved by the spacing that has been input.\\nSmart Move\\nexperimental Allows you to pull or push layers in relation to the selected layer.\\n\\nPull Left shift + ⌘ + ⌥ + ←\\nPull Up shift + ⌘ + ⌥ + ↑\\nPush Down shift + ⌘ + ⌥ + ↓\\nPush Right shift + ⌘ + ⌥ + →\\n\\n\\nSorting\\nSort Layers\\nThere are also plugins to reverse the order of the layers in the layer list and a plugin that reverses the position of the selected layers on the artboard.\\nOptions: Text (A->Z), Text (Z->A), Layer Name (A->Z), Layer Name (Z->A), Top, Left, Random\\nShortcut: ctrl + ⌘ + ⌥ + S\\nSorting Layers by text, visually\\n\\nSorting Layers in the layer list by position\\n\\n',\n",
       "  'watchers': '25',\n",
       "  'stars': '509',\n",
       "  'forks': '32',\n",
       "  'commits': '81'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Meteor Collection Helpers\\nCollection helpers automatically sets up a transformation on your collections using Meteor\\'s Mongo.Collection transform option, allowing for simple models with an interface that\\'s similar to template helpers.\\nInstallation\\n$ meteor add dburles:collection-helpers\\nUsage\\nWrite your helpers somewhere seen by both client and server.\\nBooks = new Mongo.Collection(\\'books\\');\\nAuthors = new Mongo.Collection(\\'authors\\');\\n\\nBooks.helpers({\\n  author() {\\n    return Authors.findOne(this.authorId);\\n  }\\n});\\n\\nAuthors.helpers({\\n  fullName() {\\n    return `${this.firstName} ${this.lastName}`;\\n  },\\n  books() {\\n    return Books.find({ authorId: this._id });\\n  }\\n});\\nThis will then allow you to do:\\nBooks.findOne().author().firstName; // Charles\\nBooks.findOne().author().fullName(); // Charles Darwin\\nAuthors.findOne().books()\\nOur relationships are resolved by the collection helper, avoiding unnecessary template helpers. So we can simply write:\\nTemplate.books.helpers({\\n  books() {\\n    return Books.find();\\n  }\\n});\\n...with the corresponding template:\\n<template name=\"books\">\\n  <ul>\\n    {{#each books}}\\n      <li>{{name}} by {{author.fullName}}</li>\\n    {{/each}}\\n  </ul>\\n</template>\\nMeteor.users\\nYou can also apply helpers to the Meteor.users collection\\nMeteor.users.helpers({\\n  // ...\\n});\\nApplying the transformation function\\nSometimes it may be useful to apply the transformation directly to an object.\\nvar doc = {\\n  firstName: \\'Charles\\',\\n  lastName: \\'Darwin\\'\\n};\\n\\nvar transformedDoc = Authors._transform(doc);\\n\\ntransformedDoc.fullName(); // Charles Darwin\\nLicense\\nMIT\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '504',\n",
       "  'forks': '30',\n",
       "  'commits': '53'},\n",
       " {'language': 'JavaScript 99.5',\n",
       "  'readme': \"**NOTE: this project is no longer actively maintained and not recommended for use. It is left here for reference. **\\nKanso\\nSimple, distributable JavaScript apps using CouchDB\\n\\nKanso is a set of tools and packages for creating JavaScript apps that run\\ndirectly on CouchDB. Your app and related code can be easily packaged and shared\\nwith the community, or deployed to a number of cloud-hosted services.\\nThe command-line tool is used for building and deploying these applications, as\\nwell as providing some useful utilities for working with CouchDB and JSON data.\\nWhy develop apps with Kanso?\\n\\nScalability: easily grow from hobby-project to high-demand app with CouchDB\\nDeployment: effortless to deploy, to the cloud or local machines, promoting\\nquick iterations\\nMulti-platform: runs anywhere with CouchDB (Windows, OSX, Linux, Android and\\niOS)\\nKeep your data yours: now you can keep sensitive data in-house, and avoid\\nsharing it with cloud services\\nEasy to distribute: apps are easy to share and distribute, between people and\\nservers\\nAvoid lock-in: easy to deploy and based on open-source, don't be held hostage\\nto a proprietary API\\nHomogeneity: your development environment matches your production environment\\nOne language to rule them all: with just CouchDB and the browser, all you\\nneed to speak is JavaScript!\\n\\nGet started\\nsudo npm install -g kanso\\n\\nLearn more: Simplest possible app\\nDeveloper community\\nKanso is an open-source project written by developers using CouchApps everyday, in\\nreal-world projects. Kanso provides the tools to share code and resources,\\nregardless of the way in which your app is built. It's easy to use coffee-script,\\nless stylesheets, or any number of JavaScript frameworks in your app, and still\\nbenefit from the wealth of packages provided by the community.\\nFind other Kanso developers in #kansojs on FreeNode, or on the mailing list.\\nFind out more\\nFor more information on the project, check out the Kanso website.\\n\",\n",
       "  'watchers': '22',\n",
       "  'stars': '501',\n",
       "  'forks': '60',\n",
       "  'commits': '1,528'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'http-client  \\nhttp-client lets you compose HTTP clients using JavaScript\\'s fetch API. This library has the following goals:\\n\\nPreserve the full capabilities of the fetch API\\nProvide an extendable middleware API\\nUse the same API on both client and server\\n\\nInstallation\\nUsing npm:\\n$ npm install --save http-client\\n\\nhttp-client requires you to bring your own global fetch function (for convenience when using the top-level createFetch function). isomorphic-fetch is a great polyfill if you need to support environments that don\\'t already have a global fetch function.\\nThen, use as you would anything else:\\n// using ES6 modules\\nimport { createFetch } from \\'http-client\\'\\n\\n// using CommonJS modules\\nvar createFetch = require(\\'http-client\\').createFetch\\nThe UMD build is also available on unpkg:\\n<script src=\"https://unpkg.com/http-client/umd/http-client.min.js\"></script>\\nYou can find the library on window.HTTPClient.\\nUsage\\nhttp-client simplifies the process of creating flexible HTTP clients that work in both node and the browser. You create your own fetch function using the createFetch method, optionally passing middleware as arguments.\\nimport { createFetch, base, accept, parse } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  base(\\'https://api.stripe.com/v1\\'),  // Prefix all request URLs\\n  accept(\\'application/json\\'),         // Set \"Accept: application/json\" in the request headers\\n  parse(\\'json\\')                       // Read the response as JSON and put it in response.body\\n)\\n\\nfetch(\\'/customers/5\\').then(response => {\\n  console.log(response.jsonData)\\n})\\nTop-level API\\ncreateFetch(...middleware)\\nCreates a fetch function that uses some middleware. Uses the global fetch function to actually make the request.\\ncreateStack(...middleware)\\nCombines several middleware into one, in the same order they are provided as arguments. Use this function to create re-usable middleware stacks or if you don\\'t want to use a global fetch function.\\nenableRecv(fetch)\\nReturns an \"enhanced\" version of the given fetch function that knows how to run response handlers registered using recv. This is only really useful when using stacks directly instead of createFetch.\\nMiddleware\\nhttp-client provides a variety of middleware that may be used to extend the functionality of the client. Out of the box, http-client ships with the following middleware:\\naccept(contentType)\\nAdds an Accept header to the request.\\nimport { createFetch, accept } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  accept(\\'application/json\\')\\n)\\nauth(value)\\nAdds an Authorization header to the request.\\nimport { createFetch, auth } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  auth(\\'Bearer \\' + oauth2Token)\\n)\\nbase(baseURL)\\nAdds the given baseURL to the beginning of the request URL.\\nimport { createFetch, base } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  base(\\'https://api.stripe.com/v1\\')\\n)\\n\\nfetch(\\'/customers/5\\') // GET https://api.stripe.com/v1/customers/5\\nbody(content, contentType)\\nSets the given content string as the request body.\\nimport { createFetch, body } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  body(JSON.stringify(data), \\'application/json\\')\\n)\\ndebug()\\nAdds a debug property to the response or error object so you can inspect them. Mainly useful for testing/debugging (should run after all other middleware).\\nimport { createFetch, debug } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  // ... other middleware\\n  debug()\\n)\\n\\nfetch(input).then(response => {\\n  console.log(response.debug.input, response.debug.options)\\n})\\nheader(name, value)\\nAdds a header to the request.\\nimport { createFetch, header } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  header(\\'Content-Type\\', \\'application/json\\')\\n)\\ninit(propertyName, value)\\nSets the value of an arbitrary property in the options object.\\nimport { createFetch, init } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  init(\\'credentials\\', \\'include\\')\\n)\\njson(object)\\nAdds the data in the given object as JSON to the request body.\\nmethod(verb)\\nSets the request method.\\nimport { createFetch, method } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  method(\\'POST\\')\\n)\\nparams(object)\\nAdds the given object to the query string of GET/HEAD requests and as a x-www-form-urlencoded payload on all others.\\nimport { createFetch, method, params } from \\'http-client\\'\\n\\n// Create a client that will append hello=world to the URL in the query string\\nconst fetch = createFetch(\\n  params({ hello: \\'world\\' })\\n)\\n\\n// Create a client that will send hello=world as POST data\\nconst fetch = createFetch(\\n  method(\\'POST\\'),\\n  params({ hello: \\'world\\' })\\n)\\nparse(parser, as = \\'body\\')\\nReads the response body to completion, parses the response, and puts the result on response.body (or whatever as is). parser must be the name of a valid Body parsing method. The following parsers are available in the spec:\\n\\narrayBuffer\\nblob\\nformData\\njson\\ntext\\n\\nimport { createFetch, parse } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  parse(\\'json\\')\\n)\\n\\nfetch(input).then(response => {\\n  console.log(response.body)\\n})\\nNote: Some parsers may not be available when using a fetch polyfill. In particular if you\\'re using node-fetch, you should be aware of its limitations.\\nquery(object)\\nAdds the data in the given object (or string) to the query string of the request URL.\\nrecv(handler)\\nUsed to handle the response in some way. The handler function should return the new response value, or a promise for it. Response handlers run in the order they are defined.\\nimport { createFetch, recv } from \\'http-client\\'\\n\\nconst fetch = createFetch(\\n  recv(response => (console.log(\\'runs first\\'), response)),\\n  recv(response => (console.log(\\'runs second\\'), response))\\n)\\nStacks\\nMiddleware may be combined together into re-usable middleware \"stacks\" using createStack. A stack is itself a middleware that is composed of one or more other pieces of middleware. Thus, you can pass a stack directly to createFetch as if it were any other piece of middleware.\\nThis is useful when you have a common set of functionality that you\\'d like to share between several different fetch methods, e.g.:\\nimport { createFetch, createStack, header, base, parse, query } from \\'http-client\\'\\n\\nconst commonStack = createStack(\\n  header(\\'X-Auth-Key\\', key),\\n  header(\\'X-Auth-Email\\', email),\\n  base(\\'https://api.cloudflare.com/client/v4\\'),\\n  parse(\\'json\\')\\n)\\n\\n// This fetch function can be used standalone...\\nconst fetch = createFetch(commonStack)\\n\\n// ...or we can add further middleware to create another fetch function!\\nconst fetchSinceBeginningOf2015 = createFetch(\\n  commonStack,\\n  query({ since: \\'2015-01-01T00:00:00Z\\' })\\n)\\nStacks are also useful when you don\\'t have a global fetch function, e.g. in node. In those cases, you can still use http-client middleware and supply your own fetch (we recommend node-fetch) function directly, but make sure you \"enhance\" it first:\\nconst { createStack, enableRecv, header, base } = require(\\'http-client\\')\\n\\n// We need to \"enhance\" node-fetch so it knows how to\\n// handle responses correctly. Specifically, enableRecv\\n// gives a fetch function the ability to run response\\n// handlers registered with recv (which parse, used below,\\n// uses behind the scenes).\\nconst fetch = enableRecv(\\n  require(\\'node-fetch\\')\\n)\\n\\nconst stack = createStack(\\n  header(\\'X-Auth-Key\\', key),\\n  header(\\'X-Auth-Email\\', email),\\n  base(\\'https://api.cloudflare.com/client/v4\\'),\\n  parse(\\'json\\')\\n)\\n\\nstack(fetch, input, options)\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '498',\n",
       "  'forks': '26',\n",
       "  'commits': '198'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'TouchScroll\\nTouchScroll is a JavaScript/CSS 3-based scrolling layer for Webkit Mobile, espeacially iPhone, Android, and iPad. It allows to configure scrolling behaviour in many ways and to use fixed interface elements.\\nDependencies\\nTouchScroll depends on css-beziers, a library for computations on cubic bezier curves.\\nUsage\\nTo use TouchScroll you need an element with fixed height. Have a look at the demo for an elegant solution using display: -webkit-box.\\nThe stylesheet is mandatory at the moment. It will be made optional in the future for cases when scrollbars aren’t needed.\\n<link rel=\"stylesheet\" src=\"touchscroll.css\">\\n<!-- … -->\\n<div id=\"scroller\">\\n    <!-- contents go here -->\\n</div>\\n<script src=\"css-beziers.js\"></script>\\n<script src=\"touchscroll.js\"></script>\\n<script>\\n    var scroller = new TouchScroll(document.querySelector(\"#scroller\"));\\n</script>\\n\\nTo enable the elasticity/bouncing effect, add {elastic: true} as second parameter to the instantiation:\\n<script>\\n    var scroller = new TouchScroll(document.querySelector(\"#scroller\"), {elastic: true});\\n</script>\\n\\nSet the scroller to overflow: auto to enable scrolling in other environments.\\nThe scroller automatically adapts its size to content changes and window resizes/orientation changes.\\nLimitations/Known Issues\\n\\nTouchScroll currently doesn’t work well with forms on Android.\\nThe scroller element shouldn’t have any padding.\\nBecause two wrapper <div>s are inserted inside of the scroller, the CSS\\nchild selector (#scroller > foo) might not work as expected.\\nWhen a scroller is invisible, it can’t adapt its size correctly. Call its setupScroller method to fix that (e.g. after making a scroller visible by setting display: block on it).\\nTapping the status bar on iPhone doesn’t trigger “scroll to top”.\\nSelecting text doesn’t work on the iPad and on some iPhone versions (OS 4.0b2) – an issue with cancelling events?\\n\\nTo Do\\n\\nKeep the scrollbars round while bouncing – I already know how to do this.\\nInvestigate whether support for tapping the status bar on iPhone can be added.\\nInvestigate how selecting text and using the context menu can be re-enabled on iPhone/iPad.\\nAdd an option to completely switch off scrollbars.\\nFind a solution to the event problems on Android – help greatly appreciated!\\n\\nContact\\nE-Mail: da AT uxebu.com\\nTwitter: @void_0\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '472',\n",
       "  'forks': '60',\n",
       "  'commits': '23'},\n",
       " {'language': 'JavaScript 72.3',\n",
       "  'readme': 'Keshif\\nThis repostory is not actively maintained.\\nTo access the most recent version and the online platform, visit www.keshif.me\\nKeshif is a web-based visualization and analytics tool that lets you explore datasets quickly.\\nLicense\\nBSD 3 clause (c) University of Maryland 2014-2016\\nAuthor\\nMehmet Adil Yalcin @ HCIL, University of Maryland, College Park\\nFunded in part by Huawei (2013-2014).\\n',\n",
       "  'watchers': '40',\n",
       "  'stars': '461',\n",
       "  'forks': '128',\n",
       "  'commits': '1,464'},\n",
       " {'language': 'Python 95.9',\n",
       "  'readme': \"Xiki plugin for Sublime Text 2\\n\\nBefore use: Install Xiki on your machine http://xiki.org/\\nInstall Package Control if you don't have it.\\nInstall the  SublimeXiki package.\\nTo use: open the command pallete (cmd+shift+p or ctrl+shift+p) and use Create Xiki Buffer\\n\\nHotkeys:\\n\\ncmd+enter: run or collapse the highlighted command/menu.\\ncmd+shift+enter: run the current command, and place the cursor after the output.\\n\\nIf used on a directory or file, will indent to the subdirectory level and create a command prompt ($)\\nIf used on a command prompt, will maintain the current indentation and create a prompt ($$ or $)\\n\\n\\n\\nUseful SublimeXiki commands:\\n\\n/ or ~: start a directory transversal tree.\\n\\n~ starts at your home directory\\nYou can also type a more complete path like /path/to/dir or ~/path\\n\\n\\n$: run a command directly (does not invoke a shell)\\n$$: run a command using your default shell (allows pipes, redirection, logic, etc)\\n\\nUseful Xiki commands:\\n\\ndocs\\nmysql\\nmongo\\n\\n\",\n",
       "  'watchers': '113',\n",
       "  'stars': '529',\n",
       "  'forks': '51',\n",
       "  'commits': '76'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'DidIStealThis\\nA small script made to idenitfy wether or not code is stolen\\n',\n",
       "  'watchers': '2',\n",
       "  'stars': '462',\n",
       "  'forks': '0',\n",
       "  'commits': '11'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'HttpProxyMiddleware\\nA middleware for scrapy. Used to change HTTP proxy from time to time.\\nInitial proxyes are stored in a file. During runtime, the middleware\\n  will fetch new proxyes if it finds out lack of valid proxyes.\\nRelated blog: http://www.kohn.com.cn/wordpress/?p=208\\nfetch_free_proxyes.py\\nUsed to fetch free proxyes from the Internet. Could be modified by\\n  youself.\\nUsage\\nsettings.py\\nDOWNLOADER_MIDDLEWARES = {\\n    \\'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware\\': 350,\\n    \\'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware\\': 351,\\n    # put this middleware after RetryMiddleware\\n    \\'crawler.middleware.HttpProxyMiddleware\\': 999,\\n}\\n\\nDOWNLOAD_TIMEOUT = 10           # 10-15 second is an experienmental reasonable timeout\\nchange proxy\\nOften, we wanna change to use a new proxy when our spider gets banned.\\n  Just recognize your IP being banned and yield a new Request in your\\n  Spider.parse method with:\\nrequest.meta[\"change_proxy\"] = True\\nSome proxy may return invalid HTML code. So if you get any exception\\n  during parsing response, also yield a new request with:\\nrequest.meta[\"change_proxy\"] = True\\nspider.py\\nYour spider should specify an array of status code where your spider\\n  may encouter during crawling. Any status code that is not 200 nor in\\n  the array would be treated as a result of invalid proxy and the proxy\\n  would be discarded. For example:\\nwebsite_possible_httpstatus_list = [404]\\nThis line tolds the middleware that the website you’re crawling would\\n  possibly return a response whose status code is 404, and do not\\n  discard the proxy that this request is using.\\nTest\\nUpdate HttpProxyMiddleware.py path in\\n  HttpProxyMiddlewareTest/settings.py.\\ncd HttpProxyMiddlewareTest\\nscrapy crawl test\\nThe testing server is hosted on my VPS, so take it easy… DO NOT\\n  waste too much of my data plan.\\nYou may start your own testing server using IPBanTest which is powered\\n  by Django.\\n',\n",
       "  'watchers': '25',\n",
       "  'stars': '317',\n",
       "  'forks': '134',\n",
       "  'commits': '26'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Using IBM Watson\\'s Speech-to-Text API to do Multi-Threaded Transcription of Really Long and Talky Videos, Such as Presidential Debates\\nA demonstration of how to use Python and IBM Watson\\'s Speech-to-Text API to do some decently accurate transcription of real-world video and audio, at amazingly fast speeds.\\nNote:_ I\\'m just spit-balling code here, not making a user-friendly package.  I\\'m focused on making an automated workflow to create fun supercuts of \"The Wire\"...and will polish the scripts and implementation later. These notes and scripts (and data files) are merely for your reference.\\ntl;dr\\nIBM Watson offers a REST-based Speech to Text API that allows free usage for the first 1,000 minutes each month (and $0.02 for each additional minute):\\n\\nWatson Speech to Text can be used anywhere there is a need to bridge the gap between the spoken word and its written form. This easy-to-use service uses machine intelligence to combine information about grammar and language structure with knowledge of the composition of an audio signal to generate an accurate transcription. It uses IBM\\'s speech recognition capabilities to convert speech in multiple languages into text. The transcription of incoming audio is continuously sent back to the client with minimal delay, and it is corrected as more speech is heard.\\n\\nIn my preliminary tests, it\\'s not quite as good as Google Translate in terms of pure accuracy, but it\\'s more than good enough for finding key words, whether they be relatively common verbs like \"fight\", \"death\", \"kill\" or proper nouns, such as Obama and countries of the world.\\nBut it doesn\\'t do too badly on very common (and aurally-ambiguous) short words such as pronouns and articles. Because Watson provides a confidence level for each word, it\\'s possible to write scripts to programmatically filter out ambiguous words.\\nHere\\'s a YouTube playlist of some automated supercuts I\\'ve created from the U.S. presidential primary debates. My favorite is probably this supercut of Senator Sanders and Secretary Clinton saying fighting words.\\n\\nHere\\'s the JSON returned from Watson, which includes word-by-word timestamps and confidence levels. Here\\'s a simplified version of it, in which the JSON is just a flat list of words.\\nIBM Watson\\'s API is robust enough to accept many concurrent requests. In the sample scripts I\\'ve included in this repo, I was able to break up a 90 minute debate into 5 minute segments and send them up to Watson simultaneously...resulting in a 6 to 7 minute processing time for the entire 90 minutes.\\nSome non-presidential examples:\\n\\nAttempting to transcribe the profanities in The Wire\\'s \"Old Cases\" episode -- (youtube supercut, obviously nsfw)\\nAttempting to transcribe a ProPublica podcast\\n\\nQuick *nix check!\\nBefore you look at the scary Python framework I\\'ve built for myself, you should first if you can work with movie/audio files and connect to Watson, using nothing but Unix tools: ffmpeg, and good ol\\' curl: check out this brief walkthrough\\nSupercut fun\\nYou probably want to see the final product. I\\'m too lazy to document all the code and haven\\'t organized it yet, but here\\'s one result: making supercuts by grepping the Watson Speech to Text data for certain words. For example, to find all \"fighting words\", e.g. war, wars, warriors, fight, bomb, kill, threat, terror, death, murder, torture:\\n python supercut.py republican-debate-sc-2016-02-13 \\'\\\\bwar(?:riors?|s)?\\\\b|fight|bomb|kill|threat|terror|death|murder|tortur\\'\\nHere\\'s a playlist of sample supercuts of presidential people:\\nRepublican Debate, South Carolina, 2016-02-13:\\n\\nPEOPLE\\nAmerica, and other geopolitical words\\nObama, Clinton, and Bush\\nFighting words (fight, bomb, kill)\\nNegative words (wrong, bad)\\nPositive words (good, best)\\n\\nDemocratic Debate, Wisconsin, 2016-02-11:\\n\\nPEOPLE\\nAmerica, and other geopolitical words\\nObama, Clinton, and Bush\\nFighting words (fight, bomb, kill)\\nBut, Why, Not, Yes, Now\\n\\nObama weekly address\\n\\ncriminal, justice, reform\\nwho, what, when, where, why, how (original video)\\n\\n\\nThe technical details\\nHow it works\\nAfter you\\'ve downloaded a video file to disk, the assorted scripts and commands in this repo will:\\n\\nConvert the file to mp4 if necessary\\nCreate a project subfolder to store the video file and all derived audio and transcripts file\\nExtract the audio from as 16-bit, 16khz WAV files\\nSplit the audio into segments (300 seconds each, by default)\\nSend each of those segments to Watson\\'s API to be analyzed and transcribed.\\nSaves the raw responses from Watson\\'s API for each audio file\\nCompiles all of the resulting responses into one data file, as if you had sent the entire audio file to be analyzed in a single go.\\n\\nThe advantages of splitting up the audio is that it allows the transcription to be done in parallel. An hour-long audio track would take probably an hour to get a response back (if your internet connection doesn\\'t fail), whereas 60 parallel requests to analyze 1-minute each will take roughly...1 minute to complete.\\nI haven\\'t tested the upper-bounds in concurrent requests to Watson\\'s API, though I was able to send around 30 5-minute requests all at once without getting an errors.\\nHere are some sample results in the projects/ folder:\\n\\nThe Republican Presidential Debate, South Carolina, Feb. 13, 2016\\nDonald Trump\\'s \"Live Free or Die\" commercial\\nPresident Obama\\'s Weekly Video Address, Oct. 31, 2015\\n\\nRequirements\\nIBM Watson\\nThe transcription power comes from IBM Watson\\'s Speech-to-Text REST API. After cutting up a video into 5-minute segments, I then upload all of the audio files in parallel to Watson, which can complete the entire batch in nearly just 5 minutes.\\n\\nLive Watson Speech-to-Text demo\\nWatson\\'s Speech-to-Text documentation\\nAPI reference\\n\\nGetting started with IBM Bluemix\\nYou have to sign up for an IBM Bluemix account, which is free and doesn\\'t require a credit card for the first month.\\nAfter signing up for Bluemix, you can find the console page for the speech-to-text API here, where you can get user credentials. This repo contains a sample file: credsfile_watson.SAMPLE.json\\nThe pricing is pretty generous, in terms of testing things out: 1,000 minutes free each month. Every additional minute is $0.02 -- i.e. transcribing an hour\\'s worth of audio will cost $1.20.\\nQuickie Watson Testy!\\nBefore you get into the Python stuff, you should see if you are properly initialized with Watson by making contact with it from the command-line (i.e. bash, i.e. uh not sure if it will work on Windows like this):\\nIf you don\\'t have a WAV file at hand, you can install the youtube-dl command-line tool:\\n$ pip install youtube-dl\\n\\nAnd then download Trump\\'s Live Free or Die commercial. The following command downloads a movie file, bb4TxjvQlh0.mkv, and extracts a WAV file named bb4TxjvQlh0.wav:\\nyoutube-dl \"https://www.youtube.com/watch?v=bb4TxjvQlh0\" \\\\\\n  --keep-video \\\\\\n  --extract-audio \\\\\\n  --audio-format wav \\\\\\n  --audio-quality 16K \\\\\\n  --id\\nIn the next step, I assume you have a file named bb4TxjvQlh0.wav, but you are free to use any WAV audio file.\\n(Note: the whole movie-file thing is totally ancillary...Watson doesn\\'t care if the audio file comes from a movie or you recording into your microphone or whatever. But people like to transcribe videos, which is why I include the step.)\\nThis next step is what contacts Watson\\'s API. Replace USERNAME and PASSWORD with whatever credentials you got from the IBM Bluemix Developer Panel.\\nThe --data-binary flag wants a file name (prepended with @).\\nWhen the audio file is uploaded and Watson returns a response, it will be saved to transcript.json\\ncurl -X POST \\\\\\n     -u USERNAME:PASSWORD     \\\\\\n     -o transcript.json        \\\\\\n     --header \"Content-Type: audio/wav\"    \\\\\\n     --header \"Transfer-Encoding: chunked\" \\\\\\n     --data-binary \"@bb4TxjvQlh0.wav\"        \\\\\\n     \"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?continuous=true&timestamps=true&word_confidence=true&profanity_filter=false\"\\nIf this doesn\\'t work for you, then either your Internet is down, Watson is down, or you don\\'t have the proper user/password credentials.\\nPython stuff\\nThis project uses:\\n\\nAnaconda 3-2.4.0\\nPython 3.5.1\\nRequests\\nmoviepy - currently, just being used as a very nice wrapper around ffmpeg, to do audio-video conversion and extraction. But has a lot of potential for laughter and games via programmatic editing.\\n\\nmoviepy will install ffmpeg if you don\\'t already have it installed\\n\\n\\n\\nDemonstrations\\nRepublican Debate in South Carolina, Feb. 13, 2016\\nCheck out the projects/republican-debate-sc-2016-02-13 folder in this repo to see the raw JSON response files and their corresponding .WAV audio, as extracted from the Feb. 13, 2016 Republican Presidential Candidate debate in South Carolina:\\n\\n\\n\\nDonald Trump \"Live Free or Die\" commercial (39 seconds)\\nThe commercial can be seen here on YouTube:\\n\\n\\n\\nThe project directory generated: projects/trump-nh/\\nBecause the video is so short, the directory includes the video file, the extracted audio, as well as the segmented audio and raw Watson JSON responses. For this example, I made the segments 10 seconds long.\\nTo compile the transcript text:\\nimport json\\nfrom glob import glob\\nfilenames = glob(\"./projects/trump-nh/transcripts/*.json\")\\n\\nfor fn in filenames:\\n  with open(fn, \\'r\\') as t:\\n      data = json.loads(t.read())\\n      for x in data[\\'results\\']:\\n          best_alt = x[\\'alternatives\\'][0]\\n          print(best_alt[\\'transcript\\'])\\nThe result:\\n\\nthis great slogan of the Hampshire live free or die means so much\\nso many people all over the world they use that expression it means liberty it means freedom it means free enterprise\\nmean safe\\nthe insecurity it means borders it means strong strong military where nobody\\'s going to mess with us it means taking care of our vets\\nwhat a great slogan congradulations New Hampshire\\n\\n\\nwonderful job dnmt\\nI\\nand\\n\\nNote that the last 3 tokens, dmnt I and, are a result of the Watson API getting confused by the dramatic music that closes the commercial. Luckily, the JSON response includes, among timestamp data for each work, a confidence level as well.\\nIt actually is spot on for Trump\\'s full closing sentence (not sure why \"congradulations\" is used...)...the confidence levels for dmnt I and were very low comparatively...I think dmnt is some kind of code word used by the API to indicate something, not that Watson thought that dmnt was actually said (see the full JSON response here)\\n{\\n    \"word_confidence\": [\\n        [\\n            \"what\",\\n            0.9999999999999674\\n        ],\\n        [\\n            \"a\",\\n            0.9999999999999672\\n        ],\\n        [\\n            \"great\",\\n            0.999999999999967\\n        ],\\n        [\\n            \"slogan\",\\n            0.9964234383591973\\n        ],\\n        [\\n            \"congradulations\",\\n            0.7798716606178608\\n        ],\\n        [\\n            \"New\",\\n            0.9999999999999933\\n        ],\\n        [\\n            \"Hampshire\",\\n            0.9845177369977128\\n        ]\\n    ]\\n}\\nPresident Obama weekly address for October 31, 2015 (3 minutes)\\nHere\\'s a quick demonstration of Watson\\'s accuracy given a weekly video address from President Obama (~3 minutes):\\n\\nVideo landing page at Whitehouse.gov\\nVideo file: 103115_WeeklyAddress.mp4\\nAudio file: 00000-00190.wav\\nWatson JSON response: 00000-00190.json\\nThe produced file folder: projects/obama-weekly-address-2015-10-31/\\n\\n(because President Obama\\'s video address is just about 3 minutes long, only audio file is extracted, and only one call to Watson\\'s API is made)\\nRight now there\\'s just a bunch of sloppy scripts that need to be refactored. There\\'s a script named init.py that you can run from the command-line that will read an existing video file, create a project folder, cut up the audio, and do the transcriptions. It assumes that you have a file named credsfile_watson.json relative to init.py.\\nSome code for the commandline, to download the file, then to run init.py:\\ncurl -o \"/tmp/obama-weekly-address-2015-10-31.mp4\" \\\\\\n  https://www.whitehouse.gov/WeeklyAddress/2015/103115-QREDSC/103115_WeeklyAddress.mp4\\n\\npython init.py /tmp/obama-weekly-address-2015-10-31.mp4\\nThe output produced by init.py:\\n[MoviePy] Writing audio in /Users/dtown/watson-word-watcher/projects/obama-weekly-address-2015-10-31/full-audio.wav\\n[MoviePy] Done.                                                                                            \\n[MoviePy] Writing audio in /Users/dtown/watson-word-watcher/projects/obama-weekly-address-2015-10-31/audio-segments/00000-00190.wav\\n[MoviePy] Done.  \\n\\nTranscribe\\nThe biggest bottleneck is transcribing the audio. The transcribe.py script does all the transcription in one big go:\\npython transcribe.py projects/obama-weekly-address-2015-10-31\\n\\nSending to Watson API:\\n   /Users/dtown/watson-word-watcher/projects/obama-weekly-address-2015-10-31/audio-segments/00000-00190.wav\\nTranscribed:\\n   /Users/dtown/watson-word-watcher/projects/obama-weekly-address-2015-10-31/transcripts/00000-00190.json\\n\\nAnd then run these scripts for a quickie processing of the JSON transcript:\\npython compile.py projects/obama-weekly-address-2015-10-31\\npython rawtext.py projects/obama-weekly-address-2015-10-31\\npython analyze.py projects/obama-weekly-address-2015-10-31\\n\\nThe output:\\n\\nhi everybody today there are two point two million people behind bars in America and millions more on parole or probation\\nevery year we spend eighty billion\\nin taxpayer dollars\\nkeep people incarcerated\\nmany are nonviolent offender serving unnecessarily long sentences\\nI believe we can disrupt the pipeline from underfunded schools overcrowded jails\\nI believe we can address the disparities in the application of criminal justice from arrest rates to sentencing to incarceration\\nand I believe we can help those who have served their time and earned a second chance\\nget the support they need to become productive members of society\\nthat\\'s why over the course of this year I\\'ve been talking to folks around the country about reforming our criminal justice system\\nto make it smarter fairer and more effective\\nin February I sat down in the oval office with police officers from across the country\\nin the spring\\nI met with police officers and young people in Camden New Jersey where they\\'re using community policing and data to drive down crime\\nover the summer I visited a prison in Oklahoma to talk with inmates and correction officers about rehabilitating prisoners\\npreventing more people from ending up there in the first place\\ntwo weeks ago I visit West Virginia to meet with families battling prescription drug heroin abuse\\nas well as people who are working on new solutions for treatment and rehabilitation\\nlast week I traveled to Chicago to thank police chiefs from across the country for all that their officers do to protect Americans\\nto make sure they get the resources they need to get the job done\\nand to call for common sense gun safety reforms that would make officers and their communities safe\\nwe know that having millions of people in the criminal justice system without any ability to find a job after release is unsustainable\\nit\\'s bad for communities and it\\'s bad for our economy\\nso on Monday I\\'ll travel to Newark New Jersey to highlight efforts to help Americans\\npaid their debt to society re integrate back into their communities\\neveryone has a role to play for businesses that are hiring ex offenders\\nto philanthropies they\\'re supporting education and training programs\\nand I\\'ll keep working with people in both parties to get criminal justice reform bills to my desk\\nincluding a bipartisan bill that would reduce mandatory minimums for nonviolent drug offenders and reward prisoners\\nshorter sentences if they complete programs that make them less likely\\ncommit a repeat offense\\nthere\\'s a reason good people across the country are coming together to reform our criminal justice system\\nbecause it\\'s not about politics\\nit\\'s about whether we as a nation live up to our founding ideals of liberty and justice for all\\nand working together we can make sure that we do\\nthanks everybody have a great weekend and have a safe and happy Halloween\\n\\nYou can compare it to the transcript here.\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '302',\n",
       "  'forks': '30',\n",
       "  'commits': '41'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Think of it as flatpages for small bits of reusable content you might want to insert into your templates and manage from the admin interface.\\nThis is really nothing more than a model and a template tag.\\nBy adding chunks to your installed apps list in your Django project and performing a ./manage.py syncdb, you\\'ll be able to add as many \"keyed\" bits of content chunks to your site.\\nThe idea here is that you can create a chunk of content, name it with a unique key (for example: home_page_left_bottom) and then you can call this content from a normal template.\\nWhy would anyone want this?\\nWell it essentially allows someone to define \"chunks\" (I had wanted to call it blocks, but that would be very confusing for obvious reasons) of content in your template that can be directly edited from the awesome Django admin interface.  Throwing a rich text editor control on top of it make it even easier.\\nUsage:\\n{% load chunks %}\\n<html>\\n  <head>\\n    <title>Test</title>\\n  </head>\\n  <body>\\n    <h1> Blah blah blah</h1>\\n    <div id=\"sidebar\">\\n        ...\\n    </div>\\n    <div id=\"left\">\\n        {% chunk \"home_page_left\" %}\\n    </div>\\n    <div id=\"right\">\\n        {% chunk \"home_page_right\" %}\\n    </div>\\n  </body>\\n</html>\\n\\nif you need the Chunk object in the template (maybe you\\'ve added some generic relations to it) you should use the {% get chunk %} templatetag:\\n{% load chunks %}\\n\\n{% get_chunk \"home_page_left\" as chunk_obj %}\\n\\n<!-- ... use the Chuck object, then display it\\'s contents -->\\n<div>{{ chuck_obj.content }}</div>\\n\\nThis is really helpful in those cases where you want to use django.contrib.flatpages but you need multiple content areas.  I hope this is helpful to people and I\\'ll be making minor edits as I see them necessary.\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '300',\n",
       "  'forks': '104',\n",
       "  'commits': '20'},\n",
       " {'language': 'Python 96.2',\n",
       "  'readme': 'Docker cookbooks\\nDocker cookbooks is a collection of Dockerfiles and configs to build images the way you need them.\\n',\n",
       "  'watchers': '22',\n",
       "  'stars': '296',\n",
       "  'forks': '62',\n",
       "  'commits': '36'},\n",
       " {'language': 'Python 94.9',\n",
       "  'readme': \"Faster R-CNN\\nThis repo has been deprecated. Here is the complete codes for training Faster-RCNN on your data and using the pre-trained Faster-RCNN model for new data: ChainerCV\\nThis is an experimental implementation of Faster R-CNN in Chainer based on Ross Girshick's work: py-faster-rcnn codes.\\nRequirement\\nUsing anaconda is strongly recommended.\\n\\n\\nPython 2.7.6+, 3.4.3+, 3.5.1+\\n\\nChainer 1.22.0+\\nNumPy 1.9, 1.10, 1.11\\nCython 0.25+\\nOpenCV 2.9+, 3.1+\\n\\n\\n\\nInstallation of dependencies\\npip install numpy\\npip install cython\\npip install chainer\\npip install chainercv\\n# for python3\\nconda install -c https://conda.binstar.org/menpo opencv3\\n# for python2\\nconda install opencv\\n\\nFor Windows users\\nThere's a known problem in cpu_nms.pyx. But a workaround has been posted here (and see also the issue posted to the original py-faster-rcnn).\\nSetup\\n1. Build extensions\\npython setup.py build_ext -i\\n\\nInference\\n1. Download pre-trained model\\nif [ ! -d data ]; then mkdir data; fi\\ncurl https://dl.dropboxusercontent.com/u/2498135/faster-rcnn/VGG16_faster_rcnn_final.model?dl=1 -o data/VGG16_faster_rcnn_final.model\\n\\nNOTE: The model definition in faster_rcnn.py has been changed, so if you already have the older pre-trained model file, please download it again to replace the older one with the new one.\\n2. Use forward.py\\ncurl -O http://vision.cs.utexas.edu/voc/VOC2007_test/JPEGImages/004545.jpg\\npython forward.py --img_fn 004545.jpg --gpu 0\\n\\n--gpu 0 turns on GPU. When you turn off GPU, use --gpu -1 or remove --gpu option.\\n\\nLayers\\nSummarization of Faster R-CNN layers used during inference\\nRPN\\nThe region proposal layer (RPN) is consisted of AnchorTargetLayer and ProposalLayer. RPN takes feature maps from trunk network like VGG-16, and performs 3x3 convolution to it. Then, it applies two independent 1x1 convolutions to the output of the first 3x3 convolution. Resulting outputs are rpn_cls_score and rpn_bbox_pred.\\n\\nThe shape of rpn_cls_score is (N, 2 * n_anchors, 14, 14) because each pixel on the feature map has n_anchors bboxes and each bbox should have 2 values that mean object/background.\\nThe shape of rpn_bbox_pred is (N, 4 * n_anchors, 14, 14) because each pixel on the feature map has n_anchors bboxes, and each bbox is represented with 4 values that mean left top x and y, width and height.\\n\\nTraining\\n1. Make sure chainercv has been installed\\nChainerCV is a utility library enables Chainer to treat various datasets easily. It also provides some transformation utility for data augmentation, and includes some standard algorithms for some comptuer vision tasks. Check the repo to know details. Here I use (VOCDetectionDataset)[http://chainercv.readthedocs.io/en/latest/reference/datasets.html#vocdetectiondataset] of ChainerCV. Anyway, before starting training of FasterRCNN, please install ChainerCV via pip.\\npip install chainercv\\n\\n2. Start training\\npython train_rpn.py\\n\\nFaster R-CNN Architecture\\nNote that it is a visualization of the workflow DURING INFERENCE\\n\\n\",\n",
       "  'watchers': '23',\n",
       "  'stars': '285',\n",
       "  'forks': '92',\n",
       "  'commits': '67'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Attention-Based Summarization\\nTensorflow implementation of A Neural Attention Model for Abstractive Summarization. The original code of author can be found here.\\n\\nPrerequisites\\n\\nPython 2.7 or Python 3.3+\\nTensorflow\\nGensim\\n\\nUsage\\nTo train a model with duc2013 dataset:\\n$ python main.py --dataset duc2013\\n\\nTo test an existing model:\\n$ python main.py --dataset duc2014 --forward_only True\\n\\n(This is still in progress and currently have no access to summarization dataset)\\nReferences\\n\\nEMNLP 2015 slide\\n\\nAuthor\\nTaehoon Kim / @carpedm20\\n',\n",
       "  'watchers': '33',\n",
       "  'stars': '274',\n",
       "  'forks': '72',\n",
       "  'commits': '12'},\n",
       " {'language': 'Python 97.9',\n",
       "  'readme': \"Densely Connected Convolutional Network (DenseNet)\\nThis repository contains the caffe version code for the paper Densely Connected Convolutional Networks.\\nFor a brief introduction of DenseNet, see our original Torch implementation.\\nImageNet Pretrained Models\\nSee https://github.com/shicai/DenseNet-Caffe for caffe prototxt and pre-trained models.\\nSee https://github.com/liuzhuang13/DenseNet for Torch pre-trained models.\\nSee http://pytorch.org/docs/torchvision/models.html?highlight=densenet for directly using the pretrained models in PyTorch.\\nNote\\n\\nThe models in this repo are for CIFAR datasets only (input 32x32). If you feed images with larger resolution (e.g., ImageNet images), you need to use a different downsampling strategy to keep the memory usage reasonable. See our paper or Torch code for details on ImageNet models.\\nThe code in this repo doesn't support BC-structres. However, it should be easy to modify.\\nThis code is not the code we use to obtain the results in the original paper, the details (such as input preprocessing, data augmentation, training epochs) may be different. To reproduce the results reported in our paper, see our original Torch implementation.\\n\\nResults\\nThe default setting (L=40, k=12, dropout=0.2) in the code yields a 7.09% error rate on CIFAR10 dataset (without any data augmentation).\\nUsage\\n\\nGet the CIFAR data prepared following the Caffe's official CIFAR tutorial.\\nmake_densenet.py contains the code to generate the network and solver prototxt file. First change the data path in function make_net() and preprocessing mean file in function densenet() to your own path of corresponding data file.\\nBy default make_densenet.py generates a DenseNet with Depth L=40, Growth rate k=12 and Dropout=0.2. To experiment with different settings, change the code accordingly (see the comments in the code). Example prototxt files are already included. Use python densenet_make.py to generate new prototxt files.\\nChange the caffe path in train.sh. Then use sh train.sh to train a DenseNet.\\n\\nContact\\nliuzhuangthu at gmail.com\\ngh349 at cornell.edu\\nAny discussions, suggestions and questions are welcome!\\n\",\n",
       "  'watchers': '20',\n",
       "  'stars': '268',\n",
       "  'forks': '208',\n",
       "  'commits': '13'},\n",
       " {'language': 'Python 94.0',\n",
       "  'readme': 'PYCCURACY IS LOOKING FOR A NEW MAINTAINER!\\nIf you wish to be the new maintainer leave a message in an issue.\\nPyccuracy\\nPyccuracy is a Behaviour-Driven-Development-style tool written in Python that aims to make it easier to write automated acceptance tests. It improves the readability of those tests by using a structured natural language – and a simple mechanism to extend this language – so that both developers and customers can collaborate and understand what the tests do.\\nHelp\\nPlease check our documentation. For quick usage help use the pyccuracy_help command line tool.\\nMailing list\\nJoin the Pyccuracy mailing list at Google Groups to discuss and get support from the community and team.\\nGet in touch with the team\\nIf you have further questions, please contact the team:\\n\\nBernardo Heynemann (@heynemann)\\nClaudio Figueiredo (@jcfigueiredo)\\nGabriel Falcão (@gabrielfalcao)\\nGuilherme Chapiewski (@gchapiewski)\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '231',\n",
       "  'forks': '33',\n",
       "  'commits': '599'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'RapidDevelop-Android快速开发框架\\n\\n框架持续更新中\\n这个框架是从平时项目里用的比较多的框架里整合而来\\n对本项目感兴趣的可以一起研究喜欢的朋友欢迎star\\n同时也欢迎大家的宝贵意见issues\\n如果大家对MVP模式的开发 网络爬虫以及缓存策略感兴趣的话可以看看我最新写的Freebook\\n邮箱:mychinalance@gmail.com\\nAPI地址\\n下载APK\\n\\n###English\\n##功能说明\\n\\n异常崩溃统一管理\\nretrofit rxjava okhttp rxcache------------------------------网络请求以及网络缓存\\nDemo采用MVP模式开发------------------------------------数据逻辑复用,便于维护升级\\n下拉刷新 上拉加载 及自动加载---------------------------实现监听方便快捷\\nRecyclerView设配器------------------------------------------再也不需要写ViewHolder\\nRecyclerView item加载动画--------------------------------多种动画效果一行代码解决\\n页面状态统一管理 加载中  无数据  无网络-------------所有页面均可添加\\n图片显示与缓存 GIF图片显示\\nTab+Fragment快速实现\\n视频播放(仿QQ空间,秒拍等List播放)\\n\\n\\n##效果图展示\\n     \\n\\n##使用说明\\n导入 lcrapiddeveloplibrary 到项目\\n在 build.gradle 的 dependencies 添加:\\ndependencies {\\ncompile fileTree(include: [\\'*.jar\\'], dir: \\'libs\\')\\n....\\ncompile project(\\':lcrapiddeveloplibrary\\')\\n}\\n\\n##轻松实现异常统一管理\\nMyApplication里面初始化就可以了\\npublic class MyApplication extends Application {\\n\\n   \\n    @Override\\n    public void onCreate() {\\n        super.onCreate();\\n       \\n        //初始化异常管理工具\\n        Recovery.getInstance()\\n                .debug(true)//关闭后 在错误统一管理页面不显示异常数据\\n                .recoverInBackground(false)\\n                .recoverStack(true)\\n                .mainPage(WelcomeActivity.class)//恢复页面\\n                .init(this);\\n    }\\n}\\n\\n##轻松实现 状态页面 下拉刷新 自动加载 item动画\\n首先layout.xml里面的编写啦 列表页面基本都是这个套路\\n<!--ProgressActivity用于状态页的控制 比如加载中  网络异常  无数据  适合任何页面-->\\n<com.xiaochao.lcrapiddeveloplibrary.viewtype.ProgressActivity\\n    xmlns:progressActivity=\"http://schemas.android.com/apk/res-auto\"\\n    android:id=\"@+id/progress\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\">\\n    <LinearLayout\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"match_parent\"\\n        android:orientation=\"vertical\"\\n        >\\n        <!--SpringView下拉刷新-->\\n        <com.xiaochao.lcrapiddeveloplibrary.widget.SpringView\\n            android:id=\"@+id/springview\"\\n            android:layout_width=\"match_parent\"\\n            android:layout_height=\"match_parent\"\\n            android:background=\"#FFFFFF\"\\n            >\\n            <android.support.v7.widget.RecyclerView\\n                android:id=\"@+id/rv_list\"\\n                android:layout_width=\"match_parent\"\\n                android:layout_height=\"match_parent\"\\n                android:background=\"#eeeeee\"/>\\n        </com.xiaochao.lcrapiddeveloplibrary.widget.SpringView>\\n\\n    </LinearLayout>\\n</com.xiaochao.lcrapiddeveloplibrary.viewtype.ProgressActivity>\\n\\n然后就是Activity里面的编写了 这个例子里使用MVP模式编写感兴趣的看我最新写的Freebook\\npublic class ListvViewActivity extends AppCompatActivity implements BaseQuickAdapter.RequestLoadMoreListener,SpringView.OnFreshListener,SchoolListView {\\n\\n    RecyclerView mRecyclerView;\\n    ProgressActivity progress;\\n    private Toolbar toolbar;\\n    private BaseQuickAdapter mQuickAdapter;\\n    private int PageIndex=1;\\n    private SpringView springView;\\n    private SchoolListPresent present;\\n\\n    @Override\\n    protected void onCreate(Bundle savedInstanceState) {\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.activity_listv_view);\\n        initView();\\n    }\\n\\n\\n    private void initView() {\\n        present = new SchoolListPresent(this);\\n        mRecyclerView = (RecyclerView) findViewById(R.id.rv_list);\\n        springView = (SpringView) findViewById(R.id.springview);\\n        //设置下拉刷新监听\\n        springView.setListener(this);\\n        //设置下拉刷新样式\\n        springView.setHeader(new RotationHeader(this));\\n        //springView.setFooter(new RotationFooter(this));mRecyclerView内部集成的自动加载  上啦加载用不上   在其他View使用\\n        progress = (ProgressActivity) findViewById(R.id.progress);\\n        //设置RecyclerView的显示模式  当前List模式\\n        mRecyclerView.setLayoutManager(new LinearLayoutManager(this));\\n        //如果Item高度固定  增加该属性能够提高效率\\n        mRecyclerView.setHasFixedSize(true);\\n        //设置页面为加载中..\\n        progress.showLoading();\\n        //设置适配器\\n        mQuickAdapter = new ListViewAdapter(R.layout.list_view_item_layout,null);\\n        //设置加载动画\\n        mQuickAdapter.openLoadAnimation(BaseQuickAdapter.SCALEIN);\\n        //设置是否自动加载以及加载个数\\n        mQuickAdapter.openLoadMore(6,true);\\n        //将适配器添加到RecyclerView\\n        mRecyclerView.setAdapter(mQuickAdapter);\\n         //设置自动加载监听\\n        mQuickAdapter.setOnLoadMoreListener(this);\\n        //请求网络数据\\n        present.LoadData(PageIndex,12,false);\\n    }\\n   //自动加载\\n    @Override\\n    public void onLoadMoreRequested() {\\n        PageIndex++;\\n        present.LoadData(PageIndex,12,true);\\n    }\\n    //下拉刷新\\n    @Override\\n    public void onRefresh() {\\n        PageIndex=1;\\n        present.LoadData(PageIndex,12,false);\\n    }\\n  \\n    /*\\n    * MVP模式的相关状态\\n    *\\n    * */\\n    @Override\\n    public void showProgress() {\\n        progress.showLoading();\\n    }\\n\\n    @Override\\n    public void hideProgress() {\\n        progress.showContent();\\n    }\\n\\n    @Override\\n    public void newDatas(List<UniversityListDto> newsList) {\\n        //进入显示的初始数据或者下拉刷新显示的数据\\n        mQuickAdapter.setNewData(newsList);//新增数据\\n        mQuickAdapter.openLoadMore(10,true);//设置是否可以下拉加载  以及加载条数\\n        springView.onFinishFreshAndLoad();//刷新完成\\n    }\\n\\n    @Override\\n    public void addDatas(List<UniversityListDto> addList) {\\n        //新增自动加载的的数据\\n        mQuickAdapter.notifyDataChangedAfterLoadMore(addList, true);\\n    }\\n\\n    @Override\\n    public void showLoadFailMsg() {\\n        //设置加载错误页显示\\n        progress.showError(getResources().getDrawable(R.mipmap.monkey_cry), Constant.ERROR_TITLE, Constant.ERROR_CONTEXT, Constant.ERROR_BUTTON, new View.OnClickListener() {\\n            @Override\\n            public void onClick(View v) {\\n                PageIndex=1;\\n                present.LoadData(PageIndex,12,false);\\n            }\\n        });\\n    }\\n\\n    @Override\\n    public void showLoadCompleteAllData() {\\n        //所有数据加载完成后显示\\n        mQuickAdapter.notifyDataChangedAfterLoadMore(false);\\n        View view = getLayoutInflater().inflate(R.layout.not_loading, (ViewGroup) mRecyclerView.getParent(), false);\\n        mQuickAdapter.addFooterView(view);\\n    }\\n\\n    @Override\\n    public void showNoData() {\\n        //设置无数据显示页面\\n        progress.showEmpty(getResources().getDrawable(R.mipmap.monkey_cry),Constant.EMPTY_TITLE,Constant.EMPTY_CONTEXT);\\n    }\\n}\\n\\n##轻松实现视频列表播放\\n列表部分和上面的一样就不说了,我这边主要描叙视频播放的部分 是在不懂得可以clone到本地仓库跑一边\\nitem_layout.xml\\n<LinearLayout\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"wrap_content\"\\n        android:padding=\"5dp\"\\n        android:orientation=\"vertical\">\\n        <com.xiaochao.lcrapiddeveloplibrary.Video.JCVideoPlayerStandard\\n            android:id=\"@+id/video_list_item_playr\"\\n            android:layout_width=\"match_parent\"\\n            android:layout_height=\"wrap_content\"/>\\n        <LinearLayout\\n            android:layout_width=\"match_parent\"\\n            android:layout_height=\"match_parent\"\\n            android:orientation=\"horizontal\"\\n            android:padding=\"5dp\"\\n            android:gravity=\"center_vertical\">\\n            <ImageView\\n                android:id=\"@+id/video_list_item_image\"\\n                android:layout_width=\"100dp\"\\n                android:layout_height=\"70dp\"\\n                android:src=\"@mipmap/def_head\"/>\\n            <LinearLayout\\n                android:layout_width=\"0dp\"\\n                android:layout_weight=\"1\"\\n                android:layout_height=\"wrap_content\"\\n                android:layout_marginLeft=\"15dp\"\\n                android:layout_marginTop=\"5dp\"\\n                android:layout_marginBottom=\"5dp\"\\n                android:layout_marginRight=\"10dp\"\\n                android:orientation=\"vertical\">\\n                <TextView\\n                    android:id=\"@+id/video_list_item_text_title\"\\n                    android:layout_width=\"wrap_content\"\\n                    android:layout_height=\"wrap_content\"\\n                    android:textColor=\"#666666\"\\n                    android:text=\"标题\"\\n                    android:textSize=\"15dp\"/>\\n                <TextView\\n                    android:id=\"@+id/video_list_item_text_context\"\\n                    android:layout_width=\"wrap_content\"\\n                    android:layout_marginTop=\"5dp\"\\n                    android:textColor=\"#999999\"\\n                    android:textSize=\"13dp\"\\n                    android:text=\"内容\"\\n                    android:lines=\"3\"\\n                    android:ellipsize=\"end\"\\n                    android:layout_height=\"wrap_content\"/>\\n            </LinearLayout>\\n        </LinearLayout>\\n\\n    </LinearLayout>\\n\\n然后就是adapter里面对视频控件的赋值处理\\npublic class VideoLisViewAdapter extends BaseQuickAdapter<VideoListDto> {\\n\\n    public VideoLisViewAdapter(int layoutResId, List<VideoListDto> data) {\\n        super(layoutResId, data);\\n    }\\n\\n    public VideoLisViewAdapter(List<VideoListDto> data) {\\n        super(data);\\n    }\\n\\n    public VideoLisViewAdapter(View contentView, List<VideoListDto> data) {\\n        super(contentView, data);\\n    }\\n\\n    @Override\\n    protected void convert(BaseViewHolder helper, VideoListDto item) {\\n        helper.setText(R.id.video_list_item_text_title,item.getTitle()).setText(R.id.video_list_item_text_context,item.getIntroduction());\\n        //Glide加载图片  并且支持gif动图\\n        Glide.with(mContext)\\n                .load(item.getPictureUrl())\\n                .crossFade()\\n                .placeholder(R.mipmap.def_head)\\n                .into((ImageView) helper.getView(R.id.video_list_item_image));\\n        //对视频的赋值 添加视频播放地址(使用原地址  .mp4之类的  这个要注意)和标题\\n        ((JCVideoPlayerStandard)helper.getView(R.id.video_list_item_playr)).setUp(item.getAppVideoUrl(),item.getTitle());\\n        Glide.with(mContext)\\n                .load(item.getPictureUrl())\\n                .crossFade()\\n                .placeholder(R.mipmap.main_mini_m)\\n                .into((((JCVideoPlayerStandard) helper.getView(R.id.video_list_item_playr)).thumbImageView));\\n    }\\n}\\n\\n###Tab+Fragment快速实现\\n还是原来的配方 layout.xml\\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:tools=\"http://schemas.android.com/tools\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n    android:orientation=\"vertical\"\\n    tools:context=\"com.xiaochao.lcrapiddevelop.UI.Tab.TabActivity\">\\n    <android.support.v7.widget.Toolbar\\n        android:id=\"@+id/toolbar\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"?attr/actionBarSize\"\\n        android:background=\"?attr/colorPrimary\"\\n        app:popupTheme=\"@style/AppTheme.PopupOverlay\" />\\n    <!--显示头部滑块-->\\n    <FrameLayout\\n        android:id=\"@+id/tab\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"wrap_content\"\\n        android:background=\"#fff\"\\n        />\\n    <android.support.v4.view.ViewPager\\n        android:id=\"@+id/viewpager\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"match_parent\"\\n        />\\n</LinearLayout>\\n\\n然后就是头部的xml编写了\\n<com.xiaochao.lcrapiddeveloplibrary.SmartTab.SmartTabLayout\\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n    android:id=\"@+id/viewpagertab\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"40dp\"\\n    android:background=\"#FFFFFF\"\\n\\n    app:stl_defaultTabTextColor=\"@color/custom_tab\"\\n    app:stl_distributeEvenly=\"true\"\\n    app:stl_defaultTabTextHorizontalPadding=\"5dp\"\\n    app:stl_indicatorColor=\"@color/title_bag\"\\n    app:stl_indicatorCornerRadius=\"0dp\"\\n    app:stl_indicatorInterpolation=\"smart\"\\n    app:stl_indicatorThickness=\"3dp\"\\n    app:stl_defaultTabTextSize=\"13dp\"\\n    app:stl_dividerColor=\"@color/bag_gray\"\\n    app:stl_dividerThickness=\"1dp\"\\n    app:stl_overlineColor=\"@color/bag_gray\"\\n    app:stl_underlineColor=\"#00000000\"\\n    app:stl_defaultTabBackground=\"@color/bag_gray_transparent\"\\n    />\\n\\n完全可以按照自己想要的风格玩  下面表格为 可设置的属性\\n\\n\\n\\n\\nattr\\n描述\\n\\n\\n\\n\\nstl_indicatorAlwaysInCenter\\n如果设置为真,有源标签总是显示在中心(如报摊google app),默认的错误\\n\\n\\nstl_indicatorWithoutPadding\\n如果设置为true,画的指标没有填充选项卡中,默认的错误\\n\\n\\nstl_indicatorInFront\\n画前的指示器下划线,默认的错误\\n\\n\\nstl_indicatorInterpolation\\n行为的指标:“线性”或“智能”\\n\\n\\nstl_indicatorGravity\\n图的位置指示器:“底”或“前”或“中心”,默认“底”\\n\\n\\nstl_indicatorColor\\n标志的颜色\\n\\n\\nstl_indicatorColors\\n多种颜色的指标,可以设置每个选项卡的颜色\\n\\n\\nstl_indicatorThickness\\n厚度指标\\n\\n\\nstl_indicatorWidth\\n的宽度指标,默认“汽车”\\n\\n\\nstl_indicatorCornerRadius\\n圆角半径的指标\\n\\n\\nstl_overlineColor\\n顶线的颜色\\n\\n\\nstl_overlineThickness\\n顶线的厚度\\n\\n\\nstl_underlineColor\\n颜色的底线\\n\\n\\nstl_underlineThickness\\n厚度的底线\\n\\n\\nstl_dividerColor\\n颜色之间的分隔器选项卡\\n\\n\\nstl_dividerColors\\n多种颜色的选项卡之间的分隔器,可以设置每个选项卡的颜色\\n\\n\\nstl_dividerThickness\\n分频器的厚度\\n\\n\\nstl_defaultTabBackground\\n背景可拉的每个选项卡。 一般设置StateListDrawable\\n\\n\\nstl_defaultTabTextAllCaps\\n如果设置为真,所有选项卡标题大写,违约事实\\n\\n\\nstl_defaultTabTextColor\\n文本的颜色包括默认的选项卡\\n\\n\\nstl_defaultTabTextSize\\n文本包括默认的选项卡的大小\\n\\n\\nstl_defaultTabTextHorizontalPadding\\n文本布局填充默认的选项卡包括\\n\\n\\nstl_defaultTabTextMinWidth\\n最小宽度的标签\\n\\n\\nstl_customTabTextLayoutId\\n布局ID定义自定义选项卡。 如果你不指定一个布局,使用默认选项卡\\n\\n\\nstl_customTabTextViewId\\n文本视图ID在一个自定义选项卡布局。 如果你不与customTabTextLayoutId定义,不工作\\n\\n\\nstl_distributeEvenly\\n如果设置为真,每个选项卡都给出同样的重量,默认的错误\\n\\n\\nstl_clickable\\n如果设置为false,禁用选择选项卡单击,违约事实\\n\\n\\nstl_titleOffset\\n如果设置为“auto_center”,中间的幻灯片的位置选项卡中心将继续。 如果指定一个维度将抵消从左边缘,默认24 dp\\n\\n\\nstl_drawDecorationAfterTab\\n画装饰(指示器和线)绘图选项卡后,默认的错误\\n\\n\\n\\n\\n好了接下来就TabActivity\\npublic class TabActivity extends AppCompatActivity {\\n\\n    ViewGroup tab;\\n    ViewPager viewpager;\\n    @Override\\n    protected void onCreate(Bundle savedInstanceState) {\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.activity_tab);\\n        initView();\\n    }\\n\\n    private void initView() {\\n        tab = (ViewGroup) findViewById(R.id.tab);\\n        viewpager = (ViewPager) findViewById(R.id.viewpager);\\n        //使用方才定义头部\\n        tab.addView(LayoutInflater.from(this).inflate(R.layout.tab_top_layout, tab, false));\\n    \\n        SmartTabLayout viewPagerTab = (SmartTabLayout) findViewById(R.id.viewpagertab);\\n        \\n        FragmentPagerItems pages = new FragmentPagerItems(this);\\n        \\n        //添加Fragment  FragmentPagerItem.of(\"头部显示标题\", \"建立的fragment\",\"需要传值的可以传Bundle\")\\n        for (int i=0;i<4;i++) {\\n            pages.add(FragmentPagerItem.of(\"Tab\"+i, TabFragment.class));\\n        }\\n\\n        FragmentPagerItemAdapter adapter = new FragmentPagerItemAdapter(\\n                getSupportFragmentManager(), pages);\\n\\n        viewpager.setAdapter(adapter);\\n        viewPagerTab.setViewPager(viewpager);\\n    }\\n}\\n\\n\\n##特别感谢\\n\\nJieCaoVideoPlayer\\nSpringView\\nSmartTabLayout\\nBaseRecyclerViewAdapterHelper\\nRecovery\\n\\n',\n",
       "  'watchers': '23',\n",
       "  'stars': '607',\n",
       "  'forks': '155',\n",
       "  'commits': '36'},\n",
       " {'language': 'Java 98.9',\n",
       "  'readme': 'About\\nstorm-contrib is a community repository for modules to use with Storm. These include a variety of spouts/bolts for integrating with other systems (Redis, Kafka, MongoDB, etc), as well as code for common tasks a Storm developer encounters.\\nFor more information about Storm itself, see the Storm GitHub repository.\\nOrganization\\nstorm-contrib is organized as a \"super-project\" with a sub-folder for each module. Each module is distributed independently and module owners are responsible for distribution.\\n##Git Submodules\\nSome storm-contrib modules are git submodules (links to external github repositories). This allows storm-contrib sub-projects to be maintained externally (so those projects can maintain branches and tags independently), but also included in storm-contrib to increase community visibility.\\nMore information about how git submodules work can be found in the online git documentation on submodules.\\nInitializing storm-contrib submodules\\nWhen you clone storm-contrib, the modules that are git submodules will appear as empty directories.\\nTo initialize the git submodules use the following command:\\ngit submodule init\\n\\nTo pull down the latest versions of submodules:\\ngit submodule update\\n\\nContributing\\nIf you\\'re interested in contributing a module to storm-contrib, send an email to the Storm mailing list. You will then be given commit rights to storm-contrib. The advantage of having your module be part of storm-contrib instead of your own project is more visibility for your code. However, if you\\'d rather maintain your module as its own project that\\'s fine too!\\nAdding your storm-related project as a git submodule\\nOnce you have signed the contributor licence agreement and been granted commit rights to storm-contrib, you can add your project as a git submodule with the following command:\\ngit submodule add git://github.com/[username]/[projectname].git\\n\\n',\n",
       "  'watchers': '107',\n",
       "  'stars': '588',\n",
       "  'forks': '356',\n",
       "  'commits': '238'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'android_volley_examples\\nProject with examples how to use the new Volley networking framework\\n',\n",
       "  'watchers': '50',\n",
       "  'stars': '531',\n",
       "  'forks': '205',\n",
       "  'commits': '54'},\n",
       " {'language': 'Java 99.2',\n",
       "  'readme': 'jclouds moved to Apache in 2013 and archived this repository.  Please visit our\\nnew JIRA issue tracker,\\nGitHub repository, and\\nweb page.\\n',\n",
       "  'watchers': '28',\n",
       "  'stars': '477',\n",
       "  'forks': '229',\n",
       "  'commits': '8,048'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Android-ColorTrackView\\n字体或者图片可以逐渐染色和逐渐褪色的动画效果\\n使用\\n <com.zhy.view.ColorTrackView\\n        android:id=\"@+id/id_changeTextColorView\"\\n        android:layout_width=\"wrap_content\"\\n        android:layout_height=\"wrap_content\"\\n        android:layout_centerInParent=\"true\"\\n        android:layout_centerVertical=\"true\"\\n        android:padding=\"20dp\"\\n        android:background=\"#44ff0000\"\\n        android:gravity=\"center_vertical\"\\n        zhy:progress=\"0\"\\n        zhy:direction=\"left\"\\n        zhy:text=\"张鸿洋\"\\n        zhy:text_change_color=\"#ffff0000\"\\n        zhy:text_origin_color=\"#ff000000\"\\n        zhy:text_size=\"60sp\" />\\n注：zhy为命名空间，xmlns:zhy=\"http://schemas.android.com/apk/res-auto\"，可自由修改。\\n\\nprogress  [0.0f , 1.0f]\\ntext 绘制的文本\\ntext_change_color目标颜色\\ntext_origin_color原始颜色\\ntext_size字体大小\\ndirection 方向，枚举类型，支持：left,right,top,bottom\\n\\n效果图\\n简单使用\\n\\n结合ViewPager\\n\\n关于我\\n我的博客地址\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '446',\n",
       "  'forks': '131',\n",
       "  'commits': '4'},\n",
       " {'language': 'Java 99.3',\n",
       "  'readme': ' \\n\\nAll available icons (744)\\n\\nIf, like me, you\\'re tired of copying 5 images (ldpi, mdpi, hdpi, xhdpi, xxhdpi) for each icon you want to use in your app, for each color you want to use them with android-material-icons can help you.\\n\\nAbout\\nandroid-material-icons allows you to include any of the Material Design 2.1.1 icons by Google packed by Sergey Kupletsky in your texts, your ActionBar, and even in your EditTexts. Icons are infinitely scalable, and customizable with shadows and everything you can do on texts.\\nSpecial thanks to Joan Zapata for his android-iconify project since this is mostly a copy :)\\nGet started #1\\nIf you need icons on a TextView, use the { } syntax. You can put any text around it and have more than one icon in the text. Note that the shadows apply to the icons as well.\\n<IconTextView\\n    android:text=\"{zmdi-android}\"\\n    android:shadowColor=\"#22000000\"\\n    android:shadowDx=\"3\"\\n    android:shadowDy=\"3\"\\n    android:shadowRadius=\"1\"\\n    android:textSize=\"90dp\"\\n    android:textColor=\"#FF33B5E5\"\\n    ... />\\n\\nYou can either use IconTextView / ButtonTextView or use any TextView and then programmatically call Iconify.addIcons(myTextView);.\\n\\nGet started #2\\nIf you need an icon in an ImageView or in your ActionBar, then you should use IconDrawable. Again, icons are infinitely scalable and will never get fuzzy!\\n// Set an icon in the ActionBar\\nmenu.findItem(R.id.share).setIcon(\\n   new IconDrawable(this, IconValue.zmdi_share)\\n   .colorRes(R.color.ab_icon)\\n   .actionBarSize());\\nDesign-time preview (maybe working)\\n\\nCopy material font file (do not rename it) to your $ANDROID_SDK/platforms/android-$N/data/fonts/ folder for each platform $N available.\\nAssign attribute hacky_preview like this:\\n\\n<com.malinskiy.materialicons.widget.IconTextView\\n            xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n            android:layout_width=\"wrap_content\"\\n            android:layout_height=\"wrap_content\"\\n            android:text=\"{zmdi-android}\"\\n            android:textSize=\"48dp\"\\n            app:hacky_preview=\"true\"/>\\n\\nGet it\\nGradle:\\nrepositories {\\n    ...\\n    mavenCentral()\\n    ...\\n}\\n...\\ndependencies {\\n    ...\\n    compile \\'com.malinskiy:materialicons:1.0.2\\'\\n    ...\\n}\\nLicense\\nCopyright 2013 Joan Zapata\\nCopyright 2014 Anton Malinskiy\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\nIt uses Google\\'s material font licensed under OFL 1.1, which is compatible\\nwith this library\\'s license.\\n\\n    http://scripts.sil.org/cms/scripts/render_download.php?format=file&media_id=OFL_plaintext&filename=OFL.txt\\n    \\n\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '400',\n",
       "  'forks': '55',\n",
       "  'commits': '12'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'RaiflatButton\\nA raised button that lowers down to 0dp of elevation. From my blog post: https://rubensousa.github.io/2016/10/raiflatbutton\\nIt behaves like a normal button on APIs < 21\\n\\nBuild\\ndependencies {\\n   compile \\'com.github.rubensousa:raiflatbutton:0.1\\'\\n}\\nHow to\\nJust add the following xml to your layout:\\n<com.github.rubensousa.raiflatbutton.RaiflatButton\\n    android:id=\"@+id/normalButton\"\\n    style=\"@style/Base.Widget.AppCompat.Button.Colored\"\\n    android:layout_width=\"wrap_content\"\\n    android:layout_height=\"wrap_content\"\\n    android:text=\"Colored\" />\\n    \\n<com.github.rubensousa.raiflatbutton.RaiflatImageButton\\n    android:id=\"@+id/imageButton\"\\n    style=\"@style/Base.Widget.AppCompat.Button\"\\n    android:layout_width=\"wrap_content\"\\n    android:layout_height=\"wrap_content\"\\n    android:src=\"@drawable/ic_android\" />\\nIf you want to resume the normal button behavior, just use:\\nbutton.setFlatEnabled(false);\\nStyling\\nSince RaiflatButton and RaiflatImageButton both extend AppCompatButton and AppCompatImageButton, you can reuse the same AppCompat styles.\\nExample:\\n<style name=\"RaiflatButtonPrimaryStyle\" parent=\"Base.Widget.AppCompat.Button.Colored\">\\n    <item name=\"android:colorControlNormal\">?attr/colorPrimary</item>\\n</style>\\nLicense\\nCopyright 2016 Rúben Sousa\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '327',\n",
       "  'forks': '41',\n",
       "  'commits': '18'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Vertical-Intro\\nVertical intro allows you to integrate material vertical intro to your app\\n\\nThe current minSDK version is API level 14 Android 4.0 (ICE CREAM SANDWICH).\\n \\nDownload sample apk\\n#YouTube demo\\n\\n#Installation\\nGradle:\\ncompile \\'com.github.armcha:Vertical-Intro:2.0.0\\'\\nSetup and usage\\n\\nStep 1:\\nYour activity must extends from VerticalIntro activity\\npublic class TestActivity extends VerticalIntro\\nStep 2:\\nAdd activity to manifest with defined theme:\\n<activity\\nandroid:name=\".TestActivity\"\\nandroid:theme=\"@style/VerticalIntroStyle\" />\\nStep 3:\\nAdd items in init\\naddIntroItem(new VerticalIntroItem.Builder()\\n                .backgroundColor(R.color.my_color)\\n                .image(R.drawable.my_drawable)\\n                .title(\"Lorem Ipsum Lorem Ipsum\")\\n                .text(\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\" +\\n                        \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\" +\\n                        \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\")\\n                .textColor(R.color.your_color)\\n                .titleColor(R.color.your_color)\\n                .textSize(14) // in SP\\n                .titleSize(17) // in SP\\n                .nextTextColor(R.color.color1)\\n                .build());\\nReturn color for last item bottom view background color\\n @Override\\n    protected Integer setLastItemBottomViewColor() {\\n        return R.color.my_second_color;\\n    }\\nCustomize\\nNote: You must do all customizations inside init method\\nEnable or disable skip button\\nsetSkipEnabled(true); \\n\\nEnable or disable vibrate 📳 and set vibrate intensity\\nsetVibrateEnabled(true);\\nsetVibrateIntensity(20);\\n\\nSet your texts\\nsetNextText(\"OK\");\\nsetDoneText(\"FINISH HIM\");\\nsetSkipText(\"GO GO\");\\n\\nSet custom font\\nsetCustomTypeFace(Typeface.createFromAsset(getAssets(), \"fonts/NotoSans-Regular.ttf\"));\\n#\\n\\nSet text color\\n.textColor(R.color.your_color)\\n\\nSet title color\\n.titleColor(R.color.your_color)\\n\\nSet text size in SP\\n.textSize(14)\\n\\nSet title size in SP\\n.titleSize(17)\\n\\nSet skip button text color\\nsetSkipColor(R.color.your_color);\\n\\nContact\\nPull requests are more than welcome.\\nPlease fell free to contact me if there is any problem when using the library.\\n\\nEmail: armcha01@gmail.com\\nFacebook: https://web.facebook.com/chatikyana\\nGoogle +: https://plus.google.com/+ArmanChatikyan\\nWebsite: https://armcha.github.io\\n\\nLicense\\n  Vertical Intro library for Android\\n  Copyright (c) 2017 Arman Chatikyan (https://github.com/armcha/Vertical-Intro).\\n  \\n  Licensed under the Apache License, Version 2.0 (the \"License\");\\n  you may not use this file except in compliance with the License.\\n  You may obtain a copy of the License at\\n\\n     http://www.apache.org/licenses/LICENSE-2.0\\n\\n  Unless required by applicable law or agreed to in writing, software\\n  distributed under the License is distributed on an \"AS IS\" BASIS,\\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n  See the License for the specific language governing permissions and\\n  limitations under the License.\\n\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '325',\n",
       "  'forks': '52',\n",
       "  'commits': '44'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Disclaimer: unfortunately, I don\\'t do Android dev anymore, so I can\\'t maintain this library.\\n\\nPanesLibrary\\nThis library makes it easy to make native Android apps with multi-pane tablet layouts. On the phone, the app appears as a conventional app with a sliding menu and a content pane where fragments are stacked on top of each other. On the tablet, the menu and all other fragments appear in dynamically added panes of varying sizes.\\nExample\\nExampleActivity demonstrates all the necessary configuration for using PanesActivity.\\nExampleFragment uses a layout with two TextViews and a Button. The TextViews hold the index of the fragment and the length of time it\\'s been alive. This is used to demonstrate that fragments are correctly retained on activity restarts. The Button has a callback which creates a new fragment and adds it to the parent activity.\\nUsing PanesLibrary\\nMake sure you have the most up-to-date support library and ActionBarSherlock.\\nHave your activity extend PanesActivity. Then, add fragments to the activity using the following functions:\\n\\nsetMenuFragment(fragment): set the menu\\naddFragment(prevFragment, newFragment): adds newFragment after the prevFragment (clobbering any fragments that used to be after prevFragment)\\nclearFragments(): removes all the fragments except the menu fragment\\ngetMenuFragment(): get the menu fragment\\ngetTopFragment(): get the fragment on the top of the stack\\n\\nYou also need to provide a PaneSizer. This object allows you to programatically set the width of each pane based on the type of Fragment/View you want to place inside the pane.\\nYou should also implement updateFragment(...). This is run whenever a fragment is added (even on activity restarts).\\nFragments are added onto a stack where the 0th fragment is the menu. Here\\'s an example of what this means:\\n> setMenuFragment(A)\\nstack: A\\n\\n> addFragment(A, B)\\nstack: A, B\\n\\n> addFragment (B, C)\\nstack: A, B, C\\n\\n> addFragment (C, D)\\nstack: A, B, C, D\\n\\n> addFragment(B, E)\\nstack: A, B, E\\n\\n>clearFragments()\\nstack: A\\n\\nNote: PanesActivity requires you to use fragments. If you don\\'t use fragments, you can still use PanesLayout manually.\\nPanesLayout (this controls the tablet layout)\\nThe hierarchy of a PanesLayout looks like this:\\n|--------------------------------|\\n| PanesLayout                    |\\n| |----------------------------| |\\n| | PaneScrollView             | |\\n| | |------------------------| | |\\n| | | PaneView |-----------| | | |\\n| | |          | (content) | | | |\\n| | |          |-----------| | | |\\n| | |------------------------| | |\\n| |----------------------------| |\\n|--------------------------------|\\n\\nA PanesLayout can hold any number of panes. Each pane is made up of a PaneScrollView (which allows the pane contents to slide left & right), a PaneView (which provides padding on the left of the content), and some content (i.e. fragment).\\nVariables associated with each pane:\\n\\ntype: the possible values of type are defined by the user. For example, in ExampleActivity, the only possible type is DEFAULT_PANE_TYPE. This variable is used to size the pane by the PaneSizer\\nfocused: if true, then PanesLayout will never intercept touch events associated with this pane.\\nindex: the index of this pane-- starting at 0, going to # of panes - 1.\\n\\nTo add/remove/get panes:\\n\\naddPane(int type)\\naddPane(int type, boolean focused)\\nremovePanes(int i): removes all panes after the ith index\\ngetNumPanes(): the number of panes\\ngetCurrentIndex(): the current top index\\nsetIndex(int index): scroll to the pane associated with index.\\n\\nListeners/delegates:\\n\\nPaneSizer: determines what the type/focus of a pane should be based on some associated object (i.e. Fragment or View)\\nOnIndexChangedListener: this is fired whenever the visible panes change.\\n\\nCopyright 2013 Kenrick Rilee\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n\\n',\n",
       "  'watchers': '34',\n",
       "  'stars': '324',\n",
       "  'forks': '108',\n",
       "  'commits': '42'},\n",
       " {'language': 'Java 76.2',\n",
       "  'readme': 'Lighthouse\\nPlease note: this project is no longer maintained.\\nLighthouse is a decentralised, peer to peer crowdfunding application that uses the smart contracts features of the\\nBitcoin protocol. It lets you create projects and pledge to those projects.\\nget involved\\nMailing lists:\\n\\nUser discussion and questions\\nDevelopment talk\\n\\nOr sign up for occasional email newsletters on vinumeris.com.\\nYou can chat to us in the #lighthouse chatroom on Freenode IRC.\\nIf you\\'re a developer, build instructions are at the end of this README.\\nhow to tweak the user interface\\nYou can do some work on the UI without being a Java developer. If you have web design experience modifying the Lighthouse\\nUI will seem quite familiar. Ask for a binary build if you don\\'t want to compile the app yourself, and then run\\nLighthouse with the --resdir=/path/to/ui/files flag. It should point to the client/src/main/resources/lighthouse\\ndirectory from this git repository. You can use the Shortcut+S key combination to reload the UI whilst the app\\nis running. Shortcut is the Cmd key on a Mac and Control key on Windows/Linux. The UI is defined by the CSS files\\nand FXML files (similar to HTML but different language).\\nDocumentation:\\n\\nJavaFX CSS\\nFXML is best learned by poking around in the existing code, or reviewing the JavaFX docs in general.\\n\\nYou can also use the Scene Builder\\ntool to do visual UI design. It can\\'t load main.fxml but everything else should open just fine.\\nlicense and legal stuff\\nThe Lighthouse code is under the Apache license, which allows for commercial derivatives.\\nHowever the Lighthouse logo and name are not open. In future the logo images and name may be taken\\nout of this repository and put into a proprietary branch. This would be a similar arrangement to Firefox and Chrome.\\nThe purpose of this is to ensure that if someone downloads an app called \"Lighthouse\" they know what they are getting\\nand that it is not, for example, a fork or patched version of the app that may be broken in some way. If someone were\\nto distribute a fork of the app, they would have to create a new name and logo so users can tell the fork apart from\\nthe original.\\nThus to avoid confusion if you\\'d like to distribute a version of the project that doesn\\'t match the upstream sources,\\nplease invent a new name and logo for it first. Thanks.\\nbuilding from source\\nBuilding Lighthouse from source requires the Maven build tool and the bitcoinj library.\\nThe latest version of bitcoinj should be installed from source:\\n$ git clone https://github.com/bitcoinj/bitcoinj\\n$ cd bitcoinj\\n$ mvn clean install\\n\\nCompile Lighthouse with:\\n$ git clone https://github.com/vinumeris/lighthouse\\n$ cd lighthouse\\n$ mvn clean package\\n\\nRun Lighthouse with:\\n$ java -jar client/target/shaded.jar\\n\\n',\n",
       "  'watchers': '52',\n",
       "  'stars': '311',\n",
       "  'forks': '77',\n",
       "  'commits': '416'},\n",
       " {'language': 'C++ 96.4',\n",
       "  'readme': 'LightQ\\n\\nIt is a high performance,  brokered messaging queue which supports transient (1M msg/sec with microseconds latency)  and durable (~300K msg/sec with milliseconds latency) queues. Durable queues are similar to Kafka  where data are written to the file and consumers consume from the file.\\n###Features:\\n\\nHigh Performance - 1M+ msg/sec\\nLow Latency - in microseconds\\nTransient and durable queue (similar to Kafka where producer writes to the file, consumer reads from the file)\\nAuthentication per topic (userid/password validation)\\nHeader only project (embed within your project)\\nConsumer in load balancing mode(pipeline):  One of the consumer gets a message mostly in round robin)\\nConsumer as Subscribers (Each consumer gets a copy of a message)\\nBoth subscriber and pipelining mode are supported for a single topic\\nMulti Producers/Consumers for a single topic\\nUnlimited* topics per broker\\nJSON protocol to create topic and join topic (at runtime)\\nC++11 support/require\\nLogging support\\nDynamic port allocation for topic and consumer/producer bind uri\\nApache License\\nCluster support (todo)\\nClient API (todo): C, Go, Java, Rust, Lua, Ruby\\n\\nIt is mostly header only project with main.cpp as an example for broker, producer and consumer.\\nNOTE: This is an initial version and may not be ready for production use.\\n##Protocol:\\nCreate a Topic:\\n(Admin userid and password must be passed to create a topic. Also we need to define userid/password per topic which consumer/produder need to pass for authentication)\\n Send a request  to the broker\\n{\\n \"admin_password\": \"T0p$3cr31\",\\n \"admin_user_id\": \"lightq_admin\",\\n \"broker_type\": \"queue\",\\n \"cmd\": \"create_topic\",\\n \"password\": \"T0p$3cr31\",\\n \"topic\": \"test\",\\n \"user_id\": \"test_admin\"\\n}\\n\\n\\nResponse: \\n{\\n   \"cmd\": \"create_topic\",\\n   \"description\": \"topic created successfully\",\\n   \"status\": \"ok\"\\n}\\n\\n###Join Topic (Consumer):\\n(Need to pass userid/password for topic \\'test\\')\\nRequest:\\n{\\n   \"cmd\": \"join\",\\n   \"connection_type\": \"zmq\",\\n   \"password\": \"T0p$3cr31\",\\n   \"topic\": \"test\",\\n   \"type\": \"pull\",\\n   \"user_id\": \"test_admin\"\\n}\\nResponse: \\n{\\n   \"bind_uri\": \"tcp://127.0.0.1:5002\",\\n   \"cmd\": \"join\",\\n   \"status\": \"ok\",\\n   \"topic\": \"test\"\\n}\\n\\nJoin Topic (Producer):\\n(Need to pass userid/password for topic \\'test\\')\\nRequest:\\n{\\n   \"cmd\": \"join\",\\n   \"connection_type\": \"zmq\",\\n   \"password\": \"T0p$3cr31\",\\n   \"topic\": \"test\",\\n   \"type\": \"pub\",\\n   \"user_id\": \"test_admin\"\\n }\\nResponse:\\n{\\n  \"bind_uri\": \"tcp://127.0.0.1:5003\",\\n  \"cmd\": \"join\",\\n  \"status\": \"ok\",\\n  \"topic\": \"test\"\\n}\\n\\nGet the statistics about the topic\\nRequest:\\n{\\n   \"cmd\": \"stats\",\\n   \"password\": \"T0p$3cr31\",\\n   \"topic\": \"test\",\\n   \"user_id\": \"test_admin\"\\n}\\nResponse:\\n{\\n  \"cmd\": \"stats\",\\n  \"messages_received\": 9499570,\\n  \"messages_sent\": 9491554,\\n  \"publishers_count\": 1,\\n  \"queue_size\": 8016,\\n  \"status\": \"ok\",\\n  \"subscribers_count\": 1,\\n  \"total_bytes_read\": 0,\\n  \"total_bytes_written\": 0\\n\\n}\\n#Performance:\\nLaptop hardware:\\nMacBook Pro (Retina, 15-inch, Late 2013)\\nProcessor 2.3 GHz Intel Core i7\\nMemory 16 GB 1600 MHz DDR3\\n\\nBroker Type: Transient\\n##100 bytes, 10M messages\\nProducer:\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 100 event\\nTotal Messages:10000000, Time Taken:8.46577 seconds.\\nStart Time: 1427658489112, End Time:1427658497577\\n1181227 messages per seconds.\\n1000000000 bytes sent\\n112.6507 MB per second\\n\\nConsumer:\\n./dist/Release/GNU-MacOSX/lightq consumer queue zmq  pull event\\nTotal Messages:10000001, Time Taken:8.47781 seconds.\\nStart Time: 1427658489122, End Time:1427658497600\\n1179550 messages per seconds.\\n1000000004 bytes received\\n112.4907 MB per second.\\n\\n##256 bytes 10M Messages\\nProducer:\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 256 event\\nTotal Messages:10000000, Time Taken:9.2752 seconds.\\nStart Time: 1427658738559, End Time:1427658747834\\n1078143 messages per seconds.\\n2560000000 bytes sent\\n263.2186 MB per second.\\n\\nConsumer:\\n./dist/Release/GNU-MacOSX/lightq consumer queue zmq  pull event\\nTotal Messages:10000001, Time Taken:9.30292 seconds.\\nStart Time: 1427658738562, End Time:1427658747865\\n1074931 messages per seconds.\\n2560000004 bytes received\\n262.4345 MB per second.\\n\\n512 bytes 10M Messages\\nProducer:\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 512 event\\nTotal Messages:10000000, Time Taken:10.5182 seconds.\\nStart Time: 1427658940094, End Time:1427658950612\\n950734 messages per seconds.\\n5120000000 bytes sent\\n464.2258 MB per second.\\n\\nConsumer:\\n./dist/Release/GNU-MacOSX/lightq consumer queue zmq  pull event\\nTotal Messages:10000001, Time Taken:10.5296 seconds.\\nStart Time: 1427658940097, End Time:1427658950627\\n949706 messages per seconds.\\n5120000004 bytes received\\n463.7239 MB per second.\\n\\n1024 bytes 10M Messages\\nProducer:\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 1024 event\\nTotal Messages:10000000, Time Taken:19.8285 seconds.\\nStart Time: 1427659063592, End Time:1427659083420\\n504324 messages per seconds.\\n10240000000 bytes sent\\n492.5049 MB per second.\\n\\nConsumer:\\n./dist/Release/GNU-MacOSX/lightq consumer queue zmq  pull event\\nTotal Messages:10000001, Time Taken:19.8222 seconds.\\nStart Time: 1427659063603, End Time:1427659083425\\n504485 messages per seconds.\\n10240000004 bytes received\\n492.6617 MB per second.\\n\\n#Performance: (Durable broker: file)\\n##100 bytes, 10M messages\\nProducer:\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 100 event\\nTotal Messages:10000000, Time Taken:25.0786 seconds.\\nStart Time: 1427659575158, End Time:1427659600236\\n398746 messages per seconds.\\n1000000000 bytes sent\\n38.0275 MB per second.\\n\\nConsumer:\\n./dist/Release/GNU-MacOSX/lightq consumer file socket  pull event\\nTotal Messages:10000001, Time Taken:25.0945 seconds.\\nStart Time: 1427659575170, End Time:1427659600264\\n398493 messages per seconds.\\n1000000004 bytes received\\n38.0033 MB per second.\\n\\n##256 bytes 10M Messages\\nProducer:\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 256 event\\nTotal Messages:10000000, Time Taken:28.3802 seconds.\\nStart Time: 1427659339399, End Time:1427659367779\\n352358 messages per seconds.\\n2560000000 bytes sent\\n86.0250 MB per second.\\n\\nConsumer:\\n./dist/Release/GNU-MacOSX/lightq consumer file socket  pull event\\nTotal Messages:10000001, Time Taken:28.3939 seconds.\\nStart Time: 1427659339410, End Time:1427659367804\\n352188 messages per seconds.\\n2560000004 bytes received\\n85.9834 MB per second.\\n\\n512 bytes 10M Messages\\nProducer:\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 512 event\\nTotal Messages:10000000, Time Taken:31.0832 seconds.\\nStart Time: 1427890737326, End Time:1427890768409\\n321716 messages per seconds.\\n5120000000 bytes sent\\n157.0884 MB per second.\\n\\nConsumer:\\n./dist/Release/GNU-MacOSX/lightq consumer file socket  pull event\\nTotal Messages:10000001, Time Taken:31.0935 seconds.\\nStart Time: 1427890737329, End Time:1427890768423\\n321610 messages per seconds.\\n5120000004 bytes received\\n157.0363 MB per second.\\n\\n1024 bytes 10M Messages\\nProducer:\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 1024 event\\nTotal Messages:10000000, Time Taken:37.4027 seconds.\\nStart Time: 1427890878446, End Time:1427890915848\\n267360 messages per seconds.\\n10240000000 bytes sent\\n261.0942 MB per second.\\n\\nConsumer:\\n./dist/Release/GNU-MacOSX/lightq consumer file socket  pull event\\nTotal Messages:10000001, Time Taken:37.4166 seconds.\\nStart Time: 1427890878455, End Time:1427890915871\\n267260 messages per seconds.\\n10240000004 bytes received\\n260.9970 MB per second.\\n\\nPerformance Test with 100M messages: https://github.com/rohitjoshi/LightQ/blob/master/PerfTest100M.md\\n###Example: (transient broker)\\nStart Broker: (broker type:  queue, logging level: event)\\n./dist/Release/GNU-MacOSX/lightq queue event\\n\\nStart Consumer: (client: consumer,  broker type: queue, client socket: zmq, logging level: event)\\n./dist/Release/GNU-MacOSX/lightq consumer queue zmq  pull event\\n\\nStart Producer:  (client: producer, number of messages 10M, payload size: 100 bytes, logging level: event)\\n./dist/Release/GNU-MacOSX/lightq producer 10000000 100 event\\n\\n##License : \\nDependecies:\\nZeroMQ LGPL: \\n',\n",
       "  'watchers': '22',\n",
       "  'stars': '269',\n",
       "  'forks': '28',\n",
       "  'commits': '182'},\n",
       " {'language': 'C++ 98.7',\n",
       "  'readme': 'Hexagen\\nTrue coroutines for Swift, and several familiar concurrency structures built on top of them.\\nFeatures\\n\\n\\nVery little boilerplate for most use cases. (Largely made possible by Swift\\'s type inference.)\\n\\n\\nSimple unidirectional and bidirectional generator functions, as in Python, C#, ECMAscript 6, etc.:\\nlet counter = { (n: Int) in Gen<Int> { yield in\\n    for i in 0..<n {\\n        yield(i)\\n    }\\n}}\\n\\nfor i in counter(5) {\\n    println(i)\\n}\\n\\n\\nInterruptible Grand Central Dispatch task API allowing you to write asynchronous code in straightforward blocking style. When a task is waiting on some event (a timer firing, I/O availability or completion, etc.), instead of blocking the thread, it will suspend itself so its dispatch queue can continue processing tasks. When the event arrives, a block to resume the task is added to its dispatch queue.\\n\\n\\nIncluded abstractions that know how to seamlessly suspend and resume tasks as needed:\\n\\nChannel: Supports a style of communication between tasks largely inspired by Go\\'s channels and Goroutines.\\nPromise: Allows any number of tasks to await a potentially pending result and awakens all of them when one becomes available.\\n\\nTimer: A Promise that is marked as fulfilled at a specified time.\\nFeed: A lazily constructed open-ended series of Promises of a given type wrapped in Optional: after receiving a value from a Promise obtained from a Feed, you can get its successor and await the next value, and repeat until nil is returned, indicating that the feed has ended and won\\'t contain any further values. (Feed implements Sequence so you can iterate over it with a for loop, this is often the most straightforward way to use it.)\\n\\nAsyncGen: A Task subclass with additional generator-like behavior — the body function receives a \"post\" function which is used somewhat like yield, but doesn\\'t actually suspend the task; instead it sends values to an internal Feed, which other tasks can subscribe to by iterating over the task object.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n97% elegant! Very minimal abomination content, you should almost never have to encounter it.\\n\\n\\nWarnings\\n\\n\\nHexagen is in early development and pretty experimental to begin with, don\\'t count on the API not changing drastically.\\n\\n\\nYour coroutines should always exit by returning — you can leave them hanging but you will leak memory. With Swift\\'s lack of exceptions and use of ARC instead of garbage collection, I don\\'t currently see a way to unilaterally tell a coroutine to terminate but still clean up after itself.\\n\\n\\nThe task API needs to account for Cocoa APIs with thread-local behavior in order to make them work coherently and currently doesn\\'t. This may be tricky in the cases of components that don\\'t expose their thread-local variables in any directly manipulable form. Expect it to interact badly with autorelease pools.\\n\\n\\nMore generally, this approach has turned out to be very surprisingly low on complications so far, but the whole thing is still a sketchy self-indulgent hack that violates some basic assumptions that almost all existing Objective-C and Swift code can expect to safely make. It\\'s hard to say what potential interactions I might be overlooking, particularly given that the Swift toolchain is still closed-source. For now, I strongly discourage using this in production code unless you are very, very silly and reasonably confident that you are already going to hell.\\n\\n\\nIf you are under the age of 180º or find this framework offensive, please don\\'t look at it.\\n\\n\\nNotes\\n\\n\\nHexagen is written for Swift 1.2, first available in Xcode 6.3.\\n\\n\\nCurrently Promises as implemented here are fulfill-only, i.e. there isn\\'t a separate path for errors to take, like there tends to be in other languages\\' implementations of Promises. This is meant to mirror Swift\\'s overall approach to error handling: to the extent that you need to write Promises that can express error conditions, you should encode that in your own types.\\n\\n\\nIdeas/Todo\\n\\nLibrary components\\n\\nSelect\\nTimeouts\\nRead-only and write-only views of channels (and promises?)\\nElegant task-aware I/O API\\nSubscribe to Cocoa events, notifications, key-value observing, etc. via Promises/Feeds\\nBridges to and from Hexagen features for existing widely-used Swift/Objective-C concurrency libraries/frameworks/approaches\\nTask-local storage API\\n\\n\\nInternals\\n\\nAdapt to use Boost.Context directly once the upcoming version with execution_context is released and Xcode is shipping with usable support for thread_local in clang/libc++?\\n\\n\\nProject quality\\n\\nUnit tests\\nBenchmarks\\nMore examples, better organized examples\\n\\n\\n\\nExtra Credit\\n\\nSide project: implement an alternative framework based on stackless generators (like e.g. Python\\'s built-in yield: a function can only yield from itself to the function that called or most recently reentered it, because it\\'s implemented more like an ordinary function call, getting its own frame on top of the current stack while it\\'s running rather than having a separate stack to switch to).\\nImplement exception handling in pure Swift using Hexagen. (Completing this successfully is worth negative points, and I will grudgingly respect you but never fully trust you.)\\n\\nColophon\\nHexagen is released under an MIT license (see LICENSE.md), so you can pretty freely incorporate and redistribute it wherever. The internal context-switching primitive is a thin wrapper around Boost.Coroutine. Boost is free software under a permissive MIT-style license and the parts of it used by Hexagen are already included in this repository.\\nMy name is Alice Atlas and I wrote the rest of this, I did it on purpose and I\\'m not sorry dad\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '153',\n",
       "  'forks': '5',\n",
       "  'commits': '51'},\n",
       " {'language': 'C++ 87.3',\n",
       "  'readme': 'shok\\nhttp://shok.io\\nThe shok command shell is a non-POSIX interactive command language interpreter with an expressive scripting language.  It intends to be a modern, discoverable shell intended for every-day command invocation, process management, and filesystem manipulation.\\nstatus\\nshok is in its early stages of initial development.  It has the framework for a shell, a lexer, a parser, and an \"evaluator\" (type-checking, AST execution, program invocation).  If it compiles, it may let you change directories and run commands but not write programs, or much else really.  Most core features have yet to be implemented.  All language attributes are suitable for discussion and replacement.  Nevertheless, it is progressing quickly and steadily.\\nGet involved!  See http://shok.io for details.  Description of the code layout and steps forward are coming soon.\\ntodo\\nImmediate hacking priorities:\\n\\n\\nfunctions / methods\\n\\n\\ntrivial implementations of a few basic objects\\n\\n\\nstring literals\\n\\n\\nbasic interactive niceties: left/right/home/end, ^L\\n\\n\\ncomments\\n\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '148',\n",
       "  'forks': '7',\n",
       "  'commits': '424'},\n",
       " {'language': 'C++ 90.0',\n",
       "  'readme': 'What is VoodooSpark?\\nVoodooSpark is a customized firmware build for Particle\\'s Spark Core and Photon devices to allow a remote interface definition of the firmware API over a local TCP connection. The intent is to allow client-side programs to directly control the Particle devices in real-time regardless of their programming language. The interface exposed is directly mapped to the one provided by Particle available in their docs section.\\nThe VoodooSpark uses the Particle Cloud and its REST API to provide IP address and port information to the local Particle device. It will then initiate a direct connection to the host machine, on which will need to be a TCP server. Once the connection has been made, the host machine can drive the Particle devices using the binary protocol defined below to effectively execute firmware API level commands dynamically.\\nLoading the Firmware\\nWith your Particle device connected to a Wifi network and has already gone through the \"claim\"/ownership process:\\n\\nOpen the Particle.io Editor with the credentials used when going through the claiming process.\\nCopy and paste the entire contents of firmware/voodoospark.cpp into the editor window.\\nClick \"Verify\"\\nClick \"Flash\"\\nOnce the flashing process is complete, close the Particle.io Editor.\\n\\nAlternately, the firmware may be loaded using the Particle CLI (particle-cli) instead of the Particle.io Editor:\\nnpm install -g particle-cli\\nparticle cloud login\\nparticle cloud flash PARTICLE_DEVICE_ID firmware/voodoospark.cpp\\nNow your Particle device is running VoodooSpark, lets connect to it!\\nConnecting the Particle device to You!\\nThe way VoodooSpark works is to use the Particle Cloud as a channel to identify where to initiate the TCP connection to the device from your host machine.\\nIn order to connect the Particle device to your computer, you will first need to issue an HTTP GET request to the Particle Cloud. This can be done via any programming language, but for this example we are using a simple CURL command. You will need some information outlined with curly braces below, please note the {DEVICE-ID} and {ACCESS-TOKEN} are available from the Particle.io Editor\\ncurl https://api.particle.io/v1/devices/{DEVICE-ID}/endpoint?access_token={ACCESS-TOKEN}\\n\\nThis should return a JSON document that looks similar to this:\\n{\\n  \"cmd\": \"VarReturn\",\\n  \"name\": \"endpoint\",\\n  \"result\": \"192.168.1.10:48879\",\\n  \"coreInfo\": {\\n    \"last_app\": \"\",\\n    \"last_heard\": \"2014-05-08T02:51:48.826Z\",\\n    \"connected\": true,\\n    \"deviceID\": \"{DEVICE-ID}\"\\n  }\\n}\\n\\nThe \"result\" value is the IP address of the Particle Device on your local network and the part after the colon (:) is the port that the server is currently listening on. This port will by default be 48879 (0xBEEF), but can be changed in the voodoospark firmware. Please do not hardcode the port for this reason, rather use the data returned back as the response.\\nWith the IP Address and TCP port information, use your favorite language or TCP client to connect to the device (even telnet will work) and send it the necessary BINARY protocol commands to trigger the desired API interactions as defined in our API Command guide.\\nHow to Debug\\nIn case you want to see what is going inside the VoodooSpark in real-time, we have built in a lot of debug hooks for you. You will need a USB cable and the screen or minicom utilities (one of them) on unix. Modify the firmware loaded in the Particle build system to convert the line:\\n#define DEBUG 0\\n\\nto the following definition:\\n#define DEBUG 1\\n\\nThis will enable debug mode, boot a serial port connection on the device and present on your computer for you to watch the inside voodoo. Be sure to flash the new firmware to your device, this is very important and easy to miss. Once the flashing finishes, do an\\nls /dev\\n\\nLook for something similar to tty.usbmodem1411 yours may be different, but will be something like tty.usbmodemABCD. Now using your favorite term app connect to that port using the baud rate of 115200.\\nFor screen this command will look like:\\nscreen /dev/tty.usbmodem1411 115200\\n\\nReference Implementations\\n\\nParticle-io node.js\\nVspark Go\\n\\nLicense\\nSee LICENSE file.\\nMade With Voodoo\\nThis firmware is made and cared for by the following awesome people:\\n\\nChris Williams https://github.com/voodootikigod\\nRick Waldron https://github.com/rwaldron\\nDavid Resseguie https://github.com/Resseguie\\nBrian Genisio https://github.com/BrianGenisio\\n\\n',\n",
       "  'watchers': '16',\n",
       "  'stars': '144',\n",
       "  'forks': '33',\n",
       "  'commits': '164'},\n",
       " {'language': 'C++ 57.2',\n",
       "  'readme': \"Tesseract Lib for iOS\\nAbout\\nThis project contains only the leptonica and tesseract-ocr libraries compiled for iOS.\\nThere is no support for armv6, so it won't work with iPhone 1st Gen and iPhone 3G.\\nUsage\\nYou might want to use this tesseract-ios to include Tesseract in your iOS project. For a more advanced usage, you can use the raw library with regular C++ code.\\nDon't forget to rename your implementation classes with .mm instead of .m as it uses C++ code.\\nCode Sample\\nFollow this blog post for more informations.\\n\",\n",
       "  'watchers': '22',\n",
       "  'stars': '128',\n",
       "  'forks': '33',\n",
       "  'commits': '3'},\n",
       " {'language': 'C++ 81.7',\n",
       "  'readme': 'PROJECT ON HOLD - Waiting for Jupyter ascending\\nThe IPython project is undergoing heavy development at the moment as it is split into the Jupyter environment for interactive computing with IPython as just one of many supported kernels. As such, developing a desktop wrapped around the project means tracking a rapidly moving target. Therefore, I\\'ve decided to wait until things settle down a bit - likely the split will be more or less stabilised for version 4.0 which is planned for release later this year, at which point development of IPython desktop can start up again.\\nIPython Notebook Desktop\\nThis is a proof of concept desktop interface for the IPython Notebook.\\nWhat\\'s new\\nThe latest revision improves ipython configuration and process handling. It will now try to automatically figure out the location of your ipython install and the url where the server is available when launched.\\nConcept\\nIt\\'s well established that IPython is awesome.\\nMost IPython users end up using local installs of the IPython notebook in their browser. However this is somewhat clunky, mixing the browser interface and the notebook interface and generally requiring a trip to the command line to get the server running.\\nThe IPython Notebook Desktop wraps the webapp in a more friendly interface, powered by node-webkit. You can configure a notebook to run to power the interface (optionally have it run on startup).\\nWhat this does NOT do is provide you with an IPython installation. This is deliberate, since people have different needs and tastes with regards to their Python installs. Some people want to use the Python bundled with their operating system, others use Python distributions like Canopy or Anaconda. With IPython Desktop the Python distribution and the interface are separate, but you must configure IPython desktop to use your IPython installation.\\nThe IPython Notebook Desktop doesn\\'t aim to make it easier to install a scientific python environment, but should be easy enough to get by itself. It could eventually be a candidate for bundling with existing packages or with IPython itself.\\nPretty pictures\\nIPython embedded\\n\\nStart Screen:\\n\\nConfig Screen:\\n\\nGet it!\\nApp bundle for Mac\\nYou also need to have IPython installed. My personal recommendation is the Anaconda python distribution if you are mainly doing science and engineering work.\\nComing sometime - binary bundles for each platform. Contributions welcome\\nConfiguration\\nIPython desktop can either launch the IPython notebook server for you or connect to an existing URL.\\nTo launch a server you must specify the location of your IPython executable, by default this is pre-filled (using the output of the command which ipython). You can optionally specify a Profile to use (which will be used with --profile=...)\\nIMPORTANT - you must supply the full path to your IPython install otherwise it will fail to launch the ipython server\\nWARNING: ipython-desktop is by no means idiot proof at the moment. If you don\\'t configure it correctly the page will simply fail to load without explanation. This should improve in future versions.\\nURL only\\nIf you set the \"remote\" option in the config you can simply type in the URL of your IPython server including http:// at the front! handy if you just want a nicer interface for a remote system or just for testing.\\nBuilding ipython-desktop\\nIn theory, the following steps should work (on Mac):\\nRequirements\\n\\nXcode developer tools installed\\nnode (nodejs) with npm, if you use Homebrew (and you should) just do brew install node.\\ngrunt and grunt-cli (npm install -g grunt grunt-cli normally you will have to use sudo)\\n\\nSet up the project\\nIn the terminal, cd into the source folder. Run npm install, grunt nodewebkit, grunt install to set up the dependencies for ipython desktop.\\nFINALLY you should be able to run grunt run and see you shiny new ipython-desktop app, ready to configure.\\nKnown Issues\\n\\nIMPORTANT - you must configure ipython desktop with the FULL PATH of your ipython executable\\nCertain combinations of starting/stopping ipython servers and opening/closing windows might leave orphaned IPython processes (especially if you force quit the app)\\n\\nTODO\\n\\nBundles for all OSes\\nAdd fault tolerance e.g. for missing or misconfigured Ipython\\nMore user friendly configuration of ipython\\nbetter integration with ipython notebooks - start/stop events, clean shutdown\\nIntegration with Native menus!\\nTry to find the current iPython install using \"which iPython\" -> Done!\\nTry to auto-config profiles using \"ipython profile locate\"\\nGet url/port of running ipython using json from profile folder\\n\\nSimilar Work\\n\\nCanopy: Enthought provide their Canopy desktop interface with IPython notebook integration. However this ties you into the EPD distribution. The IPython Notebook Desktop aims to be a lighter, more versatile solution\\nIPython notebook Works in a similar vein, though is Mac only. It also differs in aim, since it bundles the essentials for scientific python computing. My aim with this project is to allow the interface to work with different Python installs, making it possible to use different python version and different virtual environments.\\nIPyApp Another project that uses node-webkit to wrap IPython notebook, but embeds the full python executable environment in the app.\\n\\nCredits\\nIPython desktop is powered by Node Webkit and makes use of the angular-desktop-app template. Icon is IPython faenza\\nLICENCE\\nThis software is currently under LGPL licence.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '126',\n",
       "  'forks': '11',\n",
       "  'commits': '31'},\n",
       " {'language': 'C++ 77.0',\n",
       "  'readme': 'PROJECT ON HOLD - Waiting for Jupyter ascending\\nThe IPython project is undergoing heavy development at the moment as it is split into the Jupyter environment for interactive computing with IPython as just one of many supported kernels. As such, developing a desktop wrapped around the project means tracking a rapidly moving target. Therefore, I\\'ve decided to wait until things settle down a bit - likely the split will be more or less stabilised for version 4.0 which is planned for release later this year, at which point development of IPython desktop can start up again.\\nIPython Notebook Desktop\\nThis is a proof of concept desktop interface for the IPython Notebook.\\nWhat\\'s new\\nThe latest revision improves ipython configuration and process handling. It will now try to automatically figure out the location of your ipython install and the url where the server is available when launched.\\nConcept\\nIt\\'s well established that IPython is awesome.\\nMost IPython users end up using local installs of the IPython notebook in their browser. However this is somewhat clunky, mixing the browser interface and the notebook interface and generally requiring a trip to the command line to get the server running.\\nThe IPython Notebook Desktop wraps the webapp in a more friendly interface, powered by node-webkit. You can configure a notebook to run to power the interface (optionally have it run on startup).\\nWhat this does NOT do is provide you with an IPython installation. This is deliberate, since people have different needs and tastes with regards to their Python installs. Some people want to use the Python bundled with their operating system, others use Python distributions like Canopy or Anaconda. With IPython Desktop the Python distribution and the interface are separate, but you must configure IPython desktop to use your IPython installation.\\nThe IPython Notebook Desktop doesn\\'t aim to make it easier to install a scientific python environment, but should be easy enough to get by itself. It could eventually be a candidate for bundling with existing packages or with IPython itself.\\nPretty pictures\\nIPython embedded\\n\\nStart Screen:\\n\\nConfig Screen:\\n\\nGet it!\\nApp bundle for Mac\\nYou also need to have IPython installed. My personal recommendation is the Anaconda python distribution if you are mainly doing science and engineering work.\\nComing sometime - binary bundles for each platform. Contributions welcome\\nConfiguration\\nIPython desktop can either launch the IPython notebook server for you or connect to an existing URL.\\nTo launch a server you must specify the location of your IPython executable, by default this is pre-filled (using the output of the command which ipython). You can optionally specify a Profile to use (which will be used with --profile=...)\\nIMPORTANT - you must supply the full path to your IPython install otherwise it will fail to launch the ipython server\\nWARNING: ipython-desktop is by no means idiot proof at the moment. If you don\\'t configure it correctly the page will simply fail to load without explanation. This should improve in future versions.\\nURL only\\nIf you set the \"remote\" option in the config you can simply type in the URL of your IPython server including http:// at the front! handy if you just want a nicer interface for a remote system or just for testing.\\nBuilding ipython-desktop\\nIn theory, the following steps should work (on Mac):\\nRequirements\\n\\nXcode developer tools installed\\nnode (nodejs) with npm, if you use Homebrew (and you should) just do brew install node.\\ngrunt and grunt-cli (npm install -g grunt grunt-cli normally you will have to use sudo)\\n\\nSet up the project\\nIn the terminal, cd into the source folder. Run npm install, grunt nodewebkit, grunt install to set up the dependencies for ipython desktop.\\nFINALLY you should be able to run grunt run and see you shiny new ipython-desktop app, ready to configure.\\nKnown Issues\\n\\nIMPORTANT - you must configure ipython desktop with the FULL PATH of your ipython executable\\nCertain combinations of starting/stopping ipython servers and opening/closing windows might leave orphaned IPython processes (especially if you force quit the app)\\n\\nTODO\\n\\nBundles for all OSes\\nAdd fault tolerance e.g. for missing or misconfigured Ipython\\nMore user friendly configuration of ipython\\nbetter integration with ipython notebooks - start/stop events, clean shutdown\\nIntegration with Native menus!\\nTry to find the current iPython install using \"which iPython\" -> Done!\\nTry to auto-config profiles using \"ipython profile locate\"\\nGet url/port of running ipython using json from profile folder\\n\\nSimilar Work\\n\\nCanopy: Enthought provide their Canopy desktop interface with IPython notebook integration. However this ties you into the EPD distribution. The IPython Notebook Desktop aims to be a lighter, more versatile solution\\nIPython notebook Works in a similar vein, though is Mac only. It also differs in aim, since it bundles the essentials for scientific python computing. My aim with this project is to allow the interface to work with different Python installs, making it possible to use different python version and different virtual environments.\\nIPyApp Another project that uses node-webkit to wrap IPython notebook, but embeds the full python executable environment in the app.\\n\\nCredits\\nIPython desktop is powered by Node Webkit and makes use of the angular-desktop-app template. Icon is IPython faenza\\nLICENCE\\nThis software is currently under LGPL licence.\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '116',\n",
       "  'forks': '29',\n",
       "  'commits': '90'},\n",
       " {'language': 'C++ 71.8',\n",
       "  'readme': 'PROJECT ON HOLD - Waiting for Jupyter ascending\\nThe IPython project is undergoing heavy development at the moment as it is split into the Jupyter environment for interactive computing with IPython as just one of many supported kernels. As such, developing a desktop wrapped around the project means tracking a rapidly moving target. Therefore, I\\'ve decided to wait until things settle down a bit - likely the split will be more or less stabilised for version 4.0 which is planned for release later this year, at which point development of IPython desktop can start up again.\\nIPython Notebook Desktop\\nThis is a proof of concept desktop interface for the IPython Notebook.\\nWhat\\'s new\\nThe latest revision improves ipython configuration and process handling. It will now try to automatically figure out the location of your ipython install and the url where the server is available when launched.\\nConcept\\nIt\\'s well established that IPython is awesome.\\nMost IPython users end up using local installs of the IPython notebook in their browser. However this is somewhat clunky, mixing the browser interface and the notebook interface and generally requiring a trip to the command line to get the server running.\\nThe IPython Notebook Desktop wraps the webapp in a more friendly interface, powered by node-webkit. You can configure a notebook to run to power the interface (optionally have it run on startup).\\nWhat this does NOT do is provide you with an IPython installation. This is deliberate, since people have different needs and tastes with regards to their Python installs. Some people want to use the Python bundled with their operating system, others use Python distributions like Canopy or Anaconda. With IPython Desktop the Python distribution and the interface are separate, but you must configure IPython desktop to use your IPython installation.\\nThe IPython Notebook Desktop doesn\\'t aim to make it easier to install a scientific python environment, but should be easy enough to get by itself. It could eventually be a candidate for bundling with existing packages or with IPython itself.\\nPretty pictures\\nIPython embedded\\n\\nStart Screen:\\n\\nConfig Screen:\\n\\nGet it!\\nApp bundle for Mac\\nYou also need to have IPython installed. My personal recommendation is the Anaconda python distribution if you are mainly doing science and engineering work.\\nComing sometime - binary bundles for each platform. Contributions welcome\\nConfiguration\\nIPython desktop can either launch the IPython notebook server for you or connect to an existing URL.\\nTo launch a server you must specify the location of your IPython executable, by default this is pre-filled (using the output of the command which ipython). You can optionally specify a Profile to use (which will be used with --profile=...)\\nIMPORTANT - you must supply the full path to your IPython install otherwise it will fail to launch the ipython server\\nWARNING: ipython-desktop is by no means idiot proof at the moment. If you don\\'t configure it correctly the page will simply fail to load without explanation. This should improve in future versions.\\nURL only\\nIf you set the \"remote\" option in the config you can simply type in the URL of your IPython server including http:// at the front! handy if you just want a nicer interface for a remote system or just for testing.\\nBuilding ipython-desktop\\nIn theory, the following steps should work (on Mac):\\nRequirements\\n\\nXcode developer tools installed\\nnode (nodejs) with npm, if you use Homebrew (and you should) just do brew install node.\\ngrunt and grunt-cli (npm install -g grunt grunt-cli normally you will have to use sudo)\\n\\nSet up the project\\nIn the terminal, cd into the source folder. Run npm install, grunt nodewebkit, grunt install to set up the dependencies for ipython desktop.\\nFINALLY you should be able to run grunt run and see you shiny new ipython-desktop app, ready to configure.\\nKnown Issues\\n\\nIMPORTANT - you must configure ipython desktop with the FULL PATH of your ipython executable\\nCertain combinations of starting/stopping ipython servers and opening/closing windows might leave orphaned IPython processes (especially if you force quit the app)\\n\\nTODO\\n\\nBundles for all OSes\\nAdd fault tolerance e.g. for missing or misconfigured Ipython\\nMore user friendly configuration of ipython\\nbetter integration with ipython notebooks - start/stop events, clean shutdown\\nIntegration with Native menus!\\nTry to find the current iPython install using \"which iPython\" -> Done!\\nTry to auto-config profiles using \"ipython profile locate\"\\nGet url/port of running ipython using json from profile folder\\n\\nSimilar Work\\n\\nCanopy: Enthought provide their Canopy desktop interface with IPython notebook integration. However this ties you into the EPD distribution. The IPython Notebook Desktop aims to be a lighter, more versatile solution\\nIPython notebook Works in a similar vein, though is Mac only. It also differs in aim, since it bundles the essentials for scientific python computing. My aim with this project is to allow the interface to work with different Python installs, making it possible to use different python version and different virtual environments.\\nIPyApp Another project that uses node-webkit to wrap IPython notebook, but embeds the full python executable environment in the app.\\n\\nCredits\\nIPython desktop is powered by Node Webkit and makes use of the angular-desktop-app template. Icon is IPython faenza\\nLICENCE\\nThis software is currently under LGPL licence.\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '115',\n",
       "  'forks': '64',\n",
       "  'commits': '3'},\n",
       " {'language': 'C++ 52.4',\n",
       "  'readme': '\\nlibsvm-ruby-swig¶ ↑\\n\\nRuby interface to LIBSVM (using SWIG)\\n\\nwww.tomzconsulting.com\\n\\ntweetsentiments.com\\n\\nDESCRIPTION:¶ ↑\\nThis is the Ruby port of the LIBSVM Python SWIG (Simplified Wrapper and  Interface Generator) interface.\\nA slightly modified version of LIBSVM 2.9 is included, it allows turrning on/off the debug log. You don\\'t need your own copy of SWIG to use this library - all  needed files are generated using SWIG already.\\nLook for the README file in the ruby subdirectory for instructions. The binaries included were built under Ubuntu Linux 2.6.28-18-generic x86_64, you should run make under the libsvm-2.9 and libsvm-2.9/ruby  directories to regenerate the executables for your environment.\\nLIBSVM is in use at tweetsentiments.com - A Twitter / Tweet sentiment analysis application\\nINSTALL:¶ ↑\\nCurrently the gem is available on linux only(tested on Ubuntu 8-9 and Fedora 9-12, and on OS X by danielsdeleo), and you will need g++ installed to compile the  native code. \\nsudo gem sources -a http://gems.github.com   (you only have to do this once)\\nsudo gem install tomz-libsvm-ruby-swig\\nSYNOPSIS:¶ ↑\\nQuick Interactive Tutorial using irb (adopted from the python code from Toby Segaran\\'s “Programming Collective Intelligence” book):\\nirb(main):001:0> require \\'svm\\'\\n=> true\\nirb(main):002:0> prob = Problem.new([1,-1],[[1,0,1],[-1,0,-1]])\\nirb(main):003:0> param = Parameter.new(:kernel_type => LINEAR, :C => 10)\\nirb(main):004:0> m = Model.new(prob,param)\\nirb(main):005:0> m.predict([1,1,1])\\n=> 1.0\\nirb(main):006:0> m.predict([0,0,1])\\n=> 1.0\\nirb(main):007:0> m.predict([0,0,-1])\\n=> -1.0\\nirb(main):008:0> m.save(\"test.model\")\\nirb(main):009:0> m2 = Model.new(\"test.model\")\\nirb(main):010:0> m2.predict([0,0,-1])\\n=> -1.0\\nAUTHOR:¶ ↑\\nTom Zeng\\n\\ntwitter.com/tomzeng\\n\\nwww.tomzconsulting.com\\n\\nwww.linkedin.com/in/tomzeng\\n\\ntom.z.zeng at gmail dot com\\n\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '111',\n",
       "  'forks': '25',\n",
       "  'commits': '67'},\n",
       " {'language': 'C++ 47.7',\n",
       "  'readme': '\\nlibsvm-ruby-swig¶ ↑\\n\\nRuby interface to LIBSVM (using SWIG)\\n\\nwww.tomzconsulting.com\\n\\ntweetsentiments.com\\n\\nDESCRIPTION:¶ ↑\\nThis is the Ruby port of the LIBSVM Python SWIG (Simplified Wrapper and  Interface Generator) interface.\\nA slightly modified version of LIBSVM 2.9 is included, it allows turrning on/off the debug log. You don\\'t need your own copy of SWIG to use this library - all  needed files are generated using SWIG already.\\nLook for the README file in the ruby subdirectory for instructions. The binaries included were built under Ubuntu Linux 2.6.28-18-generic x86_64, you should run make under the libsvm-2.9 and libsvm-2.9/ruby  directories to regenerate the executables for your environment.\\nLIBSVM is in use at tweetsentiments.com - A Twitter / Tweet sentiment analysis application\\nINSTALL:¶ ↑\\nCurrently the gem is available on linux only(tested on Ubuntu 8-9 and Fedora 9-12, and on OS X by danielsdeleo), and you will need g++ installed to compile the  native code. \\nsudo gem sources -a http://gems.github.com   (you only have to do this once)\\nsudo gem install tomz-libsvm-ruby-swig\\nSYNOPSIS:¶ ↑\\nQuick Interactive Tutorial using irb (adopted from the python code from Toby Segaran\\'s “Programming Collective Intelligence” book):\\nirb(main):001:0> require \\'svm\\'\\n=> true\\nirb(main):002:0> prob = Problem.new([1,-1],[[1,0,1],[-1,0,-1]])\\nirb(main):003:0> param = Parameter.new(:kernel_type => LINEAR, :C => 10)\\nirb(main):004:0> m = Model.new(prob,param)\\nirb(main):005:0> m.predict([1,1,1])\\n=> 1.0\\nirb(main):006:0> m.predict([0,0,1])\\n=> 1.0\\nirb(main):007:0> m.predict([0,0,-1])\\n=> -1.0\\nirb(main):008:0> m.save(\"test.model\")\\nirb(main):009:0> m2 = Model.new(\"test.model\")\\nirb(main):010:0> m2.predict([0,0,-1])\\n=> -1.0\\nAUTHOR:¶ ↑\\nTom Zeng\\n\\ntwitter.com/tomzeng\\n\\nwww.tomzconsulting.com\\n\\nwww.linkedin.com/in/tomzeng\\n\\ntom.z.zeng at gmail dot com\\n\\n',\n",
       "  'watchers': '47',\n",
       "  'stars': '109',\n",
       "  'forks': '38',\n",
       "  'commits': '96'},\n",
       " {'language': 'JavaScript 91.5',\n",
       "  'readme': 'A Lineman JS Template using Angular\\n\\nThis is a project template for Angular JS applications using Lineman.\\nIt includes the following features:\\n\\nTemplate Precompilation into Angulars $templateCache using grunt-angular-templates\\nA basic login, logout service bound to sample routes inside config/server.js\\nA router, and 2 views home and login\\nA directive that shows a message on mouseover\\n2 Controllers, for home and login, with $scope variables set and bound\\nA working, bound login form (username/password don\\'t matter, but are required)\\nConfigured grunt-ng-annotate so you don\\'t have to fully qualify angular dependencies.\\nAuto generated sourcemaps with inlined sources via grunt-concat-sourcemap (you\\'ll need to enable sourcemaps in Firefox/Chrome to see this)\\nUnit Tests and End-to-End Tests\\nConfiguration to run Protractor for End-to-End Tests\\n\\nInstructions\\n\\ngit clone https://github.com/linemanjs/lineman-angular-template.git my-lineman-app\\ncd my-lineman-app\\nsudo npm install -g lineman\\nnpm install\\nlineman run\\nopen your web browser to localhost:8000\\n\\nRunning Tests\\nThis template was used as the basis of @davemo\\'s Testing Strategies for Angular JS screencast, and contains all the tests we wrote in the screencast and a few more!\\nTo run the unit tests:\\n\\nlineman run from 1 terminal window\\nlineman spec from another terminal window, this will launch Testem and execute specs in Chrome\\n\\nTo run the end-to-end tests:\\nEnd-to-End Tests\\n\\nnpm install protractor\\n./node_modules/protractor/bin/webdriver-manager update\\nMake sure you have chrome installed.\\nlineman run from 1 terminal window\\nlineman grunt spec-e2e from another terminal window\\n\\nDefining your apps angular.module in CoffeeScript\\nIf you are using Coffeescript to define the angular.module for your app, you will need to swap the concat order in config/application.js such that coffeescript files are included before javascript. (If you are using JavaScript for defining the angular.module the default concat order is fine).\\nAdd the following concat_sourcemap block to config/application.js if you want to define your app module in coffeescript:\\nmodule.exports = function(lineman) {\\n  return {\\n\\n    concat_sourcemap: {\\n      js: {\\n        src: [\\n          \"<%= files.js.vendor %>\",\\n          \"<%= files.coffee.generated %>\",\\n          \"<%= files.js.app %>\",\\n          \"<%= files.ngtemplates.dest %>\"\\n        ]\\n      }\\n    }\\n\\n  };\\n};\\nHopefully this helps you get up and running with AngularJS!\\n',\n",
       "  'watchers': '33',\n",
       "  'stars': '254',\n",
       "  'forks': '93',\n",
       "  'commits': '137'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Red Dwarf (LOOKING FOR A NEW MAINTAINER)\\n\\nAbout\\nRed Dwarf is a heatmap visualization of GitHub repository stargazers.\\nPlay with the live demo.\\nHow it Works\\nRed Dwarf uses the GitHub API to determine the locations of people who have starred a given repository. Then, using the Google Maps API, these locations are translated into geocoordinates and fed into a heatmap. The result is a beautiful and detailed visualization of global positions of a repository\\'s stargazers.\\nGetting Started\\nRed Dwarf depends on Google Maps for geocoding and mapping. You must get a Google Maps API key to access these services.\\nUsage\\nImport the Google Maps JavaScript API. Appending libraries=visualization to the source path will ensure that you have the heatmap library available. Replace YOUR_KEY with the API key provided to you by Google.\\n<script src=\"http://maps.googleapis.com/maps/api/js?key=YOUR_KEY&sensor=false&libraries=visualization\"></script>\\nInstantiate a new RedDwarf object, the constructor of which accepts a configuration object (see Settings).\\nvar stars = new RedDwarf({\\n\\tuser: config.user,\\n\\trepository: config.repository,\\n\\tmap_id: config.map_id\\n});\\nSettings\\n\\nuser (required)\\nThe GitHub user login of the repository owner.\\nrepository (required)\\nThe GitHub repository name.\\nmap_id (required)\\nThe ID of the HTML element in which to draw the map.\\ncache_location\\nThe path (relative or absolute) of a JSON file containing precomputed geolocation data. If omitted, Red Dwarf will compute all data from scratch (see Performance). Note: this file\\'s contents are equivalent to the output of the toJSON method.\\nmap_zoom The initial zoom level of the heatmap. Default: 2. (more info)\\nmap_lat The initial latitude position on which the heatmap is centered. Default: 20. (20 degrees north of the equator) (more info)\\nmap_lng The initial longitude position on which the heatmap is centered. Default: 0. (Prime Meridian) (more info)\\nmap_type The initial type of the heatmap: road, satellite, hybrid, or terrain. Default: \"roadmap\". (more info)\\n\\nMethods\\n\\ntoJSON Returns a JSON representation of a mapping of string locations to geocodes, the number of repository stars, and a mapping of stargazers\\' user logins to their respective user objects.\\n\\nEvents\\nRed Dwarf will trigger each of the following events during processing. Arguments passed to the event handlers are listed below the event name (where applicable).\\nEvent handlers are defined by including functions keyed by the respective event name in the settings object.\\n\\nonRepositoryLoaded Fired after successfully loading repository info from the GitHub API.\\n\\ndata The data object returned by GitHub.\\n\\n\\nonRepositoryError Fired after unsuccessfully loading repository info from the GitHub API.\\n\\nmessage The error message returned by GitHub.\\n\\n\\nonCacheLoaded Fired after successfully loading the JSON cache file.\\nonStargazersUpdated Fired after processing a chunk of at most 100 repository stargazers.\\n\\nnum_stargazers The number of stargazers processed so far.\\n\\n\\nonStargazersLoaded Fired after successfully loading all stargazers.\\nonLocationUpdated Fired after successfully loading a single stargazer\\'s profile.\\n\\nnum_resolved_stargazers The number of stargazers whose profiles have been loaded so far.\\nnum_stargazers The total number of profiles to load.\\n\\n\\nonLocationLoaded Fired after successfully loading all stargazers\\' profiles.\\nonPointsUpdated Fired after geocoding a chunk of at most 10 stargazer locations.\\n\\nnum_resolved_points The number of locations geocoded so far.\\nnum_stargazers The total number of locations to geocode.\\n\\n\\nonPointsLoaded Fired after successfully geocoding all locations and drawing the heatmap.\\n\\nPerformance\\nIt\\'s important to pre-cache the geolocation data because the Google Maps geocoding API places strict limits on the frequency of requests. Trial and error indicates that this limit is in the neighborhood of 40 requests per minute. This means a repository with 200 stars would take 5 minutes to get all geocoordinates. By default, Red Dwarf rate limits requests using a technique based on work by Nicholas Zakas.\\nAlso note that repositories with many thousands of stargazers will likely hit usage limits on GitHub\\'s API, because each stargazer\\'s profile must be queried in order to get their location. Rate limiting is currently in development for this scenario.\\nFor these reasons, it\\'s best to pre-cache as frequently as possible. This will prevent each page load from incurring the usage limit penalizations.\\nRed Dwarf will only make API calls for data not already in the cache.\\nPrivacy\\nRed Dwarf uses publicly-available stargazer information including login names and locations via the GitHub API. Locations are only derived from stargazers who have opted in to making their location public in their GitHub profile.\\nMIT License\\nCopyright (c) 2012 Rick Viscomi (rviscomi@gmail.com)\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '252',\n",
       "  'forks': '26',\n",
       "  'commits': '26'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"Rucksack\\nRucksack is a jquery plugin to arrange elements that can fit in the given width. It relies on the knapsack algorithm.\\nUsage:\\n$('#container').rucksack({\\n \\xa0 \\xa0width: 'width to fit in. defaults to 960px',\\n \\xa0 \\xa0class: 'class name to be given to the div that will contain the elements of one row. defaults to rucksack'\\n})\\n\\n\\nExample\\nHere's a working example by @omgmog\\nhttp://jsfiddle.net/vXbCY/\\nAnother example by ctcherry\\nhttp://jsfiddle.net/URMuA/\\nContributors:\\n\\ngauravsc\\n\\n\",\n",
       "  'watchers': '2',\n",
       "  'stars': '252',\n",
       "  'forks': '10',\n",
       "  'commits': '8'},\n",
       " {'language': 'JavaScript 93.9',\n",
       "  'readme': 'jquery.nstSlider.js\\nFully customizable with CSS, Single/Double handles, Touch-enabled, IE 7+ Compatibility, Custom Digit Rounding, Non linear step increments!\\n\\nExample\\nInitialize with:\\n$(\".mySlider\").nstSlider({\\n    \"left_grip_selector\": \".leftGrip\",\\n    \"right_grip_selector\": \".rightGrip\",\\n    \"value_bar_selector\": \".bar\",\\n    \"value_changed_callback\": function(cause, minValue, maxValue, prevMinValue, prevMaxValue) {\\n        // show the suggested values in your min/max labels elements\\n    }\\n});\\nMethod call:\\n$(\".mySlider\").nstSlider(\"set_position\", 10 /* min */, 90 /* max */);\\nDestroy with:\\n$(\".mySlider\").nstSlider(\"teardown\");\\nDemo\\nFor live demos please visit the project webpage:\\nhttp://lokku.github.io/jquery-nstslider/\\nFor a Quick Start have a look at the source html of the following file:\\nhttps://github.com/lokku/jquery-nstslider/blob/master/demo/index.html\\nOptions\\n\\n\\n\\nOption\\nType\\nDefault\\nDescription\\n\\n\\n\\n\\nanimating_css_class\\nstring\\nnst-animating\\nthe css class to be used when the slider is to be animated (this happens when a certain min/max value is being set for example).\\n\\n\\ntouch_tolerance_value_bar_x\\nnumber\\n15\\nthe horizontal tolerance in pixels by which a handle of the slider should be grabbed if the user touches outside the slider bar area.\\n\\n\\ntouch_tolerance_value_bar_y\\nnumber\\n30\\nthe vertical tolerance in pixels by which a handle of the slider should be grabbed if the user touches outside the slider bar area.\\n\\n\\nleft_grip_selector\\nstring\\n.nst-slider-grip-left\\nthe selector of the left grip handle. The left grip element must exist in the page when the slider is initialized.\\n\\n\\nright_grip_selector\\nstring\\nundefined\\nthe selector of the right grip handle. This is optional. A single handler bar is assumed if this selector is not specified.\\n\\n\\nvalue_bar_selector\\nstring\\nundefined\\nthe selector of the value bar. If not specified assumes a value bar representing the selection is not wanted.\\n\\n\\nrounding\\nobject or number\\n1\\nthe rounding for a certain value displayed on the slider. This rounds the values returned in the value_changed_callback as roundedValue : int(actualValue / rounding) * rounding. The rounding parameter can be a number (i.e., fixed rounding) or can depend on actualValue (i.e., dynamic rounding). To perform dynamic rounding an object must be passed instead of a value. For example, passing rounding : { \\'1\\' : \\'100\\', \\'10\\' : \\'1000\\', \\'50\\' : \\'10000\\' } will use rounding = 1 when actualValue <= 100, rounding = 10 when 100 < actualValue <= 1000 and so on...\\n\\n\\ncrossable_handles\\nboolean\\ntrue\\nAllow handles to cross each other while one of them is being dragged. This option is ignored if just one handle is used.\\n\\n\\nvalue_changed_callback\\nfunction\\nfunction(cause, curMin, curMax, prevMin, prevMax) { return; }\\na callback called whenever the user drags one of the handles.\\n\\n\\nuser_mouseup_callback\\nfunction\\nfunction(vmin, vmax, left_grip_moved) { return; }\\na callback called whenever the mouse button pressed while dragging a slider grip is released\\n\\n\\nuser_drag_start_callback\\nfunction\\nfunction () { return; }\\na callback called before the user drags one of the handles\\n\\n\\n\\nMethods\\nWhen calling methods, use positional arguments. For example, for the\\nset_position method, call:\\n$(\".mySlider\").nstSlider(\"set_position\", 10 /* min */, 90 /* max */);\\ndo not call:\\n$(\".mySlider\").nstSlider(\"set_position\", { min: 10, max: 90 });\\nunless the documentation says that the first argument is an object.\\n\\n\\n\\nMethod\\nArguments (positional)\\nDescription\\n\\n\\n\\n\\nget_range_min\\nNone\\nreturn the current minimum range of the slider\\n\\n\\nget_range_max\\nNone\\nreturn the current maximum range of the slider\\n\\n\\nget_current_min_value\\nNone\\nreturn the current minimum value of the slider\\n\\n\\nget_current_max_value\\nNone\\nreturn the current maximum value of the slider\\n\\n\\nis_handle_to_left_extreme\\nNone\\nreturn a boolean indicating whether or not the left handler is moved all the way to the left\\n\\n\\nis_handle_to_right_extreme\\nNone\\nreturn a boolean indicating whether or not the right handler is moved all the way to the right\\n\\n\\nrefresh\\nNone\\nforce a refresh of the slider\\n\\n\\ndisable\\nNone\\ndisable the slider (i.e., user cannot move the handles)\\n\\n\\nenable\\nNone\\nenable the slider (i.e., user can move the handles)\\n\\n\\nis_enabled\\nNone\\nreturn a boolean indicating whether or not the slider can be moved by the user\\n\\n\\nset_position\\nmin: number, max: number\\nset the handles at the specified min and max values\\n\\n\\nset_step_histogram\\nhistogram : array of numbers\\nuse a non-linear step increment for the slider that is stretched where the histogram provided counts more items\\n\\n\\nunset_step_histogram\\nNone\\nuse a linear scale of increments for the slider\\n\\n\\nset_range\\nrangeMin : number, rangeMax : number\\nset the minimum and the maximum range of values the slider\\n\\n\\nset_rounding\\nrounding: number or object\\nset the rounding for the slider\\n\\n\\nget_rounding\\nNone\\nreturn the current rounding of the slider\\n\\n\\nteardown\\nNone\\nremove all data stored in the slider\\n\\n\\nvalue_to_px\\nnumber\\ngiven a value in the range of the slider, returns the corresponding value in pixel relative to the slider width\\n\\n\\n\\nDependencies\\njQuery 1.6.4+\\nLicense\\nCopyright (c) 2014 Lokku Ltd.\\nLicensed under the MIT license.\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '252',\n",
       "  'forks': '54',\n",
       "  'commits': '119'},\n",
       " {'language': 'JavaScript 92.2',\n",
       "  'readme': 'ReplayLastGoal\\nAutomatically create and tweet a video with the latest goal at the world cup.\\nNotice: It is your responsiblity to make sure that you stay within the limits of \"Fair Use\". Laws might be different in your country. The author and contributors of this project decline all responsibility.\\nFor reference: Copyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"fair use\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\\nTry it!\\nJust follow @ReplayLastGoal on Twitter.\\nWebhooks\\nYou can add webhooks to automatically receive a notification when a goal is scored along with the animated gif and a link to the video replay. We support classic webhooks, Hipchat and Slack. Go to\\nhttp://ReplayLastGoal.com/hooks/add to configure them.\\n\\nHow does it work?\\nIt connects to a video live stream (that you need to provide) and keeps a buffer of about one minute worth of video. When a given twitter account tweets (by default @GoalFlash), it uses the buffer to generate a video of the goal and then tweets it. Videos are saved in the videos/ directory.\\nPress & testimonials\\n\\nReplayLastGoal lets you relive the agony and ecstasy of the World Cup in GIFs - TheNextWeb\\nReplayLastGoal Instantly Tweets Video Of The Latest World Cup Goal - Techcrunch\\nMissed a World Cup gooooooooooaaaaalll? A new Twitter bot will catch you up - NiemanLab\\nBest of the reactions on Twitter\\nReactions to the take down notice from FIFA\\n\\nRequirements\\nA live video stream\\nThere are a few public television channels in Europe who are live streaming the world cup. The only caveat is that your server needs to be in that country (but it\\'s not too complicated to work around it using the cloud.)\\nLive streaming works either with Flash which is hard to reverse engineer or with HTML5 for mobile devices. So open the web page with the live stream with Safari and change your user agent to the iPad and you will be able to get the HTML5 version of the live stream. Then you can inspect network traffic to identify the URL of the live stream. It should end with the extension .m3u8. Use that URL in settings.json.\\nAudio Video Converter\\nYou need to have ffmpeg 2.2x installed on your machine. On a Mac, it\\'s a piece of cake. Install brew and then do brew install ffmpeg and you are done.\\nOn Ubuntu, it\\'s a pain. This bash script should help you.\\nInstall\\ngit clone https://github.com/xdamman/ReplayLastGoal.git\\nnpm install\\n\\nEdit settings.json and save it as settings.development.json (or settings.production.json for production environment as set by the NODE_ENV variable.)\\nYou are now ready to start the application:\\nnpm start\\n\\nLike it? Love it?\\nShare the love by tweeting or favoriting this repo!\\nOh, and pull requests are more than welcome! :-)\\nContributors\\n\\nLaurent VB (facebook integration)\\nJonathan Kupferman (better gif quality)\\n\\nSpecial thanks to Benjamin Goering (@bengo) for his help.\\nTODO\\n(Pull requests welcome)\\n\\n Better pin point when the goal happens in the 20s window to trim down the duration of the video and gif (we could use a VU meter to identify when the sound level peaks)\\n Support for multichannels when there is more than one match at once\\n Better test coverage\\n Interface to manage webhooks and allow anyone to add their own webhook\\n Hipchat Add On\\n Slack add on\\n Generate image to send with the tweet (finding the right one might be tricky)\\n Refactoring to start streaming the video of the goal as soon as we start recording\\n Automatically turn on/off the input stream when there is a match\\n Automatically create a video summary with all the goals after the game ends\\n\\n',\n",
       "  'watchers': '22',\n",
       "  'stars': '251',\n",
       "  'forks': '20',\n",
       "  'commits': '149'},\n",
       " {'language': 'JavaScript 73.3',\n",
       "  'readme': 'Emmet (ex-Zen Coding)\\nEmmet is a toolkit for high-speed HTML, XML, XSL (or any other structured code format) coding and editing. The core of this plugin is a powerful abbreviation engine which allows you to expand expressions—similar to CSS selectors—into HTML code. For example:\\ndiv#page>div.logo+ul#navigation>li*5>a\\n…can be expanded into:\\n\\n<div id=\"page\">\\n        <div class=\"logo\"></div>\\n        <ul id=\"navigation\">\\n                <li><a href=\"\"></a></li>\\n                <li><a href=\"\"></a></li>\\n                <li><a href=\"\"></a></li>\\n                <li><a href=\"\"></a></li>\\n                <li><a href=\"\"></a></li>\\n        </ul>\\n</div>\\n\\nRead more about current Emmet syntax\\nInstallation\\n\\nGo to Help > Install New Software… in your Eclipse IDE\\nAdd http://emmet.io/eclipse/updates/ in update sites\\nCheck Emmet for Eclipse group in available plugins list, click Next button and follow the installation instructions\\nRestart Eclipse\\n\\nPlugin Overview\\nThis plugin provides the features:\\n\\nExpand abbreviations by Tab key\\nTab stops and linked mode support\\nSimple install and update process\\nChange action shortcuts in Eclipse’s Keys preferences page\\nWorks across all Eclipse editors\\nPreferences support to fine-tune output for each syntax and add new abbreviations and snippets\\n\\n\\nAptana 3 users: since Aptana 3 can also expand snippets by Tab key, there might be collisions in expanded result (for example, for div tag). You can remove unused snippets for Aptana bundles in order to make Emmet plugin work properly.\\nContributions\\nDjango snippets',\n",
       "  'watchers': '18',\n",
       "  'stars': '251',\n",
       "  'forks': '33',\n",
       "  'commits': '86'},\n",
       " {'language': 'JavaScript 92.8',\n",
       "  'readme': 'Whiskey\\nWhiskey is a powerful test runner for Node.js applications and a process\\norchestration framework which makes running integration tests with a lot of\\nservice / process dependencies easier.\\nFeatures\\n\\nEach test file runs isolated in a separate process\\nSupport for running multiple tests in parallel in a single suite (--concurrency option)\\nSupport for running multiple suites in parallel (--independent-tests option)\\nSupport for a test initialization function which is run before running the tests in a test file\\nSupport for a test file timeout\\nPer-test setUp / tearDown function support\\nPer-suite (test file) initialize / finalize function support\\nPer-session, or global, setUp / tearDown function support\\nSupport for different test reporters (cli, tap)\\nSupport for code coverage (cli reporter, html reporter)\\nSupport for reporting variables which have leaked into a global scope\\nNicely formatted reports (colors!)\\nIntegration with node debugger\\nSupport for generating Makefiles with different Whiskey targets\\n\\nChanges\\nFor changes please see CHANGES.md file.\\nInstallation\\nInstall it using npm:\\nnpm install whiskey\\n\\nUsage\\nwhiskey [options] --tests \"<test files>\"\\n\\nwhiskey [options] --independent-tests \"<test files>\"\\n\\nwhiskey [options] --tests \"<test files>\"  --independent-tests \"<test files>\"\\n\\nAvailable options\\n\\n-t, --tests - Whitespace separated list of test suites to run sequentially\\n-T, --independent-tests - Whitespace separated list of test suites to run concurrently\\n-m, --max-suites NUMBER - The number of concurrently executing independent test suites (defaults to 5)\\n-ti, --test-init-file - A path to the initialization file which must export\\ninit function and it is called in a child process *before running the tests in\\neach test file\\n-c, --chdir - An optional path to which the child process will chdir to before\\nrunning the tests\\n-g, --global-setup-teardown STRING - Specifies the file containing the globalSetUp and globalTearDown procedures.\\n--timeout [NUMBER] - How long to wait for tests to complete before timing\\nout\\n--failfast - Stop running the tests on a first failure or a timeout\\n--no-styles - Don\\'t use styles and colors\\n--concurrency [NUMBER] - Maximum number of tests which will run in parallel (defaults to 1)\\n--quiet - Don\\'t print stdout and stderr\\n--real-time - Print stdout and stderr as soon as it comes in\\n--test-reporter [cli,tap] - Which test reporter to use (defaults to cli)\\n--coverage - Use this option to enable the test coverage\\n--coverage-reporter [cli,html] - Which coverage reporter to use (defaults to cli)\\n--coverage-dir - Directory where the coverage HTML report is saved\\n--scope-leaks - Record which variables were leaked into a global scope\\n--scope-leaks-reporter [cli] - Which scope leak reporter to use (defaults\\nto cli)\\n--debug NUMBER - Attach a debugger to a test process listening on the specified port number\\n--report-timing - Report each test run time\\n--dependencies STRING - Specify path to the dependencies file for the\\nprocess runner. More information about the process runner can be found at\\nPROCESS_RUNNER.md\\n--only-essential-dependencies - Only start dependencies required by the tests\\nfiles which are ran. This option is only applicable if --dependencies option\\nis used.\\n\\nNote: When specifying multiple test a list with the test paths must be quoted,\\nfor example: whiskey --tests \"tests/a.js tests/b.js tests/c.js\"\\nA Note about setUp and tearDown\\nPresently, two kinds of setup and teardown procedures exist with Whiskey.\\nsetUp and tearDown work on a per-test basis; that is, Whiskey invokes setUp\\nbefore running a test in a given Javascript file, called a suite and tearDown\\nis invoked after a test run has finished.\\nIf you run multiple suites in parallel (e.g., via the\\n-T/--independent-tests option), you\\'ll get concurrent execution of setups and\\nteardowns as well.\\nSometimes, though, you need longer-lived environmental configurations, or you\\nneed safe resource sharing between entire batches of independently running\\ntests. For these, you\\'ll want to use globalSetUp and globalTearDown.\\n\\nWhen do I use setUp / tearDown?\\n\\nWhen a suite\\'s runtime environment does not influence other running suites.\\n\\n\\nWhen do I use globalSetUp / globalTearDown ?\\n\\nWhen a suite\\'s runtime environment can potentially interfere with other, concurrently running suites.\\nExample: Attempting to run multiple suites in parallel which rely on a Cassandra schema being in place, and each attempting to reset the schema to a known state on a single Cassandra instance, you\\'ll get Cassandra schema version errors.  Using globalSetUp prevents this by running the schema reset code exactly once for all tests.\\n\\n\\n\\nTest File Examples\\nA simple example (success):\\nvar called = 0;\\n\\nexports.test_async_one_equals_one = function(test, assert) {\\n  setTimeout(function() {\\n    assert.equal(1, 1);\\n    called++;\\n    test.finish();\\n  }, 1000);\\n};\\n\\nexports.tearDown = function(test, assert) {\\n  assert.equal(called, 1);\\n  test.finish();\\n};\\nA simple example (skipping a test):\\nvar dbUp = false;\\n\\nexports.test_query = function(test, assert) {\\n  if (!dbUp) {\\n    test.skip(\\'Database is not up, skipping...\\');\\n    return;\\n  }\\n\\n  assert.equal(2, 1);\\n  test.finish();\\n};\\nA simple example (failure):\\nexports.test_two_equals_one = function(test, assert) {\\n  assert.equal(2, 1);\\n  test.finish();\\n};\\nA simple example using the optional BDD module:\\nvar bdd = require(\\'whiskey\\').bdd.init(exports);\\nvar describe = bdd.describe;\\n\\ndescribe(\\'the bdd module\\', function(it) {\\n  it(\\'supports it(), expect(), and toEqual()\\', function(expect) {\\n    expect(true).toEqual(true);\\n  });\\n});\\nA simple example demonstrating how to use global setup and teardown functionality:\\nexports[\\'globalSetUp\\'] = function(test, assert) {\\n  // Set up database schema here...\\n  // Push known data set to database here...\\n  test.finish();\\n}\\n\\nexports[\\'globalTearDown\\'] = function(test, assert) {\\n  // Drop database here...\\n  test.finish();\\n}\\nFor more examples please check the example/ folder, and the test/run.sh script.\\nBuild status\\n\\nRunning Whiskey test suite\\nTo run the Whiskey test suite, run the following command in the repository root\\ndirectory.\\nnpm test\\nIf all the tests have sucessfully passed, the process should exit with a zero\\nstatus code and you should see * * * Whiskey test suite PASSED. * * *\\nmessage.\\nContributing\\nTo contribute, fork the repository, create a branch with your changes and open a\\npull request.\\nDebugging\\nIf you want to debug your test, you can use the --debug option. This will\\ncause Whiskey to start the test process with the V8 debugger functionality.\\nYou then need to manually connect to the debugger to control it (i.e. using\\nnode repl or node-inspector).\\nWhiskey will also by default set a breakpoint at the beginning of your test\\nfile.\\nNote: This option can only be used with a single test file.  Further, you\\ncannot use the --debug and --independent-tests options together.  The\\nsemantics just don\\'t make any sense.  To debug a test, make sure you invoke it\\nwith --tests instead.\\nTroubleshooting\\nI use long-stack-straces module in my own code and all of the tests get reported as succeeded\\nLong stack traces modules intercepts the default Error object and throws a custom\\none. The problem with this is that Whiskey internally relies on attaching the\\ntest name to the Error object so it can figure out to which test the exception\\nbelongs. long-stack-traces throws a custom Error object and as a consequence test\\nname attribute gets lost so Whiskey thinks your test didn\\'t throw any exceptions.\\nThe solution for this problem is to disable long-stack-trace module when running\\nthe tests. This shouldn\\'t be a big deal, because Whiskey internally already uses\\nlong-stack-traces module which means that you will still get long stack traces\\nin the exceptions which were thrown in your tests.\\nMy test gets reported as \"timeout\" instead of \"failure\"\\nIf your test gets reported as \"timeout\" instead of \"failure\" your test code most\\nlikely looks similar to the one below:\\nexports.test_failure = function(test, assert){\\n  setTimeout(function() {\\n    throw \"blaaaaah\";\\n    test.finish();\\n  },200);\\n};\\nThe problem with this is that if you run tests in parallel (--concurrency > 1)\\nand you don\\'t use a custom assert object which gets passed to each test function,\\nWhiskey can\\'t figure out to which test the exception belongs. As a consequence,\\nthe test is reported as \"timed out\" and the exception is reported as \"uncaught\".\\nThe solution for this problem is to run the tests in sequential mode (drop the\\n--concurrency option).\\nLicense\\nApache 2.0, for more info see LICENSE.\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '251',\n",
       "  'forks': '26',\n",
       "  'commits': '550'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"flux-router-component\\nNotice: This package is deprecated in favor of fluxible-router.\\n\\n\\n\\n\\n\\nProvides navigational React components (NavLink), router mixin (RouterMixin), and action navigateAction for applications built with Flux architecture.  Please check out examples of how to use these components.\\nContext and Expected Context Methods\\nBefore we explain how to use NavLink and RouterMixin, lets start with two methods they expect:\\n\\nexecuteAction(navigateAction, payload) - This executes navigate action, switches the app to the new route, and update the url.\\nmakePath(routeName, routeParams) - This is used to generate url for a given route.\\n\\nThese two methods need to be available in:\\n\\nthe React context of the component (access via this.context in the component), or\\nthe context prop of the component (this.props.context)\\nIf exists in both this.context and this.props.context, the one in this.context takes higher precedence.\\n\\nAn example of such context is the ComponentContext provided by fluxible-plugin-routr, which is a plugin for fluxible.  We have a more sophisticated example application, fluxible-router, showing how everything works together.\\nNote that React context is an undocumented feature, so its API could change without notice.  Here is a blog from Dave King that explains what it is and how to use it.\\nNavLink\\nDocs\\nRouterMixin\\nDocs\\nnavigateAction\\nDocs\\nHistory Management (Browser Support and Hash-Based Routing)\\nConsidering different application needs and different browser support levels for pushState, this library provides the following options for browser history management:\\n\\nUse History provided by this library (Default)\\nUse HistoryWithHash provided by this library\\nIn addition, you can also customize it to use your own\\n\\nHistory\\nThis is the default History implementation RouterMixin uses.  It is a straight-forward implementation that:\\n\\nuses pushState/replaceState when they are available in the browser.\\nFor the browsers without pushState support, History simply refreshes the page by setting window.location.href = url for pushState, and calling window.location.replace(url) for replaceState.\\n\\nHistoryWithHash\\nUsing hash-based url for client side routing has a lot of known issues.  History.js describes those issues pretty well.\\nBut as always, there will be some applications out there that have to use it.  This implementation provides a solution.\\nIf you do decide to use hash route, it is recommended to enable checkRouteOnPageLoad.  Because hash fragment (that contains route) does not get sent to the server side, RouterMixin will compare the route info from server and route in the hash fragment.  On route mismatch, it will dispatch a navigate action on browser side to load the actual page content for the route represented by the hash fragment.\\nuseHashRoute Config\\nYou can decide when to use hash-based routing through the useHashRoute option:\\n\\nuseHashRoute=true to force to use hash routing for all browsers, by setting useHashRoute to true when creating the HistoryWithHash instance;\\nunspecified, i.e. omitting the setting, to only use hash route for browsers without native pushState support;\\nuseHashRoute=false to turn off hash routing for all browsers.\\n\\n\\n\\n\\n\\nuseHashRoute = true\\nuseHashRoute = false\\nuseHashRoute unspecified\\n\\n\\n\\n\\nBrowsers with pushState support\\nhistory.pushState with /home#/path/to/pageB\\nhistory.pushState with /path/to/pageB\\nSame as useHashRoute = false\\n\\n\\nBrowsers without pushState support\\npage refresh to /home#/path/to/pageB\\npage refresh to /path/to/pageB\\nSame as useHashRoute = true\\n\\n\\n\\nCustom Transformer for Hash Fragment\\nBy default, the hash fragments are just url paths.  With HistoryWithHash, you can transform it to whatever syntax you need by passing props.hashRouteTransformer to the base React component that RouterMixin is mixed into.  See the example below for how to configure it.\\nExample\\nThis is an example of how you can use and configure HistoryWithHash:\\nvar RouterMixin = require('flux-router-component').RouterMixin;\\nvar HistoryWithHash = require('flux-router-component/utils').HistoryWithHash;\\n\\nvar Application = React.createClass({\\n    mixins: [RouterMixin],\\n    ...\\n});\\n\\nvar appComponent = Application({\\n    ...\\n    historyCreator: function historyCreator() {\\n        return new HistoryWithHash({\\n            // optional. Defaults to true if browser does not support pushState; false otherwise.\\n            useHashRoute: true,\\n            // optional. Defaults to '/'. Used when url has no hash fragment\\n            defaultHashRoute: '/default',\\n            // optional. Transformer for custom hash route syntax\\n            hashRouteTransformer: {\\n                transform: function (original) {\\n                    // transform url hash fragment from '/new/path' to 'new-path'\\n                    var transformed = original.replace('/', '-').replace(/^(\\\\-+)/, '');\\n                    return transformed;\\n                },\\n                reverse: function (transformed) {\\n                    // reverse transform from 'new-path' to '/new/path'\\n                    var original = '/' + (transformed && transformed.replace('-', '/'));\\n                    return original;\\n                }\\n            }\\n        });\\n    }\\n});\\nProvide Your Own History Manager\\nIf none of the history managers provided in this library works for your application, you can also customize the RouterMixin to use your own history manager implementation.  Please follow the same API as History.\\nAPI\\nPlease use History.js and HistoryWithHash.js as examples.\\n\\non(listener)\\noff(listener)\\ngetUrl()\\ngetState()\\npushState(state, title, url)\\nreplaceState(state, title, url)\\n\\nExample:\\nvar RouterMixin = require('flux-router-component').RouterMixin;\\nvar MyHistory = require('MyHistoryManagerIsAwesome');\\n\\nvar Application = React.createClass({\\n    mixins: [RouterMixin],\\n    ...\\n});\\n\\nvar appComponent = Application({\\n    ...\\n    historyCreator: function historyCreator() {\\n        return new MyHistory();\\n    }\\n});\\nScroll Position Management\\nRouterMixin has a built-in mechanism for managing scroll position upon page navigation, for modern browsers that support native history state:\\n\\nreset scroll position to (0, 0) when user clicks on a link and navigates to a new page, and\\nrestore scroll position to last visited state when user clicks forward and back buttons to navigate between pages.\\n\\nIf you want to disable this behavior, you can set enableScroll prop to false for RouterMixin.  This is an example of how it can be done:\\nvar RouterMixin = require('flux-router-component').RouterMixin;\\n\\nvar Application = React.createClass({\\n    mixins: [RouterMixin],\\n    ...\\n});\\n\\nvar appComponent = Application({\\n    ...\\n    enableScroll: false\\n});\\nonbeforeunload Support\\nThe History API does not allow popstate events to be cancelled, which results in window.onbeforeunload() methods not being triggered.  This is problematic for users, since application state could be lost when they navigate to a certain page without knowing the consequences.\\nOur solution is to check for a window.onbeforeunload() method, prompt the user with window.confirm(), and then navigate to the correct route based on the confirmation.  If a route is cancelled by the user, we reset the page URL back to the original URL by using  the History pushState() method.\\nTo implement the window.onbeforeunload() method, you need to set it within the components that need user verification before leaving a page.  Here is an example:\\ncomponentDidMount: function() {\\n  window.onbeforeunload = function () {\\n    return 'Make sure to save your changes before leaving this page!';\\n  }\\n}\\nPolyfills\\naddEventListener and removeEventListener polyfills are provided by:\\n\\nCompatibility code example on Mozilla Developer Network\\nA few DOM polyfill libaries listed on Modernizer Polyfill wiki page.\\n\\nArray.prototype.reduce and Array.prototype.map (used by dependent library, query-string) polyfill examples are provided by:\\n\\nMozilla Developer Network Array.prototype.reduce polyfill\\nMozilla Developer Network Array.prototype.map polyfill\\n\\nYou can also look into this polyfill.io polyfill service.\\nCompatible React Versions\\n\\n\\n\\nCompatible React Version\\nflux-router-component Version\\n\\n\\n\\n\\n0.12\\n>= 0.4.1\\n\\n\\n0.11\\n< 0.4\\n\\n\\n\\nLicense\\nThis software is free to use under the Yahoo! Inc. BSD license.\\nSee the LICENSE file for license text and copyright information.\\nThird-pary open source code used are listed in our package.json file.\\n\",\n",
       "  'watchers': '15',\n",
       "  'stars': '250',\n",
       "  'forks': '29',\n",
       "  'commits': '225'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"PowerArray\\nTurns out that you can re-write some of the methods of Array to obtain a much better performance than the native methods.\\nIn particular, Array.forEach seems to perform pretty badly.\\n\\nNote: the overridden methods of PowerArray break compliance, the focus is on performance so take a look at the caveat section below.\\n\\nIt looks as if a for loop with cached length is the fastest way of iterating.\\nvar i, len = array.length;\\nfor (i = 0; i < len; i += 1) {\\n  someFun(array[i]);\\n}\\nSo I rewrote the Array class as PowerArray and implemented the above mechanism in PowerArray.forEach with surprising results.\\nThe results are as follows:\\nPowerArray.forEach is averagely 5 times faster than native Array.\\nThis is only a proof of concept.\\nInstall with npm install powerarray\\nProposed Usage\\nParticularly useful for arrays that need processing on all elements often, or for numeric arrays utilized as indexes for Collections of data.\\nMethods\\nAll Array native methods are available through PowerArray. The following methods are either extending or overriding the native Array class.\\nPowerArray.forEach: utilizes a for loop for iteration, takes a callback which receives an element and the index of that element.\\nPowerArray.map: utilizes a for loop to return a PowerArray of mapped values, takes a callback processing function argument.\\nPowerArray.binarySearch: performs a binary search on the elements of the array, only relevant if the array only consists of numbers. Thanks to Oliver Caldwell's post for a quick version of the algorithm. Also note the contribution of Yehonatan and other authors of comments to the post which helped to optimise the implementation of binary search further.\\nPowerArray.numericSort: sorts array (if array only contains integers), useful for utilizing binarySearch. Optional sorting function argument.\\nPowerArray.addAndSort: adds a new value and sorts the array automatically\\nContribution\\nPull requests are more than welcome, just make sure to add a test in tests/test.js (and that it passes it obviously).\\nCaveats\\nThanks to David Souther for documenting these:\\n\\nNo this context in fn calls, handle your own binding.\\nNo determination if i is a member of PowerArray (eg for sparse arrays, [2, 4, , 6])\\nNo exception is thrown when the callback isn't callable.\\n\\nThere may be more, please feel free to flag those or include them yourself through a pull request.\\n\",\n",
       "  'watchers': '8',\n",
       "  'stars': '249',\n",
       "  'forks': '15',\n",
       "  'commits': '54'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'angular-load\\nDynamically load scripts and CSS stylesheets in your Angular.JS app.\\nCopyright (C) 2014, 2015, Uri Shaked uri@urish.org\\n\\n\\nInstallation\\nYou can choose your preferred method of installation:\\n\\nThrough bower: bower install angular-load --save\\nThrough npm: npm install angular-load --save\\nDownload from github: angular-load.js\\n\\nUsage\\nInclude angular-load.js in your application.\\n<script src=\"bower_components/angular-load/dist/angular-load.min.js\"></script>\\nAdd the module angularLoad as a dependency to your app module:\\nvar myapp = angular.module(\\'myapp\\', [\\'angularLoad\\']);\\nYou can also use ES6 or CommonJS\\nimport angularLoad from \\'angular-load\\';\\n// or\\nvar angularLoad = require(\\'angular-load\\');\\n\\nvar myapp = angular.module(\\'myapp\\', [angularLoad]);\\nangularLoad service directive\\nThe angularLoad service provides three methods: loadScript(), loadCSS() and unloadCSS().\\nCall the loadScript() and loadCSS() methods to load a script\\nor a CSS stylesheet asynchronously into the current page. Both methods return a promise that will be resolved\\nonce the resource (script or stylesheet) has been loaded. In case of an error (e.g. HTTP 404) the promise will be\\nrejected.\\nCall the unloadCSS() method to unload a CSS stylesheet from the current page. This method return boolean value, specifying whether the given stylesheet URL could be located in the page and has been unloaded. In case of trying to remove non-existent resource the function will return false\\nUsage example:\\nangularLoad.loadScript(\\'https://mysite.com/someplugin.js\\').then(function() {\\n\\t// Script loaded succesfully.\\n\\t// We can now start using the functions from someplugin.js\\n}).catch(function() {\\n    // There was some error loading the script. Meh\\n});\\nCollaborators\\nUri Shaked uri@urish.org, Colm Seale colm.seale@gmail.com\\nLicense\\nReleased under the terms of MIT License:\\nPermission is hereby granted, free of charge, to any person obtaining\\na copy of this software and associated documentation files (the\\n\\'Software\\'), to deal in the Software without restriction, including\\nwithout limitation the rights to use, copy, modify, merge, publish,\\ndistribute, sublicense, and/or sell copies of the Software, and to\\npermit persons to whom the Software is furnished to do so, subject to\\nthe following conditions:\\nThe above copyright notice and this permission notice shall be\\nincluded in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \\'AS IS\\', WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '245',\n",
       "  'forks': '63',\n",
       "  'commits': '45'},\n",
       " {'language': 'Python 99.3',\n",
       "  'readme': 'django-mediasync\\nOne of the more significant development roadblocks we have relates to local vs.\\ndeployed media. Ideally all media (graphics, css, scripts) development would\\noccur locally and not use production media. Then, when ready to deploy, the\\nmedia should be pushed to production. That way there can be significant changes\\nto media without disturbing the production web site.\\nThe goal of mediasync is to develop locally and then flip a switch in production\\nthat makes all the media URLs point to remote media URLs instead of the local\\nmedia directory.\\nAll code is under a BSD-style license, see LICENSE for details.\\nSource: http://github.com/sunlightlabs/django-mediasync/\\n\\nRequirements\\n\\ndjango >= 1.0\\nboto >= 1.8d\\nslimmer == 0.1.30 (optional)\\npython-cloudfiles == 1.7.5 (optional, for Rackspace Cloud Files backend)\\n\\n\\nUpgrading from mediasync 1.x\\n\\nUpdate your mediasync settings as described in the next section.\\n\\nRun ./manage.py syncmedia --force to force updates of all files:\\n\\ngzip instead of deflate compression\\nsync both compressed and original versions of files\\n\\n\\n\\n\\nadd \"django.core.context_processors.request\" to TEMPLATE_CONTEXT_PROCESSORS\\n\\n\\nAn important note about Django 1.3\\nWhen DEBUG = True and the project is run with manage.py runserver, Django 1.3\\nautomatically adds django.views.static.serve to urlpatterns. While this feature\\nmakes local development easier for most people, it screws everything up if\\nyou\\'ve added mediasync.urls to urlpatterns. As of now, the only way I can find\\nto disable the automatic addition of django.views.static.serve is to use a full\\nURL for STATIC_URL instead of just a path:\\nSTATIC_URL = \"http://localhost:8000/static/\"\\n\\n\\nConfiguration\\n\\nsettings.py\\nAdd to INSTALLED_APPS:\\n\\'mediasync\\'\\n\\nAdd to TEMPLATE_CONTEXT_PROCESSORS:\\n\\'django.core.context_processors.request\\'\\n\\nMake sure your STATIC_ROOT setting is the correct path to your media:\\nSTATIC_ROOT = \\'/path/to/media\\'\\n\\nWhen media is being served locally (instead of from S3 or Cloud Files),\\nmediasync serves media through a Django view. Set your STATIC_URL to what\\nyou\\'d like that local media URL to be. This can be whatever you\\'d like, as long\\nas you\\'re using the {% media_url %} tag (more details on this later):\\nSTATIC_URL = \\'http://localhost:8000/devmedia/\\'\\n\\nSTATIC_URL is the URL that will be used in debug mode. Otherwise,\\nthe STATIC_URL will be loaded from the backend settings. Please see\\nAn important note about Django 1.3.\\nThe following settings dict must also be added:\\nMEDIASYNC = {\\n    \\'BACKEND\\': \\'path.to.backend\\',\\n}\\n\\nIf you want to use a different media URL than that specified\\nin settings.STATIC_URL, you can add STATIC_URL to the MEDIASYNC\\nsettings dict:\\nMEDIASYNC = {\\n    ...\\n    \\'STATIC_URL\\': \\'/url/to/media/\\', # becomes http://yourhost.com/url/to/media/\\n    ...\\n}\\n\\nSame goes for STATIC_ROOT:\\nMEDIASYNC = {\\n    ...\\n    \\'STATIC_ROOT\\': \\'/path/to/media/\\',\\n    ...\\n}\\n\\nmediasync supports pluggable backends. Please see below for information on\\nthe provided backends as well as directions on implementing your own.\\n\\nMedia expiration\\nIf the client supports media expiration, all files are set to expire 365 days\\nafter the file was synced. You may override this value by adding\\nEXPIRATION_DAYS to the MEDIASYNC settings dict.\\n# Expire in 10 years.\\nMEDIASYNC[\\'EXPIRATION_DAYS\\'] = 365 * 10\\n\\n\\nServing media remote (S3/Cloud Files) or locally\\nThe media URL is selected based on the SERVE_REMOTE attribute in the\\nMEDIASYNC dict in settings.py. When False, media will be served locally\\ninstead of from S3.\\n# This would force mediasync to serve all media through the value\\n# specified in settings.STATIC_URL.\\nMEDIASYNC[\\'SERVE_REMOTE\\'] = False\\n\\n# This would serve all media through S3/Cloud Files.\\nMEDIASYNC[\\'SERVE_REMOTE\\'] = True\\n\\n# This would serve media locally while in DEBUG mode, and remotely when\\n# in production (DEBUG == False).\\nMEDIASYNC[\\'SERVE_REMOTE\\'] = not DEBUG\\n\\nWhen serving files locally, you can emulate the CSS/JS combo/minifying\\nbehavior we get from using media processors by specifying the following.\\nMEDIASYNC[\\'SERVE_REMOTE\\'] = False\\nMEDIASYNC[\\'EMULATE_COMBO\\'] = True\\n\\nNote that this will only work if your STATIC_URL is pointing at your\\nDjango dev server. Also keep in mind that some processors may take a while,\\nand is best used to check things over before rolling out to production.\\n\\nDOCTYPE\\nlink and script tags are written using XHTML syntax. The rendering can be\\noverridden by using the DOCTYPE setting. Allowed values are \\'html4\\',\\n\\'html5\\', or \\'xhtml\\'. The default in mediasync 2.0 is html5, just as\\nthe DOCTYPE on your site should be.\\nMEDIASYNC[\\'DOCTYPE\\'] = \\'html5\\'\\n\\nFor each doctype, the following tags are rendered:\\n\\nhtml4\\n<link rel=\"stylesheet\" href=\"...\" type=\"text/css\" media=\"...\">\\n<script type=\"text/javascript\" charset=\"utf-8\" src=\"...\"></script>\\n\\n\\nhtml5\\n<link rel=\"stylesheet\" href=\"...\" media=\"...\">\\n<script src=\"...\"></script>\\n\\n\\nxhtml\\n<link rel=\"stylesheet\" href=\"...\" type=\"text/css\" media=\"...\" />\\n<script type=\"text/javascript\" charset=\"utf-8\" src=\"...\"></script>\\n\\n\\nSSL\\nmediasync will attempt to intelligently determine if your media should be\\nserved using HTTPS. In order to use automatic SSL detection,\\ndjango.core.context_processors.request must be added to\\nTEMPLATE_CONTEXT_PROCESSORS in settings.py:\\nTEMPLATE_CONTEXT_PROCESSORS = (\\n    ...\\n    \\'django.core.context_processors.request\\',\\n    ...\\n)\\n\\nThe USE_SSL mediasync setting can be used to override the SSL\\nURL detection.\\n# Force HTTPS.\\nMEDIASYNC[\\'USE_SSL\\'] = True\\n\\nor\\n# Force HTTP.\\nMEDIASYNC[\\'USE_SSL\\'] = False\\n\\nSome backends will be unable to use SSL. In these cases USE_SSL and SSL\\ndetection will be ignored.\\n\\nurls.py\\nTo serve local media through mediasync, add a reference to mediasync.urls in\\nyour main urls.py file.\\nurlpatterns = (\\'\\',\\n    ...\\n    url(r\\'^\\', include(\\'mediasync.urls)),\\n    ...\\n)\\n\\n\\nBackends\\nmediasync now supports pluggable backends. A backend is a Python module that\\ncontains a Client class that implements a mediasync-provided BaseClient class.\\n\\nS3\\nMEDIASYNC[\\'BACKEND\\'] = \\'mediasync.backends.s3\\'\\n\\n\\nSettings\\nThe following settings are required in the mediasync settings dict:\\nMEDIASYNC = {\\n    \\'AWS_KEY\\': \"s3_key\",\\n    \\'AWS_SECRET\\': \"s3_secret\",\\n    \\'AWS_BUCKET\\': \"bucket_name\",\\n}\\n\\nOptionally you may specify a path prefix:\\nMEDIASYNC[\\'AWS_PREFIX\\'] = \"key_prefix\"\\n\\nAssuming a correct DNS CNAME entry, setting AWS_BUCKET to\\nassets.sunlightlabs.com and AWS_PREFIX to myproject/media would\\nsync the media directory to http://assets.sunlightlabs.com/myproject/media/.\\nAmazon allows users to create DNS CNAME entries to map custom domain names\\nto an AWS bucket. MEDIASYNC can be configured to use the bucket as the media\\nURL by setting AWS_BUCKET_CNAME to True.\\nMEDIASYNC[\\'AWS_BUCKET_CNAME\\'] = True\\n\\nIf you would prefer to not use gzip compression with the S3 client, it can be\\ndisabled:\\nMEDIASYNC[\\'AWS_GZIP\\'] = False\\n\\n\\nTips\\nSince files are given a far future expires header, one needs a way to do\\n\"cache busting\" when you want the browser to fetch new files before the expire\\ndate arrives.  One of the best and easiest ways to accomplish this is to alter\\nthe path to the media files with some sort of version string using the key\\nprefix setting:\\nMEDIASYNC[\\'AWS_PREFIX\\'] = \"myproject/media/v20001201\"\\n\\nGiven that and the above DNS CNAME example, the media directory URL would end\\nup being http://assets.sunlightlabs.com/myproject/media/v20001201/.  Whenever\\nyou need to update the media files, simply update the key prefix with a new\\nversioned string.\\nA CACHE_BUSTER settings can be added to the main MEDIASYNC settings\\ndict to add a query string parameter to all media URLs. The cache buster can\\neither be a value or a callable which is passed the media URL as a parameter.\\nMEDIASYNC[\\'CACHE_BUSTER\\'] = 1234567890\\n\\nThe above setting will generate a media path similar to:\\nhttp://yourhost.com/url/to/media/image.png?1234567890\\n\\nAn important thing to note is that if you\\'re running your Django site in a\\nmulti-threaded or multi-node setup, you\\'ll want to be careful about using a\\ntime-based cache buster value. Each worker/thread will probably have a slightly\\ndifferent value for datetime.now(), which means your users will find themselves\\nhaving cache misses randomly from page to page.\\n\\nRackspace Cloud Files\\nMEDIASYNC[\\'BACKEND\\'] = \\'mediasync.backends.cloudfiles\\'\\n\\n\\nSettings\\nThe following settings are required in the mediasync settings dict:\\nMEDIASYNC = {\\n    \\'CLOUDFILES_CONTAINER\\': \\'container_name\\',\\n    \\'CLOUDFILES_USERNAME\\': \\'cf_username\\',\\n    \\'CLOUDFILES_API_KEY\\': \\'cf_apikey\\',\\n}\\n\\n\\nTips\\nThe Cloud Files backend lacks support for the following features:\\n\\nsetting HTTP Expires header\\nsetting HTTP Cache-Control header\\ncontent compression (gzip)\\nSSL support\\nconditional sync based on file checksum\\n\\n\\nCustom backends\\nYou can create a custom backend by creating a Python module containing a Client\\nclass. This class must inherit from mediasync.backends.BaseClient. Additionally,\\nyou must implement two methods:\\ndef remote_media_url(self, with_ssl):\\n    ...\\n\\nremote_media_url returns the full base URL for remote media. This can be\\neither a static URL or one generated from mediasync settings:\\ndef put(self, filedata, content_type, remote_path, force=False):\\n    ...\\n\\nput is responsible for pushing a file to the backend storage.\\n\\nfiledata - the contents of the file\\ncontent_type - the mime type of the file\\nremote_path - the remote path (relative from remote_media_url) to which\\nthe file should be written\\nforce - if True, write file to remote storage even if it already exists\\n\\nIf the client supports gzipped content, you will need to override supports_gzip\\nto return True:\\ndef supports_gzip(self):\\n        return True\\n\\n\\nFile Processors\\nFile processors allow you to modify the content of a file as it is being\\nsynced or served statically.\\nMediasync ships with three processor modules:\\n\\nslim is a minifier written in Python and requires the\\nslimmer Python package. The Python package can be found here:\\nhttp://pypi.python.org/pypi/slimmer/\\nyuicompressor is a minifier written in Java and can be downloaded\\nfrom YUI\\'s download page: http://developer.yahoo.com/yui/compressor/.\\nThis processor also requires an additional setting, as defined below.\\nyuicompressor is new and should be considered experimental until\\nthe mediasync 2.1 release.\\nclosurecompiler is a javascript compiler provided by Google.\\n\\nCustom processors can be specified using the PROCESSORS entry in the\\nmediasync settings dict. PROCESSORS should be a list of processor entries.\\nEach processor entry can be a callable or a string path to a callable. If the\\npath is to a class definition, the class will be instantiated into an object.\\nThe processor callable should return a string of the processed file data, None\\nif it chooses to not process the file, or raise mediasync.SyncException if\\nsomething goes terribly wrong. The callable should take the following arguments:\\ndef proc(filedata, content_type, remote_path, is_active):\\n        ...\\n\\n\\nfiledata\\nthe content of the file as a string\\ncontent_type\\nthe mimetype of the file being processed\\nremote_path\\nthe path to which the file is being synced (contains the file name)\\nis_active\\nTrue if the processor should... process\\n\\nIf the PROCESSORS setting is used, you will need to include the defaults\\nif you plan on using them:\\n\\'PROCESSORS\\': (\\n    \\'mediasync.processors.slim.css_minifier\\',\\n    \\'mediasync.processors.slim.js_minifier\\',\\n        ...\\n),\\n\\nmediasync will attempt to use slimmer by default if you have the package\\ninstalled and do not use the PROCESSORS setting.\\n\\nGoogle Closure Compiler\\nGoogle\\'s JavaScript Closure Compiler provides an API that allows files to be\\ncompressed without installing anything locally. To use the service:\\n\\'PROCESSORS\\': (\\'mediasync.processors.closurecompiler.compile\\',)\\n\\n\\nYUI Compressor\\nTo configure YUI Compressor you need to define a PROCESSORS and\\nYUI_COMPRESSOR_PATH as follows, assuming you placed the \".jar\" file in\\nyour ~/bin path:\\n\\'PROCESSORS\\': (\\'mediasync.processors.yuicompressor.css_minifier\\',\\n               \\'mediasync.processors.yuicompressor.js_minifier\\'),\\n\\'YUI_COMPRESSOR_PATH\\': \\'~/bin/yuicompressor.jar\\',\\n\\n\\nFeatures\\n\\nIgnored Directories\\nAny directory in STATIC_ROOT that is hidden or starts with an underscore\\nwill be ignored during syncing.\\n\\nTemplate Tags\\nWhen referring to media in HTML templates you can use custom template tags.\\nThese tags can by accessed by loading the media template tag collection.\\n{% load media %}\\n\\nAny tag that has a path argument can use either a string or a variable:\\n{% media_url \"images/avatar.png\" }\\n{% media_url user.profile.avatar_path %}\\n\\nSome backends (S3) support https URLs when the requesting page is secure.\\nIn order for the https to be detected, the request must be placed in the\\ntemplate context with the key \\'request\\'. This can be done automatically by\\nadding \\'django.core.context_processors.request\\' to TEMPLATE_CONTEXT_PROCESSORS\\nin settings.py\\n\\nmedia_url\\nRenders the STATIC_URL from settings.py with trailing slashes removed.\\n<img src=\"{% media_url %}/images/stuff.png\">\\n\\nSTATIC_URL takes an optional argument that is the media path. Using the argument\\nallows mediasync to add the CACHE_BUSTER to the URL if one is specified.\\n<img src=\"{% media_url \\'/images/stuff.png\\' %}\">\\n\\nIf CACHE_BUSTER is set to 12345, the above example will render as:\\n<img src=\"http://assets.example.com/path/to/media/images/stuff.png?12345\">\\n\\nNOTE: Don\\'t use this tag to serve CSS or JS files. Use the js and css tags\\nthat were specifically designed for the purpose.\\n\\njs\\nRenders a script tag with the correct include.\\n{% js \"myfile.js\" %}\\n\\n\\ncss\\nRenders a <link> tag to include the stylesheet. It takes an optional second\\nparameter for the media attribute; the default media is \"screen, projector\".\\n{% css \"myfile.css\" %}\\n{% css \"myfile.css\" \"screen\" %}\\n\\n\\ncss_print\\nShortcut to render as a print stylesheet.\\n{% css_print \"myfile.css\" %}\\n\\nwhich is equivalent to\\n{% css \"myfile.css\" \"print\" %}\\n\\n\\nWriting Style Sheets\\nUsers are encouraged to write stylesheets using relative URLS. The media\\ndirectory is synced with S3 as is, so relative local paths will still work\\nwhen pushed remotely.\\nbackground: url(../images/arrow_left.png);\\n\\n\\nJoined files\\nWhen serving media in production, it is beneficial to combine JavaScript and\\nCSS into single files. This reduces the number of connections the browser needs\\nto make to the web server. Fewer connections can dramatically decrease page\\nload times and reduce the server-side load.\\nJoined files are specified in the MEDIASYNC dict using JOINED. This is\\na dict that maps individual media to an alias for the joined files.\\n\\'JOINED\\': {\\n    \\'styles/joined.css\\': [\\'styles/reset.css\\',\\'styles/text.css\\'],\\n    \\'scripts/joined.js\\': [\\'scripts/jquery.js\\',\\'scripts/processing.js\\'],\\n},\\n\\nFiles listed in JOINED will be combined and pushed to S3 with the name of\\nthe alias. The individual CSS files will also be pushed to S3. Aliases must end\\nin either .css or .js in order for the content-type to be set appropriately.\\nThe existing template tags may be used to refer to the joined media. Simply use\\nthe joined alias as the argument:\\n{% css_print \"joined.css\" %}\\n\\nWhen served locally, template tags will render an HTML tag for each of the files\\nthat make up the joined file:\\n<link rel=\"stylesheet\" href=\"/media/styles/reset.css\" type=\"text/css\" media=\"screen, projection\" />\\n<link rel=\"stylesheet\" href=\"/media/styles/text.css\" type=\"text/css\" media=\"screen, projection\" />\\n\\nWhen served remotely, one HTML tag will be rendered with the name of the joined file:\\n<link rel=\"stylesheet\" href=\"http://bucket.s3.amazonaws.com/styles/joined.css\" type=\"text/css\" media=\"screen, projection\" />\\n\\n\\nSmart GZIP for S3\\nIn previous versions of mediasync\\'s S3 client, certain content was always pushed\\nin a compressed format. This can cause major issues with clients that do not\\nsupport gzip. New in version 2.0, mediasync will push both a gzipped and an\\nuncompressed version of the file to S3. The template tags look at the request\\nand direct the user to the appropriate file based on the ACCEPT_ENCODING HTTP\\nheader. Assuming a file styles/layout.css, the following would be synced to S3:\\nstyles/layout.css\\nstyles/layout.css.gzt\\n\\nNote the altered use of the .gz extension. Some versions of the Safari browser\\non OSX ignore the Content-Type header for files ending in .gz and treat them\\ninstead as files to download. This altered extension allows Safari to deflate\\nand utilize the files correctly without affecting functionality in any other\\ntested browsers.\\n\\nSignals\\nmediasync provides two signals that allow you to hook into the syncing\\nprocess. pre_sync is sent after the client is opened, but before the first\\nfile is synced. post_sync is sent after the last file is synced, but before\\nthe client is closed. This allows you to call commands on the client without\\nhaving to worry about its state. The signals allow you to do common tasks such\\nas calling Django 1.3\\'s collectstatic command, process SASS stylesheets, or\\nclean up files generated during a pre_sync process.\\n\\ncollectstatic receiver\\nA receiver for calling the collectstatic management command is provided:\\nfrom mediasync.signals import pre_sync, collectstatic_receiver\\n\\n# run collectstatic before syncing media\\npre_sync.connect(collectstatic_receiver)\\n\\n\\nSASS receiver\\nA receiver for compiling SASS into CSS is provided:\\nfrom mediasync.signals import pre_sync, sass_receiver\\n\\n# compile SASS files before syncing media\\npre_sync.connect(sass_receiver)\\n\\nAny file in static root that has the sass or scss file extension will be\\ncompiled into CSS. The compiled CSS file will be placed in the same directory\\nand the original extension will be replaced with css. If a file exists with\\nthe same css extension, it will be overwritten.\\nBy default mediasync uses the sass command with no options. If you would\\nlike to specify your own command, specify SASS_COMMAND in settings:\\nMEDIASYNC = {\\n    ...\\n    \\'SASS_COMMAND\\': \\'sass -scss -l\\',\\n    ...\\n}\\n\\n\\nRunning MEDIASYNC\\n./manage.py syncmedia\\n\\n\\nChange Log\\n\\n2.2.0\\n\\nadded pre_sync and post_sync signals\\nprovide basic receiver for calling collectstatic before syncing\\nprovide receiver for compiling SASS before syncing\\nshow media directory listing when serving locally in debug mode\\nadd processor for Google\\'s Closure Compiler API for JavaScript\\ntemplate tags can now take a variable as the path argument\\n\\n\\n2.1.0\\n\\ndefault to using STATIC_URL and STATIC_ROOT (Django 1.3), falling back\\nto MEDIA_URL and MEDIA_ROOT if the STATIC_* settings are not set\\nadd AWS_GZIP setting to optionally disable gzip compression in S3 client\\n\\nThanks to Rob Hudson and Dolan Antenucci for their contributions to this\\nrelease.\\n\\n2.0.0\\n\\nupdated Rackspace Cloud Files backend\\nuse gzip instead of deflate for compression (better browser support)\\nsmart gzip client support detection\\nadd pluggable backends\\nadd pluggable file processors\\nexperimental YUI Compressor\\nsettings refactor\\nallow override of settings.MEDIA_URL\\nImprovements to the logic that decides which files to sync. Safely ignore\\na wider variety of hidden files/directories.\\nMake template tags aware of whether the current page is SSL-secured. If it\\nis, ask the backend for an SSL media URL (if implemented by your backend).\\nmade SERVE_REMOTE setting the sole factor in determining if\\nmedia should be served locally or remotely\\nadd many more tests\\ndeprecate CSS_PATH and JS_PATH\\n\\nThanks to Greg Taylor, Peter Sanchez, Jonathan Drosdeck, Rich Leland,\\nand Rob Hudson for their contributions to this release.\\n\\n1.0.1\\n\\nadd application/javascript and application/x-javascript to JavaScript\\nmimetypes\\nbreak out of CSS and JS mimetypes\\nadd support for HTTPS URLs to S3\\nallow for storage of S3 keys in ~/.boto configuration file\\n\\nThanks to Rob Hudson and Peter Sanchez for their contributions.\\n\\n1.0.0\\nInitial release.\\n',\n",
       "  'watchers': '2',\n",
       "  'stars': '176',\n",
       "  'forks': '31',\n",
       "  'commits': '228'},\n",
       " {'language': 'Python 81.3',\n",
       "  'readme': 'django-mediasync\\nOne of the more significant development roadblocks we have relates to local vs.\\ndeployed media. Ideally all media (graphics, css, scripts) development would\\noccur locally and not use production media. Then, when ready to deploy, the\\nmedia should be pushed to production. That way there can be significant changes\\nto media without disturbing the production web site.\\nThe goal of mediasync is to develop locally and then flip a switch in production\\nthat makes all the media URLs point to remote media URLs instead of the local\\nmedia directory.\\nAll code is under a BSD-style license, see LICENSE for details.\\nSource: http://github.com/sunlightlabs/django-mediasync/\\n\\nRequirements\\n\\ndjango >= 1.0\\nboto >= 1.8d\\nslimmer == 0.1.30 (optional)\\npython-cloudfiles == 1.7.5 (optional, for Rackspace Cloud Files backend)\\n\\n\\nUpgrading from mediasync 1.x\\n\\nUpdate your mediasync settings as described in the next section.\\n\\nRun ./manage.py syncmedia --force to force updates of all files:\\n\\ngzip instead of deflate compression\\nsync both compressed and original versions of files\\n\\n\\n\\n\\nadd \"django.core.context_processors.request\" to TEMPLATE_CONTEXT_PROCESSORS\\n\\n\\nAn important note about Django 1.3\\nWhen DEBUG = True and the project is run with manage.py runserver, Django 1.3\\nautomatically adds django.views.static.serve to urlpatterns. While this feature\\nmakes local development easier for most people, it screws everything up if\\nyou\\'ve added mediasync.urls to urlpatterns. As of now, the only way I can find\\nto disable the automatic addition of django.views.static.serve is to use a full\\nURL for STATIC_URL instead of just a path:\\nSTATIC_URL = \"http://localhost:8000/static/\"\\n\\n\\nConfiguration\\n\\nsettings.py\\nAdd to INSTALLED_APPS:\\n\\'mediasync\\'\\n\\nAdd to TEMPLATE_CONTEXT_PROCESSORS:\\n\\'django.core.context_processors.request\\'\\n\\nMake sure your STATIC_ROOT setting is the correct path to your media:\\nSTATIC_ROOT = \\'/path/to/media\\'\\n\\nWhen media is being served locally (instead of from S3 or Cloud Files),\\nmediasync serves media through a Django view. Set your STATIC_URL to what\\nyou\\'d like that local media URL to be. This can be whatever you\\'d like, as long\\nas you\\'re using the {% media_url %} tag (more details on this later):\\nSTATIC_URL = \\'http://localhost:8000/devmedia/\\'\\n\\nSTATIC_URL is the URL that will be used in debug mode. Otherwise,\\nthe STATIC_URL will be loaded from the backend settings. Please see\\nAn important note about Django 1.3.\\nThe following settings dict must also be added:\\nMEDIASYNC = {\\n    \\'BACKEND\\': \\'path.to.backend\\',\\n}\\n\\nIf you want to use a different media URL than that specified\\nin settings.STATIC_URL, you can add STATIC_URL to the MEDIASYNC\\nsettings dict:\\nMEDIASYNC = {\\n    ...\\n    \\'STATIC_URL\\': \\'/url/to/media/\\', # becomes http://yourhost.com/url/to/media/\\n    ...\\n}\\n\\nSame goes for STATIC_ROOT:\\nMEDIASYNC = {\\n    ...\\n    \\'STATIC_ROOT\\': \\'/path/to/media/\\',\\n    ...\\n}\\n\\nmediasync supports pluggable backends. Please see below for information on\\nthe provided backends as well as directions on implementing your own.\\n\\nMedia expiration\\nIf the client supports media expiration, all files are set to expire 365 days\\nafter the file was synced. You may override this value by adding\\nEXPIRATION_DAYS to the MEDIASYNC settings dict.\\n# Expire in 10 years.\\nMEDIASYNC[\\'EXPIRATION_DAYS\\'] = 365 * 10\\n\\n\\nServing media remote (S3/Cloud Files) or locally\\nThe media URL is selected based on the SERVE_REMOTE attribute in the\\nMEDIASYNC dict in settings.py. When False, media will be served locally\\ninstead of from S3.\\n# This would force mediasync to serve all media through the value\\n# specified in settings.STATIC_URL.\\nMEDIASYNC[\\'SERVE_REMOTE\\'] = False\\n\\n# This would serve all media through S3/Cloud Files.\\nMEDIASYNC[\\'SERVE_REMOTE\\'] = True\\n\\n# This would serve media locally while in DEBUG mode, and remotely when\\n# in production (DEBUG == False).\\nMEDIASYNC[\\'SERVE_REMOTE\\'] = not DEBUG\\n\\nWhen serving files locally, you can emulate the CSS/JS combo/minifying\\nbehavior we get from using media processors by specifying the following.\\nMEDIASYNC[\\'SERVE_REMOTE\\'] = False\\nMEDIASYNC[\\'EMULATE_COMBO\\'] = True\\n\\nNote that this will only work if your STATIC_URL is pointing at your\\nDjango dev server. Also keep in mind that some processors may take a while,\\nand is best used to check things over before rolling out to production.\\n\\nDOCTYPE\\nlink and script tags are written using XHTML syntax. The rendering can be\\noverridden by using the DOCTYPE setting. Allowed values are \\'html4\\',\\n\\'html5\\', or \\'xhtml\\'. The default in mediasync 2.0 is html5, just as\\nthe DOCTYPE on your site should be.\\nMEDIASYNC[\\'DOCTYPE\\'] = \\'html5\\'\\n\\nFor each doctype, the following tags are rendered:\\n\\nhtml4\\n<link rel=\"stylesheet\" href=\"...\" type=\"text/css\" media=\"...\">\\n<script type=\"text/javascript\" charset=\"utf-8\" src=\"...\"></script>\\n\\n\\nhtml5\\n<link rel=\"stylesheet\" href=\"...\" media=\"...\">\\n<script src=\"...\"></script>\\n\\n\\nxhtml\\n<link rel=\"stylesheet\" href=\"...\" type=\"text/css\" media=\"...\" />\\n<script type=\"text/javascript\" charset=\"utf-8\" src=\"...\"></script>\\n\\n\\nSSL\\nmediasync will attempt to intelligently determine if your media should be\\nserved using HTTPS. In order to use automatic SSL detection,\\ndjango.core.context_processors.request must be added to\\nTEMPLATE_CONTEXT_PROCESSORS in settings.py:\\nTEMPLATE_CONTEXT_PROCESSORS = (\\n    ...\\n    \\'django.core.context_processors.request\\',\\n    ...\\n)\\n\\nThe USE_SSL mediasync setting can be used to override the SSL\\nURL detection.\\n# Force HTTPS.\\nMEDIASYNC[\\'USE_SSL\\'] = True\\n\\nor\\n# Force HTTP.\\nMEDIASYNC[\\'USE_SSL\\'] = False\\n\\nSome backends will be unable to use SSL. In these cases USE_SSL and SSL\\ndetection will be ignored.\\n\\nurls.py\\nTo serve local media through mediasync, add a reference to mediasync.urls in\\nyour main urls.py file.\\nurlpatterns = (\\'\\',\\n    ...\\n    url(r\\'^\\', include(\\'mediasync.urls)),\\n    ...\\n)\\n\\n\\nBackends\\nmediasync now supports pluggable backends. A backend is a Python module that\\ncontains a Client class that implements a mediasync-provided BaseClient class.\\n\\nS3\\nMEDIASYNC[\\'BACKEND\\'] = \\'mediasync.backends.s3\\'\\n\\n\\nSettings\\nThe following settings are required in the mediasync settings dict:\\nMEDIASYNC = {\\n    \\'AWS_KEY\\': \"s3_key\",\\n    \\'AWS_SECRET\\': \"s3_secret\",\\n    \\'AWS_BUCKET\\': \"bucket_name\",\\n}\\n\\nOptionally you may specify a path prefix:\\nMEDIASYNC[\\'AWS_PREFIX\\'] = \"key_prefix\"\\n\\nAssuming a correct DNS CNAME entry, setting AWS_BUCKET to\\nassets.sunlightlabs.com and AWS_PREFIX to myproject/media would\\nsync the media directory to http://assets.sunlightlabs.com/myproject/media/.\\nAmazon allows users to create DNS CNAME entries to map custom domain names\\nto an AWS bucket. MEDIASYNC can be configured to use the bucket as the media\\nURL by setting AWS_BUCKET_CNAME to True.\\nMEDIASYNC[\\'AWS_BUCKET_CNAME\\'] = True\\n\\nIf you would prefer to not use gzip compression with the S3 client, it can be\\ndisabled:\\nMEDIASYNC[\\'AWS_GZIP\\'] = False\\n\\n\\nTips\\nSince files are given a far future expires header, one needs a way to do\\n\"cache busting\" when you want the browser to fetch new files before the expire\\ndate arrives.  One of the best and easiest ways to accomplish this is to alter\\nthe path to the media files with some sort of version string using the key\\nprefix setting:\\nMEDIASYNC[\\'AWS_PREFIX\\'] = \"myproject/media/v20001201\"\\n\\nGiven that and the above DNS CNAME example, the media directory URL would end\\nup being http://assets.sunlightlabs.com/myproject/media/v20001201/.  Whenever\\nyou need to update the media files, simply update the key prefix with a new\\nversioned string.\\nA CACHE_BUSTER settings can be added to the main MEDIASYNC settings\\ndict to add a query string parameter to all media URLs. The cache buster can\\neither be a value or a callable which is passed the media URL as a parameter.\\nMEDIASYNC[\\'CACHE_BUSTER\\'] = 1234567890\\n\\nThe above setting will generate a media path similar to:\\nhttp://yourhost.com/url/to/media/image.png?1234567890\\n\\nAn important thing to note is that if you\\'re running your Django site in a\\nmulti-threaded or multi-node setup, you\\'ll want to be careful about using a\\ntime-based cache buster value. Each worker/thread will probably have a slightly\\ndifferent value for datetime.now(), which means your users will find themselves\\nhaving cache misses randomly from page to page.\\n\\nRackspace Cloud Files\\nMEDIASYNC[\\'BACKEND\\'] = \\'mediasync.backends.cloudfiles\\'\\n\\n\\nSettings\\nThe following settings are required in the mediasync settings dict:\\nMEDIASYNC = {\\n    \\'CLOUDFILES_CONTAINER\\': \\'container_name\\',\\n    \\'CLOUDFILES_USERNAME\\': \\'cf_username\\',\\n    \\'CLOUDFILES_API_KEY\\': \\'cf_apikey\\',\\n}\\n\\n\\nTips\\nThe Cloud Files backend lacks support for the following features:\\n\\nsetting HTTP Expires header\\nsetting HTTP Cache-Control header\\ncontent compression (gzip)\\nSSL support\\nconditional sync based on file checksum\\n\\n\\nCustom backends\\nYou can create a custom backend by creating a Python module containing a Client\\nclass. This class must inherit from mediasync.backends.BaseClient. Additionally,\\nyou must implement two methods:\\ndef remote_media_url(self, with_ssl):\\n    ...\\n\\nremote_media_url returns the full base URL for remote media. This can be\\neither a static URL or one generated from mediasync settings:\\ndef put(self, filedata, content_type, remote_path, force=False):\\n    ...\\n\\nput is responsible for pushing a file to the backend storage.\\n\\nfiledata - the contents of the file\\ncontent_type - the mime type of the file\\nremote_path - the remote path (relative from remote_media_url) to which\\nthe file should be written\\nforce - if True, write file to remote storage even if it already exists\\n\\nIf the client supports gzipped content, you will need to override supports_gzip\\nto return True:\\ndef supports_gzip(self):\\n        return True\\n\\n\\nFile Processors\\nFile processors allow you to modify the content of a file as it is being\\nsynced or served statically.\\nMediasync ships with three processor modules:\\n\\nslim is a minifier written in Python and requires the\\nslimmer Python package. The Python package can be found here:\\nhttp://pypi.python.org/pypi/slimmer/\\nyuicompressor is a minifier written in Java and can be downloaded\\nfrom YUI\\'s download page: http://developer.yahoo.com/yui/compressor/.\\nThis processor also requires an additional setting, as defined below.\\nyuicompressor is new and should be considered experimental until\\nthe mediasync 2.1 release.\\nclosurecompiler is a javascript compiler provided by Google.\\n\\nCustom processors can be specified using the PROCESSORS entry in the\\nmediasync settings dict. PROCESSORS should be a list of processor entries.\\nEach processor entry can be a callable or a string path to a callable. If the\\npath is to a class definition, the class will be instantiated into an object.\\nThe processor callable should return a string of the processed file data, None\\nif it chooses to not process the file, or raise mediasync.SyncException if\\nsomething goes terribly wrong. The callable should take the following arguments:\\ndef proc(filedata, content_type, remote_path, is_active):\\n        ...\\n\\n\\nfiledata\\nthe content of the file as a string\\ncontent_type\\nthe mimetype of the file being processed\\nremote_path\\nthe path to which the file is being synced (contains the file name)\\nis_active\\nTrue if the processor should... process\\n\\nIf the PROCESSORS setting is used, you will need to include the defaults\\nif you plan on using them:\\n\\'PROCESSORS\\': (\\n    \\'mediasync.processors.slim.css_minifier\\',\\n    \\'mediasync.processors.slim.js_minifier\\',\\n        ...\\n),\\n\\nmediasync will attempt to use slimmer by default if you have the package\\ninstalled and do not use the PROCESSORS setting.\\n\\nGoogle Closure Compiler\\nGoogle\\'s JavaScript Closure Compiler provides an API that allows files to be\\ncompressed without installing anything locally. To use the service:\\n\\'PROCESSORS\\': (\\'mediasync.processors.closurecompiler.compile\\',)\\n\\n\\nYUI Compressor\\nTo configure YUI Compressor you need to define a PROCESSORS and\\nYUI_COMPRESSOR_PATH as follows, assuming you placed the \".jar\" file in\\nyour ~/bin path:\\n\\'PROCESSORS\\': (\\'mediasync.processors.yuicompressor.css_minifier\\',\\n               \\'mediasync.processors.yuicompressor.js_minifier\\'),\\n\\'YUI_COMPRESSOR_PATH\\': \\'~/bin/yuicompressor.jar\\',\\n\\n\\nFeatures\\n\\nIgnored Directories\\nAny directory in STATIC_ROOT that is hidden or starts with an underscore\\nwill be ignored during syncing.\\n\\nTemplate Tags\\nWhen referring to media in HTML templates you can use custom template tags.\\nThese tags can by accessed by loading the media template tag collection.\\n{% load media %}\\n\\nAny tag that has a path argument can use either a string or a variable:\\n{% media_url \"images/avatar.png\" }\\n{% media_url user.profile.avatar_path %}\\n\\nSome backends (S3) support https URLs when the requesting page is secure.\\nIn order for the https to be detected, the request must be placed in the\\ntemplate context with the key \\'request\\'. This can be done automatically by\\nadding \\'django.core.context_processors.request\\' to TEMPLATE_CONTEXT_PROCESSORS\\nin settings.py\\n\\nmedia_url\\nRenders the STATIC_URL from settings.py with trailing slashes removed.\\n<img src=\"{% media_url %}/images/stuff.png\">\\n\\nSTATIC_URL takes an optional argument that is the media path. Using the argument\\nallows mediasync to add the CACHE_BUSTER to the URL if one is specified.\\n<img src=\"{% media_url \\'/images/stuff.png\\' %}\">\\n\\nIf CACHE_BUSTER is set to 12345, the above example will render as:\\n<img src=\"http://assets.example.com/path/to/media/images/stuff.png?12345\">\\n\\nNOTE: Don\\'t use this tag to serve CSS or JS files. Use the js and css tags\\nthat were specifically designed for the purpose.\\n\\njs\\nRenders a script tag with the correct include.\\n{% js \"myfile.js\" %}\\n\\n\\ncss\\nRenders a <link> tag to include the stylesheet. It takes an optional second\\nparameter for the media attribute; the default media is \"screen, projector\".\\n{% css \"myfile.css\" %}\\n{% css \"myfile.css\" \"screen\" %}\\n\\n\\ncss_print\\nShortcut to render as a print stylesheet.\\n{% css_print \"myfile.css\" %}\\n\\nwhich is equivalent to\\n{% css \"myfile.css\" \"print\" %}\\n\\n\\nWriting Style Sheets\\nUsers are encouraged to write stylesheets using relative URLS. The media\\ndirectory is synced with S3 as is, so relative local paths will still work\\nwhen pushed remotely.\\nbackground: url(../images/arrow_left.png);\\n\\n\\nJoined files\\nWhen serving media in production, it is beneficial to combine JavaScript and\\nCSS into single files. This reduces the number of connections the browser needs\\nto make to the web server. Fewer connections can dramatically decrease page\\nload times and reduce the server-side load.\\nJoined files are specified in the MEDIASYNC dict using JOINED. This is\\na dict that maps individual media to an alias for the joined files.\\n\\'JOINED\\': {\\n    \\'styles/joined.css\\': [\\'styles/reset.css\\',\\'styles/text.css\\'],\\n    \\'scripts/joined.js\\': [\\'scripts/jquery.js\\',\\'scripts/processing.js\\'],\\n},\\n\\nFiles listed in JOINED will be combined and pushed to S3 with the name of\\nthe alias. The individual CSS files will also be pushed to S3. Aliases must end\\nin either .css or .js in order for the content-type to be set appropriately.\\nThe existing template tags may be used to refer to the joined media. Simply use\\nthe joined alias as the argument:\\n{% css_print \"joined.css\" %}\\n\\nWhen served locally, template tags will render an HTML tag for each of the files\\nthat make up the joined file:\\n<link rel=\"stylesheet\" href=\"/media/styles/reset.css\" type=\"text/css\" media=\"screen, projection\" />\\n<link rel=\"stylesheet\" href=\"/media/styles/text.css\" type=\"text/css\" media=\"screen, projection\" />\\n\\nWhen served remotely, one HTML tag will be rendered with the name of the joined file:\\n<link rel=\"stylesheet\" href=\"http://bucket.s3.amazonaws.com/styles/joined.css\" type=\"text/css\" media=\"screen, projection\" />\\n\\n\\nSmart GZIP for S3\\nIn previous versions of mediasync\\'s S3 client, certain content was always pushed\\nin a compressed format. This can cause major issues with clients that do not\\nsupport gzip. New in version 2.0, mediasync will push both a gzipped and an\\nuncompressed version of the file to S3. The template tags look at the request\\nand direct the user to the appropriate file based on the ACCEPT_ENCODING HTTP\\nheader. Assuming a file styles/layout.css, the following would be synced to S3:\\nstyles/layout.css\\nstyles/layout.css.gzt\\n\\nNote the altered use of the .gz extension. Some versions of the Safari browser\\non OSX ignore the Content-Type header for files ending in .gz and treat them\\ninstead as files to download. This altered extension allows Safari to deflate\\nand utilize the files correctly without affecting functionality in any other\\ntested browsers.\\n\\nSignals\\nmediasync provides two signals that allow you to hook into the syncing\\nprocess. pre_sync is sent after the client is opened, but before the first\\nfile is synced. post_sync is sent after the last file is synced, but before\\nthe client is closed. This allows you to call commands on the client without\\nhaving to worry about its state. The signals allow you to do common tasks such\\nas calling Django 1.3\\'s collectstatic command, process SASS stylesheets, or\\nclean up files generated during a pre_sync process.\\n\\ncollectstatic receiver\\nA receiver for calling the collectstatic management command is provided:\\nfrom mediasync.signals import pre_sync, collectstatic_receiver\\n\\n# run collectstatic before syncing media\\npre_sync.connect(collectstatic_receiver)\\n\\n\\nSASS receiver\\nA receiver for compiling SASS into CSS is provided:\\nfrom mediasync.signals import pre_sync, sass_receiver\\n\\n# compile SASS files before syncing media\\npre_sync.connect(sass_receiver)\\n\\nAny file in static root that has the sass or scss file extension will be\\ncompiled into CSS. The compiled CSS file will be placed in the same directory\\nand the original extension will be replaced with css. If a file exists with\\nthe same css extension, it will be overwritten.\\nBy default mediasync uses the sass command with no options. If you would\\nlike to specify your own command, specify SASS_COMMAND in settings:\\nMEDIASYNC = {\\n    ...\\n    \\'SASS_COMMAND\\': \\'sass -scss -l\\',\\n    ...\\n}\\n\\n\\nRunning MEDIASYNC\\n./manage.py syncmedia\\n\\n\\nChange Log\\n\\n2.2.0\\n\\nadded pre_sync and post_sync signals\\nprovide basic receiver for calling collectstatic before syncing\\nprovide receiver for compiling SASS before syncing\\nshow media directory listing when serving locally in debug mode\\nadd processor for Google\\'s Closure Compiler API for JavaScript\\ntemplate tags can now take a variable as the path argument\\n\\n\\n2.1.0\\n\\ndefault to using STATIC_URL and STATIC_ROOT (Django 1.3), falling back\\nto MEDIA_URL and MEDIA_ROOT if the STATIC_* settings are not set\\nadd AWS_GZIP setting to optionally disable gzip compression in S3 client\\n\\nThanks to Rob Hudson and Dolan Antenucci for their contributions to this\\nrelease.\\n\\n2.0.0\\n\\nupdated Rackspace Cloud Files backend\\nuse gzip instead of deflate for compression (better browser support)\\nsmart gzip client support detection\\nadd pluggable backends\\nadd pluggable file processors\\nexperimental YUI Compressor\\nsettings refactor\\nallow override of settings.MEDIA_URL\\nImprovements to the logic that decides which files to sync. Safely ignore\\na wider variety of hidden files/directories.\\nMake template tags aware of whether the current page is SSL-secured. If it\\nis, ask the backend for an SSL media URL (if implemented by your backend).\\nmade SERVE_REMOTE setting the sole factor in determining if\\nmedia should be served locally or remotely\\nadd many more tests\\ndeprecate CSS_PATH and JS_PATH\\n\\nThanks to Greg Taylor, Peter Sanchez, Jonathan Drosdeck, Rich Leland,\\nand Rob Hudson for their contributions to this release.\\n\\n1.0.1\\n\\nadd application/javascript and application/x-javascript to JavaScript\\nmimetypes\\nbreak out of CSS and JS mimetypes\\nadd support for HTTPS URLs to S3\\nallow for storage of S3 keys in ~/.boto configuration file\\n\\nThanks to Rob Hudson and Peter Sanchez for their contributions.\\n\\n1.0.0\\nInitial release.\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '175',\n",
       "  'forks': '11',\n",
       "  'commits': '692'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'sublime-laravelgenerator\\nA Sublime Text plugin that allows you to make use of the Laravel 4\\nGenerators by Jeffrey\\nWay directly within Sublime Text.\\nInstallation\\n\\nInstall the  Laravel 4\\ngenerator commands through Composer.\\nInstall the ST plugin through Package Control: Sublime Laravel Generator\\nIf you are on Windows or php executable is not in PATH, please specify the path to it in laravelgenerator.sublime-settings. To do so, copy laravelgenerator.sublime-settings from this\\nplugin to <Packages_Directory>/Users/ and make the edits to that file.\\n\\nUsage\\n\\nOpen a Laravel Project\\nOpen the command palette (Ctrl+Shift+P)\\nExecute any of the available Generate commands\\nSee here for a basic workflow video\\n\\nNote: artisan needs to be in the project root.\\nCustomization\\nThe plugin is quite extensible. Interested users can extend the plugin for more\\nartisan commands by adding the appropriate entries in\\nDefault.sublime-commands.\\nCredits\\n\\nJeffrey Way: for the idea and testing this\\nplugin throughout the development.\\n\\n\\nThis is a work in progress. Feedback is appreciated. Feel free to report any\\nissues you come across\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '167',\n",
       "  'forks': '44',\n",
       "  'commits': '31'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Dagny\\nDagny is a Django adaptation of Ruby on Rails’s Resource-Oriented\\nArchitecture (a.k.a. ‘RESTful Rails’).\\nDagny makes it really easy to build resourceful web applications.\\nYou can read the full documentation here.\\nAt present, this project is in an experimental phase, so APIs are very liable to\\nchange. You have been warned.\\nP.S.: the name is a reference.\\nMotivation\\nRails makes building RESTful web applications incredibly easy, because\\nresource-orientation is baked into the framework—it’s actually harder to make\\nyour app unRESTful.\\nI wanted to build a similar system for Django; one that made it incredibly\\nsimple to model my resources and serve them up with the minimum possible code.\\nOne of the most important requirements was powerful yet simple content\\nnegotiation: separating application logic from the rendering of responses makes\\nwriting an API an effortless task.\\nFinally, as strong as Rails’s inspiration was, it still needed to be consistent\\nwith the practices and idioms of the Django and Python ecosystems. Dagny doesn’t\\nuse any metaclasses (yet), and the code is well-documented and readable by most\\nPythonista’s standards.\\nAppetizer\\nDefine a resource:\\nfrom dagny import Resource, action\\nfrom django.shortcuts import get_object_or_404, redirect\\nfrom polls import forms, models\\n\\nclass Poll(Resource):\\n    \\n    @action\\n    def index(self):\\n        self.polls = models.Poll.objects.all()\\n    \\n    @action\\n    def new(self):\\n        self.form = forms.PollForm()\\n    \\n    @action\\n    def create(self):\\n        self.form = forms.PollForm(self.request.POST)\\n        if self.form.is_valid():\\n            self.poll = self.form.save()\\n            return redirect(\"Poll#show\", self.poll.id)\\n        \\n        return self.new.render()\\n    \\n    @action\\n    def edit(self, poll_id):\\n        self.poll = get_object_or_404(models.Poll, id=int(poll_id))\\n        self.form = forms.PollForm(instance=self.poll)\\n    \\n    @action\\n    def update(self, poll_id):\\n        self.poll = get_object_or_404(models.Poll, id=int(poll_id))\\n        self.form = forms.PollForm(self.request.POST, instance=self.poll)\\n        if self.form.is_valid():\\n            self.form.save()\\n            return redirect(\"Poll#show\", self.poll.id)\\n        \\n        return self.edit.render()\\n    \\n    @action\\n    def destroy(self, poll_id):\\n        self.poll = get_object_or_404(models.Poll, id=int(poll_id))\\n        self.poll.delete()\\n        return redirect(\"Poll#index\")\\n\\nCreate the templates:\\n<!-- polls/index.html -->\\n<ol>\\n  {% for poll in self.polls %}\\n    <li><a href=\"{% url Poll#show poll.id %}\">{{ poll.name }}</a></li>\\n  {% endfor %}\\n</ol>\\n<p><a href=\"{% url Poll#new %}\">Create a poll</a></p>\\n\\n<!-- polls/new.html -->\\n<form method=\"post\" action=\"{% url Poll#create %}\">\\n  {% csrf_token %}\\n  {{ self.form.as_p }}\\n  <input type=\"submit\" value=\"Create Poll\" />\\n</form>\\n\\n<!-- polls/show.html -->\\n<p>Name: {{ self.poll.name }}</p>\\n<p><a href=\"{% url Poll#edit self.poll.id %}\">Edit this poll</a></p>\\n\\n<!-- polls/edit.html -->\\n<form method=\"post\" action=\"{% url Poll#update self.poll.id %}\">\\n  {% csrf_token %}\\n  {{ self.form.as_p }}\\n  <input type=\"submit\" value=\"Update poll\" />\\n</form>\\n\\nSet up the URLs:\\nfrom django.conf.urls.defaults import *\\nfrom dagny.urls import resources\\n\\nurlpatterns = patterns(\\'\\',\\n    (r\\'^polls/\\', resources(\\'polls.resources.Poll\\', name=\\'Poll\\')),\\n)\\n\\nDone.\\nExample Project\\nThere’s a more comprehensive example project which showcases a user\\nmanagement app, built in very few lines of code on top of the standard\\ndjango.contrib.auth app.\\nTo get it running:\\ngit clone \\'git://github.com/zacharyvoase/dagny.git\\'\\ncd dagny/\\npip install -r REQUIREMENTS  # Installs runtime requirements\\npip install -r REQUIREMENTS.test  # Installs testing requirements\\ncd example/\\n./manage.py syncdb  # Creates db/development.sqlite3\\n./manage.py test users  # Runs all the tests\\n./manage.py runserver\\n\\nThen just visit http://localhost:8000/users/ to see it in action!\\nLicense\\nThis is free and unencumbered software released into the public domain.\\nAnyone is free to copy, modify, publish, use, compile, sell, or distribute this\\nsoftware, either in source code form or as a compiled binary, for any purpose,\\ncommercial or non-commercial, and by any means.\\nIn jurisdictions that recognize copyright laws, the author or authors of this\\nsoftware dedicate any and all copyright interest in the software to the public\\ndomain. We make this dedication for the benefit of the public at large and to\\nthe detriment of our heirs and successors. We intend this dedication to be an\\novert act of relinquishment in perpetuity of all present and future rights to\\nthis software under copyright law.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE\\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\\nCONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nFor more information, please refer to http://unlicense.org/\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '157',\n",
       "  'forks': '10',\n",
       "  'commits': '126'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'logstash_formatter: JSON logs for logstash\\nThis library is provided to allow standard python logging to output log data\\nas json objects ready to be shipped out to logstash.\\nThis project has been originally open sourced by exoscale (which is a great hosting service btw),\\nthanks to them.\\nInstalling\\nUsing pip (PyPI):\\npip install logstash_formatter\\n\\nManual:\\ngit clone https://github.com/ulule/python-logstash-formatter.git\\ncd python-logstash-formatter\\npython setup.py install\\n\\nUsage\\nJson outputs are provided by the LogstashFormatter logging formatter.\\nimport logging, sys\\nfrom logstash_formatter import LogstashFormatterV1\\n\\n# configure logging\\nhandler = logging.StreamHandler(stream=sys.stdout)\\nhandler.setFormatter(LogstashFormatterV1())\\nlogging.basicConfig(handlers=[handler], level=logging.INFO)\\n\\n# use it\\nlogging.info(\"my log\")\\nThe LogstashFormatter may take the following named parameters:\\n\\nfmt: Config as a JSON string that supports:\\n\\nextra: provide extra fields always present in logs.\\nsource_host: override source host name.\\n\\n\\njson_cls: JSON encoder to forward to json.dump.\\njson_default: Default JSON representation for unknown types,\\nby default coerce everything to a string.\\n\\nLogstashFormatterV1 adheres to the more 1.2.0 schema and will not update\\nfields, apart from a special handling of msg which will be updated to\\nmessage when applicable.\\nYou can also add extra fields to your json output by specifying a dict in place of message, or by specifying\\nthe named argument extra as a dictionary. When supplying the exc_info named argument with a truthy value,\\nand if an exception is found on the stack, its traceback will be attached to the payload as well.\\nlogger.info({\"account\": 123, \"ip\": \"172.20.19.18\"})\\nlogger.info(\"classic message for account: {account}\", extra={\"account\": account})\\n\\ntry:\\n    h = {}\\n    h[\\'key\\']\\nexcept:\\n    logger.info(\"something unexpected happened\", exc_info=True)\\nSample output for LogstashFormatter\\nThe following keys will be found in the output JSON:\\n\\n@source_host: source hostname for the log\\n@timestamp: ISO 8601 timestamp\\n@message: short message for this log\\n@fields: all extra fields\\n\\n{\\n    \"@fields\": {\\n        \"account\": \"pyr\",\\n        \"args\": [],\\n        \"created\": 1367480388.013037,\\n        \"exception\": [\\n            \"Traceback (most recent call last):\\\\n\",\\n            \"  File \\\\\"test.py\\\\\", line 16, in <module>\\\\n    k[\\'unknown\\']\\\\n\",\\n            \"KeyError: \\'unknown\\'\\\\n\"\\n        ],\\n        \"filename\": \"test.py\",\\n        \"funcName\": \"<module>\",\\n        \"levelname\": \"WARNING\",\\n        \"levelno\": 30,\\n        \"lineno\": 18,\\n        \"module\": \"test\",\\n        \"msecs\": 13.036966323852539,\\n        \"name\": \"root\",\\n        \"pathname\": \"test.py\",\\n        \"process\": 1819,\\n        \"processName\": \"MainProcess\",\\n        \"relativeCreated\": 18.002986907958984,\\n        \"thread\": 140060726359808,\\n        \"threadName\": \"MainThread\"\\n    },\\n    \"@message\": \"TEST\",\\n    \"@source_host\": \"phoenix.spootnik.org\",\\n    \"@timestamp\": \"2013-05-02T09:39:48.013158\"\\n}\\nSample output for LogstashFormatterV1\\nThe following keys will be found in the output JSON:\\n\\n@timestamp: ISO 8601 timestamp\\n@version: Version of the schema\\n\\n{\"@version\": 1,\\n \"account\": \"pyr\",\\n \"lineno\": 1,\\n \"levelno\": 30,\\n \"filename\": \"test.py\",\\n \"thread\": 140566036444928,\\n \"@timestamp\": \"2015-03-30T09:46:23.000Z\",\\n \"threadName\": \"MainThread\",\\n \"relativeCreated\": 51079.52117919922,\\n \"process\": 10787,\\n \"source_host\": \"phoenix.spootnik.org\",\\n \"processName\": \"MainProcess\",\\n \"pathname\": \"test.py\",\\n \"args\": [],\\n \"module\": \"test\",\\n \"msecs\": 999.9005794525146,\\n \"created\": 1427708782.9999006,\\n \"name\": \"root\",\\n \"stack_info\": null,\\n \"funcName\": \"<module>\",\\n \"levelname\": \"WARNING\",\\n \"message\": \"foo\"}\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '157',\n",
       "  'forks': '62',\n",
       "  'commits': '77'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"A simple django middleware that logs request times using the Django 1.3 logging support, and a management command to analyze the resulting data. Once installed and configured you can run a command line these:\\npython manage.py analyze_timelog\\npython manage.py analyze_timelog --noreverse\\nAnd generate useful tabular data like this:\\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\\n| view                     | method | status | count | minimum | maximum | mean  | stdev           |\\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\\n| boxes.viewsBoxDetailView | GET    | 200    | 9430  | 0.14    | 0.28    | 0.21  | 0.0700037118541 |\\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\\n| boxes.viewsBoxListView   | GET    | 200    | 66010 | 0.17    | 0.28    | 0.232 | 0.0455415351076 |\\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\\n| django.views.staticserve | GET    | 200    | 61295 | 0.00    | 0.02    | 0.007 | 0.0060574669888 |\\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\\n\\nThis project was heavily influenced by the Rails Request log analyzer.\\nInstallation\\npip install django-timelog\\nOnce installed you need to do a little configuration to get things working. First add the middleware to your MIDDLEWARE_CLASSES in your settings file.\\nMIDDLEWARE_CLASSES = (\\n  'timelog.middleware.TimeLogMiddleware',\\nNext add timelog to your INSTALLED_APPS list. This is purely for the management command discovery.\\nINSTALLED_APPS = (\\n  'timelog',\\nThen configure the logger you want to use. This really depends on what you want to do, the django 1.3 logging setup is pretty powerful. Here’s how I’ve got logging setup as an example:\\nTIMELOG_LOG = '/path/to/logs/timelog.log'\\n\\nLOGGING = {\\n  'version': 1,\\n  'formatters': {\\n    'plain': {\\n      'format': '%(asctime)s %(message)s'},\\n    },\\n  'handlers': {\\n    'timelog': {\\n      'level': 'DEBUG',\\n      'class': 'logging.handlers.RotatingFileHandler',\\n      'filename': TIMELOG_LOG,\\n      'maxBytes': 1024 * 1024 * 5,  # 5 MB\\n      'backupCount': 5,\\n      'formatter': 'plain',\\n    },\\n  },\\n  'loggers': {\\n    'timelog.middleware': {\\n      'handlers': ['timelog'],\\n      'level': 'DEBUG',\\n      'propogate': False,\\n     }\\n  }\\n}\\nLastly, if you have particular URIs you wish to ignore you can define them using basic regular expressions in the TIMELOG_IGNORE_URIS list in settings.py:\\nTIMELOG_IGNORE_URIS = (\\n    '^/admin/',         # Ignores all URIs beginning with '/admin/'\\n    '^/other_page/$',   # Ignores the URI '/other_page/' only, but not '/other_page/more/'.\\n    '.jpg$',            # Ignores all URIs ending in .jpg\\n)\",\n",
       "  'watchers': '7',\n",
       "  'stars': '154',\n",
       "  'forks': '20',\n",
       "  'commits': '25'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Hooked\\nHow to use\\nHooked is a python library for managing git hooks. It adds a plugin system to the git hooks which allow us to write simple python scripts to run for the different phases in git.\\nThere are 3 different actions that can be done with hooked.py.\\nTo demonstrate lets create a test git directory:\\n$ mkdir /tmp/testgit\\n$ cd /tmp/testgit\\n$ git init\\n\\nNone of the hooks are enabled in a normal git directory:\\n$ ls .git/hooks/\\napplypatch-msg.sample  pre-applypatch.sample      pre-rebase.sample\\ncommit-msg.sample      pre-commit.sample      update.sample\\npost-update.sample     prepare-commit-msg.sample\\n\\nIn the hooked directory, we initialize the hooked system for that git repository.\\n$ python hooked.py --git-root=/tmp/testgit\\n\\nNow, there is a few additions to the hooks directory:\\n$ ls .git/hooks/\\naction             post-update.sample     prepare-commit-msg\\napplypatch-msg.sample  pre-applypatch.sample  prepare-commit-msg.sample\\ncommit-msg         pre-commit         pre-rebase.sample\\ncommit-msg.sample      pre-commit.sample      update.sample\\n\\nAt time of writing, hooked.py creates prepare-commit-msg, commit-msg and pre-commit.\\nIt also creates the action directory.\\n$ ls .git/hooks/action/\\nconfig.json  __init__.py  other_hook.py  test_hook.py\\n\\nBy default, it includes some example hooks but they are disabled. The config.json is a simple json configuration which lists which hooks are installed. By default, none are installed.\\n$ cat .git/hooks/action/config.json \\n{ \"hooks\": [] }\\n\\nLets install some hooks, so hooks are stored outside the hooked repository as they are on a per user basis. lets create a git_hooks directory:\\n$ mkdir /tmp/git_hooks\\n$ cd /tmp/git_hooks\\n\\nAnd lets make a python hook to demonstrate:\\n$ cat - > new_hook.py\\ndef precommit(git_state):\\n    for fname in git_state[\"files\"]:\\n        if \"action\" in fname:\\n            print \"ARGGGGGHHHHHH\"\\n            return False\\n    return True\\n\\nThis simply shouts if the word action is in the filename. Lets inject this into our git repo hooks. we can inject a directory or just a file. If you use a directory it will pull out all python files from the directory and all hooks that are copied will be turned on by default.\\n$ python hooked.py --git-root=/tmp/testgit --inject=/tmp/git_hooks/new_hook.py\\n\\nNow lets look at the action folder and the contents of config.json:\\n$ ls .git/hooks/action/\\nconfig.json  __init__.py  new_hook.py  other_hook.py  test_hook.py\\n$ cat .git/hooks/action/config.json \\n{\"hooks\": [\"new_hook\"]}\\n\\nYour hooks have now been installed. If in the future you want to completely remove the usage of hooked.py you can do the following:\\n$ python hooked.py --git-root=/tmp/testgit --clean\\n\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '152',\n",
       "  'forks': '4',\n",
       "  'commits': '19'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"PyUnit provides a basic set of assertions which can get you started with unit testing python, but it’s always useful to have more. Django also has a few specific requirements and common patterns when it comes to testing. This set of classes aims to provide a useful starting point for both these situations.\\nThe application also overrides the default Django test runner, adding a few useful features:\\nInstallation\\nJust add the project to your INSTALLED_APPS.\\nINSTALLED_APPS = (\\n\\t'test_extensions',\\n)\\nNote that this application steals the test command from django, overriding it with extra toys. If another application in your INSTALLED_APPS does this too then the last one in the list will win. South migrations does this in order to use the django syncdb command for testing, which test_extensions does too. As of 0.4 test_extensions works with South, just as long as you include it after south in the list of installed apps.\\nAssertions\\nSee the examples directory in the src/test_extensions directory for details of a large number of useful assertions for testing django apps:\\n\\nassert_response_contains\\nassert_response_doesnt_contain\\nassert_regex_contains\\nassert_render_matches\\nassert_code\\nassert_render\\nassert_render_matches\\nassert_doesnt_render\\nassert_render_contains\\nassert_render_doesnt_contain\\n\\nTest Runners\\nXMLUnit\\nSometimes it’s nice to have a file reporting the results of a test run. Some applications such as CruiseControl can use this to display the results in a user interface.\\npython manage.py test --xml\\nCode Coverage\\nIf you want to know what code is being run when you run your test suite then codecoverage is for you. These two flags use two different third party libraries to calculate coverage statistics. The first dumps the results to stdout, —xmlcoverage creates a cobertura-compatible xml output, and the last one creates a series of files displaying the results.\\npython manage.py test --coverage\\npython manage.py test --xmlcoverage\\npython manage.py test --figleaf\\nNo Database\\nSometimes your don’t want the overhead of setting up a database during testing, probably because your application just doesn’t use it.\\npython manage.py test --nodb\\npython manage.py test --nodb --coverage\\npython manage.py test --nodb --xmlcoverage\\nWARNING Don’t use this if you use the ORM in your app. An outstanding issue means that you can get into trouble. Your tests will still hit the database, but it will be your non test data.\\nLocal Continuous Integration Command\\nThanks to Roberto Aguilar (http://github.com/rca) for providing a auto-reloading version of the test runner. Run the runtester command and it should run your test suite whenever you change a file (similar to how runserver reloads the server each time you change something.)\\nSee this thread\\n from the Django Developer list of more information and discussion.\\npython manage.py runtester\\nLicence\\nXMLUnit is included out of convenience. It was written by Marc-Elian Begin <Marc-Elian.Begin@cern.ch> and is Copyright © Members of the EGEE Collaboration. 2004. http://www.eu-egee.org\\nThe rest of the code is licensed under an MIT license:\\nCopyright © 2008 Gareth Rushgrove <gareth@morethanseven.net>\\nPermission is hereby granted, free of charge, to any person\\nobtaining a copy of this software and associated documentation\\nfiles (the “Software”), to deal in the Software without\\nrestriction, including without limitation the rights to use,\\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the\\nSoftware is furnished to do so, subject to the following\\nconditions:\\nThe above copyright notice and this permission notice shall be\\nincluded in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\\nOTHER DEALINGS IN THE SOFTWARE.\",\n",
       "  'watchers': '2',\n",
       "  'stars': '145',\n",
       "  'forks': '28',\n",
       "  'commits': '122'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Hookah\\nThe HTTP event engine\\nCurrent Features\\n\\nInterface is 100% HTTP (use easily from any language)\\nDispatches POST requests (webhooks) asynchronously with retry\\nProvides publish/subscribe interface using PubSubHubbub protocol (experimental)\\nProvides \"Twitter Stream API\"-style long-polling interface for topics (super experimental)\\n\\nAbout\\nHookah was originally created to ease the implementation of webhooks in your web systems. While webhooks are still at the core, it\\'s becoming a scalable HTTP event engine with HTTP pubsub and long-polling event streaming. And of course, webhooks. Any system with webhooks or looking to implement webhooks will benefit from Hookah.\\nRequirements\\nHookah currently depends on Twisted.\\nUsage\\nHookah is a simple, lightweight standalone web server that you run locally alongside your existing web stack. Starting it from the command line is simple:\\n    twistd hookah --port 8080\\n\\nUsing the Dispatcher\\nPosting to /dispatch with a _url POST parameter will queue that POST request for that URL and return immediately. This allows you to use Hookah as an outgoing request queue that handles retries, etc. Using HTTP means you can do this easily from any language using a familiar API.\\nPosting to /dispatch with a _topic POST parameter will broadcast that post to any callbacks subscribed to that topic (see following PubSub section), or any stream consumers with a long-running request on that topic.\\nUsing PubSub\\nRefer to the PubSubHubbub spec, as Hookah is currently quite compliant with this excellent protocol. The hub endpoint is at /hub, but this multiplexes (based on \\'hub.mode\\' param) between /publish for publish pings, and /subscribe for subscription requests.\\nThis feature is still very early and as a result it is incomplete. The main caveat is that there is no permanent storage of subscription data or of the queues. This means if you were to restart Hookah, all subscriptions would have to be made again.\\nUsing Streams\\nHookah implements a long-running stream API, modeled after Twitter\\'s Stream API. Just do a GET request to /stream with a topic parameter, and you\\'ll get a persistent, chunked HTTP connection that will send you messages published to that topic as they come in. Refer to the Twitter Stream API docs to get a better feel for this pragmatic Comet streaming technique.\\nTodo\\n\\nPersistent storage (SQLite, MySQL, CouchDB) and queuing (in memory, Kestrel, RabbitMQ) backends\\nConfiguration\\nBacklog/history with resend\\n\"Errback\" webhook\\nAsync response handling\\n\\nLicense\\nHookah is released under the MIT license, which can be found in the LICENSE file.\\nContributors\\n\\nyou?\\n\\nAuthor\\nJeff Lindsay progrium@gmail.com\\nLearn more about web hooks\\nhttp://webhooks.org\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '145',\n",
       "  'forks': '8',\n",
       "  'commits': '50'},\n",
       " {'language': 'Python 51.5',\n",
       "  'readme': '\\ncensus.ire.org\\nA nationwide census browser for 2000 and 2010 census data.\\n\\nDependencies\\nYou will need Python 2.7, the PostGIS stack, virtualenv and virtualenvwrapper. Mac Installation instructions at: http://blog.apps.chicagotribune.com/2010/02/17/quick-install-pythonpostgis-geo-stack-on-snow-leopard/):\\nOther required software:\\n\\nmongodb\\nwget\\nmdbtools.\\n\\nOn a Mac you can get these with Brew:\\nbrew install mongodb\\nbrew install wget\\nbrew install mdbtools\\n\\n\\nBootstrapping the webapp\\nTo get the web application running:\\ncd censusweb\\nmkvirtualenv --no-site-packages censusweb\\npip install -r requirements.txt\\npython manage.py runserver\\n\\n\\nConfiguring the webapp\\nBy default the webapp is going to use the data published to the IRE test site, which may not be accessible to you. To use your own data open censusweb/config/settings.py and modify the following line:\\nAPI_URL = \\'http://s3.amazonaws.com/census-test\\'\\n\\nSee the next section to learn how to deploy data to your custom S3 bucket.\\n\\nLoading data\\nOnce you\\'ve setup the webapp you will have the requirements needed to load data. If you want to load embargoed data you will need to define environment variables for your username and password:\\nCENSUS_USER=cgroskopf@tribune.com\\nCENSUS_PASS=NotMyRealPassword\\n\\nYou will also need to have defined your Amazon Web Services credentials so that you can upload the rendered data files to S3:\\nexport AWS_ACCESS_KEY_ID=\"foo\"\\nexport AWS_SECRET_ACCESS_KEY=\"bar\"\\n\\nYou will also need to modify the load configuration to point at the same S3 bucket you configured for the webapp. Open dataprocessing/config.py and modify the following lines:\\nS3_BUCKETS = {\\n    \\'staging\\': \\'census-test\\',\\n    \\'production\\': \\'censusdata.ire.org\\',\\n}\\n\\nTo load SF1 data for Hawaii make sure you have Mongo running and then execute the following commands:\\ncd dataprocessing\\n./batch_sf.sh Hawaii staging\\n\\n\\nCredits\\nThis application was a project of Investigative Reporters and Editors / National Institute for Computer-Assisted Reporting. Funding was generously provided by The Reynolds Journalism Institute.\\nThe following journalists and nerds contributed to this project:\\n\\nJeremy Ashkenas (New York Times)\\nBrian Boyer (Chicago Tribune)\\nJoe Germuska (Chicago Tribune)\\nChristopher Groskopf (Chicago Tribune)\\nMark Horvit (IRE)\\nRyan Mark (Chicago Tribune)\\nCurt Merrill (CNN)\\nPaul Overberg (USA Today)\\nTed Peterson (IRE)\\nAron Pilhofer (New York Times)\\nMike Tigas (Spokesman-Review)\\nMatt Waite (University of Nebraska)\\n\\n\\nLicense\\nThis software is licensed under the permissive MIT license. See COPYING for details.\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '140',\n",
       "  'forks': '32',\n",
       "  'commits': '752'},\n",
       " {'language': 'Java 99.6',\n",
       "  'readme': '这是2015年的写的项目，已经不再维护，功能可能已经失效，后期有时间会用kotlin重写一遍，无参考价值\\npedometer\\nthis is a pedometer demo,I will rewrite with kotlin\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '157',\n",
       "  'forks': '63',\n",
       "  'commits': '61'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Oh snap!\\nToolbarMenudrawer 2.0 is here :D\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '155',\n",
       "  'forks': '29',\n",
       "  'commits': '30'},\n",
       " {'language': 'Java 99.3',\n",
       "  'readme': 'Socialize SDK Android\\n\\nIntegrating Socialize into your App\\nCheck out the full documentation to learn how to integrate Socialize into your app:\\nhttp://socialize.github.io/socialize-sdk-android/\\n\\nBuilding Socialize from Source\\nNote: This is not required if you simply want to integrate Socialize into your app\\n\\nPrerequisites\\nMake sure you have the following installed on your local machine:\\n\\nAndroid SDK (https://developer.android.com/sdk/)\\n\\nPip:\\nsudo easy_install pip\\n\\n\\nSphinx 1.2.2:\\nsudo pip install sphinx\\n\\n\\nANT:\\nbrew install ant\\n\\n\\n\\nAlso make sure you have the following versions of the Android SDK installed:\\n\\nAndroid 2.2 (API 8)\\nAndroid 4.4 (API 19)\\n\\nThese are installed using the Android SDK manager:\\nhttp://developer.android.com/tools/help/sdk-manager.html\\n\\nBuilding Socialize\\nFirst clone this repo:\\ngit clone git@github.com:socialize/socialize-sdk-android.git\\n\\nSocialize depends on 3 external library projects:\\ngit clone git@github.com:socialize/android-ioc.git\\ngit clone git@github.com:socialize/loopy-sdk-android.git\\ngit clone git@github.com:facebook/facebook-android-sdk.git\\n\\n\\nSetup the Facebook SDK for Build\\nSwitch to the verified (tested) version of Facebook:\\ncd facebook-android-sdk\\ngit checkout sdk-version-3.17.2\\ncd ../\\n\\nThis version of the Facebook SDK (3.17.2) has some compilation warnings which are treated\\nas errors by facebook.  To override this, we need to change the compiler arguments in the\\nfacebook ant.properties:\\nvim facebook-android-sdk/facebook/ant.properties\\n\\nReplace the following line:\\njava.compilerargs=-Xlint -Werror\\n\\nWith:\\njava.compilerargs=-Xlint\\n\\nNow you can build the SDK distribution:\\ncd socialize-sdk-android/sdk\\nant -Dsdk.dir=/usr/local/android clean build\\n\\nMake sure you replace /usr/local/android with your local path to the Android SDK\\n\\nBuilding the Demo App\\nTo build and test the demo app from the command line:\\ncd socialize-sdk-android/demo\\nant -Dsdk.dir=/usr/local/android clean release\\n\\nMake sure you replace /usr/local/android with your local path to the Android SDK\\nNow you can install the demo app:\\n/usr/local/android/platform-tools/adb uninstall com.socialize.demo\\n/usr/local/android/platform-tools/adb install bin/socialize-demo-release.apk\\n\\nThe demo app is called, Socialize Demos\\n\\nBuilding the Documentation\\nNote: Sphinx 1.2.2 is required to generate docs:\\nsudo pip install sphinx\\n\\nTo build the html version of the documentation:\\ncd socialize-sdk-android/sdk\\nant -Dsdk.dir=/usr/local/android doc\\n\\nMake sure you replace /usr/local/android with your local path to the Android SDK\\nNow you can browse the documentation:\\nopen build/docs/user_guide/index.html\\n\\n\\nRunning the Tests\\nIn order to run the tests you need either an Android 4.4 device or emulator.  We recommend using the\\nGenymotion Android virtualization platform available here: http://www.genymotion.com/\\nEnsure the device/emulator is connected and available:\\n/usr/local/android/platform-tools/adb devices\\n\\nIf you do not see any devices listed, try restarting the adb server:\\n/usr/local/android/platform-tools/adb kill-server\\n/usr/local/android/platform-tools/adb start-server\\n\\nPrior to running the tests you MUST run an sdk cleanup so that the stage server has its state reset.\\nThis is a python script located in the test folder:\\ncd socialize-sdk-android/test\\n\\npython sdk-cleanup.py <consumer-key> <consumer-secret> \\\\\\n<http://stage.api.socialize.com/v1> \\\\\\n[facebook_user_id] [facebook_token]\\n\\nTo run the tests:\\nant -propertyfile ant.global.properties -Dsdk.dir=/usr/local/android test-with-results\\n\\nMake sure you replace /usr/local/android with your local path to the Android SDK\\nNow you can browse the coverage results:\\nopen coverage-results/coverage.html\\n\\n\\nBuilding the Distro\\nTo build the distributable SDK (zip):\\ncd socialize-sdk-android/sdk\\nant -Dsdk.dir=/usr/local/android clean build\\n\\nMake sure you replace /usr/local/android with your local path to the Android SDK\\n',\n",
       "  'watchers': '26',\n",
       "  'stars': '152',\n",
       "  'forks': '60',\n",
       "  'commits': '1,374'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Presentation\\nAn architecture for Android as a replacement of MVC.\\nWhy should I use Presentation?\\nBecause you want to have more readable, testable code.\\nAvoid \"God Objects\", mainly your Activities or Fragments.\\nHow does it work?\\nSeparation of responsibilities by module:\\n\\nPresenter: Get a Business Object from the DataProvider and give instructions to the ViewProxy\\nDataProvider: Communicate with the \"outside\" to set and get the data, following the instructions of the Presenter\\nViewProxy: Convert Presenter instructions and set values to Android Views.\\n\\nArchitecture\\n\\nLeak safe.\\nDon\\'t hold a strong reference to the DataProvider, Presenter, or ViewProxy.\\nSample\\nThe goal is to make this application:\\n\\nEach public method of the modules are defined into an interface:\\npublic interface FormDef {\\n\\n    interface IPresenter extends Base.IPresenter {\\n\\n        void onClickSaveButton(String value);\\n    }\\n\\n    interface IDataProvider extends Base.IDataProvider {\\n\\n        String getValueSaved();\\n\\n        void saveValue(String value);\\n    }\\n\\n    interface IView extends Base.IView {\\n\\n        void setValueSaved(String text);\\n    }\\n\\n}\\n\\nThen you have your:\\n\\nFormPresenter that extends BasePresenter and implements FormDef.IPresenter\\nFormDataProvider that extends BaseDataProvider and implements FormDef.IDataProvider\\nFormViewProxy that extends BaseViewProxy and implements FormDef.IView\\n\\nThe easiest way to use this architecture is to override BaseActivity, even if it\\'s NOT mandatory.\\npublic class FormActivity extends BaseActivity<FormPresenter, FormViewProxy> {\\n\\n    @Override\\n    public int getContentView() {\\n        return R.layout.activity_form;\\n    }\\n\\n    @Override\\n    public Class<FormPresenter> getPresenterClass() {\\n        return FormPresenter.class; // the class of your presenter to be used within this activity\\n    }\\n\\n    @Override\\n    public FormViewProxy newViewProxy(FormPresenter presenter, Bundle savedInstanceState) {\\n        return new FormViewProxy(this); // the view proxy, with the Activity in param\\n    }\\n}\\n\\nNote: it also exists a BaseFragment to do exactly the same but within a Fragment.\\nGo further\\nRe-use Presenter after Activity destruction\\nBecause BasePresenter extends ViewModel (from Android Architecture), it is possible to re-use the same Presenter after a rotation for example.\\nWhen you are using BaseActivity, your presenters will be automatically re-used.\\nNote: in this case, the DataProvider will also be kept, but you will have to re-create the ViewProxy.\\nPresenter into an Adapter\\nThis library has a dependency on Efficient Adapter to use the same view cache mechanism.\\nThis allows to apply the Presentation pattern to object in an Adapter as well. Your ViewHolder should extend PresenterViewHolder and your Presenter should extend BaseItemPresenter.\\nProguard\\nNothing special needed.\\nGradle\\ndependencies {\\n    compile \\'com.skocken:presentation:2.5.0\\'\\n}\\n\\nAndroid Support library\\nIf you are still using the deprecated Android Support Library (instead of AndroidX), please use the dependency 2.4.X instead.\\nLicense\\n\\nApache 2.0\\n\\nContributing\\nPlease fork this repository and contribute back using\\npull requests.\\nAny contributions, large or small, major features, bug fixes, unit/integration tests are welcomed and appreciated\\nbut will be thoroughly reviewed and discussed.\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '151',\n",
       "  'forks': '14',\n",
       "  'commits': '52'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '\\n到达顶部或底部继续拉动时，实现Item间的相互分离，有两种模式：\\n1.全部分离的模式，即屏幕内所有Item都会分离\\n2.部分分离模式，以点击位置为分界点，部分item分离\\n用法\\n在代码中\\nPullSeparateListView lv = (PullSeparateListView) findViewById(R.id.pullExpandListView);\\n//全部分离设置为true,部分分离设置为false。默认为false\\nlv.setSeparateAll(true);\\n\\n在xml中\\n<com.chiemy.pullseparate.PullSeparateListView\\n    android:id=\"@+id/pullExpandListView1\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\"\\n    android:layout_below=\"@+id/is_separateAll_cb\"\\n\\tapp:separate_all=\"true\"\\n    >\\n</com.chiemy.pullseparate.PullSeparateListView>\\n\\n另外还添加了点击缩放的效果\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '150',\n",
       "  'forks': '80',\n",
       "  'commits': '14'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '[DEPRECATED] RxAssertions\\n\\n\\nThis project is no longer maintened / active. Thanks for all OSS people that contributed and/or provided feedback !!!\\n\\nRxAssertions is a simple idea for better RxJava assertions.\\nI found the original idea from the guys of Ribot : in fact, I think this a good idea and helps to keep tests clean.\\nHowever, Ribot guys deprecated their original repo some time ago in favor of vanilla TestSubscriber, so I decided to take my own shot on this.\\nThis library mimics and improves the original Ribot`s idea with the following goodies :\\n\\nAssertJ powered assertions for RxJava (as the original one)\\nAll tests rely on BlockingObservable internally\\nInternal API rely 100% on TestSubscriber\\nImproved public API, covering most of TestSubscriber provided assertions\\nImproved Assertions entry points from factory methods, with Observable, BlockingObservable, Single and Completable support\\n\\nLets see some code diet :\\nRegular assertions with TestSubscriber\\nTestSubscriber<String> testSubscriber = new TestSubscriber<>();\\nObservable.just(\"RxJava\", \"Assertions\").toBlocking().subscribe(testSubscriber);\\ntestSubscriber.assertCompleted();\\ntestSubscriber.assertNoErrors();\\ntestSubscriber.assertValues(\"RxJava\", \"Assertions\");\\nAssertions with RxAssertions\\nRxAssertions.assertThat(Observable.just(\"RxJava\", \"Assertions\"))\\n\\t\\t.completes()\\n\\t\\t.withoutErrors()\\n\\t\\t.expectedValues(\"RxJava\", \"Assertions\");\\nor\\nassertThat(Observable.empty())\\n\\t\\t.emitsNothing()\\n\\t\\t.completes()\\n\\t\\t.withoutErrors();\\nor\\nSingle<String> single = Single.fromCallable(() -> \"RxJava\");\\nassertThat(single).completes().expectedSingleValue(\"RxJava\");\\nYou can find other examples at test folder\\nSetup\\nAdd it in your build.gradle\\nrepositories {\\n\\t...\\n\\tmaven { url \"https://jitpack.io\" }\\n\\n}\\nAdd the dependency\\ndependencies {\\n\\t...\\n\\ttestCompile \\'com.github.ubiratansoares:rxassertions:$version\\'\\n}\\nCheck the releases tab for the latest version.\\nRxAssertions uses RxJava 1.1.9 and AssertJ 2.5.0 as dependencies.\\nExperimental\\nSince v0.3.0, we have some assertions leveraging on AssertJ Conditions API. This kind of assertion offers flexible matching both for emissions and error checks. You can find samples at test folder.\\nContributing\\nPRs are wellcome. 🚀\\nCredits\\n\\nRibot guys for the original idea\\nRxJava and AssertJ guys for these awesome libraries\\n\\nLicense\\nCopyright (C) 2016 Ubiratan Soares\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '148',\n",
       "  'forks': '11',\n",
       "  'commits': '32'},\n",
       " {'language': 'Java 81.7',\n",
       "  'readme': 'play2-crud\\n\\nPowerful CRUD & DAO implementation with REST interface for play framework 2.x\\nFor the Typesafe Activator check play2-crud-activator.\\nSome screenshots\\n\\n\\nindex page\\n\\n\\n\\ncreate page\\n\\n\\n\\nlist page\\n\\n\\n\\nQuick Start\\nFollow these steps to use play2-crud. You can also use it partially just for DAO or CRUD controllers. If you think any part needs further explanation, please report a new issue.\\nAdd play2-crud dependency\\nYou can begin with adding play2-crud dependency inside conf/Build.scala file.\\n\\nAdd app dependency:\\n\\n    val appDependencies = Seq(\\n        javaCore, javaJdbc, javaEbean,\\n        \"play2-crud\" % \"play2-crud_2.10\" % \"0.7.0\"\\n    )\\n\\n\\n\\n\\nDependency version is for version 0.7.0 defined, but you can use the latest version.\\n\\n\\nAdd custom maven repositories:\\n\\n\\n    val main = play.Project(appName, appVersion, appDependencies).settings(\\n        //maven repository\\n        resolvers += \"release repository\" at  \"http://hakandilek.github.com/maven-repo/releases/\",\\n        resolvers += \"snapshot repository\" at \"http://hakandilek.github.com/maven-repo/snapshots/\"\\n    )\\n\\n\\nAssociate Global settings\\nDirect reference\\nIf you don\\'t want to override the play application launcher, you just have to notice to play that the class to use as launcher is now GlobalCRUDSettings. Change the application.global configuration key in the conf/application.conf file, and use play.utils.crud.GlobalCRUDSettings:\\n...\\napplication.global=play.utils.crud.GlobalCRUDSettings\\n...\\n\\n\\nDefine routes\\n# CRUD Controllers\\n->     /app             play.crud.Routes\\n\\n# REST API\\n->     /api             play.rest.Routes\\n\\n\\nDefine model\\n\\nModel class has to implement play.utils.dao.BasicModel with the type parameter indicating the type of the @Id field.\\n\\n@Entity\\npublic class Sample extends Model implements BasicModel<Long> {\\n\\n   @Id\\n   private Long key;\\n\\n   @Basic\\n   @Required\\n   private String name;\\n\\n   public Long getKey() {\\n      return key;\\n   }\\n\\n   public void setKey(Long key) {\\n      this.key = key;\\n   }\\n\\n   public String getName() {\\n      return name;\\n   }\\n\\n   public void setName(String name) {\\n      this.name = name;\\n   }\\n}\\n\\nHere the Sample model class implements BasicModel<Long> where key field indicated with @Id is Long.\\n\\n... call http://localhost:9000/app and voila!\\nSamples\\n\\nSample with basic dynamic CRUD controllers\\nSample with custom views is a full featured sample.\\nFull featured sample with DAO and DAOListeners\\nSample with Cache usage\\n\\nHOW-TO\\nHere you can find some HOW-TO documents introducing some powerful functionality:\\n\\nHOW-TO use simple CRUD\\nHOW-TO define a custom DAO\\nHOW-TO define a custom Controller\\nHOW-TO use DAO Listeners\\nHOW-TO use dynamic REST Controllers\\nHOW-TO use custom REST Controllers\\nHOW-TO Override Play Launcher\\n\\n',\n",
       "  'watchers': '24',\n",
       "  'stars': '147',\n",
       "  'forks': '52',\n",
       "  'commits': '159'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': \"WatchTower\\n\\n\\n\\nNote: In order to use this app, you'll need to use the Google API Console to register your SHA1 token along with the package name, this app won't work otherwise.\\nWatchTower is a simple application which was created to test and explore the functionality of the new Proximity Beacon API. The application can be used to try out:\\n\\nRegistering Beacons\\nUpdating Beacons\\nViewing Beacons\\nViewing Beacon Diagnostics\\nViewing Beacon Attachments\\nAdding Beacon Attachments\\nDeleting Single Beacon Attachments\\nDeleting Batch Beacon Attachments by Type\\n\\nSome features are not very practical within a mobile-app (for example, adding json data to attachments), so these have not been included.\\n\\n\\n\\nNote: This was built quickly to simply test the APIs functionality. If you come across any bugs please feel free to submit them as an issue, or open a pull request ;)\\nFor further information, please read the supporting blog post.\\nRequirements\\n\\nAndroid SDK.\\nAndroid 5.1 (API 22) .\\nAndroid SDK Tools\\nAndroid SDK Build tools 22.0.1\\nAndroid Support Repository\\nAndroid Support library\\nEnabled [Unit Test support] (http://tools.android.com/tech-docs/unit-testing-support)\\n\\nBuilding\\nTo build, install and run a debug version, run this from the root of the project:\\n./gradlew installRunDebug\\n\\nTesting\\nFor Android Studio to use syntax highlighting for Automated tests and Unit tests you must switch the Build Variant to the desired mode.\\nTo run unit tests on your machine using [Robolectric] (http://robolectric.org/):\\n./gradlew testDebug\\n\\nTo run automated tests on connected devices:\\n./gradlew connectedAndroidTest\\n\\nAttributions\\nThanks to the following for icons off of Noun Project:\\n\\nStéphanie Rusch - Beacon Icon \\nAbraham - Cloud Icon\\nS.Shohei - Battery Icon\\nJuergen Bauer - Alert Icon\\nPham Thi Dieu Linh - Attachment Icon\\n\",\n",
       "  'watchers': '13',\n",
       "  'stars': '146',\n",
       "  'forks': '12',\n",
       "  'commits': '90'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '这是一个极简的HelloWorld应用，主要用来展示如何在Android平台架构Flux应用。并提供一些基础代码，方便开发者直接Copy这些代码到自己的工程中，省掉重新造轮子的过程。接下来会一步步的解释这个应用是如何构建的。\\nDemo程序是用AndroidStudio开发的，假设你已经了解Android和AndroidStudioIDE，如果你已经很熟悉Android应用的开发，看完AndroidFlux一览或许已经可以开发出基于Flux框架的应用，如果你并不熟悉Flux或者Android，务必先读完这篇文档\\n源码结构\\n本着架构即目录的思想，让我们先看一下源码结构，整个的源码结构是这样的：\\n➜  tree .\\n.\\n├── MainActivity.java\\n├── actions\\n│\\xa0\\xa0 ├── Action.java\\n│\\xa0\\xa0 ├── ActionsCreator.java\\n│\\xa0\\xa0 └── MessageAction.java\\n├── dispatcher\\n│\\xa0\\xa0 └── Dispatcher.java\\n├── model\\n│\\xa0\\xa0 └── Message.java\\n└── stores\\n    ├── MessageStore.java\\n    └── Store.java\\n\\n这里包含4个目录和一个文件：\\n\\nMainActivity.java Flux框架中的Controller-View部分，在Android中可以是Activity或者Fragment\\nactions Flux框架中的Action部分，存放不同类型的XXXAction.java和ActionsCreator.java文件\\ndispatcher Flux框架中的Dispatcher部分，存放 Dispatcher.java 文件，一个应用中只需要一个Dispatcher\\nmodel 存放各种业务逻辑相关的Model文件\\nstores Flux框架中的Stores部分，存在各种类型的 XXXStore.java 文件\\n\\n创建一个Dispatcher\\n在AndroidFlux中Dispatcher是就是一个发布-订阅模式。Store会在这里注册自己的回调接口，Dispatcher会把Action分发到注册的Store，所以它会提供一些公有方法来注册监听和分发消息。\\n/**\\n * Flux的Dispatcher模块\\n * Created by ntop on 18/12/15.\\n */\\npublic class Dispatcher {\\n    private static Dispatcher instance;\\n    private final List<Store> stores = new ArrayList<>();\\n\\n    public static Dispatcher get() {\\n        if (instance == null) {\\n            instance = new Dispatcher();\\n        }\\n        return instance;\\n    }\\n\\n    Dispatcher() {}\\n\\n    public void register(final Store store) {\\n        stores.add(store);\\n    }\\n\\n    public void unregister(final Store store) {\\n        stores.remove(store);\\n    }\\n\\n    public void dispatch(Action action) {\\n        post(action);\\n    }\\n\\n    private void post(final Action action) {\\n        for (Store store : stores) {\\n            store.onAction(action);\\n        }\\n    }\\n}\\n\\nDispatcher对外仅暴露3个公有方法：\\n\\nregister(final Store store) 用来注册每个Store的回调接口\\nunregister(final Store store) 用来接触Store的回调接口\\ndispatch(Action action) 用来触发Store注册的回调接口\\n\\n这里仅仅用一个ArrayList来管理Stores，对于一个更复杂的App可能需要精心设计数据结构来管理Stores组织和相互间的依赖关系。\\n创建Stores\\n这里使用EventBus来实现Store，EventBus的主要功能是用来给Controller-View发送change事件：\\n/**\\n * Flux的Store模块\\n * Created by ntop on 18/12/15.\\n */\\npublic abstract class Store {\\n    private  static final Bus bus = new Bus();\\n\\n    protected Store() {\\n    }\\n\\n    public void register(final Object view) {\\n        this.bus.register(view);\\n    }\\n\\n    public void unregister(final Object view) {\\n        this.bus.unregister(view);\\n    }\\n\\n    void emitStoreChange() {\\n        this.bus.post(changeEvent());\\n    }\\n\\n    public abstract StoreChangeEvent changeEvent();\\n    public abstract void onAction(Action action);\\n\\n    public class StoreChangeEvent {}\\n}\\n\\n抽象的Store类，提供了一个主要的虚方法 void onAction(Action action) ，这个方法是注册在Dispatcher里面的回调接口，当Dispatcher有数据派发过来的时候，可以在这里处理。\\n下面看一下更具体的和业务相关的MessageStore类:\\n/**\\n * MessageStore类主要用来维护MainActivity的UI状态\\n * Created by ntop on 18/12/15.\\n */\\npublic class MessageStore extends Store {\\n    private static MessageStore singleton;\\n    private Message mMessage = new Message();\\n\\n    public MessageStore() {\\n        super();\\n    }\\n\\n    public String getMessage() {\\n        return mMessage.getMessage();\\n    }\\n\\n    @Override\\n    @Subscribe\\n    public void onAction(Action action) {\\n        switch (action.getType()) {\\n            case MessageAction.ACTION_NEW_MESSAGE:\\n                mMessage.setMessage((String) action.getData());\\n                break;\\n            default:\\n        }\\n        emitStoreChange();\\n    }\\n\\n\\n    @Override\\n    public StoreChangeEvent changeEvent() {\\n        return new StoreChangeEvent();\\n    }\\n}\\n\\n在这里实现了 onAction(Action action) 方法，并用一个switch语句来路由各种不同的Action类型。同时维护了一个结构 Message.java 类，这个类用来记录当前要显示的消息。Store类只能通过Dispatcher来更新（不要提供setter方法），对外仅暴露各种getter方法来获取UI状态。这里用String getMessage()方法来获取具体的消息。\\n在Controller-View里面处理“change”事件\\n在Android中，Flux的Controller-View对应于Activity或者Fragment，我们需要在这里注册Strore发生改变的事件通知，以便在Store变化的时候重新绘制UI。\\n/**\\n * Flux的Controller-View模块\\n * Created by ntop on 18/12/15.\\n */\\npublic class MainActivity extends AppCompatActivity implements View.OnClickListener {\\n    private EditText vMessageEditor;\\n    private Button vMessageButton;\\n    private TextView vMessageView;\\n\\n    private Dispatcher dispatcher;\\n    private ActionsCreator actionsCreator;\\n    private MessageStore store;\\n\\n    @Override\\n    protected void onCreate(Bundle savedInstanceState) {\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.activity_main);\\n        initDependencies();\\n        setupView();\\n    }\\n\\n    @Override\\n    protected void onDestroy() {\\n        super.onDestroy();\\n        dispatcher.unregister(store);\\n    }\\n\\n    private void initDependencies() {\\n        dispatcher = Dispatcher.get();\\n        actionsCreator = ActionsCreator.get(dispatcher);\\n        store = new MessageStore();\\n        dispatcher.register(store);\\n    }\\n\\n    private void setupView() {\\n        vMessageEditor = (EditText) findViewById(R.id.message_editor);\\n        vMessageView = (TextView) findViewById(R.id.message_view);\\n        vMessageButton = (Button) findViewById(R.id.message_button);\\n        vMessageButton.setOnClickListener(this);\\n    }\\n\\n    @Override\\n    public void onClick(View view) {\\n        int id = view.getId();\\n        if (id == R.id.message_button) {\\n            if (vMessageEditor.getText() != null) {\\n                actionsCreator.sendMessage(vMessageEditor.getText().toString());\\n                vMessageEditor.setText(null);\\n            }\\n        }\\n    }\\n\\n    private void render(MessageStore store) {\\n        vMessageView.setText(store.getMessage());\\n    }\\n\\n    @Override\\n    protected void onResume() {\\n        super.onResume();\\n        store.register(this);\\n    }\\n\\n    @Override\\n    protected void onPause() {\\n        super.onPause();\\n        store.unregister(this);\\n    }\\n\\n    @Subscribe\\n    public void onStoreChange(Store.StoreChangeEvent event) {\\n        render(store);\\n    }\\n}\\n\\n这部分的代码比较多，首先在 onCreatre(...) 方法中初始化了依赖和需要的UI组件。最重要的是 onStoreChange(...) 方法，这个方法是注册在Store中回调（使用EventBus的@Subscribe注解标识），当Store发生变化的时候会触发这个方法，我们在这里调用render()方法重绘整个界面。\\n创建Action\\nAction是简单的POJO类型，只提供两个字段：type 和 data, 分别记录Action的类型和数据。注意Action一旦创建是不可更改的，\\n所以它的字段类型修饰为final类型。\\npublic class Action<T> {\\n    private final String type;\\n    private final T data;\\n\\n    Action(String type, T data) {\\n        this.type = type;\\n        this.data = data;\\n    }\\n\\n    public String getType() {\\n        return type;\\n    }\\n\\n    public T getData() {\\n        return data;\\n    }\\n}\\n\\n下面是一个业务相关的Action实现：\\npublic class MessageAction extends Action<String> {\\n    public static final String ACTION_NEW_MESSAGE = \"new_message\";\\n\\n    MessageAction(String type, String data) {\\n        super(type, data);\\n    }\\n}\\n\\n这个实现非常简单，仅仅多定义了一个Action类型字段：public static final String ACTION_NEW_MESSAGE = \"new_message\"。如你所见，Action都是这么简单的，不包含任何业务逻辑。\\n创建ActionCreator\\nActionCreator 是Flux架构中第“四”个最重要的模块（前三：Dispatcher、Store、View），这里实际上处理很多工作，提供有一个有语义的API，构建Action，处理网络请求等。\\n/**\\n * Flux的ActionCreator模块\\n * Created by ntop on 18/12/15.\\n */\\npublic class ActionsCreator {\\n\\n    private static ActionsCreator instance;\\n    final Dispatcher dispatcher;\\n\\n    ActionsCreator(Dispatcher dispatcher) {\\n        this.dispatcher = dispatcher;\\n    }\\n\\n    public static ActionsCreator get(Dispatcher dispatcher) {\\n        if (instance == null) {\\n            instance = new ActionsCreator(dispatcher);\\n        }\\n        return instance;\\n    }\\n\\n    public void sendMessage(String message) {\\n        dispatcher.dispatch(new MessageAction(MessageAction.ACTION_NEW_MESSAGE, message));\\n    }\\n}\\n\\n此处提供了一个 sendMessage(String message) ，就像名字暗示的那样，这个方法用来发送消息（到Store）。在方法内部，会创建一个MessageAction来封装数据和Action类型，并通过Dispatcher发送到Store。\\nModel\\n无论是基于哪种框架的应用都需要Model模块，在这个简单的“HelloWorld”应用中，其实用一个String即可传递消息，但是为了架构的完整和更好的语义表达，定义一个Message类型封装一个String字段作为Model。\\n希望通过这个简单的HelloWorld应用，能够让你一窥Flux的面貌。如果你想更深入的了解在Android平台上应用Flux架构，可以查看我们的Github站点。\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '146',\n",
       "  'forks': '41',\n",
       "  'commits': '7'},\n",
       " {'language': 'Java 99.8',\n",
       "  'readme': 'IceFig   \\nJava elegant supplement\\nJava 8 delivered lambda expressions, but without the enhancement of basic libraries like List, Map, String, which makes\\nlambda expression still not delightful.\\nInspired by other popular languages like Ruby and Scala, IceFig intends to supply the missing.\\nQuick Scan\\nElegant alternative to List: Seq\\nSeq<Integer> seq = Seqs.newSeq(1,2,3);\\nseq.shuffle(); // copy to a new seq and shuffle it\\nseq.forEach((value, idx) -> { // with index\\n    // (1, 0)  (2, 1)  (3, 2)\\n});\\nseq.forEachCons(2, (values)->{\\n    // [1,2]  [2, 3]\\n});\\n\\nseq.join(\"-\"); //\"1-2-3\"\\n\\nseq.map(a -> a+ 1).distinct().reverse().join()\\nElegant alternative to Map: Hash\\nHash<Integer, Integer> hash = Hashes.<Integer, Integer>newHash().put(1, 2).put(2, 3).put(3, 3);\\nhash.containsAny((k, v) -> k+v == 5 ); //true\\nhash.keysOf(3); // [2, 3]\\nElegant alternative to String: CharSeq\\nCharSeq str = CharSeq.of(\"a b c d e f g\");\\nstr.split(\" \").join(\"-\").capitalize(); //\"A-b-c-d-e-f-g\"\\nstr.partition(\"d e\").map(CharSeq::trim);  //[\"a b c\", \"d e\", \"f g\"]\\nFull Javadoc\\nInclude it\\n<dependency>\\n    <groupId>com.worksap</groupId>\\n    <artifactId>icefig</artifactId>\\n    <version>[latest version]</version>\\n</dependency>\\nConcept\\nNot stream\\nIceFig is different from Stream, and implemented without Stream. While, it is simpler concept -- supplement methods on basic libraries.\\nStream has several characteristics:\\n\\nTrends to process each element independently\\nInfinite that we can not get the size\\nDesigned for large data flow performance\\n\\nThus, Stream may not be able to support operations related with the size of it, nor operations involving multiple or even random elements.\\nBasically, the vast majority operations on List, Map, String don\\'t need a Stream. Stream brings great merits on big data processing, but when we\\'re not facing performance problem (operating a list of about 10x elements), it is an over kill.\\nYet Stream brings the 2 additional steps \"Stream()\" and \"collect()\", which is sometimes annoying to write.\\nIceFig targets on \"small data\" operations within application logic, to provide simple & beautiful code writing about String, List, Map operation & transformation.\\nNo utilities\\nIn traditional Java way, we use a lot of utilities (StringUtils, FileUtils) for the missing methods in standard library. While in IceFig, we make an object oriented and functional way to free you from tedious codes.\\nZero runtime dependency\\nIceFig has no external runtime dependency except JDK 8.\\nMutable & default interfaces\\nIceFig firstly aggregates all operations which do not change the state into a default interface(e.x. Seq, Hash).\\nOn the other hand, there are interfaces named \"mutableXXX\" extending the default ones with additional in-place operations, which are commonly named xxxInPlace.\\nIf you don\\'t want the ability to change the object, you can use the default interface to let compiler check it for you. And it is the recommended way.\\nNote that the default interface doesn\\'t mean immutability of the object it is on, it only ensures \"if outside only uses this interface on the object, the object will not be changed\".\\nConventions\\nIceFig uses conventions on method names. If there is a pair of methods name, nameInPlace, method ends with InPlace means calling this method will change the object itself, while calling the other won\\'t.\\nLicense\\nApache License 2.0\\nContribution\\nFeel free to submit issues & PRs\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '142',\n",
       "  'forks': '7',\n",
       "  'commits': '224'},\n",
       " {'language': 'C++ 47.7',\n",
       "  'readme': 'IceFig   \\nJava elegant supplement\\nJava 8 delivered lambda expressions, but without the enhancement of basic libraries like List, Map, String, which makes\\nlambda expression still not delightful.\\nInspired by other popular languages like Ruby and Scala, IceFig intends to supply the missing.\\nQuick Scan\\nElegant alternative to List: Seq\\nSeq<Integer> seq = Seqs.newSeq(1,2,3);\\nseq.shuffle(); // copy to a new seq and shuffle it\\nseq.forEach((value, idx) -> { // with index\\n    // (1, 0)  (2, 1)  (3, 2)\\n});\\nseq.forEachCons(2, (values)->{\\n    // [1,2]  [2, 3]\\n});\\n\\nseq.join(\"-\"); //\"1-2-3\"\\n\\nseq.map(a -> a+ 1).distinct().reverse().join()\\nElegant alternative to Map: Hash\\nHash<Integer, Integer> hash = Hashes.<Integer, Integer>newHash().put(1, 2).put(2, 3).put(3, 3);\\nhash.containsAny((k, v) -> k+v == 5 ); //true\\nhash.keysOf(3); // [2, 3]\\nElegant alternative to String: CharSeq\\nCharSeq str = CharSeq.of(\"a b c d e f g\");\\nstr.split(\" \").join(\"-\").capitalize(); //\"A-b-c-d-e-f-g\"\\nstr.partition(\"d e\").map(CharSeq::trim);  //[\"a b c\", \"d e\", \"f g\"]\\nFull Javadoc\\nInclude it\\n<dependency>\\n    <groupId>com.worksap</groupId>\\n    <artifactId>icefig</artifactId>\\n    <version>[latest version]</version>\\n</dependency>\\nConcept\\nNot stream\\nIceFig is different from Stream, and implemented without Stream. While, it is simpler concept -- supplement methods on basic libraries.\\nStream has several characteristics:\\n\\nTrends to process each element independently\\nInfinite that we can not get the size\\nDesigned for large data flow performance\\n\\nThus, Stream may not be able to support operations related with the size of it, nor operations involving multiple or even random elements.\\nBasically, the vast majority operations on List, Map, String don\\'t need a Stream. Stream brings great merits on big data processing, but when we\\'re not facing performance problem (operating a list of about 10x elements), it is an over kill.\\nYet Stream brings the 2 additional steps \"Stream()\" and \"collect()\", which is sometimes annoying to write.\\nIceFig targets on \"small data\" operations within application logic, to provide simple & beautiful code writing about String, List, Map operation & transformation.\\nNo utilities\\nIn traditional Java way, we use a lot of utilities (StringUtils, FileUtils) for the missing methods in standard library. While in IceFig, we make an object oriented and functional way to free you from tedious codes.\\nZero runtime dependency\\nIceFig has no external runtime dependency except JDK 8.\\nMutable & default interfaces\\nIceFig firstly aggregates all operations which do not change the state into a default interface(e.x. Seq, Hash).\\nOn the other hand, there are interfaces named \"mutableXXX\" extending the default ones with additional in-place operations, which are commonly named xxxInPlace.\\nIf you don\\'t want the ability to change the object, you can use the default interface to let compiler check it for you. And it is the recommended way.\\nNote that the default interface doesn\\'t mean immutability of the object it is on, it only ensures \"if outside only uses this interface on the object, the object will not be changed\".\\nConventions\\nIceFig uses conventions on method names. If there is a pair of methods name, nameInPlace, method ends with InPlace means calling this method will change the object itself, while calling the other won\\'t.\\nLicense\\nApache License 2.0\\nContribution\\nFeel free to submit issues & PRs\\n',\n",
       "  'watchers': '47',\n",
       "  'stars': '109',\n",
       "  'forks': '38',\n",
       "  'commits': '96'},\n",
       " {'language': 'C++ 81.4',\n",
       "  'readme': 'Model-based Deep Hand Pose Estimation\\nThis repository is the released code of our IJCAI 2016 paper for estimating hand pose from depth image.\\nContact: zhouxy13@fudan.edu.cn\\nRequirements\\n\\nCaffe\\nPython with opencv\\n\\nInstallation\\n\\nDownload caffe\\nCopy ./libs/include to caffe_root/include and ./libs/src to caffe_root/src\\nCompile caffe\\nCopy path.config.example to path.config and set the pycaffe path\\n\\nTest\\n\\nRun demo.py in ./testing\\nOur prediction on NYU dataset here\\nOur prediction on ICVL dataset here\\n\\nTrain\\n\\ndownload NYU dataset\\nset NYU_path in path.config\\nRun GetH5DataNYU.py in ./training\\nTrain with solver.prototxt\\n\\nCitation\\nPlease cite DeepModel in your publication if it helps your research:\\n@inproceedings{zhou2016model,\\n    author = {Xingyi Zhou and Qingfu Wan and Wei Zhang and Xiangyang Xue and Yichen Wei},\\n    booktitle = {IJCAI},\\n    title = {Model-based Deep Hand Pose Estimation},\\n    year = {2016}\\n}\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '106',\n",
       "  'forks': '44',\n",
       "  'commits': '7'},\n",
       " {'language': 'C++ 33.5',\n",
       "  'readme': 'Model-based Deep Hand Pose Estimation\\nThis repository is the released code of our IJCAI 2016 paper for estimating hand pose from depth image.\\nContact: zhouxy13@fudan.edu.cn\\nRequirements\\n\\nCaffe\\nPython with opencv\\n\\nInstallation\\n\\nDownload caffe\\nCopy ./libs/include to caffe_root/include and ./libs/src to caffe_root/src\\nCompile caffe\\nCopy path.config.example to path.config and set the pycaffe path\\n\\nTest\\n\\nRun demo.py in ./testing\\nOur prediction on NYU dataset here\\nOur prediction on ICVL dataset here\\n\\nTrain\\n\\ndownload NYU dataset\\nset NYU_path in path.config\\nRun GetH5DataNYU.py in ./training\\nTrain with solver.prototxt\\n\\nCitation\\nPlease cite DeepModel in your publication if it helps your research:\\n@inproceedings{zhou2016model,\\n    author = {Xingyi Zhou and Qingfu Wan and Wei Zhang and Xiangyang Xue and Yichen Wei},\\n    booktitle = {IJCAI},\\n    title = {Model-based Deep Hand Pose Estimation},\\n    year = {2016}\\n}\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '104',\n",
       "  'forks': '81',\n",
       "  'commits': '76'},\n",
       " {'language': 'C++ 57.5',\n",
       "  'readme': 'node-libdtrace\\nOverview\\nnode-libdtrace is a Node.js addon that interfaces to libdtrace, allowing\\nnode programs to control DTrace enablings.\\nStatus\\nThe primary objective is not to create a dtrace(1M) alternative in node, but\\nrather to allow node programs to create and control programmatically useful\\nDTrace enablings.  That is, the goal is software-software interaction, and as\\nsuch, DTrace actions related to controlling output (e.g., printf(),\\nprinta()) are not supported.  Error handling is, for the moment, weak.\\nPlatforms\\nThis should work on any platform that supports DTrace, and is known to work on\\nMac OS X (tested on 10.7.5) and illumos (tested on\\nSmartOS).\\nInstallation\\nAs an addon, node-libdtrace is installed in the usual way:\\n  % npm install libdtrace\\n\\nAPI\\nnew libdtrace.Consumer()\\nCreate a new libdtrace consumer, which will correspond to a new libdtrace\\nstate.  If DTrace cannot be initalized for any reason, this will throw an\\nexception with the message member set to the more detailed reason from\\nlibdtrace.  Note that one particularly common failure mode is attempting to\\ninitialize DTrace without the necessary level of privilege; in this case, for\\nexample, the message member will be:\\n  DTrace requires additional privileges\\n\\n(The specifics of this particular message should obviously not be\\nprogrammatically depended upon.)  If encountering this error, you will\\nneed to be a user that has DTrace privileges.\\nconsumer.strcompile(str)\\nCompile the specified str as a D program.  This is required before\\nany call to consumer.go().\\nconsumer.go()\\nInstruments the system using the specified enabling.  Before consumer.go()\\nis called, the specified D program has been compiled but not executed; once\\nconsumer.go() is called, no further D compilation is possible.\\nconsumer.setopt(option, value)\\nSets the specified option (a string) to value (an integer, boolean,\\nstring, or string representation of an integer or boolean, as denoted by\\nthe option being set).\\nconsumer.consume(function func (probe, rec) {})\\nConsume any DTrace data traced to the principal buffer since the last call to\\nconsumer.consume() (or the call to consumer.go() if consumer.consume()\\nhas not been called).  For each trace record, func will be called and\\npassed two arguments:\\n\\n\\nprobe is an object that specifies the probe that corresponds to the\\ntrace record in terms of the probe tuple: provider, module, function\\nand name.\\n\\n\\nrec is an object that has a single member, data, that corresponds to\\nthe datum within the trace record.  If the trace record has been entirely\\nconsumed, rec will be undefined.\\n\\n\\nIn terms of implementation, a call to consumer.consume() will result in a\\ncall to dtrace_status() and a principal buffer switch.  Note that if the\\nrate of consumption exceeds the specified switchrate (set via either\\n#pragma D option switchrate or consumer.setopt()), this will result in no\\nnew data processing.\\nconsumer.aggwalk(function func (varid, key, value) {})\\nSnapshot and iterate over all aggregation data accumulated since the\\nlast call to consumer.aggwalk() (or the call to consumer.go() if\\nconsumer.aggwalk() has not been called).  For each aggregate record,\\nfunc will be called and passed three arguments:\\n\\n\\nvarid is the identifier of the aggregation variable.  These IDs are\\nassigned in program order, starting with 1.\\n\\n\\nkey is an array of keys that, taken with the variable identifier,\\nuniquely specifies the aggregation record.\\n\\n\\nvalue is the value of the aggregation record, the meaning of which\\ndepends on the aggregating action:\\n\\n\\nFor count(), sum(), max() and min(), the value is the\\ninteger value of the aggregation action\\n\\n\\nFor avg(), the value is the numeric value of the aggregating action\\n\\n\\nFor quantize() and lquantize(), the value is an array of 2-tuples\\ndenoting ranges and value:  each element consists of a two element array\\ndenoting the range (minimum followed by maximum, inclusive) and the\\nvalue for that range.\\n\\n\\n\\n\\nUpon return from consumer.aggwalk(), the aggregation data for the specified\\nvariable and key(s) is removed.\\nNote that the rate of consumer.aggwalk() actually consumes the aggregation\\nbuffer is clamed by the aggrate option; if consumer.aggwalk() is called\\nmore frequently than the specified rate, consumer.aggwalk() will not\\ninduce any additional data processing.\\nconsumer.aggwalk() does not iterate over aggregation data in any guaranteed\\norder, and may interleave aggregation variables and/or keys.\\nconsumer.version()\\nReturns the version string, as returned from dtrace -V.\\nExamples\\nHello world\\nThe obligatory \"hello world\":\\n  var sys = require(\\'sys\\');\\n  var libdtrace = require(\\'libdtrace\\');\\n  var dtp = new libdtrace.Consumer();\\n    \\n  var prog = \\'BEGIN { trace(\"hello world\"); }\\';\\n    \\n  dtp.strcompile(prog);\\n  dtp.go();\\n    \\n  dtp.consume(function (probe, rec) {\\n          if (rec)\\n                  sys.puts(rec.data);\\n  });\\n\\nUsing aggregations\\nA slightly more sophisticated example showing system calls aggregated and\\nsorted by executable name:\\n  var sys = require(\\'sys\\');\\n  var libdtrace = require(\\'libdtrace\\');\\n  var dtp = new libdtrace.Consumer();\\n  \\n  var prog = \\'syscall:::entry { @[execname] = count(); }\\'\\n  \\n  dtp.strcompile(prog);\\n  dtp.go();\\n  \\n  var syscalls = {};\\n  var keys = [];\\n  \\n  var pad = function (val, len)\\n  {\\n          var rval = \\'\\', i, str = val + \\'\\';\\n  \\n          for (i = 0; i < Math.abs(len) - str.length; i++)\\n                  rval += \\' \\';\\n  \\n          rval = len < 0 ? str + rval : rval + str;\\n  \\n          return (rval);\\n  };\\n  \\n  setInterval(function () {\\n          var i;\\n  \\n          sys.puts(pad(\\'EXECNAME\\', -40) + pad(\\'COUNT\\', -10));\\n  \\n          dtp.aggwalk(function (id, key, val) {\\n                  if (!syscalls.hasOwnProperty(key[0]))\\n                  \\tkeys.push(key[0]);\\n  \\n                  syscalls[key[0]] = val;\\n          });\\n  \\n          keys.sort();\\n  \\n          for (i = 0; i < keys.length; i++) {\\n                  sys.puts(pad(keys[i], -40) + pad(syscalls[keys[i]], -10));\\n                  syscalls[keys[i]] = 0;\\n          }\\n  \\n          sys.puts(\\'\\');\\n  }, 1000);\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '101',\n",
       "  'forks': '13',\n",
       "  'commits': '14'},\n",
       " {'language': 'C++ 80.6',\n",
       "  'readme': '<iframe class=\"roundPhoto\" src=\"http://player.vimeo.com/video/20904879?autoplay=1\" width=\"575\" height=\"359\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\\nKinectCoreVision is a refurnished version of the well know track application CommunityCoreVision to use a KinectSensor.\\nThis version of CCV was original designed for a light and portable setup of Communitas multiTouch Table for IEATA´s 9th International Conference (more info and videos about it). But it was lot of potencial.\\nBinaries\\nMacOSX\\n\\nSnow Leopard\\nLion\\n\\nMacports is required to be installed. Please check http://www.macports.org/ for its own dependencies and installation procedure. It requires XCode and would need to be installed on your Mac. Then:\\nsudo port install libtool\\nsudo port install libusb-devel +universal\\nLinux\\n\\nUbuntu 64bit\\n\\nsudo apt-get install libusb-1.0-0-dev\\nWindows\\n\\nWindows XP\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '99',\n",
       "  'forks': '28',\n",
       "  'commits': '28'},\n",
       " {'language': 'C++ 75.8',\n",
       "  'readme': '<iframe class=\"roundPhoto\" src=\"http://player.vimeo.com/video/20904879?autoplay=1\" width=\"575\" height=\"359\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\\nKinectCoreVision is a refurnished version of the well know track application CommunityCoreVision to use a KinectSensor.\\nThis version of CCV was original designed for a light and portable setup of Communitas multiTouch Table for IEATA´s 9th International Conference (more info and videos about it). But it was lot of potencial.\\nBinaries\\nMacOSX\\n\\nSnow Leopard\\nLion\\n\\nMacports is required to be installed. Please check http://www.macports.org/ for its own dependencies and installation procedure. It requires XCode and would need to be installed on your Mac. Then:\\nsudo port install libtool\\nsudo port install libusb-devel +universal\\nLinux\\n\\nUbuntu 64bit\\n\\nsudo apt-get install libusb-1.0-0-dev\\nWindows\\n\\nWindows XP\\n\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '99',\n",
       "  'forks': '7',\n",
       "  'commits': '245'},\n",
       " {'language': 'C++ 95.1',\n",
       "  'readme': 'OpenDetection\\nOpenDetection is a standalone open source project for object detection and recognition in images and 3D point clouds.\\nWebsite - http://opendetection.com or http://krips89.github.io/opendetection_docs\\nGit - https://github.com/krips89/opendetection\\nGetting started - http://opendetection.com/getting_started.html\\nUser guide - http://opendetection.com/introduction_general.html\\nAPI Documentation - http://opendetection.com/annotated.html\\nShort demo video - https://www.youtube.com/watch?v=sp8W0NspY54\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '98',\n",
       "  'forks': '57',\n",
       "  'commits': '58'},\n",
       " {'language': 'C++ 96.6',\n",
       "  'readme': \"ofxLearn\\nofxLearn is a general-purpose machine learning library for OpenFrameworks, built on top of dlib.\\nAbout\\nofxLearn supports classification, regression, and unsupervised clustering. The goal is to be a high-level wrapper for dlib's machine learning routines, taking care of the ugly stuff, i.e. determining a model, kernel, and parameter selection).\\nThe library contains a basic example for each of classification, regression, and clustering. Because training can take a long time, there are also examples for placing each of these tasks into its own separate thread.\\nFeatures\\nofxLearn supports classification (using kernel ridge regression), regression (using kernel ridge or multilayer perceptron (neural network)), and k-means clustering.\\nAlso has an example for doing a principal component analysis via singular value decomposition. See example_pca.\\nEach has a separate class for threading (see the _threaded examples).\\nUsage\\nSee examples for usage of classification, regression, and clustering. Depending on the size and complexity of your data, training can take a long time, and it will freeze the application, unless you use the threaded learners. The examples ending with _threaded run the training in a separate thread and alert you with a callback function when they are done.\\n\",\n",
       "  'watchers': '13',\n",
       "  'stars': '97',\n",
       "  'forks': '21',\n",
       "  'commits': '45'},\n",
       " {'language': 'C++ 70.9',\n",
       "  'readme': \"ofxLearn\\nofxLearn is a general-purpose machine learning library for OpenFrameworks, built on top of dlib.\\nAbout\\nofxLearn supports classification, regression, and unsupervised clustering. The goal is to be a high-level wrapper for dlib's machine learning routines, taking care of the ugly stuff, i.e. determining a model, kernel, and parameter selection).\\nThe library contains a basic example for each of classification, regression, and clustering. Because training can take a long time, there are also examples for placing each of these tasks into its own separate thread.\\nFeatures\\nofxLearn supports classification (using kernel ridge regression), regression (using kernel ridge or multilayer perceptron (neural network)), and k-means clustering.\\nAlso has an example for doing a principal component analysis via singular value decomposition. See example_pca.\\nEach has a separate class for threading (see the _threaded examples).\\nUsage\\nSee examples for usage of classification, regression, and clustering. Depending on the size and complexity of your data, training can take a long time, and it will freeze the application, unless you use the threaded learners. The examples ending with _threaded run the training in a separate thread and alert you with a callback function when they are done.\\n\",\n",
       "  'watchers': '24',\n",
       "  'stars': '94',\n",
       "  'forks': '56',\n",
       "  'commits': '3,903'},\n",
       " {'language': 'C++ 92.7',\n",
       "  'readme': 'mace-makefile-project\\nMace是小米发布的移动端深度学习加速库。但是现在不支持离线编译和arm-linux交叉编译，为了快速验证Mace在嵌入式端的性能，故把MACE的源码和依赖提出来，搞了一个可以使用MakeFile进行交叉编译libmace.a的工程，省去了Bazel编译的麻烦。 本工程主要针对3559A，3536，rk3288（预装ubuntu系统），rk3399（预装ubuntu系统）这类带GPU的嵌入式SOC。\\n1.如果你要在3559A上使用CPU来运行mace框架，那么请依次执行以下命令(3559a平台上的protobuf的库已经编好了)\\ncd mace;\\nvim Makefike\\n将PLATFORM改为CPU\\nmake clean;\\nmake;\\ncp libmace.a ../library/mace_cpu\\ncd unit_test_cpu\\nmake clean;make\\n./demo\\n备注：如何你的交叉编译链不是aarch64-himix100-linux-,请在mace文件夹和unit_test文件夹的的MAKEFILE文件中修改编译链路径。\\n2.如果你要在3559A上使用GPU来运行mace框架，那么请依次执行以下命令：\\ncd mace;\\nvim Makefile;\\n将PLATFORM改为GPU\\nmake clean;make\\ncp libmace.a ../library/mace_gpu\\ncd unit_test_gpu\\nmake clean;make\\n./demo\\n2.如果你要在其他嵌入式平台上运行，请按照以下步骤编译\\n(1).使用对应的交叉编译链先编译好protobuf3.4.0,https://cnbj1.fds.api.xiaomi.com/mace/third-party/protobuf/protobuf-3.4.0.zip\\n(2).https://blog.csdn.net/adrian169/article/details/9051839\\n可以按照这个链接方法来编译\\n(3).把编译好的libprotoc.a和libprotobuf.a和libproto-lite.a拷贝到library/mace/目录下\\n(4).如果是使用CPU来运行mace，接下来按照1方法运行(注意如果是armv7a平台，请在Makefile中加上编译选项-mfloat-abi=softfp -mfpu=neon，硬浮点的话-     mfloat-abi=hard)\\n(5).如果是使用GPU来运行mace（RK3399需要修改mace/Makefile中编译选项-std=c++11改为-std=c++14），参考2，只是在运行demo前，需要先把opencl的库libopencl.so，libmali.so放到opencl_library下。\\n特别需要注意的是，嵌入式端运行GPU需要保证GPU的驱动加载好了，不然运行demo的时候会“Kernel module may not have been loaded”的错误。\\n如果GPU运行时除了libopencl.so，libmali.so之外还有其他动态库，请在mace/core/runtime/opencl/opencl_wrapper.cc的278行加上库路径即可。\\n3.通过unit_test.cpp中可以看出，本工程现在只提供了mobilenetv1的测试。运行mace需要的权重文件和网络配置文件分别为mobilenetv1_v1.data和mobilenetv1.proto。特别需要注意的是部署在gpu上的model和部署到cpu上的model是不能混用的。主要原因是因为gpu模型运算是f16，cpu是f32。\\n4.如果你需要测试其他网络，需要把caffe或者tensorflow模型转换成*.data和*.pb文件，具体怎么做请参    考https://mace.readthedocs.io/en/latest/getting_started/how_to_build.html\\n模型转换ok之后，请修改unit_test.cpp中对应代码。\\n5.https://github.com/conansherry/convert_model\\n这个链接可以转caffe模型，不需要docker那一套东西\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '94',\n",
       "  'forks': '15',\n",
       "  'commits': '46'},\n",
       " {'language': 'JavaScript 93.8',\n",
       "  'readme': 'nyroModal jQuery Plugin\\nTested in all majors browsers:\\n\\nFirefox\\nChrome\\nSafari\\nInternet Explorer\\n\\n##License\\nnyroModal is licensed under the MIT License.\\n##How to use it\\nGo to http://nyromodal.nyrodev.com/ and download your package.\\nYou need to include jQuery on your page.\\nThen you can include jquery.nyromodal.custom.js on your page.\\nInclude styles/nyroModal.css. You will also need the images stored in the img folder.\\nTo add compabitibilty with IE6, you have to add the following :\\n<!--[if IE 6]>\\n  <script type=\"text/javascript\" src=\"js/jquery.nyroModal-ie6.js\"></script>\\n<![endif]-->\\n\\nThen, add the nyroModal functionnality to your links:\\n$(\\'YOUR SELECTOR\\').nyroModal();\\n\\nThis will work on many kind of elements, depending of what you included as nyroModal filters (see below)\\n##Functions available\\nnyroModal comes with a lot of functions that tou could use through your programming.\\n###$.fn.nyroModal(opts, fullObj);\\nThis is obviously the most useful. It enables the nyroModal functionnality on the element when clicked or submitted for a form.\\nYou have to call it to add the bind. The most usual place might be on a ready page event like:\\n$(function() {\\n  $(\\'.nyroModal\\').nyroModal();\\n});\\n\\nParameters:\\n\\nopts: the options parameter might be used in order to change the setting for the specified nyroModal objects\\nfullObj: a boolean to indicate if the options parameter is actually a fully nyroModal object\\n\\n###$.fn.nm(opts, fullObj);\\nThis is simply a shortcut for the nyroModal function, they both do exactly the same.\\nFor more information, see description above.\\n###$.fn.nmInit(opts, fullObj);\\nThis function is used in order to initliaze the nyroModal object as data of DOM node.\\nThis function will result by create a data(\\'nmObj\\') on the jQuery object representing the DOM node.\\nThis function is used internally, it shouldn\\'t be use in most of case.\\nUsage example:\\n$(function() {\\n  $(\\'.nyroModal\\').nmInit();\\n});\\n\\nParameters:\\n\\nopts: the options parameter might be used in order to change the setting for the specified nyroModal objects\\nfullObj: a boolean to indicate if the options parameter is actually a fully nyroModal object\\n\\n###$.fn.nmCall();\\nThis function is used to open the nyroModal on a DOM element that already have been initalised with the nyroModal or nm functions.\\nUsage example:\\n$(\\'#myLink\\').nmCall();\\n\\nNote: This code has exaclty the same effect than triggering a nyroModal event like:\\n$(\\'#myLink\\').trigger(\\'nyroModal\\');\\n\\n###$.nmManual(url, opts);\\nThis function is used to manually open a nyroModal by giving only an url.\\nTo use it, the Link Filter is required.\\nExample usage:\\n$.nmManual(\\'page.html\\');\\n\\nParameters:\\n\\nurl: The url to open. it might a DOM selector or anything else\\nopts: the options parameter might be used in order to change the setting for the specified nyroModal objects\\n\\n###$.nmData(data, opts);\\nThis function is used to manually open a nyroModal by giving it\\'s content.\\nTo use it, the Link Filter and the Data Filter are required.\\nExample usage:\\n$.nmData(\\'my content<br />is so greaaaaaaaaaaaat !\\');\\n\\nParameters:\\n\\ndata: The data to be shown inside the modal\\nopts: the options parameter might be used in order to change the setting for the specified nyroModal objects\\n\\n###$.nmObj(opts);\\nThis is used to overwrite the default function or settings of the nyroModal object created when initialising a nyroModal Object.\\nThis could be used to change the default animations to use or change the default behavior of the nyroModal.\\nExemple usage:\\n$(function() {\\n  $.nmObj({anim: {def: \\'fade\\'}});\\n});\\n\\n###$.nmInternal(opts);\\nThis is used to overwrite internal object of nyroModal used in many places in the code.\\nThis could be used to change the default behavior of nyroModal.\\nExemple usage in jquery.nyroModal-ie6 in order to calculate the size according to this browser:\\n$.nmInternal({\\n  _calculateFullSize: function() {\\n\\tvar scrollHeight = Math.max(\\n\\t  document.documentElement.scrollHeight,\\n\\t  document.body.scrollHeight\\n\\t),\\n\\toffsetHeight = Math.max(\\n\\t  document.documentElement.offsetHeight,\\n\\t  document.body.offsetHeight\\n\\t),\\n\\tscrollWidth = Math.max(\\n\\t  document.documentElement.scrollWidth,\\n\\t  document.body.scrollWidth\\n\\t),\\n\\toffsetWidth = Math.max(\\n\\t  document.documentElement.offsetWidth,\\n\\t  document.body.offsetWidth\\n\\t);\\n\\tthis.fullSize = {\\n\\t  wW: $w.width(),\\n\\t  wH: $w.height()\\n\\t};\\n\\tthis.fullSize.h = scrollHeight < offsetHeight ? $w.height() : scrollHeight;\\n\\tthis.fullSize.w = scrollWidth < offsetWidth ? $w.width() : scrollWidth;\\n\\tthis.fullSize.viewW = Math.min(this.fullSize.w, this.fullSize.wW);\\n\\tthis.fullSize.viewH = Math.min(this.fullSize.h, this.fullSize.wH);\\n  }\\n});\\n\\n###$.nmAnims(opts);\\nThis is used to add or overwrite animations pack.\\nSee js/jquery.nyroModal.anims.fade.js to see how the code looks like.\\nSee animations section below to learn how it should be written an used.\\n###$.nmFilters(opts);\\nThis is used to add or overwrite filters pack.\\nThis might be the most useful part of nyroModal as it allow devleopper to create a totally new behavior on it\\'s own, without breaking all others.\\nSee filters section below to learn how it should be written and used.\\n###$.nmTop();\\nAs nyroModal allow multiple nyroModal to be opened at the same time, this function return the nyroModal Object of the last opened modal, ie the modal at the top.\\nIf nothing is open, undefined will be returned.\\n##Selectors\\nnyroModal defines 3 selectors that could be used transparently in your code like any others jQuery selectors.\\n###:nyroModal\\nThis selector retrieves all elements that was binded using the nyroModal() function.\\nFor instance, if you want to open all modal at the same time (not recommended), you can do:\\n$(\\':nyroModal\\').nmCall();\\n\\n###:nm\\nThis is an alias for the previous selector, :nyroModal.\\nthey both do exactly the same.\\n###:nmOpen\\nThis selector retrieves all modals that are curently opened.\\nThis could be useful to close them all at the same time:\\n$(\\':nmOpen\\').each(function() {\\n  $(this).data(\\'nmObj\\').close();\\n}):\\n\\nnyroModal Object\\nDescription of what is, how and when to use it.\\nList of all of settings and functions\\n##Filters\\nFilters are used to provide many basics functionnality like Ajax download, but it alos let you the ability to fully customize the way nyroModal work, by adding as many callbacks as you need.\\nA filter is a set function that will be called by nyroModal core at many differents points. These functions call be used to do everything needed to make the modal work as you excpet or to add others behaviors needed for your projet: add some elements inside the modal, updating your page content, add some Ajax call, etc...\\nHere is how filters works:\\n\\n\\nWhen $(\\'element\\').nyroModal(); is called, the nyroModal object is created and attached to the DOM element instancied\\n\\n\\nEvery available filters are tested again the nyroModal object using the is function\\n\\nIf this function returns true, the filters will be used all the time for this element.\\nIf it returns false, the filters will not be used anymore for this element.\\n\\n\\n\\nnyroModal core calls every init function of every filters selected for the element\\n\\nThese init function should bind the event they need, create their own object if needed.\\nIf the filters need to be the one used to load the content, the loadFilter setting of the nyroModal ovject should be defined to its name.\\n\\n\\n\\nThrougout the living of nyroModal, every function or callback defined in all selected filters will be called (see the list below)\\n\\n\\nAll function or callbacks receive the same nyroModal object as a unique parameter.\\nThe list of all function or callback that can be called in a filter:\\n\\nis: should return true or false to select the filter for the current or element or not. This function is REQUIRED !\\ninitFilters: called just after every filters have been determined to use or not, and just before the init of them. Good place to force filters.\\ninit: called at the very beggining of the process. This function should bind element or create object needed later\\ninitElts: called at the beggining of the open process, juste before the load start. After that, all the needed div are created and attached to the DOM\\nload: called only for ONE filter defined in nm.loadFilter attribute. This function should load the function and set it\\'s content using the _setCont function\\nfilledContent: called once the content is placed on the hidden div\\nerror: called in case of error (URL not founr for example)\\nsize: called after the size has been defined\\nclose: called at the end of the closing process, regarding only the data (useful for gallery)\\nbeforeClose: called at the beggining of the closing process, regarding the animation\\nafterClose: called at the end of the closing process, regarding the animation\\nkeyHandle: called when a key is hit if the keyHandle is enable and if the modal is on the top\\n\\nLike the version 1, there is a bunch of others callback that you can define before and after every animation. The version 2 put it in a new way by letting you the ability to define a function before and after every animation function.\\n\\nbeforeShowBg: called just before the showBg animation\\nafterShowBg: called just after the showBg animation\\nbeforeHideBg: called just before the hideBg animation\\nafterHideBg: called just after the hideBg animation\\nbeforeShowLoad: called just before the showLoad animation\\nafterShowLoad: called just after the showLoad animation\\nbeforeHideLoad: called just before the hideLoad animation\\nafterHideLoad: called just after the hideLoad animation\\nbeforeShowCont: called just before the showCont animation (also called in case of a transition, before beforeHideTrans)\\nafterShowCont: called just after the showCont animation (also called in case of a transition, after afterHideTrans)\\nafterReposition: called just after the .nmRepositon have been placed\\nafterUnreposition: called just after the .nmRepositon have been replaced on their initial positions\\nbeforeHideCont: called just before the hideCont animation\\nafterHideCont: called just after the hideCont animation\\nbeforeShowTrans: called just before the showTrans animation (transition)\\nafterShowTrans: called just after the showTrans animation (transition)\\nbeforeHideTrans: called just before the hideTrans animation (transition)\\nafterHideTrans: called just after the hideTrans animation (transition)\\nbeforeResize: called just before the resize animation\\nafterresize: called just after the resize animation\\n\\nnyroModal automatically add 2 filters for every elements:\\n\\nbasic: used to provide basic functionnality. This filter shouldn\\'t be modified unless you know exactly what you\\'re doing\\ncustom: This filter has absolutely no programming. It\\'s added here juste to provide a quick way to add your filters callback\\n\\nThere is some some basic filters that you can use on your project that enable the basic usage of this kind of plugin.\\nHere is a list of the filters officially provided:\\n\\nTitle: Show the title for the modal using the title attribute of the opener element\\nGallery: Enable arrows and navigation keys trough the element with the same rel attribute defined in the DOM (depends on title)\\nLink: Bind an anchor element and load the target using Ajax to retrieve content\\nDom: Instead of using Ajax, it get the content within the page using an id. (exemple link : ) (depends on link)\\nImage: Show an image, and resize it if needed (depends on link)\\nSWF: Show a SWf animation (depends on link)\\nForm: Bind a form and load the target by sending it\\'s data trhough Ajax\\nForm file: Same as Form but used when form should send file\\nIframe: Used to show data from an other domain name or when target=\"_blank\" is present (depends on link)\\nIframe form: Used to open a form in an iframe (depends iframe)\\nEmbedly: Used to show a bunch of different element using embedly API (Examples) (depends on link) Embedly requires developers to sign up for an API key at embedly Pricing\\n\\n##Animations\\n##Debug\\nIn order to enable the debug mode, you should change the debug value of the internal object. You can do it like:\\n$.nmInternal({debug: true});\\n\\nBy default, the debug function show function name in the console. If you want do something else, you should overwrite the _debug function like:\\n$.nmInternal({\\n  _debug: function(msg) {\\n\\tif (this.debug && window.console && window.console.log)\\n\\t  window.console.log(msg);\\n  }\\n});\\n\\n##Help\\nYou can find some useful tips on the issues labeled with tips\\nIf it\\'s still not enought, be free to open an Issue on github.\\n',\n",
       "  'watchers': '17',\n",
       "  'stars': '236',\n",
       "  'forks': '76',\n",
       "  'commits': '91'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': '\\nOther Options\\nMaxArt2501 has started his own Object.observe polyfill, and a look at his commit history and reasoning makes me think it will probably be well supported.  If your looking for an Object.observe solution you should take a look at https://github.com/MaxArt2501/object-observe\\nNeeds a maintainer\\nI\\'d like to find someone who is willing to take this library over.  I\\'ve had no time to work with and/or maintain the library.  Honestly I have little need for Object.observe making it hard to justify time spent against it.\\nSo, if your using this shim, feel comfortable with the code, and would like to maintain it, let me know.\\nIf your curious why I don\\'t have much use for the library, this http://markdalgleish.github.io/presentation-a-state-of-change-object-observe/ pretty well sums it up.  I worked in native development when two way data binding caused all sorts of issues with application development, and I see the same coming out of Object.observe at the end of the day.\\nHopefully, someone will want to maintain this work in the future.\\nOh, and if you take it over, feel free to relicense it within reason as it seems no one likes unlicense :).  Also feel free to follow up on the polyfill-service integration if you so see fit (https://github.com/Financial-Times/polyfill-service/pull/81#issuecomment-66382432).\\nObject.observe Polyfill/shim\\nThanks to my new job I have a lot more time to devote to things like this library.  It has gone a REALLY long time without updates and there is a lot that can be done to make it more functional.  I hope to be spending more time on it soon, but for now I\\'ve fixed all the bugs that I know of and have been reported.  Thanks to everyone for their reports, and keep them coming if you find one.\\nStarted as GIST: https://gist.github.com/4173299\\nTested against Chromium build with Object.observe and acts EXACTLY the same for the basics, though Chromium build is MUCH faster.\\nTrying to stay as close to the spec as possible, this is a work in progress, feel free to comment/update\\nhttp://wiki.ecmascript.org/doku.php?id=harmony:observe\\nTODO\\nThe spec has changed a lot since I origionally wrote this, need to go back and add in a lot of things like custom update types and other fun.  For now though it seems to suffice.\\nLimits so far\\nBuilt using polling in combination with getter&setters.  This means that if you do something quick enough it won\\'t get caught.\\nExample:\\nvar myObject = {};\\nObject.observe(myObject, console.log);\\nmyObject.foo = \"bar\";\\ndelete myObject.foo;\\nThe observer function would never see the addition of foo since it was deleted so quickly.\\nTesting\\nTo run the tests first make sure you have Node.js installed.  Then use NPM to install Karam and all dependencies:\\nnpm install\\n\\nFinally run the tests with:\\nnpm test\\n\\nPlanned Updates\\nFor FireFox using Proxies will result in better performance.  Will look into this more.\\nExamples and Usage\\nSee examples folder\\nLatest Updates\\n* Memory leak fixed with PR #16\\n* Tests added to project thanks to mikeb1rd also part of PR #16\\n* Added Notifier.notify() with custom types support by klimlee\\n* Added accept list support by klimlee\\n* Stopped monitoring DOM nodes, Canary can\\'t do it and neither should the shim.\\n* Added in support for setImmediate if it is available.\\n* Memory leak fix by Moshemal\\n* Array length now reports as an update when it changes\\n* Added enumerable flag to defineProperty\\n\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '234',\n",
       "  'forks': '36',\n",
       "  'commits': '43'},\n",
       " {'language': 'JavaScript 91.9',\n",
       "  'readme': 'TransformJS 1.0 Beta\\n2D and 3D transforms as regular CSS properties you can set using .css() and animate using .animate()\\nOverview\\nCSS Transforms  were first introduced in WebKit in 2007, and have now\\nreached mass-adoption by all the major browser vendors. This is great news\\nfor web developers, especially in the case of 3D transforms which are\\nhardware-accelerated, resulting in extremely smooth animations and\\nresponsive applications.\\nThe API for applying transforms however, does not scale to complex applications\\nwhich require intricate and complex management of transformations. TransformJS\\nattempts to identify and address these problems, allowing developers to\\nmake use of transforms without having to be encumbered by cross browser\\nissues, and low-level APIs.\\nHere\\'s a snippet of code that uses TransformJS to apply multiple 3d\\ntransformations to the same element, relative to their current value,\\nand animate the changes:\\n    $(\\'#test\\').animate({\\n      translateY:\\'+=150\\',\\n      scale:\\'+=2\\',\\n      rotateY: \\'+=6.24\\',\\n      rotateX: \\'+=3.15\\',\\n      rotateZ: \\'+=3.15\\'\\n    },1500);    \\nFor more detailed usage and overview information, please visit the\\nproject homepage at http://transformjs.strobeapp.com\\nLicense\\n  \\n    Copyright (c) 2011 Strobe Inc.\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n\\n\\nBack to top\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '231',\n",
       "  'forks': '23',\n",
       "  'commits': '16'},\n",
       " {'language': 'JavaScript 95.6',\n",
       "  'readme': 'DEPRECATED, I recommend you the tool SVGR\\n\\nreact-svg-inline\\n\\n\\nA react component to clean and display raw SVGs.\\n\\nInstall\\n$ npm install react-svg-inline\\nYou might also need to npm install raw-loader if you want to use this with\\nwebpack.\\nUsage\\nHere is an example of a usage in a React stateless component:\\nimport React from \"react\"\\nimport SVGInline from \"react-svg-inline\"\\n\\nexport default () => (\\n  <div>\\n    <SVGInline svg={\"<svg....</svg>\"} />\\n  </div>\\n)\\nWebpack to require() SVGs\\nUse the raw-loader to require() raw SVGs files and pass them to\\nreact-svg-inline.\\nmodule.exports = {\\n  loaders: [\\n    {\\n      test: /\\\\.svg$/,\\n      loader: \\'raw-loader\\'\\n    }\\n  ]\\n}\\nimport React from \"react\"\\nimport SVGInline from \"react-svg-inline\"\\nimport iconSVG from \"./myicon.svg\"\\n\\nexport default () => (\\n  <div>\\n    <SVGInline svg={ iconSVG } />\\n  </div>\\n)\\nOptions (props)\\nclassName\\nPropTypes.string\\nClass name used for the component that will wrap the SVG.\\nclassSuffix\\nPropTypes.string\\nThe class suffix that will be added to the svg className (default: \"-svg\").\\ncomponent\\nPropTypes.oneOfType([ PropTypes.string, PropTypes.func, ]),\\nThe component that will wrap the svg (default: span).\\nsvg\\nPropTypes.string.isRequired\\nfill\\nPropTypes.string\\nColor to use\\ncleanup\\nPropTypes.oneOfType([ PropTypes.bool, PropTypes.array, ])\\nThis allow you to cleanup (remove) some svg attributes.\\nHere are the supported value that can be removed:\\n\\ntitle\\ndesc\\ncomment\\ndefs\\nwidth\\nheight\\nfill\\nsketchMSShapeGroup\\nsketchMSPage\\nsketchMSLayerGroup\\n\\nIf cleanup === true, it will remove all the attributes above.\\ncleanupExceptions\\nPropTypes.array\\nThis allow you to whitelist some svg attributes to keep while cleaning some\\nothers.\\nwidth\\nPropTypes.string\\nheight\\nPropTypes.string\\naccessibilityLabel\\nPropTypes.string\\nThis value is added as an svg <title> element that is accessible to screen readers.\\n(Note: when this option is used, an SVG id attribute will be automatically injected).\\naccessibilityDesc\\nPropTypes.string\\nThis value is added as an svg <desc> element that is accessible to screen readers.\\n\\nCONTRIBUTING\\n\\n⇄ Pull requests and ★ Stars are always welcome.\\nFor bugs and feature requests, please create an issue.\\nPull requests must be accompanied by passing automated tests ($ npm test).\\n\\nCHANGELOG\\nLICENSE\\n',\n",
       "  'watchers': '1',\n",
       "  'stars': '231',\n",
       "  'forks': '21',\n",
       "  'commits': '24'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Intelligist\\na jQuery plugin that makes it easy to share multiple, executable GitHub gists\\nDemo\\nSee Intelligist in action\\nRequirements\\njQuery\\nChosen (optional, it is auto-downloaded if not already included on the page)\\nHow to use\\nUsing Intelligist is easy. The general idea is:\\n$(selector).intelligist( gists, options );\\n\\nJust select the container, and pass in an object of gists that you\\'d like to display.\\n$(\"#demo\").intelligist({\\n      \"1973984\": \"Welcome to Intelligist\"\\n    , \"1973990\": \"Live CSS preview\"\\n    , \"1973575\": \"Live JS preview\"\\n}, { exec: true });\\n\\nThe object is made of keys (the gist ID) and values (titles for the drop-down menu).\\nNote: In this example, we are setting the \"exec\" option to true. This instructs Intelligist to execute the code after the gist is displayed. It is optional, and disabled by default.\\nImportant: If you are using the \"exec\" option, your Gist must be set to the correct language. Intelligist uses the Gist language to determine how it should execute the code.\\nOptions\\nexec\\nShould the gist be executed after it is loaded onto the page? Note: If you are using this option, your Gist must be set to the correct language (e.g. CSS or JavaScript) (default=false)\\nbefore\\nA function to be called before the gist is loaded onto the page. It is passed two variables, the ID of the previous gist and the ID of the newly selected gist. e.g. function(oldGistId, newGistId) {}\\nafter\\nA function to be called after the gist is loaded onto the page. It is passed one variable, the ID of the newly selected gist. e.g. function(newGistId) {}\\nThank yous\\n\\nGitHub: for creating the Gists service\\nHarvest: for creating Chosen\\nMartin Angelov: whose Shuffle Letters plugin is being used in the demo\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '230',\n",
       "  'forks': '18',\n",
       "  'commits': '6'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'AlphaBeta\\nAlphaBeta lets you build split tests (A/B tests) directly into\\nyour React app.\\nAlphaBeta is...\\n\\ndeclarative: Like React itself, AlphaBeta benefits from the advantages of declarative programming. The AlphaBeta component is, in fact, just a special type of React component that \"wraps\" the component variants you\\'re testing.\\nlightweight: The AlphaBeta package is small and the AlphaBeta component is thin, so AlphaBeta tests won\\'t measurably increase the time it takes for your application to render. Since AlphaBeta is so lightweight, you can be less selective about what you choose to test - it may even make sense to run single-variant tests, wrapping components you may test in the future in an AlphaBeta component today in order to establish a baseline for comparison.\\nbackend agnostic: Since split testing requires that you store event data, AlphaBeta needs to communicate with a datastore in order to work. But AlphaBeta will work with whatever datastore you\\'re currently using - just follow the instructions in Backend / API Setup to build your endpoint, point AlphaBeta to it, and you\\'re good to go.\\nextensible: AlphaBeta is designed to make it easy for developers to integrate basic split tests into their React apps without having to think about the underlying statistics. But it\\'s also possible to build your own custom logic around how confidence intervals are calculated and how user cohorting works within your app.\\n\\nBuilding your first A/B test is simple:\\nimport { ABComponent } from \\'react-alpha-beta\\';\\n\\nclass ButtonA extends React.Component {\\n  render() {\\n    return (<Button onClick={this.props.successAction}\\n                    style={{\\'background-color\\':\\'blue\\'}}>\\n              \"Sign Up\"\\n            </Button>);\\n  }\\n}\\n\\nclass ButtonB extends React.Component {\\n  render() {\\n    return (<Button onClick={this.props.successAction}\\n                    style={{\\'background-color\\':\\'orange\\'}}>\\n              \"Sign Up\"\\n            </Button>);\\n  }\\n}\\n\\nclass Page extends React.Component {\\n    render() {\\n        return (\\n            <div>\\n                <ABComponent\\n                    ComponentA={ButtonA}\\n                    ComponentB={ButtonB}\\n                    experimentParams={{\\n                      id: \\'1\\',\\n                      testCohortSize: 0.4,\\n                    }}\\n                />\\n            </div>\\n        );\\n    }\\n}\\n\\nReactDOM.render(\\n    <Page />,\\n    document.getElementById(\\'app\\')\\n);\\nYour experiment results will look something like this:\\n{\\n  meanDifferenceValue: -0.05023923444976075,\\n  marginOfError: 0.04837299277280508,\\n  statisticalSignificance: true,\\n  details: \"Our best estimate is that the absolute rate of success is 5% lower with variant B, and this result is statistically significant (We are 95% confident the true difference is between -10% and 0%.). Given this information, you should probably stick with variant A.\",\\n}\\nInstallation\\n$ npm install react-alpha-beta --save\\nOverview and Basic Usage\\nThe AlphaBeta component is a React component that \"wraps\" two other components. These two \"wrapped\" components are passed as ComponentA and ComponentB, and they represent the two variants you\\'re testing. Each user that encounters the AlphaBeta component will see one of the two variants, and the AlphaBeta component will report back to your server (i) which variant was displayed and (ii) if a success event occurred.\\nIn addition to your ComponentA and ComponentB variants, you\\'ll also pass experimentParams to each of your AlphaBeta components. experimentParams is an object containing the keys id and testCohortSize.\\nid is the unique id of a particular experiment, and is passed by the AlphaBeta component to your Backend / API (described below). Each AlphaBeta component that you declare should have a unique id associated with it.\\ntestCohortSize is a number between 0.0 and 1.0. Its value tells your AlphaBeta component what proportion of your users will see each experiment variant. A testCohortSize value of 0.01 means that 1% of your users should see the ComponentB variant - the other 99% should see ComponentA. A value of .5 indicates that there should be an even split between the two variants.\\nWhen you wrap your variants in an AlphaBeta component, the AlphaBeta component passes the prop successAction to each of them.\\nYou get to decide what constitutes \"success\" in the context of your experiment. If you\\'re testing a button variation, \"success\" might be defined as a click (this is the case in the above code sample). If you\\'re testing a landing page variation, \"success\" might be defined as submitting a validated form.\\nNote that while you have the ability to define \"success\" however you want, it is also your responsibility to make sure that successAction is fired by each of your variants when \"success\" occurs. Otherwise, AlphaBeta will have no way of giving you guidance about which variant is more likely to produce the desired outcome.\\nThe Button example is designed to help you get comfortable using the AlphaBeta component. You\\'ll need to set up your Backend / API for the example to work (instructions below), but reading through the example may help you better understand how to use AlphaBeta, even prior to fully setting things up.\\nBackend / API Setup\\nWait, AlphaBeta Needs a Backend?\\nIn order for AlphaBeta to be useful, it needs to be able to record data about the experiments you\\'re running. In other words, it needs to be linked to a datastore of some type. This reliance on a datastore isn\\'t unique to AlphaBeta - it is true of split testing in general.\\nImagine that you\\'re running an experiment to see if changing a particular button from a transparent background (variant A) to a solid blue background (variant B) leads to more clicks. (If you already looked at the Button example, this should look familiar...)\\nTo measure which variant performs better, you need to keep track of each variant\\'s \"impressions\" (how many users have seen each button) and \"conversions\" (how many times each button is clicked).\\nWhen we are able to keep track of these values, all it takes is a bit of math to estimate (within a specific range or \"confidence interval\") which button leads to more conversions. AlphaBeta handles this math for you, but you\\'re responsible for logging the events themselves in your datastore.\\nSo How Do I Set Up My AlphaBeta Endpoint?\\nYou can connect AlphaBeta to a datastore you\\'re already using in two steps.\\nStep 1:\\nSet up an API endpoint for AlphaBeta to Consume\\nAlphaBeta expects to be able to interact with an endpoint at www.yoursite.com/api/alphabeta/{{experimentId}}/, where {{experimentId}} is the unique id you pass to each AlphaBeta component in experimentParams.\\nAlphaBeta will both POST to and GET from this endpoint. When AlphaBeta detects an \"impression\" or a \"conversion\", it will POST to this endpoint, so all users who may encounter an experiment should be able to POST to this endpoint.\\nYou can safely restrict GET requests to only allow access to users who should be able to see data about your experiments.\\nIt\\'s also a good idea (though not strictly necessary) to set up your endpoint such that GET requests made without an {{experimentId}} return a list of your experiments. This is a good idea if you wish to build a single page where you can view data about all of our experiments.\\nEnsure Your Endpoint Accepts POST Requests Correctly\\nWhen AlphaBeta POST data to your endpoint, the POST body should look like this:\\n{\\n    variant: \"a\",             // this will either be \"a\" or \"b\"\\n    success: null,            // this will either be null or true\\n    userCohortValue: .10392,  // a number between 0 and 1\\n    metaId: null,             // this will be null unless you choose to set it\\n}\\n\\n\\nvariant tells your datastore which component variant (A or B) was presented to a particular user.\\n\\n\\nsuccess tells your datastore whether the success event occurred (true) or not (null).\\n(Note that the value for this parameter will either be true or null, as opposed to true or false. When success is passed as null, that signals that an impression has occurred. It is passed as null because when the component is loaded we don\\'t know if the user will trigger the success event or not. When success is passed as true, that signals that a success event has occurred.)\\n\\n\\nuserCohortValue is a number between 0.0 and 1.0 that AlphaBeta has associated with the particular user in this experiment. This number is randomly generated the first time a user encounters a particular AlphaBeta experiment, and is core to how AlphaBeta separates users into cohorts.\\n\\n\\nmetaId is a value that you can optionally pass to your AlphaBeta component. It should be used in cases where the component that you\\'re testing occurs more than one time times on your site.\\n\\n\\nHere is an example of when you would set the metaId attribute:\\nImagine you instead were testing the copy on a Facebook-style \"like\" button to see if changing \"like\" to \"+1\" led to more engagement. Each piece of content a user views in his/her news feed should have a \"like\" (or \"+1) button below it. But since each user has multiple items in his/her feed, a single user could \"like\" more than one piece of content.\\nIn this case, you could set a metaId that uniquely identifies the piece of content being \"liked\". If you were to set the metaId, you would be testing which variant leads to more total likes per unit of content seen. If you were to not set the metaId, you would be testing which variant is more likely to lead to a user liking at least one piece of content.\\nEnsure Your Endpoint Responds to GET Requests Correctly\\nWhen AlphaBeta GETs data from your endpoint, the returned data should look like this\\n{\\n  variantA: {\\n    trialCount:   291,  // the number of unique impressions for this variant\\n    successCount: 59,   // the number of unique success events for this variant\\n  },\\n  variantB: {\\n    trialCount:   101,\\n    successCount: 22,\\n  },\\n  confidenceInterval: .95 // the CI you\\'re looking to achieve, expressed as a float\\n}\\nStep 2:\\nEnsure Your Back End Processes POST Requests Correctly\\nWhen POST data is received, one of three things is supposed to happen:\\n1 - the trialCount for an experiment variant could be incremented by 1.\\n2 - the successCount for an experiment could be incremented by 1.\\n3 - Nothing at all.\\nThe logic for what should happen must be executed by your application\\'s backend. Here\\'s how things should work:\\n\\n\\nif success === null and no previous trial exists where both userCohortValue and metaId are equal to this trial\\'s values, you should increment trialCount by one for the appropriate variant.\\n\\n\\nif success === true and no previous trial exists where both userCohortValue and metaId are equal to this trial\\'s values and success === true, you should increment successCount by one for the appropriate variant.\\n\\n\\nin all other cases, you should not take any action.\\n\\n\\nChecking Your Experiment Results\\nimport { getExperimentData } from \\'react-alpha-beta\\';\\n\\nconsole.log(getExperimentData(experimentId));\\nTo view your experiment results, call the getExperimentData function with the experimentId for a particular experiment. It will return a json object with the keys meanDifferenceValue, marginOfError, statisticalSignificance, and details.\\n\\nmeanDifferenceValue is AlphaBeta\\'s best estimate (or mean estimate) of ComponentB\\'s performance relative to ComponentA. A positive number indicates that ComponentB is leading to more successActions per impression than ComponentA, while a negative number indicates the opposite.\\n\\nNote that meanDifferenceValue alone doesn\\'t mean much if the experiment hasn\\'t yet reached statistical significance.\\n\\n\\nmarginOfError is the margin of error (or uncertainty) that exists in the experiment.\\n\\n\\nstatisticalSignificance, a boolean, represents whether this experiment has yet reached statistical significance at the level of confidence you defined.\\n\\n\\ndetails is a human readable description of this experiment\\'s current results.\\n\\n\\nSample result from getExperimentData:\\n{\\n  meanDifferenceValue: -0.05023923444976075,\\n  marginOfError: 0.04837299277280508,\\n  statisticalSignificance: true,\\n  details: \"Our best estimate is that the absolute rate of success is 5% lower with variant B, and this result is statistically significant (We are 95% confident the true difference is between -10% and 0%.). Given this information, you should probably stick with variant A.\",\\n}\\n\\nExample\\nNote: in order for this example to work, you must first set up an API endpoint for AlphaBeta to consume. If you haven\\'t done this yet, follow the steps in Backend / API Setup\\n\\nButton example: Set up an experiment to see which of two button variants has a greater click-through rate. This example covers (i) basic experiment setup, (ii) the two ways to pass your variant components to the AlphaBeta component, and (iii) basic usage of the AlphaBeta DevTools.\\n\\nAlphaBeta DevTools\\nAlphaBeta comes with a DevTools component that can be used on any page containing an experiment.\\nimport { ABComponent, DevTools } from \\'react-alpha-beta\\';\\n\\n// ***\\n// Build your experiment component, which we\\'ll call <Page /> here.\\n// Make sure that <DevTools /> is in your <Page /> component.\\n// ***\\n\\nReactDOM.render(\\n    <Page />,\\n    document.getElementById(\\'app\\')\\n);\\nOne easy way to familiarize yourself with the DevTools component is to load the Button example.\\nIf the DevTools component is included on your page and you are not in a production environment (i.e. process.env.NODE_ENV !== \\'production\\'), you should see a DevTools box in the lower right hand corner of your screen. This box lets you control your user cohort value for each of the experiments on the page. Recall that the user cohort value for an experiment, along with the testCohortSize parameter, determine which variant a user sees.\\nIf the user cohort value is greater than or equal to testCohortSize, the user will see variant A for this experiment. If the user cohort value is less than testCohortSize, the user will see variant B. When you manipulate the DevTools sliders, you are changing your user cohort value for an experiment. These changes will take place when you refresh the page.\\nYou can add the DevTools component to the lower level components that contain your experiments, or to higher level components of your application.\\nDiscussion and Support\\nJoin our Slack team!\\nAdditional Resources\\n\\nA/B testing course (Udacity)\\nHypothesis testing with one sample (Khan Academy)\\n\\nLint\\n$ npm run lint\\nTest\\n$ npm run test        # run once\\n$ npm run test:watch  # continuous testing as file changes\\n$ npm run test:cov    # generate test coverage report\\nContribute\\nWe are using commitizen to make commit format consistent.\\n# Install the command line tool.\\n$ npm install -g commitizen\\n\\n# From then on, whenever you would like to commit:\\n$ git add .\\n$ git cz\\n# ... follow the prompt messages\\nLicense\\nMIT\\nCredits\\n\\nJack McCloy\\nBrian Park\\nBen Hall\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '222',\n",
       "  'forks': '6',\n",
       "  'commits': '170'},\n",
       " {'language': 'JavaScript 92.8',\n",
       "  'readme': 'Famous Examples\\n##DEPRECATED: please find our examples inside of our main repo\\nLicense\\nAll the code in the src/examples folder is licensed under the MIT license. This is basically a better MIT license since it removes the ambiguous language from the original MIT.\\nThe famous framework source code found in /src/famous is a git submodule cloned from the famous/famous git repo, and is licensed only under the MPL-2.0 license. More information about the licensing of that code can be found in the original repo.\\nMIT License (everything in src/examples)\\nCopyright (c) 2014 Famous Industries, Inc.\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nMPL-2.0 (everything in src/famous)\\nCopyright (c) 2014 Famous Industries, Inc.\\nThis Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\\n',\n",
       "  'watchers': '57',\n",
       "  'stars': '217',\n",
       "  'forks': '75',\n",
       "  'commits': '214'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Co Mocha\\n\\n\\n\\n\\n\\nEnable support for generators in Mocha tests using co.\\nUse the --harmony-generators flag when running node 0.11.x to access generator functions, or transpile your tests using traceur or regenerator.\\nInstallation\\nnpm install co-mocha --save-dev\\n\\nUsage\\nJust require the module in your tests and start writing generators in your tests.\\nit(\\'should do something\\', function * () {\\n  yield users.load(123)\\n})\\nNode\\nInstall the module using npm install co-mocha --save-dev. Now just require the module to automatically monkey patch any available mocha instances. With mocha, you have multiple ways of requiring the module - add --require co-mocha to your mocha.opts or add require(\\'co-mocha\\') inside your main test file.\\nIf you need to monkey patch a different mocha instance you can use the library directly:\\nvar mocha = require(\\'mocha\\')\\nvar coMocha = require(\\'co-mocha\\')\\n\\ncoMocha(mocha)\\n<script> Tag\\n<script src=\"co-mocha.js\"></script>\\nIncluding the browserified script will automatically patch window.Mocha. Just make sure you include it after mocha.js. If that is not possible the library exposes window.coMocha, which can be used (window.coMocha(window.Mocha)).\\nAMD\\nSame details as the script, but using AMD requires instead.\\nHow It Works\\nThe module monkey patches the Runnable.prototype.run method of mocha to enable generators. In contrast to other npm packages, co-mocha extends mocha at runtime - allowing you to use any compatible mocha version.\\nLicense\\nMIT\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '217',\n",
       "  'forks': '19',\n",
       "  'commits': '74'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Co Mocha\\n\\n\\n\\n\\n\\nEnable support for generators in Mocha tests using co.\\nUse the --harmony-generators flag when running node 0.11.x to access generator functions, or transpile your tests using traceur or regenerator.\\nInstallation\\nnpm install co-mocha --save-dev\\n\\nUsage\\nJust require the module in your tests and start writing generators in your tests.\\nit(\\'should do something\\', function * () {\\n  yield users.load(123)\\n})\\nNode\\nInstall the module using npm install co-mocha --save-dev. Now just require the module to automatically monkey patch any available mocha instances. With mocha, you have multiple ways of requiring the module - add --require co-mocha to your mocha.opts or add require(\\'co-mocha\\') inside your main test file.\\nIf you need to monkey patch a different mocha instance you can use the library directly:\\nvar mocha = require(\\'mocha\\')\\nvar coMocha = require(\\'co-mocha\\')\\n\\ncoMocha(mocha)\\n<script> Tag\\n<script src=\"co-mocha.js\"></script>\\nIncluding the browserified script will automatically patch window.Mocha. Just make sure you include it after mocha.js. If that is not possible the library exposes window.coMocha, which can be used (window.coMocha(window.Mocha)).\\nAMD\\nSame details as the script, but using AMD requires instead.\\nHow It Works\\nThe module monkey patches the Runnable.prototype.run method of mocha to enable generators. In contrast to other npm packages, co-mocha extends mocha at runtime - allowing you to use any compatible mocha version.\\nLicense\\nMIT\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '216',\n",
       "  'forks': '12',\n",
       "  'commits': '4'},\n",
       " {'language': 'JavaScript 97.4',\n",
       "  'readme': 'Javascript Refactor plugin for Sublime Text 2 and 3\\n \\n[![Package Control](https://packagecontrol.herokuapp.com/downloads/JavaScript%20Refactor.svg?color=50C32E)](https://packagecontrol.io/packages/JavaScript%20Refactor)\\n[![Donate](http://s-a.github.io/donate/donate.svg)](http://s-a.github.io/donate/)\\nOverview\\n\\nGoto definition of a variable or function\\nRename variable or function respecting its current scope\\nIntroduce variable\\nExtract selected source to a new method\\n\\nPreview\\nhttp://www.youtube.com/watch?v=P9K7mxWItPw\\nInstallation\\nUse the Sublime Package Control and search for: \"JavaScript Refactor\"\\nor\\nClone or download the git repository into your packages folder.\\nIn Sublime Text use \"Preferences/Browse Packages\" menu item to open this folder.\\nThe shorter way of doing this is:\\nLinux\\ngit clone https://github.com/s-a/sublime-text-refactor.git ~/.config/sublime-text-2/Packages/sublime-text-refactor\\nMac\\ngit clone https://github.com/s-a/sublime-text-refactor.git ~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2/Packages/sublime-text-refactor\\nWindows\\nIn an elevated command prompt or powershell (as Admin):\\ngit clone https://github.com/s-a/sublime-text-refactor.git \"%APPDATA%\\\\Sublime Text 2\\\\Packages\\\\sublime-text-refactor\"\\nfor Sublime Text 3:\\ngit clone https://github.com/s-a/sublime-text-refactor.git \"%APPDATA%\\\\Sublime Text 3\\\\Packages\\\\sublime-text-refactor\"\\nDependencies\\n\\nThis Plugin makes heavy usage of Node.js. So it needs a local installation of http://nodejs.org\\nmocha (only for testing)\\n\\nUsage\\nGoto Definition:\\nSelect a keyword via double click or point the cursor to the keyword and choose \"Goto Definition\" from context menu.\\nRename:\\nSelect a keyword via double click or point the cursor to the keyword and choose \"Rename\" from context menu. The plugin will select all variables or function calls occurring in the source code including its declaration. After that you rename them all on the fly. The logic respects the variables or functions scope. So it should be safe to rename them all without thinking ;) .\\nIntroduce Variable:\\nSelect an Expression from source code or point the cursor to the desired position and choose \"Introduce Variable\" from context menu.\\nExtract Method:\\nSelect the source code you want to extract into a new method and choose \"Refactor / Extract methode\" from context menu.\\nThis will extract the source code instantly to a new methode aka function. The plugin will manage undeclared variable usages and pass them within a single bundled JSON parameter to the new function.\\nIt also generates a sample function call at the bottom of the new methode.\\nThe plugin marks all variables occurring in the source code so you can rename them on the fly.\\nRun the tests\\nGoto Pluginfolder and type\\nnpm test\\nYou can find current test cases here\\nhttps://github.com/s-a/sublime-text-refactor/blob/master/js/test/\\nTroubleshoot\\n\\nNode not found\\nChoose Preferences: Refactor Settings – User from context menu and configure the nodePath setup. (Default Value is node)\\n\\n\\t\"nodePath\" : \"node\"\\n}```\\n\\n\\nTodo\\n========================\\n- ***Extract method***  \\n- Define exceptions of global scoped variable names like jQuery or $.\\n- Do not pass variables available in current Scope (optional).\\n- Let the user choose a function name before or after extraction.\\n- Let the user choose a custom position to insert extracted methode code and indent it correctly.\\n\\n\\nLicense\\n=======\\n\\n\\nMIT and GPL license.\\n\\nCopyright (c) 2013 Stephan Ahlf <stephan@ahlf-it.de>\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '216',\n",
       "  'forks': '12',\n",
       "  'commits': '73'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Amazon EC2\\nEver try to query for some instances with boto? It sucks.\\n>>> import ec2\\n>>> ec2.instances.filter(state=\\'running\\', name__startswith=\\'production\\')\\n[...]\\nInstall\\n$ pip install ec2\\nUsage\\nAWS credentials\\nCredentials are defined as a global state, either through an environment variable, or in Python.\\nec2.credentials.ACCESS_KEY_ID = \\'xxx\\'\\nec2.credentials.SECRET_ACCESS_KEY = \\'xxx\\'\\nec2.credentials.REGION_NAME = \\'us-west-2\\'  # (optional) defaults to us-east-1\\nCredentials can also be loaded from a CSV file generated by Amazon\\'s IAM.\\nNote: REGION_NAME still needs to be specified.\\nec2.credentials.from_file(\\'credentials.csv\\')\\nQuerying\\nAll instances\\nec2.instances.all()\\nAll Security Groups\\nec2.security_groups.all()\\nAll Virtual Private Clouds\\nec2.vpcs.all()\\nFiltering\\nFilter style is based on Django\\'s ORM\\nAll filters map directly to instance/security group properties.\\nec2.instances.filter(id=\\'i-xxx\\')  # Exact instance id\\nec2.instances.filter(state=\\'running\\')  # Exact instance state\\nFilters will also dig into tags.\\nec2.instances.filter(name=\\'production-web\\')  # Exact \"Name\" tag\\nFilters support many types of comparisons, similar to Django\\'s ORM filters.\\nec2.instances.filter(name__exact=\\'production-web-01\\')  # idential to `name=\\'...\\'`\\nec2.instances.filter(name__iexact=\\'PRODUCTION-WEB-01\\')  # Case insensitive \"exact\"\\nec2.instances.filter(name__like=r\\'^production-web-\\\\d+$\\')  # Match against a regular expression\\nec2.instances.filter(name__ilike=r\\'^production-web-\\\\d+$\\')  # Case insensitive \"like\"\\nec2.instances.filter(name__contains=\\'web\\')  # Field contains the search string\\nec2.instances.filter(name__icontains=\\'WEB\\')  # Case insensitive \"contains\"\\nec2.instances.filter(name__startswith=\\'production\\')  # Fields starts with the search string\\nec2.instances.filter(name__istartswith=\\'PRODUCTION\\')  # Case insensitive \"startswith\"\\nec2.instances.filter(name__endswith=\\'01\\')  # Fields ends with the search string\\nec2.instances.filter(name__iendswith=\\'01\\')  # Case insensitive \"endswith\"\\nec2.instances.filter(name__isnull=False)  # Match if the field exists\\nFilters can also be chained.\\nec2.instances.filter(state=\\'running\\', name__startswith=\\'production\\')\\nFilters can also be used with security groups.\\nec2.security_groups.filter(name__iexact=\\'PRODUCTION-WEB\\')\\nFilters can also be used with virtual private clouds.\\nec2.vpcs.filter(cidr_blocks__startswith=\\'10.10\\')\\nget() works exactly the same as filter(), except it returns just one instance and raises an exception for anything else.\\nec2.instances.get(name=\\'production-web-01\\')  # Return a single instance\\nec2.instances.get(name=\\'i-dont-exist\\')  # Raises an `ec2.instances.DoesNotExist` exception\\nec2.instances.get(name__like=r\\'^production-web-\\\\d+$\\')  # Raises an `ec2.instances.MultipleObjectsReturned` exception if matched more than one instance\\nec2.security_groups.get(name__startswith=\\'production\\')  # Raises an `ec2.security_groups.MultipleObjectsReturned` exception\\nec2.vpcs.get(cidr_block=\\'10.10.0.0/16\\')\\nSearch fields\\nInstances\\n\\nid (Instance id)\\nstate (running, terminated, pending, shutting-down, stopping, stopped)\\npublic_dns_name\\nip_address\\nprivate_dns_name\\nprivate_ip_address\\nroot_device_type (ebs, instance-store)\\nkey_name (name of the SSH key used on the instance)\\nimage_id (Id of the AMI)\\n\\nAll fields can be found at: https://github.com/boto/boto/blob/d91ed8/boto/ec2/instance.py#L157-204\\nSecurity Groups\\n\\nid (Security Group id)\\nname\\nvpc_id\\n\\nVirtual Private Clouds\\n\\nid (Virtual Private Cloud id)\\ncidr_block (CIDR Network Block of the VPC)\\nstate (Current state of the VPC, creation is not instant)\\nis_default\\ninstance_tenancy\\ndhcp_options_id (DHCP options id)\\n\\nExamples\\nGet public ip addresses from all running instances who are named production-web-{number}\\nimport ec2\\nec2.credentials.ACCESS_KEY_ID = \\'xxx\\'\\nec2.credentials.SECRET_ACCESS_KEY = \\'xxx\\'\\n\\nfor instance in ec2.instances.filter(state=\\'running\\', name__like=r\\'^production-web-\\\\d+$\\'):\\n    print instance.ip_address\\nAdd a role to a security group\\nimport ec2\\nec2.credentials.ACCESS_KEY_ID = \\'xxx\\'\\nec2.credentials.SECRET_ACCESS_KEY = \\'xxx\\'\\n\\ntry:\\n    group = ec2.security_groups.get(name=\\'production-web\\')\\nexcept ec2.security_groups.DoesNotExist:\\n    import sys\\n    sys.stderr.write(\\'Not found.\\')\\n    sys.exit(1)\\ngroup.authorize(\\'tcp\\', 80, 80, cidr_ip=\\'0.0.0.0/0\\')\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '140',\n",
       "  'forks': '14',\n",
       "  'commits': '63'},\n",
       " {'language': 'Python 56.4',\n",
       "  'readme': 'Detective.io \\nDownload •\\nFork •\\nLicense •\\nTest coverage •\\nDocumentation •\\nVersion 1.12.13 Gorilla\\nInstallation\\nSee also the full installation guide.\\n1. Prerequisite\\nsudo apt-get install build-essential git-core python python-pip python-dev libmemcached-dev libpq-dev libxslt1-dev libxml2-dev libxml2 libjpeg8-dev\\nsudo pip install virtualenv\\n2.  Download the project\\ngit clone git@github.com:jplusplus/detective.io.git\\ncd detective.io\\n3. Install\\nmake install\\nRun in development\\nmake run\\nThen visit http://127.0.0.1:8000\\nTechnical stack\\nThis small application uses the following tools and opensource projects:\\n\\nDjango Framework - Backend Web framework\\nNeo4django - Object Graph Mapper for Neo4j\\nTastypie - RestAPI for Django\\nAngularJS - Javascript Framework\\nUI Router - Application states manager\\nUnderscore - Utility library\\nBootstrap - HTML and CSS framework\\nLess - CSS pre-processor\\nCoffeeScript\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '139',\n",
       "  'forks': '18',\n",
       "  'commits': '3,623'},\n",
       " {'language': 'Python 88.4',\n",
       "  'readme': 'THIS PROJECT IS UNMAINTAINED\\nPlease use micawber -- it has much\\nof the same functionality along with many improvements.\\n\\nGetting Started with OEmbed\\n\\nInstallation\\nFirst, you need to install OEmbed.  It is available at http://github.com/worldcompany/djangoembed/\\ngit clone git://github.com/worldcompany/djangoembed/\\ncd djangoembed\\npython setup.py install\\n\\n\\nAdding to your Django Project\\nAfter installing, adding OEmbed consumption to your projects is a snap.  First,\\nadd it to your projects\\' INSTALLED_APPs and run \\'syncdb\\':\\n# settings.py\\nINSTALLED_APPS = [\\n    ...\\n    \\'oembed\\'\\n]\\n\\ndjangoembed uses a registration pattern like the admin\\'s.  In order to be\\nsure all apps have been loaded, djangoembed should run autodiscover() in the\\nurls.py.  If you like, you can place this code right below your admin.autodiscover()\\nbits:\\n# urls.py\\nimport oembed\\noembed.autodiscover()\\n\\n\\nConsuming Resources\\nNow you\\'re ready to start consuming OEmbed-able objects.  There are a couple of\\noptions depending on what you want to do.  The most straightforward way to get\\nup-and-running is to add it to your templates:\\n{% load oembed_tags %}\\n\\n{% oembed %}blog.content{% endoembed %}\\n\\n{# or use the filter #}\\n\\n{{ blog.content|oembed }}\\n\\n{# maybe you\\'re working with some dimensional constraints #}\\n\\n{% oembed \"600x600\" %}blog.content{% endoembed %}\\n\\n{{ blog.content|oembed:\"600x600\" }}\\n\\nYou can consume oembed objects in python as well:\\nimport oembed\\noembed.autodiscover()\\n\\n# just get the metadata\\nresource = oembed.site.embed(\\'http://www.youtube.com/watch?v=nda_OSWeyn8\\')\\nresource.get_data()\\n\\n{u\\'author_name\\': u\\'botmib\\',\\n u\\'author_url\\': u\\'http://www.youtube.com/user/botmib\\',\\n u\\'height\\': 313,\\n u\\'html\\': u\\'<object width=\"384\" height=\"313\"><param name=\"movie\" value=\"http://www.youtube.com/v/nda_OSWeyn8&fs=1\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/nda_OSWeyn8&fs=1\" type=\"application/x-shockwave-flash\" width=\"384\" height=\"313\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed></object>\\',\\n u\\'provider_name\\': u\\'YouTube\\',\\n u\\'provider_url\\': u\\'http://www.youtube.com/\\',\\n u\\'title\\': u\\'Leprechaun in Mobile, Alabama\\',\\n u\\'type\\': u\\'video\\',\\n u\\'version\\': u\\'1.0\\',\\n u\\'width\\': 384}\\n\\n# get the metadata and run it through a template for pretty presentation\\nfrom oembed.consumer import OEmbedConsumer\\nclient = OEmbedConsumer()\\nembedded = client.parse_text(\"http://www.youtube.com/watch?v=nda_OSWeyn8\")\\n\\n<div class=\"oembed oembed-video provider-youtube\">\\n  <object width=\"384\" height=\"313\">\\n    <param name=\"movie\" value=\"http://www.youtube.com/v/nda_OSWeyn8&fs=1\"></param>\\n    <param name=\"allowFullScreen\" value=\"true\"></param>\\n    <param name=\"allowscriptaccess\" value=\"always\"></param>\\n    <embed src=\"http://www.youtube.com/v/nda_OSWeyn8&fs=1\"\\n           type=\"application/x-shockwave-flash\"\\n           width=\"384\"\\n           height=\"313\"\\n           allowscriptaccess=\"always\"\\n           allowfullscreen=\"true\">\\n    </embed>\\n  </object>\\n  <p class=\"credit\">\\n    <a href=\"http://www.youtube.com/watch?v=nda_OSWeyn8\">Leprechaun in Mobile, Alabama</a>\\n    by\\n    <a href=\"http://www.youtube.com/user/botmib\">botmib</a>\\n  </p>\\n</div>\\'\\n\\n\\nTroubleshooting\\nProblem: You try the youtube embed example, but all you get is a link to the youtube video.\\nSolution: Djangoembed uses fixtures to load data about oembed providors like Youtube in to the database.  Try fooling around with syncdb (or migrations, if you\\'re running South) until there are objects of type oembed.storedprovider.\\nIf you have another problem, consider looking through the more extensive docs in the project\\'s doc subdirectory.\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '138',\n",
       "  'forks': '40',\n",
       "  'commits': '70'},\n",
       " {'language': 'Python 96.7',\n",
       "  'readme': 'ZeroDB server and client-side example of using it\\nDocumentation: http://docs.zerodb.io/\\n',\n",
       "  'watchers': '17',\n",
       "  'stars': '138',\n",
       "  'forks': '20',\n",
       "  'commits': '67'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'crontabber\\nA cron job runner with self-healing and job dependencies.\\nLicense: MPL 2\\n\\n\\n\\nHow to run tests\\nFirst you need to create a dedicated test database. We recommend you\\ncall it test_crontabber. Then you need the necessary credentials for\\nit.\\nBefore running the tests you need to install some extras to be able to\\nrun tests at all:\\npip install -r test-requirements.txt\\n\\nNext, in the root directory of the project create a file called\\ntest-crontabber.ini and it should look something like this:\\n[crontabber]\\nuser=myusername\\npassword=mypassword\\ndbname=test_crontabber\\n\\nTo start all the tests run:\\nPYTHONPATH=. nosetests\\n\\nIf you want to run a specific test in a specific file in a specific\\nclass you can define it per the nosetests standard like this for\\nexample:\\nPYTHONPATH=. nosetests tests crontabber/tests/test_crontabber.py:TestCrontabber.test_basic_run_job\\n\\nIf you want the tests to stop as soon as the first test fails add -x\\nto that same command above.\\nAlso, if you want nosetests to not capture stdout add -s\\nto that same command as above.\\n\\nHow to do code coverage analysis\\nFirst you need to install the\\ncoverage module. Then,\\nwith nosetests, you can run this:\\nPYTHONPATH=. nosetests --with-coverage --cover-erase --cover-html --cover-package=crontabber\\n\\nAfter it has run, you can open the file cover/index.html in browser.\\n\\nHow to run the exampleapp\\nThe example app helps you set up a playground to play around with and\\ntest crontabber to gain a better understanding of how it works.\\nThe best place to start with is to read the exampleapp/README.md\\nfile and go through its steps. Once you get the basics to work you can\\nstart experimenting with adding your job classes.\\n\\nHow locking works\\ncrontabber supports locking. It basically means if you start a second\\ninstance of crontabber whilst it\\'s already ongoing in another\\nterminal/server the second one will exist early. This is only applicable\\nif there is an actual job ongoing.\\nThere are two kinds of locking.\\n\\nGeneral locking. The first thing crontabber does before it starts\\nan app is to ask the state (stored in PostgreSQL) if it\\'s ongoing and\\nif it is, it exists with an error code of 3.\\nSub-second locking. If the general locking (see point above) says\\n\"No, the job is not ongoing\", it\\'s going to proceed to update the\\nstate with a row-level locking transaction in\\nPostgreSQL.\\nThat basically means PostgreSQL only allows one single UPDATE\\nfrom the process that gets there first. The second crontabber process\\nwill will exit early with an error code of 2 if the first\\ncrontabber process managed to run the UPDATE first.\\n\\nImagine two separate terminals starting crontabber at the almost same\\ntime:\\n# Terminal 1\\n$ python crontabber.py --admin.conf=crontabber.ini\\n$ echo $?\\n0\\n\\n# Terminal 2 (started almost simultaneously)\\n$ python crontabber.py --admin.conf=crontabber.ini\\n$ echo $?\\n3\\n\\nNote! If a job has been ongoing to a maximum period of time, the\\nlocking is ignored. This is controlled by the config option\\ncrontabber.max_ongoing_age_hours which defaults to 12 hours.\\nThis is applicable if crontabber, updates the state that it\\'s starting a\\njob, then when it tries to update the state that it finished\\n(successfully or not) and that write fails, if for example it\\'s unable\\nto make a connection to PostgreSQL. If this happens crontabber will just\\nignore the lock and run it anyway.\\n',\n",
       "  'watchers': '16',\n",
       "  'stars': '137',\n",
       "  'forks': '19',\n",
       "  'commits': '151'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'crontabber\\nA cron job runner with self-healing and job dependencies.\\nLicense: MPL 2\\n\\n\\n\\nHow to run tests\\nFirst you need to create a dedicated test database. We recommend you\\ncall it test_crontabber. Then you need the necessary credentials for\\nit.\\nBefore running the tests you need to install some extras to be able to\\nrun tests at all:\\npip install -r test-requirements.txt\\n\\nNext, in the root directory of the project create a file called\\ntest-crontabber.ini and it should look something like this:\\n[crontabber]\\nuser=myusername\\npassword=mypassword\\ndbname=test_crontabber\\n\\nTo start all the tests run:\\nPYTHONPATH=. nosetests\\n\\nIf you want to run a specific test in a specific file in a specific\\nclass you can define it per the nosetests standard like this for\\nexample:\\nPYTHONPATH=. nosetests tests crontabber/tests/test_crontabber.py:TestCrontabber.test_basic_run_job\\n\\nIf you want the tests to stop as soon as the first test fails add -x\\nto that same command above.\\nAlso, if you want nosetests to not capture stdout add -s\\nto that same command as above.\\n\\nHow to do code coverage analysis\\nFirst you need to install the\\ncoverage module. Then,\\nwith nosetests, you can run this:\\nPYTHONPATH=. nosetests --with-coverage --cover-erase --cover-html --cover-package=crontabber\\n\\nAfter it has run, you can open the file cover/index.html in browser.\\n\\nHow to run the exampleapp\\nThe example app helps you set up a playground to play around with and\\ntest crontabber to gain a better understanding of how it works.\\nThe best place to start with is to read the exampleapp/README.md\\nfile and go through its steps. Once you get the basics to work you can\\nstart experimenting with adding your job classes.\\n\\nHow locking works\\ncrontabber supports locking. It basically means if you start a second\\ninstance of crontabber whilst it\\'s already ongoing in another\\nterminal/server the second one will exist early. This is only applicable\\nif there is an actual job ongoing.\\nThere are two kinds of locking.\\n\\nGeneral locking. The first thing crontabber does before it starts\\nan app is to ask the state (stored in PostgreSQL) if it\\'s ongoing and\\nif it is, it exists with an error code of 3.\\nSub-second locking. If the general locking (see point above) says\\n\"No, the job is not ongoing\", it\\'s going to proceed to update the\\nstate with a row-level locking transaction in\\nPostgreSQL.\\nThat basically means PostgreSQL only allows one single UPDATE\\nfrom the process that gets there first. The second crontabber process\\nwill will exit early with an error code of 2 if the first\\ncrontabber process managed to run the UPDATE first.\\n\\nImagine two separate terminals starting crontabber at the almost same\\ntime:\\n# Terminal 1\\n$ python crontabber.py --admin.conf=crontabber.ini\\n$ echo $?\\n0\\n\\n# Terminal 2 (started almost simultaneously)\\n$ python crontabber.py --admin.conf=crontabber.ini\\n$ echo $?\\n3\\n\\nNote! If a job has been ongoing to a maximum period of time, the\\nlocking is ignored. This is controlled by the config option\\ncrontabber.max_ongoing_age_hours which defaults to 12 hours.\\nThis is applicable if crontabber, updates the state that it\\'s starting a\\njob, then when it tries to update the state that it finished\\n(successfully or not) and that write fails, if for example it\\'s unable\\nto make a connection to PostgreSQL. If this happens crontabber will just\\nignore the lock and run it anyway.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '137',\n",
       "  'forks': '15',\n",
       "  'commits': '34'},\n",
       " {'language': 'Python 98.3',\n",
       "  'readme': \"twosheds\\n\\n\\n\\ntwosheds is a library, written in Python, for making command language\\ninterpreters, or shells.\\nWhile shells like bash and zsh are powerful, extending them and customizing them\\nis hard; you need to write in inexpressive arcane languages, such as bash script\\nor C. twosheds helps you write and customize your own shell, in pure Python:\\n>>> import twosheds\\n>>> shell = twosheds.Shell()\\n>>> shell.serve_forever()\\n$ whoami\\narthurjackson\\n$ ls\\nAUTHORS.rst       build             requirements.txt  test_twosheds.py\\nLICENSE           dist              scripts           tests\\nMakefile          docs              setup.cfg         twosheds\\nREADME.rst        env               setup.py          twosheds.egg-info\\nGet started now.\\n\\nFeatures\\n\\nSubstitution\\nHistory\\nTab completion\\nHighly extensible\\n\\n\\nInstallation\\nTo install twosheds, simply:\\n$ pip install twosheds\\n\\nDocumentation\\nDocumentation is available at http://twosheds.readthedocs.org/en/latest/.\\n\\nContribute\\ntwosheds is under active development and contributions are especially welcome.\\n\\nCheck for open issues or open a fresh issue to start a discussion around a\\nfeature idea or a bug.\\nFork the repository on GitHub to start making your changes to the\\nmaster branch (or branch off it).\\nWrite a test which shows that the bug was fixed or that the feature works as\\nexpected.\\nSend a pull request and bug the maintainer until its get merged and\\npublished. Make sure to add yourself to AUTHORS. :)\\n\\n\\nSupport\\nIf you have questions or issues about twosheds, there are several options:\\n\\nSend a Tweet\\nIf your question is less than 140 characters, feel free to tweet at the\\nmaintainer, @Ceasar_Bautista.\\n\\nFile an Issue\\nIf you notice some unexpected behavior in twosheds, or want to see support for\\na new feature, file an issue on GitHub issues.\\n\\nE-mail\\nI'm more than happy to answer any personal or in-depth questions about twosheds.\\nFeel free to email cbautista@gmail.com.\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '136',\n",
       "  'forks': '10',\n",
       "  'commits': '150'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'uvent: A gevent core implemented using libuv\\nuvent is a gevent core implementation using the libuv library.\\nuvent uses pyuv, a Python interface for libuv. libuv is a high performance asynchronous\\nnetworking library used as the platform layer for NodeJS.\\nlibuv provides the same core functionality as libev, with some really nice\\nadditions:\\n\\nHigh performance IO on Windows (not select)\\nAsyncronous file operations\\nBuiltin thread pool\\nAsynchronous getaddrinfo\\nNicer to use API\\nEtc.\\n\\nSource code for uvent is on GitHub.\\n\\nMotivation\\nThis is an experimental project to test the feasibility of using libuv as a\\ncore for gevent.\\nMain functionality is working but not all tests are passing and there are some\\nimplementation caveats mostly due to the tight integration between gevent and\\nlibev. Implementation notes can be found in the NOTES.rst file.\\n\\nInstallation\\nuvent requires pyuv >= 0.10.0\\npip install -U pyuv\\n\\nNote: uvent only works with gevent >= 1.0rc1, earlier versions are not supported.\\n\\nUsing it\\nIn order to use uvent add the following lines at the beginning\\nof your project, before importing anything from Gevent:\\nimport uvent\\nuvent.install()\\n\\nAnother way of doing this without modifying your code is by exporting environment variables before\\nrunning your program:\\nexport GEVENT_LOOP=uvent.loop.UVLoop\\nexport GEVENT_RESOLVER=gevent.resolver_thread.Resolver\\n\\n\\nAuthor\\nSaúl Ibarra Corretgé <saghul@gmail.com>\\n\\nLicense\\nuvent uses the MIT license, check LICENSE file.\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '136',\n",
       "  'forks': '8',\n",
       "  'commits': '67'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Sea Cucumber 1.5.1\\n\\n\\nInfo:A Django email backend for Amazon Simple Email Service, backed by django-celery\\n\\nAuthor:\\nDUO Interactive, LLC\\nInspired by:Harry Marr\\'s django-ses.\\n\\nStatus:\\nUnmaintained. Let us know if you\\'d like to step in!\\n\\n\\n\\nA bird\\'s eye view\\nSea Cucumber is a mail backend for Django. Instead of sending emails\\nthrough a traditional SMTP mail server, Sea Cucumber routes email through\\nAmazon Web Services\\' excellent Simple Email Service (SES) via django-celery.\\n\\nWhy Sea Cucumber/SES instead of SMTP?\\nConfiguring, maintaining, and dealing with some complicated edge cases can be\\ntime-consuming. Sending emails with Sea Cucumber might be attractive to you if:\\n\\nYou don\\'t want to maintain mail servers.\\nYour mail server is slow or unreliable, blocking your views from rendering.\\nYou need to send a high volume of email.\\nYou don\\'t want to have to worry about PTR records, Reverse DNS, email\\nwhitelist/blacklist services.\\nYou are already deployed on EC2 (In-bound traffic to SES is free from EC2\\ninstances). This is not a big deal either way, but is an additional perk if\\nyou happen to be on AWS.\\n\\n\\nInstallation\\nAssuming you\\'ve got Django and django-celery installed, you\\'ll need\\nBoto 2.0b4 or higher. boto is a Python library that wraps the AWS API.\\nYou can do the following to install boto 2.0b4 (we\\'re using --upgrade here to\\nmake sure you get 2.0b4):\\npip install --upgrade boto\\n\\nInstall Sea Cucumber:\\npip install seacucumber\\n\\nAdd the following to your settings.py:\\nEMAIL_BACKEND = \\'seacucumber.backend.SESBackend\\'\\n\\n# These are optional -- if they\\'re set as environment variables they won\\'t\\n# need to be set here as well\\nAWS_SES_REGION_NAME = \\'YOUR-REGION\\'  # Default is us-east-1\\nAWS_ACCESS_KEY_ID = \\'YOUR-ACCESS-KEY-ID\\'\\nAWS_SECRET_ACCESS_KEY = \\'YOUR-SECRET-ACCESS-KEY\\'\\n\\n# Make sure to do this if you want the ``ses_address`` management command.\\nINSTALLED_APPS = (\\n    ...\\n    \\'seacucumber\\'\\n)\\n\\nThe region name is optional but keep in mind addresses are linked to a specific region, therefore if you add/verify them in one region they will only be available in that region.\\n\\nEmail Address Verification\\nBefore you can send email \\'from\\' an email address through SES, you must first\\nverify your ownership of it:\\n./manage.py ses_address verify batman@gotham.gov\\n\\nAfter you\\'ve run the verification above you will need to check the email\\naccount\\'s inbox (from your mail client or provider\\'s web interface) and click\\nthe authorization link in the email Amazon sends you. After that, your address\\nis ready to go.\\nTo confirm the verified email is ready to go:\\n./manage.py ses_address list\\n\\nTo remove a previously verified address:\\n./manage.py ses_address delete batman@gotham.gov\\n\\nNow, when you use django.core.mail.send_mail from a verified email address,\\nSea Cucumber will handle message delivery.\\n\\nRate Limiting\\nIf you are a new SES user, your default quota will be 1,000 emails per 24\\nhour period at a maximum rate of one email per second. Sea Cucumber defaults\\nto enforcing the one email per second at the celery level, but you must not\\nhave disabled celery rate limiting.\\nIf you have this:\\nCELERY_DISABLE_RATE_LIMITS = True\\n\\nChange it to this:\\nCELERY_DISABLE_RATE_LIMITS = False\\n\\nThen check your SES max rate by running:\\n./manage.py ses_usage\\n\\nIf your rate limit is more than 1.0/sec, you\\'ll need to set that numeric\\nvalue in your CUCUMBER_RATE_LIMIT setting like so:\\n# Rate limit to three outgoing SES emails a second.\\nCUCUMBER_RATE_LIMIT = 3\\n\\nFailure to follow the rate limits may result in BotoServerError exceptions\\nbeing raised, which makes celery unhappy.\\nAs a general note, your quota and max send rate will increase with usage, so\\ncheck the ses_usage management command again at a later date after you\\'ve\\nsent some emails. You\\'ll need to manually bump up your rate settings in\\nsettings.py.\\n\\nRouting Tasks\\nIf you want to route Sea Cucumber task to different queues.\\nAdd this to setting:\\nCUCUMBER_ROUTE_QUEUE = \\'YOUR-ROUTE-QUEUE\\'\\n\\nThen update the celery configuration for routes. Example celeryconfig.py:\\nCELERY_ROUTES = {\\n    \\'seacucumber.tasks.#\\': {\\'queue\\': \\'YOUR-ROUTE-QUEUE\\'},\\n}\\n\\n\\nDKIM\\nUsing DomainKeys is entirely optional, however it is recommended by Amazon for\\nauthenticating your email address and improving delivery success rate.  See\\nhttp://docs.amazonwebservices.com/ses/latest/DeveloperGuide/DKIM.html.\\nBesides authentication, you might also want to consider using DKIM in order to\\nremove the via email-bounces.amazonses.com message shown to gmail users -\\nsee http://support.google.com/mail/bin/answer.py?hl=en&answer=1311182.\\nTo enable DKIM signing you should install the pydkim package and specify values\\nfor the DKIM_PRIVATE_KEY and DKIM_DOMAIN settings.  You can generate a\\nprivate key with a command such as openssl genrsa 1024 and get the public key\\nportion with openssl rsa -pubout <private.key.  The public key should be\\npublished to ses._domainkey.example.com if your domain is example.com.  You\\ncan use a different name instead of ses by changing the DKIM_SELECTOR\\nsetting.\\nThe SES relay will modify email headers such as Date and Message-Id so by\\ndefault only the From, To, Cc, Subject headers are signed, not the full\\nset of headers.  This is sufficient for most DKIM validators but can be overridden\\nwith the DKIM_HEADERS setting.\\nExample settings.py:\\nDKIM_DOMAIN = \\'example.com\\'\\nDKIM_PRIVATE_KEY = \\'\\'\\'\\n-----BEGIN RSA PRIVATE KEY-----\\nxxxxxxxxxxx\\n-----END RSA PRIVATE KEY-----\\n\\'\\'\\'\\n\\nExample DNS record published to Route53 with boto:\\n\\nroute53 add_record ZONEID ses._domainkey.example.com. TXT \\'\"v=DKIM1; p=xxx\"\\' 86400\\n\\nDjango Builtin-in Error Emails\\nIf you\\'d like Django\\'s Builtin Email Error Reporting to function properly\\n(actually send working emails), you\\'ll have to explicitly set the\\nSERVER_EMAIL setting to one of your SES-verified addresses. Otherwise, your\\nerror emails will all fail and you\\'ll be blissfully unaware of a problem.\\nNote: You can use the included ses_address management command to handle\\naddress verification.\\n\\nGetting Help\\nIf you have any questions, feel free to either post them to our\\nissue tracker.\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '136',\n",
       "  'forks': '32',\n",
       "  'commits': '62'},\n",
       " {'language': 'Python 99.5',\n",
       "  'readme': \"\\nNOTICE: Deprecated\\nThis project is deprecated and no longer actively maintained by Disqus. However there is a fork being maintained by YPlan at github.com/YPlan/django-modeldict and a similar project by Disqus at github.com/disqus/durabledict.\\n\\ndjango-modeldict\\nModelDict is a very efficient way to store things like settings in your database. The entire model is transformed into a dictionary (lazily) as well as stored in your cache. It's invalidated only when it needs to be (both in process and based on CACHE_BACKEND).\\nQuick example usage. More docs to come (maybe?):\\nclass Setting(models.Model):\\n    key = models.CharField(max_length=32)\\n    value = models.CharField(max_length=200)\\nsettings = ModelDict(Setting, key='key', value='value', instances=False)\\n\\n# access missing value\\nsettings['foo']\\n>>> KeyError\\n\\n# set the value\\nsettings['foo'] = 'hello'\\n\\n# fetch the current value using either method\\nSetting.objects.get(key='foo').value\\n>>> 'hello'\\n\\nsettings['foo']\\n>>> 'hello'\\n\\n\",\n",
       "  'watchers': '24',\n",
       "  'stars': '132',\n",
       "  'forks': '25',\n",
       "  'commits': '73'},\n",
       " {'language': 'Java 95.3',\n",
       "  'readme': 'PagerIndicator\\n  \\nPager (especially for ViewPager) indicator in two styles: circle & fraction.\\nDemo\\n\\n\\n\\ncircle\\nfraction\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDependency\\nimplementation \\'me.liangfei:pagerindicator:0.0.2\\'\\nUsage\\nTwo attributes are provided:\\n\\napp:indicator_type = [fraction | circle].\\napp:indicator_spacing works only for the circle type indicator.\\n\\nStep 1: add PageIndicator to the bottom of the ViewPager.\\n<merge xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\">\\n\\n    <androidx.viewpager.widget.ViewPager\\n        android:id=\"@+id/pager\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"match_parent\" />\\n\\n    <me.liangfei.indicator.PagerIndicator\\n        android:id=\"@+id/indicator\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"match_parent\"\\n        android:layout_marginBottom=\"20dp\"\\n        android:layout_marginLeft=\"20dp\"\\n        android:layout_marginStart=\"20dp\"\\n        android:gravity=\"bottom|center_horizontal\"\\n        app:indicator_spacing=\"5dp\"\\n        app:indicator_type=\"fraction\" />\\n</merge>\\nStep 2: pass the ViewPager instance to the PagerIndicator instance.\\nval pageIndicator = findViewById(R.id.indicator);\\npageIndicator.setViewPager(pager);\\nYou can just take PageIndicator as a normal view to make your layout, because it extends LinearLayout.\\nCheck the app module for more details.\\ndependencies\\n\\nFresco\\n\\n欢迎关注我的微信公众号（Chinese only）\\n\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '215',\n",
       "  'forks': '71',\n",
       "  'commits': '30'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'ExpandableView\\n\\nExpandableView is a View showing only a content and when clicked on it, it displays more content in a fashion way. You can add views or viewgroups but remember that it will only display the content in a LinearLayout with vertical orientation.\\nYou can choose by default a \"chevron\" icon animation or a \"plus\" icon animation. The \"always visible row\" has a left icon, a text and finally a right icon which will be the animated one.\\n\\nInstructions - Maven Central\\n\\nAdd this library in your build.gradle:\\n\\ndependencies {\\n    compile \\'com.github.nicolasjafelle:expandableview:1.0\\'\\n}\\nInstructions\\n\\nClone the git repo\\nImport the \"ExpandableView\" module into your Android-gradle project.\\nAdd \"ExpandableView\" module in your settings.gradle\\nDONE\\n\\nHow to Use it\\nAs any view in Android you can add it by code or by layout xml file but remember that if you want to change the default height of the visible content you need to use:\\ntopExpandableView.setVisibleLayoutHeight(300);\\ntopExpandableView.setVisibleLayoutHeight(getResources().getDimensionPixelSize(R.dimen.new_height));\\nRemember also to fill the information inside the visible content by using:\\nexpandableView.fillData(R.drawable.ic_android, R.string.android_names, true);\\n//or\\nexpandableView.fillData(R.drawable.ic_android, getString(R.string.android_names), true);\\n//or\\nexpandableView.fillData(R.drawable.ic_android, R.string.android_names, true);\\n//or\\nexpandableView.fillData(R.drawable.ic_android, getString(R.string.android_names));\\n//or\\nexpandableView.fillData(R.drawable.ic_android, R.string.android_names);\\n\\n//or\\nexpandableView.fillData(0, R.string.android_names); // No drawable left by passing 0.\\nIf you want to add content into the discoverable LinearLayout simple use:\\nexpandableView.addContentView(itemView); // itemView could be a simple TextView or more complex custom views\\nThe most relevant part of this ExpandableView is when you want to include an ExpandableView into another ExpandableView, you need to pass the parent\\'s View hierarchy, like this:\\nexpandableViewLevel1.setOutsideContentLayout(topExpandableView.getContentLayout()); // 1 Level\\nexpandableViewLevel2.setOutsideContentLayout(topExpandableView.getContentLayout(), expandableViewLevel1.getContentLayout()); // 2 Levels\\nexpandableViewLevel3.setOutsideContentLayout(topExpandableView.getContentLayout(), expandableViewLevel1.getContentLayout(), expandableViewLevel2.getContentLayout()); // 3 Levels\\nAlso remember to use this package in your layout files:\\n<com.expandable.view.ExpandableView\\n\\tandroid:id=\"@+id/activity_main_top_expandable_view\"\\n\\tandroid:layout_width=\"match_parent\"\\n\\tandroid:layout_height=\"wrap_content\"/>\\n\\nYou can also customize the TextView inside the visible Content Layout in this way:\\n<!-- Use this style name to override the default style -->\\n<style name=\"ExpandableView_TextView\">\\n\\t<item name=\"android:textSize\">18sp</item>\\n\\t<item name=\"android:textStyle\">bold</item>\\n\\t<item name=\"android:textColor\">@android:color/black</item>\\n</style>\\n\\nDeveloped By\\n\\nNicolas Jafelle - nicolasjafelle@gmail.com\\n\\nLicense\\nCopyright 2015 Nicolas Jafelle\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '213',\n",
       "  'forks': '41',\n",
       "  'commits': '15'},\n",
       " {'language': 'Java 98.0',\n",
       "  'readme': 'StoreBox\\nAndroid library for streamlining SharedPreferences.\\n\\n\\n \\n\\nContents\\n\\nOverview\\nAdding to a project\\nInterface and creation\\nGet and set methods\\nDefaults for get methods\\nCustom types\\nPreference types\\nAdvanced\\nRemove and clear methods\\nChange listeners\\nChaining calls\\nForwarding calls\\nSave modes\\nVersioning\\nBuilder & Defaults\\nProGuard\\nContributing\\nLicense\\n\\nOverview\\nStoreBox is an annotation-based library for interacting with Android\\'s SharedPreferences, with the aim take out the the how and where parts of retrieving/storing values and instead focus on the more important what part.\\nNormally when retrieving or storing values we need to know two pieces of information during each call: the key and the type.\\nString username = preferences.getString(\"key_username\", null);\\npreferences.edit().putString(\"key_date_of_birth\", \"30/09/2004\").apply(); // should this be a String or a long?\\nWith StoreBox the operations above can be changed into pre-defined methods with improved semantics.\\n// definition\\npublic interface MyPreferences {\\n    \\n    @KeyByString(\"key_username\")\\n    String getUsername();\\n    \\n    @KeyByString(\"key_date_of_birth\")\\n    void setDateOfBirth(String value);\\n}\\n\\n// usage\\nMyPreferences preferences = StoreBox.create(context, MyPreferences.class);\\nString username = preferences.getUsername();\\npreferences.setDateOfBirth(\"30/09/2004\");\\nThe caller now doesn\\'t need to worry about the key, neither about what type the values are stored under. The only important part that needs to be taken into consideration is what is done with the values, whether that is storing them for later, showing them to the user in the UI, or just changing application behaviour.\\nRead on to find out more details about how StoreBox can be used and how it can be added to an Android project.\\nAdding to a project\\nStoreBox can be used in Android projects using minimum SDK version 10 and newer (Android 2.3+).\\nJAR\\nv1.4.0 JAR\\nv1.4.0 JavaDoc JAR\\nGradle\\ncompile \\'net.orange-box.storebox:storebox-lib:1.4.0\\'\\n\\nMaven\\n<dependency>\\n  <groupId>net.orange-box.storebox</groupId>\\n  <artifactId>storebox-lib</artifactId>\\n  <version>1.4.0</version>\\n</dependency>\\n\\nDefining an interface and bringing it to life\\nSimply create a new interface class in your IDE or a text editor, give it an access modifier which suits its use, and name it as appropriate.\\npublic interface MyPreferences {\\n    \\n}\\nNow you\\'re ready to use StoreBox.create() to obtain an instance.\\nMyPreferences instance = StoreBox.create(context, MyPreferences.class);\\nAdding get and set methods\\nIf you would like to add a getter just add a method to the interface which returns a value and make sure to annotate it using @KeyByString or @KeyByResource.\\n@KeyByString(\"key_nickname\")\\nString getNickname();\\n\\n@KeyByResource(R.string.key_notifications)\\nboolean shouldShowNotifications();\\nAdding a setter is just as easy. The same annotations will need to be used as for getter methods, but now our method will return nothing and will have to provide a parameter for supplying the value that should be saved.\\n@KeyByString(\"key_nickname\")\\nvoid setNickname(String value)\\n\\n@KeyByResource(R.string.key_notifications)\\nvoid setNotifications(boolean value)\\nSpecifying defaults for get methods\\nThis can be achieved in two ways, through an argument or by using an annotation.\\nFor the first option the following will work.\\n@KeyByString(\"key_phone_number\")\\nString getPhoneNumber(String defValue);\\nAnd using an annotation referencing a default found in the XML resources.\\n@KeyByString(\"key_phone_number\")\\n@DefaultValue(R.string.default_phone_number)\\nString getPhoneNumber();\\nFor some types, such as long, which cannot be added to the resources an integer resource may be used instead.\\n@KeyByString(\"key_refresh_interval\")\\n@DefaultValue(R.integer.default_refresh_interval)\\nlong getRefreshInterval();\\nStoring and retrieving custom types\\nSaving custom types, which are not understood by Android\\'s SharedPreferences, can be supported through the use of type adapters. A type adapter implementation can be provided by extending from one of the following classes:\\n\\nBaseBooleanTypeAdapter for storing as a Boolean\\nBaseFloatTypeAdapter for storing as a Float and so on...\\nBaseIntegerTypeAdapter\\nBaseLongTypeAdapter\\nBaseStringTypeAdapter\\nBaseStringSetTypeAdapter (only supported on API11 and newer)\\n\\nTelling StoreBox which type adapter should be used can be done by adding the @TypeAdapter annotation to the get and set methods.\\n@KeyByString(\"key_region\")\\n@TypeAdapter(RegionTypeAdapter.class)\\nRegion getRegion();\\n\\n@KeyByString(\"key_region\")\\n@TypeAdapter(RegionTypeAdapter.class)\\nvoid setRegion(Region value);\\nWhich type adapter needs to be extended depends on the use case. Take a look at the DateTypeAdapter, UriTypeAdapter, and CustomClassListTypeAdapter for some examples. It is worth noting that in the last example Gson is being used for serialising the type, as opposed to writing a custom implementation. Gson is not used internally by StoreBox, as such if you wish to use Gson for a type adapter you will need to add it to your project as a dependency.\\nThe following types will work out of the box, so type adapters don\\'t need to be provided for them:\\n\\nDate\\nDouble\\nEnum\\nUri\\n\\nDisclaimer: APIs around type adapters may change in the future, as I will keep looking for a less verbose way of achieving the same goal without requiring the use of Gson.\\nOpening different types of preferences\\nIn all of the examples so far details about what preferences are opened and how have been omitted.\\nWithout any annotation the default shared preferences will be used, but the @DefaultSharedPreferences annotation can be added to the interface definition for explicitness. Likewise, @ActivityPreferences or @FilePreferences can be used to respectively open preferences private to an activity or to open preferences using a file name.\\nThe mode with which the preferences should be opened can also be specified, although this option is not supported by all the types.\\n@ActivityPreferences(mode = PreferencesMode.MULTI_PROCESS)\\npublic interface WelcomeActivityPreferences {\\n    \\n    // method definitions here...\\n}\\nAdvanced\\nRemove and clear methods\\nIn order to remove a value stored in the preferences under a key a method to perform the removal can be annotated with the @RemoveMethod annotation. The key can be supplied in two ways;\\nThe key can be provided thorough an argument in the method, using either a String or an int in the case of the key being specified in an XML resource.\\npublic interface RemoveMethodExample {\\n    \\n    @RemoveMethod\\n    void remove(String key);\\n    \\n    @RemoveMethod\\n    void remove(int keyRes);\\n}\\n\\n// usage\\npreferences.remove(\"key_username\");\\npreferences.remove(R.string.key_password);\\nOr a value-specific remove method can be defined with the help of the @KeyByString or @KeyByResource annotations.\\npublic interface RemoveMethodExample {\\n    \\n    @KeyByString(\"key_username\")\\n    @RemoveMethod\\n    void removeUsername();\\n    \\n    @KeyByResource(R.string.key_password)\\n    @RemoveMethod\\n    void removePassword();\\n}\\n\\n// usage\\npreferences.removeUsername();\\npreferences.removePassword();\\nClearing all values stored in the preferences can be done by annotating a method with the @ClearMethod annotation.\\npublic interface ClearMethodExample {\\n    \\n    @ClearMethod\\n    void clear();\\n}\\n\\n// usage\\npreferences.clear();\\nChange listeners\\nCallbacks can be received when a preference value changes through the use of the OnPreferenceValueChangedListener interface. The listeners need to be parametrised with the type which is used for the value whose changes we would like to listen for. For example, if we would like to listen to changes to the password (from previous examples) then we could define the listener as\\nOnPreferenceValueChangedListener<String> listener = new OnPreferenceValueChangedListener<String>() {\\n    @Override\\n    public void onChanged(String newValue) {\\n        // do something with newValue\\n    }\\n}\\nTo register this listener a method for registering the listener would need to be defined using the @RegisterChangeListenerMethod annotation in the interface which gets passed to StoreBox.create(). Likewise, for unregistering a listener the method needs to be annotated with @UnregisterChangeListenerMethod instead. The @KeyByString or @KeyByResource annotation also needs to be used to specify which value we are interested in.\\npublic interface ChangeListenerExample {\\n    \\n    @KeyByString(\"key_password\")\\n    @RegisterChangeListenerMethod\\n    void registerPasswordListener(OnPreferenceValueChangedListener<String> listener);\\n    \\n    @KeyByString(\"key_password\")\\n    @UnregisterChangeListenerMethod\\n    void unregisterPasswordListener(OnPreferenceValueChangedListener<String> listener);\\n}\\nIf you would like to listen for changes to a custom type then the @TypeAdapter annotation will need to be added to the method in order to tell StoreBox how the value should be adapted when retrieving it from the preferences.\\nMore than one listener can be registered and unregistered at a time by changing the method definitions in the interface to use variable arguments.\\n// annotations omitted\\nvoid registerPasswordListeners(OnPreferenceValueChangedListener<String>... listeners);\\nCaution: StoreBox does not store strong references to the listeners. A strong reference must be kept to the listener for as long as the listener will be required, otherwise it will be susceptible to garbage collection.\\nChaining calls\\nWith Android\\'s SharedPreferences.Editor class it is possible to keep chaining put methods as each returns back the SharedPreferences.Editor instance. StoreBox allows the same functionality. All that needs to be done is to change the set/remove method definitions to either return interface type itself or SharedPreferences.Editor.\\npublic interface ChainingExample {\\n    \\n    @KeyByString(\"key_username\")\\n    ChainingExample setUsername(String value);\\n    \\n    @KeyByString(\"key_password\")\\n    ChainingExample setPassword(String value);\\n    \\n    @KeyByString(\"key_country\")\\n    ChainingExample removeCountry();\\n}\\nAnd calls can be chained as\\npreferences.setUsername(\"Joe\").setPassword(\"jOe\").removeCountry();\\nForwarding calls\\nIf you would like to access methods from the SharedPreferences or SharedPreferences.Editor, you can do that by extending your interface from either of the above (or even both).\\npublic interface ForwardingExample extends SharedPreferences, SharedPreferences.Editor {\\n    \\n    // method definitions here\\n}\\nAnd the methods from either of the extended interfaces will be callable.\\nString username = preferences.getString(\"key_username\", \"\");\\npreferences.putString(\"key_username\", \"Joe\").apply();\\nSave modes\\nChanges to preferences can normally be saved on Android either through apply() or commit(). Which method gets used can be customised in StoreBox through the use of the @SaveOption annotation.\\nUnlike any of the previous annotations @SaveOption can be used to annotate both the interface as well as individual set/remove methods, however an annotation at method-level will take precedence over an interface annotation.\\n@SaveOption(SaveMode.APPLY)\\npublic interface SaveModeExample {\\n    // key annotations omitted\\n    \\n    void setUsername(String value); // will save using apply()\\n    \\n    @SaveOption(SaveMode.COMMIT)\\n    void setPassword(String value); // will save using commit()\\n    \\n    @SaveOption(SaveMode.COMMIT)\\n    void removeUsername(); // will persist using commit()\\n}\\nVersioning\\nStoreBox supports versioning of preferences through the use of the @PreferencesVersion interface-level annotation, in a similar fashion to Android\\'s SQLiteOpenHelper. This functionality may be required in the case when the schema of the preferences needs to be changed, such as when a key or type of a preference changes, an enum constant is added/renamed/removed, or a class which is being stored in the preferences changes internally. The @PreferencesVersion annotation needs to be added to the interface which will be used with StoreBox.create().\\nBy default, without the @PreferencesVersion annotation, the version used is assumed to be 0. The first time a change is required the version for the annotation should be set to 1, with the value being incremented for any subsequent changes. To provide the logic for handling version upgrades a handler class extending from PreferencesVersionHandler needs to be specified.\\n@PreferencesVersion(version = 1, handler = MyPreferencesVersionHandler.class)\\npublic interface MyPreferences {\\n    \\n    // method definitions here\\n}\\n\\npublic class MyPreferencesVersionHandler extends PreferencesVersionHandler {\\n    \\n    @Override\\n    protected void onUpgrade(\\n            SharedPreferences prefs,\\n            SharedPreferences.Editor editor,\\n            int oldVersion,\\n            int newVersion) {\\n        \\n        // logic for handling upgrades\\n    }\\n}\\nFor an initial upgrade from 0 to 1 the onUpgrade method will be called with oldVersion = 0 and newVersion = 1. If the version of the preferences would be updated to 2, then the handler would be called with oldVersion = 1 and newVersion = 2. If however an application update using version 1 was skipped, then onUpgrade would be called with oldVersion = 0 and newVersion = 2, which means that handling the intermediate upgrade between versions 1 and 2 would be required. Calling apply() or commit() on the editor is not required after changes are made, as StoreBox will take care of this when saving the new version value into the preferences. Take a look here for an example of how upgrades could be handled.\\nThe versions are also independent of each other, and apply only to specific preference files. For example, you could have a shared preferences with version X, activity A preferences with version Y, and activity B preferences with version Z. Or none at all, if versioning is not needed.\\nObtaining a more customised instance at run-time\\nAs previously described you can build an instance of your interface using StoreBox.create(), however if you\\'d like to override at run-time any annotations you can use StoreBox.Builder and apply different options.\\nMyPreferences preferences =\\n        new StoreBox.Builder(context, MyPreferences.class)\\n        .preferencesMode(PreferencesMode.MULTI_PROCESS)\\n        .build()\\nDefaults\\nGiven the minimum amount of details provided to the interface and method definitions through the use of StoreBox\\'s annotations, the following defaults will get used:\\n\\nPreferences type: Default shared preferences\\nPreferences mode: Private\\nSave mode: Apply\\nDefault value mode: Empty\\n\\nProguard\\nIf you are using ProGuard add the following lines to your configuration.\\n-dontwarn net.jodah.typetools.TypeResolver\\n-keep class net.orange_box.storebox.** { *; }\\n-keepattributes *Annotation*,Exceptions,InnerClasses,Signature\\n\\n\\nContributing\\nAny contributions thorough pull requests as well as raised issues (bugs are rated just as highly as features!) will be welcome and highly appreciated.\\nIf you would like to submit a pull request please make sure to do so against the develop branch, and please follow a similar code style to the one used in the existing code base. Running the tests before and after any changes is highly recommended, just as is adding new test cases.\\nContributors\\n\\ncr5315\\n\\nLicense\\nCopyright 2015 Martin Bella\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '210',\n",
       "  'forks': '16',\n",
       "  'commits': '96'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'StoreBox\\nAndroid library for streamlining SharedPreferences.\\n\\n\\n \\n\\nContents\\n\\nOverview\\nAdding to a project\\nInterface and creation\\nGet and set methods\\nDefaults for get methods\\nCustom types\\nPreference types\\nAdvanced\\nRemove and clear methods\\nChange listeners\\nChaining calls\\nForwarding calls\\nSave modes\\nVersioning\\nBuilder & Defaults\\nProGuard\\nContributing\\nLicense\\n\\nOverview\\nStoreBox is an annotation-based library for interacting with Android\\'s SharedPreferences, with the aim take out the the how and where parts of retrieving/storing values and instead focus on the more important what part.\\nNormally when retrieving or storing values we need to know two pieces of information during each call: the key and the type.\\nString username = preferences.getString(\"key_username\", null);\\npreferences.edit().putString(\"key_date_of_birth\", \"30/09/2004\").apply(); // should this be a String or a long?\\nWith StoreBox the operations above can be changed into pre-defined methods with improved semantics.\\n// definition\\npublic interface MyPreferences {\\n    \\n    @KeyByString(\"key_username\")\\n    String getUsername();\\n    \\n    @KeyByString(\"key_date_of_birth\")\\n    void setDateOfBirth(String value);\\n}\\n\\n// usage\\nMyPreferences preferences = StoreBox.create(context, MyPreferences.class);\\nString username = preferences.getUsername();\\npreferences.setDateOfBirth(\"30/09/2004\");\\nThe caller now doesn\\'t need to worry about the key, neither about what type the values are stored under. The only important part that needs to be taken into consideration is what is done with the values, whether that is storing them for later, showing them to the user in the UI, or just changing application behaviour.\\nRead on to find out more details about how StoreBox can be used and how it can be added to an Android project.\\nAdding to a project\\nStoreBox can be used in Android projects using minimum SDK version 10 and newer (Android 2.3+).\\nJAR\\nv1.4.0 JAR\\nv1.4.0 JavaDoc JAR\\nGradle\\ncompile \\'net.orange-box.storebox:storebox-lib:1.4.0\\'\\n\\nMaven\\n<dependency>\\n  <groupId>net.orange-box.storebox</groupId>\\n  <artifactId>storebox-lib</artifactId>\\n  <version>1.4.0</version>\\n</dependency>\\n\\nDefining an interface and bringing it to life\\nSimply create a new interface class in your IDE or a text editor, give it an access modifier which suits its use, and name it as appropriate.\\npublic interface MyPreferences {\\n    \\n}\\nNow you\\'re ready to use StoreBox.create() to obtain an instance.\\nMyPreferences instance = StoreBox.create(context, MyPreferences.class);\\nAdding get and set methods\\nIf you would like to add a getter just add a method to the interface which returns a value and make sure to annotate it using @KeyByString or @KeyByResource.\\n@KeyByString(\"key_nickname\")\\nString getNickname();\\n\\n@KeyByResource(R.string.key_notifications)\\nboolean shouldShowNotifications();\\nAdding a setter is just as easy. The same annotations will need to be used as for getter methods, but now our method will return nothing and will have to provide a parameter for supplying the value that should be saved.\\n@KeyByString(\"key_nickname\")\\nvoid setNickname(String value)\\n\\n@KeyByResource(R.string.key_notifications)\\nvoid setNotifications(boolean value)\\nSpecifying defaults for get methods\\nThis can be achieved in two ways, through an argument or by using an annotation.\\nFor the first option the following will work.\\n@KeyByString(\"key_phone_number\")\\nString getPhoneNumber(String defValue);\\nAnd using an annotation referencing a default found in the XML resources.\\n@KeyByString(\"key_phone_number\")\\n@DefaultValue(R.string.default_phone_number)\\nString getPhoneNumber();\\nFor some types, such as long, which cannot be added to the resources an integer resource may be used instead.\\n@KeyByString(\"key_refresh_interval\")\\n@DefaultValue(R.integer.default_refresh_interval)\\nlong getRefreshInterval();\\nStoring and retrieving custom types\\nSaving custom types, which are not understood by Android\\'s SharedPreferences, can be supported through the use of type adapters. A type adapter implementation can be provided by extending from one of the following classes:\\n\\nBaseBooleanTypeAdapter for storing as a Boolean\\nBaseFloatTypeAdapter for storing as a Float and so on...\\nBaseIntegerTypeAdapter\\nBaseLongTypeAdapter\\nBaseStringTypeAdapter\\nBaseStringSetTypeAdapter (only supported on API11 and newer)\\n\\nTelling StoreBox which type adapter should be used can be done by adding the @TypeAdapter annotation to the get and set methods.\\n@KeyByString(\"key_region\")\\n@TypeAdapter(RegionTypeAdapter.class)\\nRegion getRegion();\\n\\n@KeyByString(\"key_region\")\\n@TypeAdapter(RegionTypeAdapter.class)\\nvoid setRegion(Region value);\\nWhich type adapter needs to be extended depends on the use case. Take a look at the DateTypeAdapter, UriTypeAdapter, and CustomClassListTypeAdapter for some examples. It is worth noting that in the last example Gson is being used for serialising the type, as opposed to writing a custom implementation. Gson is not used internally by StoreBox, as such if you wish to use Gson for a type adapter you will need to add it to your project as a dependency.\\nThe following types will work out of the box, so type adapters don\\'t need to be provided for them:\\n\\nDate\\nDouble\\nEnum\\nUri\\n\\nDisclaimer: APIs around type adapters may change in the future, as I will keep looking for a less verbose way of achieving the same goal without requiring the use of Gson.\\nOpening different types of preferences\\nIn all of the examples so far details about what preferences are opened and how have been omitted.\\nWithout any annotation the default shared preferences will be used, but the @DefaultSharedPreferences annotation can be added to the interface definition for explicitness. Likewise, @ActivityPreferences or @FilePreferences can be used to respectively open preferences private to an activity or to open preferences using a file name.\\nThe mode with which the preferences should be opened can also be specified, although this option is not supported by all the types.\\n@ActivityPreferences(mode = PreferencesMode.MULTI_PROCESS)\\npublic interface WelcomeActivityPreferences {\\n    \\n    // method definitions here...\\n}\\nAdvanced\\nRemove and clear methods\\nIn order to remove a value stored in the preferences under a key a method to perform the removal can be annotated with the @RemoveMethod annotation. The key can be supplied in two ways;\\nThe key can be provided thorough an argument in the method, using either a String or an int in the case of the key being specified in an XML resource.\\npublic interface RemoveMethodExample {\\n    \\n    @RemoveMethod\\n    void remove(String key);\\n    \\n    @RemoveMethod\\n    void remove(int keyRes);\\n}\\n\\n// usage\\npreferences.remove(\"key_username\");\\npreferences.remove(R.string.key_password);\\nOr a value-specific remove method can be defined with the help of the @KeyByString or @KeyByResource annotations.\\npublic interface RemoveMethodExample {\\n    \\n    @KeyByString(\"key_username\")\\n    @RemoveMethod\\n    void removeUsername();\\n    \\n    @KeyByResource(R.string.key_password)\\n    @RemoveMethod\\n    void removePassword();\\n}\\n\\n// usage\\npreferences.removeUsername();\\npreferences.removePassword();\\nClearing all values stored in the preferences can be done by annotating a method with the @ClearMethod annotation.\\npublic interface ClearMethodExample {\\n    \\n    @ClearMethod\\n    void clear();\\n}\\n\\n// usage\\npreferences.clear();\\nChange listeners\\nCallbacks can be received when a preference value changes through the use of the OnPreferenceValueChangedListener interface. The listeners need to be parametrised with the type which is used for the value whose changes we would like to listen for. For example, if we would like to listen to changes to the password (from previous examples) then we could define the listener as\\nOnPreferenceValueChangedListener<String> listener = new OnPreferenceValueChangedListener<String>() {\\n    @Override\\n    public void onChanged(String newValue) {\\n        // do something with newValue\\n    }\\n}\\nTo register this listener a method for registering the listener would need to be defined using the @RegisterChangeListenerMethod annotation in the interface which gets passed to StoreBox.create(). Likewise, for unregistering a listener the method needs to be annotated with @UnregisterChangeListenerMethod instead. The @KeyByString or @KeyByResource annotation also needs to be used to specify which value we are interested in.\\npublic interface ChangeListenerExample {\\n    \\n    @KeyByString(\"key_password\")\\n    @RegisterChangeListenerMethod\\n    void registerPasswordListener(OnPreferenceValueChangedListener<String> listener);\\n    \\n    @KeyByString(\"key_password\")\\n    @UnregisterChangeListenerMethod\\n    void unregisterPasswordListener(OnPreferenceValueChangedListener<String> listener);\\n}\\nIf you would like to listen for changes to a custom type then the @TypeAdapter annotation will need to be added to the method in order to tell StoreBox how the value should be adapted when retrieving it from the preferences.\\nMore than one listener can be registered and unregistered at a time by changing the method definitions in the interface to use variable arguments.\\n// annotations omitted\\nvoid registerPasswordListeners(OnPreferenceValueChangedListener<String>... listeners);\\nCaution: StoreBox does not store strong references to the listeners. A strong reference must be kept to the listener for as long as the listener will be required, otherwise it will be susceptible to garbage collection.\\nChaining calls\\nWith Android\\'s SharedPreferences.Editor class it is possible to keep chaining put methods as each returns back the SharedPreferences.Editor instance. StoreBox allows the same functionality. All that needs to be done is to change the set/remove method definitions to either return interface type itself or SharedPreferences.Editor.\\npublic interface ChainingExample {\\n    \\n    @KeyByString(\"key_username\")\\n    ChainingExample setUsername(String value);\\n    \\n    @KeyByString(\"key_password\")\\n    ChainingExample setPassword(String value);\\n    \\n    @KeyByString(\"key_country\")\\n    ChainingExample removeCountry();\\n}\\nAnd calls can be chained as\\npreferences.setUsername(\"Joe\").setPassword(\"jOe\").removeCountry();\\nForwarding calls\\nIf you would like to access methods from the SharedPreferences or SharedPreferences.Editor, you can do that by extending your interface from either of the above (or even both).\\npublic interface ForwardingExample extends SharedPreferences, SharedPreferences.Editor {\\n    \\n    // method definitions here\\n}\\nAnd the methods from either of the extended interfaces will be callable.\\nString username = preferences.getString(\"key_username\", \"\");\\npreferences.putString(\"key_username\", \"Joe\").apply();\\nSave modes\\nChanges to preferences can normally be saved on Android either through apply() or commit(). Which method gets used can be customised in StoreBox through the use of the @SaveOption annotation.\\nUnlike any of the previous annotations @SaveOption can be used to annotate both the interface as well as individual set/remove methods, however an annotation at method-level will take precedence over an interface annotation.\\n@SaveOption(SaveMode.APPLY)\\npublic interface SaveModeExample {\\n    // key annotations omitted\\n    \\n    void setUsername(String value); // will save using apply()\\n    \\n    @SaveOption(SaveMode.COMMIT)\\n    void setPassword(String value); // will save using commit()\\n    \\n    @SaveOption(SaveMode.COMMIT)\\n    void removeUsername(); // will persist using commit()\\n}\\nVersioning\\nStoreBox supports versioning of preferences through the use of the @PreferencesVersion interface-level annotation, in a similar fashion to Android\\'s SQLiteOpenHelper. This functionality may be required in the case when the schema of the preferences needs to be changed, such as when a key or type of a preference changes, an enum constant is added/renamed/removed, or a class which is being stored in the preferences changes internally. The @PreferencesVersion annotation needs to be added to the interface which will be used with StoreBox.create().\\nBy default, without the @PreferencesVersion annotation, the version used is assumed to be 0. The first time a change is required the version for the annotation should be set to 1, with the value being incremented for any subsequent changes. To provide the logic for handling version upgrades a handler class extending from PreferencesVersionHandler needs to be specified.\\n@PreferencesVersion(version = 1, handler = MyPreferencesVersionHandler.class)\\npublic interface MyPreferences {\\n    \\n    // method definitions here\\n}\\n\\npublic class MyPreferencesVersionHandler extends PreferencesVersionHandler {\\n    \\n    @Override\\n    protected void onUpgrade(\\n            SharedPreferences prefs,\\n            SharedPreferences.Editor editor,\\n            int oldVersion,\\n            int newVersion) {\\n        \\n        // logic for handling upgrades\\n    }\\n}\\nFor an initial upgrade from 0 to 1 the onUpgrade method will be called with oldVersion = 0 and newVersion = 1. If the version of the preferences would be updated to 2, then the handler would be called with oldVersion = 1 and newVersion = 2. If however an application update using version 1 was skipped, then onUpgrade would be called with oldVersion = 0 and newVersion = 2, which means that handling the intermediate upgrade between versions 1 and 2 would be required. Calling apply() or commit() on the editor is not required after changes are made, as StoreBox will take care of this when saving the new version value into the preferences. Take a look here for an example of how upgrades could be handled.\\nThe versions are also independent of each other, and apply only to specific preference files. For example, you could have a shared preferences with version X, activity A preferences with version Y, and activity B preferences with version Z. Or none at all, if versioning is not needed.\\nObtaining a more customised instance at run-time\\nAs previously described you can build an instance of your interface using StoreBox.create(), however if you\\'d like to override at run-time any annotations you can use StoreBox.Builder and apply different options.\\nMyPreferences preferences =\\n        new StoreBox.Builder(context, MyPreferences.class)\\n        .preferencesMode(PreferencesMode.MULTI_PROCESS)\\n        .build()\\nDefaults\\nGiven the minimum amount of details provided to the interface and method definitions through the use of StoreBox\\'s annotations, the following defaults will get used:\\n\\nPreferences type: Default shared preferences\\nPreferences mode: Private\\nSave mode: Apply\\nDefault value mode: Empty\\n\\nProguard\\nIf you are using ProGuard add the following lines to your configuration.\\n-dontwarn net.jodah.typetools.TypeResolver\\n-keep class net.orange_box.storebox.** { *; }\\n-keepattributes *Annotation*,Exceptions,InnerClasses,Signature\\n\\n\\nContributing\\nAny contributions thorough pull requests as well as raised issues (bugs are rated just as highly as features!) will be welcome and highly appreciated.\\nIf you would like to submit a pull request please make sure to do so against the develop branch, and please follow a similar code style to the one used in the existing code base. Running the tests before and after any changes is highly recommended, just as is adding new test cases.\\nContributors\\n\\ncr5315\\n\\nLicense\\nCopyright 2015 Martin Bella\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '40',\n",
       "  'stars': '206',\n",
       "  'forks': '75',\n",
       "  'commits': '5'},\n",
       " {'language': 'Java 61.1',\n",
       "  'readme': 'AndFixDemo\\n关于为什么要选择AndFix,可以去看看我之前写的文章AndFix的抉择\\nAndroidStudio，AndFix的Demo（官方的Demo是Eclipse的）\\nAPK我已经生成好了，另加基本使用教程（官方的文档写的不是很清晰）\\n反正我什么都准备好了，想体验一下的可以看一下！！\\n直接体验\\n我已经准备好了一切，是不是超贴心！\\n安装 1.apk\\ncd apkpatch-1.0.3\\nadb install 1.apk\\n\\nRun 起来，查看 log，Tag为 MainActivity，可以看到日志：\\nMainActivity: a\\nMainActivity: 0\\nMainActivity: 10\\n\\npush 补丁到SD卡\\n把apatch push到SD卡 sdcard 是我的路径 根据自己的路径来\\nadb push patch/out.apatch /sdcard/\\n\\n重新安装1.apk 并run\\nadb install -r 1.apk\\n\\nRun 起来，Tag 为APP有如下日志输出：\\nApp: inited.\\nApp: apatch loaded.\\nApp: apatch:/storage/emulated/0/out.apatch added.\\n\\nMainActivity 日志 :\\nMainActivity: b （不再是a而是b了！）\\nMainActivity: 0\\nMainActivity: 10\\n\\n同时，TAG为euler 也有fix success的信息输出\\n表示成功啦！~\\n生成apatch\\n官方没有给具体的实例，这里我提供一个实例：\\n注意我的是MAC,所以是.sh文件，debugkeystore是AS自带的，2.apk是新的apk，1是旧的\\n./apkpatch.sh -f 2.apk -t 1.apk -k debug.keystore -p android -a androiddebugkey -e android -o patch\\n\\n执行完会有如下日志输出，并且在patch目录下生成补丁文件\\nadd modified Method:Ljava/lang/String;  a(Ljava/lang/String;)  in Class:Lme/yifeiyuan/andfixdemo/A;\\nadd new Method:Ljava/lang/String;  c()  in Class:Lme/yifeiyuan/andfixdemo/A;\\nadd modified Method:I  b(Ljava/lang/String;Ljava/lang/String;)  in Class:Lme/yifeiyuan/andfixdemo/A;\\n\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '204',\n",
       "  'forks': '30',\n",
       "  'commits': '7'},\n",
       " {'language': 'Java 62.1',\n",
       "  'readme': 'What are Dexx Collections?\\nDexx Collections are a port of Scala\\'s immutable, persistent collection classes to pure Java.\\nPersistent in the context of functional data structures means the data structure preserves the previous version of itself when modified. This means any reference to a collection is effectively immutable. However, modifications can be made by returning a new version of the data structure, leaving the original structure unchanged.\\nHere\\'s an example using Dexx\\'s Sets (examples are in Kotlin for conciseness, but the collections are pure Java):\\nval set1 = Sets.of(1, 2, 3)\\nval set2 = set1.add(4)\\nval set3 = set1.remove(1)\\nprintln(set1) // Prints Set(1, 2, 3)\\nprintln(set2) // Prints Set(1, 2, 3, 4)\\nprintln(set3) // Prints Set(2, 3)\\nFrom the above example we can see that although we\\'ve made modifications to set1 to create set2 and set3, the contents of set1 remain unchanged.\\nNote: There\\'s now first class support for Kotlin - see the kollection module README for more information.\\nWhy port?\\nScala\\'s collections can be directly used from Java, but the resulting code is far from idiomatic. Scala\\'s standard library is also large and binary incompatible between versions.\\nSecondly, a pure Java implementation of functional persistent collections is usable from not only Java, but other JVM languages that interoperate with Java such as Kotlin, Ceylon or GWT. In fact, the collections have been specifically designed for use with Kotlin.\\nOverview\\nThe diagram below shows Dexx\\'s class hierarchy (interfaces are in blue and concrete implementations are in green).\\n\\nNote that the interfaces such as Map, Set and List are not related to the java.util equivalents as persistent collections require all modification methods such as add and remove to return a new collection instance.\\nStatus\\n\\nAll collections have been implemented\\nHashSet, TreeSet, HashMap, TreeMap and Vector are ports from Scala\\nConsList and ArrayList have been written from scratch.\\nHelper classes for construction and adapters to java.util collections are available\\nTest coverage is fairly comprehensive: 95% line and 90% branch at present\\n\\nDependencies\\n\\nThere are no runtime dependencies\\nJetBrain\\'s annotations (@NotNull and @Nullable) are used in the source to support Kotlin\\'s nullable types, but they are not required at runtime.\\nThe tests are written in Kotlin, but again this is not a runtime dependency\\n\\nRoadmap\\n\\nExplore annotating methods that return a new collection with @CheckReturnValue\\nto allow static verification of collection usage.\\nActive development is essentially complete. Further work is expected to be bug fixes and refinements.\\n\\nRelease Notes\\n\\n0.7:\\n\\nFixes #11 - a balancing error in red black trees\\n\\n\\n0.6:\\n\\nAdded OSGI metadata (thanks ajs6f)\\nMake internal fields final (thanks mkull)\\nPerformance improvement to first and last of TreeMap & TreeSet (thanks mkull)\\n\\n\\n0.5:\\n\\nUpdated to 1.0.0\\nAdded toImmutableMap() conversions from existing Maps\\n\\n\\n0.4:\\n\\nUpdated to 1.0.0-rc-1036\\nRemoved accidental assertJ compile dependency in kollection (thanks @brianegan)\\n\\n\\n0.3.1:\\n\\nAdded a native Kotlin api in the kollection module\\nConverted the build to gradle from maven\\nRenamed dexx-collections artifact to collection\\n\\n\\n0.2:\\n\\nAdd LinkedLists support with ConsList as the default implementation\\nAdd ArrayList as an alternative IndexedList implementation\\nFormalise the Builder contract and enforce at runtime\\n\\n\\n0.1:\\n\\nIncludes ports of Scala\\'s HashSet, TreeSet, HashMap, TreeMap and Vector\\n\\n\\n\\nLicense\\nThis project is licensed under a MIT license. Portions ported from Scala are Scala\\'s 3-clause BSD license.\\nUsage\\nAdding to your project\\nVersion 0.7 has been released and is available in Maven Central here. You can use it via the following gradle dependency:\\n\\'com.github.andrewoma.dexx:collection:0.7\\' // For Java\\n\\'com.github.andrewoma.dexx:kollection:0.7\\' // For Kotlin\\nConstructing collections\\nEach of the leaf interfaces (Set, SortedSet, Map, SortedMap, IndexedList and LinkedList) have\\nassociated companion classes with static methods for construction.\\nThe companion class uses the plural form of the interface. e.g. Set has a companion class of Sets.\\nTo build a collection from a fixed number of elements, use the overloaded of() methods. e.g.\\nval set = Sets.of(1, 2, 3)\\nTo build a collection from a java.util collection, use the copyOf() methods. e.g.\\nval set = Sets.copyOf(javaCollection)\\nBuilders should be used when incrementally constructing a collection. This allows for more efficient structures\\nto be used internally during construction. In the case of LinkedList, using a builder is important as LinkedList does not support appending without copying the entire collection.\\nval builder = Sets.builder<Int>()\\nfor (i in 1..100) {\\n    builder.add(i)\\n}\\nval set = builder.build()\\nViewing as java.util collections\\nUnfortunately, the java.util collection interfaces are not compatible with persistent collections as\\nmodifications such as add() must return a new collection instance, leaving the original untouched.\\nHowever, all collections can be viewed as an immutable form of their java.util equivalent by using the\\nthe as...() methods.\\nval javaSet = Sets.of(1, 2, 3).asSet() // Now a java.util.Set\\nWhere are filter(), map() and friends?\\nSuch transformations are deliberately not supported:\\n\\n\\nIn JDK versions < 1.8, using a functional style is ugly and not recommended.\\nSee Excessive use of Guava\\'s functional programming idioms can lead to verbose, confusing, unreadable, and inefficient code.\\n\\n\\nIn Kotlin and JDK 1.8, the platform provides transformations that can be used on the collections for free.\\nAdding another set of transformations directly to the collections (with subtly different semantics) seems harmful.\\n\\n\\nHere\\'s an example of using lazy evaluation in a functional style with Kotlin:\\nval set = SortedSets.of(1, 2, 3, 4, 5, 6).asSequence()\\n        .filter { it % 2 == 0 }\\n        .map { \"$it is even\" }\\n        .take(2)\\n        .toImmutableSet()\\n\\nassertEquals(SortedSets.of(\"2 is even\", \"4 is even\"), set)\\nThe example above uses Kotlins in-built extension function that converts any Iterable into a Sequence.\\nIt also uses the following extension functions to add Sequence<T>.toImmutableSet() to cleanly convert the sequence\\nback into a Dexx Collection.\\nfun <T, R> Sequence<T>.build(builder: Builder<T, R>): R {\\n    this.forEach { builder.add(it) }\\n    return builder.build()\\n}\\n\\nfun <T> Sequence<T>.toImmutableSet(): SortedSet<T> = build(SortedSets.builder<T>())\\nPerformance\\nBenchmarking is still a work in progress (all the warnings about JVM benchmarks apply). The results so far\\nrunning on Mac OS X 10.11.1 x86_64 with JDK 1.8.0_65 (Oracle Corporation 25.65-b01) are here.\\nMy conclusions so far are that the collections perform adequately to be used as a drop-in replacement\\nfor the majority of use cases. While slower, slow is generally referring to millions of operations per second.\\nIn general, mutating methods incur a overhead of 2-5 times that of java.util equivalents and\\nreading operations are 1-1.5 time slower.\\n@ptitjes Has done some more rigorous benchmarks here: https://github.com/ptitjes/benchmark-immutables/blob/master/results/2016-10-02-23:56:36.pdf\\nDevelopment\\n\\nDexx is built with gradle. Use ./gradlew install to build and install into your local repository.\\nTo run the benchmarks, use ./gradlew check -PdexxTestMode=BENCHMARK --info | grep \\'^    BENCHMARK:\\'.\\nTo generate coverage reports, use:\\n\\n./gradlew :collection:clean :collection:check :collection:jacocoTestReport\\n open collection/build/jacocoHtml/index.html\\n\\n\\nBy default, a quick version of tests are run. Getting better test coverage of Vectors requires large\\ncollections. To run tests with complete coverage use: ./gradlew -PdexxTestMode=COMPLETE :collection:clean :collection:check :collection:jacocoTestReport\\n\\nMethod counts\\nFor android developers, here are method counts:\\n\\n\\'com.github.andrewoma.dexx:collection:0.6\\' = 1036 methods\\n\\'com.github.andrewoma.dexx:kollection:0.6\\' = 1213 methods\\n\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '203',\n",
       "  'forks': '12',\n",
       "  'commits': '146'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': ' Gank\\n干货集中营Android客户端！\\n\\n数据来源：\\n本应用所有数据均来自干货集中营的开放API。\\n应用简介：\\n干货集中营安卓客户端，随时随地了解技术最新动态(MeiZi)。除周末部分节假日外，每日更新一个美图，一个小视频，以及各类技术最新文章。\\n界面预览：\\n\\n下载应用：\\n\\n【扫描二维码或点击这里获取应用】\\n设计理念：\\n本应用秉承简洁而不简单的理念进行设计，没有任何多余的功能：\\n例如：没有常见的内置浏览器，没有收藏，虽然内置浏览器非常常见，然而在此处是很糟糕的一种体验，因为大部分的技术相关文章并不适合在手机端阅读，看到比较好的文章通常需要收藏下来留待在电脑上详细阅览，然而因为鄙人没有开发电脑客户端的能力，所以即使在手机端 收藏了也不能提供更良好的体验效果，因此采用了外置浏览器，目前来说很多浏览器都具有账号登陆功能，例如360浏览器，Chrome等，账号登陆后可方便的将手机收藏的网页同步到电脑浏览器上，反而可以提供更好的体验效果。\\n以方便为主，注重细节：\\n对内容也采用了磁盘缓存，在没网状态下依旧可以看到内容(没网只能看到缓存的妹子)，图片部分则采用双缓存，节省流量，提高加载速度，并且图片部分也有用浏览器打开选项，配合Chrome和Google，可以在浏览器中搜索图片出处，高清大图，相似的妹子图等.......让一张妹子变成一系列妹子🔞\\n建议配合Chrome使用:\\n\\nChrome\\n科学上网\\n\\n开发计划：\\n基于用户的反馈，修复完善或者添加以下功能(欢迎在Issues反馈)：\\n\\n 解决部分机型闪退或者崩溃问题\\n 完善妹子页面图片加载缓存问题\\n 添加左右滑动切换妹子图片功能\\n 缓存WebView，提供离线阅读\\n\\n特别鸣谢：\\n\\n@代码家\\n@Mr.Simple\\n#Retrofit\\n#ASimpleCache\\n#Android-Universal-Image-Loader\\n众多的开源党\\n\\n\\nAbout Me\\n作者微博: @GcsSloop\\n  \\nLicense\\nCopyright (c) 2016 GcsSloop\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '201',\n",
       "  'forks': '59',\n",
       "  'commits': '26'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Metaball-Menu\\nA menu consisting of icons (ImageViews) and metaball bouncing selection to give a blob effect. Inspired by Material design\\n\\nScreenShot\\n\\n\\nUsage\\nThe usage is fairly easy.\\n<com.metaballmenu.widgets.MetaballMenu\\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n    xmlns:tools=\"http://schemas.android.com/tools\"\\n    android:id=\"@+id/metaball_menu\"\\n    android:layout_width=\"wrap_content\"\\n    android:layout_height=\"wrap_content\"\\n    android:gravity=\"center\"\\n    android:orientation=\"horizontal\"\\n    android:padding=\"10dp\"\\n    android:layout_gravity=\"center\"\\n    app:backgroundColor=\"@android:color/holo_purple\"\\n    app:metaballColor=\"@android:color/white\"\\n    app:drawablePadding=\"10dp\"\\n    app:backgroundShapeRadius=\"30dp\"\\n    app:needsElevation=\"true\">\\n \\n    <ImageView\\n        android:id=\"@+id/menuitem1\"\\n        android:layout_width=\"0dp\"\\n        android:layout_height=\"wrap_content\"\\n        android:layout_weight=\"1\"\\n        android:layout_margin=\"5dp\"\\n        android:padding=\"5dp\"\\n        android:src=\"@mipmap/card\"\\n        />\\n</com.metaballmenu.widgets.MetaballMenu>\\nI have used an Imageview. But any view can be used to obtain the effect.\\nCheck out the uploaded project for usage.\\nThe code is based on the following references:\\n\\nMetaball Loading by Dodola - Thanks for the Path draw functions on Android\\nPaperJS Metaball Example\\nCalvin Metcalf\\n\\n\\nLICENSE\\nThe MIT License (MIT)\\nCopyright (c) 2015 Melvin Lobo\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '199',\n",
       "  'forks': '44',\n",
       "  'commits': '12'},\n",
       " {'language': 'Java 93.4',\n",
       "  'readme': \"trackr backend\\nWhat is it?\\ntrackr is an application to track petty much everything that is going on in your company.\\nKeep track of vacations, sick days, invoices and many more.\\ntrackr comes with a Java-based backend and a frontend written in AngularJS. This project is the Java/Spring based\\nbackend, a stateless REST API with either OAuth2 or basic authentication.\\nYou can read all about trackr in our developer blog:\\n\\nArchitecture and Backend\\nTesting\\nMail Approvals with Spring Integration\\nFrontend\\nFile Downloads with AngularJS\\nProcesses and Tools\\n\\nFor the API documentation just go here.\\nThere is also a Vagrant project building the whole application over here.\\nHow to start\\nIf you just want to mess around with the API a bit the default configuration is very sensible and has no external dependencies (well, except Java).\\nIf you have gradle, just run\\ngradle run\\n\\nIf you don't have gradle and want to use the wrapper run\\n./gradlew run\\n# or\\ngradlew.bat run\\n\\nIf you want to start from your IDE, i.e. for debugging open the class Trackr and start the main method.\\nTo verify it works you can use curl. The users don't have a password in this configuration, so just press enter when curl asks for one. If you don't like the usernames\\nchange them in import.sql.\\ncurl --user moritz.schulze@techdev.de localhost:8080\\n\\nThe default config uses port 8080, if that is used on your system you can add\\nserver:\\n    port: $port\\n\\nto the top of the application.yaml and choose a port that you want for $port.\\nProfiles\\ntrackr has a lot of Spring profiles to add/switch features.\\n\\n\\n\\nprofile\\ndescription\\nnotes\\n\\n\\n\\n\\nin-memory-database\\nuses a H2 database, creates the schema with hibernate\\nexcluse with real-database\\n\\n\\nreal-database\\nuses a configurable database, executes flyway\\nexclusive with in-memory-database\\n\\n\\nhttp-basic\\nprotects the API with HTTP basic authentication\\nexclusive with oauth\\n\\n\\noauth\\nprotects the API as a OAuth2 resource server\\nexclusive with http-basic. Database for OAuth2 tokens needed.\\n\\n\\ngranular-security\\nroles and per endpoint security\\n\\n\\n\\ngmail\\nsends mail with Gmail and enables mail receiving\\nwhen off, does not receive mails and uses a logging mail sender.\\n\\n\\ndev\\ninitialize the database with data.sql\\n\\n\\n\\nprod\\nJust some different settings for our production env\\n\\n\\n\\n\\nTake a look in the application.yaml to see what properties these profiles need.\\nThe default profiles are in-memory-database,dev,granular-security,http-basic. If you want to use other profiles, there are several possible ways.\\n\\nYou can change the spring.profiles.active value in application.yaml\\nIf you use gradle run you can prepend (example) SPRING_PROFILES_ACTIVE=dev,gmail,real-database. You can also use this to overwrite e.g. the port with SERVER_PORT=8000.\\nIf you run from your IDE, you can add --spring.profiles.active=dev,gmail,real-database as program arguments to the run configuration.\\n\\nPlease refer to the Spring Boot Reference for more information.\\nThe oauth profile\\nThe oauth profile marks the trackr backend as a OAuth2 resource server, that means access is only possible with a valid access token issued by an authorization server. We use a\\nJDBC token store, so valid tokens need to be put there. Please take a look at our (soon to be open sourced) techdev portal to see how we do this.\\nThe granular-security profile\\nWhen this is not selected, to access the API the user needs to be authenticated. With granular security the access to some endpoints depend on the role of the user or even the\\nid of the user. In trackr, the id of a user is the email address of the belonging employee.\\nWhen the oauth profile is switched off, all users have the role ROLE_ADMIN. When oauth is on, the roles must be stored in the access token.\\nTake a look at the @PreAuthorize and @PostAuthorize annotations in the code to see what this will activate.\\nHow to build\\nJust run\\ngradle build\\n\\n(or use the wrapper if you don't have gradle installed). The JAR file will be in build/libs and can just be run with java -jar. The application.yaml file has to be in the\\nworking directory where the java command was issued.\\n\",\n",
       "  'watchers': '38',\n",
       "  'stars': '198',\n",
       "  'forks': '103',\n",
       "  'commits': '519'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'RetroDagger\\nAndroid project using MVP + Dagger2 + Retrofit.\\nThis code was created to support this article: https://medium.com/@franpulido/retrofit-dagger-mvp-305ac2cf646c\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '194',\n",
       "  'forks': '25',\n",
       "  'commits': '12'},\n",
       " {'language': 'C++ 91.2',\n",
       "  'readme': 'Mephisto\\n\\nDependencies\\nAll Platforms\\nReSwitched unicorn fork:\\ngit clone git@github.com:reswitched/unicorn.git\\ncd unicorn\\nUNICORN_ARCHS=\"aarch64\" ./make.sh\\nsudo ./make.sh install\\n\\nPython packages:\\npip install -r requirements.txt\\n\\nUbuntu\\nInstall Clang 5 from the LLVM PPA: http://apt.llvm.org/\\nYou may need to update libc++ as well, if you get tuple errors.\\nOSX\\nInstall llvm (will take a while)\\nbrew install llvm --HEAD\\n\\nPatch Makefile\\ndiff --git a/Makefile b/Makefile\\nindex e4c921b..4d53420 100644\\n--- a/Makefile\\n+++ b/Makefile\\n@@ -1,4 +1,4 @@\\n-CC := clang++-4.0\\n+CC := clang++\\nRunning\\nMuch like the original CageTheUnicorn, the default use of Mephisto is via the load files.  Create a directory, e.g. ns23, and then copy in the NSO file(s).  Create a file inside this, called load.meph with the following format:\\n(load-nso \"main\" 0x7100000000)\\n(run-from 0x7100000000)\\n\\nRunning it is then as simple as:\\n./ctu ns23\\n\\nAlternatively, you can pass a single NSO file on the command line:\\n./ctu --load-nso ns23/main\\n\\nSee help for other info, e.g. enabling GDB support.\\nRun through Docker\\nFirst build the docker image, this may take some time\\ndocker build -t reswitched/mephisto .\\nTo run Mephisto it needs access to your NSO/NRO files, make sure to bind mount the location into the container.\\nExample:\\ndocker run -ti --rm -p 24689:24689 -v $HOME:$HOME -u $UID reswitched/mephisto --load-nro $HOME/Coding/libtransistor/build/test/test_helloworld.nro\\nYou can also create a bash alias.\\nalias ctu=\\'docker run -ti --rm -p 24689:24689 -v $HOME:$HOME -u $UID reswitched/mephisto\\'\\n\\nNow you can simply run ctu with your desired arguments.\\nExample:\\nctu --load-nro $HOME/Coding/libtransistor/build/test/test_helloworld.nro\\n',\n",
       "  'watchers': '26',\n",
       "  'stars': '118',\n",
       "  'forks': '19',\n",
       "  'commits': '104'},\n",
       " {'language': 'C++ 66.8',\n",
       "  'readme': 'Warning!\\nThis library has been discarded. A new library named WeeESP8266 is recommended, which is more easy-to-use for users.\\nWeeESP8266 can be downloaded at https://github.com/itead/ITEADLIB_Arduino_WeeESP8266.\\nESP8266 library\\nWhen you use with UNO board, uncomment the follow line in uartWIFI.h.\\n#define UNO\\n\\nWhen you use with MEGA board, uncomment the follow line in uartWIFI.h.\\n#define MEGA\\n\\nConnection:\\nWhen you use it with UNO board, the connection should be like these:\\nESP8266_TX->D0\\nESP8266_RX->D1\\nESP8266_CHPD->3.3V\\nESP8266_VCC->3.3V\\nESP8266_GND->GND\\nFTDI_RX->D3\\t\\t\\t//The baud rate of software serial can\\'t be higher that 19200, so we use software serial as a debug port\\nFTDI_TX->D2\\nWhen you use it with MEGA board, the connection should be like these:\\nESP8266_TX->RX1(D19)\\nESP8266_RX->TX1(D18)\\nESP8266_CH_PD->3.3V\\nESP8266_VCC->3.3V\\nESP8266_GND->GND\\nWhen you want to output the debug information, please use DebugSerial. For example,\\nDebugSerial.println(\"hello\");\\nAttention\\nNote1:\\tThe size of message from ESP8266 is too big for arduino sometimes, so the library can\\'t receive the whole buffer because\\nthe size of the hardware serial buffer which is defined in HardwareSerial.h is too small.\\nOpen the file from \\\\arduino\\\\hardware\\\\arduino\\\\avr\\\\cores\\\\arduino\\\\HardwareSerial.h.\\nSee the follow line in the HardwareSerial.h file.\\n#define SERIAL_BUFFER_SIZE 64\\n\\nThe default size of the buffer is 64. Change it into a bigger number, like 256 or more.\\nThe SRAM size of mega is bigger than UNO\\'s, so it is better to use MEGA board to communicate with ESP8266.\\nBUG: When you use this library and receive the http package, it might miss some characters because the library can\\'t process so much data in the same time.\\nCreated by Stan Lee(Lizq@iteadstudio.com)\\n2014/10/8\\nModified version\\nV1.0\\treleased the first version of ESP8266 library\\n',\n",
       "  'watchers': '39',\n",
       "  'stars': '118',\n",
       "  'forks': '72',\n",
       "  'commits': '5'},\n",
       " {'language': 'C++ 96.3',\n",
       "  'readme': 'This repository and the associated neural2d.net website are no longer supported and will be going away soon. Neural2d started as a fun weekend project, then gained some momentum from excellent code contributions from a number of programmers. But I’m at a place in my life where I need to turn my attention elsewhere. I would refer users and educators to the many excellent neural net resources that are more pedagogically effective and more technologically current than this project.\\nThe most recent neural2d user manual is shown below for historical reference.\\nA sincere thank-you to everyone who particpiated in neural2d, and best wishes in your neural-netting.\\n— Dave\\nNeural2d - Neural Net Simulator User Manual\\nFeatures\\n\\nOptimized for 2D input data\\nNeuron layers can be abstracted as 1D or 2D arrangements of neurons\\nInput data can binary or text\\nNetwork topology is defined in a text file\\nNeurons in layers can be fully or sparsely connected\\nSelectable transfer function per layer\\nAdjustable or automatic training rate (eta)\\nOptional momentum (alpha) and regularization (lambda)\\nConvolution filtering and convolution networking\\nStandalone console program\\nSimple, heavily-commented code, suitable for prototyping, learning, and experimentation\\nOptional web-browser-based GUI controller\\nGraphic visualizations of hidden-layer data\\nNo dependencies! Just C++11 (and POSIX networking for the optional webserver interface)\\n\\nDocument Contents\\nOverview\\nRequirements\\nCompiling the source\\nHow to run the digits demo\\nHow to run the XOR example\\nGUI interface\\nHow to use your own data\\nThe 2D in neural2d\\nConvolution filtering\\nConvolution networking and pooling\\nLayer depth\\nTopology config file format\\nTopology config file examples\\nHow-do-I X?\\n\\nHow do I run the command-line program?\\nHow do I run the GUI interface?\\nHow do I disable the GUI interface?\\nHow do I use my own data instead of the digits images?\\nHow do I use a trained net on new data?\\nHow do I train on the MNIST handwritten digits data set?\\nHow do I change the learning rate parameter?\\nAre the output neurons binary or floating point?\\nHow do I use a different transfer function?\\nHow do I define a convolution filter?\\nHow do I define convolution networking and pooling?\\nHow do the color image pixels get converted to floating point for the input layer?\\nHow can I use .jpg and .png images as inputs to the net?\\nHow do I find my way through the source code?\\nWhy does the net error rate stay high? Why doesn\\'t my net learn?\\nWhat other parameters do I need to know about?\\n\\nOverview\\nNeural2d is a standalone console program with optional HTTP web\\ninterface written in C++. It\\'s a backpropagation neural net simulator,\\nwith features that make it easy to think of your input data as either\\none-dimensional or two-dimensional. You specify a network topology in\\na text file (topology.txt). The input data to the neural is specified\\nin a text file (inputData.txt). The inputData.txt file can contain the\\nactual input values for all the input samples, or it can contain a list\\nof .bmp or .dat files that contain the input data in binary form.\\nNeural2d is for educational purposes. It\\'s not production-quality code,\\nand it\\'s not burdened with a lot of bling. It\\'s just heavily-commented\\nneural net code that you can take apart and see how it works. Or modify\\nit and experiment with new ideas. Or extract the functions you need and\\nembed them into your own project.\\nIf you\\'re not using the optional GUI interface, then neural2d has no\\ndependencies other than a conforming C++11 compiler. If using the GUI\\ninterface, you\\'ll need to link with a standard POSIX sockets networking\\nlibrary.\\nRequirements\\n\\nC++-11 compiler\\n\\ne.g., g++ 4.7 on Linux or Cygwin, Visual Studio 2013 on Windows\\n\\n\\nPOSIX sockets (only needed if compiling the optional GUI)\\n\\ne.g., Cygwin on Windows\\n\\n\\nCMake 2.8.12 or later\\nCompiles and runs on Linux, Windows, and probably Mac\\n\\nCompiling the source\\nWe use CMake to configure the build system. First get the source code\\nfrom the Gitub repository. If using the command line, the command is:\\n git clone https://github.com/davidrmiller/neural2d\\n\\nThat will put the source code tree into a directory named neural2d.\\nCompiling with CMake graphical interface\\nIf you are using the CMake graphical interface, run it and set the\\n\"source\" directory to the neural2d top directory, and set the binary\\noutput directory to a build directory under that (you must create the\\nbuild directory), then click Configure and Generate. Uncheck WEBSERVER\\nif you don\\'t want to compile the optional GUI. Here is what it looks like:\\n\\nCompiling with CMake command line interface\\nIf you are using CMake from the command line, cd to the neural2d top\\nlevel directory, make a build directory, then run cmake from there:\\ngit clone https://github.com/davidrmiller/neural2d\\ncd neural2d\\nmkdir build\\ncd build\\ncmake ..\\nmake\\n\\nThere is no \"install\" step. After the neural2d program is compiled,\\nyou can execute it or open the project file from the build directory.\\nOn Windows, by default CMake generates a Microsoft Visual Studio project\\nfile in the build directory. On Linux and Cygwin, CMake generates\\na Makefile that you can use to compile neural2d. You can specify a\\ndifferent CMake generator with the -G option, for example:\\n cmake -G \"Visual Studio 14 2015\" ..\\n\\nTo get a list of available CMake generators:\\n cmake --help\\n\\nIf you get errors when compiling the integrated webserver, you can\\nbuild neural2d without webserver support by running CMake with the\\n-DWEBSERVER=OFF option, like this:\\n cmake -DWEBSERVER=OFF ..\\n\\nHow to run the digits demo\\nOn systems using Makefiles, in the build directory, execute:\\nmake test\\n\\nThis will do several things: it will compile the\\nneural2d program if necessary; it will expand the\\narchive named image/digits/digits.zip into 5000 individual\\nimages;\\nand it will then train the neural net to classify those digit images.\\nThe input data, or \"training set,\" consists of images of numeric\\ndigits. The first 50 look like these:\\n\\nThe images are 32x32 pixels each, stored in .bmp format. In this demo,\\nthe neural net is configured to have 32x32 input neurons, and 10 output\\nneurons. The net is trained to classify the digits in the images and\\nto indicate the answer by driving the corresponding output neuron to a\\nhigh level.\\nOnce the net is sufficiently trained, all the connection weights are\\nsaved in a file named \"weights.txt\".\\nIf you are not using Makefiles, you will need to expand the archive in\\nimages/digits, then invoke the neural2d program like this:\\n neural2d ../images/digits/topology.txt ../images/digits/inputData.txt weights.txt\\n\\nHow to run the XOR example\\nOn systems using Makefiles, in the build directory, execute:\\n make test-xor\\n\\nFor more information about the XOR example, see\\nthis wiki page.\\nGUI interface (optional)\\nFirst, launch the neural2d console program in a command window with the -p option:\\n ./neural2d topology.txt inputData.txt weights.txt -p\\n\\nThe -p option causes the neural2d program to wait for a command before\\nstarting the training. The screen will look something like this:\\n\\nAt this point, the neural2d console program is paused and waiting for\\na command to continue. Using any web browser, open:\\n http://localhost:24080\\n\\nA GUI interface will appear that looks like:\\n\\nPress Resume to start the neural net training. It will automatically\\npause when the average error rate falls below a certain threshold (or\\nwhen you press Pause). You now have a trained net. You can press Save\\nWeights to save the weights for later use.\\nSee the neural2d wiki for\\ndesign notes on the web interface.\\nVisualizations\\nAt the bottom of the GUI window, a drop-down box shows the visualization\\noptions that are available for your network topology, as shown\\nbelow. There will be options to display the activation (the outputs)\\nof any 2D layer of neurons 3x3 or larger, and convolution kernels of\\nsize 3x3 or larger. Visualization images appear at the bottom of the\\nGUI. You can mouse-over the images to zoom in.\\n\\nHow to use your own data\\nInput data to the neural net can be specified in text or binary format. If\\nyou want to specify input values in text format, just list the input\\nvalues in the inputData.txt file. If you\\'re inputting from .bmp or .dat\\nbinary files, then list those filenames in inputData.txt. Details are\\nexplained below.\\nText input format\\nTo specify input data as text, prepare a text file, typically called\\ninputData.txt, with one input sample per line. The input values go inside\\ncurly braces. If the data is for training, the target output values must\\nbe specified on the same line after the input values. In each sample, the\\ninput values are given in a linear list, regardless whether the neural\\nnet input layer is one-dimensional or two-dimensional. For example, if\\nyour neural net has 9 input neurons of any arrangement and two output\\nneurons, then the inputData.txt file for training will look like this\\nformat:\\n{ 0.32 0.98 0.12 0.44 0.98 0.22 0.34 0.72 0.84 } -1  1   \\n{ 1.00 0.43 0.19 0.83 0.97 0.87 0.75 0.47 0.92 }  1  1   \\n{ 0.87 0.75 0.47 0.92 1.00 0.43 0.19 0.83 0.97 } -1 -1   \\n{ 0.34 0.83 0.97 0.87 0.75 0.43 0.19 0.47 0.92 } -1  1   \\netc.. . .  \\n\\nBinary input formats\\nThere are two binary options -- .bmp image files, and .dat data\\nfiles. First, prepare your .bmp or .dat files, one sample per file. If\\nusing .bmp files, the number of pixels in each image should equal the\\nnumber of input neurons in your neural net. If using .dat files, each\\nfile must contain a linear list of input values, with the same number\\nof values as the number of input neurons in your neural net.\\nNext, prepare an input data configuration file, called inputData.txt\\ncontaining a list of the .bmp or .dat filenames, one per line. If the\\ndata is for training, then also list the target output values for each\\ninput sample after the filename, like this:\\nimages/thumbnails/test-918.bmp -1 1 -1 -1 -1 -1 -1 -1 -1 -1\\nimages/thumbnails/test-919.bmp -1 -1 -1 -1 -1 -1 -1 -1 1 -1\\nimages/thumbnails/test-920.bmp -1 -1 -1 -1 -1 -1 1 -1 -1 -1\\nimages/thumbnails/test-921.bmp -1 -1 -1 -1 -1 1 -1 -1 -1 -1\\netc. . .\\n\\nThe path and filename cannot contain any spaces.\\nThe path_prefix directive can be used to specify a string to be attached\\nto the front of all subsequent filenames, or until the next path_prefix\\ndirective. For example, the previous example could be written:\\n path_prefix = images/thumbnails/\\n test-918.bmp\\n test-919.bmp\\n test-920.bmp\\n test-921.bmp\\n etc. . .\\n\\nFor more information on the .bmp file format, see this Wikipedia\\narticle..\\nFor more information on the neural2d .dat binary format, see this wiki\\npage.\\nand the design\\nnotes.\\nTopology file\\nIn addition to the input data config file (inputData.txt), you\\'ll also\\nneed a topology config file (typically named topology.txt by default)\\nto define your neural net topology (the number and arrangement\\nof neurons and connections). Its format is described in a later\\nsection.  A typical one looks like this example:\\ninput size 32x32  \\nlayer1 size 32x32 from input radius 8x8  \\nlayer2 size 16x16 from layer1  \\noutput size 1x10 from layer2  \\n\\nThen run neuron2d (optionally with the web browser interface) and\\nexperiment with the parameters until the net is adequately trained,\\nthen save the weights in a file for later use.\\nIf you run the web interface, you can change the global parameters\\nfrom the GUI while the neural2d program is running. If you run the\\nneural2d console program without the GUI interface, there is no way\\nto interact with it while running. Instead, you\\'ll need to examine and\\nmodify the parameters in the code at the top of the files neural2d.cpp\\nand neural2d-core.cpp.\\nThe 2D in neural2d\\nIn a simple traditional neural net model, the neurons are arranged in\\na column in each layer:\\n\\nIn neural2d, you can specify a rectangular arrangement of neurons in\\neach layer, such as:\\n\\nThe neurons can be sparsely connected to mimic how retinal neurons\\nare connected in biological brains. For example, if a radius of \"1x1\"\\nis specified in the topology config file, each neuron on the right\\n(destination) layer will connect to a circular patch of neurons in the\\nleft (source) layer as shown here (only a single neuron on the right\\nside is shown connected in this picture so you can see what\\'s going on,\\nbut imagine all of them connected in the same pattern):\\n\\nThe pattern that is projected onto the source layer is elliptical. (Layers\\nconfigured as convolution filters work slightly differently; see the\\nlater section about convolution filtering.)\\nHere are some projected connection patterns for various radii:\\nradius 0x0\\n\\nradius 1x1\\n\\nradius 2x2\\n\\nradius 3x1\\n\\nConvolution filtering\\nAny layer other than the input layer can be configured as a convolution\\nfilter layer by specifying a convolve-matrix specification for the\\nlayer in the topology config file.  The neurons are still called neurons,\\nbut their operation differs in the following ways:\\n\\n\\nThe connection pattern to the source layer is defined by the convolution\\nmatrix (kernel) dimensions (not by a radius parameter)\\n\\n\\nThe connection weights are initialized from the convolution matrix,\\nand are constant throughout the life of the net.\\n\\n\\nThe transfer function is automatically set to the identity function\\n(not by a tf parameter).\\n\\n\\nFor example, the following line in the topology config file defines a\\n3x3 convolution matrix for shrarpening the source layer:\\n layerConv1 size 64x64 from input convolve {{0,-1,0},{-1,5,-1},{0,-1,0}}\\n\\nWhen a convolution matrix is specified for a layer, you cannot also\\nspecify a radius parameter for that layer, as the convolution matrix\\nsize determines the size and shape of the rectangle of neurons in the\\nsource layer. You also cannot also specify a tf parameter, because the\\ntransfer function on a convolution layer is automatically set to be the\\nidentity function.\\nThe elements of the convolution matrix are stored as connection weights\\nto the source neurons. Connection weights on convolution layers are not\\nupdated by the back propagation algorithm, so they remain constant for\\nthe life of the net.\\nFor illustrations of various convolution kernels, see\\nthis Wikipedia article\\nIn the following illustration, the topology config file defines a\\nconvolution filter with a 2x2 kernel that is applied to the input layer,\\nthen the results are combined with a reduced-resolution fully-connected\\npathway. The blue connections in the picture are the convolution\\nconnections; the green connections are regular neural connections:\\ninput size 8x8\\nlayerConvolve size 8x8 from input convolve {{-1,2},{-1,2}}\\nlayerReducedRes size 4x4 from input\\noutput size 2 from layerConvolve\\noutput size 2 from layerReducedRes\\n\\n\\nConvolution networking and pooling\\nA convolution network\\nlayer is\\nlike a convolution filter layer, except that the kernel participates in\\nbackprop training, and everything inside the layer is replicated N times\\nto train N separate kernels. A convolution network layer is said to have\\ndepth N. A convolution network layer has depth * X * Y neurons.\\nAny layer other than the input or output layer can be configured as\\na convolution networking layer by specifying a layer depth > 1, and\\nspecifying the kernel size with a convolve parameter. For example,\\nto train 40 kernels of size 7x7 on an input image of 64x64 pixels:\\n  input size 64x64\\n  layerConv size 40*64x64 from input convolve 7x7\\n  . . .\\n\\nA pooling layer\\ndown-samples the previous layer by finding the average or maximum in\\npatches of source neurons.  A pooling layer is defined in the topology\\nconfig file by specifying a pool parameter on a layer.\\nIn the topology config syntax, the pool parameter requires the argument\\n\"avg\" or \"max\" followed by the operator size, For example, in a\\nconvolution network pipeline of depth 20, you might have these layers:\\n  input size 64x64\\n  layerConv size 20*64x64 from input convolve 5x5\\n  layerPool size 20*16x16 from layerConv pool max 4x4\\n  . . .\\n\\nLayer depth\\nAll layers have a depth, whether explicit or implicit. Layer depth is\\nspecified in the topology config file in the layer size parameter by an\\ninteger and asterisk before the layer size. If the depth is not specified,\\nit defaults to one. For example:\\n\\n\\nsize 10*64x64 means 64x64 neurons, depth 10\\n\\n\\nsize 64x64 means 64x64 neurons, depth 1\\n\\n\\nsize 1*64x64 also means 64x64 neurons, depth 1\\n\\n\\nsize 10*64 means 64x1 neurons, depth 10 (the Y dimension defaults to 1)\\n\\n\\nThe primary purpose of layer depth is to allow convolution network\\nlayers to train multiple kernels.  However, the concept of layer depth\\nis generalized in neural2d, allowing any layer to have any depth and\\nconnect to any other layer of any kind with any depth.\\nThe way two layers are connected depends on the relationship of the\\nsource and destination layer depths as shown below:\\n\\n\\n\\nRelationship\\nHow connected\\n\\n\\n\\n\\nsrc depth == dst depth\\nconnect only to the corresponding depth in source\\n\\n\\nsrc depth != dst depth\\nfully connect across all depths\\n\\n\\n\\nTopology config file format\\nHere is the grammar of the topology config file:\\n\\nlayer-name parameters := parameter [ parameters ]\\n\\n\\nparameters := parameter [ parameters ]\\n\\n\\nparameter :=\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0input | output | layername\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0size dxy-spec\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0from layer-name\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0channel channel-spec\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0radius xy-spec\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tf transfer-function-spec\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0convolve filter-spec\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0convolve xy-spec\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pool { max | avg } xy-spec\\n\\n\\ndxy-spec := [ integer * ] integer [ x integer ]\\n\\n\\nxy-spec := integer [ x integer ]\\n\\n\\nchannel-spec := R | G | B | BW\\n\\n\\ntransfer-function-spec := tanh | logistic | linear | ramp | gaussian | relu\\n\\n\\nfilter-spec := same {{,},{,}} syntax used for array initialization in C, C#, VB, Java, etc.\\n\\nRules:\\n\\n\\nComment lines that begin with \"#\" and blank lines are ignored.\\n\\n\\nThe first layer defined must be named \"input\".\\n\\n\\nThe last layer defined must be named \"output\".\\n\\n\\nThe hidden layers can be named anything beginning with \"layer\".\\n\\n\\nThe argument for \"from\" must be a layer already defined.\\n\\n\\nThe color channel parameter can be specified only on the input layer.\\n\\n\\nIf a size parameter is omitted, the size is copied from the layer\\nspecified in the from parameter.\\n\\n\\nA radius parameter cannot be used on the same line with a convolve\\nor pool parameter.\\n\\n\\nThe same layer name can be defined multiple times with different\\n\"from\" parameters.  This allows source neurons from more than one layer\\nto be combined in one destination layer. The source layers can be any\\nsize, but the repeated (the destination) layer must have the same size\\nin each specification. For example, in the following, layerCombined is\\nsize 16x16 and takes inputs from two source layers of different sizes:\\n\\n\\n     input size 128x128  \\n     layerVertical size 32x32 from input radius 1x8  \\n     layerHorizontal size 16x16 from input radius 8x1  \\n     layerCombined from layerHorizontal          <= assumes size 16x16 from the source layer  \\n     layerCombined size 16x16 from layerVertical <= repeated destination, must match 16x16  \\n     output size 1 from layerCombined  \\n\\n\\nIn the xy-spec  and in the X,Y part of the dxy-spec, you may\\nspecify one or two dimensions.  Spaces are not allowed in the size\\nspec. If only one dimension is given, the other is assumed to be 1.\\nFor example:\\n\\n\\n\"8x8\" means 64 neurons in an 8 x 8 arrangement.\\n\"8x1\" means a row of 8 neurons\\n\"1x8\" means a column of 8 neurons.\\n\"8\" means the same as \"8x1\"\\n\\nTopology config file examples\\nHere are a few complete topology config files and the nets they specify.\\ninput size 4x4\\nlayer1 size 3x3 from input\\nlayer2 size 2x2 from layer1\\noutput size 1 from layer2\\n\\n\\ninput size 4x4\\nlayer1 size 1x4 from input\\nlayer2 size 3x1 from layer1\\noutput size 1 from layer2\\n\\n\\ninput size 4x4\\noutput size 4x1 from input radius 0x2\\n\\n\\ninput size 16x16\\nlayer1 size 4x4 from input radius 1x1\\noutput size 7x1 from layer1\\n\\n\\n# In the picture that follows, layerVertical is the set of 4 neurons\\n# in the upper part of the picture, and layerHorizontal is the lower\\n# set of 4 neurons.\\n\\ninput size 6x6\\nlayerHorizontal size 2x2 from input radius 2x0\\nlayerVertical size 2x2 from input radius 0x2\\noutput size 1 from layerHorizontal\\noutput size 1 from layerVertical\\n\\n\\n# This example shows how vertical and horizontal image features can be\\n# extracted through separate paths and combined in a subsequent layer.\\n\\ninput size 4x4\\n\\nlayerH1 size 1x4 from input radius 4x0\\nlayerH2 size 1x4 from layerH1\\nlayerH3 size 1x4 from layerH2\\n\\nlayerV1 size 4x1 from input radius 0x4\\nlayerV2 size 4x1 from layerV1\\nlayerV3 size 4x1 from layerV2\\n\\noutput size 2 from layerV3\\noutput size 2 from layerH3\\n\\n\\nHow-do-I X?\\nHow do I run the command-line program?\\nRun neural2d with three arguments specifying the topology configuration,\\ninput data configuration, and where to store the weights if training\\nsucceeds:\\n ./neural2d topology.txt inputData.txt weights.txt\\n\\nHow do I run the GUI interface?\\nFirst launch the neural2d program with the -p option:\\n ./neural2d topology.txt inputData.txt weights.txt -p\\n\\nThen open a web browser and point it at\\nhttp://localhost:24080 .\\nIf your firewall complains, you may need to allow access to TCP port\\n24080.\\nHow do I disable the GUI interface?\\nRun CMake with the -DWEBSERVER=OFF option. Or if you are using\\nyour own home-grown Makefiles, you can define the preprocessor\\nmacro DISABLE_WEBSERVER. For example, with gnu compilers, add\\n-DDISABLE_WEBSERVER to the g++ command line. Alternatively, you can\\nundefine the macro ENABLE_WEBSERVER in neural2d.h.\\nWhen the web server is disabled, there is no remaining dependency on\\nPOSIX sockets.\\nAlso see the illustrations above.\\nHow do I use my own data instead of the digits images?\\nFor binary input data, create your own directory of BMP image\\nfiles or .dat binary files, and an input data config file\\n(inputData.txt) that follows the same format as shown in the\\nexamples elsewhere. Then define a topology config\\nfile with the appropriate number of network inputs and\\noutputs, then run the neural2d program.\\nOr if you don\\'t want to use .bmp image files or .dat binary files for\\ninput, make an input config file containing all the literal input values\\nand the target output values.\\nSee above for more information on the input formats.\\nHow do I use a trained net on new data?\\nIt\\'s all about the weights file. After the net has been successfully\\ntrained, save the internal connection weights in a weights file. That\\'s\\ntypically done in neural2d.cpp by calling the member function\\nsaveWeights(filename).\\nThe weights you saved can be loaded back into a neural net of the same\\ntopology using the member function loadWeights(filename). Once the net\\nhas been loaded with weights, it can be used applied to new data by\\ncalling feedForward(). Prior to calling feedForward(), you\\'ll want to\\nset a couple of parameters:\\n myNet.repeatInputSamples = false;\\n myNet.reportEveryNth = 1;\\n\\nThis is normally done in neural2d.cpp.\\nYou\\'ll need to prepare a new input data config file (default name\\ninputData.txt) that contains a list of only those new input data images\\nthat you want the net to process.\\nHow do I train on the MNIST handwritten digits data set?\\nSee the instructions in the wiki.\\nHow do I change the learning rate parameter?\\nIn the command-line program, you can set the eta parameter or change it\\nby directly setting the eta member of the Net object, like this:\\n myNet.eta = 0.1;\\n\\nWhen using the web interface, you can change the eta parameter (and\\nother parameters) in the GUI at any time, even while the network is busy\\nprocessing input data.\\nAlso see the Parameter\\nList\\nin the wiki.\\nAre the output neurons binary or floating point?\\nThey are interpreted in whatever manner you train them to be, but you\\ncan only train the outputs to take values in the range that the transfer\\nfunction is capable of producing.\\nIf you\\'re training a net to output binary values, it\\'s best if you use\\nthe maxima of the transfer function to represent the two binary values.\\nFor example, when using the default tanh() transfer function, train\\nthe outputs to be -1 and +1 for false and true. When using the logistic\\ntransfer function, train the outputs to be 0 and 1.\\nHow do I use a different transfer function?\\nYou can add a \"tf\" parameter to any layer definition line in\\nthe topology config file.  The argument to tf can be \"tanh\",\\n\"logistic\", \"linear\", \"ramp\", \"gaussian\", or \"relu\".  The\\ntransfer function you specify will be used by all the neurons\\nin that layer.  Here are the graphs of the built-in transfer\\nfunctions.\\nIn the topology config file, the tf parameter is specified as in this\\nexample:\\n layerHidden1 size 64x64 from input radius 3x3 tf linear\\n\\nYou can add new transfer functions by following the examples in\\nneural2d-core.cpp.  There are two places to change: first find where\\ntransferFunctionTanh() is defined and add your new transfer function and\\nits derivative there. Next, locate the constructor for class Neuron and\\nadd a new else-if clause there, following the examples.\\nHow do I define a convolution filter?\\nIn the topology config file, any layer defined with a convolve\\nparameter and a list of constant weights will operate as a convolution\\nfilter applied to the source layer.  The syntax is of the form:\\n layer2 size 64x64 from input convolve {{1,0,-1},{0,0,0},{-1,0,1}}\\n\\nAlso see above for more information.\\nSee this article for the difference between\\nconvolution filter and convolution networking.\\nHow do I define convolution networking and pooling?\\nIn the topology config file, define a layer with an X,Y size and a depth\\n(number of kernels to train), and add a convolve parameter to specify\\nthe kernel size. For example, to train 40 kernels of size 7x7 on an\\ninput image of 64x64 pixels:\\n input size 64x64  \\n layerConv size 40*64x64 from input convolve 7x7\\n . . .\\n\\nTo define a pooling layer, add a pool parameter, followed by the argument\\n\"avg\" or \"max,\" followed by the operator size, e.g.:\\n layerConv size 10*32x32 ...\\n layerPool size 10*8x8 from layerConv pool max 4x4\\n . . .\\n\\nAlso see above for more information.\\nHow do the color image pixels get converted to floating point for the input layer?\\nThat\\'s in the ImageReaderBMP class in neural2d-core.cpp. The default\\nversion provided converts each RGB pixel to a single floating point\\nvalue in the range -1.0 to 1.0.\\nBy default, the three color channels are converted to monochrome and\\nnormalized to the range -1.0 to 1.0. That can be changed at runtime by\\nsetting the colorChannel member of the Net object to R, G, B, or BW prior\\nto calling feedForward(). E.g., to use only the green color channel of\\nthe images, use:\\nmyNet.colorChannel = NNet::G;\\n\\nThe color conversion can also be specified in the topology config file on\\nthe line that defines the input layer by setting the \"channel\" parameter\\nto R, G, B, or BW, e.g.:\\ninput size 64x64 channel G\\n\\nThere is no conversion when inputting floating point data directly from a\\n.dat file, or from literal values embedded in the input data config file.\\nHow can I use .jpg and .png images as inputs to the net?\\nCurrently only .bmp images files are supported. This is because the\\nuncompressed BMP format is so simple that we can use simple, standard\\nC/C++ to read the image data without any dependencies on third-party\\nimage libraries.\\nTo support a new input file format, derive a new subclass\\nfrom class ImageReader and implement its getData() member\\nfollowing the examples of the existing image readers. For\\nmore information on the input data readers, see the design\\nnotes.\\nHow can I find my way through the source code?\\nHere is a little map of the important files:\\n\\nAlso see the class relationship diagram in the project root directory.\\nWhy does the net error rate stay high? Why doesn\\'t my net learn?\\nNeural nets are finicky. Try different network topologies. Try starting\\nwith a larger eta values and reduce it incrementally. It could also\\nbe due to redundancy in the input data, or mislabeled target output\\nvalues. Or you may need more training samples.\\nWhat other parameters do I need to know about?\\nCheck out the list of parameters in the\\nwiki.\\nLicenses\\nThe neural2d program and its documentation are\\ncopyrighted and licensed under the terms of the MIT\\nlicense. See the LICENSE file for\\nmore information.\\nThe set of digits images in the images/digits/ subdirectory is released\\nto the public domain.\\n',\n",
       "  'watchers': '26',\n",
       "  'stars': '116',\n",
       "  'forks': '54',\n",
       "  'commits': '158'},\n",
       " {'language': 'C++ 88.1',\n",
       "  'readme': 'HttpClient for Spark Core | Arduino\\nThis is a work in progress Http Client Library for the Spark Core. It is not ready for use other than for people who have very basic needs, or are willing to help with the development. Because of this, it is currently very verbose and makes heavy use of the serial connection so you can see what\\'s going on. That said, if you are reasonably familiar with Arduino or embedded development you might find it useful. I am publishing it in this early stage mostly because I am myself just starting out with C++ and could use all the help I can get. If you find errors or bad code just let me know and I\\'ll work on fixing it!\\nThere are a couple other options that are probably better suited if you are using a vanilla Arduino and not a Spark Core. First is Arduino HTTP library from Adrian McEwen. It depends on the Arduino Ethernet API Library though, which may or may not make sense in your implementation. Second there is HTTPClient from Interactive Matter but it also depends on the same Arduino Ethernet Library. Both of these libraries are orders of magnitude more mature than this one. In the future, it might very well make more sense to reuse a lot of code from these other libraries but to get rid of the dependencies rather than reimplementing things again.\\nExample usage\\n#include \"application.h\"\\n\\n/**\\n* Declaring the variables.\\n*/\\nunsigned int nextTime = 0;    // Next time to contact the server\\nHttpClient http;\\n\\n// Headers currently need to be set at init, useful for API keys etc.\\nhttp_header_t headers[] = {\\n    //  { \"Content-Type\", \"application/json\" },\\n    //  { \"Accept\" , \"application/json\" },\\n    { \"Accept\" , \"*/*\"},\\n    { NULL, NULL } // NOTE: Always terminate headers will NULL\\n};\\n\\nhttp_request_t request;\\nhttp_response_t response;\\n\\nvoid setup() {\\n    Serial.begin(9600);\\n}\\n\\nvoid loop() {\\n    if (nextTime > millis()) {\\n        return;\\n    }\\n\\n    Serial.println();\\n    Serial.println(\"Application>\\\\tStart of Loop.\");\\n    // Request path and body can be set at runtime or at setup.\\n    request.hostname = \"www.timeapi.org\";\\n    request.port = 80;\\n    request.path = \"/utc/now\";\\n\\n    // The library also supports sending a body with your request:\\n    //request.body = \"{\\\\\"key\\\\\":\\\\\"value\\\\\"}\";\\n\\n    // Get request\\n    http.get(request, response, headers);\\n    Serial.print(\"Application>\\\\tResponse status: \");\\n    Serial.println(response.status);\\n\\n    Serial.print(\"Application>\\\\tHTTP Response Body: \");\\n    Serial.println(response.body);\\n\\n    nextTime = millis() + 10000;\\n}\\n\\n\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '114',\n",
       "  'forks': '154',\n",
       "  'commits': '61'},\n",
       " {'language': 'C++ 96.4',\n",
       "  'readme': 'Vinduino\\nThe Vinduino project started from the necessity to manage irrigation in my small Southern California vineyard, by monitoring soil moisture at different depths and at several vineyard locations.\\nSoil moisture monitoring systems have been around for decades, but they cost hundreds to thousands of dollars and -because these systems are proprietary- there will be ongoing cost to customize and maintain these systems. As a small vineyard owner, I needed something low cost and flexible.\\nThe open source Arduino platform, together with low cost gypsum soil moisture and salinity sensors, provides all that. While I first envisioned the Vinduino project (Vineyard + Arduino) for my personal interest and needs, but now the scope has broadened to providing easy to use, open source, low cost solutions for agricultural irrigation management.\\nSaving water is more important now than ever. The four year drought in California made everybody realize the importance of reducing water use. But it is not just California that is plagued by prolonged drought periods. Large agricultural areas in South America, India, China, and Africa suffer from continuing water shortage as well.\\nThe Vinduino project provided the following results, published on the Vinduino blog, Hackaday, and Github:\\n\\nDIY calibrated gypsum soil moisture sensors (Watermark SS200 is also supported)\\nHand held sensor reader (soil moisture, soil/water salinity, water pressure)\\nSolar powered remote sensor platform (Vinduino R3), available on Tindie.com\\nOptions include:\\n4 electrically separated inputs for soil moisture sensors\\nWifi (ESP8266), Globalsat LM-210 private LoRa network module, Globalsat LM-513 module for LoRaWAN (The Things Network)\\nIrrigation valve control, optional pressure sensor for valve operation feedback\\nseveral options for temperature/humidity sensors\\nBuilt in solar battery charger\\nBuilt in real time clock for precise irrigation timing\\nGateway to connect multiple LoRa end nodes to the Internet via Wifi (Vinduino Gateway)\\n\\nAll publicly released programming and documentation is available for free under the GNU General Public License 3.0.\\nFor non-DIY we offer commercial products at www.vinduino.com and https://www.amazon.com/Vinduino-Agricultural-Irrigation-Monitoring-Starter/dp/B0828JCK49\\nSupport web site: https://vinduino.freshdesk.com/support/home\\n',\n",
       "  'watchers': '40',\n",
       "  'stars': '115',\n",
       "  'forks': '45',\n",
       "  'commits': '47'},\n",
       " {'language': 'C++ 71.8',\n",
       "  'readme': 'Vinduino\\nThe Vinduino project started from the necessity to manage irrigation in my small Southern California vineyard, by monitoring soil moisture at different depths and at several vineyard locations.\\nSoil moisture monitoring systems have been around for decades, but they cost hundreds to thousands of dollars and -because these systems are proprietary- there will be ongoing cost to customize and maintain these systems. As a small vineyard owner, I needed something low cost and flexible.\\nThe open source Arduino platform, together with low cost gypsum soil moisture and salinity sensors, provides all that. While I first envisioned the Vinduino project (Vineyard + Arduino) for my personal interest and needs, but now the scope has broadened to providing easy to use, open source, low cost solutions for agricultural irrigation management.\\nSaving water is more important now than ever. The four year drought in California made everybody realize the importance of reducing water use. But it is not just California that is plagued by prolonged drought periods. Large agricultural areas in South America, India, China, and Africa suffer from continuing water shortage as well.\\nThe Vinduino project provided the following results, published on the Vinduino blog, Hackaday, and Github:\\n\\nDIY calibrated gypsum soil moisture sensors (Watermark SS200 is also supported)\\nHand held sensor reader (soil moisture, soil/water salinity, water pressure)\\nSolar powered remote sensor platform (Vinduino R3), available on Tindie.com\\nOptions include:\\n4 electrically separated inputs for soil moisture sensors\\nWifi (ESP8266), Globalsat LM-210 private LoRa network module, Globalsat LM-513 module for LoRaWAN (The Things Network)\\nIrrigation valve control, optional pressure sensor for valve operation feedback\\nseveral options for temperature/humidity sensors\\nBuilt in solar battery charger\\nBuilt in real time clock for precise irrigation timing\\nGateway to connect multiple LoRa end nodes to the Internet via Wifi (Vinduino Gateway)\\n\\nAll publicly released programming and documentation is available for free under the GNU General Public License 3.0.\\nFor non-DIY we offer commercial products at www.vinduino.com and https://www.amazon.com/Vinduino-Agricultural-Irrigation-Monitoring-Starter/dp/B0828JCK49\\nSupport web site: https://vinduino.freshdesk.com/support/home\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '115',\n",
       "  'forks': '64',\n",
       "  'commits': '3'},\n",
       " {'language': 'C++ 95.3',\n",
       "  'readme': \"HMIYC\\n\\n\\n\\nHunt Me If You Can is an UnrealEngine4 Battle Lan Game\\nIt's a total 3d engraved version of AssassinWar\\n\\n\\n\\nThank you UE4, I made it!\\nTo begin\\nDownload ue4.10.4 and Visual Studio 2015\\nOpen HuntMeIfYouCan.uproject and refresh Visual Studio Project\\nHow To Play\\nWASD walk\\nleft mouse use skill\\n1 or 2 switch skill\\nget 5 kills you win\\n\\nDownload Windows-32bit\\nFor some reasons.Content of character is only packaged in release version.\\nHere is a new game engine I'm currectly working on Qin.js\\n\",\n",
       "  'watchers': '8',\n",
       "  'stars': '112',\n",
       "  'forks': '22',\n",
       "  'commits': '148'},\n",
       " {'language': 'C++ 100.0',\n",
       "  'readme': 'cocos2d-x-extensions\\nHere is my repository for cocos2d-x extensions.\\nPlease check Wiki for more information and documentation.\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '112',\n",
       "  'forks': '60',\n",
       "  'commits': '14'},\n",
       " {'language': 'C++ 52.4',\n",
       "  'readme': '\\nlibsvm-ruby-swig¶ ↑\\n\\nRuby interface to LIBSVM (using SWIG)\\n\\nwww.tomzconsulting.com\\n\\ntweetsentiments.com\\n\\nDESCRIPTION:¶ ↑\\nThis is the Ruby port of the LIBSVM Python SWIG (Simplified Wrapper and  Interface Generator) interface.\\nA slightly modified version of LIBSVM 2.9 is included, it allows turrning on/off the debug log. You don\\'t need your own copy of SWIG to use this library - all  needed files are generated using SWIG already.\\nLook for the README file in the ruby subdirectory for instructions. The binaries included were built under Ubuntu Linux 2.6.28-18-generic x86_64, you should run make under the libsvm-2.9 and libsvm-2.9/ruby  directories to regenerate the executables for your environment.\\nLIBSVM is in use at tweetsentiments.com - A Twitter / Tweet sentiment analysis application\\nINSTALL:¶ ↑\\nCurrently the gem is available on linux only(tested on Ubuntu 8-9 and Fedora 9-12, and on OS X by danielsdeleo), and you will need g++ installed to compile the  native code. \\nsudo gem sources -a http://gems.github.com   (you only have to do this once)\\nsudo gem install tomz-libsvm-ruby-swig\\nSYNOPSIS:¶ ↑\\nQuick Interactive Tutorial using irb (adopted from the python code from Toby Segaran\\'s “Programming Collective Intelligence” book):\\nirb(main):001:0> require \\'svm\\'\\n=> true\\nirb(main):002:0> prob = Problem.new([1,-1],[[1,0,1],[-1,0,-1]])\\nirb(main):003:0> param = Parameter.new(:kernel_type => LINEAR, :C => 10)\\nirb(main):004:0> m = Model.new(prob,param)\\nirb(main):005:0> m.predict([1,1,1])\\n=> 1.0\\nirb(main):006:0> m.predict([0,0,1])\\n=> 1.0\\nirb(main):007:0> m.predict([0,0,-1])\\n=> -1.0\\nirb(main):008:0> m.save(\"test.model\")\\nirb(main):009:0> m2 = Model.new(\"test.model\")\\nirb(main):010:0> m2.predict([0,0,-1])\\n=> -1.0\\nAUTHOR:¶ ↑\\nTom Zeng\\n\\ntwitter.com/tomzeng\\n\\nwww.tomzconsulting.com\\n\\nwww.linkedin.com/in/tomzeng\\n\\ntom.z.zeng at gmail dot com\\n\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '111',\n",
       "  'forks': '25',\n",
       "  'commits': '67'},\n",
       " {'language': 'C++ 47.7',\n",
       "  'readme': '\\nlibsvm-ruby-swig¶ ↑\\n\\nRuby interface to LIBSVM (using SWIG)\\n\\nwww.tomzconsulting.com\\n\\ntweetsentiments.com\\n\\nDESCRIPTION:¶ ↑\\nThis is the Ruby port of the LIBSVM Python SWIG (Simplified Wrapper and  Interface Generator) interface.\\nA slightly modified version of LIBSVM 2.9 is included, it allows turrning on/off the debug log. You don\\'t need your own copy of SWIG to use this library - all  needed files are generated using SWIG already.\\nLook for the README file in the ruby subdirectory for instructions. The binaries included were built under Ubuntu Linux 2.6.28-18-generic x86_64, you should run make under the libsvm-2.9 and libsvm-2.9/ruby  directories to regenerate the executables for your environment.\\nLIBSVM is in use at tweetsentiments.com - A Twitter / Tweet sentiment analysis application\\nINSTALL:¶ ↑\\nCurrently the gem is available on linux only(tested on Ubuntu 8-9 and Fedora 9-12, and on OS X by danielsdeleo), and you will need g++ installed to compile the  native code. \\nsudo gem sources -a http://gems.github.com   (you only have to do this once)\\nsudo gem install tomz-libsvm-ruby-swig\\nSYNOPSIS:¶ ↑\\nQuick Interactive Tutorial using irb (adopted from the python code from Toby Segaran\\'s “Programming Collective Intelligence” book):\\nirb(main):001:0> require \\'svm\\'\\n=> true\\nirb(main):002:0> prob = Problem.new([1,-1],[[1,0,1],[-1,0,-1]])\\nirb(main):003:0> param = Parameter.new(:kernel_type => LINEAR, :C => 10)\\nirb(main):004:0> m = Model.new(prob,param)\\nirb(main):005:0> m.predict([1,1,1])\\n=> 1.0\\nirb(main):006:0> m.predict([0,0,1])\\n=> 1.0\\nirb(main):007:0> m.predict([0,0,-1])\\n=> -1.0\\nirb(main):008:0> m.save(\"test.model\")\\nirb(main):009:0> m2 = Model.new(\"test.model\")\\nirb(main):010:0> m2.predict([0,0,-1])\\n=> -1.0\\nAUTHOR:¶ ↑\\nTom Zeng\\n\\ntwitter.com/tomzeng\\n\\nwww.tomzconsulting.com\\n\\nwww.linkedin.com/in/tomzeng\\n\\ntom.z.zeng at gmail dot com\\n\\n',\n",
       "  'watchers': '47',\n",
       "  'stars': '109',\n",
       "  'forks': '38',\n",
       "  'commits': '96'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"Note: This is not actively maintained, please make an issue if you are interested in helping maintain this project.\\ngrunt-nodemon\\n\\nRun nodemon as a grunt task for easy configuration and integration with the rest of your workflow\\n\\n  \\nGetting Started\\nIf you haven't used grunt before, be sure to check out the Getting Started guide, as it explains how to create a gruntfile as well as install and use grunt plugins. Once you're familiar with that process, install this plugin with this command:\\nnpm install grunt-nodemon --save-dev\\nThen add this line to your project's Gruntfile.js gruntfile:\\ngrunt.loadNpmTasks('grunt-nodemon');\\nDocumentation\\nMinimal Usage\\nThe minimal usage of grunt-nodemon runs with a script specified:\\nnodemon: {\\n  dev: {\\n    script: 'index.js'\\n  }\\n}\\nUsage with all available options set\\nnodemon: {\\n  dev: {\\n    script: 'index.js',\\n    options: {\\n      args: ['dev'],\\n      nodeArgs: ['--debug'],\\n      callback: function (nodemon) {\\n        nodemon.on('log', function (event) {\\n          console.log(event.colour);\\n        });\\n      },\\n      env: {\\n        PORT: '8181'\\n      },\\n      cwd: __dirname,\\n      ignore: ['node_modules/**'],\\n      ext: 'js,coffee',\\n      watch: ['server'],\\n      delay: 1000,\\n      legacyWatch: true\\n    }\\n  },\\n  exec: {\\n    options: {\\n      exec: 'less'\\n    }\\n  }\\n}\\nAdvanced Usage\\nA common use case is to run nodemon with other tasks concurrently. It is also common to open a browser tab when starting a server, and reload that tab when the server code changes. These workflows can be achieved with the following config, which uses a custom options.callback function, and grunt-concurrent to run nodemon, node-inspector, and watch in a single terminal tab:\\nconcurrent: {\\n  dev: {\\n    tasks: ['nodemon', 'node-inspector', 'watch'],\\n    options: {\\n      logConcurrentOutput: true\\n    }\\n  }\\n},\\nnodemon: {\\n  dev: {\\n    script: 'index.js',\\n    options: {\\n      nodeArgs: ['--debug'],\\n      env: {\\n        PORT: '5455'\\n      },\\n      // omit this property if you aren't serving HTML files and \\n      // don't want to open a browser tab on start\\n      callback: function (nodemon) {\\n        nodemon.on('log', function (event) {\\n          console.log(event.colour);\\n        });\\n        \\n        // opens browser on initial server start\\n        nodemon.on('config:update', function () {\\n          // Delay before server listens on port\\n          setTimeout(function() {\\n            require('open')('http://localhost:5455');\\n          }, 1000);\\n        });\\n\\n        // refreshes browser when server reboots\\n        nodemon.on('restart', function () {\\n          // Delay before server listens on port\\n          setTimeout(function() {\\n            require('fs').writeFileSync('.rebooted', 'rebooted');\\n          }, 1000);\\n        });\\n      }\\n    }\\n  }\\n},\\nwatch: {\\n  server: {\\n    files: ['.rebooted'],\\n    options: {\\n      livereload: true\\n    }\\n  } \\n}\\nNote that using the callback config above assumes you have open installed and are injecting a LiveReload script into your HTML file(s). You can use grunt-inject to inject the LiveReload script.\\nRequired property\\nscript\\nType: String\\nScript that nodemon runs and restarts when changes are detected.\\nOptions\\nargs\\nType: Array of Strings\\nList of arguments to be passed to your script.\\nnodeArgs\\nType: Array of Strings\\nList of arguments to be passed to node. The most common argument is --debug or --debug-brk to start a debugging server.\\ncallback\\nType:  Function\\nDefault:\\nfunction(nodemon) {\\n  // By default the nodemon output is logged\\n  nodemon.on('log', function(event) {\\n    console.log(event.colour);\\n  });\\n};\\nCallback which receives the nodemon object. This can be used to respond to changes in a running app, and then do cool things like LiveReload a web browser when the app restarts. See the nodemon docs for the full list of events you can tap into.\\nignore\\nType: Array of String globs Default: ['node_modules/**']\\nList of ignored files specified by a glob pattern relative to the watched folder. Here is an explanation of how to use the patterns to ignore files.\\next\\nType: String Default: 'js'\\nString with comma separated file extensions to watch. By default, nodemon watches .js files.\\nwatch\\nType: Array of Strings Default: ['.']\\nList of folders to watch for changes. By default nodemon will traverse sub-directories, so there's no need in explicitly including sub-directories.\\ndelay\\nType: Number Default: 1000\\nDelay the restart of nodemon by a number of milliseconds when compiling a large amount of files so that the app doesn't needlessly restart after each file is changed.\\nlegacyWatch\\nType: Boolean Default: false\\nIf you wish to force nodemon to start with the legacy watch method. See https://github.com/remy/nodemon/blob/master/faq.md#help-my-changes-arent-being-detected for more details.\\ncwd\\nType: String\\nThe current working directory to run your script from.\\nenv\\nType: Object\\nHash of environment variables to pass to your script.\\nexec\\nType: String\\nYou can use nodemon to execute a command outside of node. Use this option to specify a command as a string with the argument being the script parameter above. You can read more on exec here.\\nChangelog\\n0.3.0 - Updated to nodemon 1.2.0.\\n0.2.1 - Updated README on npmjs.org with correct options.\\n0.2.0 - Updated to nodemon 1.0, added new callback option.\\nBreaking changes:\\n\\noptions.file is now script and is a required property. Some properties were changed to match nodemon: ignoredFiles -> ignore, watchedFolders -> watch, delayTime -> delay, watchedExtensions -> ext(now a string) to match nodemon.\\n\\n0.1.2 - nodemon can now be listed as a dependency in the package.json and grunt-nodemon will resolve the nodemon.js file's location correctly.\\n0.1.1 - Added legacyWatch option thanks to @jonursenbach.\\n0.1.0 - Removed debug and debugBrk options as they are encapsulated by the nodeArgs option.\\nBreaking changes:\\n\\nConfigs with the debug or debugBrk options will no longer work as expected. They simply need to be added to nodeArgs.\\n\\n0.0.10 - Added nodeArgs option thanks to @eugeneiiim.\\n0.0.9 - Fixed bug when using cwd with ignoredFiles.\\n0.0.8 - Added error logging for incorrectly installed nodemon.\\n0.0.7 - Added debugBreak option thanks to @bchu.\\n0.0.6 - Added env option.\\n0.0.5 - Added cwd option.\\n0.0.4 - Added nodemon as a proper dependency.\\n0.0.3 - Uses local version of nodemon for convenience and versioning.\\n0.0.2 - Removes .nodemonignore if it was previously generated and then the ignoredFiles option is removed.\\n0.0.1 - Added warning if nodemon isn't installed as a global module.\\n0.0.0 - Initial release\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '343',\n",
       "  'forks': '37',\n",
       "  'commits': '152'},\n",
       " {'language': 'JavaScript 96.6',\n",
       "  'readme': 'D3xter\\n\\nAbout\\n\\nSimple and powerful syntax to make common charts with minimal code.\\nHighly flexible plotting for deep customization.\\nSensible defaults but easy to configure when desired.\\nEasily extendable via familiar D3.js syntax.\\n\\nInstall\\nbower install d3xter\\nDocumentation\\nFor full documentation complete with examples, visit this page.\\nTesting and Contribution\\nRun unit tests by opening test/test.html in the browser.\\nPull requests welcome!\\nLicense\\nThe MIT License (MIT)\\n\\nCopyright (c) 2014 Nathan Epstein\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '342',\n",
       "  'forks': '17',\n",
       "  'commits': '85'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': '\\n\\n\\n\\n\\n\\n\\nA git changelog based on ANGULAR JS commit standards (but adaptable to your needs). NPM page\\n\\nWorks as a CLI option or grunt plugin\\nExample output\\n\\n\\n\\nBreaking changes and updates\\n\\nv2.0.0\\nV1.0.0\\n\\nv1.1.0\\n\\n\\n\\n\\n.changelogrc specification\\n\\nOptions | Defaults\\n\\n\\nThe \"git_changelog\" task\\n\\nGrunt Task\\n\\nGetting Started\\n\\n\\nCommand Line\\n\\n\\nGit Commit Guidelines - Source : \"Angular JS\"\\n\\nCommit Message Format\\nExample types\\nScope\\nSubject\\n\\n\\nTagging your project\\nRelease History\\n\\nv2.0.0\\nv1.0.0\\n\\n\\nContributors\\n\\nBreaking changes and updates\\nv2.0.0\\nIncluded commit_template option, allows to customize commit outputs.\\nYou can use the following commit properties:\\n\\ncommit.subject\\ncommit.body\\ncommit.link\\ncommit.hash\\ncommit.breaks\\ncommit.type\\ncommit.component\\ncloses: String containing the issues closed\\nlink: Link to the commit\\ncommit.author\\n\\nV1.0.0\\nSince version 1.0.0 git-changelog has included the .changelogrc specification and has discontinued the next options:\\n\\ngrep_commits option has been removed in favour of the .changelogrc options\\nrepo_url fixed as parameter\\nbranch_name changed to branch\\n\\nv1.1.0\\n\\nversion_name instead of version\\n\\n.changelogrc specification\\nThe .changelogrc file contains the \"standard commit guideliness\" that you and your team are following.\\nThis specification is used to grep the commits on your log, it contains a valid JSON that will tell git-changelog which sections to include on the changelog.\\n{\\n    \"app_name\": \"Git Changelog\",\\n    \"logo\": \"https://github.com/rafinskipg/git-changelog/raw/master/images/git-changelog-logo.png\",\\n    \"intro\": \"Git changelog is a utility tool for generating changelogs. It is free and opensource. :)\",\\n    \"branch\" : \"\",\\n    \"repo_url\": \"\",\\n    \"version_name\" : \"v1.0.0\",\\n    \"file\": \"CHANGELOG.md\",\\n    \"template\": \"myCustomTemplate.md\",\\n    \"commit_template\": \"myCommitTemplate.md\"\\n    \"sections\": [\\n        {\\n            \"title\": \"Bug Fixes\",\\n            \"grep\": \"^fix\"\\n        },\\n        {\\n            \"title\": \"Features\",\\n            \"grep\": \"^feat\"\\n        },\\n        {\\n            \"title\": \"Documentation\",\\n            \"grep\": \"^docs\"\\n        },\\n        {\\n            \"title\": \"Breaking changes\",\\n            \"grep\": \"BREAKING\"\\n        },\\n        {\\n            \"title\": \"Refactor\",\\n            \"grep\": \"^refactor\"\\n        },\\n        {\\n            \"title\": \"Style\",\\n            \"grep\": \"^style\"\\n        },\\n        {\\n            \"title\": \"Test\",\\n            \"grep\": \"^test\"\\n        },\\n        {\\n            \"title\": \"Chore\",\\n            \"grep\": \"^chore\"\\n        },\\n        {\\n            \"title\": \"Branchs merged\",\\n            \"grep\": \"^Merge branch\"\\n        },\\n        {\\n            \"title\" : \"Pull requests merged\",\\n            \"grep\": \"^Merge pull request\"\\n        }\\n    ]\\n}\\nOptions | Defaults\\n\\nbranch : The name of the branch. Defaults to  \\nrepo_url : The url of the project. For issues and commits links. Defaults to git config --get remote.origin.url\\nprovider : Optional field, the provider is calculated from the repo_url, but can also be passed as config parameter. Values available: gitlab, github, bitbucket.\\nversion_name: The version name of the project.\\nfile: The name of the file that will be generated. Defaults to CHANGELOG.md, leave empty for console stream\\ntemplate: The template for generating the changelog. It defaults to the one inside this project (/templates/template.md)\\ncommit_template: The template for printing each of the commits of the project. It defaults to the one inside this project (/templates/commit_template.md)\\napp_name : The name of the project. Defaults to My App - Changelog\\nintro : The introduction text on the header of the changelog. Defaults to null\\nlogo : A logo URL to be included in the header of the changelog. Defaults to null\\nchangelogrc : Relative path indicating the location of the .changelogrc file, defaults to current dir.\\ntag: You can select from which tag to generate the log, it defaults to the last one. Set it to false for log since the beginning of the project\\ndebug: Debug mode, false by default\\nsections: Group the commit by sections. The sections included by default are the ones that are on the previous example of .changelogrc file.\\n\\nThe \"git_changelog\" task\\nGrunt Task\\nGetting Started\\nThis plugin requires Grunt 1.0.0\\nIf you haven\\'t used Grunt before, be sure to check out the Getting Started guide, as it explains how to create a Gruntfile as well as install and use Grunt plugins. Once you\\'re familiar with that process, you may install this plugin with this command:\\nnpm install git-changelog --save-dev\\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\\ngrunt.loadNpmTasks(\\'git-changelog\\');\\nIn your project\\'s Gruntfile, add a section named git_changelog to the data object passed into grunt.initConfig().\\ngrunt.initConfig({\\n  git_changelog: {\\n    minimal: {\\n      options: {\\n        file: \\'MyChangelog.md\\',\\n        app_name : \\'Git changelog\\',\\n        changelogrc : \\'/files/.changelogrc\\',\\n        logo : \\'https://github.com/rafinskipg/git-changelog/raw/master/images/git-changelog-logo.png\\',\\n        intro : \\'Git changelog is a utility tool for generating changelogs. It is free and opensource. :)\\'\\n      }\\n    },\\n    extended: {\\n      options: {\\n        app_name : \\'Git changelog extended\\',\\n        file : \\'EXTENDEDCHANGELOG.md\\',\\n        version_name : \\'squeezy potatoe\\',\\n        sections : [\\n          {\\n            \"title\": \"Test commits\",\\n            \"grep\": \"^test\"\\n          },\\n          {\\n            \"title\": \"New Awesome Features!\",\\n            \"grep\": \"^feat\"\\n          }\\n        ],\\n        debug: true,\\n        tag : false //False for commits since the beggining\\n      }\\n    },\\n    fromCertainTag: {\\n      options: {\\n        repo_url: \\'https://github.com/rafinskipg/git-changelog\\',\\n        app_name : \\'My project name\\',\\n        file : \\'tags/certainTag.md\\',\\n        tag : \\'v0.0.1\\'\\n      }\\n    },\\n    customTemplate: {\\n      options: {\\n        app_name : \\'Custom Template\\',\\n        intro: \\'This changelog is generated with a custom template\\',\\n        file: \\'output/customTemplate.md\\',\\n        template: \\'templates/template_two.md\\',\\n        logo : \\'https://github.com/rafinskipg/git-changelog/raw/master/images/git-changelog-logo.png\\',\\n        version_name : \\'squeezy potatoe\\',\\n        tag: \\'v0.0.1\\',\\n        debug: true\\n      }\\n    }\\n  }\\n})\\nCommand Line\\nInstall it globally\\nnpm install -g git-changelog\\n\\nSee commands\\ngit-changelog -h\\n\\nUse it directly with the common options\\n Usage: git-changelog [options]\\n\\n  Options:\\n    -V, --version                               output the version number\\n    -e, --extended                              Extended log\\n    -n, --version_name [version_name]           Name of the version\\n    -a, --app_name [app_name]                   Name [app_name]\\n    -b, --branch [branch]                       Branch name [branch]\\n    -f, --file [file]                           File [file]\\n    -tpl, --template [template]                 Template [template]\\n    -ctpl, --commit_template [commit_template]  Commit Template [commit_template]\\n    -r, --repo_url [repo_url]                   Repo url [repo_url]\\n    -l, --logo [logo]                           Logo path [logo]\\n    -i, --intro [intro]                         intro text [intro]\\n    -t, --tag [tag]                             Since tag [tag]\\n    -rc, --changelogrc [changelogrc]            .changelogrc relative path [changelogrc]\\n    -g, --grep [grep]                           Grep commits for [grep]\\n    -d, --debug                                 Debugger\\n    -p, --provider [provider]                   Provider: gitlab, github, bitbucket (Optional)\\n    -h, --help                                  output usage information\\n\\n\\n\\nFor example:\\ngit-changelog -t false -a \"My nice application\"\\n\\nGit Commit Guidelines - Source : \"Angular JS\"\\nWe have very precise rules over how our git commit messages can be formatted.  This leads to more\\nreadable messages that are easy to follow when looking through the project history.  But also,\\nwe use the git commit messages to generate the AngularJS change log.\\nCommit Message Format\\nEach commit message consists of a header, a body and a footer.  The header has a special\\nformat that includes a type, a scope and a subject:\\n<type>(<scope>): <subject>\\n<BLANK LINE>\\n<body>\\n<BLANK LINE>\\n<footer>\\n\\nAny line of the commit message cannot be longer 100 characters! This allows the message to be easier\\nto read on github as well as in various git tools.\\nExample commit messages\\ngit commit -m \"docs(readme): Add documentation for explaining the commit message\"\\ngit commit -m \"refactor: Change other things\"\\n\\nClosing issues :\\ngit commit -m \"fix(git_changelog_generate): pass tag if it exists to gitReadLog\\nPreviously if a tag was found the script would try to find commits\\nbetween undefined..HEAD. By passing the tag, it now finds tags between\\ntag..HEAD.\\n\\nCloses #5.\"\\n\\nExample types\\nYou may define your own types refering to the .changelogrc specification\\nMust be one of the following:\\n\\nfeat: A new feature\\nfix: A bug fix\\ndocs: Documentation only changes\\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\nrefactor: A code change that neither fixes a bug or adds a feature\\ntest: Adding missing tests\\nchore: Changes to the build process or auxiliary tools and libraries such as documentation generation\\n\\nScope\\nThe scope could be anything specifying place of the commit change. For example $location,\\n$browser, $compile, $rootScope, ngHref, ngClick, ngView, etc...\\nSubject\\nThe subject contains succinct description of the change:\\n\\nuse the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\\ndon\\'t capitalize first letter\\nno dot (.) at the end\\n\\n###Body\\nJust as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\\nThe body should include the motivation for the change and contrast this with previous behavior.\\n###Footer\\nThe footer should contain any information about Breaking Changes and is also the place to\\nreference GitHub issues that this commit Closes.\\nA detailed explanation can be found in this [document][commit-message-format].\\n[commit-message-format]: https://docs.google.com/document/d/1QrDFcIiPjSLDn3EL15IJygNPiHORgU1_OOAqWjiDU5Y/edit#\\nTagging your project\\nIn order to have you project versions correctly displayed on your changelog, try to use this commit message format:\\nchore(release): v1.4.0 codename(jaracimrman-existence)\\n\\nIn order to do that, you can use git annotated tags:\\ngit tag -a v1.4.0 -m \\'chore(release): v1.4.0 codename(jaracimrman-existence)\\'\\n\\nIf you are publishing NPM modules you can let NPM do that for you:\\nnpm version patch -m \"chore(release): %s codename(furious-stallman)\"\\n\\nRelease History\\nv2.0.0\\n\\nIntroduced Commit Template\\n\\nv1.0.0\\n\\nSupport for .changelogrc\\n\\nContributors\\nAdd your name here by contributing to this project\\n\\nrafinskipg\\nJohnnyEstilles\\ncolegleason\\njodybrewster\\nGlebcha\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '326',\n",
       "  'forks': '82',\n",
       "  'commits': '209'},\n",
       " {'language': 'JavaScript 95.8',\n",
       "  'readme': ' Vienna IKS Editables\\nVIE is a utility library for implementing decoupled Content Management systems. VIE is developed as part of the EU-funded IKS project.\\n\\n\\nIn French, vie means life, showcasing how VIE makes your website come alive\\nIn English, vie means striving for victory or superiority\\n\\nVIE development is now targeting a 2.0 release. Read this blog post to find out the changes from VIE 1.0. There is also a good introductory post on VIE on the IKS blog.\\nVIE integration in nutshell\\nAdding VIE to your system is as easy as:\\n\\nMark up your pages with RDFa annotations\\nInclude vie.js into the pages\\nImplement Backbone.sync\\n\\nChanges\\nPlease refer to the CHANGES.md document.\\nCommon representation of content on HTML level\\nA web editing tool has to understand the contents of the page. It has to understand what parts of the page should be editable, and how they connect together. If there is a list of news for instance, the tool needs to understand it enough to enable users to add new news items. The easy way of accomplishing this is to add some semantic annotations to the HTML pages. These annotations could be handled via Microformats, HTML5 microdata, but the most power lies with RDFa.\\nRDFa is a way to describe the meaning of particular HTML elements using simple attributes. For example:\\n<div id=\"myarticle\" typeof=\"http://rdfs.org/sioc/ns#Post\" about=\"http://example.net/blog/news_item\">\\n    <h1 property=\"dcterms:title\">News item title</h1>\\n    <div property=\"sioc:content\">News item contents</div>\\n</div>\\n\\nHere we get all the necessary information for making a blog entry editable:\\n\\ntypeof tells us the type of the editable object. On typical CMSs this would map to a content model or a database table\\nabout gives us the identifier of a particular object. On typical CMSs this would be the object identifier or database row primary key\\nproperty ties a particular HTML element to a property of the content object. On a CMS this could be a database column\\n\\nAs a side effect, this also makes pages more understandable to search engines and other semantic tools. So the annotations are not just needed for UI, but also for SEO.\\nCommon representation of content on JavaScript level\\nHaving contents of a page described via RDFa makes it very easy to extract the content model into JavaScript. We can have a common utility library for doing this, but we also should have a common way of keeping track of these content objects. Enter Backbone.js:\\n\\nBackbone supplies structure to JavaScript-heavy applications by providing models with key-value binding and custom events, collections with a rich API of enumerable functions, views with declarative event handling, and connects it all to your existing application over a RESTful JSON interface.\\n\\nWith Backbone, the content extracted from the RDFa-annotated HTML page is easily manageable via JavaScript. Consider for example:\\nv = new VIE();\\nv.use(new v.RdfaService());\\nv.load({element: jQuery(\\'#myarticle\\')}).from(\\'rdfa\\').execute().success(function(entities) {\\n    _.forEach(entities, function(entity) {\\n        entity.set({\\'dcterms:title\\': \\'Hello, world\\'});\\n        entity.save(null, {\\n            success: function(savedModel, response) {\\n                alert(\"Your article \\'\" + savedModel.get(\\'dcterms:title\\') + \"\\' was saved to server\");\\n            }\\n        });\\n    })\\n    console.log(\"We got \" + entities.length + \" editable objects from the page\");\\n});\\n\\nThe classic VIE API will also work:\\nvar v = new VIE({classic: true});\\nvar objectInstance = v.RDFaEntities.getInstance(jQuery(\\'#myarticle\\'));\\nobjectInstance.set({\\'dcterms:title\\': \\'Hello, world\\'});\\nobjectInstance.save(null, {\\n    success: function(savedModel, response) {\\n        alert(\"Your article \\'\" + savedModel.get(\\'dcterms:title\\') + \"\\' was saved to server\");\\n    }\\n});\\n\\nThis JS would work across all the different CMS implementations. Backbone.js provides a quite nice RESTful implementation of communicating with the server with JSON, but it can be easily overridden with CMS-specific implementation by just implementing a new Backbone.sync method.\\nExample\\nThere is a full static HTML example of VIE at work. Saving outputs the edited contents as JSON into the JavaScript console:\\n\\nExample with Hallo\\n\\nBe sure to read the annotated VIE source code for API documentation.\\nI/O operations\\nAll Input/Output operations of VIE are based on the jQuery Deferred object, which means that you can attach callbacks to them either before they run, or also after they\\'ve been run.\\nThe operations may either succeed, in which case the then callbacks will fire, or be rejected, in which case the fail callbacks will fire. Any then callbacks will fire in either case.\\nDependencies\\nVIE uses the following JavaScript libraries:\\n\\njQuery for DOM manipulation and Deferreds\\nBackbone.js for entities (models) and collections\\nUnderscore.js for various JavaScript utilities\\n\\nSome functionality in VIE additionally uses:\\n\\nRdfQuery as a triplestore and for reasoning over rules\\n\\nIntegrations\\n\\nCreate\\nGoogle Web Toolkit\\nSymfony2\\nPalsu\\n\\nUsing VIE on Node.js\\nVIE is a CommonJS library that works on both browser and the server. On Node.js you can install it with:\\nnpm install vie\\n\\nHere is a simple Node.js script that uses VIE for parsing RDFa:\\nvar jQuery = require(\\'jquery\\');\\nvar vie = require(\\'vie\\');\\n\\n// Instantiate VIE\\nvar VIE = new vie.VIE();\\n\\n// Enable the RDFa service\\nVIE.use(new VIE.RdfaService());\\n\\nvar html = jQuery(\\'<p xmlns:dc=\"http://purl.org/dc/elements/1.1/\" about=\"http://www.example.com/books/wikinomics\">In his latest book <cite property=\"dc:title\">Wikinomics</cite>, <span property=\"dc:creator\">Don Tapscott</span> explains deep changes in technology, demographics and business.</p>\\');\\n\\n// \\nVIE.load({element: html}).from(\\'rdfa\\').execute().done(function() {\\n\\n  var objectInstance = VIE.entities.get(\\'http://www.example.com/books/wikinomics\\');\\n\\n  console.log(objectInstance.get(\\'dc:title\\'));\\n\\n});\\n\\nDevelopment\\nVIE development is coordinated using Git at bergie/VIE.\\nFeel free to report issues or send pull requests if you have ideas for pushing VIE forward. Contributions that include their own unit tests appreciated!\\nDevelopment discussions happen on the VIE mailing list and the #iks channel on Freenode. See also VIE on Ohloh.\\nCode organization\\nVIE source code is inside the src directory. Each separate unit of functionality should be handled in its own file, with the src/VIE.js being the entry point to the system.\\n\\nBuilding VIE\\nThe VIE library consists of many individual pieces that we merge together in the build process. You\\'ll need Grunt. Then just run:\\n$ grunt build\\n\\nThe built VIE library will appear in the dist folder.\\nCore-only distribution\\nIn addition to the regular full build, there is also a slimmer build of VIE available that only includes the core parts of the library and no external service. To build that instead, run:\\n$ grunt build:core\\n\\nRunning Unit Tests\\nDirect your browser to the test/index.html file to run VIE\\'s QUnit tests.\\nUnit tests on Node.js\\nThe Grunt testing setup includes multiple parts. With it, you can test the library on both Node.js and a headless browser. Run:\\n$ grunt test\\n\\nor:\\n$ npm test\\n\\nAutomatic unit tests\\nYou can also run the Grunt setup in watch mode, where any change in VIE sources or tests will trigger a rebuild and test run:\\n$ grunt watch\\n\\nContinuous integration\\nVIE uses Travis for continuous integration. Simply add your fork there and every time you push you\\'ll get the tests run.\\n\\n\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '307',\n",
       "  'forks': '56',\n",
       "  'commits': '1,309'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"jQuery Image Preload Plugin\\nThe jQuery.imgpreload plugin lets you preload images before and after the DOM has loaded.\\nTested: IE6, IE7, IE8, IE9+, FF, Chrome, Safari\\nUsage\\nCallbacks\\nThe following are available callbacks, you may change them globally or override the defaults by passing the settings object to the imgpreload method.\\n$.fn.imgpreload.defaults =\\n{\\n    each: null // callback invoked when each image is loaded\\n    , all: null // callback invoked when all images have loaded\\n};\\n\\nAfter DOM loaded\\nThe following illustrates using the plugin to preload images after the dom has loaded (e.g. using jQuery selectors):\\n$('#content img').imgpreload(function()\\n{\\n    // callback invoked when all images have loaded\\n    // this = array of dom image objects\\n    // check for success with: $(this[i]).data('loaded')\\n});\\n\\n$('img.logos').imgpreload\\n({\\n    each: function()\\n    {\\n        // callback invoked when each image is loaded\\n        // this = dom image object\\n        // check for success with: $(this).data('loaded')\\n    },\\n    all: function()\\n    {\\n        // callback invoked when all images have loaded\\n        // this = array of dom image objects\\n        // check for success with: $(this[i]).data('loaded')\\n    }\\n});\\n\\nBefore DOM loaded\\nTo preload images before the dom has loaded, for instance in the head of the document, you would have to use specific image paths.\\n$.imgpreload('/images/a.gif',function()\\n{\\n    // callback invoked when all images have loaded\\n    // this = array of dom image objects\\n    // check for success with: $(this[i]).data('loaded')\\n});\\n\\nYou can pass a single image path (as above) or an array of image paths.\\n$.imgpreload(['/images/a.gif','/images/b.gif'],function()\\n{\\n    // this = array of dom image objects\\n    // check for success with: $(this[i]).data('loaded')\\n    // callback executes when all images are loaded\\n});\\n\\neach and all callbacks are available to use.\\n$.imgpreload(['/images/a.gif','/images/b.gif'],\\n{\\n    each: function()\\n    {\\n        // callback invoked when each image is loaded\\n        // this = dom image object\\n        // check for success with: $(this).data('loaded')\\n    },\\n    all: function()\\n    {\\n        // callback invoked when all images have loaded\\n        // this = array of dom image objects\\n        // check for success with: $(this[i]).data('loaded')\\n    }\\n});\\n\\n\",\n",
       "  'watchers': '11',\n",
       "  'stars': '302',\n",
       "  'forks': '110',\n",
       "  'commits': '41'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': '\\xa0\\n\\n\\n\\n\\xa0\\nRecon\\nCode Intelligence for React Applications.\\n\\nWhat?\\nRecon provides a new level of insight into your codebase allowing you to understand\\nhow your components are being used, easily grep their complexity, draw dependency graphs,\\nand isolate points for optimisation.\\nOn top of this the raw information Recon provides gives a strong base for creating tools\\n(such as living styleguides) which may need to plug in to component meta data.\\nHow?\\nThe core of Recon revolves around recon-engine. This engine parses your application pulling out\\nany data which may be useful (eg. Props, component dependencies, enhancements etc.). Then a\\ngraphql query interface is exposed allowing you to explore your applications in an incredibly\\nintuitive manner!\\nChecking out our test fixtures is a\\ngreat place to an example of this.\\nOnce this data is consolidated the possibility of tools to be built on top are endless!\\nGetting Started\\nPrerequisites: Node >v6. Also ensure using import/export syntax, using JSX (see roadmap for full list)\\nInstall and configuration\\nThe quickest way to get going with Recon for your project is to use our CLI application.\\nFirstly install with\\n$ npm install -g recon-cli\\n\\nNow, within your application working directory, simply run\\n$ recon\\n\\nYou are now inside Recon! :O\\nFrom this point forwards the entire power of Recon should be just a help command away.\\nrecon$ help\\n\\nThe first thing you\\'re going to want to do from here is create a new config file .reconrc in the working directory\\nof your project.\\nYou can do this interactively by running\\nrecon$ init\\n\\nTo view all configuration possibilities you can view the docs here.\\nShow me the power!\\nWhy not start off by trying stats. This will analyse your application and dump out a bunch of objective\\nstatements and statistics.\\nrecon$ stats\\n\\nThen if you\\'re feeling extra adventurous give server a go. This will spawn a new graphql server which will\\nallow you to query your application meta data freely.\\nrecon$ server\\n\\nFor everything else see what is available via the help command!\\nI want to integrate Recon into x\\nDocumentation is going to be a little skimpy here for a while since we are planning on getting\\nthe internals of recon-engine to be as powerful as possible and stabilising the api as much as\\npossible.\\nMost likely you\\'ll want to look at using recon-engine and recon-server (their tests are a decent\\nplace to start looking).\\n\\nDisclaimer: This is a very early preview of Recon and you should expect breaking changes within the <v1 range of releases.\\n\\nContributing\\n\\nBugs? Please submit a Pull Request with your minimal source code and a test which breaks.\\nWant to fix something or add a new feature? Get started with our Dev Guide!\\n\\nFor more details on all contributions see CONTRIBUTING.md\\nHigh-level Roadmap\\n\\n Move CLI away from vorpal to a non-interactive version\\n\\n The engine should be spawned in the background. Ie. similar to flow server\\n\\n\\n Identify initial parsing issues across many codebases\\n\\n Improve test .toMatch to not care about ordering (ie. add toMatchUnordered)\\n Provide better DX for dumping debug data and reporting issues\\n\\n\\n Pull prop type definitions from components\\n Broader React usage support\\n\\n React.createClass, React.createElement, hyperscript\\n Support decorator syntax as enhancements\\n Support require(\\'module\\')\\n\\n\\n Better prop usage information\\n\\n Pull out static values (eg. prop=\"stringValue\")\\n Resolve basic flows to determine possible values (upto prop types)\\n Resolve rest/spread props\\n\\n\\n More trivial meta data! (eg. docblocks)\\n Stabilise graphql interface\\n Support long-running persisted engine (ie. watch file changes)\\n Work on documentation and integrations for developer tools\\n Explore plugin api\\n Reassess and tidy implementation\\n\\n Performance optimisations\\n\\n\\n Identify application critical paths\\n Explore using Flow for internal type resolution\\n\\nLicense\\nApache 2.0\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '301',\n",
       "  'forks': '9',\n",
       "  'commits': '75'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"numtel:pg \\nReactive PostgreSQL for Meteor\\nProvides Meteor integration of the pg-live-select NPM module, bringing reactive SELECT statement result sets from PostgreSQL >= 9.3.\\n\\nIf you do not have PostgreSQL server already installed, you may use the numtel:pg-server Meteor Package to bundle the PostgreSQL server directly to your Meteor application.\\n\\n\\nHow to publish joined queries that update efficiently\\nLeaderboard example modified to use PostgreSQL\\nReactive MySQL for Meteor\\n\\nServer Implements\\nThis package provides the LivePg class as defined in the pg-live-select NPM package.\\nAlso exposed on the server is the pg object as defined in the node-postgres NPM package (useful for other operations like INSERT and UPDATE).\\nLivePg.prototype.select()\\nIn this Meteor package, the SelectHandle object returned by the select() method is modified to act as a cursor that can be published.\\nvar liveDb = new LivePg(CONNECTION_STRING, CHANNEL);\\n\\nMeteor.publish('allPlayers', function(){\\n  return liveDb.select('SELECT * FROM players ORDER BY score DESC');\\n});\\nClient/Server Implements\\nPgSubscription([connection,] name, [args...])\\nConstructor for subscribing to a published select statement. No extra call to Meteor.subscribe() is required. Specify the name of the subscription along with any arguments.\\nThe first argument, connection, is optional. If connecting to a different Meteor server, pass the DDP connection object in this first argument. If not specified, the first argument becomes the name of the subscription (string) and the default Meteor server connection will be used.\\nThe prototype inherits from Array and is extended with the following methods:\\n\\n\\n\\nName\\nDescription\\n\\n\\n\\n\\nchange([args...])\\nChange the subscription's arguments. Publication name and connection are preserved.\\n\\n\\naddEventListener(eventName, listener)\\nBind a listener function to this subscription\\n\\n\\nremoveEventListener(eventName)\\nRemove listener functions from an event queue\\n\\n\\ndispatchEvent(eventName, [args...])\\nCall the listeners for a given event, returns boolean\\n\\n\\ndepend()\\nCall from inside of a Template helper function to ensure reactive updates\\n\\n\\nreactive()\\nSame as depend() except returns self\\n\\n\\nchanged()\\nSignal new data in the subscription\\n\\n\\nready()\\nReturn boolean value corresponding to subscription fully loaded\\n\\n\\nstop()\\nStop updates for this subscription\\n\\n\\n\\nNotes:\\n\\nchanged() is automatically called when the query updates and is most likely to only be called manually from a method stub on the client.\\nEvent listener methods are similar to native methods. For example, if an event listener returns false exactly, it will halt listeners of the same event that have been added previously. A few differences do exist though to make usage easier in this context:\\n\\nThe event name may also contain an identifier suffix using dot namespacing (e.g. update.myEvent) to allow removing/dispatching only a subset of listeners.\\nremoveEventListener() and dispatchEvent() both refer to listeners by name only. Regular expessions allowed.\\nuseCapture argument is not available.\\n\\n\\n\\nEvent Types\\n\\n\\n\\nName\\nListener Arguments\\nDescription\\n\\n\\n\\n\\nupdate\\ndiff, data\\nNew change, before data update\\n\\n\\nupdated\\ndiff, data\\nNew change, after data update\\n\\n\\nreset\\nmsg\\nSubscription reset (most likely due to code-push), before update\\n\\n\\n\\nClosing connections between hot code-pushes\\nWith Meteor's hot code-push feature, new triggers and functions on database server are created with each restart. In order to remove old items, a handler to your application process's SIGTERM signal event must be added that calls the cleanup() method on each LivePg instance in your application. Also, a handler for SIGINT can be used to close connections on exit.\\nOn the server-side of your application, add event handlers like this:\\nvar liveDb = new LivePg(CONNECTION_STRING, CHANNEL);\\n\\nvar closeAndExit = function() {\\n  // Call process.exit() as callback\\n  liveDb.cleanup(process.exit);\\n};\\n\\n// Close connections on hot code push\\nprocess.on('SIGTERM', closeAndExit);\\n// Close connections on exit (ctrl + c)\\nprocess.on('SIGINT', closeAndExit);\\nTests / Benchmarks\\nThe test suite does not require a separate Postgres server installation as it uses the numtel:pg-server package to run the tests.\\nThe database connection settings must be configured in test/settings/local.json.\\nThe database specified should be an empty database with no tables because the tests will create and delete tables as needed.\\n# Install Meteor\\n$ curl -L https://install.meteor.com/ | /bin/sh\\n\\n# Clone Repository\\n$ git clone https://github.com/numtel/meteor-pg.git\\n$ cd meteor-pg\\n\\n# Optionally, configure port and data dir in test/settings/test.pg.json.\\n# If changing port, keep port value updated in test/index.es6 as well.\\n\\n# Test database will be created in dbtest directory.\\n\\n# Run test server\\n$ meteor test-packages ./\\n\\nLicense\\nMIT\\n\",\n",
       "  'watchers': '28',\n",
       "  'stars': '299',\n",
       "  'forks': '16',\n",
       "  'commits': '66'},\n",
       " {'language': 'JavaScript 84.0',\n",
       "  'readme': 'angular-datepicker\\nWarning: This project is no longer maintained. Use at your own risk!\\nThe mobile-friendly, responsive, and lightweight Angular.js date & time input picker. Perfect for Ionic apps!\\n, \\nThis is basically a pickadate.js fork that completely removes the jQuery dependency and adds Angular.js directives.\\nBower\\nbower install angular-native-picker\\nUsage\\nInclude build/angular-datepicker.js in your application:\\n<script src=\"angular-datepicker.js\"></script>\\nInclude CSS files in your application:\\n<link rel=\"stylesheet\" href=\"build/themes/default.css\"/>\\n<link rel=\"stylesheet\" href=\"build/themes/default.date.css\"/>\\n<link rel=\"stylesheet\" href=\"build/themes/default.time.css\"/>\\nNote: for usage within a modal the default (not classic) CSS files are recommended.\\nAdd the module angular-datepicker as a dependency to your app module:\\nvar myapp = angular.module(\\'myapp\\', [\\'angular-datepicker\\']);\\nTo create a date or time picker, add the pick-a-date or pick-a-time directive to your input tag:\\n<input type=\"text\" pick-a-date=\"date\" placeholder=\"Select Date\" /> {{ date }}\\n<input type=\"text\" pick-a-time=\"time\" placeholder=\"Select Time\" /> {{ time }}\\nYou can also provide an options object to the directive which will be passed\\ninto the pickadate or pickatime constructor.\\n// somewhere in your controller\\n$scope.options = {\\n  format: \\'yyyy-mm-dd\\', // ISO formatted date\\n  onClose: function(e) {\\n    // do something when the picker closes   \\n  }\\n}\\n<input type=\"text\" pick-a-date=\"date\" pick-a-date-options=\"options\" /> {{ date }}\\n<input type=\"text\" pick-a-time=\"time\" pick-a-time-options=\"options\" /> {{ time }}\\nIf you don\\'t need to provide any callbacks (like onClose) you can specify the\\noptions object as an angular expression:\\n<input type=\"text\" pick-a-date=\"date\" pick-a-date-options=\"{ format: \\'yyyy-mm-dd\\' }\" />\\nFor a full list of available options please refer to the pickadate documentation\\nfor datepicker options and\\ntimepicker options.\\n',\n",
       "  'watchers': '20',\n",
       "  'stars': '298',\n",
       "  'forks': '139',\n",
       "  'commits': '95'},\n",
       " {'language': 'JavaScript 72.1',\n",
       "  'readme': 'v2er\\nA simple v2ex client app, use React Native.\\n使用 V2EX 的 api 进行的一个关于 React-native 的试验。\\n旨在试验各个功能是否都可以用 React-native 来实现。\\n\\nUPDATE 2017-11-1\\n\\n更新 react-native 到最新版本\\n\\n  \"react\": \"16.0.0-beta.5\",\\n  \"react-native\": \"^0.49.5\",\\n\\n\\n增加 react-native-vector-icons ，在 npm install 后需要运行 react-native link\\n使用新的语法进行功能的制作\\n增加 Store 部分的处理\\n优化代码结构，去除不必要的部分\\nTODO\\n\\n新的节点显示\\n节点相关的操作\\n\\n\\n\\n\\n2015-04-01\\n\\n修正了一些样式上的问题\\n完善了节点的显示\\n增加了对于网络状态的判断，如果为 cell 的时候不载入列表的图片\\n\\n\\n2015-03-31\\n\\n增加了 Tabbar\\n增加了所有节点的显示\\n\\n\\n2015-03-29\\n\\n获取最近主题列表。\\n获取主题的内容以及相应的评论。\\n\\n',\n",
       "  'watchers': '17',\n",
       "  'stars': '298',\n",
       "  'forks': '39',\n",
       "  'commits': '26'},\n",
       " {'language': 'JavaScript 89.5',\n",
       "  'readme': 'Skitter - Slideshow for anytime!\\nSkitter has 38 different animations, two types of navigations and many options to customize!\\nLicense: Dual licensed under the MIT or GPL Version 2\\nNPM\\nnpm install skitter-slider\\nBower\\nbower install skitter --save\\nManually\\nDownload zip https://github.com/thiagosf/skitter/archive/master.zip and move files in dist directory for you application. Attention: you need to download the dependencies manually.\\nHow to install\\nAdd the CSS and JS files inside \\n<link type=\"text/css\" href=\"dist/skitter.css\" media=\"all\" rel=\"stylesheet\" />\\n<script type=\"text/javascript\" src=\"https://code.jquery.com/jquery-2.1.1.min.js\"></script>\\n<script type=\"text/javascript\" src=\"js/jquery.easing.1.3.js\"></script>\\n<script type=\"text/javascript\" src=\"dist/jquery.skitter.min.js\"></script>\\nInit the Skitter\\n$(document).ready(function() {\\n  $(\".skitter-large\").skitter();\\n});\\nAdd the images through the unordered list\\n<div class=\"skitter skitter-large\">\\n  <ul>\\n    <li>\\n      <a href=\"#cut\"><img src=\"images/001.jpg\" class=\"cut\" /></a>\\n      <div class=\"label_text\"><p>cut</p></div>\\n    </li>\\n    <li>\\n      <a href=\"#swapBlocks\"><img src=\"images/002.jpg\" class=\"swapBlocks\" /></a>\\n      <div class=\"label_text\"><p>swapBlocks</p></div>\\n    </li>\\n    <li>\\n      <a href=\"#swapBarsBack\"><img src=\"images/003.jpg\" class=\"swapBarsBack\" /></a>\\n      <div class=\"label_text\"><p>swapBarsBack</p></div>\\n    </li>\\n  </ul>\\n</div>\\nTodo\\n\\n Update WP-Plugin\\n Update CakePHP Plugin\\n Refactor variable names applying a pattern.\\n Separate into small pieces the source code.\\n Create unit tests\\n Exchange images for background divs (for animations)\\n\\n',\n",
       "  'watchers': '22',\n",
       "  'stars': '295',\n",
       "  'forks': '115',\n",
       "  'commits': '119'},\n",
       " {'language': 'Python 51.0',\n",
       "  'readme': 'SecPub\\nPublished security vulnerabilities, research, write-ups, and associated\\ninformation which I have worked on. Vulnerabilities have been broken down\\nper vendor - where applicable.\\nExternal\\nResearch\\n\\nMessing with SWD - Part I\\nPivoting from blind SSRF to RCE with HashiCorp Consul\\nRemote Code Execution (RCE) on Microsoft\\'s \\'signout.live.com\\'\\n\\nCTF\\n\\nBKP CTF - Wackusensor Write-Up\\nBKP CTF - Good Morning (Wonderland)\\nBKP CTF - Bug Bounty (Suffolk Downs)\\n9447 CTF - Super Turbo Atomic GIF Converter\\n\\nVulnerabilities\\n\\nJenkins Swarm Plugin - XXE (XML External Entities) via UDP broadcast\\nJenkins GitLab Plugin - Information disclosure vulnerability\\nJenkins Artifactory Plugin - Information disclosure vulnerabilities\\nJenkins Ansible Tower Plugin - Information disclosure vulnerability\\nNetGear WNDR Authentication Bypass / Information Disclosure\\nD-Link and TRENDnet \\'ncc2\\' service - multiple vulnerabilities\\nD-Link DSP-W110 - multiple vulnerabilities\\nCisco Nexus OS (NX-OS) - Command \"injection\" / sanitization issues.\\n\\n',\n",
       "  'watchers': '37',\n",
       "  'stars': '133',\n",
       "  'forks': '33',\n",
       "  'commits': '32'},\n",
       " {'language': 'Python 99.5',\n",
       "  'readme': \"\\nNOTICE: Deprecated\\nThis project is deprecated and no longer actively maintained by Disqus. However there is a fork being maintained by YPlan at github.com/YPlan/django-modeldict and a similar project by Disqus at github.com/disqus/durabledict.\\n\\ndjango-modeldict\\nModelDict is a very efficient way to store things like settings in your database. The entire model is transformed into a dictionary (lazily) as well as stored in your cache. It's invalidated only when it needs to be (both in process and based on CACHE_BACKEND).\\nQuick example usage. More docs to come (maybe?):\\nclass Setting(models.Model):\\n    key = models.CharField(max_length=32)\\n    value = models.CharField(max_length=200)\\nsettings = ModelDict(Setting, key='key', value='value', instances=False)\\n\\n# access missing value\\nsettings['foo']\\n>>> KeyError\\n\\n# set the value\\nsettings['foo'] = 'hello'\\n\\n# fetch the current value using either method\\nSetting.objects.get(key='foo').value\\n>>> 'hello'\\n\\nsettings['foo']\\n>>> 'hello'\\n\\n\",\n",
       "  'watchers': '24',\n",
       "  'stars': '132',\n",
       "  'forks': '25',\n",
       "  'commits': '73'},\n",
       " {'language': 'Python 98.7',\n",
       "  'readme': 'hermes\\nHermes is an extensible XMPP-based chatroom server written in Python.\\nEasily setup and manage chatrooms for friends or colleagues.\\nHow it Works\\nSupply your own XMPP-based accounts (Google accounts work great!) to serve as chatroom hosts, add a bit of configuration, and that\\'s it.\\nAll chatroom members are invited to chat with the chatroom host which will in turn broadcast their messages to all other members.\\nUsage\\nThe \"Hello World\" usage of Hermes looks like this. Put the following in chatserver.py, update the user and chatroom info, and run it:\\nfrom hermes.api import run_server\\n\\nbrain = { \\'JID\\': \\'brain@wb.com\\', \\'NICK\\': \\'brain\\', \\'ADMIN\\': True }\\npinky = { \\'JID\\': \\'pinky.suavo@wb.com\\', \\'NICK\\': \\'pinky\\' }\\n\\nchatrooms = {\\n    \\'world-domination-planning\\': {\\n        \\'JID\\': \\'world.domination.planning@wb.com\\',\\n        \\'PASSWORD\\': \\'thesamethingwedoeverynight\\',\\n        \\'SERVER\\': (\\'talk.google.com\\', 5223,),\\n        \\'MEMBERS\\': [pinky, brain],\\n    },\\n}\\n\\nrun_server(chatrooms)\\n\\nInstallation\\nAvailable from PyPI: http://pypi.python.org/pypi/hermes/. pip is the recommended installation method:\\npip install hermes\\n\\nCommands\\nHermes interprets some messages as commands:\\n\\n\\n/mute - Mute the chatroom. Messages are queued up for whenever you unmute the chatroom.\\n\\n\\n/unmute - Unmute the chatroom. Receive all messages that were queued while the chatroom was muted.\\n\\n\\n/invite <handle> - Invite members to the chatroom. Available to admins only.\\n\\n\\n/kick <handle> - Kick members from the chatroom. Available to admins only.\\n\\n\\n/marco - Not sure if you\\'re still connected to the chatroom? Chatroom replies to you (and only you) with \"polo\".\\n\\n\\nExtensibility\\nYou can extend the base chatroom class hermes.Chatroom to modify or add extra functionality.\\nAdding a command_patterns static property to your class should be particularly useful for extensions.\\nIt\\'s a list of regular expression/method name pairs. Each incoming message is tested against the regexes until a match is found.\\nOn a match, the named instance method is invoked to handle the message instead of the default message-handling pipeline.\\nSpecify the path to your creation as a string or the Class itself as the CLASS of your chatroom:\\nfrom hermes.api import run_server, Chatroom\\n\\nclass BillyMaysChatroom(Chatroom):\\n\\tcommand_patterns = ((r\\'.*\\', \\'shout\\'),)\\n\\n\\tdef shout(self, sender, body, match):\\n        body = body.upper() #SHOUT IT\\n        self.broadcast(body)\\n\\nchatrooms = {\\n    \\'world-domination-planning\\': {\\n        \\'CLASS\\': \\'BillyMaysChatroom\\',\\n        \\'JID\\': \\'world.domination.planning@wb.com\\',\\n        \\'PASSWORD\\': \\'thesamethingwedoeverynight\\',\\n        \\'SERVER\\': (\\'talk.google.com\\', 5223,),\\n        \\'MEMBERS\\': [pinky, brain],\\n    },\\n}\\n\\nrun_server(chatrooms)\\n\\nUpcoming Features\\n\\nPersistence: Allow chatroom configuration/state/history to be persisted\\n\\nIs it any good?\\nYes.\\nElaborate, you say? Hermes has been successfully used \"in production\" to run several chatrooms for the Crocodoc team since the first commit. It\\'s good to have guinea pigs.\\nLicense\\nHermes is an ISC licensed library. See LICENSE for more details. If you insist on compensating me, I\\'d let you buy me a beer. Or just donate money to a good cause...that\\'d probably be best.\\nCan I Contribute?\\nYes, please do. Pull requests are great. I\\'ll totally add a CONTRIBUTORS.txt when Hermes gets its first contributor.\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '132',\n",
       "  'forks': '10',\n",
       "  'commits': '39'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'git-goggles Readme\\ngit-goggles is a git management utilities that allows you to manage your source code as\\nit evolves through its development lifecycle.\\n\\nOverview\\nThis project accomplishes two things:\\n\\nManage the code review state of your branches\\nGives a snapshot of the where your local branches are vs origin in terms of being ahead / behind on commits\\n\\nThere is a nice blog post describing the features along with screenshots at http://bit.ly/git-goggles\\n\\nField Reference\\nIn the table outputted by git-goggles, each row corresponds to a branch, with the following fields:\\n\\nStatus: the current status of your branch\\n\\n\\nnew: this is a branch that has never been through the review process\\nreview: this branch has code that needs to be reviewed\\nmerge: everything has been reviewed, but needs to be merged into parent (same as done for being ahead)\\ndone: reviewed and merged (doens\\'t matter if you\\'re behind but you can\\'t be ahead)\\n\\n\\n\\nBranch: the branch name\\n\\nReview: how many commits have taken place since the last review\\n\\nAhead: how many commits are in your local branch that are not in origin\\n\\nBehind: how many commits are in origin that are not in your local branch\\n\\nPull & Push: whether your branches need to be pushed or pulled to track origin\\n\\n\\ngreen checkbox: you don\\'t need to pull\\nred cross: you need to pull\\nquestion mark: you either don\\'t have a checked out copy of this branch or you need to prune your local tree\\n\\n\\n\\nModified: the last time that HEAD was modified (NOT the last time the review happened)\\n\\n\\n\\nInstallation\\nTo install from PyPi you should run one of the following commands. (If you use pip for your package installation, you should take a look!)\\npip install git-goggles\\n\\nor\\neasy_install git-goggles\\n\\nCheckout the project from github http://github.com/nowells/git-goggles\\ngit clone git://github.com/nowells/git-goggles.git\\n\\nRun setup.py as root\\ncd git-goggles\\nsudo python setup.py install\\n\\nDocumentation:\\nWith Sphinx docs deployment: in the docs/ directory, type:\\nmake html\\n\\nThen open docs/_build/index.html\\n\\nUsage\\nViewing the status of your branches:\\ngit goggles\\n\\nStarting your review process (shows an origin diff):\\ngit goggles codereview\\n\\nComplete your review process (automatically pushes up):\\ngit goggles codereview complete\\n\\n\\nConfiguration\\nYou can set a few configuration variables to alter to way git-goggles works out of the box.\\nDisable automatic fetching from all remote servers.\\ngit config --global gitgoggles.fetch false\\n\\nDisable colorized output\\ngit config --global gitgoggles.colors false\\n\\nAlter the symbols used to display success, failure, unknown states\\ngit config --global gitgoggles.icons.success \"OK\"\\ngit config --global gitgoggles.icons.failure \"FAIL\"\\ngit config --global gitgoggles.icons.unknown \"N/A\"\\n\\nAlter the colors of branch states. The available colors are [grey, red, green, yellow, blue, magenta, cyan, white]\\ngit config --global gitgoggles.colors.local cyan\\ngit config --global gitgoggles.colors.new red\\ngit config --global gitgoggles.colors.review red\\ngit config --global gitgoggles.colors.merge yellow\\ngit config --global gitgoggles.colors.done green\\n\\nAlter the width of branch column to turn on wordwrap.\\ngit config --global gitgoggles.table.branch-width 15\\n\\nAlter the table cell padding (defaults to 0)\\ngit config --global gitgoggles.table.left-padding 1\\ngit config --global gitgoggles.table.right-padding 1\\n\\nAlter the display of horizontal rule between rows of table (default false)\\ngit config --global gitgoggles.table.horizontal-rule true\\n\\n\\nInternals\\ngit-goggles works by creating and managing special tags called\\n\\'codereview-<branch_name>\\' and tracking them against HEAD.\\nThe first time a codereview is completed, the tag is created. Subsequent\\nreviews delete and re-create the tag so that it awlays accurately tracks HEAD.\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '132',\n",
       "  'forks': '13',\n",
       "  'commits': '110'},\n",
       " {'language': 'Python 97.8',\n",
       "  'readme': \"Exelixi\\nExelixi is a distributed framework based on Apache Mesos,\\nmostly implemented in Python using gevent for high-performance concurrency\\nIt is intended to run cluster computing jobs (partitioned batch jobs, which include some messaging) in pure Python.\\nBy default, it runs genetic algorithms at scale.\\nHowever, it can handle a broad range of other problem domains by\\nusing --uow command line option to override the UnitOfWorkFactory class definition.\\nPlease see the project wiki for more details,\\nincluding a tutorial\\non how to build Mesos-based frameworks.\\nQuick Start\\nTo check out the GA on a laptop (with Python 2.7 installed), simply run:\\n./src/ga.py\\n\\nOtherwise, to run at scale, the following steps will help you get Exelixi running on Apache Mesos.\\nFor help in general with command line options:\\n./src/exelixi.py -h\\n\\nThe following instructions are based on using the Elastic Mesos service,\\nwhich uses Ubuntu Linux servers running on Amazon AWS.\\nEven so, the basic outline of steps shown here apply in general.\\nFirst, launch an Apache Mesos cluster.\\nOnce you have confirmation that your cluster is running\\n(e.g., Elastic Mesos sends you an email messages with a list of masters and slaves)\\nthen use ssh to login on any of the masters:\\nssh -A -l ubuntu <master-public-ip>\\n\\nYou must install the Python bindings for Apache Mesos.\\nThe default version of Mesos changes in this code as there are updates to Elastic Mesos,\\nsince the tutorials are based on that service.\\nYou can check http://mesosphere.io/downloads/ for the latest.\\nIf you run Mesos in different environment,\\nsimply make a one-line change to the EGG environment variable in the bin/local_install.sh script.\\nAlso, you need to install the Exelixi source.\\nOn the Mesos master, download the master branch of the Exelixi code repo on GitHub and install the required libraries:\\nwget https://github.com/ceteri/exelixi/archive/master.zip ; \\\\\\nunzip master.zip ; \\\\\\ncd exelixi-master ; \\\\\\n./bin/local_install.sh\\n\\nIf you've customized the code by forking your own GitHub code repo, then substitute that download URL instead.\\nAlternatively, if you've customized by subclassing the uow.UnitOfWorkFactory default GA,\\nthen place that Python source file into the src/ subdirectory.\\nNext, run the installation command on the master, to set up each of the slaves:\\n./src/exelixi.py -n localhost:5050 | ./bin/install.sh\\n\\nNow launch the Framework, which in turn launches the worker services remotely on slave nodes.\\nIn the following case, it runs workers on two slave nodes:\\n./src/exelixi.py -m localhost:5050 -w 2\\n\\nOnce everything has been set up successfully, the log file in exelixi.log will show a line:\\nall worker services launched and init tasks completed\\n\\nFrom there, the GA runs.\\nSee a GitHub gist for an example of a successful run.\\nBlame List\\nPaco Nathan\\n\",\n",
       "  'watchers': '12',\n",
       "  'stars': '131',\n",
       "  'forks': '22',\n",
       "  'commits': '159'},\n",
       " {'language': 'Python 99.8',\n",
       "  'readme': \"notice\\nUnfortunatly I can no longer support this project. We moved to AppFollow it supports App Store, Google Play and Windows Phone Store.\\nInterstellar\\nSmall ruby app to get reviews for you Google Play Store-released application to the Slack channel.\\n\\nWhy do we need it\\nMonitoring Google Play reviews is a must for a responsible Android developer.\\nGoing every morning to the webpage, finding what's the last you've already answered does not sound like a 2015-thing.\\nUsers treat Google Play reviews as a way to seek for support.\\nRemember, you have only one hit. Only your first reply is being emailed to the user. Consecutive edits of your reply won’t be emailed to the user as the first one.\\nReplies help a lot in troubleshooting, especially given the range of different devices and OS versions on the market.\\nPlus, you want a good rating in the Google Play Store, right?\\nHow it works\\nGoogle Play exports all your app reviews once a day to the Google Cloud Storage bucket.\\nInterstellar downloads reviews via google-provided gsutil tool and triggers Slack incoming webhook for all new or updated reviews.\\nIt is intended to be fired once a day via cron.\\nConfiguration\\n\\ncreate a file secrets/secrets.yml. There is an example in secrets/secrets.yml.example.\\n\\nYou will need to provide:\\n\\nreport bucket id. Found in the Reviews page of Google Play Developer console, e.g. pubsite_prod_rev_01234567890987654321\\npackage name, eg com.example.reader, found in the Google Play Developer console\\nslack incoming webhook url, create new one via direct link once you've logged in to the slack\\n\\n\\n\\nconfigure gsutil. It’s a python app from Google, instructions provided below.\\n\\n\\ngem install rest-client\\n\\n\\nGsutil configuration\\n\\nRun gsutil/gsutil config and follow the steps.\\nOnce done, there will be a .boto file in your home dir.\\nCopy this file to the ./secrets folder and you are good to go.\\n\\nYou can always get the latest gsutil(https://cloud.google.com/storage/docs/gsutil_install) and change this line\\nsystem “BOTO_PATH=./secrets/.boto gsutil/gsutil cp -r gs://#{CONFIG[“app_repo”]}/reviews/#{csv_file_name} .”\\nto point it to whatever place you like. Keep in mind though, that the sender.rb script expects that the csv file is in the same folder.\\nUsage\\nOnce configured - run ruby sender.rb\\nLicense\\nThis piece of software is distributed under 2-clause BSD license.\\nWell, actually, you code it yourself during the coffee-break.\\n\",\n",
       "  'watchers': '17',\n",
       "  'stars': '131',\n",
       "  'forks': '30',\n",
       "  'commits': '28'},\n",
       " {'language': 'Python 70.4',\n",
       "  'readme': '#Sticky\\n\\n###D3 + IPython widgets\\nSticky is a framework for easily integrating IPython widgets with D3 libraries.\\nWhy\\nBecause the IPython widgets and comms give us a very powerful communication layer between Python and D3. This allows for dynamic data updating and the potential for other fun things, such as linked chart brushing where brush events trigger Python data filtering.\\nStatus\\nThe most alpha of alpha. There are no tests yet. d3plus charts have strange updating behavior.\\nExperimentation invited, production highly discouraged.\\nSo what can I do with this thing?\\nGo run the example notebook. Make some charts with the integrated libraries. Read the code. Add new libraries!\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '130',\n",
       "  'forks': '11',\n",
       "  'commits': '18'},\n",
       " {'language': 'Python 70.8',\n",
       "  'readme': '###豆瓣小组图片采集程序\\n#####By 肾虚公子\\n官网: http://Douban.miaowu.asia\\n主程序:dou2.py\\nMac/Liunx 运行程序(python dou2.py)即可。\\nWindows用户请下载压缩包: 下载\\n程序基本功能\\n\\n用户可以自由选择用户组下载图片支持豆瓣所有小组\\n本程序自动采集代理\\n采集代理后随机选择\\n自动下载图片并保存\\n\\n获取用户组ID方法\\nhttp://www.douban.com/group/264964/ #只需要输入人group/后面的字符 不包括斜杠\\n2015-6-3 更新\\n\\n无需用户手动创建文件夹\\n更新可用性去掉代理采集\\n优化Windows客户端大小以及文件数量\\n\\n2014-7-17 更新\\n\\n修改程序Bug\\n增加错误输出\\n加快程序执行效率\\n尝试启用多线程失败!\\n\\n2014-7-16 更新\\n\\n用户可以自由选择用户组下载图片支持豆瓣所有小组\\n增加默认功能\\n模拟UA\\n更换代理源\\nWin版支持\\n优化程序\\n美化程序\\n\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '130',\n",
       "  'forks': '37',\n",
       "  'commits': '42'},\n",
       " {'language': 'Python 97.6',\n",
       "  'readme': '###豆瓣小组图片采集程序\\n#####By 肾虚公子\\n官网: http://Douban.miaowu.asia\\n主程序:dou2.py\\nMac/Liunx 运行程序(python dou2.py)即可。\\nWindows用户请下载压缩包: 下载\\n程序基本功能\\n\\n用户可以自由选择用户组下载图片支持豆瓣所有小组\\n本程序自动采集代理\\n采集代理后随机选择\\n自动下载图片并保存\\n\\n获取用户组ID方法\\nhttp://www.douban.com/group/264964/ #只需要输入人group/后面的字符 不包括斜杠\\n2015-6-3 更新\\n\\n无需用户手动创建文件夹\\n更新可用性去掉代理采集\\n优化Windows客户端大小以及文件数量\\n\\n2014-7-17 更新\\n\\n修改程序Bug\\n增加错误输出\\n加快程序执行效率\\n尝试启用多线程失败!\\n\\n2014-7-16 更新\\n\\n用户可以自由选择用户组下载图片支持豆瓣所有小组\\n增加默认功能\\n模拟UA\\n更换代理源\\nWin版支持\\n优化程序\\n美化程序\\n\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '130',\n",
       "  'forks': '12',\n",
       "  'commits': '386'},\n",
       " {'language': 'Python 91.9',\n",
       "  'readme': '###豆瓣小组图片采集程序\\n#####By 肾虚公子\\n官网: http://Douban.miaowu.asia\\n主程序:dou2.py\\nMac/Liunx 运行程序(python dou2.py)即可。\\nWindows用户请下载压缩包: 下载\\n程序基本功能\\n\\n用户可以自由选择用户组下载图片支持豆瓣所有小组\\n本程序自动采集代理\\n采集代理后随机选择\\n自动下载图片并保存\\n\\n获取用户组ID方法\\nhttp://www.douban.com/group/264964/ #只需要输入人group/后面的字符 不包括斜杠\\n2015-6-3 更新\\n\\n无需用户手动创建文件夹\\n更新可用性去掉代理采集\\n优化Windows客户端大小以及文件数量\\n\\n2014-7-17 更新\\n\\n修改程序Bug\\n增加错误输出\\n加快程序执行效率\\n尝试启用多线程失败!\\n\\n2014-7-16 更新\\n\\n用户可以自由选择用户组下载图片支持豆瓣所有小组\\n增加默认功能\\n模拟UA\\n更换代理源\\nWin版支持\\n优化程序\\n美化程序\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '126',\n",
       "  'forks': '15',\n",
       "  'commits': '6'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Morphos\\n\\n\\n\\n\\n They say a pic is worth a 1000 words. Is it true to admit a .gif is worth a 1000 pics? \\n  \\nInclude in your project\\n In your root/build.gradle\\nallprojects {\\n  repositories {\\n  ...\\n  maven { url \\'https://jitpack.io\\' }\\n  }\\n}  \\n In your app/build.gradle\\ndependencies {\\n  compile \\'com.github.rjsvieira:morphos:1.0.0\\'\\n}\\nIntroducing Morphos : an animation wrapper.\\nMorphos will take care of your views\\' animations without you having to write all that boring boilerplate code.\\nInitialization\\nMorphos are easy to interact with. Go ahead and create the following simple Morpho :\\nView viewToAnimate = ... ;\\nMorpho morph = new Morpho(viewToAnimate)\\n  .translate(50,50,0,1500) // will translate the view (x=50,y=50,z=0) in 1500 milliseconds, \\n  .rotationXY(45,45,2000); // will rotate the view by 45 degrees on both the X-axis and Y-axis in 2000 milliseconds\\nYou can then animate it by doing\\nmorph.animate(); \\nWhich will then animate the desired Morphos using the default animation type (SEQUENTIAL).\\nWhat if I want to reverse the animation? Sure, just do :\\nmorph.reverse();\\n Configuration \\nCreate a Morpho\\nMorpho morphoOne = new Morphos(view);\\nConfigure the Morphos\\' animations\\nAs of the first release, Morphos supports the 7 most basic and common animations.\\nSince Morphos has plenty of combinations for interpolation, animation type, duration, etc, the user is allowed to configure them according to his needs.\\nEvery animation configuration method returns the Morphos object itself, thus allowing the user to chain his preferred animations.\\nNote : If the user does not specify the duration and/or interpolator, the animation will assume a 0 second duration and the default interpolator.\\nalpha(double value)\\nalpha(double value, int duration)\\nalpha(double value, int duration, Interpolator interpolator)\\n\\nscale(double valueX, double valueY)\\nscale(double valueX, double valueY, int duration)\\nscale(double valueX, double valueY, int duration, Interpolator interpolator)\\n\\ntranslationX(AnimationTarget target, float valueX)\\ntranslationX(AnimationTarget target, float valueX, int duration)\\ntranslationX(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\\n\\ntranslationY(AnimationTarget target, float valueX)\\ntranslationY(AnimationTarget target, float valueX, int duration)\\ntranslationY(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\\n\\ntranslationZ(AnimationTarget target, float valueX)\\ntranslationZ(AnimationTarget target, float valueX, int duration)\\ntranslationZ(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\\n\\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ)\\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ, int duration)\\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ, int duration, Interpolator interpolator)\\n\\ndimensions(float width, float height)\\ndimensions(float width, float height, int duration)\\ndimensions(float width, float height, int duration, Interpolator interpolator)\\n\\nrotationXY(AnimationTarget target, double degreesX, double degreesY)\\nrotationXY(AnimationTarget target, double degreesX, double degreesY, int duration)\\nrotationXY(AnimationTarget target, double degreesX, double degreesY, int duration, Interpolator interpolator)\\n\\nrotation(AnimationTarget target, double degrees)\\nrotation(AnimationTarget target, double degrees, int duration)\\nrotation(AnimationTarget target, double degrees, int duration, Interpolator interpolator)\\nSet a Listener to track the Morphos\\' animation process\\nmorphoOne.setListener(new Animator.AnimatorListener() {\\n  @Override\\n  public void onAnimationStart(Animator animator) {\\n      System.out.println(\"Start\");\\n  }\\n\\n  @Override\\n  public void onAnimationEnd(Animator animator) {\\n      System.out.println(\"End\");\\n  }\\n\\n  @Override\\n  public void onAnimationCancel(Animator animator) {\\n\\n  }\\n\\n  @Override\\n  public void onAnimationRepeat(Animator animator) {\\n\\n  }\\n})  \\n Animate() the Morpho\\nThis can be done through one of the several methods created just to make the invocation easier.\\nThe animate() method\\'s default values are : \\n\\nAnimationType : SEQUENTIAL - All animations are executed one sequentially\\nDuration : -1 - Since no duration was specified, -1 is assumed thus executing every animation in its given duration. For example : morphoOne.translate(100,0,0,3000).scale(2,2,2000).animate() will execute the translation animation in 3 seconds, followed by an upscale animation of 2 seconds\\nInterpolator - The overall interpolator (LinearInterpolator)\\nanimate()\\nanimate(AnimationType type, int duration)\\nanimate(AnimationType type, int duration, Interpolator interpolator)\\nReverse\\nAfter executing animate(), the user can rollback the animation by invoking the reverse() method.\\nThe reverse method works just like the animate() method, having the same combinations and parameters.\\nreverse()\\nreverse(AnimationType type, int duration)\\nreverse(AnimationType type, int duration, Interpolator interpolator)\\nCancel\\nIf the user wishes to cancel the on-going animation by any reason, he can do so by invoking the cancel method :\\nmorphoOne.cancel();\\nupdateView\\nLike the method explicitly indicates, the user can update the view associated with the Morpho. This will clear every animation already configured for the given object.\\nupdateView(View v) also invokes reset();\\nmorphoOne.updateView(newView);\\nReset\\nIf by any chance the user wants to reset he Morpho and re-build it from scratch, he can do so by invoking the reset() method\\nmorphoOne.reset()\\nDispose\\nLast but not least, if they user wishes to discard the Morphos object, he can invoke the dispose() method, thus clearing and preparing the Morphos\\' inner variables for garbage collection.\\nmorphoOne.dispose();\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '131',\n",
       "  'forks': '14',\n",
       "  'commits': '10'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'This is library project with a custom view that implements concept of Submit Button (https://dribbble.com/shots/1426764-Submit-Button?list=likes&offset=3) made by Colin Garven.\\n###Demo###\\n\\n###Usage###\\n <com.tuesda.submit.SubmitView\\n        android:layout_centerInParent=\"true\"\\n        android:id=\"@+id/submit\"\\n        android:layout_width=\"200dp\"\\n        android:layout_height=\"200dp\" />\\nmSubmit.setOnProgressStart(new SubmitView.OnProgressStart() {\\n            @Override\\n            public void progressStart() {\\n                // do something when progress start\\n            }\\n        });\\n        \\n        mSubmit.setOnProgressDone(new SubmitView.OnProgressDone() {\\n            @Override\\n            public void progressDone() {\\n                // do something when progress is done\\n            }\\n        });\\n###public interface###\\n\\n\\n\\n函数名\\n作用\\n\\n\\n\\n\\nsetBackColor(int color)\\n设置图标背景色，默认是绿色(0xff00cd97)，上图Demo设置为蓝色(0xff0097cd)\\n\\n\\nsetText(String str)\\n设置按钮名字，默认是Submit\\n\\n\\nreset()\\n将按钮重置到初始状态\\n\\n\\nsetProgress(float progress)\\n设置正在执行工作的执行进程\\n\\n\\nisProgressDone()\\n正在执行工作是否完成\\n\\n\\nsetOnProgressStart(OnProgressStart listener)\\n设置progress开始回调\\n\\n\\nsetOnProgressDone(OnProgressDone listener)\\n设置progress完成回调\\n\\n\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '130',\n",
       "  'forks': '21',\n",
       "  'commits': '7'},\n",
       " {'language': 'Java 88.2',\n",
       "  'readme': \"SugaredListAnimations\\nSugaredListAnimations is a library that pretends to add animations to your listview with minor changes to your already existing code.\\nCurrently the available animations are:\\n\\nGoogle Plus alike\\nGoogle Now alike\\n\\nVersion\\n0.9 - Yeah, it's still beta\\nDemo\\nSee demo here\\nLicense\\nMIT\\nFree Software, Fuck Yeah!\\n\",\n",
       "  'watchers': '14',\n",
       "  'stars': '124',\n",
       "  'forks': '42',\n",
       "  'commits': '2'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': \"SugaredListAnimations\\nSugaredListAnimations is a library that pretends to add animations to your listview with minor changes to your already existing code.\\nCurrently the available animations are:\\n\\nGoogle Plus alike\\nGoogle Now alike\\n\\nVersion\\n0.9 - Yeah, it's still beta\\nDemo\\nSee demo here\\nLicense\\nMIT\\nFree Software, Fuck Yeah!\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '123',\n",
       "  'forks': '10',\n",
       "  'commits': '31'},\n",
       " {'language': 'Java 45.1',\n",
       "  'readme': 'amap-running-app\\n\\nUse weex-amap to build a running app\\nHow to use\\n1.首先克隆这个项目(后面会写如何自己创建这样的项目). 确保你自己环境安装了weex-toolkit\\ngit clone https://github.com/weex-plugins/amap-running-app\\n2.进入克隆的项目目录，然后执行 npm install\\n3.测试你的需要运行的平台，比如android 或者 ios\\nweex platform add android\\n4.添加插件 weex-amap\\nweex plugin add weex-amap\\n这个时候你就可以运行命令看具体运行的效果了：\\nweex run android\\n如果你自己使用weex-toolkit创建项目，你只需要这样做：\\nweex create runningapp\\ncd runningapp && npm install\\nweex platform add android\\nweex plugin add weex-amap\\nweexpack run android\\n\\n运行demo截图\\niOS 版\\n\\nAndroid 版\\n\\n截图数据仅供演示\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '122',\n",
       "  'forks': '38',\n",
       "  'commits': '26'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'AndroidMVVM\\n\\nNote: issue the following in command line - gradlew build --refresh-dependencies in Windows or ./gradlew build --refresh-dependencies in Linux or Mac to force update to the latest RoboBinding snapshot when required.\\n\\n\\nA minimal Android app with MVVM pattern based on RoboBinding.\\n\\n\\nThe project can be directly imported into Android Studio.\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '120',\n",
       "  'forks': '41',\n",
       "  'commits': '17'},\n",
       " {'language': 'Java 79.9',\n",
       "  'readme': 'Quarks has been renamed to Apache Edgent (incubating)\\nApache Edgent is an open source programming model and runtime for edge devices that enables you to analyze data and events at the device.\\nGo to http://edgent.incubator.apache.org/ for information about Apache Edgent.\\nYou will find the new code repository where you can create pull requests at https://github.com/apache/incubator-edgent.\\nPlease joins us by subscribing to the developer mailing list dev at edgent.incubator.apache.org.\\nTo subscribe send an email to dev-subscribe at edgent.incubator.apache.org.\\n',\n",
       "  'watchers': '39',\n",
       "  'stars': '120',\n",
       "  'forks': '44',\n",
       "  'commits': '204'},\n",
       " {'language': 'Java 59.3',\n",
       "  'readme': 'Carat has moved! Please find the latest code at: https://github.com/carat-project/carat\\n',\n",
       "  'watchers': '24',\n",
       "  'stars': '117',\n",
       "  'forks': '40',\n",
       "  'commits': '1,152'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Simple RxJava library for observing and requesting Android runtime permissions introduced in Android 6.0.\\nObserving permissions\\nSometimes it makes sense to ask user for absolutely required permissions in a separate onboarding step. Simple code down below shows either main or onboarding fragment depending on permissions granted, without asking user for those permissions. This is pure observe case.\\npublic class MainActivity extends AppCompatActivity {\\n\\n    @Override public void onStart() {\\n        super.onStart();\\n        \\n        mSubscription = RxPermissions.get(this)\\n              .observe(Manifest.permission.WRITE_EXTERNAL_STORAGE,\\n                       Manifest.permission.READ_EXTERNAL_STORAGE)\\n              .subscribe(granted -> {\\n                  if (granted) {\\n                      // you can pass to main fragment\\n                  } else {\\n                      // you can open onboarding fragment \\n                  }\\n              });\\n    }\\n    \\n    @Override public void onStop() {\\n        mSubsrciption.unsubsribe();\\n        mSubsrciption = null;\\n        super.onStop();\\n    }\\n}\\n\\nNote: Returned observable does never complete, so be sure to unsubscribe properly.\\nRequesting permissions\\nRequesting permissions requires additional binding to component (an activity or a fragment) which can ask user for permissions and receives result. Then you can request permissions. Here is how you do it.\\npublic class OnboardingFragment extends Fragment {\\n    private static final int REQ_PERMISSIONS = 101;\\n\\n    private final PermissionsRequester mPermissionsRequester = new PermissionsRequester() {\\n        @Override public void performRequestPermissions(String[] permissions) {\\n            // forward request to the system by calling fragment\\'s method\\n            requestPermissions(permissions, REQ_PERMISSIONS);\\n        }\\n    };\\n    \\n    @Override\\n    public void onRequestPermissionsResult(int requestCode, String[] permissions, int[] grantResults) {\\n        if (requestCode == REQ_PERMISSIONS) {\\n            // forward response back to requester class for further processing\\n            mPermissionsRequester.onRequestPermissionsResult(permissions, grantResults);\\n        }\\n    }\\n    \\n    @Override public void onViewCreated(View view, Bundle savedInstanceState) {\\n        super.onViewCreated(view, savedInstanceState);\\n        \\n        mSubscription = RxPermissions.get(getActivity())\\n            .request(mPermissionsRequester, \\n                        Manifest.permission.WRITE_EXTERNAL_STORAGE,\\n                        Manifest.permission.READ_EXTERNAL_STORAGE)\\n            .subscribe(granted -> {\\n                if (granted) {\\n                    // permissions were granted, observable completes\\n                } else {\\n                    // user denied request\\n                }\\n            });\\n    }\\n    \\n    @Override public void onDestroyView() {\\n        mSubsrciption.unsubsribe();\\n        mSubsrciption = null;\\n        super.onDestroyView();\\n    }\\n    \\n    public void onButtonClicked(View view) {\\n        mPermissionsRequester.request();\\n    }\\n}\\nObservable returned by RxPermissions.request() method does only complete when all requested permissions are granted. Until then it emitts false every time user denies a permission request. You can show user a better explanation of why the app needs these permissions and ask for permissions again. Use PermissionsRequester.request() method for triggering permissions request dialog once again.\\nTests & stability\\nAlthough this library has unit tests covering whole functionality described above, there is still no productive apps using it yet.\\nCredits\\nThis library was inspired by https://github.com/tbruyelle/RxPermissions, but it uses a bit different design allowing pure observation of permissions and follow up permissions request without necessity to create new observable.\\nLicense\\nCopyright (c) 2015, 2016 Sergej Shafarenka, halfbit.de\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '115',\n",
       "  'forks': '24',\n",
       "  'commits': '13'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'JFreeChart - Future State Edition (FSE)\\nThis is a development version of JFreeChart that has been branched from the\\nJFreeChart 1.0.x series.  Here are some important notes:\\n\\n\\nthe API has been changed and will continue to change until I (and the other\\ncontributors) feel it is ready to be frozen - that may be in 6 months, or\\nit may be in 6 years;\\n\\n\\nMaven is used to build this edition of JFreeChart, and the default build\\nrequires JDK 1.8 (because we include JavaFX support in the build);\\n\\n\\nthere is an Ant script that will build the code without the JavaFX support\\nclasses.  This requires AT LEAST JDK 1.6.0 to compile and, depending\\non how I (and the other contributors) feel, in the future we may\\nrequire JDK 1.7 or even a later version;\\n\\n\\nJCommon is no longer a dependency (the subset of classes from JCommon that\\nare still used have been incorporated directly in the source tree);\\n\\n\\nThe aim is to modernise the JFreeChart API and code and have some fun with new\\nstuff and NOT to be constrained by backwards compatibility.\\nDavid Gilbert (david.gilbert@object-refinery.com)\\nJFreeChart Project Leader\\n\\n',\n",
       "  'watchers': '40',\n",
       "  'stars': '114',\n",
       "  'forks': '60',\n",
       "  'commits': '708'},\n",
       " {'language': 'C++ 72.4',\n",
       "  'readme': 'About in_constexpr\\nAn approach of detecting if inside a constexpr context in a constexpr function.\\nBy being able to detect if we\\'re within a constexpr context we can choose to implement\\na runtime specific algorithm while having a different algorithm for doing something at compile time.\\nThe approach is discussed in this post\\nFeatures\\n\\nif (in_constexpr()/in_runtime()) - Being able to detect if in runtime or compile time\\nsmart_assert                     - constexpr friendly assert\\n\\nConstraints\\n\\nOnly tested on GCC 5+ and Clang 3.8+\\nx86 only so far\\nRuntime approach only works on linux (replacing code in the binary should work under windows for now)\\nSee detailed descriptions about caveats here\\nThe library must be statically linked\\nThe initialization logic relies on being able to modify the .text segment of your program\\n\\nUsing the library\\nBuilding and installing\\nYou can compile and install this library using cmake. You will need to link this library in as a static library.\\nYou can customize and install the library using the following:\\ncmake <code directory> -DCMAKE_INSTALL_PREFIX=<install directory> && make -j && make install\\n\\nThen just link in as you would a normal library. You can try the examples in examples/ to see how to use the library.\\nUsing the API and an example\\nTo test if this library is working, you can call in_constexpr::is_setup() to see if it is working. The library should automatically call the setup by means of the constructor\\nattribute and so you shouldn\\'t need to explicitly call in_constexpr::initialize() but if it\\'s not happening automatically, you can call that function.\\nThe library provides an  in_constexpr() and in_runtime() macro/method that returns if a constexpr function is within which context. This can be used to provide different code paths in each case.\\nNote, you can\\'t do something like if (!in_constexpr())  due to those methods being a syntatic sugar. Use either if (in_constexpr()) or if (in_runtime()).\\nThe library also provides a smart_assert which you can use regular assert but stil work within constexpr functions.\\nYou can find more examples under the examples folder.\\nconst int N_MAX            = 30;\\nint factorial_cache[N_MAX] = {0};\\n\\nconstexpr int factorial(int n) {\\n  smart_assert(n >= 0 && n < N_MAX, \"N >= 0 && N <= N_MAX\");\\n\\n  if (n == 0)\\n    return 1;\\n  else {\\n    if (in_constexpr()) {\\n      return n * factorial(n - 1);\\n    } else {\\n      std::cout << \"Calling factorial \" << n << std::endl;\\n      // Since we\\'re in runtime, we can cache results.\\n      if (factorial_cache[n] == 0) {\\n        factorial_cache[n] = n * factorial(n - 1);\\n      }\\n      return factorial_cache[n];\\n    }\\n  }\\n}\\n\\nint main() {\\n  volatile int a = 5;\\n  volatile int b = 6;\\n  std::cout << factorial(a) << std::endl;\\n  std::cout << factorial(b) << std::endl;\\n\\n  constexpr int c = factorial(3);\\n  // Compiler error!\\n  // constexpr int d = factorial(-5);\\n\\n  std::cout << c << std::endl;\\n  // std::cout << factorial(d) << std::endl;\\n  return 0;\\n}\\nWill produce the following outputs\\nCalling factorial 5\\nCalling factorial 4\\nCalling factorial 3\\nCalling factorial 2\\nCalling factorial 1\\n120\\nCalling factorial 6\\nCalling factorial 5\\n720\\n6\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '81',\n",
       "  'forks': '1',\n",
       "  'commits': '24'},\n",
       " {'language': 'C++ 88.3',\n",
       "  'readme': 'This code is obsolete and the activity moved to https://github.com/idni/tau under a new design\\n',\n",
       "  'watchers': '41',\n",
       "  'stars': '81',\n",
       "  'forks': '10',\n",
       "  'commits': '1,484'},\n",
       " {'language': 'C++ 63.5',\n",
       "  'readme': 'This code is obsolete and the activity moved to https://github.com/idni/tau under a new design\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '81',\n",
       "  'forks': '16',\n",
       "  'commits': '9'},\n",
       " {'language': 'C++ 92.7',\n",
       "  'readme': \"text_extraction\\nThis code is the implementation of the method proposed in the paper “Multi-script text extraction from natural scenes” (Gomez & Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\\nIncludes the following third party code:\\n\\nfast_clustering.cpp Copyright (c) 2011 Daniel Müllner, under the BSD license. http://math.stanford.edu/~muellner/fastcluster.html\\nmser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http://www.idiap.ch/~cdubout/\\nbinomial coefficient approximations are due to Rafael Grompone von Gioi. http://www.ipol.im/pub/art/2012/gjmr-lsd/\\n\\n\",\n",
       "  'watchers': '15',\n",
       "  'stars': '80',\n",
       "  'forks': '56',\n",
       "  'commits': '20'},\n",
       " {'language': 'C++ 50.2',\n",
       "  'readme': \"text_extraction\\nThis code is the implementation of the method proposed in the paper “Multi-script text extraction from natural scenes” (Gomez & Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\\nIncludes the following third party code:\\n\\nfast_clustering.cpp Copyright (c) 2011 Daniel Müllner, under the BSD license. http://math.stanford.edu/~muellner/fastcluster.html\\nmser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http://www.idiap.ch/~cdubout/\\nbinomial coefficient approximations are due to Rafael Grompone von Gioi. http://www.ipol.im/pub/art/2012/gjmr-lsd/\\n\\n\",\n",
       "  'watchers': '13',\n",
       "  'stars': '80',\n",
       "  'forks': '81',\n",
       "  'commits': '89'},\n",
       " {'language': 'C++ 63.0',\n",
       "  'readme': 'FaceDetect\\n\\n\\n\\n\\n\\n\\n\\n\\nFace detection for iPhone using OpenCV\\nAbout\\nThis project is an example of using OpenCV on the iPhone.\\nIt provides a simple application that let you take a picture (or select one from your photo library), and that detects the face(s) on the picture.\\nBuilding\\nThe application does not compile (yet) for the iPhone simulator.\\nYou need to build it for your iOS device in order to test it.\\nOpenCV port\\nFaceDetect uses the OpenCV port provided by Macmade (myself), and which is also available on GitHub.\\nProject Status\\nThis project is no longer maintained.\\nIt might not build on latest iOS versions, and might not be compatible with latest OpenCV versions.\\nLicense\\nFaceDetect is released under the terms of the Boost Software License - Version 1.0.\\nRepository Infos\\nOwner:\\t\\t\\tJean-David Gadina - XS-Labs\\nWeb:\\t\\t\\twww.xs-labs.com\\nBlog:\\t\\t\\twww.noxeos.com\\nTwitter:\\t\\t@macmade\\nGitHub:\\t\\t\\tgithub.com/macmade\\nLinkedIn:\\t\\tch.linkedin.com/in/macmade/\\nStackOverflow:\\tstackoverflow.com/users/182676/macmade\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '80',\n",
       "  'forks': '23',\n",
       "  'commits': '7'},\n",
       " {'language': 'C++ 85.9',\n",
       "  'readme': ' _______  _______  _______  _______  _______  _______  _______  _______  _______ \\n|       ||   _   ||       ||       ||       ||   _   ||       ||       ||       |\\n|    _  ||  |_|  ||  _____||_     _||    _  ||  |_|  ||    ___||    ___||  _____|\\n|   |_| ||       || |_____   |   |  |   |_| ||       ||   | __ |   |___ | |_____ \\n|    ___||       ||_____  |  |   |  |    ___||       ||   ||  ||    ___||_____  |\\n|   |    |   _   | _____| |  |   |  |   |    |   _   ||   |_| ||   |___  _____| |\\n|___|    |__| |__||_______|  |___|  |___|    |__| |__||_______||_______||_______|\\nThe news homepage archive. At pastpages.org.\\nHow it works\\nPastPages is built using Python, Django and PostgreSQL. Screenshots are taken by PhantomJS via a Celery task queue. The files collected are stored in the Rackspace cloud. Deployment is managed with Chef and you can review how the environment is configured in its configuration files.',\n",
       "  'watchers': '8',\n",
       "  'stars': '78',\n",
       "  'forks': '10',\n",
       "  'commits': '422'},\n",
       " {'language': 'C++ 92.7',\n",
       "  'readme': ' _______  _______  _______  _______  _______  _______  _______  _______  _______ \\n|       ||   _   ||       ||       ||       ||   _   ||       ||       ||       |\\n|    _  ||  |_|  ||  _____||_     _||    _  ||  |_|  ||    ___||    ___||  _____|\\n|   |_| ||       || |_____   |   |  |   |_| ||       ||   | __ |   |___ | |_____ \\n|    ___||       ||_____  |  |   |  |    ___||       ||   ||  ||    ___||_____  |\\n|   |    |   _   | _____| |  |   |  |   |    |   _   ||   |_| ||   |___  _____| |\\n|___|    |__| |__||_______|  |___|  |___|    |__| |__||_______||_______||_______|\\nThe news homepage archive. At pastpages.org.\\nHow it works\\nPastPages is built using Python, Django and PostgreSQL. Screenshots are taken by PhantomJS via a Celery task queue. The files collected are stored in the Rackspace cloud. Deployment is managed with Chef and you can review how the environment is configured in its configuration files.',\n",
       "  'watchers': '3',\n",
       "  'stars': '78',\n",
       "  'forks': '7',\n",
       "  'commits': '8'},\n",
       " {'language': 'C++ 97.7',\n",
       "  'readme': \"The AndroidTransporterPlayer is a media player for the Raspberry Pi\\nDemo video\\n\\nTake a look at the Android Transporter blog post for more information.\\nSetup\\nsudo apt-get install git-core\\ncd /opt/vc/src/hello_pi/libs/ilclient\\nmake\\ncd /home/pi\\nmkdir Projects\\ncd Projects\\ngit clone https://github.com/esrlabs/AndroidTransporterPlayer.git\\ngit clone -b android-transporter https://github.com/esrlabs/Mindroid.cpp.git Mindroid\\ngit clone https://github.com/esrlabs/fdk-aac.git\\ncd Mindroid\\nmake -f Makefile.RPi\\nsudo cp libmindroid.so /usr/lib/\\ncd ..\\ncd fdk-aac\\nmake -f Makefile.RPi\\nsudo cp libaac.so /usr/lib/\\ncd ..\\ncd AndroidTransporterPlayer\\nmake -f Makefile.RPi\\n./AndroidTransporterPlayer rtsp://<IP-Address>:9000/test.sdp\\n\\nUsage\\n<IP-Address> is always the IP address of the VLC streaming server.\\nRaspberry Pi media player\\n/home/pi/AndroidTransporterPlayer/AndroidTransporterPlayer rtsp://<IP-Address>:9000/test.sdp\\nVLC streaming server\\nvlc <video>.mp4 --sout '#rtp{sdp=rtsp://<IP-Address>:9000/test.sdp}' --rtsp-timeout=-1\\n\",\n",
       "  'watchers': '21',\n",
       "  'stars': '77',\n",
       "  'forks': '23',\n",
       "  'commits': '160'},\n",
       " {'language': 'C++ 58.9',\n",
       "  'readme': 'APRIL-ANN\\n Travis CI (master branch)\\n Travis CI (devel branch)\\nUser documentation\\nFollow wiki for user documentation.\\nTutorials\\nTutorials are available at april-ann-tutorials\\nrepository.\\nContributions\\nContributions are wellcome. Only pull requests to devel branch will be\\naccepted, so avoid to create pull requests to master. A Travis CI instance\\nwill check that your request passes all tests. For Lua unit testing use the\\npackage basics/utest, and for C++ unit testing the package basics/gtest.\\nAt the end of this document there are information about Doxygen documentation\\nwhich can be useful for C/C++ developing. For Lua developing use the wiki.\\nIt is possible to make a pull request directly to master branch for bug or\\nmemory leak solving.\\nDependencies and basic setup\\nClone the repository and enter to it:\\n$ git clone https://github.com/april-org/april-ann.git\\n$ cd april-ann\\n\\nThe first time you want to run APRIL-ANN, you need to install\\ndependencies in Linux (via apt-get) and in MacOS X (via MacPorts/Homebrew).\\nThere is an automatic shellscript which covers majorly Ubuntu (other Debian\\nbased distributions can be supported) and MacOS X systems. You just need\\nto run:\\n$ ./DEPENDENCIES-INSTALLER.sh\\nRequirements, installed by previous script\\nRequires the following libraries. Versions are only orientative, it could work\\nwith older and newer versions whenver the API was compatible.\\n\\nGNU C++ compiler (g++): v 4.7.2\\nOnly in Linux systems: Lua 5.2 headers to tell APRIL-ANN the default system\\npath for Lua modules (lua5.2-deb-multiarch.h header).\\nBLAS implementation: ATLAS (v. 3), Intel MKL (v. 10.3.6), MacOS Accelerate Framework\\nThreads posix (pthread)\\nReadline (libreadline)\\nOpenMP\\nLAPACK library, offered by liblapack_atlas, mkl_lapack, or MacOS Accelerate Framework\\nLAPACKE library when compiling with ATLAS\\n\\nThe following libreries are recommended, but optional, you will need to remove\\nits package from the path profile_build_scripts/package_list.lua:\\n\\n[OPTIONAL] libpng: if you want to open PNG images, package libpng.\\n[OPTIONAL] libtiff: if you want to open TIFF images, package libtiff.\\n[OPTIONAL] libz: support for open of GZIPPED files, package gzio.\\n[OPTIONAL] libzip: support for open ZIP packages, package zip.\\n\\nFor perform computation on GPU, this optional library, which has an specific\\nmake target:\\n\\n[OPTIONAL] CUDA and CUBLAS: release 4.2.6.\\n\\nCompilation\\nFirst, it is mandatory to configure the repo PATH and other stuff.\\nThis step is needed every time you start a session, and before compilation:\\n$ . configure.sh\\nSecond, you can compile the APRIL-ANN version which you need.\\nWe have developed compiling files for using different libraries.\\nThe most simple compilation way is\\n$ make\\nand it would detect automatically your platform (Darwin or Linux) and your\\ntarget SUFIX (mkl, atlas, macports or homebrew). The automatic targets are:\\n\\nrelease automatic platform and sufix detection target (the same as make\\nwithout any argument).\\ndebug automatic platform and sufix detection target for debug version.\\ntest automatic platform and sufix detection target for test-debug version.\\n\\nThere are available more targets, just depending in your particular system\\ninstallation you can execute them by using $ make TARGET:\\n\\nrelease-mkl needs of MKL library installed at /opt/MKL as prefix.\\nrelease-atlas needs of OMP and ATLAS library.\\nrelease-no-omp needs ATLAS library.\\nrelease-cuda-mkl needs CUDA and MKL installed at /opt/MKL as prefix.\\nrelease-macports needs Mac OS X with MacPorts and Accelerate Framework.\\nrelease-homebrew needs Mac OS X with Homebrew and Accelerate Framework.\\n\\nBesides this targets, it is possible to compile for debug replacing release\\nstring with debug string, and for testing replacing release by\\ntest-debug.\\nThe makefile has the following variables which can be forced by the user:\\n\\nPREFIX indicates the prefix for libraries and binaries. In Linux it is\\n/usr, in Darwin it depends on MacPorts (/opt/local) or Homebrew\\n(/usr/local).\\nLIB indicates shared library install directory, by default it is\\n$(PREFIX)/lib\\nINCLUDE indicates header sources install directory, by default it is\\n$(PREFIX)/include/april-ann\\nLUALIB indicates where Lua library modules are installed, by default it is\\n$(PREFIX)/lib/lua/5.2\\nLUAMOD indicates where Lua code modules are installed, by default it is\\n$(PREFIX)/share/lua/5.2 and there will be created april_tools folder with\\na copy of the content of repository\\'s tools folder.\\nBIN indicates where you want to install binary files, by default it is\\n$(PREFIX)/bin\\n\\nEach of this targets will need a little configuration depending on your library\\ninstallation. For example, in order to compile with MKL, the file\\nprofile_build_scripts/build_mkl_release.lua contains the following sections\\n(among others):\\n  global_flags = {\\n    debug=\"no\",\\n    use_lstrip = \"no\",\\n    use_readline=\"yes\",\\n    optimization = \"yes\",\\n    platform = \"unix\",\\n    extra_flags={\\n      -- For Intel MKL :)\\n      \"-DUSE_MKL\",\\n      \"-I/opt/MKL/include\",\\n      --------------------\\n      \"-march=native\",\\n      \"-msse\",\\n      \"-DNDEBUG\",\\n    },\\n    extra_libs={\\n      \"-lpthread\",\\n      -- For Intel MKL :)\\n      \"-L/opt/MKL/lib\",\\n      \"-lmkl_intel_lp64\",\\n      \"-Wl,--start-group\",\\n      \"-lmkl_intel_thread\",\\n      \"-lmkl_core\",\\n      \"-Wl,--end-group\",\\n      \"-liomp5\"\\n    },\\n  },\\n\\nYou need to especify the -I option to the compiler, and all the extra_libs\\nstuff related with MKL.  Exists one build file for each possible target:\\nbuild_release.lua, build_debug.lua, build_mkl_release.lua,\\nbuild_mkl_debug.lua, ... and so on.\\nThe binary will be generated at bin/april-ann, which incorporates the Lua 5.2\\ninterpreter and works without any dependency in Lua. Besides, a shared library\\nand a Lua module will be generated at lib/libapril-ann.so and\\nlib/aprilann.so, so it is possible to use require from Lua to load APRIL-ANN\\nin a standard Lua 5.2 interpreter. In order to require aprilann module it is\\nrequired the installation of both libraries in their corresponding place in your\\nsystem. Normally this can be done executing $ sudo make install.\\nNOTE that loading april-ann as a Lua 5.2 module, you need to have the\\n.so library in the package.cpath or LUA_CPATH. It is possible to install it\\nin your system defaults following next section.\\nENJOY!\\nInstallation\\nThe installation is done executing:\\n$ sudo make install\\n\\nThis procedure copies the binary to system location in /usr (or in\\n/opt/local for Mac OS X via MacPorts). The shared library is copied\\nto Lua default directory, in order to load it by using require function.\\nIf you are using a non default installation (a custom one), please\\ncopy the .so files manually to your package.cpath or LUA_CPATH.\\nUse\\n\\nYou can execute the standalone binary:\\n\\n$ april-ann\\nAPRIL-ANN v0.2.1-beta COMMIT 920  Copyright (C) 2012-2013 DSIC-UPV, CEU-UCH\\nThis program comes with ABSOLUTELY NO WARRANTY; for details see LICENSE.txt.\\nThis is free software, and you are welcome to redistribute it\\nunder certain conditions; see LICENSE.txt for details.\\nLua 5.2.2  Copyright (C) 1994-2013 Lua.org, PUC-Rio\\n> print \"Hello World!\"\\nHello World!\\n\\n\\nIt is possible to use APRIL-ANN as a Lua module, loading only the packages\\nwhich you need (i.e. require(\"aprilann.matrix\")), or loading the full\\nlibrary (require(\"aprilann\")). Be careful, the APRIL-ANN modules doesn\\'t\\nfollow Lua guidelines and have lateral effects because of the declaration of\\ntables, functions, and other values at the GLOBALs Lua table. Before using\\nAPRIL-ANN as a Lua module you need to install it into your system (currently\\nonly available for Linux systems) by executing $ sudo make install.\\n\\n$ lua\\nLua 5.2.2  Copyright (C) 1994-2013 Lua.org, PUC-Rio\\n> require \"aprilann.matrix\"\\n> require \"aprilann\"\\nAPRIL-ANN v0.2.1-beta COMMIT 920  Copyright (C) 2012-2013 DSIC-UPV, CEU-UCH\\nThis program comes with ABSOLUTELY NO WARRANTY; for details see LICENSE.txt.\\nThis is free software, and you are welcome to redistribute it\\nunder certain conditions; see LICENSE.txt for details.\\n> print \"Hello World!\"\\nHello World!\\n\\nCitation\\nIf you are interested in use this software, please cite correctly the source. In academic publications\\nyou can use this bibitem:\\n@misc{aprilann,\\n  Author = {Francisco Zamora-Mart\\\\\\'inez and Salvador Espa{\\\\~n}a-Boquera and\\n\\t        Jorge Gorbe-Moya and Joan Pastor-Pellicer and Adri\\\\\\'an Palacios-Corella},\\n  Note = {{https://github.com/april-org/april-ann}},\\n  Title = {{APRIL-ANN toolkit, A Pattern Recognizer In Lua with Artificial Neural Networks}},\\n  Year = {2013}}\\nPublications\\nList of research papers which uses this tool:\\n\\n\\nJoan Pastor-Pellicer, Salvador España-Boquera, Francisco Zamora-Martínez,\\nM. Zeshan Afzal, M.J. Castro-Bleda.\\nInsights on the Use of Convolutional Neural Networks for Document Image Binarization,\\nIWANN, Advances in Computational Intelligence, pages 115-126, 2015.\\n\\n\\nJoan Pastor-Pellicer, Salvador España-Boquera, Francisco Zamora-Martínez,\\nMaría José Castro-Bleda.\\nHandwriting Normalization by Zone Estimation using HMM/ANNs,\\nICFHR, pages 633-638, 2014.\\n\\n\\nFrancisco Zamora-Martínez, Pablo Romeu, Paloma Botella-Rocamora, and Juan Pardo.\\nOn-line learning of indoor temperature forecasting models towards energy efficiency,\\nEnergy and Buildings, 83:162-172, 2014.\\n\\n\\nFrancisco Zamora-Martínez, Pablo Romeu, Paloma Botella-Rocamora, and Juan\\nPardo. Towards Energy Efficiency: Forecasting Indoor Temperature via Multivariate Analysis.\\nEnergies, 6(9):4639-4659, 2013.\\n\\n\\nPablo Romeu, Francisco Zamora-Martinez, Paloma Botella, and Juan Pardo.\\nTime-Series Forecasting of Indoor Temperature Using Pre-trained Deep Neural Networks.\\nIn ICANN, pages 451-458. 2013.\\n\\n\\nJoan Pastor-Pellicer, Francisco Zamora-Martinez, Salvador España-Boquera, and M.J. Castro-Bleda.\\nF-Measure as the error function to train Neural Networks.\\nIn Advances in Computational Intelligence, IWANN, part I, LNCS, pages 376-384. Springer, 2013.\\n\\n\\nF. Zamora-Martínez, Pablo Romeu, Juan Pardo, and Daniel Tormo.\\nSome empirical evaluations of a temperature forecasting module based on Artificial Neural Networks for a domotic home environment.\\nIn IC3K - KDIR, pages 206-211, 2012.\\n\\n\\nOur ancient ANN implementation in the former APRIL tookit was published here:\\n\\nS. España-Boquera, F. Zamora-Martinez, M.J. Castro-Bleda, J. Gorbe-Moya.\\nEfficient BP algorithms for general feedforward neural networks.\\nIn IWINAC, pages 327-336, 2007.\\n\\nOther projects using it\\nCompetition participations and other projects where APRIL-ANN has been used:\\n\\nKaggle American Epilepsy Society Seizure Prediction Challenge,\\nsystem ESAI-CEU-UCH positioned as 4th in the leaderboard.\\nDownload from GitHub the code\\nto run this system.\\n\\nPackages\\nAPRIL-ANN is compiled following a package system. In the directory packages you could find a\\ntree of directory entries. Leaves in the tree are directories which contain file \"package.lua\".\\nThe \"package.lua\" defines requirements, dependencies, libraries, and other stuff needed by the\\ncorresponding package.\\nEach package could contain this directories:\\n\\nc_src: source files (.h, .cc, .c, .cpp, .cu, and others).\\nbinding: binding files (.lua.cc), a kind of templatized file which generates the glue code between C/C++ and Lua.\\nlua_src: lua source files which define functions, tables, and pseudo-classes in Lua.\\ndoc: doxygen documentation additional files.\\ntest: examples and files for testing.\\n\\nAt root directory exists a file named \"package_list.lua\". It is a Lua table with the name of packages that\\nyou want to compile. If you don\\'t want or don\\'t have libpng, or libtiff, or other library, you could\\nerase the package name from this list to avoid its compilation.\\nIncludes these sources\\n\\nLua virtual machine 5.2.2: http://www.lua.org/\\nLuiz\\'s lstrip for Lua 5.1, adapted to compile with Lua 5.2.2: http://www.tecgraf.puc-rio.br/~lhf/ftp/lua/5.1/lstrip.tar.gz\\nMersenneTwister: http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html\\nMedian filter from Simon Perreault: http://nomis80.org/ctmf.html\\nRuningStat class for efficient and stable computation of mean and variance: http://www.johndcook.com/standard_deviation.html\\nMike Pall\\'s advanced readline patch for Lua: http://smbolton.com/lua.html#readline\\nGoogle C++ Testing Framework: https://code.google.com/p/googletest/\\n(Included, but not used) Lua autocompletion rlcompleter release 2, by rthomas:\\nhttps://github.com/rrthomas/lua-rlcompleter\\nLua base64 encoding developed by\\nErnie Ewert.\\n\\nWiki documentation\\n\\nPDF version\\nHTML one-page\\n\\nDoxygen documentation\\nThe documentation of the devel branch will be mantained as updated as possible\\nin the following links:\\n\\nC/C++ developer manual\\nC/C++ binding manual\\n\\nHowever, you can produce the Doxygen documentation of the branch where\\nyou are working by using the makefile\\'s document target. Please, note that\\nyou need to have installed Doxygen and\\nGraphviz.\\n$ make document\\n$ open doxygen_doc/developer/html/index.html\\n\\nThe last command can be substituted by you opening the indicated\\nlocation in your prefered web browser ;)\\nLINUX dependencies installation\\nExecute: $ ./DEPENDENCIES-INSTALLER.sh\\nIf your distribution is not supported (currently only Ubuntu has support), then\\ninstall g++, libatlas-dev, libreadline-dev, libpng-dev, libtiff-dev, libz-dev,\\nlibopenmp-dev, libzip-dev, liblua5.2-dev.\\nMAC OS X dependencies installation\\nVia MacPorts:\\n\\nInstall MacPorts\\nExecute $ ./DEPENDENCIES-INSTALLER.sh\\n\\nOr via HomeBrew:\\n\\nInstall Homebrew\\nExecute $ ./DEPENDENCIES-INSTALLER.sh\\n\\nBuilding new modules out of APRIL-ANN repository\\nFind the code base necessary for compilation of new modules at\\nAPRIL-ANN module example.\\nCurrently this option has been tested for Linux systems, despite it can be done\\nin MacOS X. So, for Linux systems, you need to install APRIL-ANN using the\\nfollowing commands (after you have downloaded or cloned the main repository):\\n$ ./DEPENDENCIES-INSTALLER.sh\\n$ make\\n$ sudo make install\\n\\nAfter that, you need to link your software using pkg-config to configure your\\ncompiler:\\n$ g++ -fPIC -shared -o YOUR_MODULE_NAME.so *.o $(pkg-config --cflags --libs april-ann)\\n\\nDon\\'t forget to require APRIL-ANN in your C++ code using the following\\ninstruction:\\nluaL_requiref(L, \"aprilann\", luaopen_aprilann, 1);\\nOnce you have done this steps, you can load your module into APRIL-ANN using Lua\\ninterpreter:\\n$ lua\\nLua 5.2.2  Copyright (C) 1994-2013 Lua.org, PUC-Rio\\n> your_module = require \"YOUR_MODULE_NAME\"\\nAPRIL-ANN v0.4.0  Copyright (C) 2012-2015 DSIC-UPV, CEU-UCH\\nCompiled at Sat Jul 18 13:45:52 2015, timestamp 1437219952\\nThis program comes with ABSOLUTELY NO WARRANTY; for details see LICENSE.txt.\\nThis is free software, and you are welcome to redistribute it\\nunder certain conditions; see LICENSE.txt for details.\\n\\nThe next C++ code is an example of file which can be loaded as external module\\nfor APRIL-ANN:\\n// includes all APRIL-ANN dependencies and declares luaopen_aprilann header\\n#include \"april-ann.h\"\\nusing AprilMath::MatrixExt::Initializers::matFill;\\nusing AprilUtils::LuaTable;\\nusing AprilUtils::SharedPtr;\\nusing Basics::MatrixFloat;\\n// exported function example\\nint get(lua_State *L) {\\n  SharedPtr<MatrixFloat> m = new MatrixFloat(2, 10, 20);\\n  matFill(m.get(), 20.0f);\\n  // using LuaTable you can push APRIL-ANN objects in Lua stack (be careful,\\n  // not all objects can be pushed)\\n  LuaTable::pushInto(L, m.get());\\n  return 1;\\n}\\n// declaration of module opening function\\nextern \"C\" {\\n  int luaopen_example(lua_State *L) {\\n    static const luaL_Reg funcs[] = {\\n      {\"get\", get},\\n      {NULL, NULL}\\n    };\\n    luaL_requiref(L, \"aprilann\", luaopen_aprilann, 1);\\n    lua_pop(L, 1);\\n    luaL_newlib(L, funcs);\\n    return 1;\\n  }\\n}\\nThe module can be loaded using a Lua 5.2 interpreter (for instance, the one\\ndeployed with APRIL-ANN, but not the april-ann executable command), as\\nindicated above.\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '75',\n",
       "  'forks': '12',\n",
       "  'commits': '3,035'},\n",
       " {'language': 'JavaScript 75.0',\n",
       "  'readme': \"Si Ping Pong\\nWe hacked our Ping Pong table! Read the blog post.\\nTechnology\\n\\nnode.js\\nsocket.io\\nReact\\ngulp.js\\nBrowserify\\n\\nBuilding\\nGulp is used to build the client. From the project root, run npm install and gulp to build.\\nThe default Gulp task will build and then start watching.\\nSounds\\nAudiosprite is used to build the sound sprite.\\nYou'll first need to install Audiosprite:\\nnpm install -g audiosprite\\nbrew install ffmpeg --with-theora --with-libogg --with-libvorbis\\n\\nThen run gulp sounds from the project root to rebuild the sprite. This will:\\n\\nFetch audio announcements for all players from Google's unofficial TTS API\\nFetch point announcements for scores 0–40 from Google's unofficial TTS API\\nInclude any .mp3 or .wav files in the ui/public/sounds directory\\nRebuild the JSON file that contains the audio data required to play the individual sounds\\n\\ngulp sounds depends on a DB connection in order to get the player list. You may need to specify the environment to use, for example:\\nNODE_ENV=development gulp sounds\\nRemember to rebuild the frontend after regenerating the sounds in order to include the updated sprite JSON in the Browserify build.\\nTodo\\n\\nGeneral restructuring and refactoring (v1 – new architecture, tidy events, move game logic client-side)\\nRather than mashing up audio, use full clips for each phrase for each player for more natural sounds (this should be automated)\\nAdd an easy method for plugging in events from custom hardware (adding players, recording points)\\nRemove dependency on global vars (game, for example)\\nAim for strict mode compliance\\nWhen an RFID tag that does not have an associated player is scanned, the ID should be emailed, posted to HipChat, etc. so that it can be easily added ... or just add an inactive player to the database?\\n\\nUI\\n\\nOn first load, the leaderboard does not fade in like the other stats\\n\\nCSS\\n\\nAdd Autoprefixer and strip vendor prefixes\\nMake variables for commonly used values\\n\\n\",\n",
       "  'watchers': '15',\n",
       "  'stars': '188',\n",
       "  'forks': '29',\n",
       "  'commits': '24'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Google Analytics\\nUNMAINTAINED: In case it wasn\\'t clear from the lack of activity, I don\\'t have the time to work on this project anymore.\\nI\\'d be happy to transfer ownership to someone else or add someone as a contributor to the project. Please reach out to me\\nand let me know!\\n\\n\\nThis project doesn\\'t work and hasn\\'t worked for some time due to google removing the use based configuration.\\n\\tSee more at https://github.com/ncb000gt/node-googleanalytics/issues/36#issuecomment-383822453\\n\\nPull data from Google Analytics for use in projects.\\nThe library maintains tracking of the token so that you don\\'t have to and will push the token around with your requests.\\nShould you require a different token, just create a new GA instance. However, this is asynchronous through eventing so if you do want the token you can latch onto the event.\\n\\nUpdated for NodeJS 0.6.x *\\n\\nUsage\\nWith a user and password:\\nvar GA = require(\\'googleanalytics\\'),\\n    util = require(\\'util\\'),\\n    config = {\\n        \"user\": \"myusername\",\\n        \"password\": \"mypassword\"\\n    },\\n    ga = new GA.GA(config);\\n\\nga.login(function(err, token) {\\n    var options = {\\n        \\'ids\\': \\'ga:<profileid>\\',\\n        \\'start-date\\': \\'2010-09-01\\',\\n        \\'end-date\\': \\'2010-09-30\\',\\n        \\'dimensions\\': \\'ga:pagePath\\',\\n        \\'metrics\\': \\'ga:pageviews\\',\\n        \\'sort\\': \\'-ga:pagePath\\'\\n    };\\n\\n    ga.get(options, function(err, entries) {\\n       util.debug(JSON.stringify(entries));\\n    });\\n});\\n\\nIf you have already gotten permission from a user, you can simply use the oAuth access token you have:\\nvar GA = require(\\'googleanalytics\\'),\\n    util = require(\\'util\\'),\\n    config = {\\n        \"token\": \"XXXXXXXXXXXX\"\\n    },\\n    ga = new GA.GA(config);\\n\\nvar options = {\\n    \\'ids\\': \\'ga:<profileid>\\',\\n    \\'start-date\\': \\'2010-09-01\\',\\n    \\'end-date\\': \\'2010-09-30\\',\\n    \\'dimensions\\': \\'ga:pagePath\\',\\n    \\'metrics\\': \\'ga:pageviews\\',\\n    \\'sort\\': \\'-ga:pagePath\\'\\n};\\n\\nga.get(options, function(err, entries) {\\n    util.debug(JSON.stringify(entries));\\n});\\n\\nYou can specify the type of token by setting \\'tokenType\\', default is \\'Bearer\\'.\\nSee node-gapitoken for easy service account Server to Server authorization flow.\\nAPI\\n\\nlogin([callback]) - The callback is optional. However, if it is given, it is added to the token event.\\nget(options, callback)\\n\\nEvent API\\n\\ntoken(err, token)\\nentries(err, entries)\\n\\nEntry API\\n\\nmetrics[]\\ndimensions[]\\n\\nEach array contains objects. These objects contain the following:\\n\\nname - The name of the metric or dimension requested\\nvalue - The value associated. If the value is a Number, it is parsed for you. Otherwise, it will be a string.\\n\\nContributors\\n\\nBrian Zeligson - Updates for a more recent version of node. Also makes use of better selectors.\\nMike Schierberl\\nGal Ben-Haim - Bug fixes for access token flow.\\nPatrick Nolan - Bug fixes for parsed_data not containing a rows field.\\nRyan Smith - Fixed GH-28.\\n\\nLicense\\nsee license file\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '187',\n",
       "  'forks': '42',\n",
       "  'commits': '85'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': '#Phonegap desktop\\nThe aim of this project is to allow developers to use PhoneGap in a desktop browser.\\nHave a look at the online demo (Best viewed in Chrome)\\nThis is a copy of the phonegap kitchen sink demo using my desktop library.\\nAll PhoneGap API calls are simulated and the data returned is determined by json files.\\nSee the Wiki for some notes on getting started.\\n###Current Status:\\nAdded section headings and links to API docs\\nAdded Cordova File API methods\\nMoved Connection API for v2.2 and up (previous can still be used)\\n###Next Steps:\\nUpdate features for recent API changes\\nLook at using new getUserMedia method for camera capture (latest release of Chrome, Opera)\\nLook at emulation (not simulation) of other features\\nInvestigate overriding Firefox geolocation\\n##License\\nPhoneGap Desktop is licensed under Apache v2.\\nA copy of the license is included in the project or you can view it at http://www.apache.org/licenses/LICENSE-2.0\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '186',\n",
       "  'forks': '41',\n",
       "  'commits': '45'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'toe.js\\ntoe.js is a tiny library based on jQuery to enable sophisticated gestures on touch devices.\\nWhy toe.js\\nThe goal of toe.js is\\n\\nSmooth integration into jQuery\\'s event handling\\nFast responding events to give the user a better experience\\nExtensible, hooks allow new touch events to be part of toe and use existing functionality\\nCustomizable through grunt. The build process allows you to remove not needed gestures\\nTiny overhead (1649 bytes gzipped by version 1.0)\\n\\nAvailable events (v. 2.0)\\n\\ntap\\ntaphold\\nswipe (all directions)\\ntransformstart, transform, transformend (scale and rotation)\\n\\nComing soon: fancy drag and drop\\nUsage\\nUse the default jQuery event binding to bind a toe.js event\\n$(\\'div.myElem\\').on(\\'tap\\', $.noop);\\n\\nMost of the events support multiple fingers. So if you want to find out the amount of fingers used by a gesture, just look into the TouchList of the original event.\\n$(\\'div.myElem\\').on(\\'tap\\', function (event) {\\n\\tvar original = event.originalEvent,\\n\\t\\ttouches = original.touches.length > 0 ? original.touches : original.changedTouches;\\n\\t\\n\\tif (touches.length === 2) {\\n\\t\\t// do something if the user tapped with two fingers\\n\\t}\\n});\\n\\nDefault eventing behavior will not be influenced by the default toe.js events. So in case you want to catch to a swipe event in a scrollable direction then you have to stop the the default behavior on your own.\\n$(\\'div.myElem\\').on(\\'touchstart touchmove touchend\\', function (event) {\\n\\tevent.preventDefault();\\n});\\n\\n$(\\'div.myElem\\').on(\\'swipe\\', function (event) {\\n\\t\\n});\\n\\nHow to extend toe.js? I\\'ll provide a simple template as soon as possible!\\nEvents\\ntap, doubletap, taphold\\nThe \"tap\" event is somehow similar to a click event with a pointer device so there is nothing more to say about it.\\n\"taphold\" is triggered if the user starts touching the target and keeps his finger on it for a certain amount of time (default: 500ms)\\nswipe\\nThe swipe event can be occur in any direction on the element.\\n$(\\'div.myElem\\').on(\\'swipe\\', function (event) {\\n\\tif(event.direction === \\'up\\') { // or right, down, left\\n\\t\\n\\t}\\n});\\n\\ntransformstart, transform, transformend\\nThis event is also known as pinch event. It allows the user to use two fingers moving away from or towards each other. The user will be able to signalize a scale or rotation of an object. All three events will deliver the center of the pinch, the rotation and the scale.\\n$(\\'div.myElem\\').on(\\'transform\\', function (event) {\\n\\tvar center = event.center, //center.pageX and center.pageY\\n\\t\\tscale = event.scale,\\n\\t\\trotation = event.rotation; //in deegres\\n\\t\\n\\t// do sth\\n\\t\\n});\\n\\nCustom build\\nToe.js is a modular library. In order you do not want to use all events just clone this repo and remove the unwanted gestures under src/gestures. The grunt script does the rest for you.\\nMore information\\n(blog entry)[http://damien.antipa.at/2013/03/24/toe-js-version-2-was-released/]\\nThanks\\nto the developers of all related libraries which inspired me: jQuery mobile, Hammer.JS, jGestures and TouchSwipe.\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '185',\n",
       "  'forks': '46',\n",
       "  'commits': '29'},\n",
       " {'language': 'JavaScript 49.3',\n",
       "  'readme': 'Check out the demo of Bootstrap file input at http://gregpike.net/demos/bootstrap-file-input/demo.html\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '184',\n",
       "  'forks': '62',\n",
       "  'commits': '41'},\n",
       " {'language': 'JavaScript 80.8',\n",
       "  'readme': 'Hotdot - Realtime webapp using Django + Orbited + Twisted\\n\\nWhat is Hotdot?\\nA very complete example of how to a create a\\nrealtime web application using Django + Orbited + Twisted.\\nRead more about the motivation behind Hotdot:\\nhttp://clemesha.org/blog/2009/dec/17/realtime-web-apps-python-django-orbited-twisted\\nCurrently the example is:\\nRealtime Voting Both: Collaborative Realtime Voting, Chatting, and Editing Polls.\\n\\nIs Hotdot a \\'Realtime Web Framework\\'?\\nNot currently. Maybe it should turn into one? UPDATE [12/27/2009]: I\\'ve started\\nwork on making Hotdot more \"framework\" like.  That is, I\\'m making it easier to \"plug in\"\\nyour own application logic, etc, by following a small set of conventions.  The work-in-progress\\ncode is in this branch: http://github.com/clemesha/hotdot/tree/framework\\nFor comparison, consider Tornado (http://www.tornadoweb.org),\\nwhich is the \\'realtime web framework\\' used to build FriendFeed.\\nTornado is minimal and clean, and does include some very nice\\nfeatures beyond just being a non-blocking webserver.\\nMy proposition with Hotdot is that the combination of\\nDjango, Orbited, and Twisted results in a more\\n\\'full-featured realtime web framework\\', with a greater\\ncommunity and total features, than can be found in Tornado.\\n\\nHow and Why?\\nThe combination of Django + Orbited + Twisted is everything\\nyou need to make a \\'real-world\\' realtime web application with Python.\\n\\nTheir roles:\\n\\nDjango: Excellent web framework for creating the backbone of a great web application.\\nOrbited: Realtime web (Comet) library to build the realtime components with.\\nTwisted: Scalable asynchronous network lib, for serving Orbited (and Django too, with WSGI!)\\n\\n\\nOther reasons for Hotdot:\\n\\nIncorporate core bits into http://codenode.org (http://github.com/codenode/codenode) to make it realtime.\\nMy personal education on this awesome topic.\\n\\n\\n\\n\\nInstall\\n\\nRecommended: Use a virtualenv and install with pip, to get them type:\\neasy_install virtualenv pip\\n\\n\\nCreate a fresh virtualenv:\\nvirtualenv --no-site-packages hotdot_env\\n\\n\\nInstall dependencies into your virtualenv:\\n#You must have Django 1.0+ and Twisted 9.0+\\n\\npip -E hotdot_env install -U django orbited twisted simplejson\\n\\n\\nMove into your virtualenv and activate it:\\n$ cd hotdot_env\\n$ source bin/activate\\n\\n\\nGet a copy of Hotdot from here:\\ncurl http://github.com/clemesha/hotdot/tarball/master\\n\\n#Or clone a copy:\\n\\n$ git clone git://github.com/clemesha/hotdot.git\\n\\n\\n\\n\\nUsage\\n\\nIn the directory hotdot/djangoweb, type:\\ndjango-admin.py syncdb --pythonpath=\\'.\\' --settings=\\'settings\\'\\n\\n\\nIn the toplevel directory hotdot, type:\\ntwistd -ny server.py\\n\\n\\n\\n\\nNow open browser to http://localhost:8000/\\nAlso see settings in server.py and djangoweb/settings.py\\nTo change host interface, see server.py->INTERFACE and djangoweb/settings.py->INTERFACE.\\n\\n\\nTests\\n\\nIn the directory hotdot/djangoweb, type:\\ndjango-admin.py test --pythonpath=\\'.\\' --settings=\\'settings\\'\\n\\n\\n\\n\\nDetails of how Hotdot works\\n(WORK IN PROGRESS)\\n\\nOrbited as a Twisted Service\\nDjango running from twisted.web.wsgi\\nAuthentication using Twisted Cred+Django models\\nFiltering + modification + logging of in-transit Orbited messages\\nSTOMP as the default, example protocol.\\n\\n\\nWhy the name Hotdot?\\n\\'Hot\\' as in the latest goodness.\\n\\'Dot\\' as in _D_jango + _O_rbited + _T_wisted.\\n\\nLicense, Questions, Contact\\nHotdot is licensed under the BSD.\\nPlease fork a copy here!: http://github.com/clemesha/hotdot\\nContact: Alex Clemesha <alex@clemesha.org> | http://twitter.com/clemesha\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '184',\n",
       "  'forks': '15',\n",
       "  'commits': '36'},\n",
       " {'language': 'JavaScript 99.0',\n",
       "  'readme': \"--> Demos, Examples, Playground, Docu\\n--> d3-react-squared-c3-loader\\n--> New live example\\n--> New blog post, based on live example\\nNotes\\n--> v0.6.0 and later require d3 v4!\\n--> v0.3.0 and later require React 0.14!\\nc3\\nDocumentation is still missing, sorry!\\nv0.3.6 and newer\\nStarting in 0.3.6, c3 charts are loaded using d3-react-squared-c3-loader\\nv0.2.7 through v0.3.5:\\nPlease note that this is still 'beta'. So far, there is no docu on the docu page.\\n--> please check out the c3example.js in the source (./examples).\\nd3-react-squared\\n\\nFeedback, ideas, PRs, etc. very welcome!\\nWhy yet another d3-react component?\\nThere are already some great solutions out there, combining React and D3, e.g.:\\nA gist with some links here\\nMost of these articles/code aims to combine/add d3 into the lifecycle methods to\\ngenerate charts that way. Have a look at them, great ideas there.\\nSee docu page for some details about my approach. I don't want to bore you with details here -\\njust contact us (contacts on docu page). I am very happy to discuss ideas/concepts!\\nSome keywords:\\n\\nUse D3 charts 'directly', maybe very limited adjustments needed (just think examples!)\\nProvide viewboxes etc. to get responsive graphs\\nMake chart modular (a.k.a. reusable)\\nProvide a clean API to create and update charts (from ANY component!).\\nParametrize charts\\nBe lightweight\\nProvide a way to share events between charts (and using a wrapper: any component!)\\nProvide access to a charts library (we currently offer c3js, as of v0.2.7)\\nProvide a limited set of examples in this repo and make it easy to the users to add their own custom charts\\n\\nWe believe that especially the last bullet is helpful to teams separate concerns and have maintainable solutions.\\nWhy? The chart generating code is in its own module and the interaction designer doesn't really have to care about React (maybe he should, but that's another story...).\\nDetails?\\nSee also here\\n(click on DR2 in top right navigation!)\\nDocumentation\\n--> See here\\n(click on DR2 in top right navigation!)\\nThe documentation is still somewhat basic. Definitely check out the examples in the repo!\\nBut hey, writing docu is sooooo time consuming...\\nStand-alone example\\nThis repo now includes a stand-alone example. Simply:\\nnpm install\\n\\nand then\\nnpm run dev\\n\\nand it should be running on localhost:8080.\\nRequirements\\nAs far as I know, you shouldn't need anything fancy.\\nWe run it in a babel/webpack/react setup, plain vanilla, so to speak (plenty of setup guides out there),\\nand it works.\\nAlso: we have bootstrap, no other css/sass/... (actually: we love react-bootstrap)\\n(Note: you could, if you wanted, to use SASS to style your graphs, must require the files where and when needed; you know how.).\\nThanks\\nHuge thanks to all the people involved in providing awesome tools such as:\\n\\nReactJS\\nD3\\nwebpack\\nBabelJS\\nReflux (no longer using it, thanks anyway!)\\nredux (replaces Reflux)\\nc3.js\\n\\nand many others...\\nSome screenshots\\nNew Docu-Page\\n\\nPlayground (--> See here) to learn about parameters:\\n\\n\",\n",
       "  'watchers': '18',\n",
       "  'stars': '183',\n",
       "  'forks': '13',\n",
       "  'commits': '219'},\n",
       " {'language': 'JavaScript 82.9',\n",
       "  'readme': \"Terminus\\nTerminus is an experimental\\nCapybara driver for real browsers. It\\nlets you control your application in any browser on any device (including\\nPhantomJS), without needing browser plugins. This\\nallows several types of testing to be automated:\\n\\nCross-browser testing\\nHeadless testing\\nMulti-browser interaction e.g. messaging apps\\nTesting on remote machines, phones, iPads etc\\n\\nSince it is experimental, this project is sporadically maintained. Usage is\\nentirely at your own risk.\\nInstallation\\n$ gem install terminus\\n\\nRunning the example\\nInstall the dependencies and boot the Terminus server, then open\\nhttp://localhost:70004/ in your browser.\\n$ bundle install\\n$ bundle exec bin/terminus\\n\\nWith your browser open, start an IRB session and begin controlling the app:\\n$ irb -r ./example/app\\n>> extend Capybara::DSL\\n>> visit '/'\\n>> click_link 'Sign up!'\\n>> fill_in 'Username', :with => 'jcoglan'\\n>> fill_in 'Password', :with => 'hello'\\n>> choose 'Web scale'\\n>> click_button 'Go!'\\n\\nLicense\\n(The MIT License)\\nCopyright (c) 2010-2013 James Coglan\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the 'Software'), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\\nof the Software, and to permit persons to whom the Software is furnished to do\\nso, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\",\n",
       "  'watchers': '1',\n",
       "  'stars': '183',\n",
       "  'forks': '5',\n",
       "  'commits': '361'},\n",
       " {'language': 'JavaScript 72.5',\n",
       "  'readme': 'Really Simple Color Picker\\nThis is a very minimal, yet robust Color Picker based on jQuery.\\nFor more details check the introductory blog post - http://laktek.com/2008/10/27/really-simple-color-picker-in-jquery/\\nUsage\\nYou can either clone this repo or download the latest build as a zip from here - http://github.com/laktek/really-simple-color-picker/zipball/master\\nColor Picker requires jQuery 1.2.6 or higher. Make sure to load it before Color Picker (there\\'s no other dependencies!).\\nFor default styles of the color picker load the CSS file that comes with the plugin.\\n  <script language=\"javascript\" type=\"text/javascript\" src=jquery.min.js\"></script>\\n  <script language=\"javascript\" type=\"text/javascript\" src=\"jquery.colorPicker.min.js\"/></script>\\n\\n  <link rel=\"stylesheet\" href=\"colorPicker.css\" type=\"text/css\" />\\nAdd a text field to take the color input.\\n  <div><label for=\"color1\">Color 1</label> <input id=\"color1\" type=\"text\" name=\"color1\" value=\"#333399\" /></div>\\nThen call \\'colorPicker\\' method on the text field when document loads.\\n  <script language=\"javascript\">\\n    jQuery(document).ready(function($) {\\n      $(\\'#color1\\').colorPicker();\\n    }\\n  </script>\\nOptions\\nThere are several options you can set at the time of binding.\\nSelected color\\nColor Picker will use the value of the input field, which the picker is attached to as the selected color. If not, it will use the color passed with pickerDefault property.\\n  $(\\'#color1\\').colorPicker({pickerDefault: \"ffffff\"});\\nColor Palette\\nOverrides the default color palette by passing an array of color values.\\n  $(\\'#color1\\').colorPicker({colors: [\"333333\", \"111111\"]});\\nTransparency\\nEnable transparency value as an option.\\n  $(\\'#color1\\').colorPicker({transparency: true});\\nColor Change Callback\\nRegisters a callback that can be used to notify the calling code of a color change.\\n  $(\\'#color1\\').colorPicker( { onColorChange : function(id, newValue) { console.log(\"ID: \" + id + \" has been changed to \" + newValue); } } );\\nIf you want to set an option gloablly (to apply for all color pickers), use:\\n  $.fn.colorPicker.defaults.colors = [\\'151337\\', \\'111111\\']\\nDefault text on picker field\\nYou can set some text to show on the picker field. For example, you could show a user\\'s initials.\\n  <input id=\"color4\" type=\"text\" name=\"color4\" value=\"#FF0000\" data-text=\"AG\" />\\n  $(\\'#color4\\').colorPicker();\\nDemo\\nDemo can be found at http://laktek.github.com/really-simple-color-picker/demo.html\\nReal-world Examples\\n\\nCurdBee\\nReadability\\n\\nLet us know how you are using Really Simple Color Picker...\\nContributors\\n\\nLakshan Perera - http://laktek.com\\nDaniel Lacy  - http://daniellacy.com\\n\\nIssues & Suggestions\\nPlease report any bugs or feature requests here:\\nhttps://github.com/laktek/really-simple-color-picker/issues\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '180',\n",
       "  'forks': '66',\n",
       "  'commits': '72'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Installation and usage\\nVia bower:\\nbower install jquery.serialScroll\\nVia npm:\\nnpm install jquery.serialscroll\\nUsing a public CDN\\nCDN provided by jsdelivr\\n<script src=\"//cdn.jsdelivr.net/npm/jquery.serialscroll@1.3.0/jquery.serialScroll.min.js\"></script>\\nDownloading Manually\\nIf you want the latest stable version, get the latest release from the releases page.\\njQuery.scrollTo\\nThis plugin requires jQuery.scrollTo.\\nIn order to use jQuery.scrollTo 2.0 you need to update jQuery.localScroll to 1.3.0 and above.\\nNotes\\n\\n\\nThe hash of settings is passed in to jQuery.scrollTo, so, in addition to jQuery.localScroll\\'s settings, you can use any of jQuery.scrollTo\\'s. Check that plugin\\'s documentation for further information.\\n\\n\\nMost of this plugin\\'s defaults, belong to jQuery.scrollTo, check it\\'s demo for an example of each option.\\n\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '180',\n",
       "  'forks': '53',\n",
       "  'commits': '28'},\n",
       " {'language': 'Python 73.3',\n",
       "  'readme': '\\nDjango API Playground\\nA django app that creates api explorer for RESTful APIs.\\nWorks with any RESTful API. For example, you can create api explorer for your tastypie based API with this app.\\nDemo: http://api-playground-demo.hipo.biz\\n\\n\\nInstructions\\nTo get this application up and running, please follow the steps below:\\nInstall from pip:\\npip install django-api-playground\\n\\nOr from source:\\npip install git+git://github.com/Hipo/Django-API-Playground.git\\n\\nAdd to installed apps:\\nINSTALLED_APPS =(\\n    # ...\\n\\n    \\'apiplayground\\',\\n)\\n\\nCreate database tables:\\n./manage.py syncdb\\n\\nInstallation is completed. You can define the API schema now.\\nFirst step, Create an url:\\n# urls.py\\n\\nfrom api.playgrounds import ExampleAPIPlayground\\n\\nurlpatterns = patterns(\\'\\',\\n    (r\\'api-explorer/\\', include(ExampleAPIPlayground().urls)),\\n)\\n\\nSecond step, Define a subclass for your API:\\n# api/playgrounds.py\\n\\nfrom apiplayground import APIPlayground\\n\\nclass ExampleAPIPlayground(APIPlayground):\\n\\n    schema = {\\n        \"title\": \"API Playground\",\\n        \"base_url\": \"http://localhost/api/\",\\n        \"resources\": [\\n            {\\n                \"name\": \"/feedbacks\",\\n                \"description\": \"This resource allows you to manage feedbacks.\",\\n                \"endpoints\": [\\n                    {\\n                        \"method\": \"GET\",\\n                        \"url\": \"/api/feedbacks/{feedback-id}\",\\n                        \"description\": \"Returns a specific feedback item\",\\n                        \"parameters\": [{\\n                            \"name\": \"order_by\",\\n                            \"type\": \"select\",\\n                            \"choices\": [[\"\", \"None\"], [\"id\", \"id\"], [\"-id\", \"-id\"]],\\n                            \"default\": \"id\"\\n                        }]\\n                    },\\n                    {\\n                        \"method\": \"POST\",\\n                        \"url\": \"/api/feedbacks/\",\\n                        \"description\": \"Creates new feedback item\",\\n                        \"parameters\": [{\\n                            \"name\": \"title\",\\n                            \"type\": \"string\"\\n                        },\\n                        {\\n                            \"name\": \"resource\",\\n                            \"type\": \"string\"\\n                        },\\n                        {\\n                           \"name\": \"description\",\\n                           \"type\": \"string\"\\n                        }]\\n                    }\\n                ]\\n            },\\n        ]\\n    }\\n\\nThat\\'s all. More detailed documentation will be coming soon.\\n\\nSpecial Thanks\\n\\n\\nBerker Peksag (for such a beautiful project name suggestion)\\n\\n\\n',\n",
       "  'watchers': '50',\n",
       "  'stars': '182',\n",
       "  'forks': '21',\n",
       "  'commits': '65'},\n",
       " {'language': 'Python 85.7',\n",
       "  'readme': 'face-search\\nFace search engine\\nDownload demo dataset (Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\\nLabeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.\\nUniversity of Massachusetts, Amherst, Technical Report 07-49, October, 2007.) or/and add your own images\\n./download_images.sh\\n\\nGenerate db:\\ncd code\\n./create_db.sh\\n\\nSearch by face:\\n# csv output\\n./run.sh URL 0\\n# image output\\n./run.sh URL 1\\n\\nQuery:\\n\\nResults:\\n\\nrequirements:\\nsudo apt-get install imagemagick\\nsudo pip install awscli\\nsudo pip install click\\n\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '182',\n",
       "  'forks': '48',\n",
       "  'commits': '10'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'face-search\\nFace search engine\\nDownload demo dataset (Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\\nLabeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.\\nUniversity of Massachusetts, Amherst, Technical Report 07-49, October, 2007.) or/and add your own images\\n./download_images.sh\\n\\nGenerate db:\\ncd code\\n./create_db.sh\\n\\nSearch by face:\\n# csv output\\n./run.sh URL 0\\n# image output\\n./run.sh URL 1\\n\\nQuery:\\n\\nResults:\\n\\nrequirements:\\nsudo apt-get install imagemagick\\nsudo pip install awscli\\nsudo pip install click\\n\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '181',\n",
       "  'forks': '64',\n",
       "  'commits': '7'},\n",
       " {'language': 'Python 78.0',\n",
       "  'readme': 'Overmind\\nThis project aims to provider a complete server provisioning and configuration management application.\\nThe first version is a unified front-end to public and private clouds, custom server providers and dedicated hardware.\\nFeatures\\n\\nEC2 and Rackspace server provisioning. All clouds supported by libcloud will be supported given enough testing\\nProvider Plugins: Any provider can be integrated by writing either a libcloud driver or an Overmind provisioning plugin\\nImport any server into Overmind witht the \"Dedicated Hardware\" plugin\\nComplete REST API for provider and nodes\\nAuthentication with three user roles\\n\\nSee the wiki for architectural info.\\nInstallation\\nRequirements\\n\\nPython 2.6+\\nDjango 1.3\\napache-libcloud\\ndjango-celery\\nRabbitMQ (or alternative message queue supported by Celery)\\n\\nAll python dependencies can be installed using the requirements file:\\n$ pip install -r requirements.txt\\n\\nInstall Overmind\\n\\n\\nDownload the last stable release from\\nhttp://github.com/tobami/overmind/downloads\\nand unpack it\\n\\n\\nCreate the DB by changing to the overmind/ directory and running:\\n  python manage.py syncdb\\n\\n\\n\\nFor testing purposes start the celery server on a console\\n  python manage.py celeryd -l info\\n\\nand the django development server\\n  python manage.py runserver\\n\\n\\n\\nNow you can visit the Overmind overview page on localhost:8000/overview\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '178',\n",
       "  'forks': '15',\n",
       "  'commits': '171'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'CNN-RelationExtraction\\nConvolution neural network for relation classification between two given entities\\nThe CNN architecture implemented is inspired be Nguyen et al. 2015 in which for each:\\n\\nReferences:\\n\\n\\nNguyen, Thien Huu, and Ralph Grishman. \"Relation Extraction: Perspective from Convolutional Neural Networks.\"\\n\\n\\nKim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\\n\\n\\nZeng, D., Liu, K., Lai, S., Zhou, G. and Zhao, J., 2014, August. Relation classification via convolutional deep neural network. In Proceedings of COLING (pp. 2335-2344).\\nVancouver\\n\\n\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '175',\n",
       "  'forks': '63',\n",
       "  'commits': '105'},\n",
       " {'language': 'Python 81.3',\n",
       "  'readme': 'CNN-RelationExtraction\\nConvolution neural network for relation classification between two given entities\\nThe CNN architecture implemented is inspired be Nguyen et al. 2015 in which for each:\\n\\nReferences:\\n\\n\\nNguyen, Thien Huu, and Ralph Grishman. \"Relation Extraction: Perspective from Convolutional Neural Networks.\"\\n\\n\\nKim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\\n\\n\\nZeng, D., Liu, K., Lai, S., Zhou, G. and Zhao, J., 2014, August. Relation classification via convolutional deep neural network. In Proceedings of COLING (pp. 2335-2344).\\nVancouver\\n\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '175',\n",
       "  'forks': '11',\n",
       "  'commits': '692'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Annotator Store\\nThis is a backend store for Annotator.\\nThe functionality can roughly be separated in two parts:\\n\\nAn abstraction layer wrapping Elasticsearch, to easily manage annotation\\nstorage. It features authorization to filter search results according to\\ntheir permission settings.\\nA Flask blueprint for a web server that exposes an HTTP API to the annotation\\nstorage. To use this functionality, build this package with the [flask]\\noption.\\n\\n\\nGetting going\\nYou'll need a recent version of Python (Python 2 >=2.6\\nor Python 3 >=3.3) and ElasticSearch (>=1.0.0)\\ninstalled.\\nThe quickest way to get going requires the pip and virtualenv\\ntools (easy_install virtualenv will get them both). Run the\\nfollowing in the repository root:\\nvirtualenv pyenv\\nsource pyenv/bin/activate\\npip install -e .[flask]\\ncp annotator.cfg.example annotator.cfg\\npython run.py\\n\\nYou should see something like:\\n* Running on http://127.0.0.1:5000/\\n* Restarting with reloader...\\n\\nIf you wish to customize the configuration of the Annotator Store, make\\nyour changes to annotator.cfg or dive into run.py.\\nAdditionally, the HOST and PORT environment variables override\\nthe default socket binding of address 127.0.0.1 and port 5000.\\n\\nStore API\\nThe Store API is designed to be compatible with the\\nAnnotator. The annotation store, a\\nJSON-speaking REST API, will be mounted at /api by default. See the\\nAnnotator\\ndocumentation for\\ndetails.\\n\\nRunning tests\\nWe use nosetests to run tests. You can just\\npip install -e .[testing], ensure ElasticSearch is running, and\\nthen:\\n$ nosetests\\n......................................................................................\\n----------------------------------------------------------------------\\nRan 86 tests in 19.171s\\n\\nOK\\n\\nAlternatively (and preferably), you should install\\nTox, and then run tox. This will run\\nthe tests against multiple versions of Python (if you have them\\ninstalled).\\nPlease open an issue\\nif you find that the tests don't all pass on your machine, making sure to include\\nthe output of pip freeze.\\n\",\n",
       "  'watchers': '26',\n",
       "  'stars': '173',\n",
       "  'forks': '68',\n",
       "  'commits': '485'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"chainer-char-rnn\\nkarpathy's char-rnn implementation by Chainer\\nRequirement\\n\\nChainer\\n\\n$ pip install chainer\\n\\nTrain\\nStart training the model using train.py, for example\\n$ python train.py\\n\\nThe --data_dir flag specifies the dataset to use. By default it is set to data/tinyshakespeare which consists of a subset of works of Shakespeare.\\nYour own data: If you'd like to use your own data create a single file input.txt and place it into a folder in data/. For example, data/some_folder/input.txt.\\nSampling\\nGiven a checkpoint file (such as those written to cv) we can generate new text. For example:\\n$ python sample.py \\\\\\n--vocabulary data/tinyshakespeare/vocab.bin \\\\\\n--model cv/some_checkpoint.chainermodel \\\\\\n--primetext some_text --gpu -1\\n\\nReferences\\n\\nOriginal implementation: https://github.com/karpathy/char-rnn\\nBlog post: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\\n\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '168',\n",
       "  'forks': '64',\n",
       "  'commits': '33'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'sublime-laravelgenerator\\nA Sublime Text plugin that allows you to make use of the Laravel 4\\nGenerators by Jeffrey\\nWay directly within Sublime Text.\\nInstallation\\n\\nInstall the  Laravel 4\\ngenerator commands through Composer.\\nInstall the ST plugin through Package Control: Sublime Laravel Generator\\nIf you are on Windows or php executable is not in PATH, please specify the path to it in laravelgenerator.sublime-settings. To do so, copy laravelgenerator.sublime-settings from this\\nplugin to <Packages_Directory>/Users/ and make the edits to that file.\\n\\nUsage\\n\\nOpen a Laravel Project\\nOpen the command palette (Ctrl+Shift+P)\\nExecute any of the available Generate commands\\nSee here for a basic workflow video\\n\\nNote: artisan needs to be in the project root.\\nCustomization\\nThe plugin is quite extensible. Interested users can extend the plugin for more\\nartisan commands by adding the appropriate entries in\\nDefault.sublime-commands.\\nCredits\\n\\nJeffrey Way: for the idea and testing this\\nplugin throughout the development.\\n\\n\\nThis is a work in progress. Feedback is appreciated. Feel free to report any\\nissues you come across\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '167',\n",
       "  'forks': '44',\n",
       "  'commits': '31'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'django-websocket (ABANDONED, do not use)\\nTHIS PROJECT IS ABANDONED! Please use django-channels to implement\\nwebsockets with Django on the server.\\nFor legacy reasons, you can access the old documentation in the README of\\nversion 0.3.0.\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '164',\n",
       "  'forks': '32',\n",
       "  'commits': '45'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Oh snap!\\nToolbarMenudrawer 2.0 is here :D\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '155',\n",
       "  'forks': '29',\n",
       "  'commits': '30'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': ' PixlUI\\nProvide few methods for visual elements.\\n\\n\\n\\n\\n\\n\\n\\nCheckBox:\\n\\nCustom font\\nText All caps (works on all API version)\\n\\nButton:\\n\\nCustom font\\nText All caps (works on all API version)\\n\\nEditText:\\n\\nCustom font\\nCopy/Cut/Paste (enable/disable) (works on all API version)\\nCancel clipboard content (works on all API version)\\nText All caps (works on all API version) - (in progress)\\nFocus listener\\nBatch listener (replace TextWatcher, wich that you can intercept DEL touch on all API)\\n\\nAutoCompleteEditText:\\n\\nCustom font\\nCopy/Cut/Paste (enable/disable) (works on all API version)\\nCancel clipboard content (works on all API version)\\nText All caps (works on all API version) - (in progress)\\nFocus listener\\nBatch listener (replace TextWatcher, wich that you can intercept DEL touch on all API)\\n\\nImage View:\\n\\nAlpha  (works on all API version)\\n\\nRelativeLayout:\\n\\nAlpha  (works on all API version)\\n\\nTextView:\\n\\nContains a fix to do proper ellipsizing\\nCustom font\\nText All caps (works on all API version)\\n\\nScreenshot\\n\\nGradle Setup\\nCompile with one line easy code!\\nrepositories {\\n    maven {\\n        url \"https://jitpack.io\"\\n    }\\n}\\n\\nCompile in the build.gradle file. for X.X.X please refer to the change log.\\ndependencies{\\n  compile \\'com.github.neopixl:PixlUI:vX.X.X.\\'\\n}\\nMaven Setup\\nAdd Repository\\n<repository>\\n\\t    <id>jitpack.io</id>\\n\\t    <url>https://jitpack.io</url>\\n\\t</repository>\\n\\nAdd Dependency:\\n\\t<dependency>\\n\\t    <groupId>com.github.neopixl</groupId>\\n\\t    <artifactId>PixlUI</artifactId>\\n\\t    <version>v1.0.5</version>\\n\\t</dependency>\\nHow use it ?\\n\\n\\nAdd your custom fonts in /assets/fonts/\\n\\n\\nDefine your fonts in styles.xml\\n\\n\\n    <style name=\"AppTheme.TextGearedSlab\">\\n        <item name=\"typeface\">GearedSlab.ttf</item>\\n    </style>\\n\\n\\n    <style name=\"AppTheme.TextGearedSlab.t1\">\\n        <item name=\"android:textSize\">12sp</item>\\n    </style>\\n\\n\\n    <style name=\"AppTheme.TextGearedSlab.t2\">\\n        <item name=\"android:textSize\">14sp</item>\\n    </style>\\n\\nUse it in XML:\\n\\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:pixlui=\"http://schemas.android.com/apk/com.neopixl.pixlui\"\\n    xmlns:tools=\"http://schemas.android.com/tools\" >\\n\\n    <np.TextView\\n        style=\"@style/AppTheme.TextGearedSlab.t1\"\\n        android:id=\"@+id/textView1\"\\n        android:layout_width=\"wrap_content\"\\n        android:layout_height=\"wrap_content\"\\n        android:text=\"@string/hello_world\"\\n        pixlui:copyandpaste=\"false\"\\n        pixlui:clearclipboardcontent=\"true\"/>\\n</RelativeLayout>\\n ChangeLog\\n1.1.2\\n\\ncloses #31 (Memory Leak: Activity Leak on PixlUIfaceManager.INSTANCE)\\nUpdate buildTools (25.0.3) and compileSdkVersion / targetSdkVersion = 25.\\n\\n1.1\\n\\nAndroid studio 1.5.1 Support\\nAndroid Layout Preview support (on API >=23 preview)\\nCheckedTextView added\\nChronometer added\\nExtractEditText added\\nSwitch added\\nRefactor\\nFixed some bugs\\nFind easy accessor to our components:\\n- Before: <com.neopixl.pixlui.components.textview.TextView\\n- Now: <np.TextView\\n\\n1.0.6\\n\\nUpgraded to last build tools\\nEllipsizingTextView is now removed from com.neopixl.pixlui.components.textview\\n\\n1.0.5a (Prerelease)\\n\\nTemp release to fix jitpack.io build\\ninclude a gradle\\'ized versin of PixlUI (now a library project)\\n\\n1.0.5\\n\\nAdded custom RadioButton (Custom font, Text all caps)\\n\\n1.0.4\\n\\nAdded RelativeLayoutAnimator (VISIBLE/GONE/INVISIBLE transition animation)\\nAdded LinearLayoutAnimator (VISIBLE/GONE/INVISIBLE transition animation)\\nAdded custom AutoCompleteEditText\\nAdded custom AutoResizeTextView (in progress)\\n\\n1.0.3\\n\\nAdded custom CheckBox (Custom font, Text all caps)\\n\\n1.0.2\\n\\nAdded method in custom EditText (Autofocus Listener, Hide/Show Keyboard)\\n\\n1.0.1\\n\\nAdded method in custom TextView (Text all caps) - for old api version\\nAdded method in custom EditText (Text all caps) - for old api version\\nAdded method in custom Button (Text all caps) - for old api version\\nFix NPE in Batch Listener\\nAdded custom RelativeLayout (Alpha) - for old api version\\n\\n1.0.0\\n\\nAdded custom TextView (Custom font)\\nAdded custom EditText (Custom font, Focus Listener, Batch Listener)\\nAdded custom Button (Custom font)\\nFix many crash\\n\\n Application using PixlUI\\n\\n \\n\\n\\n\\n\\n\\n[Flow] (https://play.google.com/store/apps/details?id=com.metalab.flow)\\t\\t-\\nW-Zup  \\t-       FLASHiZ      -       MeeTincS      -       Wort.lu     -\\nHypebeast     -       iBeezi-       RootCoinExplorer\\nDonation\\n\\nCopyright\\nCopyright 2014-2016 Neopixl - Olivier Demolliens\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this\\n\\nfile except in compliance with the License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under\\n\\nthe License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF \\n\\nANY KIND, either express or implied. See the License for the specific language governing\\n\\npermissions and limitations under the License.\\n\\n',\n",
       "  'watchers': '16',\n",
       "  'stars': '153',\n",
       "  'forks': '50',\n",
       "  'commits': '116'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'FlickableView\\n\\nFlickable ImageView for Android. It\\'s like a view of twitter\\'s detail image.\\nIt\\'s possible that other views animate with FlickableView.\\nFeature\\n\\nMove (Up, Down)\\nZoom\\nFlick (Up, Down)\\n\\nHow to use\\n final FlickableImageView flickableImageView = (FlickableImageView) findViewById(R.id.fiv);\\n // Resource\\n flickableImageView.setImageResource(R.drawable.travel);\\n\\n // Http Request\\n // String url = \"...\";\\n // Picasso.with(context).load(url).into(flickableImageView);\\n \\n \\n // Listeners\\n \\n // Flick Listener\\n flickableImageView.setOnFlickListener(new FlickableImageView.OnFlickableImageViewFlickListener() {\\n     @Override\\n     public void onStartFlick() {\\n     }\\n     \\n     @Override\\n     public void onFinishFlick() {\\n     }\\n });\\n \\n // Drag Listener\\n flickableImageView.setOnDraggingListener(new FlickableImageView.OnFlickableImageViewDraggingListener() {\\n     @Override\\n     public void onStartDrag() {\\n     }\\n     \\n     @Override\\n     public void onCancelDrag() {\\n     }\\n });\\n \\n // SingleTap Listener\\n flickableImageView.setOnSingleTapListener(new FlickableImageView.OnFlickableImageViewSingleTapListener() {\\n     @Override\\n     public void onSingleTapConfirmed() {\\n     }\\n });\\n \\n // DoubleTap Listener\\n flickableImageView.setOnDoubleTapListener(new FlickableImageView.OnFlickableImageViewDoubleTapListener() {\\n     @Override\\n     public void onDoubleTap() {\\n     }\\n });\\n \\n // Zoom Listener\\n flickableImageView.setOnZoomListener(new FlickableImageView.OnFlickableImageViewZoomListener() {\\n     @Override\\n     public void onStartZoom() {\\n     }\\n     \\n     @Override\\n     public void onBackFromMinScale() {\\n     }\\n });\\n \\nCheck this sample code.\\nGradle\\nrepositories {\\n    jcenter()\\n}\\n\\ndependencies {\\n    compile \\'com.github.goka.flickableview:flickableview:1.0.0\\'\\n}\\nRelease\\n1.0.0\\n\\u3000First release.\\nReference\\nImageViewZoom\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '152',\n",
       "  'forks': '21',\n",
       "  'commits': '17'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Android Lifecycle [Deprecated]\\n\\nA \\'compatibility\\' version of the ActivityLifecycleCallbacks APIs (http://developer.android.com/reference/android/app/Application.ActivityLifecycleCallbacks.html)\\nthat were introduced in Android 4 (API Level 14) and adding similar mechanism for Fragment.\\nWhy & When you need this\\nThe reason google introduced ActivityLifecycleCallbacks APIs in Android 4 is try to simplify and modularize the code which need to \"inject\" to Activity\\'s lifecycle.\\nFor instance the Google Analytics service requires call a specific method in onStart and onStop of all activities, another good example is ViewServer enable developer inspect UI hierarchy in an un-rooted device.\\nFurther more, you might expect those code could be plug-in or out in your building, and this library can make it easier for you.\\nMore info about ActivityLifecycleCallbacks APIs, please check android documents.\\nSince fragment play more and more important role in android UI development(One Activity + fragments), you would need similar tools to simplify your fragment development.\\nHow to use\\n\\nYou can grab the jar from Maven Central Repository and put it to your libs dictionary\\nMaven\\n\\n<dependency>\\n  <groupId>com.cocosw</groupId>\\n  <artifactId>lifecycle</artifactId>\\n  <version>0.1</version>\\n</dependency>\\n\\nGradle\\n\\ncompile \\'com.cocosw:lifecycle:0.1\\'\\nAPI\\n\\n\\nHave all your activities extend one of the base activities in the com.cocosw.lifecycle.app package.\\n\\n\\nCreate your activity/fragment lifecycle callbacks class extend from ActivityLifecycleCallbacksCompat and FragmentLifecycleCallbacks\\n\\n\\nCall LifecycleDispatcher.registerActivityLifecycleCallbacks(this, callback) and/or LifecycleDispatcher.registerFragmentLifecycleCallbacks(this, callback).\\n\\n\\nFor Android 4.0\\n\\nThis library will use build-in activity lifecycle mechanism for API >14 target platform.\\nIf you decide to drop 2.x support and move to android activity lifecycle API, you could done that by modify few lines, because the API is basically the same as the official one.\\n\\nActionBarSherlock or AppCompact\\nIf you use a library that already requires your activities to extend a base class, you can simply create your own base activity.\\nLicence\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '150',\n",
       "  'forks': '42',\n",
       "  'commits': '8'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '[DEPRECATED] RxAssertions\\n\\n\\nThis project is no longer maintened / active. Thanks for all OSS people that contributed and/or provided feedback !!!\\n\\nRxAssertions is a simple idea for better RxJava assertions.\\nI found the original idea from the guys of Ribot : in fact, I think this a good idea and helps to keep tests clean.\\nHowever, Ribot guys deprecated their original repo some time ago in favor of vanilla TestSubscriber, so I decided to take my own shot on this.\\nThis library mimics and improves the original Ribot`s idea with the following goodies :\\n\\nAssertJ powered assertions for RxJava (as the original one)\\nAll tests rely on BlockingObservable internally\\nInternal API rely 100% on TestSubscriber\\nImproved public API, covering most of TestSubscriber provided assertions\\nImproved Assertions entry points from factory methods, with Observable, BlockingObservable, Single and Completable support\\n\\nLets see some code diet :\\nRegular assertions with TestSubscriber\\nTestSubscriber<String> testSubscriber = new TestSubscriber<>();\\nObservable.just(\"RxJava\", \"Assertions\").toBlocking().subscribe(testSubscriber);\\ntestSubscriber.assertCompleted();\\ntestSubscriber.assertNoErrors();\\ntestSubscriber.assertValues(\"RxJava\", \"Assertions\");\\nAssertions with RxAssertions\\nRxAssertions.assertThat(Observable.just(\"RxJava\", \"Assertions\"))\\n\\t\\t.completes()\\n\\t\\t.withoutErrors()\\n\\t\\t.expectedValues(\"RxJava\", \"Assertions\");\\nor\\nassertThat(Observable.empty())\\n\\t\\t.emitsNothing()\\n\\t\\t.completes()\\n\\t\\t.withoutErrors();\\nor\\nSingle<String> single = Single.fromCallable(() -> \"RxJava\");\\nassertThat(single).completes().expectedSingleValue(\"RxJava\");\\nYou can find other examples at test folder\\nSetup\\nAdd it in your build.gradle\\nrepositories {\\n\\t...\\n\\tmaven { url \"https://jitpack.io\" }\\n\\n}\\nAdd the dependency\\ndependencies {\\n\\t...\\n\\ttestCompile \\'com.github.ubiratansoares:rxassertions:$version\\'\\n}\\nCheck the releases tab for the latest version.\\nRxAssertions uses RxJava 1.1.9 and AssertJ 2.5.0 as dependencies.\\nExperimental\\nSince v0.3.0, we have some assertions leveraging on AssertJ Conditions API. This kind of assertion offers flexible matching both for emissions and error checks. You can find samples at test folder.\\nContributing\\nPRs are wellcome. 🚀\\nCredits\\n\\nRibot guys for the original idea\\nRxJava and AssertJ guys for these awesome libraries\\n\\nLicense\\nCopyright (C) 2016 Ubiratan Soares\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '148',\n",
       "  'forks': '11',\n",
       "  'commits': '32'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Android Design Support Sample\\nA sample that use Android Design Support Library.\\nNew Widgets\\n\\nNavigationView\\nTabLayout\\nCoordinatorLayout\\nAppBarLayout\\nCollapsingToolbarLayout\\nFloatingActionButton\\nSnackbar\\nNestedScrollView\\nTextInputLayout\\n\\nRequired Dependencies\\n\\nappcompat-v7:22.2.0\\nrecyclerview-v7:22.2.0\\n\\nSample Download\\n\\nScreenshots\\n\\n\\n\\n\\n\\nLicense\\nCopyright 2015 Eric Liu\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '147',\n",
       "  'forks': '47',\n",
       "  'commits': '17'},\n",
       " {'language': 'Java 81.7',\n",
       "  'readme': 'play2-crud\\n\\nPowerful CRUD & DAO implementation with REST interface for play framework 2.x\\nFor the Typesafe Activator check play2-crud-activator.\\nSome screenshots\\n\\n\\nindex page\\n\\n\\n\\ncreate page\\n\\n\\n\\nlist page\\n\\n\\n\\nQuick Start\\nFollow these steps to use play2-crud. You can also use it partially just for DAO or CRUD controllers. If you think any part needs further explanation, please report a new issue.\\nAdd play2-crud dependency\\nYou can begin with adding play2-crud dependency inside conf/Build.scala file.\\n\\nAdd app dependency:\\n\\n    val appDependencies = Seq(\\n        javaCore, javaJdbc, javaEbean,\\n        \"play2-crud\" % \"play2-crud_2.10\" % \"0.7.0\"\\n    )\\n\\n\\n\\n\\nDependency version is for version 0.7.0 defined, but you can use the latest version.\\n\\n\\nAdd custom maven repositories:\\n\\n\\n    val main = play.Project(appName, appVersion, appDependencies).settings(\\n        //maven repository\\n        resolvers += \"release repository\" at  \"http://hakandilek.github.com/maven-repo/releases/\",\\n        resolvers += \"snapshot repository\" at \"http://hakandilek.github.com/maven-repo/snapshots/\"\\n    )\\n\\n\\nAssociate Global settings\\nDirect reference\\nIf you don\\'t want to override the play application launcher, you just have to notice to play that the class to use as launcher is now GlobalCRUDSettings. Change the application.global configuration key in the conf/application.conf file, and use play.utils.crud.GlobalCRUDSettings:\\n...\\napplication.global=play.utils.crud.GlobalCRUDSettings\\n...\\n\\n\\nDefine routes\\n# CRUD Controllers\\n->     /app             play.crud.Routes\\n\\n# REST API\\n->     /api             play.rest.Routes\\n\\n\\nDefine model\\n\\nModel class has to implement play.utils.dao.BasicModel with the type parameter indicating the type of the @Id field.\\n\\n@Entity\\npublic class Sample extends Model implements BasicModel<Long> {\\n\\n   @Id\\n   private Long key;\\n\\n   @Basic\\n   @Required\\n   private String name;\\n\\n   public Long getKey() {\\n      return key;\\n   }\\n\\n   public void setKey(Long key) {\\n      this.key = key;\\n   }\\n\\n   public String getName() {\\n      return name;\\n   }\\n\\n   public void setName(String name) {\\n      this.name = name;\\n   }\\n}\\n\\nHere the Sample model class implements BasicModel<Long> where key field indicated with @Id is Long.\\n\\n... call http://localhost:9000/app and voila!\\nSamples\\n\\nSample with basic dynamic CRUD controllers\\nSample with custom views is a full featured sample.\\nFull featured sample with DAO and DAOListeners\\nSample with Cache usage\\n\\nHOW-TO\\nHere you can find some HOW-TO documents introducing some powerful functionality:\\n\\nHOW-TO use simple CRUD\\nHOW-TO define a custom DAO\\nHOW-TO define a custom Controller\\nHOW-TO use DAO Listeners\\nHOW-TO use dynamic REST Controllers\\nHOW-TO use custom REST Controllers\\nHOW-TO Override Play Launcher\\n\\n',\n",
       "  'watchers': '24',\n",
       "  'stars': '147',\n",
       "  'forks': '52',\n",
       "  'commits': '159'},\n",
       " {'language': 'Java 77.8',\n",
       "  'readme': '##About\\nA classic password visualization concept, ported to Android\\n\\n\\nChroma-Hash is a concept for visualizing secure text input using ambient color bars\\nPassword entry can be frustrating, especially with long or difficult passwords. Secure fields obscure your input with •\\'s, so others can\\'t read it. Unfortunately, neither can you—you can\\'t tell if you got your password right until you tap \"Log In\".\\nChroma-Hash displays a series of colored bars at the end of field inputs so you can instantly see if your password is right. Chroma-Hash takes an MD5 hash of your input and uses that to compute the colors in the visualization. The resulting color pattern is non-reversible, so no one could know what your password just from the colors.\\nSee the original web version for a live demonstration, and a bit more explanation.\\n\\n##Usage\\nChromaHashView is a drop in replacement for an EditText for password input.\\nRight now you need to import the library to your project, but it will be uploaded to Maven Central shortly.\\n##License\\nCopyright 2014 Michael Evans\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '144',\n",
       "  'forks': '6',\n",
       "  'commits': '10'},\n",
       " {'language': 'Java 99.8',\n",
       "  'readme': 'IceFig   \\nJava elegant supplement\\nJava 8 delivered lambda expressions, but without the enhancement of basic libraries like List, Map, String, which makes\\nlambda expression still not delightful.\\nInspired by other popular languages like Ruby and Scala, IceFig intends to supply the missing.\\nQuick Scan\\nElegant alternative to List: Seq\\nSeq<Integer> seq = Seqs.newSeq(1,2,3);\\nseq.shuffle(); // copy to a new seq and shuffle it\\nseq.forEach((value, idx) -> { // with index\\n    // (1, 0)  (2, 1)  (3, 2)\\n});\\nseq.forEachCons(2, (values)->{\\n    // [1,2]  [2, 3]\\n});\\n\\nseq.join(\"-\"); //\"1-2-3\"\\n\\nseq.map(a -> a+ 1).distinct().reverse().join()\\nElegant alternative to Map: Hash\\nHash<Integer, Integer> hash = Hashes.<Integer, Integer>newHash().put(1, 2).put(2, 3).put(3, 3);\\nhash.containsAny((k, v) -> k+v == 5 ); //true\\nhash.keysOf(3); // [2, 3]\\nElegant alternative to String: CharSeq\\nCharSeq str = CharSeq.of(\"a b c d e f g\");\\nstr.split(\" \").join(\"-\").capitalize(); //\"A-b-c-d-e-f-g\"\\nstr.partition(\"d e\").map(CharSeq::trim);  //[\"a b c\", \"d e\", \"f g\"]\\nFull Javadoc\\nInclude it\\n<dependency>\\n    <groupId>com.worksap</groupId>\\n    <artifactId>icefig</artifactId>\\n    <version>[latest version]</version>\\n</dependency>\\nConcept\\nNot stream\\nIceFig is different from Stream, and implemented without Stream. While, it is simpler concept -- supplement methods on basic libraries.\\nStream has several characteristics:\\n\\nTrends to process each element independently\\nInfinite that we can not get the size\\nDesigned for large data flow performance\\n\\nThus, Stream may not be able to support operations related with the size of it, nor operations involving multiple or even random elements.\\nBasically, the vast majority operations on List, Map, String don\\'t need a Stream. Stream brings great merits on big data processing, but when we\\'re not facing performance problem (operating a list of about 10x elements), it is an over kill.\\nYet Stream brings the 2 additional steps \"Stream()\" and \"collect()\", which is sometimes annoying to write.\\nIceFig targets on \"small data\" operations within application logic, to provide simple & beautiful code writing about String, List, Map operation & transformation.\\nNo utilities\\nIn traditional Java way, we use a lot of utilities (StringUtils, FileUtils) for the missing methods in standard library. While in IceFig, we make an object oriented and functional way to free you from tedious codes.\\nZero runtime dependency\\nIceFig has no external runtime dependency except JDK 8.\\nMutable & default interfaces\\nIceFig firstly aggregates all operations which do not change the state into a default interface(e.x. Seq, Hash).\\nOn the other hand, there are interfaces named \"mutableXXX\" extending the default ones with additional in-place operations, which are commonly named xxxInPlace.\\nIf you don\\'t want the ability to change the object, you can use the default interface to let compiler check it for you. And it is the recommended way.\\nNote that the default interface doesn\\'t mean immutability of the object it is on, it only ensures \"if outside only uses this interface on the object, the object will not be changed\".\\nConventions\\nIceFig uses conventions on method names. If there is a pair of methods name, nameInPlace, method ends with InPlace means calling this method will change the object itself, while calling the other won\\'t.\\nLicense\\nApache License 2.0\\nContribution\\nFeel free to submit issues & PRs\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '142',\n",
       "  'forks': '7',\n",
       "  'commits': '224'},\n",
       " {'language': 'Java 85.1',\n",
       "  'readme': \"⚠️\\nTHIS REPO IS DEPRECATED SINCE THE DRAWER LAYOUT WAS INTEGRATED INTO THE TITANIUM CORE. CONSIDER USING: https://docs.appcelerator.com/platform/latest/#!/api/Titanium.UI.Android.DrawerLayout\\n\\nDEPRECATED (Ti.DrawerLayout)  \\nNative Android Navigation Drawer for Titanium\\n\\nOverview\\nInstallation\\nUsage\\nDocs\\nDemo\\nLicense\\n\\nThis is a fork of Tripvi/Ti.DrawerLayout\\nOverview\\n\\nThis module adds support for using the DrawerLayout in Titanium Apps.\\nThe Drawer Layout is a view that can be pulled from the edge of a window. This can answer various purposes. The most common use case is the Navigation Drawer as seen in the above screenshot. The Navigation Drawer displays navigation options in a drawer which slides in from the left edge.\\nTo expand the drawer the user can either touch the app icon or swipe from the left edge. The navigation drawer overlays the content but not the action bar.\\nInstallation\\n\\nGrab the latest package from the dist folder\\nInstall it following this guide\\nwith gittio: $ gittio install com.tripvi.drawerlayout\\n\\nUsage\\nHere's an example of how to use the module.\\n\\nPlease note: This module requires a Theme without ActionBar such as Theme.AppCompat.Light.NoActionBar since it adds a Toolbar to its own layout. If you do not want the Toolbar, just pass the hideToolbar property at creation-time.\\n\\n// Load module\\nvar TiDrawerLayout = require('com.tripvi.drawerlayout');\\n\\n// define left and center view\\nvar leftView = Ti.UI.createView({backgroundColor:'gray'});\\nvar centerView = Ti.UI.createView({backgroundColor:'white'});\\n\\n// create the Drawer\\nvar drawer = TiDrawerLayout.createDrawer({\\n    leftView: leftView,\\n    centerView: centerView\\n});\\n\\n// create a window\\nvar win = Ti.UI.createWindow();\\n\\n// add the drawer to the window\\nwin.add(drawer);\\n\\n// listen for the open event...\\nwin.addEventListener('open', function(){\\n    \\n    // ...to access activity and action bar\\n    var activity = win.getActivity();\\n    var actionbar = activity.getActionBar();\\n    \\n    if (actionbar){\\n    \\n        // this makes the drawer indicator visible in the action bar\\n        actionbar.displayHomeAsUp = true;\\n        \\n        // open and close with the app icon\\n        actionbar.onHomeIconItemSelected = function() {\\n            drawer.toggleLeftWindow();\\n        };\\n    }\\n});\\n\\n// open the window\\nwin.open();\\nAPI Documentation\\nDemo App\\nDevelopment\\nContributions are very welcome. If you are able to fix bugs or want to add features to the module, please refer to the official module development guide. You can build the module with appc as described there. You can also use grunt build here. For distribution a new version, please update the version number in package.json and run grunt. It will update the version number in manifest and readme. Then it builds the new version and installs it globally with gittio.\\nLicense\\nMIT license, see LICENSE\\nCopyright (c) 2013 - 2014 by Tripvi Inc., 2015 - 2016 by Manuel Lehner\\n\",\n",
       "  'watchers': '23',\n",
       "  'stars': '141',\n",
       "  'forks': '56',\n",
       "  'commits': '165'},\n",
       " {'language': 'C++ 93.7',\n",
       "  'readme': '\\nIOTA C++ Library\\n\\n\\n\\n\\n\\n\\n\\n\\nThis is not (yet) an official C++ client library for the IOTA Reference Implementation (IRI).\\nIt implements both the official API, as well as newly proposed features.\\nTable of Contents\\n\\nDisclaimer\\nDocumentation\\n\\nInstallation\\nGetting Started\\nDoxygen\\nWiki\\nExamples\\n\\n\\nTechnologies, tools & dependencies\\n\\nTechnologies\\nDependencies\\n\\n\\nCommunity\\n\\nResources\\nContributing\\nDonating\\n\\n\\n\\nDisclaimer\\n\\nThis is an early beta release, expect unexpected results.\\nThere may be performance and stability issues.\\nThis library is still in early development and there may be breaking changes in the near future.\\nYou may lose all your money.\\n\\nDocumentation\\nInstallation\\ngit clone --recursive git@github.com:thibault-martinez/iota.lib.cpp.git\\ncd iota.lib.cpp\\nmkdir build\\ncd build\\ncmake ..\\nmake\\n\\nGetting Started\\nIOTA::API::Core api(\"node.iotawallet.info\", 14265);\\nauto            res = api.getTransactionsToApprove(27);\\n\\nstd::cout << res.getTrunkTransaction() << std::endl;\\nstd::cout << res.getBranchTransaction() << std::endl;\\nDoxygen\\nA Doxygen documentation is available and provides full API documentation for the library.\\nWiki\\nA Wiki is available and provides full documentation for the library as well as installation explanations.\\nExamples\\nThere\\'s an extensive list of test cases on the test folder that can be used as reference when developing apps with IOTA.\\nTechnologies, tools & dependencies\\nTechnologies\\nThis library currently uses C++11.\\nTools\\n\\nxsltproc (needed in Keccak toolchain)\\n\\nDependencies\\n\\nC++ Requests\\nJSON For Modern C++\\nKeccak Code Package\\n\\nCommunity\\nResources\\nIf you want to get involved in the community, need help with getting setup, have any issues related with the library or just want to discuss Blockchain, Distributed Ledgers and IoT with other people, feel free to visit one of our resources :\\n\\nWebsite\\nDiscord\\nForum\\nStack Exchange\\n\\nContributing\\nPlease report any issues using the Issue Tracker.\\nDonating\\nOZCTDHTFCB9PTAZWGYCGOA9XKDKPSHWVSZDJKZCOINNQTPNNEANGPBBDLSNGKDGCAAKBDVBOVCPTRLHTANMIRGFFGD\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '86',\n",
       "  'forks': '24',\n",
       "  'commits': '417'},\n",
       " {'language': 'C++ 52.9',\n",
       "  'readme': 'ICU Character Set Detection for Node.js\\n\\nCharacter set detection is the process of determining the character set, or encoding, of character data in an unknown format.\\n\\nA simple binding of ICU character set detection (http://userguide.icu-project.org/conversion/detection) for Node.js.\\nInstallation\\nAt first, install libicu into your system (See this instruction for details).\\nAfter that, install node-icu-charset-detector from npm.\\nnpm install node-icu-charset-detector\\n\\nInstalling ICU\\nLinux\\n\\n\\nDebian (Ubuntu)\\napt-get install libicu-dev\\n\\n\\nGentoo\\nemerge icu\\n\\n\\nFedora/CentOS\\nyum install libicu-devel\\n\\n\\nOSX\\n\\n\\nMacPorts\\nport install icu +devel\\n\\n\\nHomebrew\\n\\n\\nbrew install icu4c\\nbrew link icu4c --force\\nIf experiencing issues with \\'homebrew\\' installing version 50.1 of icu4c, try the following:\\nbrew search icu4c\\nbrew tap homebrew/versions\\nbrew versions icu4c\\ncd $(brew --prefix) && git pull --rebase\\ngit checkout c25fd2f $(brew --prefix)/Library/Formula/icu4c.rb\\nbrew install icu4c\\n\\nFrom source\\n\\ncurl -O http://download.icu-project.org/files/icu4c/52.1/icu4c-52_1-src.tgz\\ntar xzvf icu4c-4_4_2-src.tgz\\ncd icu/source\\nchmod +x runConfigureICU configure install-sh\\n./runConfigureICU MacOSX\\nmake\\nsudo make install\\nxcode-select --install\\nUsage\\nSimple usage\\nnode-icu-charset-detector provides a function detectCharset(buffer), where buffer is an instance of Buffer whose charset should be detected.\\nvar charsetDetector = require(\"node-icu-charset-detector\");\\n\\nvar buffer = fs.readFileSync(\"/path/to/the/file\");\\nvar charset = charsetDetector.detectCharset(buffer);\\n\\nconsole.log(\"charset name: \" + charset.toString());\\nconsole.log(\"language: \" + charset.language);\\nconsole.log(\"detection confidence: \" + charset.confidence);\\ndetectCharset(buffer) returns the detected charset name for buffer, and the returned charset name has two extra properties language and confidence:\\n\\ncharset.language\\n\\nlanguage name for the detected character set.\\n\\n\\ncharset.confidence\\n\\nconfidence of the charset detection for charset.\\n\\n\\n\\nLeveraging node-iconv\\nSince ICU itself does not have a feature to convert character sets, you may need to use node-iconv (https://github.com/bnoordhuis/node-iconv), which has a powerful character sets converting feature.\\nHere is a simple example to leverage node-iconv to convert character sets not supported by Node itself.\\nfunction bufferToString(buffer) {\\n  var charsetDetector = require(\"node-icu-charset-detector\");\\n  var charset = charsetDetector.detectCharset(buffer).toString();\\n\\n  try {\\n    return buffer.toString(charset);\\n  } catch (x) {\\n    var Iconv = require(\"iconv\").Iconv;\\n    var charsetConverter = new Iconv(charset, \"utf8\");\\n    return charsetConverter.convert(buffer).toString();\\n  }\\n}\\n\\nvar buffer = fs.readFileSync(\"/path/to/the/file\");\\nvar bufferString = bufferToString(buffer);\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '86',\n",
       "  'forks': '22',\n",
       "  'commits': '55'},\n",
       " {'language': 'C++ 94.2',\n",
       "  'readme': 'OpenCVBlobsLib is a library written in C++ on the base of cvblobslib. It allows for labelling, filling, filtering, gathering information when dealing with \"zones\" with homogeneous features in an image. It uses OpenCV and PThread in order to boost the performance. The used algorithm is very efficient with big images and/or many blobs and can become even faster exploiting the multi-core architecture of modern CPUs.\\nA list of its features:\\n\\nBinary image 8-connected component labelling/blob extraction.\\nBlob filtering (based on size or other user-defined features).\\nBlob properties computation, e.g.:\\nMean and standard deviation of the pixel values in the covered region.\\nArea and perimeter.\\nBounding box.\\nContaining ellipse.\\nMoments computation.\\nColor-fill of the blob region.\\n\\nOpenCVBlobsLib added Features:\\n\\nMulti core support for the extraction stage.\\nOpenCV 2.0 compliant interface.\\nBlob joining, allowing for distinct regions to be grouped as one.\\nGeneric bug fixing.\\n\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '86',\n",
       "  'forks': '43',\n",
       "  'commits': '108'},\n",
       " {'language': 'C++ 73.6',\n",
       "  'readme': 'STATUS: DEPRECATED, See ofxOMXCamera for future development/releases\\nMaster may be unstable, features untested. See Releases for tested versions\\nDESCRIPTION:\\nopenFrameworks addon to control the Raspberry Pi Camera Module. This does not provide still camera functionality.\\nREQUIREMENTS:\\nopenFrameworks .9 or higher Setup Guide\\nDeveloped with GPU memory set at 256, overclock to medium but 128/default should work as well\\nDesktop Mode (X11 enabled) may work but untested\\nUSAGE:\\nClone into your openFrameworks/addons folder\\nEither copy one of the examples into /myApps or add ofxRPiCameraVideoGrabber to the addons.make file in your project\\nLED Toggling requires gpio program provided via wiringPi\\n$sudo apt-get install wiringpi\\nThe addon works in a few different modes:\\nTEXTURE MODE:\\nAllows the use of:\\n\\nShaders\\nPixel access\\nOverlays, etc\\n\\nNON-TEXTURE MODE (or direct-to screen)\\nIn non-texture mode the camera is rendered directly to the screen. It typically looks a bit faster/cleaner but no other drawing operations can happen.\\nRECORDING:\\nRecording is available in both texture and non-texture modes\\nEXAMPLES:\\nexample-demo-mode\\nShows different settings available to tweak the camera exposure, metering, cropping, zooming, filters, mirroring, white balance\\nexample-direct-mode\\nCamera turns on and is rendered full screen via OMX acceleration\\nPress the \"e\" key to toggle through built in filters\\nexample-direct-mode-transform\\nDemos cropping, alpha, mirroring of direct display (not camera)\\nexample-texture-mode\\nCamera turns on and renders to a texture that is drawn at full screen and a scaled version\\nPress the \"e\" key to toggle through built in filters\\nexample-shaders\\nBasic shader usage with texture-mode\\nPress the \"e\" key to toggle through built in filters\\nPress the \"s\" key to toggle shader\\nexample-saved-settings:\\nAlternative way to load a camera configuration through a text file\\nexample-recording:\\nRecording of video in texture or direct mode\\nexample-wrapper:\\nDrop-in replacement for ofVideoGrabber (texture-mode only)\\nTHANKS:\\nThanks to @tjormola for sharing his demos and exploration - especially in regards to recording\\nhttps://github.com/tjormola/rpi-openmax-demos\\nand thanks to @linuxstb for helping get started with the camera and OpenMax\\nhttps://github.com/linuxstb/pidvbip\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '86',\n",
       "  'forks': '17',\n",
       "  'commits': '192'},\n",
       " {'language': 'C++ 98.4',\n",
       "  'readme': \"kinectable_pipe\\nkinectable_pipe is a command-line utility that dumps user skeleton data from a Microsoft Kinect device to a standard Unix pipe.\\nWhy?\\nTo bring Unix-y goodness to the world of Microsoft Kinect programming!\\nNo, really, why?\\nBecause Kinect programming is a pain in the neck, and by trivializing the device's output into a simple text format, it becomes infinitely easier to digest in the scripting language of your choice.\\nThis seems simple to the point of being almost useless\\nYes, that's the point. Do One Thing and Do It Well. There's an accompanying rubygem that will add all the smart stuff like advanced gesture recognition, events, etc.\\nUSAGE\\n% kinectable_pipe | ruby gesture_recognizer.rb | python play_light_show.py\\n\\nINSTALLATION (OS X / homebrew)\\n# must have universal binary for libusb\\nbrew uninstall libusb\\nbrew install libusb --universal\\n\\nbrew tap marshally/alt\\n\\nbrew install kinectable_pipe\\n\\n# now plug in your kinect\\n# and run this command in the terminal\\n\\nkinectable_pipe\\n\\n# step back from the sensor, wave your arms like a lunatic\\n# until it recognizes you and starts pumping out skeleton data\\n# to STDOUT\\n\\nOPTIONS\\n-r 15 # restrict output to 15fps, Kinect max is 30fps\\n\\nTODO\\n\\nLinux install instructions\\nRecognize all available xn::GestureGenerator\\nFix output to work with non-interactive terminal sessions.\\nImprove Ruby sample app.\\nAdd sample apps for other scripting languages.\\nAdd CLI argument for alternate output encodings (XML, msgpack, BERT, whatever).\\n\\n\",\n",
       "  'watchers': '2',\n",
       "  'stars': '84',\n",
       "  'forks': '7',\n",
       "  'commits': '31'},\n",
       " {'language': 'C++ 98.5',\n",
       "  'readme': '\\n\\nSingle file embedded C++ web server\\n===\\nHow to use in own project\\n#include <iostream>\\n#include <map>\\n\\n#include \"web++.hpp\"\\n\\nusing namespace WPP;\\n\\nvoid web(Request* req, Response* res) {\\n    std::cout << req->method << \" \" << req->path << std::endl;\\n\\n    std::cout << \"Headers:\" << std::endl;\\n\\n    std::map<std::string, std::string>::iterator iter;\\n    for (iter = req->headers.begin(); iter != req->headers.end(); ++iter) {\\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\\n    }\\n\\n    std::cout << \"Query:\" << std::endl;\\n\\n    for (iter = req->query.begin(); iter != req->query.end(); ++iter) {\\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\\n    }\\n\\n    std::cout << \"Cookies: \" << req->cookies.size() << std::endl;\\n\\n    for (iter = req->cookies.begin(); iter != req->cookies.end(); ++iter) {\\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\\n    }\\n\\n    res->body << \"HELLO\";\\n}\\n\\nint main(int argc, const char* argv[]) {\\n    try {\\n        std::cout << \"Listening on port 5000\" << std::endl;\\n\\n        WPP::Server server;\\n        server.get(\"/\", &web);\\n        server.all(\"/dir\", \"./\");\\n        server.start(5000);\\n    } catch(WPP::Exception e) {\\n        std::cerr << \"WebServer: \" << e.what() << std::endl;\\n    }\\n\\n    return EXIT_SUCCESS;\\n}\\nHow to compile\\ng++ demo.cpp -o demo\\n\\nSpecial requirements\\nNop\\nTested on\\n\\nMac OS X\\nLinux\\n\\nThe MIT License\\nCopyright (c) Alex Movsisyan\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \\'Software\\'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \\'AS IS\\', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '84',\n",
       "  'forks': '29',\n",
       "  'commits': '35'},\n",
       " {'language': 'C++ 50.5',\n",
       "  'readme': 'WebKit\\nWebKit is an open source web browser engine. QtWebKit is the Qt Port of WebKit.\\nThis code is based on the source found in webkit.org but it contains modifications introduced\\nby the isis-project.\\nFor more information see:\\nhttp://www.webkit.org\\nhttp://developer.qt.nokia.com/wiki/QtWebKit\\nhttp://en.wikipedia.org/wiki/Webkit\\nhttp://isis-project.org/\\nLicense\\nSome parts of WebKit are available under the GNU Lesser General Public License and others under\\na BSD-style license.\\n',\n",
       "  'watchers': '32',\n",
       "  'stars': '83',\n",
       "  'forks': '46',\n",
       "  'commits': '104,142'},\n",
       " {'language': 'C++ 55.3',\n",
       "  'readme': '丸の内MongoDB勉強会 Marunouchi.mongo\\n主に丸の内MongoDB勉強会の資料を管理しています。\\nThe guide of meetup MongoDB in Marunouchi Tokyo.\\n\\n開催予定\\n2015/7/16（木）19:00 - 20:30 丸の内MongoDB勉強会 #22 「初心者向け：MongoDBの基礎とハンズオン(ver3対応)」と「MongoDB World 2015レポート」 2015/7/16DoorKeeper\\nお知らせ\\ngihyo.jpで「MongoDBでゆるふわDB体験」連載終了！ありがとうございました！\\nご発表いただいた資料はOpenStandiaのWebサイトからリンクを貼らせていただく場合があります。ご了承ください。\\n\\nMongoDB最初の1歩\\n@fetarodcの初心者向けMongoDBのキホン！を一通り読むと良いと思います。\\nとりあえず手を動かしたい人は、丸の内MongoDB勉強会 #1のstep01をやってみてください。\\n\\nWiki\\n勉強会で出た質問を中心にノウハウ、Tipsをまとめていきます。\\nwiki\\n\\nお役立ちリンク集\\nmongodbソースコード(rpmもあるよ) - github\\n公式ドキュメント(日本語) マニュアル\\n公式ドキュメント(日本語) チュートリアル\\n新・公式ドキュメント(日本語) マニュアル\\nMongoDBの基礎(動画) - dotinstall\\n@doryokujinさんのスライド\\n@fetarodcの初心者向けMongoDBのキホン！\\nMongoDBでゆるふわDB体験\\n\\n開催記録\\n\\n丸の内MongoDB勉強会 #1 SQLと比較しながらクエリを学ぶ&RubyからMongoDBを触ってみる 2012/07/30 ATND\\n丸の内MongoDB勉強会 #2 みんなでシャーディング 2012/08/28 ATND\\n丸の内MongoDB勉強会 #3 2.2の新機能&レプリケーションハンズオン 2012/09/26 ATND\\n丸の内MongoDB勉強会 #4 「MongoDBでWebアプリを作ってみよう」と「Configパラメータ解説」 2012/11/06 ATND\\n丸の内MongoDB勉強会 #5 「ソースコードリーディング入門」と「MongoDBをカスタムビルドしてみよう」と「運用について」 2012/12/18 ATND\\n丸の内MongoDB勉強会 #6 「GridFSハンズオン」と「Shardingのコネクション数周りのチューニングについて」 2012/01/23 ATND\\n丸の内MongoDB勉強会 #7 「MongoDB 2.4 新機能紹介」と「MongoDBでXMLデータを扱うシステム開発」 2013/02/19 ATND\\n丸の内MongoDB勉強会 #8 in CookPad 「ghostsync and slaveDelay」、「権限によるACLのハンズオン」他 2013/03/27 ATND\\n丸の内MongoDB勉強会 #9 in 楽天 「MongoDB 2.4の注目の新機能ハンズオン」と「MongoDBの検証環境を作ろう」 2013/04/17 ATND\\n丸の内MongoDB勉強会 #10 「初心者向けMongoDB入門」と「MongoDBの運用」 2013/05/22 ATND\\n丸の内MongoDB勉強会 #11 「初心者向けレプリケーションハンズオン」「MongoDBでゲームを作る」 2013/06/26 ATND\\n丸の内MongoDB勉強会 #12 in 納涼もんご祭り 「Node.js+Mongoose+MongoDB で作るWebアプリ」「ホスティングサービスで始めるMongoDB」 2013/07/28 ATND\\n丸の内MongoDB勉強会 #13 「初心者向け：みんなでシャーディング」 2013/09/25 ATND\\n丸の内MongoDB勉強会 #14 「初心者向け：レプリケーション＋シャーディング」「もんごで自然言語処理！第一弾」 2013/10/31 ATND\\n丸の内MongoDB勉強会 #15 「初心者向け：レプリケーション＋シャーディング（第14回のリベンジ）」「MongoDBと私（事例紹介）」 2013/12/19 ATND\\n丸の内MongoDB勉強会 #16 「がっつり事例紹介」 2014/3/19 ATND\\n丸の内MongoDB勉強会 #17 「初心者向けMongoDBのキホン！」「初心者向けMongoDB入門」「月間10億PVを支えるMongoDBアーキテクチャ」 2014/5/14 DoorKeeper\\n丸の内MongoDB勉強会 #18 「初心者向け：レプリケーション＋シャーディング」「MongoDB World@NewYork 報告会」「LT：MongoDB World 2014 プチ報告」 2014/7/17 DoorKeeper\\n丸の内MongoDB勉強会 #19 in もんご祭り「初心者向け：SQLと比較しながらクエリを学ぶ」「MongoDBのindexについて」 2014/10/11 DoorKeeper\\n丸の内MongoDB勉強会 #20 「初心者向け：SQLと比較しながらクエリを学ぶハンズオン」「MMS、特に新機能『Automation』について」「LT:MongoDB 2.8RC0のストレージエンジンについて」 2014/11/26 DoorKeeper\\n丸の内MongoDB勉強会 #21「Couchbase Serverの紹介」「MongoDB の紹介とCouchbaseとの違い」 2015/03/05 DoorKeeper\\n\\n',\n",
       "  'watchers': '20',\n",
       "  'stars': '83',\n",
       "  'forks': '14',\n",
       "  'commits': '807'},\n",
       " {'language': 'C++ 81.5',\n",
       "  'readme': \"DEPRECATED: See https://www.mysensors.org/build/raspberry\\n#Wiring the NRF\\t24L01+ radio\\n\\n\\n\\nNRF24l01+\\nRpi Header Pin\\n\\n\\n\\n\\nGND\\n25\\n\\n\\nVCC\\n17\\n\\n\\nCE\\n22\\n\\n\\nCSN\\n24\\n\\n\\nSCK\\n23\\n\\n\\nMOSI\\n19\\n\\n\\nMISO\\n21\\n\\n\\nIRQ\\n--\\n\\n\\n\\n#Building & Installing\\n##RF24 library\\n\\nDownload the library from https://github.com/TMRh20/RF24\\nEither an official release(tested with 1.1.3) or clone the master branch.\\nDecompress(if needed) and change to the library directory\\nRun make all followed by sudo make install\\n\\n##Serial Gateway\\nThe standard configuration will build the Serial Gateway with a tty name of\\n'/dev/ttyMySensorsGateway' and PTS group ownership of 'tty' the PTS will be group read\\nand write. The default install location will be /usr/local/sbin. If you want to change\\nthat edit the variables in the head of the Makefile.\\n###Build the Gateway\\n\\nClone this repository\\nChange to the Raspberry directory\\nRun make all followed by sudo make install\\n(if you want to start daemon at boot) sudo make enable-gwserial\\n\\nFor some controllers a more recognisable name needs to be used: e.g. /dev/ttyUSB020 (check if this is free).\\nsudo ln -s /dev/ttyMySensorsGateway /dev/ttyUSB20\\nTo automatically create the link on startup, add ln -s /dev/ttyMySensorsGateway /dev/ttyUSB20 just before exit0 in /etc/rc.local\\n#Uninstalling\\n\\nChange to Raspberry directory\\nRun sudo make uninstall\\n\\nSupport: http://forum.mysensors.org\\n\",\n",
       "  'watchers': '54',\n",
       "  'stars': '83',\n",
       "  'forks': '72',\n",
       "  'commits': '57'},\n",
       " {'language': 'C++ 56.1',\n",
       "  'readme': 'Welcome to python-nlpir\\nThis open source project is a python wrapper for NLPIR.\\nNLPIR is a powerful tool for Chinese segmentation. This program supported Windows and Linux, both 32bit and 64bit platform.\\nCurrent Version: v3.0\\nChange Log\\nVersion 3.0\\n\\nUprgrade NLPIR to NPLIR2015(v20141230).\\nFix bug in installation script.\\n\\nVersion 2.0\\n\\nUprgrade NLPIR core library from NPLIR to NPLIR2014.\\nInstall SWIG automatically during PyNLPIR installation.\\nRefactor installation scripts.\\n\\nVersion 1.1\\n\\nAdd Windows 64bit and Linux 64bit supported.\\nCode constructure adjustment.\\nUpgrade NLPIR2013 core library.\\nAdd installation scripts for each platform.\\n\\nInstallation\\n\\nWindows: python install.py\\nLinux: sudo python install.py\\n\\nOthers\\nIf you want to know more details, you can access my blog.\\nLinks\\n\\nNLPIR Home: http://www.nlpir.org/\\n\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '83',\n",
       "  'forks': '44',\n",
       "  'commits': '56'},\n",
       " {'language': 'JavaScript 89.3',\n",
       "  'readme': 'Ioniclub App\\n\\n\\nhttp://ionichina.com Ionichina社区客户端，采用Ionic Framework开发\\n\\n\\n快速开始\\n1. 首先安装ionic\\n$ sudo npm install -g cordova ionic\\n\\n2. 项目Clone到本地\\n$ git clone https://github.com/IonicChina/ioniclub.git\\n\\n3. 添加 android 或 ios 平台\\n注： 真机调试，浏览器可以跳过此步骤；\\nios 开发只能在 mac 下进行。\\n$ cd ioniclub\\n$ ionic platform add ios\\n$ ionic platform add android\\n\\n4. 添加所有用到的插件\\nbower install moment --save\\nbower install ngCordova\\nbower install angular-moment --save\\nbower install angular-resource --save\\ncordova plugin add https://github.com/danwilson/google-analytics-plugin.git\\ncordova plugin add https://github.com/EddyVerbruggen/SocialSharing-PhoneGap-Plugin.git\\ncordova plugin add cordova-plugin-inappbrowser\\ncordova plugin add https://github.com/katzer/cordova-plugin-email-composer.git\\ncordova plugin add https://github.com/whiteoctober/cordova-plugin-app-version.git\\ncordova plugin add cordova-plugin-network-information\\ncordova plugin add https://github.com/wildabeast/BarcodeScanner.git\\ncordova plugin add https://github.com/VersoSolutions/CordovaClipboard.git\\n\\njpush 插件安装参考：[https://github.com/jpush/jpush-phonegap-plugin](https://github.com/jpush/jpush-phonegap-plugin\\n注：在添加了BarcodeScanner插件后android 打包会遇到个蛋疼error，解决方案是在platforms/android/build.gradle文件里 android {} 结点（243行）添加\\n  abortOnError false\\n}```\\n[#详情](http://forum.ionicframework.com/t/error-when-running-cordova-build-release-android/25136/4)\\n### 5. 运行\\n#### 浏览器\\n  $  ionic serve\\n#### ios\\n  $  ionic build ios\\n  $  ionic run ios\\n#### android\\n  $  ionic build android\\n  $  ionic run android\\n\\n\\n## 贡献\\n[贡献者列表](https://github.com/IonicChina/ioniclub/graphs/contributors)\\n\\n有任何意见或建议都欢迎提 issue，或者直接在社区 [http://ionichina.com](http://ionichina.com) 提给 [@DongHongfei](http://ionichina.com/user/DongHongfei)\\n\\n## License\\n[MIT](LICENSE)\\n\\n',\n",
       "  'watchers': '22',\n",
       "  'stars': '196',\n",
       "  'forks': '95',\n",
       "  'commits': '11'},\n",
       " {'language': 'JavaScript 99.4',\n",
       "  'readme': 'Framer Presentation Templates\\n\\nFeatures\\n\\nWhen viewed on a desktop browser, your prototype will appear inside a device \"frame\", with an optional background behind it. The content of your prototype will be scaled to match the size of your browser window.\\nWhen viewed on a mobile device, the frame disappears, so your prototype can feel like a real app. As a bonus, Mobile Safari\\'s bouncy scrolling is also prevented automatically.\\nSelf-contained. All you need to do is link to a script. No need to bother with additional assets or write code.\\nOption to prompt users to add the prototype as an app on their homescreen before using it.\\nSwitch between presentation and development mode by pressing Alt + P.\\nSwitch between zoom levels with Alt + 1 (100%), Alt + 2 (75%), Alt + 3 (50%), Alt + 4 (25%), Alt + 0 (Fit to Screen). You can also use Alt + - to zoom out and Alt + = to zoom in.\\n\\nUsage\\n\\nDownload a template file from templates/ that matches your needs and place it in your prototype\\'s main directory. Here\\'s the list of all currently available templates:\\n\\n\\niPhone 5s (White, Portrait)\\niPhone 5s (White, Landscape)\\niPhone 5s (Black, Portrait)\\niPhone 5s (Black, Landscape)\\niPhone 5s (Gold, Portrait)\\niPhone 5s (Gold, Landscape)\\niPhone 5c (White, Portrait)\\niPhone 5c (White, Landscape)\\niPhone 5c (Blue, Portrait)\\niPhone 5c (Blue, Landscape)\\niPhone 5c (Green, Portrait)\\niPhone 5c (Green, Landscape)\\niPhone 5c (Red, Portrait)\\niPhone 5c (Red, Landscape)\\niPhone 5c (Yellow, Portrait)\\niPhone 5c (Yellow, Landscape)\\niPad Mini (White, Portrait)\\niPad Mini (White, Landscape)\\niPad Mini (Black, Portrait)\\niPad Mini (Black, Landscape)\\nNexus 5 (Portrait)\\nNexus 5 (Landscape)\\nAndroid Wear (Circle)\\nAndroid Wear (Square)\\n\\n\\nOpen your index.html in a text editor and link to the template using a <script> tag. The template should be included after the framer.js script:\\n\\n<script src=\"framer/framer.js\"></script>\\n<script src=\"framer/framerps.js\"></script>\\n<script src=\"app.js\"></script>\\n\\n<!-- Link to the template script: -->\\n<script src=\"iphone-5s-white.js\"></script>\\n\\nLoad index.html in a browser. You should see your prototype appear inside the presentation template.\\n\\nUsage with Framer Studio\\n\\n\\nDownload one of the templates and place it in the folder of your project, under framer/.\\n\\n\\nPut this at the top of your script:\\n\\n\\nUtils.domLoadScriptSync(\\'framer/iphone-5c-blue-landscape.js\\')\\n\\nChoose the Fullscreen option in the Preview pane, so you don\\'t get the \"phone in a phone\" effect.\\n\\nInstalling with Bower\\nYou can use bower to install and keep the templates up to date:\\nbower install framer-templates\\n\\nAll of the available templates will be installed to bower_components/framer-templates/templates.\\nA Note on Compatibility\\nThe presentation templates are fully compatible with Framer 3. You can still download the old, Framer 2 compatible version of the templates from here: Download Framer 2 Templates.\\nIf you\\'re upgrading from a Framer 2 template, everything should work as expected by just replacing the template file. If you\\'ve used the config.template = { ... } syntax to customize the appearance of your template, you should change it to Framer.Config.template = { ... }.\\nSetting the Background Image\\nThe default background image is a plain off-white color, but if that\\'s not to your tastes, you can use your own by specifying it as a template option before you load the template:\\n<script>FramerTemplateConfig = { backgroundImage: \\'[url to your background image]\\' };</script>\\n<script src=\"iphone-5s-white.js\"></script>\\n\"Add to Homescreen\" Prompt\\n\\nThe templates have built-in capability for reminding users that view your prototype inside a mobile browser to add it to their homescreen for a more app-like experience. This feature is off by default, but you can turn it on like this:\\n<script>FramerTemplateConfig = { shouldShowAddToHomescreenPrompt: true };</script>\\n<script src=\"iphone-5s-white.js\"></script>\\nTips\\n\\nAppend #dev to the URL of your prototype to load it directly in developer mode (no device frame, scaled to 100%). E.g. http://localhost/prototype/index.html#dev\\nAppend #z75 to the URL of your prototype to load it at 75% scale. Putting any number after the z will work.  E.g. http://localhost/prototype/index.html#z75\\n\\nBuilding Your Own Template\\nAll templates use the same basic code, but vary in configuration, depending on which device you want to use to present your prototype.\\nHere\\'s what the configuration file for the white iPhone template looks like:\\n{\\n  \"backgroundImage\": \"\",\\n  \"shouldShowAddToHomescreenPrompt\": false,\\n  \"deviceWidth\": 385,\\n  \"deviceHeight\": 805,\\n  \"screenWidth\": 320,\\n  \"screenHeight\": 568,\\n  \"contentWidth\": 640,\\n  \"contentHeight\": 1136,\\n  \"cursorWidth\": 32,\\n  \"promptAnchorTop\": 704,\\n  \"promptAnchorLeft\": 320,\\n  \"addToHomescreenPromptImage\": \"images/addtohomescreen-prompt-iphone.png\",\\n  \"deviceImage\": \"images/iphone-5s-white.png\",\\n  \"cursorImage1x\": \"\",\\n  \"cursorImage2x\": \"\"\\n}\\nLet\\'s go over the different attributes:\\n\\n\\nbackgroundImage (data URI, URL, or image path) Default: empty\\nAn image that will be placed behind the device image.\\n\\n\\npreventBounce (true or false) Default: true\\nPrevents the entire page from bouncing when scrolled up/down (will also prevent scrolling beyond the content boundaries).\\n\\n\\nshouldShowAddToHomescreenPrompt (true or false) Default: false\\nSpecifies whether an \"add to homescreen\" prompt should be shown when the prototype is viewed on an iOS device, but not in homescreen standalone app mode.\\n\\n\\ndeviceWidth and deviceHeight (number)\\nThe width and height of the device image you\\'re using. Note that the device image is automatically scaled by 50% in order to look crisp and clear on a retina screen, so you\\'ll need to specify the size / 2 (e.g. if your image is 2050px, put in 1025 for device width).\\n\\n\\nscreenWidth and screenHeight (number)\\nThe size of the actual screen in your device image. For the iPhone background, for example, the size of the screen is 320x568.\\n\\n\\ncontentWidth and contentHeight (number)\\nThe size of your actual prototype. This is usually the screen size multiplied by 2. By default, Framer\\'s template assumes that your content is 640px wide (iPhone sized).\\n\\n\\nsidePadding (number) Default: 50\\nThe minimum amount of padding (in px) you want to leave between the edges of the browser window and the device image.\\n\\n\\nzoomFactor (number) Default: not set\\nA specific zoom factor to use, instead of automatically re-scaling your prototype to fit inside your browser window. A zoom factor of 0.75 would mean 75%, a factor of 2 would mean 200%.\\n\\n\\ncursorWidth (number)\\nThe width of the custom cursor image you supplied. This is used to figure out the location of anchor point of the cursor (for circular cursors, for example, this would be the middle of the circle).\\n\\n\\npromptAnchorTop and promptAnchorLeft (number)\\nThis is the point in the \"add to homescreen\" prompt image that the arrow points to. You don\\'t have to specify this if you\\'re not using the prompt in the first place.\\n\\n\\naddToHomescreenPromptImage (data URI, URL, or image path)\\nThis image will be displayed if the prototype is viewed on a mobile device and not in standalone (homescreen app) mode.\\n\\n\\ndeviceImage (data URI, URL, or image path)\\nThis is the device image that will be used to \"hold\" your prototype. Make sure that the actual screen portion of the device is centered perfectly in the image (hint: use one of the existing images for a template).\\n\\n\\ncursorImage1x and cursorImage2x (data URI, URL, or image path) Default: a 66x66 bobble cursor.\\nCustom cursor images in normal and retina resolutions.\\n\\n\\ncursorPressedImage1x and cursorPressedImage2x (data URI, URL, or image path) Default: a 66x66 bobble cursor.\\nCustom pressed state cursor images in normal and retina resolutions.\\n\\n\\nNote: All image paths are automatically converted to data URIs, so that the template is self-contained.\\nTo create your own template, copy one of the config-*.json files, edit the configuration values in there and then run:\\nnpm install\\ncake build\\n\\nYou should see something like this:\\nclean Cleaning out templates directory...\\nbuild Using src/config-ipad-mini-black.json to generate templates/ipad-mini-black.js\\nbuild Using src/config-ipad-mini-white.json to generate templates/ipad-mini-white.js\\nbuild Using src/config-iphone-5c-blue.json to generate templates/iphone-5c-blue.js\\nbuild Using src/config-iphone-5c-green.json to generate templates/iphone-5c-green.js\\nbuild Using src/config-iphone-5c-red.json to generate templates/iphone-5c-red.js\\nbuild Using src/config-iphone-5c-white.json to generate templates/iphone-5c-white.js\\nbuild Using src/config-iphone-5c-yellow.json to generate templates/iphone-5c-yellow.js\\nbuild Using src/config-iphone-5s-black.json to generate templates/iphone-5s-black.js\\nbuild Using src/config-iphone-5s-gold.json to generate templates/iphone-5s-gold.js\\nbuild Using src/config-iphone-5s-white.json to generate templates/iphone-5s-white.js\\nbuild Using src/config-nexus-5.json to generate templates/nexus-5.js\\n\\nAll done. Have a nice day!\\n\\nThe newly generated template will appear under the templates/ directory.\\nThanks & Acknowledgements\\n\\niPhone 5s & 5c PSD by Louie Mantia\\nNexus 5 PSD by Victor Stuber\\nBobble Cursor Image from Josh Puckett\\'s FramerWebView\\nAndroid Wear templates by Patrick Keenan\\n\\n',\n",
       "  'watchers': '22',\n",
       "  'stars': '196',\n",
       "  'forks': '9',\n",
       "  'commits': '35'},\n",
       " {'language': 'JavaScript 96.1',\n",
       "  'readme': 'Библиотека блоков bem-bl\\nОписание библиотеки и примеры.\\n\\n',\n",
       "  'watchers': '42',\n",
       "  'stars': '196',\n",
       "  'forks': '85',\n",
       "  'commits': '1,291'},\n",
       " {'language': 'JavaScript 89.7',\n",
       "  'readme': 'NodeSource Docker Images\\nThe NodeSource docker images deliver NodeSource\\'s deb and rpm packages across all of our supported platforms! We offer version pinning, allowing your project to track major, minor, or patch versions of Node or iojs.\\nUsage\\nUse any one of our images as a base for your image. We suggest caching your package.json and npm install in layers to reduce build time:\\nFROM nodesource/jessie:0.12.7\\n\\n# cache package.json and node_modules to speed up builds\\nADD package.json package.json\\nRUN npm install\\n\\n# Add your source files\\nADD . .\\nCMD [\"npm\",\"start\"]\\nNotes\\n\\nNODE_ENV is set to production on these images. If you are using these images for development work, add the line: ENV NODE_ENV dev to your Dockerfile.\\n\\nImages\\nWe offer tags for tracking major and minor releases. These tags enable you to receive either security patches or new features, while avoiding breaking changes.\\nNote: some images may not have all tags, if a major/minor version is not supported on a particular release, these tags will not work.\\nFor example, Ubuntu Precise does not have any iojs tags, and Ubuntu Vivid does not have any tags before iojs-1.8.1.\\n\\nlatest points to the latest release of Node\\nLTS points to the latest LTS release of Node\\nargon points to the named LTS release of Node\\nX points to the latest release of Node X\\nX.Y points to the latest release of Node X.Y\\niojs points to the latest release of iojs\\niojs-X points to the latest minor release of iojs-X\\niojs-X.Y points to the latest patch release of iojs-X.Y\\n\\nDebian-based images\\n\\nDebian jessie - docker pull nodesource/jessie\\n\\nNode 0.10.30 - docker pull nodesource/jessie:0.10.30\\nNode 0.10.31 - docker pull nodesource/jessie:0.10.31\\nNode 0.10.32 - docker pull nodesource/jessie:0.10.32\\nNode 0.10.33 - docker pull nodesource/jessie:0.10.33\\nNode 0.10.34 - docker pull nodesource/jessie:0.10.34\\nNode 0.10.35 - docker pull nodesource/jessie:0.10.35\\nNode 0.10.36 - docker pull nodesource/jessie:0.10.36\\nNode 0.10.37 - docker pull nodesource/jessie:0.10.37\\nNode 0.10.38 - docker pull nodesource/jessie:0.10.38\\nNode 0.10.39 - docker pull nodesource/jessie:0.10.39\\nNode 0.10.40 - docker pull nodesource/jessie:0.10.40\\nNode 0.10.41 - docker pull nodesource/jessie:0.10.41\\nNode 0.10.42 - docker pull nodesource/jessie:0.10.42\\nNode 0.10.43 - docker pull nodesource/jessie:0.10.43\\nNode 0.10.44 - docker pull nodesource/jessie:0.10.44\\nNode 0.10.45 - docker pull nodesource/jessie:0.10.45\\nNode 0.10.46 - docker pull nodesource/jessie:0.10.46\\nNode 0.12.0 - docker pull nodesource/jessie:0.12.0\\nNode 0.12.10 - docker pull nodesource/jessie:0.12.10\\nNode 0.12.11 - docker pull nodesource/jessie:0.12.11\\nNode 0.12.12 - docker pull nodesource/jessie:0.12.12\\nNode 0.12.13 - docker pull nodesource/jessie:0.12.13\\nNode 0.12.14 - docker pull nodesource/jessie:0.12.14\\nNode 0.12.15 - docker pull nodesource/jessie:0.12.15\\nNode 0.12.2 - docker pull nodesource/jessie:0.12.2\\nNode 0.12.3 - docker pull nodesource/jessie:0.12.3\\nNode 0.12.4 - docker pull nodesource/jessie:0.12.4\\nNode 0.12.5 - docker pull nodesource/jessie:0.12.5\\nNode 0.12.6 - docker pull nodesource/jessie:0.12.6\\nNode 0.12.7 - docker pull nodesource/jessie:0.12.7\\nNode 0.12.8 - docker pull nodesource/jessie:0.12.8\\nNode 0.12.9 - docker pull nodesource/jessie:0.12.9\\nNode 4.0.0 - docker pull nodesource/jessie:4.0.0\\nNode 4.1.0 - docker pull nodesource/jessie:4.1.0\\nNode 4.1.1 - docker pull nodesource/jessie:4.1.1\\nNode 4.1.2 - docker pull nodesource/jessie:4.1.2\\nNode 4.2.0 - docker pull nodesource/jessie:4.2.0\\nNode 4.2.1 - docker pull nodesource/jessie:4.2.1\\nNode 4.2.2 - docker pull nodesource/jessie:4.2.2\\nNode 4.2.3 - docker pull nodesource/jessie:4.2.3\\nNode 4.2.4 - docker pull nodesource/jessie:4.2.4\\nNode 4.2.5 - docker pull nodesource/jessie:4.2.5\\nNode 4.2.6 - docker pull nodesource/jessie:4.2.6\\nNode 4.3.0 - docker pull nodesource/jessie:4.3.0\\nNode 4.3.1 - docker pull nodesource/jessie:4.3.1\\nNode 4.3.2 - docker pull nodesource/jessie:4.3.2\\nNode 4.4.0 - docker pull nodesource/jessie:4.4.0\\nNode 4.4.1 - docker pull nodesource/jessie:4.4.1\\nNode 4.4.2 - docker pull nodesource/jessie:4.4.2\\nNode 4.4.3 - docker pull nodesource/jessie:4.4.3\\nNode 4.4.4 - docker pull nodesource/jessie:4.4.4\\nNode 4.4.5 - docker pull nodesource/jessie:4.4.5\\nNode 4.4.6 - docker pull nodesource/jessie:4.4.6\\nNode 4.4.7 - docker pull nodesource/jessie:4.4.7\\nNode 6.0.0 - docker pull nodesource/jessie:6.0.0\\nNode 6.1.0 - docker pull nodesource/jessie:6.1.0\\nNode 6.2.0 - docker pull nodesource/jessie:6.2.0\\nNode 6.2.1 - docker pull nodesource/jessie:6.2.1\\nNode 6.2.2 - docker pull nodesource/jessie:6.2.2\\nNode 6.3.0 - docker pull nodesource/jessie:6.3.0\\nNode 6.3.1 - docker pull nodesource/jessie:6.3.1\\n\\n\\nDebian sid - docker pull nodesource/sid\\n\\nNode 0.10.30 - docker pull nodesource/sid:0.10.30\\nNode 0.10.31 - docker pull nodesource/sid:0.10.31\\nNode 0.10.32 - docker pull nodesource/sid:0.10.32\\nNode 0.10.33 - docker pull nodesource/sid:0.10.33\\nNode 0.10.34 - docker pull nodesource/sid:0.10.34\\nNode 0.10.35 - docker pull nodesource/sid:0.10.35\\nNode 0.10.36 - docker pull nodesource/sid:0.10.36\\nNode 0.10.37 - docker pull nodesource/sid:0.10.37\\nNode 0.10.38 - docker pull nodesource/sid:0.10.38\\nNode 0.10.39 - docker pull nodesource/sid:0.10.39\\nNode 0.10.40 - docker pull nodesource/sid:0.10.40\\nNode 0.10.41 - docker pull nodesource/sid:0.10.41\\nNode 0.10.42 - docker pull nodesource/sid:0.10.42\\nNode 0.10.43 - docker pull nodesource/sid:0.10.43\\nNode 0.10.44 - docker pull nodesource/sid:0.10.44\\nNode 0.10.45 - docker pull nodesource/sid:0.10.45\\nNode 0.10.46 - docker pull nodesource/sid:0.10.46\\nNode 0.12.0 - docker pull nodesource/sid:0.12.0\\nNode 0.12.10 - docker pull nodesource/sid:0.12.10\\nNode 0.12.11 - docker pull nodesource/sid:0.12.11\\nNode 0.12.12 - docker pull nodesource/sid:0.12.12\\nNode 0.12.13 - docker pull nodesource/sid:0.12.13\\nNode 0.12.14 - docker pull nodesource/sid:0.12.14\\nNode 0.12.15 - docker pull nodesource/sid:0.12.15\\nNode 0.12.2 - docker pull nodesource/sid:0.12.2\\nNode 0.12.3 - docker pull nodesource/sid:0.12.3\\nNode 0.12.4 - docker pull nodesource/sid:0.12.4\\nNode 0.12.5 - docker pull nodesource/sid:0.12.5\\nNode 0.12.6 - docker pull nodesource/sid:0.12.6\\nNode 0.12.7 - docker pull nodesource/sid:0.12.7\\nNode 0.12.8 - docker pull nodesource/sid:0.12.8\\nNode 0.12.9 - docker pull nodesource/sid:0.12.9\\nNode 4.0.0 - docker pull nodesource/sid:4.0.0\\nNode 4.1.0 - docker pull nodesource/sid:4.1.0\\nNode 4.1.1 - docker pull nodesource/sid:4.1.1\\nNode 4.1.2 - docker pull nodesource/sid:4.1.2\\nNode 4.2.0 - docker pull nodesource/sid:4.2.0\\nNode 4.2.1 - docker pull nodesource/sid:4.2.1\\nNode 4.2.2 - docker pull nodesource/sid:4.2.2\\nNode 4.2.3 - docker pull nodesource/sid:4.2.3\\nNode 4.2.4 - docker pull nodesource/sid:4.2.4\\nNode 4.2.5 - docker pull nodesource/sid:4.2.5\\nNode 4.2.6 - docker pull nodesource/sid:4.2.6\\nNode 4.3.0 - docker pull nodesource/sid:4.3.0\\nNode 4.3.1 - docker pull nodesource/sid:4.3.1\\nNode 4.3.2 - docker pull nodesource/sid:4.3.2\\nNode 4.4.0 - docker pull nodesource/sid:4.4.0\\nNode 4.4.1 - docker pull nodesource/sid:4.4.1\\nNode 4.4.2 - docker pull nodesource/sid:4.4.2\\nNode 4.4.3 - docker pull nodesource/sid:4.4.3\\nNode 4.4.4 - docker pull nodesource/sid:4.4.4\\nNode 4.4.5 - docker pull nodesource/sid:4.4.5\\nNode 4.4.6 - docker pull nodesource/sid:4.4.6\\nNode 4.4.7 - docker pull nodesource/sid:4.4.7\\nNode 6.0.0 - docker pull nodesource/sid:6.0.0\\nNode 6.1.0 - docker pull nodesource/sid:6.1.0\\nNode 6.2.0 - docker pull nodesource/sid:6.2.0\\nNode 6.2.1 - docker pull nodesource/sid:6.2.1\\nNode 6.2.2 - docker pull nodesource/sid:6.2.2\\nNode 6.3.0 - docker pull nodesource/sid:6.3.0\\nNode 6.3.1 - docker pull nodesource/sid:6.3.1\\n\\n\\nDebian wheezy - docker pull nodesource/wheezy\\n\\nNode 0.10.30 - docker pull nodesource/wheezy:0.10.30\\nNode 0.10.31 - docker pull nodesource/wheezy:0.10.31\\nNode 0.10.32 - docker pull nodesource/wheezy:0.10.32\\nNode 0.10.33 - docker pull nodesource/wheezy:0.10.33\\nNode 0.10.34 - docker pull nodesource/wheezy:0.10.34\\nNode 0.10.35 - docker pull nodesource/wheezy:0.10.35\\nNode 0.10.36 - docker pull nodesource/wheezy:0.10.36\\nNode 0.10.37 - docker pull nodesource/wheezy:0.10.37\\nNode 0.10.38 - docker pull nodesource/wheezy:0.10.38\\nNode 0.10.39 - docker pull nodesource/wheezy:0.10.39\\nNode 0.10.40 - docker pull nodesource/wheezy:0.10.40\\nNode 0.10.41 - docker pull nodesource/wheezy:0.10.41\\nNode 0.10.42 - docker pull nodesource/wheezy:0.10.42\\nNode 0.10.43 - docker pull nodesource/wheezy:0.10.43\\nNode 0.10.44 - docker pull nodesource/wheezy:0.10.44\\nNode 0.10.45 - docker pull nodesource/wheezy:0.10.45\\nNode 0.10.46 - docker pull nodesource/wheezy:0.10.46\\nNode 0.12.0 - docker pull nodesource/wheezy:0.12.0\\nNode 0.12.10 - docker pull nodesource/wheezy:0.12.10\\nNode 0.12.11 - docker pull nodesource/wheezy:0.12.11\\nNode 0.12.12 - docker pull nodesource/wheezy:0.12.12\\nNode 0.12.13 - docker pull nodesource/wheezy:0.12.13\\nNode 0.12.14 - docker pull nodesource/wheezy:0.12.14\\nNode 0.12.15 - docker pull nodesource/wheezy:0.12.15\\nNode 0.12.2 - docker pull nodesource/wheezy:0.12.2\\nNode 0.12.3 - docker pull nodesource/wheezy:0.12.3\\nNode 0.12.4 - docker pull nodesource/wheezy:0.12.4\\nNode 0.12.5 - docker pull nodesource/wheezy:0.12.5\\nNode 0.12.6 - docker pull nodesource/wheezy:0.12.6\\nNode 0.12.7 - docker pull nodesource/wheezy:0.12.7\\nNode 0.12.8 - docker pull nodesource/wheezy:0.12.8\\nNode 0.12.9 - docker pull nodesource/wheezy:0.12.9\\nNode 4.1.1 - docker pull nodesource/wheezy:4.1.1\\nNode 4.1.2 - docker pull nodesource/wheezy:4.1.2\\nNode 4.2.0 - docker pull nodesource/wheezy:4.2.0\\nNode 4.2.1 - docker pull nodesource/wheezy:4.2.1\\nNode 4.2.2 - docker pull nodesource/wheezy:4.2.2\\nNode 4.2.3 - docker pull nodesource/wheezy:4.2.3\\nNode 4.2.4 - docker pull nodesource/wheezy:4.2.4\\nNode 4.2.5 - docker pull nodesource/wheezy:4.2.5\\nNode 4.2.6 - docker pull nodesource/wheezy:4.2.6\\nNode 4.3.0 - docker pull nodesource/wheezy:4.3.0\\nNode 4.3.1 - docker pull nodesource/wheezy:4.3.1\\nNode 4.3.2 - docker pull nodesource/wheezy:4.3.2\\nNode 4.4.0 - docker pull nodesource/wheezy:4.4.0\\nNode 4.4.1 - docker pull nodesource/wheezy:4.4.1\\nNode 4.4.2 - docker pull nodesource/wheezy:4.4.2\\nNode 4.4.3 - docker pull nodesource/wheezy:4.4.3\\nNode 4.4.4 - docker pull nodesource/wheezy:4.4.4\\nNode 4.4.5 - docker pull nodesource/wheezy:4.4.5\\nNode 4.4.6 - docker pull nodesource/wheezy:4.4.6\\nNode 4.4.7 - docker pull nodesource/wheezy:4.4.7\\nNode 6.0.0 - docker pull nodesource/wheezy:6.0.0\\nNode 6.1.0 - docker pull nodesource/wheezy:6.1.0\\nNode 6.2.0 - docker pull nodesource/wheezy:6.2.0\\nNode 6.2.1 - docker pull nodesource/wheezy:6.2.1\\nNode 6.2.2 - docker pull nodesource/wheezy:6.2.2\\nNode 6.3.0 - docker pull nodesource/wheezy:6.3.0\\nNode 6.3.1 - docker pull nodesource/wheezy:6.3.1\\n\\n\\n\\nUbuntu-based images\\n\\nUbuntu precise - docker pull nodesource/precise\\n\\nNode 0.10.30 - docker pull nodesource/precise:0.10.30\\nNode 0.10.31 - docker pull nodesource/precise:0.10.31\\nNode 0.10.32 - docker pull nodesource/precise:0.10.32\\nNode 0.10.33 - docker pull nodesource/precise:0.10.33\\nNode 0.10.34 - docker pull nodesource/precise:0.10.34\\nNode 0.10.35 - docker pull nodesource/precise:0.10.35\\nNode 0.10.36 - docker pull nodesource/precise:0.10.36\\nNode 0.10.37 - docker pull nodesource/precise:0.10.37\\nNode 0.10.38 - docker pull nodesource/precise:0.10.38\\nNode 0.10.39 - docker pull nodesource/precise:0.10.39\\nNode 0.10.40 - docker pull nodesource/precise:0.10.40\\nNode 0.10.41 - docker pull nodesource/precise:0.10.41\\nNode 0.10.42 - docker pull nodesource/precise:0.10.42\\nNode 0.10.43 - docker pull nodesource/precise:0.10.43\\nNode 0.10.44 - docker pull nodesource/precise:0.10.44\\nNode 0.10.45 - docker pull nodesource/precise:0.10.45\\nNode 0.10.46 - docker pull nodesource/precise:0.10.46\\nNode 0.12.0 - docker pull nodesource/precise:0.12.0\\nNode 0.12.10 - docker pull nodesource/precise:0.12.10\\nNode 0.12.11 - docker pull nodesource/precise:0.12.11\\nNode 0.12.12 - docker pull nodesource/precise:0.12.12\\nNode 0.12.13 - docker pull nodesource/precise:0.12.13\\nNode 0.12.14 - docker pull nodesource/precise:0.12.14\\nNode 0.12.15 - docker pull nodesource/precise:0.12.15\\nNode 0.12.2 - docker pull nodesource/precise:0.12.2\\nNode 0.12.3 - docker pull nodesource/precise:0.12.3\\nNode 0.12.4 - docker pull nodesource/precise:0.12.4\\nNode 0.12.5 - docker pull nodesource/precise:0.12.5\\nNode 0.12.6 - docker pull nodesource/precise:0.12.6\\nNode 0.12.7 - docker pull nodesource/precise:0.12.7\\nNode 0.12.8 - docker pull nodesource/precise:0.12.8\\nNode 0.12.9 - docker pull nodesource/precise:0.12.9\\nNode 4.1.1 - docker pull nodesource/precise:4.1.1\\nNode 4.1.2 - docker pull nodesource/precise:4.1.2\\nNode 4.2.0 - docker pull nodesource/precise:4.2.0\\nNode 4.2.1 - docker pull nodesource/precise:4.2.1\\nNode 4.2.2 - docker pull nodesource/precise:4.2.2\\nNode 4.2.3 - docker pull nodesource/precise:4.2.3\\nNode 4.2.4 - docker pull nodesource/precise:4.2.4\\nNode 4.2.5 - docker pull nodesource/precise:4.2.5\\nNode 4.2.6 - docker pull nodesource/precise:4.2.6\\nNode 4.3.0 - docker pull nodesource/precise:4.3.0\\nNode 4.3.1 - docker pull nodesource/precise:4.3.1\\nNode 4.3.2 - docker pull nodesource/precise:4.3.2\\nNode 4.4.0 - docker pull nodesource/precise:4.4.0\\nNode 4.4.1 - docker pull nodesource/precise:4.4.1\\nNode 4.4.2 - docker pull nodesource/precise:4.4.2\\nNode 4.4.3 - docker pull nodesource/precise:4.4.3\\nNode 4.4.4 - docker pull nodesource/precise:4.4.4\\nNode 4.4.5 - docker pull nodesource/precise:4.4.5\\nNode 4.4.6 - docker pull nodesource/precise:4.4.6\\nNode 4.4.7 - docker pull nodesource/precise:4.4.7\\nNode 6.0.0 - docker pull nodesource/precise:6.0.0\\nNode 6.1.0 - docker pull nodesource/precise:6.1.0\\nNode 6.2.0 - docker pull nodesource/precise:6.2.0\\nNode 6.2.1 - docker pull nodesource/precise:6.2.1\\nNode 6.2.2 - docker pull nodesource/precise:6.2.2\\nNode 6.3.0 - docker pull nodesource/precise:6.3.0\\nNode 6.3.1 - docker pull nodesource/precise:6.3.1\\n\\n\\nUbuntu trusty - docker pull nodesource/trusty\\n\\nNode 0.10.30 - docker pull nodesource/trusty:0.10.30\\nNode 0.10.31 - docker pull nodesource/trusty:0.10.31\\nNode 0.10.32 - docker pull nodesource/trusty:0.10.32\\nNode 0.10.33 - docker pull nodesource/trusty:0.10.33\\nNode 0.10.34 - docker pull nodesource/trusty:0.10.34\\nNode 0.10.35 - docker pull nodesource/trusty:0.10.35\\nNode 0.10.36 - docker pull nodesource/trusty:0.10.36\\nNode 0.10.37 - docker pull nodesource/trusty:0.10.37\\nNode 0.10.38 - docker pull nodesource/trusty:0.10.38\\nNode 0.10.39 - docker pull nodesource/trusty:0.10.39\\nNode 0.10.40 - docker pull nodesource/trusty:0.10.40\\nNode 0.10.41 - docker pull nodesource/trusty:0.10.41\\nNode 0.10.42 - docker pull nodesource/trusty:0.10.42\\nNode 0.10.43 - docker pull nodesource/trusty:0.10.43\\nNode 0.10.44 - docker pull nodesource/trusty:0.10.44\\nNode 0.10.45 - docker pull nodesource/trusty:0.10.45\\nNode 0.10.46 - docker pull nodesource/trusty:0.10.46\\nNode 0.12.0 - docker pull nodesource/trusty:0.12.0\\nNode 0.12.10 - docker pull nodesource/trusty:0.12.10\\nNode 0.12.11 - docker pull nodesource/trusty:0.12.11\\nNode 0.12.12 - docker pull nodesource/trusty:0.12.12\\nNode 0.12.13 - docker pull nodesource/trusty:0.12.13\\nNode 0.12.14 - docker pull nodesource/trusty:0.12.14\\nNode 0.12.15 - docker pull nodesource/trusty:0.12.15\\nNode 0.12.2 - docker pull nodesource/trusty:0.12.2\\nNode 0.12.3 - docker pull nodesource/trusty:0.12.3\\nNode 0.12.4 - docker pull nodesource/trusty:0.12.4\\nNode 0.12.5 - docker pull nodesource/trusty:0.12.5\\nNode 0.12.6 - docker pull nodesource/trusty:0.12.6\\nNode 0.12.7 - docker pull nodesource/trusty:0.12.7\\nNode 0.12.8 - docker pull nodesource/trusty:0.12.8\\nNode 0.12.9 - docker pull nodesource/trusty:0.12.9\\nNode 4.0.0 - docker pull nodesource/trusty:4.0.0\\nNode 4.1.0 - docker pull nodesource/trusty:4.1.0\\nNode 4.1.1 - docker pull nodesource/trusty:4.1.1\\nNode 4.1.2 - docker pull nodesource/trusty:4.1.2\\nNode 4.2.0 - docker pull nodesource/trusty:4.2.0\\nNode 4.2.1 - docker pull nodesource/trusty:4.2.1\\nNode 4.2.2 - docker pull nodesource/trusty:4.2.2\\nNode 4.2.3 - docker pull nodesource/trusty:4.2.3\\nNode 4.2.4 - docker pull nodesource/trusty:4.2.4\\nNode 4.2.5 - docker pull nodesource/trusty:4.2.5\\nNode 4.2.6 - docker pull nodesource/trusty:4.2.6\\nNode 4.3.0 - docker pull nodesource/trusty:4.3.0\\nNode 4.3.1 - docker pull nodesource/trusty:4.3.1\\nNode 4.3.2 - docker pull nodesource/trusty:4.3.2\\nNode 4.4.0 - docker pull nodesource/trusty:4.4.0\\nNode 4.4.1 - docker pull nodesource/trusty:4.4.1\\nNode 4.4.2 - docker pull nodesource/trusty:4.4.2\\nNode 4.4.3 - docker pull nodesource/trusty:4.4.3\\nNode 4.4.4 - docker pull nodesource/trusty:4.4.4\\nNode 4.4.5 - docker pull nodesource/trusty:4.4.5\\nNode 4.4.6 - docker pull nodesource/trusty:4.4.6\\nNode 4.4.7 - docker pull nodesource/trusty:4.4.7\\nNode 6.0.0 - docker pull nodesource/trusty:6.0.0\\nNode 6.1.0 - docker pull nodesource/trusty:6.1.0\\nNode 6.2.0 - docker pull nodesource/trusty:6.2.0\\nNode 6.2.1 - docker pull nodesource/trusty:6.2.1\\nNode 6.2.2 - docker pull nodesource/trusty:6.2.2\\nNode 6.3.0 - docker pull nodesource/trusty:6.3.0\\nNode 6.3.1 - docker pull nodesource/trusty:6.3.1\\n\\n\\nUbuntu vivid - docker pull nodesource/vivid\\n\\nNode 0.10.38 - docker pull nodesource/vivid:0.10.38\\nNode 0.10.39 - docker pull nodesource/vivid:0.10.39\\nNode 0.10.40 - docker pull nodesource/vivid:0.10.40\\nNode 0.10.41 - docker pull nodesource/vivid:0.10.41\\nNode 0.10.42 - docker pull nodesource/vivid:0.10.42\\nNode 0.10.43 - docker pull nodesource/vivid:0.10.43\\nNode 0.10.44 - docker pull nodesource/vivid:0.10.44\\nNode 0.12.10 - docker pull nodesource/vivid:0.12.10\\nNode 0.12.11 - docker pull nodesource/vivid:0.12.11\\nNode 0.12.12 - docker pull nodesource/vivid:0.12.12\\nNode 0.12.13 - docker pull nodesource/vivid:0.12.13\\nNode 0.12.2 - docker pull nodesource/vivid:0.12.2\\nNode 0.12.3 - docker pull nodesource/vivid:0.12.3\\nNode 0.12.4 - docker pull nodesource/vivid:0.12.4\\nNode 0.12.5 - docker pull nodesource/vivid:0.12.5\\nNode 0.12.6 - docker pull nodesource/vivid:0.12.6\\nNode 0.12.7 - docker pull nodesource/vivid:0.12.7\\nNode 0.12.8 - docker pull nodesource/vivid:0.12.8\\nNode 0.12.9 - docker pull nodesource/vivid:0.12.9\\nNode 4.0.0 - docker pull nodesource/vivid:4.0.0\\nNode 4.1.0 - docker pull nodesource/vivid:4.1.0\\nNode 4.1.1 - docker pull nodesource/vivid:4.1.1\\nNode 4.1.2 - docker pull nodesource/vivid:4.1.2\\nNode 4.2.0 - docker pull nodesource/vivid:4.2.0\\nNode 4.2.1 - docker pull nodesource/vivid:4.2.1\\nNode 4.2.2 - docker pull nodesource/vivid:4.2.2\\nNode 4.2.3 - docker pull nodesource/vivid:4.2.3\\nNode 4.2.4 - docker pull nodesource/vivid:4.2.4\\nNode 4.2.5 - docker pull nodesource/vivid:4.2.5\\nNode 4.2.6 - docker pull nodesource/vivid:4.2.6\\nNode 4.3.0 - docker pull nodesource/vivid:4.3.0\\nNode 4.3.1 - docker pull nodesource/vivid:4.3.1\\nNode 4.3.2 - docker pull nodesource/vivid:4.3.2\\nNode 4.4.0 - docker pull nodesource/vivid:4.4.0\\nNode 4.4.1 - docker pull nodesource/vivid:4.4.1\\nNode 4.4.2 - docker pull nodesource/vivid:4.4.2\\n\\n\\nUbuntu wily - docker pull nodesource/wily\\n\\nNode 0.10.44 - docker pull nodesource/wily:0.10.44\\nNode 0.10.45 - docker pull nodesource/wily:0.10.45\\nNode 0.10.46 - docker pull nodesource/wily:0.10.46\\nNode 0.12.10 - docker pull nodesource/wily:0.12.10\\nNode 0.12.11 - docker pull nodesource/wily:0.12.11\\nNode 0.12.12 - docker pull nodesource/wily:0.12.12\\nNode 0.12.13 - docker pull nodesource/wily:0.12.13\\nNode 0.12.14 - docker pull nodesource/wily:0.12.14\\nNode 0.12.15 - docker pull nodesource/wily:0.12.15\\nNode 0.12.8 - docker pull nodesource/wily:0.12.8\\nNode 0.12.9 - docker pull nodesource/wily:0.12.9\\nNode 4.2.1 - docker pull nodesource/wily:4.2.1\\nNode 4.2.2 - docker pull nodesource/wily:4.2.2\\nNode 4.2.3 - docker pull nodesource/wily:4.2.3\\nNode 4.2.4 - docker pull nodesource/wily:4.2.4\\nNode 4.2.5 - docker pull nodesource/wily:4.2.5\\nNode 4.2.6 - docker pull nodesource/wily:4.2.6\\nNode 4.3.0 - docker pull nodesource/wily:4.3.0\\nNode 4.3.1 - docker pull nodesource/wily:4.3.1\\nNode 4.3.2 - docker pull nodesource/wily:4.3.2\\nNode 4.4.0 - docker pull nodesource/wily:4.4.0\\nNode 4.4.1 - docker pull nodesource/wily:4.4.1\\nNode 4.4.2 - docker pull nodesource/wily:4.4.2\\nNode 4.4.3 - docker pull nodesource/wily:4.4.3\\nNode 4.4.4 - docker pull nodesource/wily:4.4.4\\nNode 4.4.5 - docker pull nodesource/wily:4.4.5\\nNode 4.4.6 - docker pull nodesource/wily:4.4.6\\nNode 4.4.7 - docker pull nodesource/wily:4.4.7\\nNode 6.0.0 - docker pull nodesource/wily:6.0.0\\nNode 6.1.0 - docker pull nodesource/wily:6.1.0\\nNode 6.2.0 - docker pull nodesource/wily:6.2.0\\nNode 6.2.1 - docker pull nodesource/wily:6.2.1\\nNode 6.2.2 - docker pull nodesource/wily:6.2.2\\nNode 6.3.0 - docker pull nodesource/wily:6.3.0\\nNode 6.3.1 - docker pull nodesource/wily:6.3.1\\n\\n\\nUbuntu xenial - docker pull nodesource/xenial\\n\\nNode 0.10.44 - docker pull nodesource/xenial:0.10.44\\nNode 0.10.45 - docker pull nodesource/xenial:0.10.45\\nNode 0.10.46 - docker pull nodesource/xenial:0.10.46\\nNode 0.12.13 - docker pull nodesource/xenial:0.12.13\\nNode 0.12.14 - docker pull nodesource/xenial:0.12.14\\nNode 0.12.15 - docker pull nodesource/xenial:0.12.15\\nNode 4.4.2 - docker pull nodesource/xenial:4.4.2\\nNode 4.4.3 - docker pull nodesource/xenial:4.4.3\\nNode 4.4.4 - docker pull nodesource/xenial:4.4.4\\nNode 4.4.5 - docker pull nodesource/xenial:4.4.5\\nNode 4.4.6 - docker pull nodesource/xenial:4.4.6\\nNode 4.4.7 - docker pull nodesource/xenial:4.4.7\\nNode 6.0.0 - docker pull nodesource/xenial:6.0.0\\nNode 6.1.0 - docker pull nodesource/xenial:6.1.0\\nNode 6.2.0 - docker pull nodesource/xenial:6.2.0\\nNode 6.2.1 - docker pull nodesource/xenial:6.2.1\\nNode 6.2.2 - docker pull nodesource/xenial:6.2.2\\nNode 6.3.0 - docker pull nodesource/xenial:6.3.0\\nNode 6.3.1 - docker pull nodesource/xenial:6.3.1\\n\\n\\n\\nFedora-based images\\n\\nFedora 20 - docker pull nodesource/fedora20\\n\\nNode 0.10.31 - docker pull nodesource/fedora20:0.10.31\\nNode 0.10.32 - docker pull nodesource/fedora20:0.10.32\\nNode 0.10.33 - docker pull nodesource/fedora20:0.10.33\\nNode 0.10.34 - docker pull nodesource/fedora20:0.10.34\\nNode 0.10.35 - docker pull nodesource/fedora20:0.10.35\\nNode 0.10.36 - docker pull nodesource/fedora20:0.10.36\\nNode 0.10.38 - docker pull nodesource/fedora20:0.10.38\\nNode 0.10.39 - docker pull nodesource/fedora20:0.10.39\\nNode 0.10.40 - docker pull nodesource/fedora20:0.10.40\\nNode 0.12.1 - docker pull nodesource/fedora20:0.12.1\\nNode 0.12.2 - docker pull nodesource/fedora20:0.12.2\\nNode 0.12.3 - docker pull nodesource/fedora20:0.12.3\\nNode 0.12.5 - docker pull nodesource/fedora20:0.12.5\\nNode 0.12.6 - docker pull nodesource/fedora20:0.12.6\\nNode 0.12.7 - docker pull nodesource/fedora20:0.12.7\\nNode 4.0.0 - docker pull nodesource/fedora20:4.0.0\\nNode 4.1.0 - docker pull nodesource/fedora20:4.1.0\\nNode 4.1.1 - docker pull nodesource/fedora20:4.1.1\\nNode 4.1.2 - docker pull nodesource/fedora20:4.1.2\\nNode 4.2.0 - docker pull nodesource/fedora20:4.2.0\\nNode 4.2.1 - docker pull nodesource/fedora20:4.2.1\\n\\n\\nFedora 21 - docker pull nodesource/fedora21\\n\\nNode 0.10.35 - docker pull nodesource/fedora21:0.10.35\\nNode 0.10.36 - docker pull nodesource/fedora21:0.10.36\\nNode 0.10.38 - docker pull nodesource/fedora21:0.10.38\\nNode 0.10.39 - docker pull nodesource/fedora21:0.10.39\\nNode 0.10.40 - docker pull nodesource/fedora21:0.10.40\\nNode 0.10.41 - docker pull nodesource/fedora21:0.10.41\\nNode 0.12.1 - docker pull nodesource/fedora21:0.12.1\\nNode 0.12.2 - docker pull nodesource/fedora21:0.12.2\\nNode 0.12.3 - docker pull nodesource/fedora21:0.12.3\\nNode 0.12.5 - docker pull nodesource/fedora21:0.12.5\\nNode 0.12.6 - docker pull nodesource/fedora21:0.12.6\\nNode 0.12.7 - docker pull nodesource/fedora21:0.12.7\\nNode 0.12.8 - docker pull nodesource/fedora21:0.12.8\\nNode 0.12.9 - docker pull nodesource/fedora21:0.12.9\\nNode 4.0.0 - docker pull nodesource/fedora21:4.0.0\\nNode 4.1.0 - docker pull nodesource/fedora21:4.1.0\\nNode 4.1.1 - docker pull nodesource/fedora21:4.1.1\\nNode 4.1.2 - docker pull nodesource/fedora21:4.1.2\\nNode 4.2.0 - docker pull nodesource/fedora21:4.2.0\\nNode 4.2.1 - docker pull nodesource/fedora21:4.2.1\\nNode 4.2.2 - docker pull nodesource/fedora21:4.2.2\\nNode 4.2.3 - docker pull nodesource/fedora21:4.2.3\\nNode 4.2.4 - docker pull nodesource/fedora21:4.2.4\\nNode 4.2.5 - docker pull nodesource/fedora21:4.2.5\\nNode 4.2.6 - docker pull nodesource/fedora21:4.2.6\\nNode 5.0.0 - docker pull nodesource/fedora21:5.0.0\\nNode 5.1.0 - docker pull nodesource/fedora21:5.1.0\\nNode 5.1.1 - docker pull nodesource/fedora21:5.1.1\\nNode 5.2.0 - docker pull nodesource/fedora21:5.2.0\\nNode 5.3.0 - docker pull nodesource/fedora21:5.3.0\\nNode 5.4.0 - docker pull nodesource/fedora21:5.4.0\\nNode 5.4.1 - docker pull nodesource/fedora21:5.4.1\\nNode 5.5.0 - docker pull nodesource/fedora21:5.5.0\\n\\n\\nFedora 22 - docker pull nodesource/fedora22\\n\\nNode 0.10.40 - docker pull nodesource/fedora22:0.10.40\\nNode 0.10.41 - docker pull nodesource/fedora22:0.10.41\\nNode 0.10.42 - docker pull nodesource/fedora22:0.10.42\\nNode 0.10.43 - docker pull nodesource/fedora22:0.10.43\\nNode 0.10.44 - docker pull nodesource/fedora22:0.10.44\\nNode 0.10.45 - docker pull nodesource/fedora22:0.10.45\\nNode 0.10.46 - docker pull nodesource/fedora22:0.10.46\\nNode 0.12.10 - docker pull nodesource/fedora22:0.12.10\\nNode 0.12.11 - docker pull nodesource/fedora22:0.12.11\\nNode 0.12.12 - docker pull nodesource/fedora22:0.12.12\\nNode 0.12.13 - docker pull nodesource/fedora22:0.12.13\\nNode 0.12.14 - docker pull nodesource/fedora22:0.12.14\\nNode 0.12.15 - docker pull nodesource/fedora22:0.12.15\\nNode 0.12.7 - docker pull nodesource/fedora22:0.12.7\\nNode 0.12.8 - docker pull nodesource/fedora22:0.12.8\\nNode 0.12.9 - docker pull nodesource/fedora22:0.12.9\\nNode 4.0.0 - docker pull nodesource/fedora22:4.0.0\\nNode 4.1.0 - docker pull nodesource/fedora22:4.1.0\\nNode 4.1.1 - docker pull nodesource/fedora22:4.1.1\\nNode 4.1.2 - docker pull nodesource/fedora22:4.1.2\\nNode 4.2.0 - docker pull nodesource/fedora22:4.2.0\\nNode 4.2.1 - docker pull nodesource/fedora22:4.2.1\\nNode 4.2.2 - docker pull nodesource/fedora22:4.2.2\\nNode 4.2.3 - docker pull nodesource/fedora22:4.2.3\\nNode 4.2.4 - docker pull nodesource/fedora22:4.2.4\\nNode 4.2.5 - docker pull nodesource/fedora22:4.2.5\\nNode 4.2.6 - docker pull nodesource/fedora22:4.2.6\\nNode 4.3.0 - docker pull nodesource/fedora22:4.3.0\\nNode 4.3.1 - docker pull nodesource/fedora22:4.3.1\\nNode 4.3.2 - docker pull nodesource/fedora22:4.3.2\\nNode 4.4.0 - docker pull nodesource/fedora22:4.4.0\\nNode 4.4.1 - docker pull nodesource/fedora22:4.4.1\\nNode 4.4.2 - docker pull nodesource/fedora22:4.4.2\\nNode 4.4.3 - docker pull nodesource/fedora22:4.4.3\\nNode 4.4.4 - docker pull nodesource/fedora22:4.4.4\\nNode 4.4.5 - docker pull nodesource/fedora22:4.4.5\\nNode 4.4.6 - docker pull nodesource/fedora22:4.4.6\\nNode 4.4.7 - docker pull nodesource/fedora22:4.4.7\\nNode 5.0.0 - docker pull nodesource/fedora22:5.0.0\\nNode 5.1.0 - docker pull nodesource/fedora22:5.1.0\\nNode 5.1.1 - docker pull nodesource/fedora22:5.1.1\\nNode 5.10.0 - docker pull nodesource/fedora22:5.10.0\\nNode 5.10.1 - docker pull nodesource/fedora22:5.10.1\\nNode 5.11.0 - docker pull nodesource/fedora22:5.11.0\\nNode 5.11.1 - docker pull nodesource/fedora22:5.11.1\\nNode 5.12.0 - docker pull nodesource/fedora22:5.12.0\\nNode 5.2.0 - docker pull nodesource/fedora22:5.2.0\\nNode 5.3.0 - docker pull nodesource/fedora22:5.3.0\\nNode 5.4.0 - docker pull nodesource/fedora22:5.4.0\\nNode 5.4.1 - docker pull nodesource/fedora22:5.4.1\\nNode 5.5.0 - docker pull nodesource/fedora22:5.5.0\\nNode 5.6.0 - docker pull nodesource/fedora22:5.6.0\\nNode 5.7.0 - docker pull nodesource/fedora22:5.7.0\\nNode 5.7.1 - docker pull nodesource/fedora22:5.7.1\\nNode 5.8.0 - docker pull nodesource/fedora22:5.8.0\\nNode 5.9.0 - docker pull nodesource/fedora22:5.9.0\\nNode 5.9.1 - docker pull nodesource/fedora22:5.9.1\\nNode 6.0.0 - docker pull nodesource/fedora22:6.0.0\\nNode 6.1.0 - docker pull nodesource/fedora22:6.1.0\\nNode 6.2.0 - docker pull nodesource/fedora22:6.2.0\\nNode 6.2.1 - docker pull nodesource/fedora22:6.2.1\\nNode 6.2.2 - docker pull nodesource/fedora22:6.2.2\\nNode 6.3.0 - docker pull nodesource/fedora22:6.3.0\\n\\n\\nFedora 23 - docker pull nodesource/fedora23\\n\\nNode 0.10.44 - docker pull nodesource/fedora23:0.10.44\\nNode 0.10.45 - docker pull nodesource/fedora23:0.10.45\\nNode 0.10.46 - docker pull nodesource/fedora23:0.10.46\\nNode 0.10.43 - docker pull nodesource/fedora23:0.10.43\\nNode 0.12.11 - docker pull nodesource/fedora23:0.12.11\\nNode 0.12.12 - docker pull nodesource/fedora23:0.12.12\\nNode 0.12.13 - docker pull nodesource/fedora23:0.12.13\\nNode 0.12.14 - docker pull nodesource/fedora23:0.12.14\\nNode 0.12.15 - docker pull nodesource/fedora23:0.12.15\\nNode 4.2.1 - docker pull nodesource/fedora23:4.2.1\\nNode 4.2.2 - docker pull nodesource/fedora23:4.2.2\\nNode 4.2.3 - docker pull nodesource/fedora23:4.2.3\\nNode 4.2.4 - docker pull nodesource/fedora23:4.2.4\\nNode 4.2.5 - docker pull nodesource/fedora23:4.2.5\\nNode 4.2.6 - docker pull nodesource/fedora23:4.2.6\\nNode 4.3.0 - docker pull nodesource/fedora23:4.3.0\\nNode 4.3.1 - docker pull nodesource/fedora23:4.3.1\\nNode 4.3.2 - docker pull nodesource/fedora23:4.3.2\\nNode 4.4.0 - docker pull nodesource/fedora23:4.4.0\\nNode 4.4.1 - docker pull nodesource/fedora23:4.4.1\\nNode 4.4.2 - docker pull nodesource/fedora23:4.4.2\\nNode 4.4.3 - docker pull nodesource/fedora23:4.4.3\\nNode 4.4.4 - docker pull nodesource/fedora23:4.4.4\\nNode 4.4.5 - docker pull nodesource/fedora23:4.4.5\\nNode 4.4.6 - docker pull nodesource/fedora23:4.4.6\\nNode 4.4.7 - docker pull nodesource/fedora23:4.4.7\\nNode 5.0.0 - docker pull nodesource/fedora23:5.0.0\\nNode 5.1.0 - docker pull nodesource/fedora23:5.1.0\\nNode 5.1.1 - docker pull nodesource/fedora23:5.1.1\\nNode 5.10.0 - docker pull nodesource/fedora23:5.10.0\\nNode 5.10.1 - docker pull nodesource/fedora23:5.10.1\\nNode 5.11.0 - docker pull nodesource/fedora23:5.11.0\\nNode 5.11.1 - docker pull nodesource/fedora23:5.11.1\\nNode 5.12.0 - docker pull nodesource/fedora23:5.12.0\\nNode 5.2.0 - docker pull nodesource/fedora23:5.2.0\\nNode 5.3.0 - docker pull nodesource/fedora23:5.3.0\\nNode 5.4.0 - docker pull nodesource/fedora23:5.4.0\\nNode 5.4.1 - docker pull nodesource/fedora23:5.4.1\\nNode 5.5.0 - docker pull nodesource/fedora23:5.5.0\\nNode 5.6.0 - docker pull nodesource/fedora23:5.6.0\\nNode 5.7.0 - docker pull nodesource/fedora23:5.7.0\\nNode 5.7.1 - docker pull nodesource/fedora23:5.7.1\\nNode 5.8.0 - docker pull nodesource/fedora23:5.8.0\\nNode 5.9.0 - docker pull nodesource/fedora23:5.9.0\\nNode 5.9.1 - docker pull nodesource/fedora23:5.9.1\\nNode 6.0.0 - docker pull nodesource/fedora23:6.0.0\\nNode 6.1.0 - docker pull nodesource/fedora23:6.1.0\\nNode 6.2.0 - docker pull nodesource/fedora23:6.2.0\\nNode 6.2.1 - docker pull nodesource/fedora23:6.2.1\\nNode 6.2.2 - docker pull nodesource/fedora23:6.2.2\\nNode 6.3.0 - docker pull nodesource/fedora23:6.3.0\\nNode 6.3.1 - docker pull nodesource/fedora23:6.3.1\\n\\n\\nFedora 24 - docker pull nodesource/fedora24\\n\\nNode 0.12.15 - docker pull nodesource/fedora24:0.12.15\\nNode 4.4.6 - docker pull nodesource/fedora24:4.4.6\\nNode 4.4.7 - docker pull nodesource/fedora24:4.4.7\\nNode 5.12.0 - docker pull nodesource/fedora24:5.12.0\\nNode 6.2.2 - docker pull nodesource/fedora24:6.2.2\\nNode 6.3.0 - docker pull nodesource/fedora24:6.3.0\\nNode 6.3.1 - docker pull nodesource/fedora24:6.3.1\\n\\n\\n\\nCentos-based images\\n\\nCentos 5 - docker pull nodesource/centos5\\n\\nNode 0.10.31 - docker pull nodesource/centos5:0.10.31\\nNode 0.10.32 - docker pull nodesource/centos5:0.10.32\\nNode 0.10.33 - docker pull nodesource/centos5:0.10.33\\nNode 0.10.34 - docker pull nodesource/centos5:0.10.34\\nNode 0.10.35 - docker pull nodesource/centos5:0.10.35\\nNode 0.10.36 - docker pull nodesource/centos5:0.10.36\\nNode 0.10.38 - docker pull nodesource/centos5:0.10.38\\nNode 0.10.39 - docker pull nodesource/centos5:0.10.39\\nNode 0.10.40 - docker pull nodesource/centos5:0.10.40\\nNode 0.10.41 - docker pull nodesource/centos5:0.10.41\\nNode 0.10.42 - docker pull nodesource/centos5:0.10.42\\nNode 0.10.43 - docker pull nodesource/centos5:0.10.43\\nNode 0.10.44 - docker pull nodesource/centos5:0.10.44\\nNode 0.10.45 - docker pull nodesource/centos5:0.10.45\\nNode 0.10.46 - docker pull nodesource/centos5:0.10.46\\n\\n\\nCentos 6 - docker pull nodesource/centos6\\n\\nNode 0.10.31 - docker pull nodesource/centos6:0.10.31\\nNode 0.10.32 - docker pull nodesource/centos6:0.10.32\\nNode 0.10.33 - docker pull nodesource/centos6:0.10.33\\nNode 0.10.34 - docker pull nodesource/centos6:0.10.34\\nNode 0.10.35 - docker pull nodesource/centos6:0.10.35\\nNode 0.10.36 - docker pull nodesource/centos6:0.10.36\\nNode 0.10.38 - docker pull nodesource/centos6:0.10.38\\nNode 0.10.39 - docker pull nodesource/centos6:0.10.39\\nNode 0.10.40 - docker pull nodesource/centos6:0.10.40\\nNode 0.10.41 - docker pull nodesource/centos6:0.10.41\\nNode 0.10.42 - docker pull nodesource/centos6:0.10.42\\nNode 0.10.43 - docker pull nodesource/centos6:0.10.43\\nNode 0.10.44 - docker pull nodesource/centos6:0.10.44\\nNode 0.10.45 - docker pull nodesource/centos6:0.10.45\\nNode 0.10.46 - docker pull nodesource/centos6:0.10.46\\nNode 0.12.1 - docker pull nodesource/centos6:0.12.1\\nNode 0.12.10 - docker pull nodesource/centos6:0.12.10\\nNode 0.12.11 - docker pull nodesource/centos6:0.12.11\\nNode 0.12.12 - docker pull nodesource/centos6:0.12.12\\nNode 0.12.13 - docker pull nodesource/centos6:0.12.13\\nNode 0.12.14 - docker pull nodesource/centos6:0.12.14\\nNode 0.12.15 - docker pull nodesource/centos6:0.12.15\\nNode 0.12.2 - docker pull nodesource/centos6:0.12.2\\nNode 0.12.3 - docker pull nodesource/centos6:0.12.3\\nNode 0.12.5 - docker pull nodesource/centos6:0.12.5\\nNode 0.12.6 - docker pull nodesource/centos6:0.12.6\\nNode 0.12.7 - docker pull nodesource/centos6:0.12.7\\nNode 0.12.8 - docker pull nodesource/centos6:0.12.8\\nNode 0.12.9 - docker pull nodesource/centos6:0.12.9\\nNode 4.2.6 - docker pull nodesource/centos6:4.2.6\\nNode 4.3.0 - docker pull nodesource/centos6:4.3.0\\nNode 4.3.1 - docker pull nodesource/centos6:4.3.1\\nNode 4.3.2 - docker pull nodesource/centos6:4.3.2\\nNode 4.4.0 - docker pull nodesource/centos6:4.4.0\\nNode 4.4.1 - docker pull nodesource/centos6:4.4.1\\nNode 4.4.2 - docker pull nodesource/centos6:4.4.2\\nNode 4.4.3 - docker pull nodesource/centos6:4.4.3\\nNode 4.4.4 - docker pull nodesource/centos6:4.4.4\\nNode 4.4.5 - docker pull nodesource/centos6:4.4.5\\nNode 4.4.6 - docker pull nodesource/centos6:4.4.6\\nNode 4.4.7 - docker pull nodesource/centos6:4.4.7\\nNode 5.10.0 - docker pull nodesource/centos6:5.10.0\\nNode 5.10.1 - docker pull nodesource/centos6:5.10.1\\nNode 5.11.0 - docker pull nodesource/centos6:5.11.0\\nNode 5.11.1 - docker pull nodesource/centos6:5.11.1\\nNode 5.12.0 - docker pull nodesource/centos6:5.12.0\\nNode 5.5.0 - docker pull nodesource/centos6:5.5.0\\nNode 5.6.0 - docker pull nodesource/centos6:5.6.0\\nNode 5.7.0 - docker pull nodesource/centos6:5.7.0\\nNode 5.7.1 - docker pull nodesource/centos6:5.7.1\\nNode 5.8.0 - docker pull nodesource/centos6:5.8.0\\nNode 5.9.0 - docker pull nodesource/centos6:5.9.0\\nNode 5.9.1 - docker pull nodesource/centos6:5.9.1\\nNode 6.0.0 - docker pull nodesource/centos6:6.0.0\\nNode 6.1.0 - docker pull nodesource/centos6:6.1.0\\nNode 6.2.0 - docker pull nodesource/centos6:6.2.0\\nNode 6.2.1 - docker pull nodesource/centos6:6.2.1\\nNode 6.3.0 - docker pull nodesource/centos6:6.3.0\\nNode 6.3.1 - docker pull nodesource/centos6:6.3.1\\n\\n\\nCentos 7 - docker pull nodesource/centos7\\n\\nNode 0.10.31 - docker pull nodesource/centos7:0.10.31\\nNode 0.10.32 - docker pull nodesource/centos7:0.10.32\\nNode 0.10.33 - docker pull nodesource/centos7:0.10.33\\nNode 0.10.34 - docker pull nodesource/centos7:0.10.34\\nNode 0.10.35 - docker pull nodesource/centos7:0.10.35\\nNode 0.10.36 - docker pull nodesource/centos7:0.10.36\\nNode 0.10.38 - docker pull nodesource/centos7:0.10.38\\nNode 0.10.39 - docker pull nodesource/centos7:0.10.39\\nNode 0.10.40 - docker pull nodesource/centos7:0.10.40\\nNode 0.10.41 - docker pull nodesource/centos7:0.10.41\\nNode 0.10.42 - docker pull nodesource/centos7:0.10.42\\nNode 0.10.43 - docker pull nodesource/centos7:0.10.43\\nNode 0.10.44 - docker pull nodesource/centos7:0.10.44\\nNode 0.10.45 - docker pull nodesource/centos7:0.10.45\\nNode 0.10.46 - docker pull nodesource/centos7:0.10.46\\nNode 0.12.1 - docker pull nodesource/centos7:0.12.1\\nNode 0.12.10 - docker pull nodesource/centos7:0.12.10\\nNode 0.12.11 - docker pull nodesource/centos7:0.12.11\\nNode 0.12.12 - docker pull nodesource/centos7:0.12.12\\nNode 0.12.13 - docker pull nodesource/centos7:0.12.13\\nNode 0.12.14 - docker pull nodesource/centos7:0.12.14\\nNode 0.12.15 - docker pull nodesource/centos7:0.12.15\\nNode 0.12.2 - docker pull nodesource/centos7:0.12.2\\nNode 0.12.3 - docker pull nodesource/centos7:0.12.3\\nNode 0.12.5 - docker pull nodesource/centos7:0.12.5\\nNode 0.12.6 - docker pull nodesource/centos7:0.12.6\\nNode 0.12.7 - docker pull nodesource/centos7:0.12.7\\nNode 0.12.8 - docker pull nodesource/centos7:0.12.8\\nNode 0.12.9 - docker pull nodesource/centos7:0.12.9\\nNode 4.0.0 - docker pull nodesource/centos7:4.0.0\\nNode 4.1.0 - docker pull nodesource/centos7:4.1.0\\nNode 4.1.1 - docker pull nodesource/centos7:4.1.1\\nNode 4.1.2 - docker pull nodesource/centos7:4.1.2\\nNode 4.2.0 - docker pull nodesource/centos7:4.2.0\\nNode 4.2.1 - docker pull nodesource/centos7:4.2.1\\nNode 4.2.2 - docker pull nodesource/centos7:4.2.2\\nNode 4.2.3 - docker pull nodesource/centos7:4.2.3\\nNode 4.2.4 - docker pull nodesource/centos7:4.2.4\\nNode 4.2.5 - docker pull nodesource/centos7:4.2.5\\nNode 4.2.6 - docker pull nodesource/centos7:4.2.6\\nNode 4.3.0 - docker pull nodesource/centos7:4.3.0\\nNode 4.3.1 - docker pull nodesource/centos7:4.3.1\\nNode 4.3.2 - docker pull nodesource/centos7:4.3.2\\nNode 4.4.0 - docker pull nodesource/centos7:4.4.0\\nNode 4.4.1 - docker pull nodesource/centos7:4.4.1\\nNode 4.4.2 - docker pull nodesource/centos7:4.4.2\\nNode 4.4.3 - docker pull nodesource/centos7:4.4.3\\nNode 4.4.4 - docker pull nodesource/centos7:4.4.4\\nNode 4.4.5 - docker pull nodesource/centos7:4.4.5\\nNode 4.4.6 - docker pull nodesource/centos7:4.4.6\\nNode 4.4.7 - docker pull nodesource/centos7:4.4.7\\nNode 5.0.0 - docker pull nodesource/centos7:5.0.0\\nNode 5.1.0 - docker pull nodesource/centos7:5.1.0\\nNode 5.1.1 - docker pull nodesource/centos7:5.1.1\\nNode 5.10.0 - docker pull nodesource/centos7:5.10.0\\nNode 5.10.1 - docker pull nodesource/centos7:5.10.1\\nNode 5.11.0 - docker pull nodesource/centos7:5.11.0\\nNode 5.11.1 - docker pull nodesource/centos7:5.11.1\\nNode 5.12.0 - docker pull nodesource/centos7:5.12.0\\nNode 5.2.0 - docker pull nodesource/centos7:5.2.0\\nNode 5.3.0 - docker pull nodesource/centos7:5.3.0\\nNode 5.4.0 - docker pull nodesource/centos7:5.4.0\\nNode 5.4.1 - docker pull nodesource/centos7:5.4.1\\nNode 5.5.0 - docker pull nodesource/centos7:5.5.0\\nNode 5.6.0 - docker pull nodesource/centos7:5.6.0\\nNode 5.7.0 - docker pull nodesource/centos7:5.7.0\\nNode 5.7.1 - docker pull nodesource/centos7:5.7.1\\nNode 5.8.0 - docker pull nodesource/centos7:5.8.0\\nNode 5.9.0 - docker pull nodesource/centos7:5.9.0\\nNode 5.9.1 - docker pull nodesource/centos7:5.9.1\\nNode 6.0.0 - docker pull nodesource/centos7:6.0.0\\nNode 6.1.0 - docker pull nodesource/centos7:6.1.0\\nNode 6.2.0 - docker pull nodesource/centos7:6.2.0\\nNode 6.2.1 - docker pull nodesource/centos7:6.2.1\\nNode 6.2.2 - docker pull nodesource/centos7:6.2.2\\nNode 6.3.0 - docker pull nodesource/centos7:6.3.0\\nNode 6.3.1 - docker pull nodesource/centos7:6.3.1\\n\\n\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '195',\n",
       "  'forks': '19',\n",
       "  'commits': '167'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'jquery-timing\\nA jQuery plugin that provides easy-to-use timing methods to write less and do more.\\nExamples and complete reference can be found at http://creativecouple.github.com/jquery-timing\\nAuthor\\nCreativeCouple - Peter Liske\\nLicense\\nMIT License\\nThe MIT License (MIT)\\nCopyright (c) 2012 CreativeCouple, Peter Liske\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '195',\n",
       "  'forks': '11',\n",
       "  'commits': '223'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'DEPRECATION NOTICE!\\nThis project is deprecated in favor of chjj/marked. I never created the parser myself, the module was created in the early days of node as a wrapper for an existing browser based parser Showdown so if you are using this module and have problems with the parsing logic, I can\\'t help you much as I\\'m not familiar with the inner details.\\nPull requests are still welcomed - if you find a bug and fix it, then I\\'ll pull the change in but I won\\'t be fixing the bugs myself. Sorry for that.\\nnode-markdown\\nnode-markdown is based on Showdown parser and is meant to parse Markdown syntax into HTML code.\\nInstallation\\nUse npm package manager\\nnpm install node-markdown\\n\\nUsage\\nInclude Markdown parser\\nvar md = require(\"node-markdown\").Markdown;\\n\\nParse Markdown syntax into HTML\\nvar html = md(\"**markdown** string\");\\n\\nAllow only default set of HTML tags to be used\\nvar html = md(\"**markdown** string\", true);\\n\\nAllow only specified HTML tags to be used (default set of allowed attributes is used)\\nvar html = md(\"**markdown** string\", true, \"p|strong|span\");\\n\\nAllow specified HTML tags and specified attributes\\nvar html = md(\"**markdown** string\", true, \"p|strong|span\", {\\n    \"a\":\"href\",        // \\'href\\' for links\\n    \"*\":\"title|style\"  // \\'title\\' and \\'style\\' for all\\n});\\n\\nComplete example\\nvar md_text = \"**bold** *italic* [link](http://www.neti.ee) `code block`\",\\n    md_parser = require(\"node-markdown\").Markdown;\\n\\n// simple\\nconsole.log(md_parser(md_text));\\n\\n// limit HTML tags and attributes\\nconsole.log(md_parser(md_text, true, \\'h1|p|span\\'));\\n\\n// limit HTML tags and keep attributes for allowed tags\\nvar allowedTags = \\'a|img\\';\\n    allowedAttributes = {\\n        \\'a\\':\\'href|style\\',\\n        \\'img\\': \\'src\\',\\n        \\'*\\': \\'title\\'\\n    }\\nconsole.log(md_parser(md_text, true, allowedTags, allowedAttributes));\\n\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '195',\n",
       "  'forks': '18',\n",
       "  'commits': '14'},\n",
       " {'language': 'JavaScript 86.2',\n",
       "  'readme': 'jCarouselLite - Original Version\\nThis is the original version of jCarouselLite.\\nCurrent stable version is 1.1.\\nFor detailed documentation visit the Project Page\\n\\nProject Page\\nInstallation Instruction\\nDefault & Custom Styling\\nDemos\\nDocumentation\\nChange Log\\n\\n',\n",
       "  'watchers': '23',\n",
       "  'stars': '192',\n",
       "  'forks': '112',\n",
       "  'commits': '27'},\n",
       " {'language': 'JavaScript 89.6',\n",
       "  'readme': 'Parse for AngularJS\\nThis is pre-alpha/actively developed. There are no guarantees of\\nstability, but you are welcome to play around and submit issues\\nangular-parse is an AngularJS module for\\ninteracting with the Parse REST\\nAPI. It does not utlize the Parse\\nJavaScript API but instead is built\\nfrom (mostly) scratch. The reason is the existing Parse JavaScript API\\nis not ideal for AngularJS applications.\\nWhy Angular-Parse\\nThere are a few things that are not ideal about the existing Parse\\nJavaScript API in AngularJS. The existing API is modeled after Backbone\\nModels and the main problem is setters\\nare used instead of object properties. instance.set(\\'property\\', \\'value\\')\\ndoesn\\'t really fit well with things like ng-model\\nInstead, angular-parse is based loosely on Spine\\nModels where properties directly\\ndefined on the object are used. To facilitate this, when defining a\\nmodel, it is \"configured\" by supplying the class name (as defined in\\nParse) as well as which properties are part of that class.\\nAngular-parse also uses promises for any methods making network calls.\\nGetting started\\nInclude the JavaScript file\\n<!-- Include AngularJS -->\\n<script src=\"path/to/angular-parse.js\"></script>\\nMake sure to add \"Parse\" as a dependency of your main module\\nvar app = angular.module(\"YourApp\", [\"Parse\"])\\nAngular-parse also requires you provide the value \"ParseConfig\" as an\\nobject with the following format\\napp.config(function (ParseProvider) {\\n  ParseProvider.initialize(\"PARSE_APPLICATION_ID\", \"PARSE_REST_API_KEY\");\\n});\\nDefining Models\\nYou can define models by extending Parse.Model. You must call configure\\non the class and pass it the Parse class name, and the name of any\\nattributes of that class\\nUsing CoffeeScript:\\napp.factory \\'Car\\', (Parse) ->\\n  class Car extends Parse.model\\n    @configure \"Car\", \"make\", \"model\", \"year\"\\n\\n    @customClassMethod: (arg) ->\\n      # add custom class methods like this\\n\\n    customInstanceMethod: (arg) ->\\n      # add custom instance methods like this\\nUsing JavaScript:\\n// Not implemented yet, sorry\\nUsing Models\\nA model acts much the same as a normal JavaScript object with a\\nconstructor\\nCreating a new instance\\nYou can create a new instance by using new. Any attributes passed in\\nwill be set on the instance. This does not save it to parse, that must\\nbe done with .save(). The save method returns a promise, which is\\nfulfilled with the instance itself.\\nvar car = new Car({\\n  make: \"Scion\",\\n  model: \"xB\",\\n  year: 2008\\n});\\n\\ncar.isNew() === true;\\ncar.objectId == null;\\n\\ncar.save().then(function (_car) {\\n  _car === car;\\n  car.isNew() === false;\\n  car.objectId === \"...aParseId\";\\n  car.createdAt === \"...aDateString\";\\n  car.updatedAt === \"...aDateString\";\\n}\\nIf the object has an objectId, it will be updated properly, and will not\\ncreate a new instance. save() can be used either for new or existing\\nrecords.\\nGetting an instance By Id\\nThe find method on your model class takes an objectId, and returns a\\npromise that will be fulfilled with your instance if it exists.\\nCar.find(\"someObjectId\").then(function (car) {\\n  car.objectId === \"someObjectId\";\\n})\\nDestroying an instance\\nThe destroy method on an instance will destroy it set destroyed to true\\nand set the item\\'s objectId to null\\nCar.find(\"someObjectId\").then(function (car) {\\n  car.objectId === \"someObjectId\";\\n\\n  car.destroy().then(function (_car) {\\n    car === _car;\\n    car.destroyed === true;\\n    car.isNew() === true;\\n    car.objectId === null;\\n  })\\n})\\nDefining a custom user class\\nA simple User class is provided to you. However, you can subclass it:\\nangular.module(\\'Parse\\').factory \\'ParseCustomUser\\', (ParseDefaultUser) ->\\n      class CustomUser extends ParseDefaultUser\\n        @configure \\'users\\', \\'username\\', \\'password\\', \\'property\\'\\nIn this manner, all User instances returned by the Parse methods\\nwill be of your custom class.\\nContributing\\nPull requests and issues are welcome.\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '190',\n",
       "  'forks': '39',\n",
       "  'commits': '13'},\n",
       " {'language': 'JavaScript 71.8',\n",
       "  'readme': 'Screw.Unit is a Behavior-Driven Testing Framework for Javascript. It features nested describes. Its goals are to provide:\\n\\na DSL for elegant, readable, organized specs;\\nan interactive runner that can execute focused specs and describes;\\nand brief, extensible source-code.\\n\\nWhat it is\\n\\nThe testing language is closure-based. Consider,\\ndescribe(\"Matchers\", function() {\\n  it(\"invokes the provided matcher on a call to expect\", function() {\\n    expect(true).to(equal, true);\\n    expect(true).to_not(equal, false);\\n  });\\n});\\n\\nA key feature of Screw.Unit are nested describes and the cascading before (and after) behavior that entails:\\ndescribe(\"a nested describe\", function() {\\n  var invocations = [];\\n  \\n  before(function() {\\n    invocations.push(\"before\");\\n  });\\n\\n  describe(\"a doubly nested describe\", function() {\\n    before(function() {\\n      invocations.push(\\'inner before\\');\\n    });\\n\\n    it(\"runs befores in all ancestors prior to an it\", function() {\\n      expect(invocations).to(equal, [\"before\", \"inner before\"]);\\n    });\\n  });\\n});\\n\\nThe Runner\\nThe Screw.Unit runner is pretty fancy, supporting focused describes and focused its:\\n\\nClick on a describe or it to run just those tests.\\nGlobal Befores and Afters\\nA global before is a before block run before all tests in a test suite, regardless of their nesting. This is often useful to reset global variables, or blank-out DOM nodes before each test is run. Put this at the top of the your suite file or in your spec helper.\\nScrew.Unit(function(c) { with(c) {\\n  before(function() { ... });\\n}});\\n\\nNote that you can have any number of Screw.Unit(...) blocks in one file. Thus, you can have multiple global befores and afters.\\nCustom Matchers\\nA custom matcher is a custom assertion specifically tailored to your application. These are helpful in increasing the readability and declarativity of your tests. To create a custom matcher, fill in the blanks for this code:\\nScrew.Matchers[\"be_even\"] = {\\n  match: function(expected, actual) {\\n    return actual % 2 == 0;\\n  },\\n  failure_message: function(expected, actual, not) {\\n    return \\'expected \\' + $.print(actual) + (not ? \\' not\\' : \\'\\') + \\' to be even\\';\\n  }\\n}\\n\\nYou can invoke this matcher as follows: expect(2).to(be_even).\\nThe Anatomy of Test Infrastructure\\nTypical test infrastructure spans multiple files:\\n\\nA suite.html file that has the necessary html, script tags, and link tags, to include your source code as well as the test infrastructure.\\nA spec_helper.js file with global before and after blocks.\\nA set of custom matchers.\\nYour individual tests.\\n\\nThe file structure will typically look like:\\nspec/\\n  suite.html\\n  spec_helper.js\\n  matchers/\\n    a_matcher.js\\n    another_matcher.js\\n  models/\\n    a_spec.js\\n    another_spec.js\\n  views/\\n    yet_another_spec.js\\n\\nThe models and views directories are here only for comparison. As a general rule, mirror the file structure of your source code in your spec directory. For example, if you have an MVC application and you organize your source code into models, views, and controllers directories, have parallel directories in your spec/ directory, with tests for your models, views, and controllers in their respective directories.\\nWriting Good Tests\\nA great test maximizes these features:\\n\\nit provides documentation, explaining the intended functioning of the system as well as how the source code works;\\nit supports ongoing development, as you bit-by-bit write a failing test and make it pass;\\nit supports later refactoring and prevents regression as you add other features;\\nand it requires little modification as the implementation of the system changes, especially changes to unrelated code.\\n\\nThis section focuses principally on tests as documentation. To provide documentation, as well as support future modification, a test should be readable and well organized. Here are some recommendations on how to do just that.\\nUse Nested Describes to Express Context\\nOften, when you test a system (a function, an object), it behaves differently in different contexts. Use nested describes liberally to express the context under which you make an assertion.\\ndescribe(\"Caller#prioritize\", function() {\\n  describe(\"when there are two callers in the queue\", function() {\\n    describe(\"and one caller has been waiting longer than another\", function() {\\n      ...\\n    });\\n  });\\n});\\n\\nIn addition to using nested describes to express context, use them to organize tests by the structural properties of your source code and programming language. In Javascript this is typically prototype and function. A parent describe for a prototype contains nested describes for each of its methods. If you have cross-cutting concerns (e.g., related behavior that spans across methods or prototypes), use a describe to group them conceptually.\\ndescribe(\"Car\", function() {\\n  describe(\"#start\", function() {\\n  });\\n  \\n  describe(\"#stop\", function() {\\n  });\\n  \\n  describe(\"callbacks\", function() {\\n    describe(\"after_purchase\", function() {\\n    });\\n  });\\n  \\n  describe(\"logging\", function() {\\n  });\\n});\\n\\nIn this example, one parent describe is used for all Car behavior. There is a describe for each method. Finally, cross-cutting concerns like callbacks and logging are grouped because of their conceptual affinity.\\nTest Size\\nIndividual tests should be short and sweet. It is sometimes recommended to make only one assertion per test:\\nit(\"chooses the caller who has been waiting the longest\", function() {\\n  expect(Caller.prioritize()).to(equal, caller_waiting_the_longest);\\n});\\n\\nAccording to some, the ideal test is one line of code. In practice, it may be excessive to divide your tests to be this small. At ten lines of code (or more), a test is difficult to read quickly. Be pragmatic, bearing in mind the aims of testing.\\nAlthough one assertion per test is a good rule of thumb, feel free to violate the rule if equal clarity and better terseness is achievable:\\nit(\"returns the string representation of the boolean\", function() {\\n  expect($.print(true)).to(equal, \\'true\\');\\n  expect($.print(false)).to(equal, \\'false\\');\\n});\\n\\nTwo tests would be overkill in this example.\\nVariable Naming\\nName variables descriptively, especially ones that will become expected values in assertions. caller_waiting_the_longest is better than c1.\\nDividing code between tests and befores\\nIf there is only one line of setup and it is used in only one test, it may be better to include the setup in the test itself:\\nit(\"decrements the man\\'s luck by 5\", function() {\\n  var man = new Man({luck: 5});\\n  \\n  cat.cross_path(man);\\n  expect(man.luck()).to(equal, 0);\\n});\\n\\nBut in general, it\\'s nice to keep setup code in before blocks, especially if the setup can be shared across tests.\\ndescribe(\\'Man\\', function() {\\n  var man;\\n  before(function() {\\n    man = new Man({luck: 5});\\n  });\\n\\n  describe(\\'#decrement_luck\\', function() {\\n    it(\"decrements the luck field by the given amount\", function() {\\n      man.decrement_luck(3);\\n      expect(man.luck()).to(equal, 2)\\n    });\\n  });\\n  ...\\n});\\n\\nPreconditions\\nIt is ideal, if there is any chance that your preconditions are non-obvious, to make precondition asserts in your test. The last example, were it more complicated, might be better written:\\nit(\"decrements the luck field by the given amount\", function() {\\n  expect(man.luck()).to(equal, 5);\\n\\n  man.decrement_luck(3);\\n  expect(man.luck()).to(equal, 2)\\n});\\n\\nWhitespace, as seen here, can be helpful in distinguishing setup and preconditions from the system under test (SUT) and its assertions. It is nice to be consistent in your use of whitespace (e.g., \"always follow a group of preconditions by a newline\"). But it is better to use whitespace as makes the most sense given the context. As with everything in life, do it consciously and deliberately, but change your mind frequently.\\nBehavioral Testing\\nBehavioral testing, that is, asserting that certain functions are called rather than certain values returned, is best done with closures. The dynamic nature of JavaScript makes mocking frameworks mostly unnecessary.\\nit(\"invokes #decrement_luck\", function() {\\n  var decrement_luck_was_called = false;\\n  man.decrement_luck = function(amount) {\\n    decrement_luck_was_called = true;\\n  });\\n  \\n  cat.cross_path(man);\\n  expect(decrement_luck_was_called).to(equal, true);\\n});\\n\\nHow to Test the DOM\\nThe simplest way to test the DOM is to have a special DOM node in your suite.html file. Have all tests insert nodes into this node; have a global before reset the node between tests.\\nIn suite.html:\\n<div id=\"dom_test\"></div>\\n\\nIn spec_helper.js:\\nScrew.Unit(function() {\\n  before(function() {\\n    document.getElementById(\\'dom_test\\').innerHTML = \\'\\'; // but use your favorite JS library here.\\n  });\\n});\\n\\nIn some_spec.js:\\ndescribe(\"something that manipulates the DOM\", function() {\\n  it(\"is effortless to test!\", function() {\\n    var dom_test = document.getElementById(\\'dom_test\\');\\n    dom_test.innerHTML = \\'awesome\\';\\n    expect(dom_test.innerHTML).to(equal, \\'awesome\\');\\n  });\\n});\\n\\nA Javascript library like jQuery, Prototype, or YUI is a essential for testing events.\\nImplementation Details\\nScrew.Unit is implemented using some fancy metaprogramming learned from the formidable Yehuda Katz. This allows the describe and it functions to not pollute the global namespace. Essentially, we take the source code of your test and wrap it in a with block which provides a new scope:\\nvar contents = fn.toString().match(/^[^\\\\{]*{((.*\\\\n*)*)}/m)[1];\\nvar fn = new Function(\"matchers\", \"specifications\",\\n  \"with (specifications) { with (matchers) { \" + contents + \" } }\"\\n);\\n\\nfn.call(this, Screw.Matchers, Screw.Specifications);\\n\\nFurthermore, Screw.Unit is implemented using the Concrete Javascript style, which is made possible by the Effen plugin and jQuery. Concrete Javascript is an alternative to MVC. In Concrete Javascript, DOM objects serve as the model and view simultaneously. The DOM is constructed using semantic (and visual) markup, and behaviors are attached directly to DOM elements. For example,\\n$(\\'.describe\\').fn({\\n  parent: function() {\\n    return $(this).parent(\\'.describes\\').parent(\\'.describe\\');\\n  },\\n  run: function() {\\n    $(this).children(\\'.its\\').children(\\'.it\\').fn(\\'run\\');\\n    $(this).children(\\'.describes\\').children(\\'.describe\\').fn(\\'run\\');\\n  },\\n});\\n\\nHere two methods (#parent and #run) are attached directly to DOM elements that have class describe. To invoke one of these methods, simply:\\n$(\\'.describe\\').fn(\\'run\\');\\n\\nBind behaviors by passing a hash (see the previous example). Using CSS3 selectors and cascading to attach behaviors provides interesting kind of multiple inheritance and polymorphism:\\n$(\\'.describe, .it\\').fn({...}); // applies to both describe and its\\n$(\\'.describe .describe\\').fn({...}); // applies to nested describes only\\n\\nExtensibility\\nScrew.Unit is designed from the ground-up to be extensible. For example, to add custom logging, simply subscribe to certain events:\\n$(\\'.it\\')\\n  .bind(\\'enqueued\\', function() {...})\\n  .bind(\\'running\\', function() {...})\\n  .bind(\\'passed\\', function() {...})\\n  .bind(\\'failed\\', function(e, reason) {...})\\n\\nThere are also events for the loading and loaded test code code, as well as just before and just after all tests are run:\\n$(Screw)\\n  .bind(\\'loading\\', function() {...})\\n  .bind(\\'loaded\\', function() {...})\\n  .bind(\\'before\\', function() {...})\\n  .bind(\\'after\\', function() {...})\\n\\nDownload\\nYou can download the source from Github. There is are plenty of examples in the distribution.\\nThanks to\\n\\nNathan Sobo\\nYehuda Katz\\nBrian Takita\\nAman Gupta\\nTim Connor\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '189',\n",
       "  'forks': '51',\n",
       "  'commits': '275'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': '⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️\\nThis repository is no longer maintained, please use Shipit instead\\n⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️\\ngrunt-shipit\\n\\n\\n\\n\\nGrunt plugin for Shipit, an automation engine and a deployment tool written for node / iojs.\\nIf you prefer using Shipit without grunt, please go to Shipit repository.\\nGetting Started\\nThis plugin requires Grunt ~0.4.0\\nIf you haven\\'t used Grunt before, be sure to check out the Getting Started guide, as it explains how to create a Gruntfile as well as install and use Grunt plugins. Once you\\'re familiar with that process, you may install this plugin with this command:\\nnpm install grunt-shipit --save-dev\\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\\ngrunt.loadNpmTasks(\\'grunt-shipit\\');\\nUsage\\nExample Gruntfile.js\\nmodule.exports = function (grunt) {\\n  grunt.initConfig({\\n    shipit: {\\n      options: {\\n        workspace: \\'/tmp/github-monitor\\',\\n        deployTo: \\'/tmp/deploy_to\\',\\n        repositoryUrl: \\'https://github.com/user/repo.git\\',\\n        ignores: [\\'.git\\', \\'node_modules\\'],\\n        keepReleases: 2,\\n        key: \\'/path/to/key\\',\\n        shallowClone: true\\n      },\\n      staging: {\\n        servers: [\\'user@myserver.com\\', \\'user2@myserver2.com\\']\\n      }\\n    }\\n  });\\n\\n  grunt.loadNpmTasks(\\'grunt-shipit\\');\\n  grunt.loadNpmTasks(\\'shipit-deploy\\');\\n\\n  grunt.registerTask(\\'pwd\\', function () {\\n    grunt.shipit.remote(\\'pwd\\', this.async());\\n  });\\n};\\nLaunch command\\ngrunt shipit:<environment> <tasks ...>\\nFor more documentation about Shipit commands please refer to Shipit repository.\\nFor more documentation about Shipit deploy task, please refer to Shipit deploy repository.\\nUpgrading from v0.5.x\\nMethods\\nNow all methods returns promises, you can still use callback but the result has changed.\\nBefore:\\nshipit.remote(\\'echo \"hello\"\\', function (err, stdout, stderr) {\\n  console.log(stdout, stderr);\\n});\\nNow:\\nshipit.remote(\\'echo \"hello\"\\', function (err, res) {\\n  console.log(res.stdout, res.stderr);\\n});\\nDeployment task\\nThe deployment task is now separated from Shipit. You must install it and load it separately:\\nnpm install shipit-deploy\\n\\ngrunt.loadNpmTasks(\\'shipit-deploy\\');\\nAPI change\\nThe exposed property grunt.shipit.stage is now grunt.shipit.environment.\\nLicense\\nMIT\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '189',\n",
       "  'forks': '21',\n",
       "  'commits': '150'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Armstrong\\nArmstrong is an open-source publishing system designed for news organizations\\nthat gives your team the technology edge it needs to report in a media-rich\\nenvironment.\\nThis package is a meta package that loads all of the various components of\\nArmstrong.  Installing this package is the easiest way to get the full\\ndistribution of Armstrong, but is not required to use the various components of\\nArmstrong.\\n\\nGetting Started\\n\\nInstallation\\nFor the latest released version of Armstrong, use pip to install it from\\nPyPI like this:\\n$ pip install armstrong\\n\\nThe latest release is 12.03.1.  This is beta software, so please\\nkeep that in mind while developing on it.  While we are making every effort to\\nmaintain backwards compatibility between releases while in beta, things may\\nchange in ways that break your code.\\n\\nNote on virtualenv\\nWe recommend that you use virtualenv to isolate Armstrong.  We highly\\nrecommend that you use the --distribute flag when creating a virtual\\nenvironment, as that\\'s what we use for testing.  Your results with traditional\\nsetuptools may vary.\\n\\nDevelopment Releases\\nYou can track the latest development of Armstrong by installing the development\\nversion from Git.  Obtain the latest version by visiting our GitHub page and\\neither cloning or downloading a tarball.\\nOnce obtained, switch into the directory of the repository (or snapshot if a\\ntarball was downloaded) and tell pip to install it:\\n$ git clone git://github.com/armstrong/armstrong.git\\n... a few lines of output from Git ...\\n$ cd armstrong\\n$ pip install .\\n\\n\\nCreating an Armstrong project\\nTo help get started, the armstrong.cli component can create a basic project\\nstructure for you.  Create a new project like this:\\n$ armstrong init mysite\\narmstrong initialized!\\n\\nYou can initialize a project using the --template=demo parameter to\\ninitialize with a demo SQLite3 database already set up.  This provides a\\nworking example of how you can use Armstrong.\\n\\nArmstrong Project Structure\\nThe following files are created in the mysite directory:\\n|~fixtures/\\n| |-initial_data.json\\n|~requirements/\\n| |-development.txt\\n| `-project.txt\\n|~settings/\\n| |-__init__.py\\n| |-defaults.py\\n| |-development.py\\n| `-production.py\\n|~templates/\\n| `-index.html\\n|~urls/\\n| |-__init__.py\\n| |-defaults.py\\n| |-development.py\\n| `-production.py\\n|-wsgi.py\\n\\nThe settings directory contains your Django settings.  The\\nsettings.defaults module contains all of the base settings that are common\\nto your environment.  settings.development has settings specific to your\\ndevelopment environment, while settings.production contains all of your\\nproduction settings.\\nYou need to edit the settings.development and settings.production to\\nconfigure the database engine you want to use.\\nYou can also use the settings.local_development and\\nsettings.local_production modules to store values that are specific to a\\nparticular box.  You shouldn\\'t include these files in your\\nrepository---anything that should be shared should go in the appropriate\\nsettings module.\\nsettings.development and settings.production configure you\\nROOT_URLCONF as either urls.development or urls.production,\\nrespectively.  Like their settings.* counterparts, you can use these for\\nenvironment-specific settings while storing all of your default values in\\nurls.defaults.\\nAll of your requirements are specified inside the two text files in the\\nrequirements directory: development.txt and project.txt.  You can\\nuse pip to install the dependencies of your project by providing either file\\nas an argument to pip install -r.  development.txt should contain all\\nof requirements for your development environment and include project.txt.\\nThe project.txt file should contain all of requirements that you have\\nto have for your project.\\nThe templates directory is configured as the base for your project\\'s\\ntemplates.  It contains a simple index.html that is loaded on a request to\\n/ so you can verify that everything is setup correctly.\\nThe wsgi.py file provides a basic WSGI module for running your project.  It\\nis configured to run using the settings.development settings, so you must\\nadjust it prior to running in production.\\nNote: You do not have to use the Armstrong project layout.  You can utilize\\nall of Armstrong\\'s components inside an existing Django project.  These are\\nhere simply to help get you started.\\n\\nNext Steps\\nOnce you have the project created and configured (remember, you need to setup\\nyour database just like any other Django project), you\\'ve got two final steps.\\nFirst, you need to install the requirements file as there are packages that\\nArmstrong relies on that need to be installed from GitHub.\\n$ cd mysite\\n$ pip install -r requirements/project.txt\\n\\nAfter you\\'ve configured the database engine and installed the base\\nrequirements, you\\'re last step is to create the database .  You run armstrong\\nsyncdb which initial the database based on the apps listed in your\\nINSTALLED_APPS setting.  After this runs, you will have a database created\\nby Django (for more information on syncdb, see the Django docs).\\nFinally, now that you have all of the dependencies installed and have a\\ndatabase, you can test everything out by running armstrong runserver from\\ninside your project.  By default, it listens to the localhost on port\\n8000.  Loading that up should either give you the Welcome to Armstrong!\\npage or the demo site, depending on whether you used the --template=demo\\nflag when called armstrong init.\\nCongrats, you\\'re now setup and ready to start developing on Armstrong.\\n\\nVersions\\nArmstrong uses date-based versions for this main armstrong package.  The\\ncurrent release is 11.09.0.alpha.1.  For more information about how\\nversions are handled in Armstrong, see the Versions page on the wiki.\\n\\nChangelog\\n\\n12.03.0\\nThis updates the various packages to their current stable releases.\\n\\nDjango 1.4 Support\\nArmstrong now supports Django 1.4 and has maintained backwards\\ncompatibility with Django 1.3.1.\\nArmstrong Wells\\nWells now support allow empty wells (you must explicitly opt-in to the\\nnew styles), provides abstract models for creating custom well models\\nfrom and allows duplication in the admin.\\nArmstrong Sections\\nSections have undergone numerous small enhancements.  They now have a\\nbetter admin, are more signal friendly, and have support for only\\nshowing published items.\\nArmstrong Layouts\\nThe utils.render_model function now boasts configurable backends so\\nyou can customize how models are rendered.\\nRelated Content\\nBackwards Incompatible Changes: The internal representation of\\nfields have been changed to better reflect what they should.  A full\\nexplanation of all changes is available in the\\narmstrong.apps.related_content README.  No database migrations are\\nrequired for this new code.\\n\\n\\n11.12.0\\nThis updates the various packages to their current release.\\n\\nArmstrong Hatband\\nWe\\'ve updated the wells interface inside Hatband to make it more\\naccessible.\\nArmstrong Images\\nWe now include an ImageSet for dealing with, as you might have\\nguessed it, sets of Image models.  Thanks for @pizzapanther at\\nMouth Watering Media for the contribution.\\nImproved Related Content\\nWe\\'ve added better handling of Related Content, a new admin, and new\\nhelper fields for dealing with both sides of a related content\\nrelationship.\\nArmstrong CLI\\nWe\\'ve removed the --demo flag in favor of --template=demo\\nwhich provides more flexbility going forward.\\n\\n\\n11.09.0\\nThis updates the various packages to their current release.  In addition,\\nit adds armstrong.hatband and armstrong.core.arm_layout to the\\nmix.\\n\\nArmstrong Hatband\\nEvery good hat needs a hatband.  Armstrong\\'s Hatband app is the\\nfoundation for our enhancements to Django\\'s built-in admin interface.\\nWe\\'ve got lots planned for it, but there are a couple of things worth\\ncalling out specifically.\\n\\nIntegration with VisualSearch\\nWells now have a much better UI thanks VisualSearch.  This new\\nUI allows you to quickly search through all of your models when\\nattaching a new Node to a Well.\\nRich Text Editor\\nWe\\'ve added a new RichTextWidget that allows you to easily\\nconfigure the rich-text editor of your choice and have all of the\\nadmin fields across Armstrong switch to using it.  We\\'re shipping\\nwith CKEditor support built-in.\\n\\n\\nNew Demo Data\\nNow you can include the --demo parameter to armstrong init to\\nuse our demo database.  This includes lorem ipsum articles and some\\ndefault sections.\\nNew Layout Code\\narmstrong.core.arm_layout introduces the {% render_model %}\\ntemplate tag which handles switching the template used for rendering\\nmodels.\\nBackwards Incompatible Changes\\n\\narmstrong.core.arm_wells had all of its display logic moved to\\nthe new armstrong.core.arm_layout app.\\nWe\\'ve removed primary_section from ContentBase\\n\\n\\n\\n\\n11.06.0\\nThe first generally available release of Armstrong.  It is an unstable,\\ndeveloper preview.\\n\\n\\nComponents\\nArmstrong is broken down into multiple components.  The main armstrong\\npackage installs these individually with each being pinned to a specific\\npoint release.\\nIncluded in the 11.09 release are the following components:\\n\\narmstrong.cli\\nA command line tool for creating and working with an Armstrong environment.\\nYou can use this inside an Armstrong environment as a replacement for the\\ntraditional manage.py in Django.\\nSee the armstrong.cli repository for more information.\\n\\narmstrong.core.arm_content\\nContains the basic elements for Armstrong-style content.  This does not\\nprovide any concrete implementations of models, instead it includes lower\\nlevel functionality: fields, mixins, and a base ContentBase for\\ncreating a shared content model.\\nSee the armstrong.core.arm_content repository for more information.\\n\\narmstrong.core.arm_layout\\nContains helpers for managing the display of data in the context of its\\ncurrent layout.\\nSee the armstrong.core.arm_layout repository for more information.\\n\\narmstrong.core.arm_sections\\nProvides a system for structuring models into \"sections\" to be used on the\\nsite for organizational purposes.\\nSee the armstrong.core.arm_sections repository for more information.\\n\\narmstrong.core.arm_wells\\nFunctionality related to \"pinning\" content to a particular area.  Wells\\ngive you the ability to specify any collection of models and their order to\\ndisplay in various places throughout the site.\\nSee the armstrong.core.arm_wells repository for more information.\\n\\narmstrong.apps.articles\\nSimple application for handling basic articles.  This provides a thin layer\\non top of the article-specific features found in the arm_content\\ncomponent, but will meet the needs of many newsrooms with simple\\nrequirements.\\nSee the armstrong.apps.articles repository for more information.\\n\\narmstrong.apps.content\\nSimple application for providing a concrete Content model that other\\nDjango apps can build off of.\\nSee the armstrong.apps.content repository for more information.\\n\\narmstrong.hatband\\nArmstrong\\'s enhanced version of Django\\'s built-in django.contrib.admin\\napplication.\\nSee the armstrong.hatband repository for more information.\\n\\n\\n\\nContributing\\nStart by finding the component of Armstrong that you would like to change.  It\\nis rare that you will need to start by modifying the main Armstrong repository\\nto start.\\n\\nCreate something awesome -- make the code better, add some functionality,\\nwhatever (this is the hardest part).\\nFork it\\nCreate a topic branch to house your changes\\nGet all of your commits in the new topic branch\\nSubmit a pull request\\n\\n\\nState of Project\\nArmstrong is an open-source news platform that is freely available to any\\norganization.  It is the result of a collaboration between the The Texas Tribune\\nand The Bay Citizen, and a grant from the John S. and James L. Knight\\nFoundation.\\nTo follow development, be sure to join the Google Group.\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '140',\n",
       "  'forks': '22',\n",
       "  'commits': '90'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': '\\n===========\\nMafan - Toolkit for working with Chinese in Python\\nMafan is a collection of Python tools for making your life working with Chinese so much less 麻烦 (mafan, i.e. troublesome).\\nContained in here is an ever-growing collection of loosely-related tools, broken down into several files. These are:\\ninstallation\\nInstall through pip:\\npip install mafan\\n\\nencodings\\nencodings contains functions for converting files from any number of 麻烦 character encodings to something more sane (utf-8, by default). For example:\\nfrom mafan import encoding\\n\\nfilename = \\'ugly_big5.txt\\' # name or path of file as string\\nencoding.convert(filename) # creates a file with name \\'ugly_big5_utf-8.txt\\' in glorious utf-8 encoding\\ntext\\ntext contains some functions for working with strings. Things like detecting english in a string, whether a string has Chinese punctuation, etc. Check out text.py for all the latest goodness. It also contains a handy wrapper for the jianfan package for converting between simplified and traditional:\\n>>> from mafan import simplify, tradify\\n>>> string = u\\'这是麻烦啦\\'\\n>>> print tradify(string) # convert string to traditional\\n這是麻煩啦\\n>>> print simplify(tradify(string)) # convert back to simplified\\n这是麻烦啦\\nThe has_punctuation and contains_latin functions are useful for knowing whether you are really dealing with Chinese, or Chinese characters:\\n>>> from mafan import text\\n>>> text.has_punctuation(u\\'这是麻烦啦\\') # check for any Chinese punctuation (full-stops, commas, quotation marks, etc)\\nFalse\\n>>> text.has_punctuation(u\\'这是麻烦啦.\\')\\nFalse\\n>>> text.has_punctuation(u\\'这是麻烦啦。\\')\\nTrue\\n>>> text.contains_latin(u\\'这是麻烦啦。\\')\\nFalse\\n>>> text.contains_latin(u\\'You are麻烦啦。\\')\\nTrue\\nYou can also test whether sentences or documents use simplified characters, traditional characters, both or neither:\\n>>> import mafan\\n>>> from mafan import text\\n>>> text.is_simplified(u\\'这是麻烦啦\\')\\nTrue\\n>>> text.is_traditional(u\\'Hello,這是麻煩啦\\') # ignores non-chinese characters\\nTrue\\n\\n# Or done another way:\\n>>> text.identify(u\\'这是麻烦啦\\') is mafan.SIMPLIFIED\\nTrue\\n>>> text.identify(u\\'這是麻煩啦\\') is mafan.TRADITIONAL\\nTrue\\n>>> text.identify(u\\'这是麻烦啦! 這是麻煩啦\\') is mafan.BOTH\\nTrue\\n>>> text.identify(u\\'This is so mafan.\\') is mafan.NEITHER # or None\\nTrue\\nThe identification functionality is introduced as a very thin wrapper to Thomas Roten\\'s hanzidentifier, which is included as part of mafan.\\nAnother function that comes pre-built into Mafan is split_text, which tokenizes Chinese sentences into words:\\n>>> from mafan import split_text\\n>>> split_text(u\"這是麻煩啦\")\\n[u\\'\\\\u9019\\', u\\'\\\\u662f\\', u\\'\\\\u9ebb\\\\u7169\\', u\\'\\\\u5566\\']\\n>>> print \\' \\'.join(split_text(u\"這是麻煩啦\"))\\n這 是 麻煩 啦\\nYou can also optionally pass the boolean include_part_of_speech parameter to get tagged words back:\\n>>> split_text(u\"這是麻煩啦\", include_part_of_speech=True)\\n[(u\\'\\\\u9019\\', \\'r\\'), (u\\'\\\\u662f\\', \\'v\\'), (u\\'\\\\u9ebb\\\\u7169\\', \\'x\\'), (u\\'\\\\u5566\\', \\'y\\')]\\npinyin\\npinyin contains functions for working with or converting between pinyin. At the moment, the only function in there is one to convert numbered pinyin to the pinyin with correct tone marks. For example:\\n>>> from mafan import pinyin\\n>>> print pinyin.decode(\"ni3hao3\")\\nnǐhǎo\\ntraditional characters\\nIf you want to be able to use split_text on traditional characters, you can make use of one of two options:\\n\\nEither set an environment variable, MAFAN_DICTIONARY_PATH, to the absolute path to a local copy of this dictionary file,\\nor install the mafan_traditional convenience package: pip install mafan_traditional. If this package is installed and available, mafan will default to use this extended dictionary file.\\n\\nContributors:\\n\\nHerman Schaaf (IronZebra.com) (Author)\\nThomas Roten (Github)\\nJOEWONGLVFS\\nCasper CY Chiang (Github)\\n\\nAny contributions are very welcome!\\nSites using this:\\n\\nChineseLevel.com\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '139',\n",
       "  'forks': '32',\n",
       "  'commits': '83'},\n",
       " {'language': 'Python 56.4',\n",
       "  'readme': 'Detective.io \\nDownload •\\nFork •\\nLicense •\\nTest coverage •\\nDocumentation •\\nVersion 1.12.13 Gorilla\\nInstallation\\nSee also the full installation guide.\\n1. Prerequisite\\nsudo apt-get install build-essential git-core python python-pip python-dev libmemcached-dev libpq-dev libxslt1-dev libxml2-dev libxml2 libjpeg8-dev\\nsudo pip install virtualenv\\n2.  Download the project\\ngit clone git@github.com:jplusplus/detective.io.git\\ncd detective.io\\n3. Install\\nmake install\\nRun in development\\nmake run\\nThen visit http://127.0.0.1:8000\\nTechnical stack\\nThis small application uses the following tools and opensource projects:\\n\\nDjango Framework - Backend Web framework\\nNeo4django - Object Graph Mapper for Neo4j\\nTastypie - RestAPI for Django\\nAngularJS - Javascript Framework\\nUI Router - Application states manager\\nUnderscore - Utility library\\nBootstrap - HTML and CSS framework\\nLess - CSS pre-processor\\nCoffeeScript\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '139',\n",
       "  'forks': '18',\n",
       "  'commits': '3,623'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Detective.io \\nDownload •\\nFork •\\nLicense •\\nTest coverage •\\nDocumentation •\\nVersion 1.12.13 Gorilla\\nInstallation\\nSee also the full installation guide.\\n1. Prerequisite\\nsudo apt-get install build-essential git-core python python-pip python-dev libmemcached-dev libpq-dev libxslt1-dev libxml2-dev libxml2 libjpeg8-dev\\nsudo pip install virtualenv\\n2.  Download the project\\ngit clone git@github.com:jplusplus/detective.io.git\\ncd detective.io\\n3. Install\\nmake install\\nRun in development\\nmake run\\nThen visit http://127.0.0.1:8000\\nTechnical stack\\nThis small application uses the following tools and opensource projects:\\n\\nDjango Framework - Backend Web framework\\nNeo4django - Object Graph Mapper for Neo4j\\nTastypie - RestAPI for Django\\nAngularJS - Javascript Framework\\nUI Router - Application states manager\\nUnderscore - Utility library\\nBootstrap - HTML and CSS framework\\nLess - CSS pre-processor\\nCoffeeScript\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '138',\n",
       "  'forks': '40',\n",
       "  'commits': '13'},\n",
       " {'language': 'Python 88.4',\n",
       "  'readme': 'THIS PROJECT IS UNMAINTAINED\\nPlease use micawber -- it has much\\nof the same functionality along with many improvements.\\n\\nGetting Started with OEmbed\\n\\nInstallation\\nFirst, you need to install OEmbed.  It is available at http://github.com/worldcompany/djangoembed/\\ngit clone git://github.com/worldcompany/djangoembed/\\ncd djangoembed\\npython setup.py install\\n\\n\\nAdding to your Django Project\\nAfter installing, adding OEmbed consumption to your projects is a snap.  First,\\nadd it to your projects\\' INSTALLED_APPs and run \\'syncdb\\':\\n# settings.py\\nINSTALLED_APPS = [\\n    ...\\n    \\'oembed\\'\\n]\\n\\ndjangoembed uses a registration pattern like the admin\\'s.  In order to be\\nsure all apps have been loaded, djangoembed should run autodiscover() in the\\nurls.py.  If you like, you can place this code right below your admin.autodiscover()\\nbits:\\n# urls.py\\nimport oembed\\noembed.autodiscover()\\n\\n\\nConsuming Resources\\nNow you\\'re ready to start consuming OEmbed-able objects.  There are a couple of\\noptions depending on what you want to do.  The most straightforward way to get\\nup-and-running is to add it to your templates:\\n{% load oembed_tags %}\\n\\n{% oembed %}blog.content{% endoembed %}\\n\\n{# or use the filter #}\\n\\n{{ blog.content|oembed }}\\n\\n{# maybe you\\'re working with some dimensional constraints #}\\n\\n{% oembed \"600x600\" %}blog.content{% endoembed %}\\n\\n{{ blog.content|oembed:\"600x600\" }}\\n\\nYou can consume oembed objects in python as well:\\nimport oembed\\noembed.autodiscover()\\n\\n# just get the metadata\\nresource = oembed.site.embed(\\'http://www.youtube.com/watch?v=nda_OSWeyn8\\')\\nresource.get_data()\\n\\n{u\\'author_name\\': u\\'botmib\\',\\n u\\'author_url\\': u\\'http://www.youtube.com/user/botmib\\',\\n u\\'height\\': 313,\\n u\\'html\\': u\\'<object width=\"384\" height=\"313\"><param name=\"movie\" value=\"http://www.youtube.com/v/nda_OSWeyn8&fs=1\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/nda_OSWeyn8&fs=1\" type=\"application/x-shockwave-flash\" width=\"384\" height=\"313\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed></object>\\',\\n u\\'provider_name\\': u\\'YouTube\\',\\n u\\'provider_url\\': u\\'http://www.youtube.com/\\',\\n u\\'title\\': u\\'Leprechaun in Mobile, Alabama\\',\\n u\\'type\\': u\\'video\\',\\n u\\'version\\': u\\'1.0\\',\\n u\\'width\\': 384}\\n\\n# get the metadata and run it through a template for pretty presentation\\nfrom oembed.consumer import OEmbedConsumer\\nclient = OEmbedConsumer()\\nembedded = client.parse_text(\"http://www.youtube.com/watch?v=nda_OSWeyn8\")\\n\\n<div class=\"oembed oembed-video provider-youtube\">\\n  <object width=\"384\" height=\"313\">\\n    <param name=\"movie\" value=\"http://www.youtube.com/v/nda_OSWeyn8&fs=1\"></param>\\n    <param name=\"allowFullScreen\" value=\"true\"></param>\\n    <param name=\"allowscriptaccess\" value=\"always\"></param>\\n    <embed src=\"http://www.youtube.com/v/nda_OSWeyn8&fs=1\"\\n           type=\"application/x-shockwave-flash\"\\n           width=\"384\"\\n           height=\"313\"\\n           allowscriptaccess=\"always\"\\n           allowfullscreen=\"true\">\\n    </embed>\\n  </object>\\n  <p class=\"credit\">\\n    <a href=\"http://www.youtube.com/watch?v=nda_OSWeyn8\">Leprechaun in Mobile, Alabama</a>\\n    by\\n    <a href=\"http://www.youtube.com/user/botmib\">botmib</a>\\n  </p>\\n</div>\\'\\n\\n\\nTroubleshooting\\nProblem: You try the youtube embed example, but all you get is a link to the youtube video.\\nSolution: Djangoembed uses fixtures to load data about oembed providors like Youtube in to the database.  Try fooling around with syncdb (or migrations, if you\\'re running South) until there are objects of type oembed.storedprovider.\\nIf you have another problem, consider looking through the more extensive docs in the project\\'s doc subdirectory.\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '138',\n",
       "  'forks': '40',\n",
       "  'commits': '70'},\n",
       " {'language': 'Python 96.7',\n",
       "  'readme': 'ZeroDB server and client-side example of using it\\nDocumentation: http://docs.zerodb.io/\\n',\n",
       "  'watchers': '17',\n",
       "  'stars': '138',\n",
       "  'forks': '20',\n",
       "  'commits': '67'},\n",
       " {'language': 'Python 99.7',\n",
       "  'readme': \"##Hello, this is FileRock Client\\nThis is the client of FileRock,\\na backup and synchronization service that provides confidentiality and\\nchecks the integrity of your data.\\nFor instructions about how to run FileRock Client from the source code, look here.\\nFor the list of required dependencies, look here.\\nRelease notes for FileRock Client are available here.\\nIn order to use FileRock Client, you will need a FileRock account.\\nYou can get one here.\\nIf you don't have an invitation code,\\nyou can leave your email address on FileRock landing page,\\nand you will receive one as soon as possible. First arrived, first served ;-)\\n ______ _ _      _____            _       _____ _ _            _\\n|  ____(_) |    |  __ \\\\          | |     / ____| (_)          | |\\n| |__   _| | ___| |__) |___   ___| | __ | |    | |_  ___ _ __ | |_\\n|  __| | | |/ _ \\\\  _  // _ \\\\ / __| |/ / | |    | | |/ _ \\\\ '_ \\\\| __|\\n| |    | | |  __/ | \\\\ \\\\ (_) | (__|   <  | |____| | |  __/ | | | |_\\n|_|    |_|_|\\\\___|_|  \\\\_\\\\___/ \\\\___|_|\\\\_\\\\  \\\\_____|_|_|\\\\___|_| |_|\\\\__|\\n\\nCopyright (C) 2012 Heyware s.r.l.\\n\\nThis file is part of FileRock Client.\\n\\nFileRock Client is free software: you can redistribute it and/or modify\\nit under the terms of the GNU General Public License as published by\\nthe Free Software Foundation, either version 3 of the License, or\\n(at your option) any later version.\\n\\nFileRock Client is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\\nGNU General Public License for more details.\\n\\nYou should have received a copy of the GNU General Public License\\nalong with FileRock Client. If not, see <http://www.gnu.org/licenses/>.\\n\\n--\\n###How to run FileRock Client\\nIn order to run FileRock client from the source code, follow these instructions:\\n\\nMake sure that your system has all the required dependencies installed.\\nClone this repository: git clone https://github.com/filerock/FileRock-Client.git\\nRun the client from its main python script:\\n\\non Mac OS X, make sure tu use the 32-bit version of python 2.7.x: e.g., python2.7-32 FileRock.py\\non Linux or Windows, if your default python version is python 2.7.x, just run python FileRock.py\\n\\n\\nPlease not that FileRock Client used from the source code does not update itself automatically or notify the user when updates are available. If you run FileRock Client from the source code, you will need to periodically pull from this repository to get the latest version.\\n\\nPackaged version of FileRock Client are availble here.\\nWe will do our best to keep the code in this repository and the packaged versions synchronized.\\n--\\n###Required dependencies\\n\\nReference python version is 2.7.2\\n\\nMost of the following dependencies can be easily installed via pip.\\nIf you don't have pip yet, installation instructions are available here.\\n\\npbkdf2 1.3 - Can be installed through pip or from the tarball\\npycrypto 2.5 - Can be ckecked out from git or from the tarball\\nwxPython 2.8.12.1 - Can be downloaded from wxpython.org. Binaries for MS Windows are available [here](http://downloads.sourceforge.net/wxpython/wxPython2.8-win32-uni\\ncode-2.8.12.1-py27.exe) and for Mac Os X here\\nPIL (Python Image Library) - Can be installed through pip.\\n\\nInstallation through pip requires a C compiler\\nBinaries for MS Windows are available here\\n\\n\\napscheduler 2.0.3 - Can be installed through pip.\\nPySocks 1.04 - Can be ckecked out from git.\\nportalocker 0.3 - Can be installed through pip.\\n\\nThe following are required only on Linux and Mac OS X:\\n\\nsetproctitle 1.1.6 - Can be installed through pip.\\n\\nThe following are required only on Mac OS X:\\n\\nxattr 0.6.4 - Can be installed through pip.\\n\\nThe following are required only on MS Windows:\\n\\npywin32 217 - Can be checked out from sourceforge project page\\nprotobuf 2.4.1 - Can be installed through pip.\\n\\nThe following are optional dependencies, needed only for the i18n support (that is, translation to languages different from English).\\n\\ndistutils-extra 2.37 - Can be checked out from Launchpad.\\nintltool 0.50.2 - Can be checked out from Launchpad.\\n\\nRequires to install a perl interpreter as a dependency.\\n\\n\\n\\nThe following are required only for developers, in order to run the automated tests:\\n\\nnose 1.1.2 - Can be installed through pip.\\nmock 1.0.1 - Can be installed through pip.\\n\\n\",\n",
       "  'watchers': '15',\n",
       "  'stars': '137',\n",
       "  'forks': '16',\n",
       "  'commits': '7'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'django-discover-runner\\n\\nNote\\nThis runner has been added to Django 1.6 as the default test runner.\\nIf you use Django 1.6 or above you don\\'t need this app.\\n\\nAn alternative Django TEST_RUNNER which uses the unittest2 test discovery\\nfrom a base path specified in the settings, or any other module or package\\nspecified to the test management command -- including app tests.\\nIf you just run ./manage.py test, it\\'ll discover and run all tests\\nunderneath the current working directory. E.g. if you run\\n./manage.py test full.dotted.path.to.test_module, it\\'ll run the tests in\\nthat module (you can also pass multiple modules). If you give it a single\\ndotted path to a package (like a Django app) like ./manage.py test myapp\\nand that package does not itself directly contain any tests, it\\'ll do\\ntest discovery in all submodules of that package.\\n\\nNote\\nThis code uses the default unittest2 test discovery behavior, which\\nonly searches for tests in files named test*.py. To override this\\nsee the TEST_DISCOVER_PATTERN setting or use the --pattern\\noption.\\n\\n\\nWhy?\\nDjango\\'s own test discovery is very much tied to the directory structure\\nof Django apps, partly due to historic reasons (the unittest library\\ndidn\\'t have its own discovery for a long time) and prevents Django app\\nauthors from being good Python citizens. django-discover-runner uses the\\nofficial test discovery feature of the new unittest2 library which is\\nincluded in Django.\\nBy default there is no way to put project specific tests in a separate\\nfolder outside the Python package of the Django project, which is a great\\nway to organize your code, separating the tests and non-test code.\\ndjango-discover-runner helps you clean up your project tests.\\nThere is also no way to specify fully dotted import paths to test\\nmodules, functions, class or methods to the test management command\\nbut only Django\\'s odd standard <appname>.<TestClassName>.\\ndjango-discover-runner allows you to specify any type of label to Django\\'s\\ntest management command.\\nBy default Django\\'s test runner will execute the tests of Django\\'s own\\ncontrib apps, which doesn\\'t make sense if you just want to run your\\nown app\\'s or project\\'s tests. django-discover-runner fixes this by allowing\\nyou to specify which tests to run and organize your test code outside the\\nreach of the Django test runner.\\nMore reasons can be found in Carl Meyer\\'s excellent talk about\\nTesting and Django (slides).\\n\\nInstallation\\nInstall it with your favorite installer, e.g.:\\npip install -U django-discover-runner\\n\\ndjango-discover-runner requires at least Django 1.4 and also works on 1.5.x.\\nStarting in Django 1.6 the discover runner is a built-in.\\n\\nSetup\\n\\nTEST_RUNNER (required) needs to point to the DiscoverRunner class\\nto enable it:\\nTEST_RUNNER = \\'discover_runner.DiscoverRunner\\'\\n\\n\\nAdd \\'discover_runner\\' to your INSTALLED_APPS setting to enable the\\nability to override the discovery settings below when using the test\\nmanagement command.\\n\\nTEST_DISCOVER_TOP_LEVEL (optional) should be the directory containing\\nyour top-level package(s); in other words, the directory that should be on\\nsys.path for your code to import. This is for example the directory\\ncontaining manage.py in the new Django 1.4 project layout.\\nThe management command option is called --top-level.\\n\\nTEST_DISCOVER_PATTERN (optional) is the pattern to use when discovering\\ntests and defaults to the unittest2 standard test*.py. The management\\ncommand option is called --pattern.\\n\\n\\n\\nExamples\\n\\nDjango app\\nTo test a reusable Django app it\\'s recommended to add a test_settings.py\\nfile to your app package to easily run the app tests with the test\\nmanagement command. Simply set the TEST_RUNNER setting to\\n\\'discover_runner.DiscoverRunner\\', configure the other settings necessary\\nto run your tests and call the test management command with the name of\\nthe app package, e.g.:\\ndjango-admin.py test --settings=myapp.test_settings myapp\\n\\n\\nDjango project\\nIf you want to test a project and want to store the project\\'s tests outside\\nthe project main package (recommended), you can simply follow the app\\ninstructions above, applying it to the \"project\" package, but set a few\\nadditional settings to tell the test runner to find the tests:\\nfrom os import path\\nTEST_DISCOVER_TOP_LEVEL = path.dirname(path.dirname(__file__))\\n\\nThis would find all the tests within a top-level \"tests\" package. Running the\\ntests is as easy as calling:\\ndjango-admin.py test --settings=mysite.test_settings tests\\n\\nAlternatively you can specify the --top-level-directory management\\ncommand option.\\n\\nMultiple Django versions\\nIn case you want to test your app on older Django versions as well as\\nDjango >= 1.6 you can simply conditionally configure the test runner in your\\ntest settings, e.g.:\\nimport django\\n\\nif django.VERSION[:2] < (1, 6):\\n  TEST_RUNNER = \\'discover_runner.DiscoverRunner\\'\\n\\n\\nChangelog\\n\\n1.0 06/15/2013\\n\\nGOOD NEWS! This runner was added to Django 1.6 as the new default!\\nThis version backports that runner for Django 1.4.x and 1.5.x.\\nRemoved TEST_DISCOVER_ROOT setting in favor of unittest2\\'s own way to\\nfigure out the root.\\nDropped support for Django 1.3.x.\\n\\n\\n0.4 04/12/2013\\n\\nAdded ability to override the discover settings with a custom test management\\ncommand.\\n\\n\\n0.3 01/28/2013\\n\\nFixed setup.py to work on Python 3. This should make this app compatible\\nto Python 3.\\n\\n\\n0.2.2 09/04/2012\\n\\nStopped setting the top level variable in the case of using a module path\\nas the test label as it made the wrong assumption that the parent directory\\nis the top level.\\n\\n\\n0.2.1 08/20/2012\\n\\nFixed a rather esoteric bug with testing test case class methods\\nthat was caused by a wrong import and the way Django wraps itself\\naround the unittest2 module (if availale) or unittest on Python >= 2.7.\\n\\n\\n0.2 05/26/2012\\n\\nAdded ability to use an optionally installed unittest2 library\\nfor Django projects using Django < 1.3 (which added unittest2 to the\\ndjango.utils.unittest package).\\n\\n\\n0.1.1 05/23/2012\\n\\nFixed a bug that prevented the project based feature to work correctly.\\n\\n\\n0.1 05/20/2012\\n\\nInitial release with support for Django >= 1.3.\\n\\n\\nThanks\\nThis test runner is a humble rip-off of Carl Meyer\\'s DiscoveryRunner\\nwhich he published as a gist a while ago. All praise should be directed at\\nhim. Thanks, Carl!\\nThis was also very much related to ticket #17365 which eventually led\\nto the replacement of the default test runner in Django. Thanks again,\\nCarl!\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '137',\n",
       "  'forks': '10',\n",
       "  'commits': '33'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'crontabber\\nA cron job runner with self-healing and job dependencies.\\nLicense: MPL 2\\n\\n\\n\\nHow to run tests\\nFirst you need to create a dedicated test database. We recommend you\\ncall it test_crontabber. Then you need the necessary credentials for\\nit.\\nBefore running the tests you need to install some extras to be able to\\nrun tests at all:\\npip install -r test-requirements.txt\\n\\nNext, in the root directory of the project create a file called\\ntest-crontabber.ini and it should look something like this:\\n[crontabber]\\nuser=myusername\\npassword=mypassword\\ndbname=test_crontabber\\n\\nTo start all the tests run:\\nPYTHONPATH=. nosetests\\n\\nIf you want to run a specific test in a specific file in a specific\\nclass you can define it per the nosetests standard like this for\\nexample:\\nPYTHONPATH=. nosetests tests crontabber/tests/test_crontabber.py:TestCrontabber.test_basic_run_job\\n\\nIf you want the tests to stop as soon as the first test fails add -x\\nto that same command above.\\nAlso, if you want nosetests to not capture stdout add -s\\nto that same command as above.\\n\\nHow to do code coverage analysis\\nFirst you need to install the\\ncoverage module. Then,\\nwith nosetests, you can run this:\\nPYTHONPATH=. nosetests --with-coverage --cover-erase --cover-html --cover-package=crontabber\\n\\nAfter it has run, you can open the file cover/index.html in browser.\\n\\nHow to run the exampleapp\\nThe example app helps you set up a playground to play around with and\\ntest crontabber to gain a better understanding of how it works.\\nThe best place to start with is to read the exampleapp/README.md\\nfile and go through its steps. Once you get the basics to work you can\\nstart experimenting with adding your job classes.\\n\\nHow locking works\\ncrontabber supports locking. It basically means if you start a second\\ninstance of crontabber whilst it\\'s already ongoing in another\\nterminal/server the second one will exist early. This is only applicable\\nif there is an actual job ongoing.\\nThere are two kinds of locking.\\n\\nGeneral locking. The first thing crontabber does before it starts\\nan app is to ask the state (stored in PostgreSQL) if it\\'s ongoing and\\nif it is, it exists with an error code of 3.\\nSub-second locking. If the general locking (see point above) says\\n\"No, the job is not ongoing\", it\\'s going to proceed to update the\\nstate with a row-level locking transaction in\\nPostgreSQL.\\nThat basically means PostgreSQL only allows one single UPDATE\\nfrom the process that gets there first. The second crontabber process\\nwill will exit early with an error code of 2 if the first\\ncrontabber process managed to run the UPDATE first.\\n\\nImagine two separate terminals starting crontabber at the almost same\\ntime:\\n# Terminal 1\\n$ python crontabber.py --admin.conf=crontabber.ini\\n$ echo $?\\n0\\n\\n# Terminal 2 (started almost simultaneously)\\n$ python crontabber.py --admin.conf=crontabber.ini\\n$ echo $?\\n3\\n\\nNote! If a job has been ongoing to a maximum period of time, the\\nlocking is ignored. This is controlled by the config option\\ncrontabber.max_ongoing_age_hours which defaults to 12 hours.\\nThis is applicable if crontabber, updates the state that it\\'s starting a\\njob, then when it tries to update the state that it finished\\n(successfully or not) and that write fails, if for example it\\'s unable\\nto make a connection to PostgreSQL. If this happens crontabber will just\\nignore the lock and run it anyway.\\n',\n",
       "  'watchers': '16',\n",
       "  'stars': '137',\n",
       "  'forks': '19',\n",
       "  'commits': '151'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'crontabber\\nA cron job runner with self-healing and job dependencies.\\nLicense: MPL 2\\n\\n\\n\\nHow to run tests\\nFirst you need to create a dedicated test database. We recommend you\\ncall it test_crontabber. Then you need the necessary credentials for\\nit.\\nBefore running the tests you need to install some extras to be able to\\nrun tests at all:\\npip install -r test-requirements.txt\\n\\nNext, in the root directory of the project create a file called\\ntest-crontabber.ini and it should look something like this:\\n[crontabber]\\nuser=myusername\\npassword=mypassword\\ndbname=test_crontabber\\n\\nTo start all the tests run:\\nPYTHONPATH=. nosetests\\n\\nIf you want to run a specific test in a specific file in a specific\\nclass you can define it per the nosetests standard like this for\\nexample:\\nPYTHONPATH=. nosetests tests crontabber/tests/test_crontabber.py:TestCrontabber.test_basic_run_job\\n\\nIf you want the tests to stop as soon as the first test fails add -x\\nto that same command above.\\nAlso, if you want nosetests to not capture stdout add -s\\nto that same command as above.\\n\\nHow to do code coverage analysis\\nFirst you need to install the\\ncoverage module. Then,\\nwith nosetests, you can run this:\\nPYTHONPATH=. nosetests --with-coverage --cover-erase --cover-html --cover-package=crontabber\\n\\nAfter it has run, you can open the file cover/index.html in browser.\\n\\nHow to run the exampleapp\\nThe example app helps you set up a playground to play around with and\\ntest crontabber to gain a better understanding of how it works.\\nThe best place to start with is to read the exampleapp/README.md\\nfile and go through its steps. Once you get the basics to work you can\\nstart experimenting with adding your job classes.\\n\\nHow locking works\\ncrontabber supports locking. It basically means if you start a second\\ninstance of crontabber whilst it\\'s already ongoing in another\\nterminal/server the second one will exist early. This is only applicable\\nif there is an actual job ongoing.\\nThere are two kinds of locking.\\n\\nGeneral locking. The first thing crontabber does before it starts\\nan app is to ask the state (stored in PostgreSQL) if it\\'s ongoing and\\nif it is, it exists with an error code of 3.\\nSub-second locking. If the general locking (see point above) says\\n\"No, the job is not ongoing\", it\\'s going to proceed to update the\\nstate with a row-level locking transaction in\\nPostgreSQL.\\nThat basically means PostgreSQL only allows one single UPDATE\\nfrom the process that gets there first. The second crontabber process\\nwill will exit early with an error code of 2 if the first\\ncrontabber process managed to run the UPDATE first.\\n\\nImagine two separate terminals starting crontabber at the almost same\\ntime:\\n# Terminal 1\\n$ python crontabber.py --admin.conf=crontabber.ini\\n$ echo $?\\n0\\n\\n# Terminal 2 (started almost simultaneously)\\n$ python crontabber.py --admin.conf=crontabber.ini\\n$ echo $?\\n3\\n\\nNote! If a job has been ongoing to a maximum period of time, the\\nlocking is ignored. This is controlled by the config option\\ncrontabber.max_ongoing_age_hours which defaults to 12 hours.\\nThis is applicable if crontabber, updates the state that it\\'s starting a\\njob, then when it tries to update the state that it finished\\n(successfully or not) and that write fails, if for example it\\'s unable\\nto make a connection to PostgreSQL. If this happens crontabber will just\\nignore the lock and run it anyway.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '137',\n",
       "  'forks': '15',\n",
       "  'commits': '34'},\n",
       " {'language': 'Java 69.8',\n",
       "  'readme': 'Jetstream\\n\\nJetstream is a real-time stream processing system for analyzing live event streams to enable business to react to signals much earlier than is possible with batch oriented systems like Hadoop.\\nIt is targeted for security, risk, machine learning, fault monitoring, predictive analytics and many more use cases where there is a need to track, detect and react to user or machine behavior patterns over windows of time. Examples of these could be monitoring DOS attacks, predicting failures by watching metrics, correlating events etc.\\nFollow wiki for more details.\\n',\n",
       "  'watchers': '27',\n",
       "  'stars': '110',\n",
       "  'forks': '33',\n",
       "  'commits': '102'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '效果展示\\n\\n\\n详解\\nhttp://yueban.github.io/2015/04/28/MaterialDesign文字缩放并入Toolbar效果的一种实现/\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '110',\n",
       "  'forks': '26',\n",
       "  'commits': '4'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'TimeSinceTextView\\nThis is a subclass of android.widget.TextView that exposes a method setDate() which accepts a long Unix timestamp or java.util.Date. The view converts the date into a String which describes the date in terms of time since that timestamp. For example, if the current timestamp is Unix 1453503166 and we call timeSinceTextView.setDate(1453503116), \"50 seconds ago\" is displayed.\\nChangelog\\nJavadoc\\nComparison to DateUtils.getRelativeTimeSpanString\\nI actually wrote this library before I knew about DateUtils.getRelativeTimeSpanString, but the output is actually quite a bit different. The DateUtils implementation should return localized text and allows for customizable flags. See here for a comparison of the output of different time stamps.\\nUsage\\nSimply declare a TimeSinceTextView in XML or create one in code.\\n<com.ddiehl.timesincetextview.TimeSinceTextView\\n  android:id=\"@+id/timestamp\"\\n  android:layout_width=\"wrap_content\"\\n  android:layout_height=\"wrap_content\" />\\nThen call setDate(Date) or setDate(long) with a Unix timestamp, and the text will be automatically generated and set to the view.\\n((TimeSinceTextView) findViewById(R.id.timestamp)).setDate(1452827942);\\nTo get an abbreviated form of the converted text, add app:abbreviated=\"true\" to your XML layout.\\n<com.ddiehl.timesincetextview.TimeSinceTextView\\n  xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n  android:layout_width=\"wrap_content\"\\n  android:layout_height=\"wrap_content\"\\n  app:tstv_abbreviated=\"true\" />\\nThe class TimeSince also contains static methods which can be used to retrieve a relative timestamp string without an instance of TimeSinceTextView.\\nAdd to your project\\n\\nrepositories {\\n    maven { url \"https://jitpack.io\" }\\n}\\ndependencies {\\n  compile \\'com.github.damien5314:TimeSinceTextView:1.+\\'\\n}\\n\\nContributions\\nPull requests are welcome, in particular in would be nice to have strings.xml translated into as many languages as possible.\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '109',\n",
       "  'forks': '21',\n",
       "  'commits': '45'},\n",
       " {'language': 'Java 96.9',\n",
       "  'readme': 'WanAndroid\\nAn Android app for www.wanandroid.com\\n\\n\\n\\nWanAndroid App aims to help people access the latest android articles, which is designed with Material Design style, built on MVP(Model-View-Presenter) architecture with RxJava2, Retrofit2, Realm database, Glide.\\nThe source code in the repository reflects the app which supports mobile devices running Android 6.0+.\\nAbout This Repository And App\\nThis app is inspired by Espresso which is developed by TonnyL, Awesome-WanAndroid and has a lot of similar elements in design.\\nAnd this app is using the WanAndroid API(doc) designed by HongYang.\\nFeatures\\n\\nCollect many articles of Android.\\nUpdate the articles everyday.\\nSupport collect the articles ,which will synchronize the user data so you can visit website to browse the list of articles collected.\\nSupport mark the articles so you can read it later.\\nDay mode and night mode are supported.\\nSupport send feedback on using experience from your devices.\\n\\nScreenshots\\n\\n\\nI hope the source code for this app is useful for you as a reference or starting point for creating your own app. Here is some instructions to help you better build and run the code in Android Studio.\\nClone the Repository:\\ngit clone https://github.com/CoderLengary/WanAndroid\\n\\nCheck out the master branch:\\ngit checkout master\\n\\nNotice: If you want to review a different branch, replace the master with the name you want to checkout (if it does exist). Finally open the WanAndroid/ directory in Android Studio.\\nSuggestion: It is better for you to update your Android Studio to version 3.0 when you open this project.\\nLibraries Used in This App\\n\\n\\n\\nName\\nIntroduction\\n\\n\\n\\n\\nAndroid Support Libraries\\nThe Android Support Library offers a number of features that are not built into the framework. These libraries offer backward-compatible versions of new features, provide useful UI elements that are not included in the framework, and provide a range of utilities that apps can draw on.\\n\\n\\nCircleImageView\\nA circular ImageView for Android\\n\\n\\nRealm\\nRealm is a mobile database: a replacement for SQLite & ORMs.\\n\\n\\nRetrofit\\nType-safe HTTP client for Android and Java by Square, Inc.\\n\\n\\nRxAndroid\\nRxJava bindings for Android.\\n\\n\\nRxJava\\nRxJava – Reactive Extensions for the JVM – a library for composing asynchronous and event-based programs using observable sequences for the Java VM.\\n\\n\\nGlide\\nAn image loading and caching library for Android focused on smooth scrolling.\\n\\n\\nAgentWeb\\nAgentWeb is an Android WebView based, extremely easy to use and powerful library.\\n\\n\\nFlowLayout\\nA very convenient and powerful flow layout.\\n\\n\\n\\nThanks to\\n\\nTonnyL Really appreciate his help!\\nWanAndroid\\nAwesome-WanAndroid\\n\\nLicense\\nCopyright 2018 CoderLengary\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '2',\n",
       "  'stars': '109',\n",
       "  'forks': '19',\n",
       "  'commits': '47'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'MaterialDesignLearn\\n主要是学习design lib里面的控件使用\\n##NavigationView\\n\\n##RecyclerView实现瀑布流\\n\\n##CoordinatorLayout结合ToolBar显示隐藏 类似淘宝商品详情\\n###里面还可以看到TabLayout使用，结合了ViewPager\\n\\n##FloatActionButton点击实现淘宝加入购物车动画\\n\\n##自定义Behavior实现知乎 简书效果\\n####知乎效果\\n\\n####简书效果\\n\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '109',\n",
       "  'forks': '53',\n",
       "  'commits': '8'},\n",
       " {'language': 'Java 97.8',\n",
       "  'readme': 'Vert.x 2.x is deprecated - use instead http://vertx.io/docs/vertx-rx/java/\\nmod-rxvertx\\nVert.x module which uses RxJava to add support for Reactive Extensions (RX) using the RxJava library. This allows VertX developers to use the RxJava type-safe composable API to build VertX verticles.\\nDependencies\\n\\nThe module wraps the VertX core objects to add Observable support so it is tightly bound to the VertX release.\\nThis module also contains the Netflix RxJava library.\\n\\nStatus\\nCurrently Observable wrappers are provided for\\n\\nEventBus\\nHttpServer\\nHttpClient\\nNetServer\\nNetClient\\nTimer\\n\\nThere are also base Observable adapters that map Handler and AsyncResultHandler to Observable that can be used to call other Handler based APIs.\\nSupport coming soon for\\n\\nFileSystem\\nSockJSServer\\n\\nUsage\\nThis is a non-runnable module, which means you add it to your module via the \"includes\" attribute of mod.json.\\nAll standard API methods of the form\\nvoid method(args...,Handler<T> handler)\\nare typically available in the form\\nObservable<T> method(args...)\\nwhere the operation is executed immediately or\\nObservable<T> observeMethod(args...)\\nwhere the operation is executed on subscribe. This latter form is the more \\'pure\\' Rx method and should be used where possible (required to maintain semantics of concat eg)\\nEventBus\\nRxEventBus rxEventBus = new RxEventBus(vertx.eventBus());\\nrxEventBus.<String>registerHandler(\"foo\").subscribe(new Action1<RxMessage<String>>() {\\n  public void call(RxMessage<String> message) {\\n    // Send a single reply\\n    message.reply(\"pong!\");\\n  }\\n});\\n\\nObservable<RxMessage<String>> obs = rxEventBus.send(\"foo\", \"ping!\");\\n\\nobs.subscribe(\\n  new Action1<RxMessage<String>>() {\\n    public void call(RxMessage<String> message) {\\n      // Handle response \\n    }\\n  },\\n  new Action1<Throwable>() {\\n    public void call(Throwable err) {\\n     // Handle error\\n    }\\n  }\\n);\\n\\nScheduler\\nThe standard RxJava schedulers are not compatible with VertX. In order to preserve the Vert.x Threading Model all callbacks to a Verticle must be made in the context of that Verticle instance.\\nRxVertx provides a custom Scheduler implementation that uses the Verticle context to scheduler timers and ensure callbacks run on the correct context.\\nIn the following example the scheduler is used to run a Timer and then buffer the output.\\nNote: The RxVertx scheduler must always be used to observe results inside the Verticle. It is possible to use the other Schedulers (eg for blocking calls) as long as you always use observeOn to route the callbacks onto the Verticle EventLoop. For timers it is more efficient to just use the Vert.x scheduler\\nRxVertx rx = new RxVertx(vertx);\\nObservable o = (some observable source)\\n\\nObservable\\n      .timer(10, 10, TimeUnit.MILLISECONDS, rx.contextScheduler())\\n      .buffer(100,TimeUnit.MILLISECONDS,rx.contextScheduler())\\n      .take(10)\\n      .subscribe(...)\\nTimer\\nThe timer functions are provided via the RxVertx wrapper. The timer is set on-subscribe. To cancel a timer that has not first, or a periodic timer, just unsubscribe.\\nRxVertx rx = new RxVertx(vertx);\\nrx.setTimer(100).subscribe(new Action1<Long>() {\\n  public void call(Long t) {\\n    // Timer fired\\n  }\\n});\\nThe new Scheduler means you can use the native RxJava Timer methods - this Timer may be deprecated in future\\nHelper\\nThe support class RxSupport provides several helper methods for some standard tasks\\nStreams\\nThere are two primary wrappers\\nObservable RxSupport.toObservable(ReadStream)\\nConvert a ReadStream into an Observable<Buffer>\\nRxSupport.stream(Observable,WriteStream)\\nStream the output of an Observable to a WriteStream.\\nplease note that this method does not handle writeQueueFull so cannot be used as a pump\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '108',\n",
       "  'forks': '30',\n",
       "  'commits': '135'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Android Autowire\\nUsing Java Annotations and Reflection, this library will allow you to replace some of annoying boilerplate setup from your Activities, Fragments, and Views with an annotation based approach.\\nThis repository is referenced in the blog post: http://www.cardinalsolutions.com/cardinal/blog/mobile/2014/01/dealing_with_android.html\\nFeatures\\n\\nSupports Inheritance of Activities. You can inherit views from parent Activities, and every view will be picked up and wired in\\nAs it uses reflection, it will work with private variables\\nComes with several out of the box ways of specifying IDs allowing for flexibility in naming IDs and implementing the annotations\\nProvides an optional required field in the annotation, so if an ID is not found, the variable will be skipped without an Exception being thrown\\nSupport Annotations for Layout as well as Views\\nSupport an Annotation based approach for saving instance state.  This also allows for inheritance.\\nCan be adapted to work with Fragments as well as Activities\\nCan be adapted to work with CustomViews\\n\\nThe Android Way\\nHere are some Examples of Android Boilerplate code that we can make more clear, readable, and easier to use with Annotations.\\nfindViewById()\\nOne particularly jarring example of Android boilerplate code is the findViewById() method.  Every time you want to access an Android view defined in your XML, you need to use this method, often with a typecast.  For large Activities with many views, this can add a lot of code that does nothing but pull variables out of the xml.\\npublic class MainActivity extends BaseActivity{\\n\\n\\tprivate ImageView logo;\\n\\n\\t@Override\\n    public void onCreate(Bundle savedInstanceState){\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.main);\\n\\n    \\tlogo = (ImageView) findViewById(R.id.logo);\\n\\t}\\n}\\nsetContentView()\\nIn the code example above, we have the setContentView(R.layout.main) line.  You need something like this in every Activity class, with the sole purpose of inflating your layout.  It\\'s not a big deal, but it is one extra step you have to go through when creating your Activity classes because it has to be put in exactly the right spot.  It needs to be in onCreate() before any findViewById() call.\\nSaving Instance State\\nA quirk of how the Android operating systems works, Activities can be destroyed at almost anytime to make room for other OS processes.  They are also destroyed and re-created on rotation.  The developer is in charge of saving the Activity\\'s state, making sure the Activity comes back exactly the same way before it was destroyed.\\nIn the Android way, instance variables that you have to manually store are put into a Bundle in the onSaveInstanceState method.  Then they must be pulled out again in the onCreate() method.\\npublic class MainActivity extends BaseActivity{\\n\\n    private static final String SOME_STATE_KEY = \"some_state_key\";\\n\\tprivate int someState;\\n\\n\\t@Override\\n    public void onCreate(Bundle savedInstanceState){\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.main);\\n\\n    \\tif(savedInstanceState != null){\\n              someState = savedInstanceState.getInt(SOME_STATE_KEY);\\n        }\\n\\t}\\n\\n    @Override\\n    protected void onSaveInstanceState(Bundle outState){\\n\\t\\tsuper.onSaveInstanceState(outState);\\n        outState.putInt(SOME_STATE_KEY, someState);    \\n    }\\n}\\nWith AndroidAutowire\\nThis library will help streamline this process into a more readable format using annotations and reflection.\\nfindViewById()\\nBy annotating a class variable for the View with the @AndroidView custom annotation, you enable the reflection code to pull the view out of the xml.  The variable name will be the view id, or alternatively, the view id can be specified in the annotation.  The annotation processing occurs in an overridden method of setContentView(int layoutResID) in the Activity’s base class.\\nMainActivity Class\\npublic class MainActivity extends BaseActivity{\\n\\n\\t@AndroidView\\n\\tprivate ImageView logo;\\n\\n\\t@Override\\n    public void onCreate(Bundle savedInstanceState){\\n        super.onCreate(savedInstanceState);\\n        setContentView(R.layout.main);\\n\\t}\\n}\\nBaseActivity class\\npublic class BaseActivity extends Activity {\\n\\n\\t@Override\\n    public void setContentView(int layoutResID) {\\n    \\tsuper.setContentView(layoutResID);\\n    \\tAndroidAutowire.autowire(this, BaseActivity.class);\\n    }\\n}\\nsetContentView()\\nSpecifying the layout resource in the onCreate is not difficult, but it can create problems if you forget add the method call, or if you do it out of order.  Instead, use an annotation:\\nMainActivity Class\\n@AndroidLayout(R.layout.main)\\npublic class MainActivity extends BaseActivity{\\n\\n\\t@Override\\n    public void onCreate(Bundle savedInstanceState){\\n        super.onCreate(savedInstanceState);\\n\\t}\\n}\\nBaseActivity class\\npublic class BaseActivity extends Activity {\\n\\n    @Override\\n    protected void onCreate(Bundle savedInstanceState){\\n        super.onCreate(savedInstanceState);\\n        int layoutId = AndroidAutowire.getLayoutResourceByAnnotation(this, this, BaseActivity.class);\\n\\t\\t//If this activity is not annotated with AndroidLayout, do nothing\\n\\t\\tif(layoutId == 0){\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tsetContentView(layoutId);\\n    }\\n\\n\\t@Override\\n    public void setContentView(int layoutResID) {\\n    \\tsuper.setContentView(layoutResID);\\n    \\tAndroidAutowire.autowire(this, BaseActivity.class);\\n    }\\n}\\nSaving Instance State\\nAll of the reading/writing with the Bundle can be done with reflection.  Simply annotate the instance variable you want to save/load, and the AndroidAutowire library will do the work for you.\\nMainActivity Class\\n@AndroidLayout(R.layout.main)\\npublic class MainActivity extends BaseActivity{\\n    @SaveInstance\\n    private int someState;\\n\\n\\t@Override\\n    public void onCreate(Bundle savedInstanceState){\\n        super.onCreate(savedInstanceState);\\n\\t}\\n}\\nBaseActivity Class\\npublic class BaseActivity extends Activity {\\n\\n    @Override\\n    protected void onCreate(Bundle savedInstanceState){\\n        super.onCreate(savedInstanceState);\\n        AndroidAutowire.loadFieldsFromBundle(savedInstanceState, this, BaseActivity.class);\\n    }\\n\\n    @Override\\n\\tprotected void onSaveInstanceState(Bundle outState){\\n\\t\\tsuper.onSaveInstanceState(outState);\\n\\t\\tAndroidAutowire.saveFieldsToBundle(outState, this, BaseActivity.class);\\n\\t}\\n}\\nConfiguration\\nSimply include the jar in your classpath.  The process for including the AndroidAutowire library will be IDE specific, but once the library is included in the project, the methods will all be there for you to use.\\nYou can create your own BaseActivity using the process above, or you can use a provided BaseActivity called BaseAutowireActivity.  That will provide support for all features given above, as well as including a new abstract method that acts as a callback once the autowiring is complete. If you use features like BaseAutowireActivity and @AndroidLayout it may not even be necessary to override onCreate in your Activity class.\\nFragments\\nMuch like Activities, Fragments have layouts, state to be saved, and views to be autowired. But the process for setting up a Fragment is different than an Activity.  None the less, AndroidAutowire provides the ability to do all of this using Annotations as well by providing a new method: AndroidAutowire.autowireFragment().\\nHere is an Example base class for Fragments:\\npublic abstract class BaseFragment extends Fragment {\\n\\n\\tprotected View contentView;\\n\\t\\n\\t@Override\\n    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {\\n\\t\\t//Load any annotated fields from the bundle\\n\\t\\tAndroidAutowire.loadFieldsFromBundle(savedInstanceState, this, BaseFragment.class);\\n\\t\\t\\n\\t\\t//Load the content view using the AndroidLayout annotation\\n        contentView = super.onCreateView(inflater, container, savedInstanceState);\\n        if (contentView == null) {\\n        \\tint layoutResource = AndroidAutowire.getLayoutResourceByAnnotation(this, getActivity(), BaseFragment.class);\\n        \\tif(layoutResource == 0){\\n            \\treturn null;\\n            }\\n        \\tcontentView = inflater.inflate(layoutResource, container, false);\\n        }\\n        //If we have the content view, autowire the Fragment\\'s views\\n        autowireViews(contentView);\\n        //Callback for when autowiring is complete\\n        afterAutowire(savedInstanceState);\\n        return contentView;\\n    }\\n\\t\\n\\tprotected void autowireViews(View contentView){\\n\\t\\tAndroidAutowire.autowireFragment(this, BaseFragment.class, contentView, getActivity());\\n\\t}\\n\\t\\n\\t@Override\\n\\tpublic void onSaveInstanceState(Bundle outState){\\n\\t\\tsuper.onSaveInstanceState(outState);\\n\\t\\tAndroidAutowire.saveFieldsToBundle(outState, this, BaseFragment.class);\\n\\t}\\n\\t\\n\\tprotected abstract void afterAutowire(Bundle savedInstanceState);\\n}\\nUnfortunately, do to fragmentation between the Android Core API and the Support Library, this class is not included with the Jar (whereas BaseAutowireActivity is included).\\nCustom Views\\nIf you are writing a non-trivial Android App, chances are you will need to make your own custom Views at some point.  These views may have subviews.  Again, rather than being forced to use findViewById(), we can use AndroidAutowire and Annotations with the AndroidAutowire.autowireView() method.\\npublic class CustomView extends RelativeLayout {\\n\\n\\t@AndroidView(R.id.title)\\n\\tprivate TextView title;\\n\\t\\n\\t@AndroidView(R.id.icon)\\n\\tprivate ImageView icon;\\n\\n    public CustomView(Context context, AttributeSet attrs, int defStyle) {\\n\\t\\tsuper(context, attrs, defStyle);\\n\\t\\tLayoutInflater inflater = LayoutInflater.from(context);\\n\\t\\tinflater.inflate(R.layout.custome_view, this);\\n\\t\\tAndroidAutowire.autowireView(this, CustomView.class, context);\\n\\t}\\n}\\nComparison to Other Libraries\\nThere are some other open source libraries that accomplish something similar to what Android Autowire hopes to provide\\nRoboGuice is a dependency injection library that can inject views in much the same way.  However, you must extend the Robo* classes, and there may be performance issues. (https://github.com/roboguice/roboguice/wiki)\\nAndroid Annotations can wire in views by annotation, but the approach they take is quite different.  Android Annotations requires you to use an extra compile step, creating generated Activity classes that must be referenced in the AndroidManifest.xml.  As this approach will create subclasses of your Activity, you cannot use this on private variables.  Additionally, there is much more configuration and initial setup. (https://github.com/excilys/androidannotations/wiki)\\nButter Knife does the same compile time annotation approach as Android Annotations, but instead of generating a new Activity, they generate a class to pass your activity into. This way, you don\\'t have to deal with generated sub classes, but you still get some of the heavy hitting features like onClick Listeners. (http://jakewharton.github.io/butterknife/)\\nThe real advantage to this \"Android Autowire\" library is ease of use.  There is minimal configuration in just about every IDE, and little overhead, allowing you to quickly start using these annotations in your new or existing project.  Instead of providing a full feature set, this library concentrates only on limited number of features, such as views, layouts and Bundle resources, allowing it to fill the gap while still being lightweight.\\nPerformance\\nThe more you use the library, the more you want to keep an eye out for performance hits. Most of this reflection code is going to be done on the main thread, and that is always a risk. However, I have been using all of the features, from loading Serializable objects from the Bundle to finding views inside of Fragments, and I have not noticed any type of performance decrease. In fact, even some very complex Activities have made full use of this reflection code without any issue. My biggest concern would be older devices that I have not tested on, devices that may be slow to begin with.\\nTo illustrate this, I did some benchmarks on an HTC Nexus One running 2.3.4 Gingerbread. The application I used is a fairly complex production Android App. The time is the total time for the reflection to complete, not including the time it takes for the system to start the Activity/Fragment and not including any time to inflate XML layouts.\\n\\nActivity wiht 1 Autowired View, 0 Save Instance variables, and layout: 0.7ms\\nActivity with 15 Autowired Views, 2 Save Instance variables, and layout: 4.9ms\\nFragment with 1 Autowired View, 0 Save Instance variables, and layout: 2.0ms\\nFragment with 3 Autowired Views, 4 Save Instance variables, and layout: 6.5ms\\nFragment with 18 Autowired Views, 6 Save Instance variables, layout, and inheritance: 44.6ms\\n\\nThis is hardly a scientific endeavour, but it should give some pretty clear direction as to what the performance impact of using this library would be. Using this library with API level 10 and up seems to be fairly safe, as the most complicated bit of reflection using a Fragment with many views and instance state was still completed in less than 50 milliseconds.\\nAuthor / License\\nCopyright Cardinal Solutions 2015. Licensed under the MIT license.\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '108',\n",
       "  'forks': '20',\n",
       "  'commits': '43'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'PoiPhoto\\nA simple Photo Selecter\\nWhat is PoiPhoto\\nPoiPhoto is a simple lib to select photos for Android.\\n\\n\\n##How to use\\n###Gradle\\ncompile \\'com.flying.xiaopo:poiphoto:0.4.2\\'\\n###AndroidManifest.xml\\n<uses-permission\\n    android:name=\"android.permission.READ_EXTERNAL_STORAGE\"\\n    android:maxSdkVersion=\"23\"/>\\n<uses-permission\\n    android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"\\n    android:maxSdkVersion=\"23\"/>\\n###Java\\nPhotoPicker.newInstance()\\n          .setAlbumTitle(\"Album\")\\n          .setPhotoTitle(\"Photo\")\\n          .setToolbarColor(Color.BLACK)\\n          .setToolbarTitleColor(Color.WHITE)\\n          .setMaxNotice(\"can not select more\") //the message when user selected photos too more\\n          .setStatusBarColor(Color.BLACK)   //when sdk >21 ,it will work\\n          .setMaxCount(6)             //max count of selected count\\n          .pick(MainActivity.this);   //context\\nalso\\nPhotoPicker.newInstance()\\n                .inflate(RecyclerView, RecyclerView.LayoutManager);\\nor just get data\\nPhotoManager photoManager = new PhotoManager(context);\\n###To Get Data\\n@Override\\nprotected void onActivityResult(int requestCode, int resultCode, Intent data) {\\n    super.onActivityResult(requestCode, resultCode, data);\\n    if (resultCode == RESULT_OK && requestCode == Define.DEFAULT_REQUEST_CODE) {\\n        //to get path of the selected photos\\n        List<String> paths = data.getStringArrayListExtra(Define.PATHS);\\n        //to get datatype of photo of the selected photos\\n        List<Photo> photos = data.getParcelableArrayListExtra(Define.PHOTOS);\\n    }\\n}\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '108',\n",
       "  'forks': '22',\n",
       "  'commits': '15'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '###示例代码\\n\\n\\n\\n包\\n作用\\n\\n\\n\\n\\naidl\\naidl示例代码\\n\\n\\nbrzier\\n贝塞尔曲线示例\\n\\n\\njni\\njni示例\\n\\n\\nmvp\\nmvp绑定示例\\n\\n\\nrecycler\\nrecycler的点击拖拽示例\\n\\n\\nretrofit\\nretrofit巧妙的封装示例\\n\\n\\nsensor\\n传感器示例\\n\\n\\nview\\nLightingColorFilter使用示例\\n\\n\\nzxing\\n一个zxing的第三方封装库使用示例\\n\\n\\n\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '107',\n",
       "  'forks': '35',\n",
       "  'commits': '13'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Amazon Kinesis Storm Spout\\nThe Amazon Kinesis Storm spout helps Java developers integrate Amazon Kinesis with Storm.\\nRequirements\\n\\nAWS SDK for Java\\nJava 1.7 (Java SE 7) or later\\nApache Commons Lang 3.0 or later\\nGoogle Guava 13.0 or later\\nAnd, of course, Amazon Kinesis and Storm\\n\\nOverview\\nThe Amazon Kinesis Storm spout fetches data records from Amazon Kinesis and emits them as tuples. The spout stores checkpoint state in ZooKeeper to track the current position in the stream.\\nThe Amazon Kinesis Storm spout can be configured to retry failed records. By default, it retries a failed record 3 times. If a record fails and the retry limit has been reached, the spout will log an error and skip over the record. The spout buffers pending records in memory, so it can re-emit a failed record without having to re-fetch the record from Amazon Kinesis. The spout sets the checkpoint to the highest sequence number that has been ack\\'ed (or exhausted retry attempts).\\nTo use the spout, you\\'ll need to add it to your Storm topology.\\n\\nKinesisSpout: Constructs an instance of the spout, using your AWS credentials and the configuration specified in KinesisSpoutConfig (as well as com.amazonaws.ClientConfiguration, via the AWS SDK). Each task executed by the spout operates on a distinct set of Amazon Kinesis shards. Shard states are periodically committed to ZooKeeper. When the spout is deactivated, it will disconnect from ZooKeeper, but the spout will continue monitoring its local state so you can activate it again later.\\nKinesisSpoutConfig: Configures the spout, including the Storm topology name, the Amazon Kinesis stream name, the endpoint for connecting to ZooKeeper, and the prefix for the ZooKeeper paths where the spout state is stored. See the samples folder for configuration examples.\\nDefaultKinesisRecordScheme: This default scheme, used by the sample topology, emits a tuple of (partitionKey, record). If you want to emit more structured data, you can provide your own implementation of IKinesisRecordScheme.\\n\\nThe samples folder includes a sample topology and sample bolt, using the number of Amazon Kinesis shards as the parallelism hint for the spout. For more information about Storm topologies and bolts, see the Storm documentation.\\nUsing the Sample\\n\\nEdit the *.properties file to configure your Storm topology, Amazon Kinesis stream, and ZooKeeper details. For your AWS Credentials, we recommend using IAM roles on Amazon EC2 when possible. You can also specify your credentials using system properties, environment variables, or AwsCredentials.properties.\\nPackage the spout and the sample (including all dependencies but excluding Storm itself) into one JAR file.\\nDeploy the package to Storm via the JAR file, e.g., storm jar my-spout-sample.jar SampleTopology sample.properties RemoteMode\\n\\nRelease Notes\\nRelease 1.1.1 (June 1, 2015)\\n\\nWhen a Kinesis Stream is resharded, \"storm rebalance\" can be invoked to refresh the shard list and distribute the latest shards across the Spout tasks.\\n\\nRelease 1.1.0 (October 21, 2014)\\n\\nAdded support for retrying failed records\\nAdded region name support\\n\\nFuture Work\\n\\nAutomatically handle closed, split, and merged shards.\\n\\nRelated Resources\\nAmazon Kinesis Developer Guide\\nAmazon Kinesis API Reference\\nAmazon Kinesis Client Library\\nAmazon Kinesis Connector Library\\n',\n",
       "  'watchers': '41',\n",
       "  'stars': '107',\n",
       "  'forks': '71',\n",
       "  'commits': '11'},\n",
       " {'language': 'C++ 63.5',\n",
       "  'readme': 'Amazon Kinesis Storm Spout\\nThe Amazon Kinesis Storm spout helps Java developers integrate Amazon Kinesis with Storm.\\nRequirements\\n\\nAWS SDK for Java\\nJava 1.7 (Java SE 7) or later\\nApache Commons Lang 3.0 or later\\nGoogle Guava 13.0 or later\\nAnd, of course, Amazon Kinesis and Storm\\n\\nOverview\\nThe Amazon Kinesis Storm spout fetches data records from Amazon Kinesis and emits them as tuples. The spout stores checkpoint state in ZooKeeper to track the current position in the stream.\\nThe Amazon Kinesis Storm spout can be configured to retry failed records. By default, it retries a failed record 3 times. If a record fails and the retry limit has been reached, the spout will log an error and skip over the record. The spout buffers pending records in memory, so it can re-emit a failed record without having to re-fetch the record from Amazon Kinesis. The spout sets the checkpoint to the highest sequence number that has been ack\\'ed (or exhausted retry attempts).\\nTo use the spout, you\\'ll need to add it to your Storm topology.\\n\\nKinesisSpout: Constructs an instance of the spout, using your AWS credentials and the configuration specified in KinesisSpoutConfig (as well as com.amazonaws.ClientConfiguration, via the AWS SDK). Each task executed by the spout operates on a distinct set of Amazon Kinesis shards. Shard states are periodically committed to ZooKeeper. When the spout is deactivated, it will disconnect from ZooKeeper, but the spout will continue monitoring its local state so you can activate it again later.\\nKinesisSpoutConfig: Configures the spout, including the Storm topology name, the Amazon Kinesis stream name, the endpoint for connecting to ZooKeeper, and the prefix for the ZooKeeper paths where the spout state is stored. See the samples folder for configuration examples.\\nDefaultKinesisRecordScheme: This default scheme, used by the sample topology, emits a tuple of (partitionKey, record). If you want to emit more structured data, you can provide your own implementation of IKinesisRecordScheme.\\n\\nThe samples folder includes a sample topology and sample bolt, using the number of Amazon Kinesis shards as the parallelism hint for the spout. For more information about Storm topologies and bolts, see the Storm documentation.\\nUsing the Sample\\n\\nEdit the *.properties file to configure your Storm topology, Amazon Kinesis stream, and ZooKeeper details. For your AWS Credentials, we recommend using IAM roles on Amazon EC2 when possible. You can also specify your credentials using system properties, environment variables, or AwsCredentials.properties.\\nPackage the spout and the sample (including all dependencies but excluding Storm itself) into one JAR file.\\nDeploy the package to Storm via the JAR file, e.g., storm jar my-spout-sample.jar SampleTopology sample.properties RemoteMode\\n\\nRelease Notes\\nRelease 1.1.1 (June 1, 2015)\\n\\nWhen a Kinesis Stream is resharded, \"storm rebalance\" can be invoked to refresh the shard list and distribute the latest shards across the Spout tasks.\\n\\nRelease 1.1.0 (October 21, 2014)\\n\\nAdded support for retrying failed records\\nAdded region name support\\n\\nFuture Work\\n\\nAutomatically handle closed, split, and merged shards.\\n\\nRelated Resources\\nAmazon Kinesis Developer Guide\\nAmazon Kinesis API Reference\\nAmazon Kinesis Client Library\\nAmazon Kinesis Connector Library\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '81',\n",
       "  'forks': '16',\n",
       "  'commits': '9'},\n",
       " {'language': 'C++ 92.7',\n",
       "  'readme': \"text_extraction\\nThis code is the implementation of the method proposed in the paper “Multi-script text extraction from natural scenes” (Gomez & Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\\nIncludes the following third party code:\\n\\nfast_clustering.cpp Copyright (c) 2011 Daniel Müllner, under the BSD license. http://math.stanford.edu/~muellner/fastcluster.html\\nmser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http://www.idiap.ch/~cdubout/\\nbinomial coefficient approximations are due to Rafael Grompone von Gioi. http://www.ipol.im/pub/art/2012/gjmr-lsd/\\n\\n\",\n",
       "  'watchers': '15',\n",
       "  'stars': '80',\n",
       "  'forks': '56',\n",
       "  'commits': '20'},\n",
       " {'language': 'C++ 50.2',\n",
       "  'readme': \"text_extraction\\nThis code is the implementation of the method proposed in the paper “Multi-script text extraction from natural scenes” (Gomez & Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\\nIncludes the following third party code:\\n\\nfast_clustering.cpp Copyright (c) 2011 Daniel Müllner, under the BSD license. http://math.stanford.edu/~muellner/fastcluster.html\\nmser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http://www.idiap.ch/~cdubout/\\nbinomial coefficient approximations are due to Rafael Grompone von Gioi. http://www.ipol.im/pub/art/2012/gjmr-lsd/\\n\\n\",\n",
       "  'watchers': '13',\n",
       "  'stars': '80',\n",
       "  'forks': '81',\n",
       "  'commits': '89'},\n",
       " {'language': 'C++ 69.1',\n",
       "  'readme': \"text_extraction\\nThis code is the implementation of the method proposed in the paper “Multi-script text extraction from natural scenes” (Gomez & Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\\nIncludes the following third party code:\\n\\nfast_clustering.cpp Copyright (c) 2011 Daniel Müllner, under the BSD license. http://math.stanford.edu/~muellner/fastcluster.html\\nmser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http://www.idiap.ch/~cdubout/\\nbinomial coefficient approximations are due to Rafael Grompone von Gioi. http://www.ipol.im/pub/art/2012/gjmr-lsd/\\n\\n\",\n",
       "  'watchers': '8',\n",
       "  'stars': '80',\n",
       "  'forks': '50',\n",
       "  'commits': '35'},\n",
       " {'language': 'C++ 55.6',\n",
       "  'readme': \"410 Obsolete Repository\\nThese are not the codes you are looking for\\nWe've made a ton of progress on Couchbase Mobile technologies since this repository was current. You can find the latest code in these Github projects:\\n\\nCouchbase Mobile documentation repository. Look here for an architecture overview, tutorials, and links to API docs.\\nCouchbase Lite iOS sync client. This is the latest version of TouchDB. We've changed the name because it is lighter, and built by Couchbase. TouchDB will continue to get bug fixes but new development is happening on Couchbase Lite.\\nCouchbase Lite Android is coming soon. If you need something for Android today, look at TouchDB for Android\\nLiteGap iOS PhoneGap / Cordova container for Couchbase Lite, makes it easy to build HTML5 mobile sync apps. If you are creating a PhoneGap container on Android, it should be as simple as adding TouchDB Android to the generic PhoneGap Android app.\\n\\nTo discuss Couchbase Mobile and find answers about the latest state of the art, please join our Couchbase Mobile Google Group\\nIf you really wanted the legacy code for this repository, it's available on the master branch. You are currently looking at the redirect branch. Check out the repo and switch to the master branch, or use the branch selector in the Github UI.\\n\",\n",
       "  'watchers': '141',\n",
       "  'stars': '80',\n",
       "  'forks': '8',\n",
       "  'commits': '2'},\n",
       " {'language': 'C++ 93.4',\n",
       "  'readme': 'GraphLite version 0.20\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '80',\n",
       "  'forks': '30',\n",
       "  'commits': '29'},\n",
       " {'language': 'C++ 63.0',\n",
       "  'readme': 'FaceDetect\\n\\n\\n\\n\\n\\n\\n\\n\\nFace detection for iPhone using OpenCV\\nAbout\\nThis project is an example of using OpenCV on the iPhone.\\nIt provides a simple application that let you take a picture (or select one from your photo library), and that detects the face(s) on the picture.\\nBuilding\\nThe application does not compile (yet) for the iPhone simulator.\\nYou need to build it for your iOS device in order to test it.\\nOpenCV port\\nFaceDetect uses the OpenCV port provided by Macmade (myself), and which is also available on GitHub.\\nProject Status\\nThis project is no longer maintained.\\nIt might not build on latest iOS versions, and might not be compatible with latest OpenCV versions.\\nLicense\\nFaceDetect is released under the terms of the Boost Software License - Version 1.0.\\nRepository Infos\\nOwner:\\t\\t\\tJean-David Gadina - XS-Labs\\nWeb:\\t\\t\\twww.xs-labs.com\\nBlog:\\t\\t\\twww.noxeos.com\\nTwitter:\\t\\t@macmade\\nGitHub:\\t\\t\\tgithub.com/macmade\\nLinkedIn:\\t\\tch.linkedin.com/in/macmade/\\nStackOverflow:\\tstackoverflow.com/users/182676/macmade\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '80',\n",
       "  'forks': '23',\n",
       "  'commits': '7'},\n",
       " {'language': 'C++ 85.9',\n",
       "  'readme': ' _______  _______  _______  _______  _______  _______  _______  _______  _______ \\n|       ||   _   ||       ||       ||       ||   _   ||       ||       ||       |\\n|    _  ||  |_|  ||  _____||_     _||    _  ||  |_|  ||    ___||    ___||  _____|\\n|   |_| ||       || |_____   |   |  |   |_| ||       ||   | __ |   |___ | |_____ \\n|    ___||       ||_____  |  |   |  |    ___||       ||   ||  ||    ___||_____  |\\n|   |    |   _   | _____| |  |   |  |   |    |   _   ||   |_| ||   |___  _____| |\\n|___|    |__| |__||_______|  |___|  |___|    |__| |__||_______||_______||_______|\\nThe news homepage archive. At pastpages.org.\\nHow it works\\nPastPages is built using Python, Django and PostgreSQL. Screenshots are taken by PhantomJS via a Celery task queue. The files collected are stored in the Rackspace cloud. Deployment is managed with Chef and you can review how the environment is configured in its configuration files.',\n",
       "  'watchers': '8',\n",
       "  'stars': '78',\n",
       "  'forks': '10',\n",
       "  'commits': '422'},\n",
       " {'language': 'C++ 92.7',\n",
       "  'readme': ' _______  _______  _______  _______  _______  _______  _______  _______  _______ \\n|       ||   _   ||       ||       ||       ||   _   ||       ||       ||       |\\n|    _  ||  |_|  ||  _____||_     _||    _  ||  |_|  ||    ___||    ___||  _____|\\n|   |_| ||       || |_____   |   |  |   |_| ||       ||   | __ |   |___ | |_____ \\n|    ___||       ||_____  |  |   |  |    ___||       ||   ||  ||    ___||_____  |\\n|   |    |   _   | _____| |  |   |  |   |    |   _   ||   |_| ||   |___  _____| |\\n|___|    |__| |__||_______|  |___|  |___|    |__| |__||_______||_______||_______|\\nThe news homepage archive. At pastpages.org.\\nHow it works\\nPastPages is built using Python, Django and PostgreSQL. Screenshots are taken by PhantomJS via a Celery task queue. The files collected are stored in the Rackspace cloud. Deployment is managed with Chef and you can review how the environment is configured in its configuration files.',\n",
       "  'watchers': '3',\n",
       "  'stars': '78',\n",
       "  'forks': '7',\n",
       "  'commits': '8'},\n",
       " {'language': 'C++ 74.3',\n",
       "  'readme': 'Spherical Harmonic Tools\\nCopyright © 2012–2013 — Robert Kooima\\nThis module provides a straightforward implementation of the real spherical harmonic transform and its inverse. It uses the orthonormalized associated Legendre functions, generated by an m-varying recurrence. The implementation is expressed in terms of the Cooley-Tukey fast Fourier transform, so the running time of both synthesis and analysis is O(n3) and n must be a power of two. It is parallelized using OpenMP and will use all available processor cores.\\n\\nsh.hpp\\nshtrans.cpp\\nshimage.cpp\\nsherror.cpp\\nMakefile\\n\\nCommand Line Tools\\nA few command line tools are provided to directly apply the transform to real image data. These tools are compiled with this image handling module, which supports input and output of TIFF, PNG, JPEG, and OpenEXR files of any supported depth. For reliable resynthesis, it is highly recommended that frequency-domain images be stored using 32-bit floating point samples in TIFF format, which are the defaults.\\nSpherical Harmonic Analysis\\nshtrans    [-FDL] [-o output] [-b bytes] input\\nGiven a 2n × 2n spatial-domain input image, perform a spherical harmonic analysis of degree n and produce an n × n frequency-domain output image.\\n\\n\\n-o output\\nOutput file name. Default is \"out.tif\".\\n\\n\\n-b bytes\\nOutput depth in bytes per sample. 1 requests 8-bit unsigned integer. 2 requests 16-bit unsigned integer. 4 selects selects 32-bit float.\\n\\n\\n-h width\\n-l width\\n-g width\\nAfter analysis, apply a Hanning -h, Lanczos -l, or Gauss -g filter window with the given width. A width of n is used if zero is given. Filter selection is documented below.\\n\\n\\n-d\\nAfter analysis, apply the diffuse convolution.\\n\\n\\n-F\\n-D\\n-L\\nPerform the computation using float, double, or long double values. Default is long double.\\n\\n\\nSpherical Harmonic Synthesis\\nshtrans -i [-FDL] [-o output] [-b bytes] input\\nGiven an n × n frequency-domain input image, perform a spherical harmonic synthesis of degree n and produce a 2n × 2n spatial-domain output image.\\n\\n\\n-o output\\nOutput file name. Default is \"out.tif\".\\n\\n\\n-b bytes\\nOutput depth in bytes per sample. 1 requests 8-bit unsigned integer. 2 requests 16-bit unsigned integer. 4 selects selects 32-bit float.\\n\\n\\n-F\\n-D\\n-L\\nPerform the computation using float, double, or long double values. Default is long double.\\n\\n\\nStorage of Spherical Harmonics\\nSpatial domain images are stored in a normal 2n × 2n raster with the top of the image corresponding to the north pole of the sphere. Frequency domain images are stored as an n × n raster with the following layout. Degree zero is placed at the upper left and the degree increases moving toward the lower right. Negative orders are stored in the rows and positive orders in the column, putting the \"zonal\" harmonics of order zero along the diagonal. This table shows the degree and order (l, m) corresponding to each pixel of an 8 × 8 image.\\n\\n0,        01,       +12,       +23,       +34,       +45,       +56,       +67, 7\\n1, –11,        02,       +13,       +24,       +35,       +46,       +57, 6\\n2, –22, –12,        03,       +14,       +25,       +36,       +47, 5\\n3, –33, –23, –13,        04,       +15,       +26,       +37, 4\\n4, –44, –34, –24, –14,        05,       +16,       +27, 3\\n5, –55, –45, –35, –25, –15,        06,       +17, 2\\n6, –66, –56, –46, –36, –26, –16,        07, 1\\n7, –77, –67, –57, –47, –37, –27, –17, 0\\n\\nThis is a useful layout as it casts an otherwise triangular structure into a square and distinguishes low-frequency harmonics from high-frequency. Together, these properties allow frequency domain images to be edited with common image editing software, enabling interactive spherical harmonic filtering.\\nVisualization of Spherical Harmonics\\nshimage [-o output] [-b bytes] [-c channels] [-l l] [-m m] [-n n]\\nThis tool synthesizes an example image of a single spherical harmonic function of degree l and order m. Positive values are rendered in green and negative values in red. The resulting n × n image is helpful in understanding the appearance and behavior of the spherical harmonics.\\n\\n\\n-o output\\nOutput file name. Default is \"out.tif\".\\n\\n\\n-l l\\nHarmonic degree. Default is 0.\\n\\n\\n-m m\\nHarmonic order. Default is 0.\\n\\n\\n-n n\\nSynthesis degree, which determines output image size.\\n\\n\\n-b bytes\\nOutput depth in bytes per sample. 1 requests 8-bit unsigned integer. 2 requests 16-bit unsigned integer. 4 selects selects 32-bit float.\\n\\n\\n-c channels\\nOutput channel count. 3 requests unsigned RGB where green implies positive values and red implies negative. 1 selects signed grayscale. In particular, b=1 c=3 generates a reasonable visualization of a single spherical harmonic, while b=4 c=1 generates the real value of that harmonic.\\n\\n\\nHere we see the first eight degrees and orders synthesized at degree 64. They are laid out in an 8 × 8 grid as described by the table above. The zonal harmonics are clearly visible along the diagonal and the increasing frequency is obvious toward the right and down.\\n\\nValidation of Spherical Harmonics\\nsherror [-FDL] [-n degree]\\n\\n\\n-n degree\\nAnalysis degree. Default is 1.\\n\\n\\n-F\\n-D\\n-L\\nPerform the computation using float, double, or long double values. Default is long double.\\n\\n\\nThis tool quantifies the performance and precision of the implementation. It begins with white noise, defined as frequency coefficients of one for all l and m up to n. These coefficients are synthesized at a resolution of 2n × 2n, and re-analyzed up to degree n. The round trip time is measured, and the output frequency coefficients are compared with one, giving root-mean-square error and maximum error. Upon completion, a table of results in printed to stdout including\\n\\nthe degree n,\\nthe run time in seconds,\\nthe RMS error,\\nthe log2 RMS error,\\nthe maximum absolute error, and\\nthe log2 maximum absolute error.\\n\\nThe log2 results indicate the number of bits to which the output agrees with the input, thus quantifying the numerical stability of synthesis together with analysis. Several runs of this tool are graphed below.\\nAPI\\nEach of these tools uses a spherical harmonic transformation template library given in the file sh.hpp. Templating allows the transform to be calculated using float, double, or long double built-in types. User-defined types are even supported if they overload the basic arithmetic operators and include a few of the functions of the math library.\\n\\n\\nsht<real>::sht(int n, int c)\\nConstruct a spherical harmonic transform object. All internal computation will be performed using the real type. The frequency domain representation has order n the spatial domain representation has order 2n × 2n. c gives the number of channels of both.\\n\\n\\nThis object has public attributes for input and output. S is a 2n × 2n spatial domain image with c channels of type real. F is an n × n frequency domain image with c channels of type real. These images overload the function operator allowing direct access to their contents.\\n\\n\\nreal& Flm<real>::operator()(int l, int m, int k)\\nGive a reference to the frequency sample at degree l, order m, and channel k suitable for reading or writing.\\n\\n\\nreal& Sij<real>::operator()(int i, int j, int k)\\nGive a reference to the spatial sample at row i, column j, and channel k suitable for reading or writing.\\n\\n\\nWith either the spatial domain or frequency domain image set, analysis or synthesis may be performed.\\n\\n\\nvoid sht<real>::ana()\\nPerform a spherical harmonic analysis of S giving F.\\n\\n\\nvoid sht<real>::syn()\\nPerform a spherical harmonic synthesis of F giving S.\\n\\n\\nWhen performing image IO, both S and F support bulk transfer functions that automatically cast to and from the internal floating point type to 32-bit floating point.\\n\\n\\nvoid Flm<real>::set(const float *data, int N)\\nvoid Flm<real>::get(float *data, int N)\\nTransfer 32-bit floating point data into or out of the frequency domain image. The data argument must accomodate N × N × c 32-bit floats. N need not be a power of two, and when N does not equal n then a truncated set of coefficients is accepted or provided.\\n\\n\\nvoid Sij<real>::set(const float *src)\\nvoid Sij<real>::get(float *dst)\\nTransfer 32-bit floating point data into or out of the spatial domain image. The data buffer must accommodate 2n × 2n × c 32-bit floats.\\n\\n\\nIn both cases, passing a null pointer to the set function initializes the contents of the buffer to zero.\\nExamples\\nThis segment of code shows the basic usage of the API for analysis. A square source image with power-of-two size is read from a file and a floating point destination buffer is allocated. A double precision SHT object is instanced with the desired degree and image parameters supplied to the constructor. The spatial domain input is set and the analysis is performed. Finally, the frequency domain output is acquired and written to a file. Synthesis is similar.\\nsrc = image_read_float(\"input.tif\", &w, &h, &c, &b);\\n\\nn = w / 2;\\n\\ndst = (float *) calloc(n * n * c, sizeof (float);\\n\\nsht<double> T(n, c);\\n\\nT.S.set(src);\\nT.ana();\\nT.F.get(dst);\\n\\nimage_write_float(\"output.tif\", n, n, c, b, dst);\\n\\nEnvironment Mapping\\nThe following examples demonstrate the application of the spherical harmonic tools to real-time environment mapping. We begin with this 32-bit floating-point 512 × 512 spherical panorama of a St. Peter\\'s Basilica, one of several de facto standard light probes provided by Paul Debevec, unwrapped and resampled using envtools.\\n\\nIt\\'s a common practice to map such an image onto a model to generate the appearance of a reflective mirror finish...\\n\\nor a refractive glass material with magnifying effects and more subtle reflections. Here, let me hold that up to the light for you.\\n\\nApplying the spherical harmonic transform to the cathedral image allows us to generalize the appearance of such materials through manipulation in the frequency domain. To begin, the image is analyzed at n=512 giving a 32-bit 256 × 256 floating point frequency domain image.\\nshtrans -o st-peters-sht.tif st-peters.tif\\n\\nThe output looks like this.\\n\\nWe can achieve some interesting material effects by removing some of the high-frequency harmonics from it. However, this must be done carefully. To simply crop the frequency domain image at the upper left would corrupt the synthesis with ringing artifacts. Instead, use the -g option to apply a Gaussian filter window with a width of 64.\\nshtrans -g64 -o st-peters-sht-g64.tif st-peters.tif\\n\\nHere\\'s the result.\\n\\nSynthesize this result...\\nshtrans -i -o st-peters-g64.tif st-peters-sht-g64.tif\\n\\ngiving this image.\\n\\nIt\\'s a blurry cathedral. Critically, however, this is not a simple 2D blur like that produced by a Photoshop filter. Instead, it behaves as though the blur radius were uniform at every point on the sphere, including the poles. In contrast, a 2D blur of a spherical image would behave as if the blur became increasingly thin toward the poles, producing unsightly artifacts there.\\nApplying the blurred image to the model shows an imperfect reflection and a frosted refraction, giving a much more natural and realistic material. In addition, all of these images use the width-64 blurred image as a backdrop, giving the appearance of shallow focus regardless of view direction.\\n\\n\\nWe can take this a step further. Here we apply a Gaussian filter with a width of 32.\\nshtrans -g32 -o st-peters-sht-g32.tif st-peters.tif\\n\\nThere\\'s a lot less information in the frequency domain.\\n\\nTherefore there\\'s much more blur in the spatial domain. With only 32 degrees of spherical harmonics, only the very low frequencies remain.\\nshtrans -i -o st-peters-g32.tif st-peters-sht-g64.tif\\n\\n\\nThe resulting reflection no longer resembles chrome, and instead takes on the character of pewter or brushed aluminum. In the pinkish glow of St. Peter\\'s Basilica, it almost looks like copper. In comparison, the perfect reflection provided by the original unfiltered environment map looks downright fake.\\n\\nThe refractive effect is that of diffuse or frosted glass.\\n\\nDiffuse Convolution\\nThese are all specular illumination effects, but the spherical harmonic transform provides a means to extend environment mapping into diffuse illumination, as described by Ramamoorthi and Hanrahan in their 2001 SIGGRAPH paper An Efficient Representation for Irradiance Environment Maps. In this work, Ramamoorthi and Hanrahan note that diffuse illumination is essentially convolution with a cosine-weighted hemisphere, and demonstrate that the resulting irradiance need be represented using only three degrees of spherical harmonics.\\nWe can apply the diffuse convolution using the -d option.\\nshtrans -d -o st-peters-sht-dif.tif sh-peters.tif\\n\\nIndeed, only the top-left 3 × 3 block of pixels contains much data. There are a few dim pixels elsewhere, but the literature demonstrates that eliminating these will result in an average error of at most 3%.\\n\\nThere\\'s so little information here that we could validly crop that image down to 4 × 4 (a power of two), resulting in an 8 × 8 synthesis with no significant loss of information. But for the sake of consistency with the other examples...\\nshtrans -i -o st-peters-dif.tif sh-peters-sht-dif.tif\\n\\nThe resulting irradiance environment map is not just very heavily blurred, it actually gives a weighted sum of every input pixel visible at every possible orientation. The sphere map represents the total light falling upon every point on a sphere from every direction. In this example, St. Peter\\'s is well lit from above, and diffuse reflection of the marble gives a pink ambient glow.\\n\\nWhen rendering, a single texture reference is made along the object normal. This returns a value equivalent to the usual diffuse lighting calculation, but done for every light source in the room simultaneously. The resulting material is a perfectly matte white, like unglazed ceramic.\\n\\nCombining this with one of the reflection maps above gives a specular effect, like glazed ceramic, the material of the original Utah teapot.\\n\\nOf course, color and gloss texture maps and normal maps can be combined with these blurred and convolved environment maps, giving very rich and extremely realistic materials.\\nHere\\'s a summary image giving all of the examples together. Click to enlarge.\\n\\nFilter Selection\\nThe choice of filter is an important one. Ringing artifacts will always arise from a spherical harmonic synthesis with an equirectangular projection. This is because the equirectangular projection badly matches the true distribution of information on the sphere. In particular, the spatial resolution near the poles is far higher than near the equator, so any reasonable set of spherical harmonic coefficients will be band-limited at high latitude. The available filters will handle different types of images with varying levels of success. The following examples are intended to help clarify this, but ultimately, experience gained through trial and error are most valuable.\\nThe three filter windows are graphed here for n=256. The Gauss window, in red, has the familiar shape of the bell curve, dropping off rapidly and approaching zero smoothly. The Hann (a.k.a. Hanning) window, in green, is a cosine wave with equal balance. The Lanczos window (in blue) is the first lobe of the sinc function, dropping off slowly and meeting zero abruptly.\\n\\nWhen the filter width is less than n, the Gauss and Hann windows still clamp out at zero. However, the Lanczos window will oscillate about zero as one would expect from the sync function.\\nThe following example uses a 512 × 512 image of a 16 × 16 black-white checker pattern. With its high contrast, it represents a worst-case scenario for 8-bit images, and will demonstrate the best choice for round-trip frequency-domain operations upon common spherical images.\\nThe checker input is analyzed and re-synthesized four times. Image A shows the center of the input, which covers the equator where the spherical harmonics match the information density in the image well. Image B shows a synthesis of an unfiltered set of coefficients. The ringing is obvious even in this easy area. Images C, D, and E shows the Gauss, Hann, and Lanczos filters, with each sharper than the last.\\n\\nHere is the same set of images, showing instead a portion of the checker pattern in the troublesome region near the north pole. Image B, with no filtering, is dominated by ringing artifacts. The Gauss, Hann, and Lanczos filters are, again, increasingly sharp. Ringing is generally under control, though slightly apparent in the Lanczos image when magnified.\\n\\nWe can infer from this that some form of filtering is necessary. The Lanczos filter preserves the sharpness in the input best, and is probably the go-to choice when doing frequency-domain manipulations of spherical data sets. Blurring is expected and usually even desirable near the poles, especially if the filtered output is to be mapped onto a sphere for real-time texture mapping. Under such circumstances the Gauss filter might produce the most visually appealing results.\\nThings change when we shift to high dynamic range inputs, such as the light probe of St. Peter\\'s Basilica used in the teapot renderings, above. The following images show the results of each filter applied with a width of 32, cropped at top of the image.\\n\\nThe Gauss filtering in image A gives a nice, smooth blur. The Hann window, image B, does not handle extremely bright light sources well at all, and produces a severe overshoot. The Lanczos window, image C, produces a surprising kind of bokeh effect with high-frequency ringing produced by the many lobes of the tail of the sinc. It\\'s interesting, but not what we\\'re looking for. For high dynamic range light probes, stick with Gauss.\\nIn general, there will always be a trade-off between sharpness and ringing. The type of the data and its intended usage will determine how this trade-off is best made.\\nTests\\nConformance\\nAs a basic eye-ball level test of conformance, the shtrans tool was used to synthesize the EGM 2008 data set, the Earth Gravitational Model consisting of 2190 degrees of spherical harmonic coefficients. The 8192 × 8192 grayscale output was gradient-mapped using GIGO and scaled down using Photoshop. It matches.\\n\\nPrecision\\nThe sherror tool tests the numerical precision of the spherical harmonic transform implementation by synthesizing and re-analyzing white noise. The task is performed at degrees from n= 21 to 212 using single, double, and long double precision floating point values. The output is compared with the input and the degree to which the two match is quantified.\\nThe software was compiled for OSX 10.8 using g++ 4.7.2. The test hardware used 2 × 2.0 GHz eight-core Intel Xeon E5 processors. Data shown here was collected using 16 OpenMP threads.\\nHere are the round trip times, with time in seconds plotted on a log-10 scale. Single precision floating point performance is shown in red, double in green, and long double in blue. G++\\'s long double is in fact an Intel-native 80-bit float, which is the same type that implements single and double precision calculation, so performance parity is not a big surprise. Their stored size varies though, so these results would indicate that the process is not cache-bound on this hardware. The computation is effectively instantaneous when n is smaller than 6 and the resolution of the timer was not sufficient to demonstrate any distinction.\\n\\nHere we see the agreement between the input and the round trip output. Specifically, this graph shows the average number of bits to which each pair of values coincides, as given by the base-2 log of their root-mean-square difference. Single precision floating point (in red) gives at most 24 bits of useful precision and sees a precipitous drop in stability past n=24, with a total failure past n=27. Double precision (in green) gives 50 bits and sees a similar limit at n=27. Long double (in blue) continues to n=211. This places an upper bound on the input size that this utility can usefully handle when instanced using native data types: 2048.\\n\\nHere is the base-2 log maximum difference. Again, it shows the number of binary digits to which the input and output values agree. The results are largely the same as the RMS, and this test is done merely to demonstrate that the RMS does not average away any outliers.\\n\\nThese numbers show that the current implementation is useful in a number of circumstances, but not as powerful as we would like. The use of extended-range arithmetic (Lozier & Smith) will take us beyond the inevitable failure of long double precision, and a GPU-cluster implementation will extend the performance envelope by sheer brute force.\\nOn the bright side, the spherical harmonic transform is a trivially parallelizable process that fully benefits from all floating point capacity placed at its disposal. He we see the speed-up of the round trip test run at n=29 in double precision with from one to 32 threads. Speed-up is linear to 16 threads, with 90% efficiency, as this is the number of real processor cores in the test system. They\\'re hyperthreaded cores, and modest gains can be achieved by overcommiting them, though this is not reliable.\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '77',\n",
       "  'forks': '12',\n",
       "  'commits': '9'},\n",
       " {'language': 'JavaScript 56.2',\n",
       "  'readme': \"Node-mysql-libmysqlclient \\nAsynchronous MySQL binding for Node.js using libmysqlclient.\\nThis module has been tested with Node.js versions 0.8.26, v0.10.25 and 0.11.10\\nOverview\\nThese bindings provides all general connection/querying functions from the MySQL C API,\\nand partial support for prepared statements. Connect, query and fetchAll are asynchronous.\\nThis module also includes support for asynchronous querySend from internals of libmysqlclient.\\nI started this project in 2010 when Node.js was growing. Ryan had plans to write this binding as part of GSoC.\\nIt is now used by many projects and has more than 10 contributors,\\nwho are listed in the AUTHORS file.\\nI also maintain the Node.js MySQL bindings benchmark which shows how mysql-libmysqlclient performs.\\nNode-mysql-libmysqlclient's source code is available in the Github repo and you can report issues at the project tracker.\\nVisit the module site for API docs and examples. You can also read some extra information in wiki.\\nDependencies\\nTo build this module you must install the libmysqlclient library and the development files for it.\\nmysql_config is used to determine the paths to the library and header files.\\nTo install these dependencies, execute the commands below for the OS you're running.\\nFor CentOS:\\n#> yum install mysql-devel\\n\\nFor openSUSE:\\n#> zypper install libmysqlclient-devel\\n\\nFor Debian-based systems/Ubuntu:\\n#> apt-get install libmysqlclient-dev\\n\\nAlternatively, you can use aptitude for Debian-based systems.\\nPlease refer to your system's documentation for more information and feel free to send me a patch for this readme.\\nInstallation\\nYou can install this module via NPM:\\n$> npm install mysql-libmysqlclient\\n\\nYou can also build latest source code from repository.\\nPlease refer to the developers documentation for more information.\\nContributing\\nThis module is written in collaboration with many peoples listed on GitHub contributors page.\\nList of authors ordered by first contribution also available.\\nIf you are interested in wide MySQL usage in Node.JS applications,\\nleave your comments to the code.\\nTo contribute any patches, simply fork this repository using GitHub\\nand send a pull request to me. Thanks!\\nAll information about development use and contribution is placed in the DEVELOPMENT file.\\nUsers and related projects\\nThis module is used by Taobao guys\\nfor their distributed MySQL proxy Myfox-query module.\\nThere is long time developed Node.js ORM library called noblerecord.\\nIt is inspired by Rails and widely used by Noblesamurai.\\nIf you are looking for lightweight Node.js ORM on top of this module,\\ntry mapper by Mario Gutierrez.\\nLicense\\nNode-mysql-libmysqlclient itself is published under MIT license.\\nSee license text in LICENSE file.\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '228',\n",
       "  'forks': '48',\n",
       "  'commits': '832'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'grunt-env \\nSpecify an ENV configuration as a task, e.g.\\ngrunt.registerTask(\\'dev\\', [\\'env:dev\\', \\'lint\\', \\'server\\', \\'watch\\']);\\ngrunt.registerTask(\\'build\\', [\\'env:build\\', \\'lint\\', \\'other:build:tasks\\']);\\n\\nGetting Started\\nInstall this grunt plugin next to your project\\'s grunt.js gruntfile with: npm install grunt-env\\nThen add this line to your project\\'s grunt.js gruntfile:\\ngrunt.loadNpmTasks(\\'grunt-env\\');\\nConfiguration\\n  env : {\\n    options : {\\n \\t//Shared Options Hash\\n    },\\n    dev : {\\n      NODE_ENV : \\'development\\',\\n      DEST     : \\'temp\\'\\n    },\\n    build : {\\n      NODE_ENV : \\'production\\',\\n      DEST     : \\'dist\\',\\n      concat   : {\\n        PATH     : {\\n          \\'value\\': \\'node_modules/.bin\\',\\n          \\'delimiter\\': \\':\\'\\n        }\\n      }\\n    },\\n    functions: {\\n      BY_FUNCTION: function() {\\n        var value = \\'123\\';\\n        grunt.log.writeln(\\'setting BY_FUNCTION to \\' + value);\\n        return value;\\n      }\\n    }\\n  }\\nUsing external files\\nYou can specify environment values in INI, JSON or YAML style and load them via the src option.\\n  env : {\\n    dev : {\\n      src : \"dev.json\"\\n    },\\n    prod: {\\n      src: \"settings.yaml\"\\n    }\\n    heroku : {\\n      src : \".env\"\\n    }\\n  }\\nUsing envdir\\nYou can specify files to read environment variables from, similar to the daemontools envdir utility.\\n  env : {\\n    dev : {\\n      src : [\"envdir/*\"],\\n      options: {\\n        envdir: true\\n      }\\n    }\\n  }\\nDynamic ENV configuration\\nThe following directives can be specified in the options to alter the environment in more specific ways\\n\\nadd\\n\\nThis will add the variables only if they don\\'t already exist\\n\\n\\nreplace\\n\\nWill replace the variable with the value specified\\n\\n\\nunshift\\n\\nWill prepend the value to the variable specified, optionally specifying a \\'delimiter\\'\\n\\n\\npush\\n\\nSame as unshift, but at the end of the value.\\n\\n\\nconcat\\n\\nFunctionally same as push, added for readability\\n\\n\\n\\nyourtask : {\\n  USER : \\'you\\',\\n  PATH : \\'/bin:/usr/bin\\'\\n\\n  options : {\\n    add : {\\n      VERBOSE : \\'1\\' // will only be added if VERBOSE isn\\'t already set\\n    },\\n    replace : {\\n      USER : \\'me\\'\\n    },\\n    push : {\\n      PATH : {\\n        value : \\'~/bin\\',\\n        delimiter : \\':\\'\\n      }\\n    },\\n    unshift : {\\n      PATH : \\'/sbin:\\'\\n    }\\n  }\\n}\\n\\nEnvironment-specific configuration\\nIn order to configure your tasks based on the environment, you need to define a task and use templates:\\n\\ngrunt.initConfig({\\n  env: {\\n    dev: {\\n      MY_CONST: \\'a\\'\\n    },\\n    prod: {\\n      MY_CONST: \\'b\\'\\n    }\\n  },\\n  myTask: {\\n    options: {\\n      myOpt: <%= MY_CONST %>\\n    }\\n  }\\n});\\n\\ngrunt.registerTask(\\'loadconst\\', \\'Load constants\\', function() {\\n    grunt.config(\\'MY_CONST\\', process.env.MY_CONST);\\n});\\n\\ngrunt.registerTask(\\'default\\', [\\n    \\'env:dev\\',\\n    \\'loadconst\\',\\n    \\'myTask\\'\\n]);\\n\\n\\nImportant note on data types\\nEnvironment variables are strings only. If you attempt to assign complex objects, they will be converted to strings.\\nContributing\\nIn lieu of a formal styleguide, take care to maintain the existing coding style. Add unit tests for any new or changed functionality. Lint and test your code using grunt.\\nRelease History\\n\\n0.4.0 Removed automatic parse, added ability to add ini or json style src files\\n0.3.0 Automatically parses .env files now\\n0.2.1 fixed npm install\\n0.2.0 grunt 0.4.0 support, simplified\\n0.1.0 Initial release\\n\\nLicense\\nLicensed under the Apache 2.0 license.\\nAuthor\\nJarrod Overson\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '226',\n",
       "  'forks': '39',\n",
       "  'commits': '66'},\n",
       " {'language': 'JavaScript 96.6',\n",
       "  'readme': 'Chart\\nAscii bar chart for nodejs.\\n\\nInstallation\\n$ npm install jstrace/chart\\n\\nExample\\nWhen data exceeds the available width the data will \"roll\" to the tail-end\\nof the array. This may become an option in the future, but that\\'s the default\\nbehaviour for now ;)\\nvar chart = require(\\'chart\\');\\nvar clear = require(\\'clear\\');\\n\\nvar data = [1, 2, ...];\\n\\nclear();\\nconsole.log(chart(data, {\\n  width: 130,\\n  height: 30,\\n  pointChar: \\'█\\',\\n  negativePointChar: \\'░\\'\\n}));\\nLicense\\nMIT\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '225',\n",
       "  'forks': '19',\n",
       "  'commits': '32'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'AlphaBeta\\nAlphaBeta lets you build split tests (A/B tests) directly into\\nyour React app.\\nAlphaBeta is...\\n\\ndeclarative: Like React itself, AlphaBeta benefits from the advantages of declarative programming. The AlphaBeta component is, in fact, just a special type of React component that \"wraps\" the component variants you\\'re testing.\\nlightweight: The AlphaBeta package is small and the AlphaBeta component is thin, so AlphaBeta tests won\\'t measurably increase the time it takes for your application to render. Since AlphaBeta is so lightweight, you can be less selective about what you choose to test - it may even make sense to run single-variant tests, wrapping components you may test in the future in an AlphaBeta component today in order to establish a baseline for comparison.\\nbackend agnostic: Since split testing requires that you store event data, AlphaBeta needs to communicate with a datastore in order to work. But AlphaBeta will work with whatever datastore you\\'re currently using - just follow the instructions in Backend / API Setup to build your endpoint, point AlphaBeta to it, and you\\'re good to go.\\nextensible: AlphaBeta is designed to make it easy for developers to integrate basic split tests into their React apps without having to think about the underlying statistics. But it\\'s also possible to build your own custom logic around how confidence intervals are calculated and how user cohorting works within your app.\\n\\nBuilding your first A/B test is simple:\\nimport { ABComponent } from \\'react-alpha-beta\\';\\n\\nclass ButtonA extends React.Component {\\n  render() {\\n    return (<Button onClick={this.props.successAction}\\n                    style={{\\'background-color\\':\\'blue\\'}}>\\n              \"Sign Up\"\\n            </Button>);\\n  }\\n}\\n\\nclass ButtonB extends React.Component {\\n  render() {\\n    return (<Button onClick={this.props.successAction}\\n                    style={{\\'background-color\\':\\'orange\\'}}>\\n              \"Sign Up\"\\n            </Button>);\\n  }\\n}\\n\\nclass Page extends React.Component {\\n    render() {\\n        return (\\n            <div>\\n                <ABComponent\\n                    ComponentA={ButtonA}\\n                    ComponentB={ButtonB}\\n                    experimentParams={{\\n                      id: \\'1\\',\\n                      testCohortSize: 0.4,\\n                    }}\\n                />\\n            </div>\\n        );\\n    }\\n}\\n\\nReactDOM.render(\\n    <Page />,\\n    document.getElementById(\\'app\\')\\n);\\nYour experiment results will look something like this:\\n{\\n  meanDifferenceValue: -0.05023923444976075,\\n  marginOfError: 0.04837299277280508,\\n  statisticalSignificance: true,\\n  details: \"Our best estimate is that the absolute rate of success is 5% lower with variant B, and this result is statistically significant (We are 95% confident the true difference is between -10% and 0%.). Given this information, you should probably stick with variant A.\",\\n}\\nInstallation\\n$ npm install react-alpha-beta --save\\nOverview and Basic Usage\\nThe AlphaBeta component is a React component that \"wraps\" two other components. These two \"wrapped\" components are passed as ComponentA and ComponentB, and they represent the two variants you\\'re testing. Each user that encounters the AlphaBeta component will see one of the two variants, and the AlphaBeta component will report back to your server (i) which variant was displayed and (ii) if a success event occurred.\\nIn addition to your ComponentA and ComponentB variants, you\\'ll also pass experimentParams to each of your AlphaBeta components. experimentParams is an object containing the keys id and testCohortSize.\\nid is the unique id of a particular experiment, and is passed by the AlphaBeta component to your Backend / API (described below). Each AlphaBeta component that you declare should have a unique id associated with it.\\ntestCohortSize is a number between 0.0 and 1.0. Its value tells your AlphaBeta component what proportion of your users will see each experiment variant. A testCohortSize value of 0.01 means that 1% of your users should see the ComponentB variant - the other 99% should see ComponentA. A value of .5 indicates that there should be an even split between the two variants.\\nWhen you wrap your variants in an AlphaBeta component, the AlphaBeta component passes the prop successAction to each of them.\\nYou get to decide what constitutes \"success\" in the context of your experiment. If you\\'re testing a button variation, \"success\" might be defined as a click (this is the case in the above code sample). If you\\'re testing a landing page variation, \"success\" might be defined as submitting a validated form.\\nNote that while you have the ability to define \"success\" however you want, it is also your responsibility to make sure that successAction is fired by each of your variants when \"success\" occurs. Otherwise, AlphaBeta will have no way of giving you guidance about which variant is more likely to produce the desired outcome.\\nThe Button example is designed to help you get comfortable using the AlphaBeta component. You\\'ll need to set up your Backend / API for the example to work (instructions below), but reading through the example may help you better understand how to use AlphaBeta, even prior to fully setting things up.\\nBackend / API Setup\\nWait, AlphaBeta Needs a Backend?\\nIn order for AlphaBeta to be useful, it needs to be able to record data about the experiments you\\'re running. In other words, it needs to be linked to a datastore of some type. This reliance on a datastore isn\\'t unique to AlphaBeta - it is true of split testing in general.\\nImagine that you\\'re running an experiment to see if changing a particular button from a transparent background (variant A) to a solid blue background (variant B) leads to more clicks. (If you already looked at the Button example, this should look familiar...)\\nTo measure which variant performs better, you need to keep track of each variant\\'s \"impressions\" (how many users have seen each button) and \"conversions\" (how many times each button is clicked).\\nWhen we are able to keep track of these values, all it takes is a bit of math to estimate (within a specific range or \"confidence interval\") which button leads to more conversions. AlphaBeta handles this math for you, but you\\'re responsible for logging the events themselves in your datastore.\\nSo How Do I Set Up My AlphaBeta Endpoint?\\nYou can connect AlphaBeta to a datastore you\\'re already using in two steps.\\nStep 1:\\nSet up an API endpoint for AlphaBeta to Consume\\nAlphaBeta expects to be able to interact with an endpoint at www.yoursite.com/api/alphabeta/{{experimentId}}/, where {{experimentId}} is the unique id you pass to each AlphaBeta component in experimentParams.\\nAlphaBeta will both POST to and GET from this endpoint. When AlphaBeta detects an \"impression\" or a \"conversion\", it will POST to this endpoint, so all users who may encounter an experiment should be able to POST to this endpoint.\\nYou can safely restrict GET requests to only allow access to users who should be able to see data about your experiments.\\nIt\\'s also a good idea (though not strictly necessary) to set up your endpoint such that GET requests made without an {{experimentId}} return a list of your experiments. This is a good idea if you wish to build a single page where you can view data about all of our experiments.\\nEnsure Your Endpoint Accepts POST Requests Correctly\\nWhen AlphaBeta POST data to your endpoint, the POST body should look like this:\\n{\\n    variant: \"a\",             // this will either be \"a\" or \"b\"\\n    success: null,            // this will either be null or true\\n    userCohortValue: .10392,  // a number between 0 and 1\\n    metaId: null,             // this will be null unless you choose to set it\\n}\\n\\n\\nvariant tells your datastore which component variant (A or B) was presented to a particular user.\\n\\n\\nsuccess tells your datastore whether the success event occurred (true) or not (null).\\n(Note that the value for this parameter will either be true or null, as opposed to true or false. When success is passed as null, that signals that an impression has occurred. It is passed as null because when the component is loaded we don\\'t know if the user will trigger the success event or not. When success is passed as true, that signals that a success event has occurred.)\\n\\n\\nuserCohortValue is a number between 0.0 and 1.0 that AlphaBeta has associated with the particular user in this experiment. This number is randomly generated the first time a user encounters a particular AlphaBeta experiment, and is core to how AlphaBeta separates users into cohorts.\\n\\n\\nmetaId is a value that you can optionally pass to your AlphaBeta component. It should be used in cases where the component that you\\'re testing occurs more than one time times on your site.\\n\\n\\nHere is an example of when you would set the metaId attribute:\\nImagine you instead were testing the copy on a Facebook-style \"like\" button to see if changing \"like\" to \"+1\" led to more engagement. Each piece of content a user views in his/her news feed should have a \"like\" (or \"+1) button below it. But since each user has multiple items in his/her feed, a single user could \"like\" more than one piece of content.\\nIn this case, you could set a metaId that uniquely identifies the piece of content being \"liked\". If you were to set the metaId, you would be testing which variant leads to more total likes per unit of content seen. If you were to not set the metaId, you would be testing which variant is more likely to lead to a user liking at least one piece of content.\\nEnsure Your Endpoint Responds to GET Requests Correctly\\nWhen AlphaBeta GETs data from your endpoint, the returned data should look like this\\n{\\n  variantA: {\\n    trialCount:   291,  // the number of unique impressions for this variant\\n    successCount: 59,   // the number of unique success events for this variant\\n  },\\n  variantB: {\\n    trialCount:   101,\\n    successCount: 22,\\n  },\\n  confidenceInterval: .95 // the CI you\\'re looking to achieve, expressed as a float\\n}\\nStep 2:\\nEnsure Your Back End Processes POST Requests Correctly\\nWhen POST data is received, one of three things is supposed to happen:\\n1 - the trialCount for an experiment variant could be incremented by 1.\\n2 - the successCount for an experiment could be incremented by 1.\\n3 - Nothing at all.\\nThe logic for what should happen must be executed by your application\\'s backend. Here\\'s how things should work:\\n\\n\\nif success === null and no previous trial exists where both userCohortValue and metaId are equal to this trial\\'s values, you should increment trialCount by one for the appropriate variant.\\n\\n\\nif success === true and no previous trial exists where both userCohortValue and metaId are equal to this trial\\'s values and success === true, you should increment successCount by one for the appropriate variant.\\n\\n\\nin all other cases, you should not take any action.\\n\\n\\nChecking Your Experiment Results\\nimport { getExperimentData } from \\'react-alpha-beta\\';\\n\\nconsole.log(getExperimentData(experimentId));\\nTo view your experiment results, call the getExperimentData function with the experimentId for a particular experiment. It will return a json object with the keys meanDifferenceValue, marginOfError, statisticalSignificance, and details.\\n\\nmeanDifferenceValue is AlphaBeta\\'s best estimate (or mean estimate) of ComponentB\\'s performance relative to ComponentA. A positive number indicates that ComponentB is leading to more successActions per impression than ComponentA, while a negative number indicates the opposite.\\n\\nNote that meanDifferenceValue alone doesn\\'t mean much if the experiment hasn\\'t yet reached statistical significance.\\n\\n\\nmarginOfError is the margin of error (or uncertainty) that exists in the experiment.\\n\\n\\nstatisticalSignificance, a boolean, represents whether this experiment has yet reached statistical significance at the level of confidence you defined.\\n\\n\\ndetails is a human readable description of this experiment\\'s current results.\\n\\n\\nSample result from getExperimentData:\\n{\\n  meanDifferenceValue: -0.05023923444976075,\\n  marginOfError: 0.04837299277280508,\\n  statisticalSignificance: true,\\n  details: \"Our best estimate is that the absolute rate of success is 5% lower with variant B, and this result is statistically significant (We are 95% confident the true difference is between -10% and 0%.). Given this information, you should probably stick with variant A.\",\\n}\\n\\nExample\\nNote: in order for this example to work, you must first set up an API endpoint for AlphaBeta to consume. If you haven\\'t done this yet, follow the steps in Backend / API Setup\\n\\nButton example: Set up an experiment to see which of two button variants has a greater click-through rate. This example covers (i) basic experiment setup, (ii) the two ways to pass your variant components to the AlphaBeta component, and (iii) basic usage of the AlphaBeta DevTools.\\n\\nAlphaBeta DevTools\\nAlphaBeta comes with a DevTools component that can be used on any page containing an experiment.\\nimport { ABComponent, DevTools } from \\'react-alpha-beta\\';\\n\\n// ***\\n// Build your experiment component, which we\\'ll call <Page /> here.\\n// Make sure that <DevTools /> is in your <Page /> component.\\n// ***\\n\\nReactDOM.render(\\n    <Page />,\\n    document.getElementById(\\'app\\')\\n);\\nOne easy way to familiarize yourself with the DevTools component is to load the Button example.\\nIf the DevTools component is included on your page and you are not in a production environment (i.e. process.env.NODE_ENV !== \\'production\\'), you should see a DevTools box in the lower right hand corner of your screen. This box lets you control your user cohort value for each of the experiments on the page. Recall that the user cohort value for an experiment, along with the testCohortSize parameter, determine which variant a user sees.\\nIf the user cohort value is greater than or equal to testCohortSize, the user will see variant A for this experiment. If the user cohort value is less than testCohortSize, the user will see variant B. When you manipulate the DevTools sliders, you are changing your user cohort value for an experiment. These changes will take place when you refresh the page.\\nYou can add the DevTools component to the lower level components that contain your experiments, or to higher level components of your application.\\nDiscussion and Support\\nJoin our Slack team!\\nAdditional Resources\\n\\nA/B testing course (Udacity)\\nHypothesis testing with one sample (Khan Academy)\\n\\nLint\\n$ npm run lint\\nTest\\n$ npm run test        # run once\\n$ npm run test:watch  # continuous testing as file changes\\n$ npm run test:cov    # generate test coverage report\\nContribute\\nWe are using commitizen to make commit format consistent.\\n# Install the command line tool.\\n$ npm install -g commitizen\\n\\n# From then on, whenever you would like to commit:\\n$ git add .\\n$ git cz\\n# ... follow the prompt messages\\nLicense\\nMIT\\nCredits\\n\\nJack McCloy\\nBrian Park\\nBen Hall\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '222',\n",
       "  'forks': '6',\n",
       "  'commits': '170'},\n",
       " {'language': 'JavaScript 92.8',\n",
       "  'readme': 'Famous Examples\\n##DEPRECATED: please find our examples inside of our main repo\\nLicense\\nAll the code in the src/examples folder is licensed under the MIT license. This is basically a better MIT license since it removes the ambiguous language from the original MIT.\\nThe famous framework source code found in /src/famous is a git submodule cloned from the famous/famous git repo, and is licensed only under the MPL-2.0 license. More information about the licensing of that code can be found in the original repo.\\nMIT License (everything in src/examples)\\nCopyright (c) 2014 Famous Industries, Inc.\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nMPL-2.0 (everything in src/famous)\\nCopyright (c) 2014 Famous Industries, Inc.\\nThis Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\\n',\n",
       "  'watchers': '57',\n",
       "  'stars': '217',\n",
       "  'forks': '75',\n",
       "  'commits': '214'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Co Mocha\\n\\n\\n\\n\\n\\nEnable support for generators in Mocha tests using co.\\nUse the --harmony-generators flag when running node 0.11.x to access generator functions, or transpile your tests using traceur or regenerator.\\nInstallation\\nnpm install co-mocha --save-dev\\n\\nUsage\\nJust require the module in your tests and start writing generators in your tests.\\nit(\\'should do something\\', function * () {\\n  yield users.load(123)\\n})\\nNode\\nInstall the module using npm install co-mocha --save-dev. Now just require the module to automatically monkey patch any available mocha instances. With mocha, you have multiple ways of requiring the module - add --require co-mocha to your mocha.opts or add require(\\'co-mocha\\') inside your main test file.\\nIf you need to monkey patch a different mocha instance you can use the library directly:\\nvar mocha = require(\\'mocha\\')\\nvar coMocha = require(\\'co-mocha\\')\\n\\ncoMocha(mocha)\\n<script> Tag\\n<script src=\"co-mocha.js\"></script>\\nIncluding the browserified script will automatically patch window.Mocha. Just make sure you include it after mocha.js. If that is not possible the library exposes window.coMocha, which can be used (window.coMocha(window.Mocha)).\\nAMD\\nSame details as the script, but using AMD requires instead.\\nHow It Works\\nThe module monkey patches the Runnable.prototype.run method of mocha to enable generators. In contrast to other npm packages, co-mocha extends mocha at runtime - allowing you to use any compatible mocha version.\\nLicense\\nMIT\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '217',\n",
       "  'forks': '19',\n",
       "  'commits': '74'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Co Mocha\\n\\n\\n\\n\\n\\nEnable support for generators in Mocha tests using co.\\nUse the --harmony-generators flag when running node 0.11.x to access generator functions, or transpile your tests using traceur or regenerator.\\nInstallation\\nnpm install co-mocha --save-dev\\n\\nUsage\\nJust require the module in your tests and start writing generators in your tests.\\nit(\\'should do something\\', function * () {\\n  yield users.load(123)\\n})\\nNode\\nInstall the module using npm install co-mocha --save-dev. Now just require the module to automatically monkey patch any available mocha instances. With mocha, you have multiple ways of requiring the module - add --require co-mocha to your mocha.opts or add require(\\'co-mocha\\') inside your main test file.\\nIf you need to monkey patch a different mocha instance you can use the library directly:\\nvar mocha = require(\\'mocha\\')\\nvar coMocha = require(\\'co-mocha\\')\\n\\ncoMocha(mocha)\\n<script> Tag\\n<script src=\"co-mocha.js\"></script>\\nIncluding the browserified script will automatically patch window.Mocha. Just make sure you include it after mocha.js. If that is not possible the library exposes window.coMocha, which can be used (window.coMocha(window.Mocha)).\\nAMD\\nSame details as the script, but using AMD requires instead.\\nHow It Works\\nThe module monkey patches the Runnable.prototype.run method of mocha to enable generators. In contrast to other npm packages, co-mocha extends mocha at runtime - allowing you to use any compatible mocha version.\\nLicense\\nMIT\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '216',\n",
       "  'forks': '12',\n",
       "  'commits': '4'},\n",
       " {'language': 'JavaScript 86.3',\n",
       "  'readme': 'webpack ES6 demo\\nA small demo project that shows how to use webpack for client-side development in ECMAScript 6.\\nInstallation\\n\\nInstall  node\\nrun npm install\\n\\nUsage\\n\\nnpm run watch to start Webpack in watch mode - will recompile when you change a file.\\nopen index.html in a browser.\\nChange or add files in es6 folder. main.js is the entry point.\\nReload the browser when you have made a change.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '216',\n",
       "  'forks': '51',\n",
       "  'commits': '16'},\n",
       " {'language': 'JavaScript 97.4',\n",
       "  'readme': 'Javascript Refactor plugin for Sublime Text 2 and 3\\n \\n[![Package Control](https://packagecontrol.herokuapp.com/downloads/JavaScript%20Refactor.svg?color=50C32E)](https://packagecontrol.io/packages/JavaScript%20Refactor)\\n[![Donate](http://s-a.github.io/donate/donate.svg)](http://s-a.github.io/donate/)\\nOverview\\n\\nGoto definition of a variable or function\\nRename variable or function respecting its current scope\\nIntroduce variable\\nExtract selected source to a new method\\n\\nPreview\\nhttp://www.youtube.com/watch?v=P9K7mxWItPw\\nInstallation\\nUse the Sublime Package Control and search for: \"JavaScript Refactor\"\\nor\\nClone or download the git repository into your packages folder.\\nIn Sublime Text use \"Preferences/Browse Packages\" menu item to open this folder.\\nThe shorter way of doing this is:\\nLinux\\ngit clone https://github.com/s-a/sublime-text-refactor.git ~/.config/sublime-text-2/Packages/sublime-text-refactor\\nMac\\ngit clone https://github.com/s-a/sublime-text-refactor.git ~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2/Packages/sublime-text-refactor\\nWindows\\nIn an elevated command prompt or powershell (as Admin):\\ngit clone https://github.com/s-a/sublime-text-refactor.git \"%APPDATA%\\\\Sublime Text 2\\\\Packages\\\\sublime-text-refactor\"\\nfor Sublime Text 3:\\ngit clone https://github.com/s-a/sublime-text-refactor.git \"%APPDATA%\\\\Sublime Text 3\\\\Packages\\\\sublime-text-refactor\"\\nDependencies\\n\\nThis Plugin makes heavy usage of Node.js. So it needs a local installation of http://nodejs.org\\nmocha (only for testing)\\n\\nUsage\\nGoto Definition:\\nSelect a keyword via double click or point the cursor to the keyword and choose \"Goto Definition\" from context menu.\\nRename:\\nSelect a keyword via double click or point the cursor to the keyword and choose \"Rename\" from context menu. The plugin will select all variables or function calls occurring in the source code including its declaration. After that you rename them all on the fly. The logic respects the variables or functions scope. So it should be safe to rename them all without thinking ;) .\\nIntroduce Variable:\\nSelect an Expression from source code or point the cursor to the desired position and choose \"Introduce Variable\" from context menu.\\nExtract Method:\\nSelect the source code you want to extract into a new method and choose \"Refactor / Extract methode\" from context menu.\\nThis will extract the source code instantly to a new methode aka function. The plugin will manage undeclared variable usages and pass them within a single bundled JSON parameter to the new function.\\nIt also generates a sample function call at the bottom of the new methode.\\nThe plugin marks all variables occurring in the source code so you can rename them on the fly.\\nRun the tests\\nGoto Pluginfolder and type\\nnpm test\\nYou can find current test cases here\\nhttps://github.com/s-a/sublime-text-refactor/blob/master/js/test/\\nTroubleshoot\\n\\nNode not found\\nChoose Preferences: Refactor Settings – User from context menu and configure the nodePath setup. (Default Value is node)\\n\\n\\t\"nodePath\" : \"node\"\\n}```\\n\\n\\nTodo\\n========================\\n- ***Extract method***  \\n- Define exceptions of global scoped variable names like jQuery or $.\\n- Do not pass variables available in current Scope (optional).\\n- Let the user choose a function name before or after extraction.\\n- Let the user choose a custom position to insert extracted methode code and indent it correctly.\\n\\n\\nLicense\\n=======\\n\\n\\nMIT and GPL license.\\n\\nCopyright (c) 2013 Stephan Ahlf <stephan@ahlf-it.de>\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '216',\n",
       "  'forks': '12',\n",
       "  'commits': '73'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Analytics\\n\\nComplete Google Analytics, Mixpanel, KISSmetrics (and more) integration for Meteor\\n\\nNOTE: This pkg is no longer being actively maintained, if you\\'d like to maintain this pkg please express interest by opening an issue.\\nOK GROW! analytics uses a combination of the browser History API, Meteor\\'s accounts package and Segment.io\\'s analytics.js to automatically record and send user identity and page view event data from your Meteor app to your analytics platforms.\\nTable of Contents\\n\\nBackground\\n\\nAnalytics 3.0+\\nAnalytics 2.1.0+\\nPre Meteor 1.3.1\\n\\n\\nInstall\\nUsage\\n\\nCurrently Supported Analytic Services\\nPage views\\nRouters\\n\\nReact Router\\nFlow Router\\nIron Router\\n\\n\\nDisabling automatic page views\\nLog signin/signout\\nEvent tracking\\nTrack visitor scrolling\\nBrowser Policy\\n\\nExample browser policy\\n\\n\\n\\n\\nDebugging\\n\\nURL Whitelisting on Android Devices\\nAd-blocker\\n\\n\\nExample React, Flow and Iron Router Apps\\nMaintainers\\nContributing\\nLicense\\n\\nBackground\\nAnalytics 3.0+\\nIn version 3.X of this package, the automatic page view tracking is handled by our new router-agnostic @okgrow/auto-analytics NPM package, which can be used by any JavaScript application whether using Meteor or not. This package adds automatic user identification by using hooks in the Meteor accounts package and building on Segment.io\\'s analytics package through the @okgrow/auto-analytics package.\\nAnalytics 2.1.0+\\nOur Analytics package has been rewritten to be router agnostic. You should be able to use this package with any router that you use with Meteor app. We have tested and used our Analytics package with iron-router, flow-router, and react-router. You can view and test this out in our iron-router, flow-router and react-router example apps located in the examples folder.\\nNOTE: A fundamental change that may affect some applications is that we no longer look for or use the router\\'s route name when logging page views. Instead we use document.title. This may affect applications that do not change or set their document.title for each screen or page of their application. The simplest solution is to simply set document title like this document.title = \"My new title\"; for each screen or page in your application. If you are using flow router or iron router you can remain at okgrow:analytics@2.0.1 to keep using the name of the route for your analytic events.\\nPre Meteor 1.3.1\\nFor Meteor Apps older than v1.3.1, please use v1.0.9 of this package. Going forward this package will officially only be supporting Meteor Apps >= v1.3.1\\nInstall\\nIn your Meteor project folder, run:\\nmeteor add okgrow:analytics\\nUsage\\nThis package will automatically configure the underlying @okgrow/auto-analytics package using Meteor.settings.public.analyticsSettings. In Meteor you typically specify your settings using a settings.json file:\\n{\\n  \"public\": {\\n    \"analyticsSettings\": {\\n      // Add your analytics tracking ids here (remove this line before running)\\n      \"Google Analytics\" : {\"trackingId\": \"Your tracking ID\"},\\n      \"Amplitude\"        : {\"apiKey\": \"...\"},\\n      \"Chartbeat\"        : {\"uid\": \"...\"},\\n      \"comScore\"         : {\"c2\": \"...\"},\\n      \"HubSpot\"          : {\"portalId\": \"...\"},\\n      \"Intercom\"         : {\"appId\": \"...\"},\\n      \"Keen IO\"          : {\"projectId\": \"...\", \"writeKey\": \"...\"},\\n      \"KISSmetrics\"      : {\"apiKey\": \"...\"},\\n      \"Mixpanel\"         : {\"token\":  \"...\", \"people\": true},\\n      \"Quantcast\"        : {\"pCode\": \"...\"},\\n      \"Segment.io\"       : {\"apiKey\": \"...\"}\\n    }\\n  }\\n}\\nAnd run your app with that settings file as follows:\\nmeteor --settings settings.json\\nSee the @okgrow/auto-analytics package for more details on configuration.\\nCurrently Supported Analytic Services\\nSee the @okgrow/auto-analytics package for up-to-date details of supported analytics services.\\nPage views\\nSee the @okgrow/auto-analytics package for details on page view tracking. In short, that package uses the browser History API to automatically track page views.\\nSince the History API is used to automatically track page views, document.title is used instead of the router\\'s route name as the default page name.\\nIf you rely on your router\\'s route name for the page name in page view events, you can easily set document.title programming using the router\\'s route name. Here are examples of how to do this with React Router, Flow Router and Iron Router:\\nRouters\\nThis package is router agnostic. It will work with any router, and by default it uses the document.title as the page name for reporting to your analytics service.\\nReact Router\\nIn your router setup, add a name property to your routes:\\n<Router history={ browserHistory }>\\n  <Route path=\"/\" name=\"Home\" component={ App } />\\n  <Route path=\"/one\" name=\"One\" component={ App } />\\n  <Route path=\"/two\" name=\"Two\" component={ App } />\\n  <Route path=\"/three\" name=\"Three\" component={ App } />\\n</Router>\\nNOTE The current route is passed in as a property named route to your component.\\nThen, in the render() function of your main layout component, using a package like react-document-title:\\nrender() {\\n  return (\\n    <DocumentTitle title={this.props.route.name}>\\n      ...\\n    </DocumentTitle>\\n  );\\n}\\nFlow Router\\nTemplate.mainLayout.onRendered(function() {\\n  Tracker.autorun(() => {\\n    document.title = FlowRouter.getRouteName();\\n  });\\n});\\nIron Router\\nTemplate.mainLayout.onRendered(function() {\\n  Tracker.autorun(() => {\\n    document.title = Router.current().route.getName();\\n  });\\n});\\nDisabling automatic page views\\nTo disable automatic page view tracking change Meteor.settings as shown below then manually log a page view by calling analytics.page(\\'page name\\'):\\n{\\n  \"public\": {\\n    \"analyticsSettings\": {\\n      // Disable autorun if you do not want analytics running on every route (remove this line before running)\\n      \"autorun\"  : false\\n    }\\n  }\\n}\\nLog signin/signout\\nIf you have the accounts package installed, this package will automatically track when a user logs in and logs out. Logging in will call identify on the user and associate their Meteor.userId to their previous anonymous activities.\\nEvent tracking\\nSee the @okgrow/auto-analytics package for details on event tracking. In short, track any event by calling the analytics.track() function:\\nanalytics.track(\"Bought Ticket\", {\\n  eventName: \"Wine Tasting\",\\n  couponValue: 50,\\n});\\nTrack visitor scrolling\\nJosh Owens\\' article, Google Analytics events, goals, and Meteor.js, goes over a great way to capture how far a visitor has scrolled down a page.\\nBrowser Policy\\nIf your project uses the Browser Policy package, we\\'ve included the Google Analytics and MixPanel domains in our browser policy configuration. Any additional services you add will need to be added to your browser policy config as well.\\nExample browser policy\\nBrowserPolicy.content.allowOriginForAll(\"www.google-analytics.com\");\\nBrowserPolicy.content.allowOriginForAll(\"cdn.mxpnl.com\");\\nIf your project doesn\\'t use the Browser Policy package, don\\'t worry, it won\\'t affect your usage.\\nDebugging\\nTo log package activity to the console for debugging purposes, turn on debugging in the console:\\n> analytics.debug()\\nTurn debug logging off with:\\n> analytics.debug(false)\\nURL Whitelisting on Android Devices\\nIf your app is running on Android devices you will probably have to add the cordova-plugin-whitelist package and set access rules in your mobile-config.js for all URLs of the platforms that you are using.\\nExample for Intercom:\\nApp.accessRule(\\'https://js.intercomcdn.com/*\\');\\nApp.accessRule(\\'https://static.intercomcdn.com/*\\');\\nApp.accessRule(\\'https://api-iam.intercom.io/*\\');\\nApp.accessRule(\\'https://widget.intercom.io/*\\');\\nApp.accessRule(\\'https://nexus-websocket-a.intercom.io/*\\');\\nApp.accessRule(\\'https://nexus-websocket-b.intercom.io/*\\');\\n\\nTo find all the necessary URLs for your project, build your production app and install it on your Android device. Then connect it via USB and open the Android Studio Device Monitor (Tools >> Android Device Monitor >> LogCat). Perform a relevant action and then search for \"whitelist\". It should a show message for each URL that was blocked.\\nAd-blocker\\nWhen running your Meteor app in \"development mode\" ad-blocking web-browser extensions may block the okgrow:analytics package due to the word \"analytics\" in the package name. This only occurs when running Meteor in \"development mode\" because files are not bundled together and minified. To work around this issue you can disable your ad-blocker when running in development mode.\\nTo test that application with an ad-blocker, run your Meteor app in production mode with this command:\\nmeteor run --production --settings settings.json\\nNOTE If an ad-blocker is enabled the expected behavior is that analytic events will not be received. You\\'ll see an error message in your console reporting the events being blocked.\\nExample React, Flow and Iron Router Apps\\nWhile page view event tracking is router agnostic, the examples directory contains example apps using the three most common routers used in Meteor apps: React Router, Flow Router and Iron Router. These apps can be run from within their respective directories with:\\nmeteor npm start\\nMaintainers\\nThis is an open source package. We hope to deal with contributions in a timely manner, but that\\'s not always the case. The main maintainers are:\\n@okgrow\\nFeel free to ping if there are open issues or pull requests which are taking a while to be dealt with!\\nAdditional Notes\\nThere has been at least one report of Google Analytics taking over a day in between GA account creation and any data showing up on the actual GA dashboard. See this issue for details. You may just need to wait if nothing\\'s showing up.\\nContributing\\nIssues and Pull Requests are always welcome.\\nPlease read our contribution guidelines.\\nIf you are interested in becoming a maintainer, get in touch with us by sending an email or opening an issue. You should already have code merged into the project. Active contributors are encouraged to get in touch.\\nPlease note that all interactions in @okgrow\\'s repos should follow our Code of Conduct.\\nLicense\\nReleased under the MIT license © 2015-2017 OK GROW!.\\n',\n",
       "  'watchers': '16',\n",
       "  'stars': '211',\n",
       "  'forks': '59',\n",
       "  'commits': '299'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"taiga-ncurses\\n\\n\\n\\n\\nA NCurses client for Taiga.\\n\\nProject status\\nCurrently on design phases: This project was a proof of concept to try to create a curses client\\nfor Taiga in the 6th PiWeek. It isn't finished yet and currently it isn't\\nfeature complete. You can see some screenshots at https://github.com/taigaio/taiga-ncurses/issues/4#issuecomment-57717386\\n\\nSetup development environment\\nJust execute these commands in your virtualenv(wrapper):\\n$ pip install -r dev-requirements.txt\\n$ python setup.py develop\\n$ py.test               # to run the tests\\n$ taiga-ncurses         # to run the app\\n\\nObviously you need the taiga backend and, if you don't fancy living in darkness,\\nyou can use the taiga web client, sometimes. :P\\nNote: taiga-ncurses only runs with python 3.3+.\\n\\nCommunity\\nTaiga has a mailing list. Feel free to join it and ask any questions you may have.\\nTo subscribe for announcements of releases, important changes and so on, please follow\\n@taigaio on Twitter.\\n\",\n",
       "  'watchers': '27',\n",
       "  'stars': '110',\n",
       "  'forks': '23',\n",
       "  'commits': '379'},\n",
       " {'language': 'Python 98.4',\n",
       "  'readme': 'Welcome to Brownie!\\n\\nHave you ever started a new project and implemented this little function\\nfoo or this datastructure bar you already implemented for another project?\\nEver wondered why a specific feature is not in the standard library\\nalready?\\nWanted to use that new datastructure but you are still stuck with this\\nancient Python version or are just not willing or able to switch to\\nPython 3.x, yet?\\nAnd most importantly were too lazy to implement this datastructure which\\nwould be more appropriate to use?\\n\\nBrownie wants to solve these problems by providing all these small things\\nwell documented, well tested and most importantly right now when you need\\nit.\\nTake a look at the documentation for further information, visit Github\\nfor development or issue tracking or fetch the latest development version.\\n\\nInstallation\\nBrownie runs on Python 2.5-2.7 (tested on CPython and PyPy). In order to\\ninstall it simply run pip install brownie or easy_install brownie,\\nshould you prefer the latter, to get the latest stable version.\\nIf you really need to, you can also install the development version of\\nBrownie with pip install brownie==dev. However I do not recommend it.\\n\\nDevelopment\\nShould you want to participate in the development, fork the repository on\\nGithub and take a moment to read :ref:`contributing`, for a couple of\\ngeneral and not so general guidelines you should follow.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '109',\n",
       "  'forks': '12',\n",
       "  'commits': '503'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'YUNOMI: Y U NO MEASURE IT\\n\\n\\nAs in:\\n\\nIt has performance implications, Y U NO MEASURE IT!?\\nYunomi provides insights to the internal behavior of an application, providing useful statistics and metrics on selected portions of your code.\\nIt’s a Python port of the core portion of a Java Metrics library by Coda Hale.\\nStop guessing, start measuring:\\n$ pip install yunomi\\n\\nCore Features\\n\\nCounter\\nSimple interface to increment and decrement a value.\\nFor example, this can be used to measure the total number of jobs sent to the queue, as well as the pending (not yet complete) number of jobs in the queue.\\nSimply increment the counter when an operation starts and decrement it when it completes.\\n\\nMeter\\nMeasures the rate of events over time.\\nUseful to track how often a certain portion of your application gets requests so you can set resources accordingly.\\nTracks the mean rate (the overall rate since the meter was reset) and the rate statistically significant regarding only events that have happened in the last 1, 5, and 15 minutes (Exponentially weighted moving average).\\n\\nHistogram\\nMeasures the statistical distribution of values in a data stream.\\nKeeps track of minimum, maximum, mean, standard deviation, etc.\\nIt also measures median, 75th, 90th, 95th, 98th, 99th, and 99.9th percentiles.\\nAn example use case would be for looking at the number of daily logins for 99 percent of your days, ignoring outliers.\\n\\nTimer\\nA useful combination of the Meter and the Histogram letting you measure the rate that a portion of code is called and a distribution of the duration of an operation.\\nYou can see, for example, how often your code hits the database and how long those operations tend to take.\\n\\nExamples\\n\\nDecorators\\nThe simplest and easiest way to use the yunomi library.\\n\\nCounter\\nYou can use the count_calls decorator to count the number of times a function is called.\\n>>> from yunomi import counter, count_calls\\n>>> @count_calls\\n... def test():\\n...     pass\\n...\\n>>> for i in xrange(10):\\n...     test()\\n...\\n>>> print counter(\"test_calls\").get_count()\\n10\\n\\nTimer\\nYou can use the time_calls decorator to time the execution of a function and get distributtion data from it.\\n>>> import time\\n>>> from yunomi import timer, time_calls\\n>>> @time_calls\\n... def test():\\n...     time.sleep(0.1)\\n...\\n>>> for i in xrange(10):\\n...     test()\\n...\\n>>> print timer(\"test_calls\").get_mean()\\n0.100820207596\\n\\nRequirements\\nYunomi has no external dependencies and runs on PyPy and Python 2.6, 2.7, and 3.3.\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '109',\n",
       "  'forks': '7',\n",
       "  'commits': '101'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'About\\nA simple and efficient paginator.\\n\\nJinja2\\nJinja2 is supported via Coffin:\\n{% with paginate(request, my_queryset) as results %}\\n  {{ results.paging }}\\n  {% for result in results.objects %}\\n    {{ result }}\\n  {% endfor %}\\n  {{ results.paging }}\\n{% endwith %}\\n\\n\\nDjango\\nDjango templatetags require django-templatetag-sugar:\\n{% load paging_extras %}\\n\\n{% paginate my_queryset from request as results %}\\n{{ results.paging }}\\n{% for result in results.objects %}\\n  {{ result }}\\n{% endfor %}\\n{{ results.paging }}\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '108',\n",
       "  'forks': '27',\n",
       "  'commits': '34'},\n",
       " {'language': 'Python 65.1',\n",
       "  'readme': 'GoogleClosureCompiler\\nMakes integrating the Google JavaScript Compiler with your Rails deployment process dead simple. Read why compressing your JavaScript is important\\nBoth the Google Closure Compiler API and Application are supported. Sensible defaults are provided.\\nInstalling the plugin\\nscript/plugin install git://github.com/mkelly12/google_closure_compiler.git\\n\\nRequirements\\nAny version of Rails 2.x; including Rails 2.3.4 and 2.1.2.\\nSo how does it work?\\nRead how this integrates with your workflow\\nThe plugin uses the Google Closure Compiler to optimize JavaScript files cached by Rails.\\nAnytime you use the javascript_include_tag with the :cache => true or :cache => \\'bundle_name\\' the resulting JavaScript file will be compiled. Read more about Rails asset caching\\nYou will also need this in your production.rb (and in your development.rb only when testing):\\nconfig.action_controller.perform_caching = true\\n\\nKeep in mind that cached files are saved to your public directory and only generated when needed. If you forget to delete them in the development environment they can lead to some serious headaches. It\\'s a good practice to use a naming scheme like \\'cache/bundle_name\\' so you can easily remove the cached files and add ignore rules to your version control.\\nHow does it work with the Google Closure Compiler?\\nThere are three ways to integrate with the Google Closure Compiler which are attempted in the following order:\\n\\nIf you have the Closure Compiler Application properly installed (yes we check this) then that is always used.\\nIf the Application is not detected then the API is used with your JavaScript embeded in the POST data.\\nIf your JavaScript file is larger then POST data will allow then a link to your JavaScript is sent to the API. If your host is not specified or not reachable by the Google service then no compilation is performed.\\n\\nApplication\\nThe preferred method is to use the compile.jar file which is included in this plugin.\\nYou will need the Java Runtime Environment version 6.\\nIf you are on OS X with the latest updates you will need to specify the path to the 1.6 JRE since /usr/bin/java still point to 1.5.\\nAdd the following to /config/google_closure_compiler.yml\\ndevelopment:\\n\\tjava_path: \\'/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Commands/java\\'\\n\\nAPI with code in request\\nYou don\\'t need anything besides an outgoing internet connection. However you JavaScript needs to fit in the POST data.\\nAPI with code urls\\nThis works well if your server is accessible to the world and you specified your host name using the following:\\nconfig.action_mailer.default_url_options = { :host => HOST_NAME }\\n\\nThe only limitation is that cached JavaScript files larger then 500k cannot be processed with this method.\\nUsing the Google Closure Library\\nIf you are using the Google Closure Library use the following view helper to include your Closure JavaScript:\\nclosure_include_tag \"wooly_zurbian.js\"\\n\\nIn this example wooly_zurbian.js is a JavaScript file in /public/javascripts/.\\nThis helper inserts the Google Closure Library base.js and events.js scripts when perform_caching is false. When perform_caching is true these files are not required since they are added using the calcdeps.py script.\\nThe Google Closure Library is required and is assumed to be in public/javascripts/closure. If you put it somewhere else specify the relative path from /public/javascripts/ using closure_library_path in google_closure_compiler.yml.\\nThe wooly_zurbian.js script is inserted and cached in /public/javascripts/cache/closure/wooly_zurbian.js.\\nUsing the Dependency Calculation Script\\nIf any of your cached JavaScript files contain a call to goog.require() then that cached file will be expanded using the calcdeps.py script. This requires Python 2.4 or greater and the Google Closure Library (see Using the Google Closure Library).\\ncalcdeps.py takes each goog.require() call and replaces it with the required libraries. Read more about calcdeps.py and why it is important when using the Google Closure Library.\\nADVANCED_OPTIMIZATIONS is used for the compilation of cached file that are expanded with calcdeps.py regardless of what is specified in the /config/google_closure_compiler.yml file.\\nFAQ\\nCan I change the compilation level?\\nYes, you can specify it in the config/google_closure_compiler.yml file.\\ndevelopment:\\n\\tcompilation_level: \\'ADVANCED_OPTIMIZATIONS\\'\\n\\nThe default is SIMPLE_OPTIMIZATIONS. Other options are WHITESPACE_ONLY and ADVANCED_OPTIMIZATIONS.\\nMake sure you read the documentation on Advanced Optimizations before enabling them.\\nWhat happens if there is an error or the API is down?\\nIf all compilation methods fail then the original JavaScripts are used in the bundles.\\nDoes this play nice with Smurf?\\nIt sure does. If you have Smurf installed then CSS minification works as expected and JavaScript files are processed by both Smurf and the Google Closure Compiler.\\nCopyright (c) 2009 Matt Kelly - ZURB, released under the MIT license\\n',\n",
       "  'watchers': '2',\n",
       "  'stars': '108',\n",
       "  'forks': '7',\n",
       "  'commits': '15'},\n",
       " {'language': 'Python 94.1',\n",
       "  'readme': \"Japont\\nDynamic Subsetting System for CJK fonts.\\nフォントを軽量化して配信するために，動的に必要文字を抽出したWebフォントを生成するシステム\\n比較的簡単に日本語Webフォントを導入できます\\n⚠️ Notice\\nThis branch is under developing.\\nIf you use Japont, please access with-fonttools branch.\\nDEMO\\nDEMO\\nInstallation\\nWIP\\nUsage\\nWIP\\nEnvironment variables\\n\\n\\n\\nENVS\\nDefault\\nNote\\n\\n\\n\\n\\nX_ROBOTS_TAG\\nnoindex, nofollow\\nX-Robots-Tag Header\\n\\n\\nSERVER_OWNER\\nAnonymous\\nServer owner's name\\n\\n\\nFONTS_DIR_PATH\\n./fonts\\nFolder path where fonts are\\n\\n\\nZIP_COMPRESSION_TYPE\\nZIP_STORED\\n\\n\\n\\nBIND_IP\\n0.0.0.0\\n\\n\\n\\nPORT\\n8000\\n\\n\\n\\n\\nContribution\\n\\nFork it ( http://github.com/Japont/Japont-core/fork )\\nCreate your feature branch ( git checkout -b my-new-feature )\\nCommit your changes ( git commit -am 'Add some feature' )\\nPush to the branch ( git push origin my-new-feature )\\nCreate new Pull Request\\n\\nLICENSE\\nApply the Apache License version 2.0.\\nApache License version 2.0 を適用します．\\nCopyright 2015- 3846masa\\nAuthor\\n 3846masa\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '106',\n",
       "  'forks': '6',\n",
       "  'commits': '22'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"What about running nose with a smarter interactive debugger?\\nUse this and never risk yourself forgetting import ipdb; ipdb.set_trace() in your code again!\\nThis plugin is 99.99% based on nose's builtin debug plugin.\\nIf you have any ideas about how to improve it, come and fork the code at http://github.com/flavioamieiro/nose-ipdb\\nInstall\\npip install ipdbplugin\\n\\nUsage\\nTo drop into ipdb on errors:\\nnosetests --ipdb\\n\\nTo drop into ipdb on failures:\\nnosetests --ipdb-failures\\n\\nLicense\\nGNU Lesser General Public License\\nAuthors\\n\\nBernardo Fontes (falecomigo@bernardofontes.net)\\nFlávio Amieiro (amieiro.flavio@gmail.com)\\nHenrique Bastos (henrique@bastos.net)\\n\\n\",\n",
       "  'watchers': '4',\n",
       "  'stars': '106',\n",
       "  'forks': '17',\n",
       "  'commits': '58'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"django-compat\\n\\n\\n\\n\\nForward and backwards compatibility layer for Django 1.4, 1.7, 1.8, 1.9, 1.10 and 1.11\\nConsider django-compat as an experiment based on the discussion on reddit. Let's see where it goes.\\nWhat started as an experiment based on this discussion on reddit has proven to be true in real life.\\ndjango-compat is under active development. To learn about other features, bug fixes, and changes, please refer to the changelog.\\nWho uses django-compat\\nTwo popular examples of open source reusable app that uses django-compat are django-hijack and django-background-tasks.\\nWant to have yours listed here? Send us a PR.\\nWhy use django-compat\\n\\nBe able to use the LTS versions of Django and support newer versions in your app\\nUse features from newer Django versions in an old one\\nManage and master the gap between different framework versions\\n\\nHow to use django-compat\\nInstall compat from the PyPI or download and install manually. All relevant  releases are listed here under releases.\\nUsing one of the compatible objects is easy. For example\\nfrom compat import patterns, url\\n\\nurlpatterns = patterns('ABC.views',\\n\\t\\turl(r'^abc/$', 'abc', name='abc-link'),\\n...\\n\\nSee a full example here.\\n\\n\\n\\ndjango-compat is free software. If you find it useful and would like to give back, please consider to make a donation using Bitcoin or PayPal. Thank you!\\n\\n\\n\\nCompatible objects\\n\\n\\n\\nCompatible object\\nSpecifically tested\\n1.8\\n1.9\\n1.10\\n1.11\\nNotes\\n\\n\\n\\n\\nBytesIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nDjangoJSONEncoder\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nEmailValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nGenericForeignKey\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nmodels.GenericForeignKey\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nHttpResponseBase\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nJsonResponse\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexProvider\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nNoReverseMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLPattern\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolver404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolverMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nSortedDict\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nStringIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nURLValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nVariableNode\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nView\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nadd_to_builtins\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nadmin_utils\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\natomic\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclean_manytomany_helptext\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclear_url_caches\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclose_connection\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit_on_success\\n✔️\\n✔️\\n✔️\\n✔️\\n`✔️\\ncommit_on_success replaced by atomic in Django >= 1.8\\n\\n\\nforce_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nformat_html\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_callable\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_current_site\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ident\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_mod_func\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model_name\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ns_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_template_loaders\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_user_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_username_field\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler500\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_module\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_string\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ninclude\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nis_valid_path\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nparse_qs\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\npatterns\\n✖️\\n✔️\\n✔️\\n❌\\n❌\\n\\n\\n\\npython_2_unicode_compatible\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrender_to_string\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nThe new function signature (https://docs.djangoproject.com/en/1.9/releases/1.8/#dictionary-and-context-instance-arguments-of-rendering-functions) is backported to pre-1.8.\\n\\n\\nresolve\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nresolve_url\\n✔️\\n✔️\\n✔️\\n⚠️\\n⚠️\\n1.10: Reversing by dotted path has been removed\\n\\n\\nreverse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nreverse_lazy\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrollback\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTransaction savepoint (sid) is required for Django < 1.8\\n\\n\\nset_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nset_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsimplejson\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nslugify\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsmart_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nunquote_plus\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurl\\n✖️\\n✔️\\n✔️\\n✖️\\n✖️\\nFunction used in urlpatterns\\n\\n\\ntempat.url\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n`uravy_multiplication_x:\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlresolvers\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlunparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nuser_model_label\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ntemplatetags.compat.verbatim\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTemplatetag; import with {% load verbatim from compat %}. 1.4: Does not allow specific closing tags, e.g. {% endverbatim myblock %}, and does not preserve whitespace inside tags.\\n\\n\\n\\nResources and references\\nResources\\n\\nhttps://github.com/ubernostrum/django-compat-lint\\nhttps://docs.djangoproject.com/en/dev/misc/api-stability/\\nhttps://docs.djangoproject.com/en/dev/topics/python3/\\nhttp://andrewsforge.com/presentation/upgrading-django-to-17/\\n\\ncompat.py\\nBits and bites of the following projects were re-used to build django-compat.\\n\\n https://github.com/lukaszb/django-guardian/blob/devel/guardian/compat.py\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/compat.py\\n https://github.com/toastdriven/django-tastypie/blob/master/tastypie/compat.py\\n https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/compat.py\\n\\n TODO: MinValueValidator, MaxValueValidator et al. (other relevant bits are included) Django 1.8\\n\\n\\n https://gist.github.com/theskumar/ff8de60ff6a33bdacaa8\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/templatetags/compat.py\\n https://github.com/kennethreitz/requests/blob/master/requests/compat.py\\n https://github.com/mitsuhiko/jinja2/blob/master/jinja2/_compat.py\\n https://github.com/jaraco/setuptools/blob/master/setuptools/compat.py\\n https://github.com/mariocesar/sorl-thumbnail/blob/master/sorl/thumbnail/compat.py\\n\\nChangelog\\n2017/04/07\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/08/02\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/06/01\\n\\nAdd get_current_site and admin_utils\\n\\n2016/05/11\\n\\nFix error when installing package under python 3.4\\n\\n###\\xa02015/11/12\\n\\nBackport new render_to_string function signature to Django < 1.8\\nBackport verbatim tag to Django 1.4\\nAdd get_template_loaders\\nAdd close_connection\\nImprove JsonResponse backport to Django 1.4\\nAdd tests for import_module, get_model and add_to_builtins\\nAnticipate renaming of django.core.urlresolvers to django.urls in 1.10\\nAvoid warnings in setup.py\\n\\n2015/11/11\\n\\n1.9 compatibility for existing objects with the following changes:\\n\\nadd_to_builtins was removed for Django >= 1.9\\nGenericForeignKey` was moved to compat.models`` for Django >= 1.9\\n\\n\\n\\n2015/07/15\\n\\nadd_to_builtins was added\\n\\n2015/07/08\\n\\nget_query_set/get_queryset support was dropped again (see #29)\\n\\n\",\n",
       "  'watchers': '5',\n",
       "  'stars': '106',\n",
       "  'forks': '21',\n",
       "  'commits': '215'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"django-compat\\n\\n\\n\\n\\nForward and backwards compatibility layer for Django 1.4, 1.7, 1.8, 1.9, 1.10 and 1.11\\nConsider django-compat as an experiment based on the discussion on reddit. Let's see where it goes.\\nWhat started as an experiment based on this discussion on reddit has proven to be true in real life.\\ndjango-compat is under active development. To learn about other features, bug fixes, and changes, please refer to the changelog.\\nWho uses django-compat\\nTwo popular examples of open source reusable app that uses django-compat are django-hijack and django-background-tasks.\\nWant to have yours listed here? Send us a PR.\\nWhy use django-compat\\n\\nBe able to use the LTS versions of Django and support newer versions in your app\\nUse features from newer Django versions in an old one\\nManage and master the gap between different framework versions\\n\\nHow to use django-compat\\nInstall compat from the PyPI or download and install manually. All relevant  releases are listed here under releases.\\nUsing one of the compatible objects is easy. For example\\nfrom compat import patterns, url\\n\\nurlpatterns = patterns('ABC.views',\\n\\t\\turl(r'^abc/$', 'abc', name='abc-link'),\\n...\\n\\nSee a full example here.\\n\\n\\n\\ndjango-compat is free software. If you find it useful and would like to give back, please consider to make a donation using Bitcoin or PayPal. Thank you!\\n\\n\\n\\nCompatible objects\\n\\n\\n\\nCompatible object\\nSpecifically tested\\n1.8\\n1.9\\n1.10\\n1.11\\nNotes\\n\\n\\n\\n\\nBytesIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nDjangoJSONEncoder\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nEmailValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nGenericForeignKey\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nmodels.GenericForeignKey\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nHttpResponseBase\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nJsonResponse\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexProvider\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nNoReverseMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLPattern\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolver404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolverMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nSortedDict\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nStringIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nURLValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nVariableNode\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nView\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nadd_to_builtins\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nadmin_utils\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\natomic\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclean_manytomany_helptext\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclear_url_caches\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclose_connection\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit_on_success\\n✔️\\n✔️\\n✔️\\n✔️\\n`✔️\\ncommit_on_success replaced by atomic in Django >= 1.8\\n\\n\\nforce_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nformat_html\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_callable\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_current_site\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ident\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_mod_func\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model_name\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ns_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_template_loaders\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_user_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_username_field\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler500\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_module\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_string\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ninclude\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nis_valid_path\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nparse_qs\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\npatterns\\n✖️\\n✔️\\n✔️\\n❌\\n❌\\n\\n\\n\\npython_2_unicode_compatible\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrender_to_string\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nThe new function signature (https://docs.djangoproject.com/en/1.9/releases/1.8/#dictionary-and-context-instance-arguments-of-rendering-functions) is backported to pre-1.8.\\n\\n\\nresolve\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nresolve_url\\n✔️\\n✔️\\n✔️\\n⚠️\\n⚠️\\n1.10: Reversing by dotted path has been removed\\n\\n\\nreverse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nreverse_lazy\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrollback\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTransaction savepoint (sid) is required for Django < 1.8\\n\\n\\nset_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nset_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsimplejson\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nslugify\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsmart_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nunquote_plus\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurl\\n✖️\\n✔️\\n✔️\\n✖️\\n✖️\\nFunction used in urlpatterns\\n\\n\\ntempat.url\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n`uravy_multiplication_x:\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlresolvers\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlunparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nuser_model_label\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ntemplatetags.compat.verbatim\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTemplatetag; import with {% load verbatim from compat %}. 1.4: Does not allow specific closing tags, e.g. {% endverbatim myblock %}, and does not preserve whitespace inside tags.\\n\\n\\n\\nResources and references\\nResources\\n\\nhttps://github.com/ubernostrum/django-compat-lint\\nhttps://docs.djangoproject.com/en/dev/misc/api-stability/\\nhttps://docs.djangoproject.com/en/dev/topics/python3/\\nhttp://andrewsforge.com/presentation/upgrading-django-to-17/\\n\\ncompat.py\\nBits and bites of the following projects were re-used to build django-compat.\\n\\n https://github.com/lukaszb/django-guardian/blob/devel/guardian/compat.py\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/compat.py\\n https://github.com/toastdriven/django-tastypie/blob/master/tastypie/compat.py\\n https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/compat.py\\n\\n TODO: MinValueValidator, MaxValueValidator et al. (other relevant bits are included) Django 1.8\\n\\n\\n https://gist.github.com/theskumar/ff8de60ff6a33bdacaa8\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/templatetags/compat.py\\n https://github.com/kennethreitz/requests/blob/master/requests/compat.py\\n https://github.com/mitsuhiko/jinja2/blob/master/jinja2/_compat.py\\n https://github.com/jaraco/setuptools/blob/master/setuptools/compat.py\\n https://github.com/mariocesar/sorl-thumbnail/blob/master/sorl/thumbnail/compat.py\\n\\nChangelog\\n2017/04/07\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/08/02\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/06/01\\n\\nAdd get_current_site and admin_utils\\n\\n2016/05/11\\n\\nFix error when installing package under python 3.4\\n\\n###\\xa02015/11/12\\n\\nBackport new render_to_string function signature to Django < 1.8\\nBackport verbatim tag to Django 1.4\\nAdd get_template_loaders\\nAdd close_connection\\nImprove JsonResponse backport to Django 1.4\\nAdd tests for import_module, get_model and add_to_builtins\\nAnticipate renaming of django.core.urlresolvers to django.urls in 1.10\\nAvoid warnings in setup.py\\n\\n2015/11/11\\n\\n1.9 compatibility for existing objects with the following changes:\\n\\nadd_to_builtins was removed for Django >= 1.9\\nGenericForeignKey` was moved to compat.models`` for Django >= 1.9\\n\\n\\n\\n2015/07/15\\n\\nadd_to_builtins was added\\n\\n2015/07/08\\n\\nget_query_set/get_queryset support was dropped again (see #29)\\n\\n\",\n",
       "  'watchers': '4',\n",
       "  'stars': '106',\n",
       "  'forks': '11',\n",
       "  'commits': '411'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Zippopotamus Cloud\\n\\nAPI Moved\\nOur full crowd-source zip-db can be found\\nhere\\nTrea has taken over Zippopotamus - you can find it at the ekotechnology fork\\nThanks\\nJeff & Samir\\n\\nThis is a repository for  Zippopotamus the global postal code API\\nZippopotamus is hosted by dotCloud. This repo is used to build and maintain the site.\\nIf you want to contribute to the improving the site, back-end, front-end etc. Just fork away and submit pull requests.\\nSample Implementations\\nCheckout the static/ folder to see some of the sample implementations of Zippopotamus for inspiration and examples for how to implement Zippopotamus API for use in your website etc.\\nIf you want to share an implementation, we would love to post example cases of Zippopotamus on our homepage.\\nResponse Format\\nOn May 1st Zippopotamus changed their JSON response format to work better with international postal codes.  Now we support a one-to-many format service. That is that one zip code may map to many regions, this is common in countries like Spain and France (but not in the US and Germany).\\nPostal Code Information\\nFor information our postal codes and countries supported, you should check out the zippopotamus crowd-sourcing project.  Here you can download the entire database dump, or fork and add changes that we will incorporate into our DB.\\nTechnical Information\\nWhat is Zippopotamus built on\\nAt the moment the zippopotamus is built on Python, MongoDB and bottle.py framework.\\nLocal Testing?\\nThe site is configured to run on dotCloud, if you want to test out the web interfaceyou can change the wsgi.py file to include the last commented line, which is used to run the site on your local host.\\nSuggestions and Comments?\\nHate it? Love it? Open an issue if you have a problem or contact\\nJeff Crowell or Samir Ahmed\\nAlso, we aren't bottle or python or mongo experts. So if you see a way that we can improve, please let us know. Additionally, if you have examples (translation corrects etc) of using Zippopotamus that you want to share let us know and we can feature your site / blog on the homepage.\\n\",\n",
       "  'watchers': '12',\n",
       "  'stars': '105',\n",
       "  'forks': '56',\n",
       "  'commits': '44'},\n",
       " {'language': 'Java 60.2',\n",
       "  'readme': \"mongo-spark\\nExample application on how to use mongo-hadoop connector with Apache Spark.\\nRead more details at http://codeforhire.com/2014/02/18/using-spark-with-mongodb/\\nPrerequisites\\n\\nMongoDB installed and running on localhost\\nScala 2.10 and SBT installed\\n\\nRunning\\nImport data into the database, run either JavaWordCount or ScalaWordCount and print the results.\\nmongoimport -d beowulf -c input beowulf.json\\nsbt 'run-main JavaWordCount'\\nsbt 'run-main ScalaWordCount'\\nmongo beowulf --eval 'printjson(db.output.find().toArray())' | less\\n\\nLicense\\nThe code itself is released to the public domain according to the Creative Commons CC0.\\nThe example files are based on Beowulf from Project Gutenberg and is under its corresponding license.\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '93',\n",
       "  'forks': '58',\n",
       "  'commits': '5'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'CollapseLayout\\nCollapseLayout can collapse/expand layout with smooth animation.\\nHere is a gif showing the effect:\\n\\nHow to use\\n<com.example.collapselayout.CollapseLayout\\n\\tandroid:id=\"@+id/el\"\\n\\tandroid:layout_width=\"match_parent\"\\n\\tandroid:layout_height=\"wrap_content\" >\\n\\t\\n\\t<LinearLayout\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"wrap_content\"\\n        android:orientation=\"vertical\" >\\n        \\n        .........\\n        \\n\\t</LinearLayout>\\n\\t\\n</com.example.collapselayout.CollapseLayout>\\npublic void showEffect(View v) {\\n\\tint modeId = modeGroup.getCheckedRadioButtonId();\\n\\tswitch (modeId) {\\n\\tcase R.id.fixedTop:\\n\\t\\tel.setCollapseMode(Mode.FIXED_START);\\n\\t\\tbreak;\\n\\tcase R.id.fixedBottom:\\n\\t\\tel.setCollapseMode(Mode.FIXED_END);\\n\\t\\tbreak;\\n\\t}\\n\\t\\n\\tint interpolatorId = interpolatorGroup.getCheckedRadioButtonId();\\n\\tswitch (interpolatorId) {\\n\\tcase R.id.linear:\\n\\t\\tel.setInterpolator(linearInterpolator);\\n\\t\\tbreak;\\n\\tcase R.id.bounce:\\n\\t\\tel.setInterpolator(bounceInterpolator);\\n\\t\\tbreak;\\n\\tcase R.id.accelerate:\\n\\t\\tel.setInterpolator(accelerateDecelerateInterpolator);\\n\\t\\tbreak;\\n\\t}\\n\\t\\n\\tint orientationId = orientationGroup.getCheckedRadioButtonId();\\n\\tswitch (orientationId) {\\n\\tcase R.id.vertical:\\n\\t\\tel.setCollapseOrientation(Orientation.VERTICAL);\\n\\t\\tbreak;\\n\\tcase R.id.horizontal:\\n\\t\\tel.setCollapseOrientation(Orientation.HORIZONTAL);\\n\\t\\tbreak;\\n\\t}\\n\\t\\n\\tint state = el.getState();\\n\\tswitch (state) {\\n\\tcase CollapseLayout.STATE_OPEN:\\n\\t\\tel.close();\\n\\t\\tbreak;\\n\\tcase CollapseLayout.STATE_CLOSE:\\n\\t\\tel.open();\\n\\t\\tbreak;\\n\\t}\\n}\\n<declare-styleable name=\"CollapseLayout\">\\n\\t<attr name=\"collapseOrientation\">\\n\\t    <enum name=\"vertical\" value=\"0\" />\\n\\t    <enum name=\"horizontal\" value=\"1\" />\\n\\t</attr>\\n\\t<attr name=\"collapseMode\">\\n\\t    <enum name=\"fixed_end\" value=\"0\"/>\\n\\t    <enum name=\"fixed_start\" value=\"1\"/>\\n\\t</attr>\\n\\t<attr name=\"initialCollapseState\">\\n\\t    <enum name=\"open\" value=\"0\"/>\\n\\t    <enum name=\"close\" value=\"1\"/>\\n\\t</attr>\\n\\t<attr name=\"collapseDuration\" format=\"integer\"/>\\n</declare-styleable>\\n',\n",
       "  'watchers': '0',\n",
       "  'stars': '93',\n",
       "  'forks': '14',\n",
       "  'commits': '14'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Kafka Graphite Metrics Reporter\\n\\nThis is a simple reporter for kafka using the\\nGraphiteReporter. It works with\\nkafka 0.8.x and 0.9.x versions.\\nBig thanks to Maxime Brugidou from Criteo who did the initial commit of the Ganglia version,\\navailable here https://github.com/criteo/kafka-ganglia\\nInstall On Broker\\n\\nBuild the kafka-graphite-1.0.*.jar jar using mvn package or download it from the releases.\\nHint: The jar will include the metrics-graphite dependency\\nwhich is not brought by Kafka.\\nAdd kafka-graphite-1.0.*.jar to the libs/ directory of your kafka broker installation\\nConfigure the broker (see the configuration section below)\\nRestart the broker\\n\\nConfiguration\\nEdit the server.properties file of your installation, activate the reporter by setting:\\nkafka.metrics.reporters=com.criteo.kafka.KafkaGraphiteMetricsReporter\\nkafka.graphite.metrics.reporter.enabled=true\\n\\nYou may also specify multiple comma-separated reporter classes for the kafka.metrics.reporters property:\\nkafka.metrics.reporters=com.criteo.kafka.KafkaGraphiteMetricsReporter,kafka.metrics.KafkaCSVMetricsReporter[,....]\\n\\nHere is a list of default properties used:\\nkafka.graphite.metrics.host=localhost\\nkafka.graphite.metrics.port=2003\\n# The group value is going to be part of the metrics name to distinguish between different brokers\\nkafka.graphite.metrics.group=kafka\\n# This can be use to exclude some metrics from graphite \\n# since kafka has quite a lot of metrics, it is useful\\n# if you have many topics/partitions. For example :\\nkafka.graphite.metrics.exclude.regex=(kafka.network.*|kafka.*.topic.*)\\n\\n# Each metric provides multiple dimensions: min, max, meanRate, etc\\n# This might be too much data.\\n# It is possible to disable some metric dimensions with the following properties:\\n# By default all dimensions are enabled. Only the unwanted dimensions have to be configured\\nkafka.graphite.dimension.enabled.count=false\\nkafka.graphite.dimension.enabled.meanRate=false\\nkafka.graphite.dimension.enabled.rate1m=false\\nkafka.graphite.dimension.enabled.rate5m=false\\nkafka.graphite.dimension.enabled.rate15m=false\\nkafka.graphite.dimension.enabled.min=false\\nkafka.graphite.dimension.enabled.max=false\\nkafka.graphite.dimension.enabled.mean=false\\nkafka.graphite.dimension.enabled.sum=false\\nkafka.graphite.dimension.enabled.stddev=false\\nkafka.graphite.dimension.enabled.median=false\\nkafka.graphite.dimension.enabled.p75=false\\nkafka.graphite.dimension.enabled.p95=false\\nkafka.graphite.dimension.enabled.p98=false\\nkafka.graphite.dimension.enabled.p99=false\\nkafka.graphite.dimension.enabled.p999=false\\n\\nKnown Issues\\nWith Kafka  <= 0.8.2.2 there is an issue if topics get deleted or partions are moved between brokers.\\nThe metrics are not get deleted in this case and because they are implemented as a Gauge, a NoSuchElementException\\nis thrown when the metrics are reported.\\nThere is already a fix for this, see KAFKA-1866 but it did not make\\nit into an 0.8.x release. Because of this we implemented a workaround for this within the FilterMetricsPredicate.\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '92',\n",
       "  'forks': '54',\n",
       "  'commits': '58'},\n",
       " {'language': 'Java 91.1',\n",
       "  'readme': 'Big O\\nA big collection of data structures and algorithms puzzles.\\nAs on May 2013, the repository has roughly 130 problems with solutions.\\nSolutions are primarily in Java (under java/). Some solutions are in Ruby (under ruby/).\\nProblem statements are specified as Javadoc at the top of every solution. I have tried to be as descriptive as I can, but if something is not clear then please do let me know!\\nProblems are broadly classified into the following categories which are self explanatory.\\nPlease feel free to contribute!\\n\\narrays/\\nbinarytrees/\\nbitsandbytes/\\ncache/\\ncollections/\\nconcurrency/\\ndp/\\ngeneral/\\ngraphs/\\nlinkedlists/\\nsorting/\\nstrings/\\ntrees/\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '91',\n",
       "  'forks': '25',\n",
       "  'commits': '293'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '此项目已废弃\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '91',\n",
       "  'forks': '17',\n",
       "  'commits': '7'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'HipChat plugin for Jenkins\\nStarted with a fork of the Campfire plugin:\\nhttps://github.com/jgp/hudson_campfire_plugin\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '90',\n",
       "  'forks': '201',\n",
       "  'commits': '65'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Carma-Public/swagger-jaxrs-doclet has been doing more recent development on a fork of this project. Please consider checking that fork out first.\\nSwagger Doclet \\nA JavaDoc Doclet that can be used to generate a Swagger resource listing suitable for feeding to\\nswagger-ui.\\nUsage\\nTo use the Swagger Doclet in your Maven project, add the following to your POM file.\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\\n    <modelVersion>4.0.0</modelVersion>\\n\\n    <groupId>…</groupId>\\n    <artifactId>…</artifactId>\\n    <version>…</version>\\n    \\n    <dependencies>\\n        …\\n    </dependencies>\\n\\n    <build>\\n        <plugins>\\n            <plugin>\\n                <groupId>org.apache.maven.plugins</groupId>\\n                <artifactId>maven-javadoc-plugin</artifactId>\\n                <version>2.9.1</version>\\n                <executions>\\n                    <execution>\\n                        <id>generate-service-docs</id>\\n                        <phase>generate-resources</phase>\\n                        <configuration>\\n                            <doclet>com.hypnoticocelot.jaxrs.doclet.ServiceDoclet</doclet>\\n                            <docletArtifact>\\n                                <groupId>com.hypnoticocelot</groupId>\\n                                <artifactId>jaxrs-doclet</artifactId>\\n                                <version>0.0.4-SNAPSHOT</version>\\n                            </docletArtifact>\\n                            <reportOutputDirectory>${project.build.outputDirectory}</reportOutputDirectory>\\n                            <useStandardDocletOptions>false</useStandardDocletOptions>\\n                            <additionalparam>-apiVersion 1 -docBasePath /apidocs -apiBasePath /</additionalparam>\\n                        </configuration>\\n                        <goals>\\n                            <goal>javadoc</goal>\\n                        </goals>\\n                    </execution>\\n                </executions>\\n            </plugin>\\n        </plugins>\\n    </build>\\n</xml>\\nExample\\nAn example project using Dropwizard is included in jaxrs-doclet-sample-dropwizard. To get it running, run the following commands.\\n$ cd jaxrs-doclet-sample-dropwizard\\n$ mvn package\\n$ java -jar target/jaxrs-doclet-sample-dropwizard-0.0.4-SNAPSHOT.jar server sample.yml\\n\\nThe example server should be running on port 8080:\\n$ curl localhost:8080/apidocs/service.json\\n{\\n  \"apiVersion\" : \"1\",\\n  \"basePath\" : \"/apidocs/\",\\n  \"apis\" : [ {\\n    \"path\" : \"/Auth.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/HttpServletRequest.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/ModelResource_modelid.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/Recursive.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/Response.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/greetings_name.{format}\",\\n    \"description\" : \"\"\\n  } ],\\n  \"swaggerVersion\" : \"1.1\"\\n}\\n$\\n\\nOverride Swagger UI\\nTo override the swagger ui included with the doclet, create your own swagger-ui.zip file and add a swaggerUiZipPath to the additionalparam attribute in the pom file.\\n<additionalparam>-apiVersion 1 -docBasePath /apidocs -apiBasePath / -swaggerUiZipPath ../../../src/main/resources/swagger-ui.zip</additionalparam>\\n\\n',\n",
       "  'watchers': '20',\n",
       "  'stars': '90',\n",
       "  'forks': '139',\n",
       "  'commits': '162'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Expandable-RecyclerView\\nA RecyclerView that behaves like an ExpandableListView, also includes a RecyclerView with header/footer support.\\nDownload\\nDownload the latest AAR via Maven:\\n<dependency>\\n  <groupId>com.levelupstudio</groupId>\\n  <artifactId>expandable-recyclerview</artifactId>\\n  <version>1.0.1</version>\\n</dependency>\\nor Gradle:\\ncompile \\'com.levelupstudio:expandable-recyclerview:1.0.1\\'\\nLicense\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '89',\n",
       "  'forks': '30',\n",
       "  'commits': '9'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'MultiTypeDemo\\n采用真实的网络请求数据演示 MultiType 框架的用法，简化 recyclerView 的复杂列表视图\\n1.代码基本完全模仿 MaterialHome 的开源项目，因为我在浏览此项目的时候发现这个布局样式和请求数据能很好的演示  MultiType  的基本用法，所以抠出了其中的一块功能用来专门演示  MultiType 的用法，这样也便于大家去对比使用 MultiType 之后有多爽.\\n2.采用MVP，rxJava，retrofit。非常有实用性的列子来描述 MultiType 的用法。当然，如果你不了解MVP，rxJava，retrofit，刚好，你只需要看recyclerView和MultiType的搭配使用，她显得更简单\\n3.以下整个界面采用的就是一个recyclerView + MultiType 完成的\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '89',\n",
       "  'forks': '7',\n",
       "  'commits': '23'},\n",
       " {'language': 'C++ 63.4',\n",
       "  'readme': 'ZeroNights2017 (c) 2017 James Forshaw\\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\\nWindows access tokens.\\nConsists of two parts.\\n\\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\\n\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '62',\n",
       "  'forks': '16',\n",
       "  'commits': '2'},\n",
       " {'language': 'C++ 89.0',\n",
       "  'readme': 'ZeroNights2017 (c) 2017 James Forshaw\\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\\nWindows access tokens.\\nConsists of two parts.\\n\\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '62',\n",
       "  'forks': '6',\n",
       "  'commits': '34'},\n",
       " {'language': 'C++ 70.1',\n",
       "  'readme': 'ZeroNights2017 (c) 2017 James Forshaw\\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\\nWindows access tokens.\\nConsists of two parts.\\n\\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\\n\\n',\n",
       "  'watchers': '17',\n",
       "  'stars': '62',\n",
       "  'forks': '22',\n",
       "  'commits': '2,109'},\n",
       " {'language': 'C++ 66.5',\n",
       "  'readme': \"ssh\\nCreate ssh servers in node.js!\\nstatus\\nThis module has all kinds of problems since the node.js thread pool does not\\nplay nicely with libssh.\\nexample\\nsimple echo shell\\nvar ssh = require('ssh');\\n\\nssh.createServer(function (session) {\\n    session.on('password', function (user, pass, cb) {\\n        cb(user === 'foo' && pass === 'bar');\\n    });\\n    \\n    session.on('shell', function (sh) {\\n        sh.pipe(sh); // echo on\\n    });\\n}).listen(2222);\\n\\ninstallation\\nYou'll need a version of libssh with my patches:\\ngit clone git://github.com/substack/libssh.git master\\ncd libssh && mkdir build && cd build\\ncmake -DCMAKE_INSTALL_PREFIX=$PREFIX -DCMAKE_BUILD_TYPE=Debug ..\\nmake && make install\\n\\nThat should install libssh.pc, which is used by pkg-config during the\\nwscript installation. Make sure libssh.pc got installed into your\\n$PKG_CONFIG_PATH someplace.\\nYou can then install with npm:\\nnpm install ssh\\n\\n\",\n",
       "  'watchers': '8',\n",
       "  'stars': '61',\n",
       "  'forks': '13',\n",
       "  'commits': '44'},\n",
       " {'language': 'C++ 73.7',\n",
       "  'readme': \"SPDY daemon\\nThis is a wrapper around the original Google's SPDY Framer.\\nIt includes a standalone server (spdyd) which can act as a SPDY-HTTP proxy (or use yet another HTTP proxy)\\nas well as a Rack adapter.\\nThe server is built around Eventmachine, and should be pretty fast.\\nInstallation:\\nGem\\n\\ngem build spdy.gemspec\\nsudo gem install ./spdy-0.1.gem\\n\\nManual\\n\\ngem install em-http-request -v 0.3.0\\nOptional, for daemonization: gem install daemons\\ncd ext; ruby extconf.rb; make\\n\\nRunning standalone server:\\nRunning it standalone is as simple as:\\nbin/spdyd\\n\\nCheck bin/spdyd -h for options.\\nRack:\\nYou can also run it as a rack server:\\nrackup -s Spdy examples/local.ru\\n\\nor for Rails application:\\nrackup -s Spdy config.ru\\n\\nTODO:\\n\\nIntegrate with npn-enabled openssl which can be built using these steps:\\nhttps://gist.github.com/944386\\n\\nCopyright 2010 (c) Roman Shterenzon, released under the AGPLv3 license.\\n\",\n",
       "  'watchers': '1',\n",
       "  'stars': '61',\n",
       "  'forks': '6',\n",
       "  'commits': '9'},\n",
       " {'language': 'C++ 87.1',\n",
       "  'readme': \"SPDY daemon\\nThis is a wrapper around the original Google's SPDY Framer.\\nIt includes a standalone server (spdyd) which can act as a SPDY-HTTP proxy (or use yet another HTTP proxy)\\nas well as a Rack adapter.\\nThe server is built around Eventmachine, and should be pretty fast.\\nInstallation:\\nGem\\n\\ngem build spdy.gemspec\\nsudo gem install ./spdy-0.1.gem\\n\\nManual\\n\\ngem install em-http-request -v 0.3.0\\nOptional, for daemonization: gem install daemons\\ncd ext; ruby extconf.rb; make\\n\\nRunning standalone server:\\nRunning it standalone is as simple as:\\nbin/spdyd\\n\\nCheck bin/spdyd -h for options.\\nRack:\\nYou can also run it as a rack server:\\nrackup -s Spdy examples/local.ru\\n\\nor for Rails application:\\nrackup -s Spdy config.ru\\n\\nTODO:\\n\\nIntegrate with npn-enabled openssl which can be built using these steps:\\nhttps://gist.github.com/944386\\n\\nCopyright 2010 (c) Roman Shterenzon, released under the AGPLv3 license.\\n\",\n",
       "  'watchers': '7',\n",
       "  'stars': '60',\n",
       "  'forks': '41',\n",
       "  'commits': '32'},\n",
       " {'language': 'C++ 72.7',\n",
       "  'readme': \"SPDY daemon\\nThis is a wrapper around the original Google's SPDY Framer.\\nIt includes a standalone server (spdyd) which can act as a SPDY-HTTP proxy (or use yet another HTTP proxy)\\nas well as a Rack adapter.\\nThe server is built around Eventmachine, and should be pretty fast.\\nInstallation:\\nGem\\n\\ngem build spdy.gemspec\\nsudo gem install ./spdy-0.1.gem\\n\\nManual\\n\\ngem install em-http-request -v 0.3.0\\nOptional, for daemonization: gem install daemons\\ncd ext; ruby extconf.rb; make\\n\\nRunning standalone server:\\nRunning it standalone is as simple as:\\nbin/spdyd\\n\\nCheck bin/spdyd -h for options.\\nRack:\\nYou can also run it as a rack server:\\nrackup -s Spdy examples/local.ru\\n\\nor for Rails application:\\nrackup -s Spdy config.ru\\n\\nTODO:\\n\\nIntegrate with npn-enabled openssl which can be built using these steps:\\nhttps://gist.github.com/944386\\n\\nCopyright 2010 (c) Roman Shterenzon, released under the AGPLv3 license.\\n\",\n",
       "  'watchers': '5',\n",
       "  'stars': '60',\n",
       "  'forks': '25',\n",
       "  'commits': '14,488'},\n",
       " {'language': 'C++ 96.8',\n",
       "  'readme': \"Cryptose\\nCryptose is an addictive cryptogram game with three gritty occupations to choose from: Hacker, Detective, or Spy. It's available in the iOS App Store: http://itunes.apple.com/us/app/cryptose/id368874791?mt=8 and the Android Market: market://details?id=com.insurgentgames.cryptose\\nYou can see video, screenshots, and more details at: http://www.insurgentgames.com/cryptose/\\nCryptose was programmed in C++ using the Airplay SDK, which has since been renamed to the Marmalade SDK: http://www.madewithmarmalade.com/\\nIt was made using Airplay SDK 4.2, and it probably won't compile out-of-the box in the latest version of Marmalade. If anyone wants to get it to run using the newest Marmalade, patches are welcome.\\nGame Description\\nCryptose is an addictive cryptogram puzzle game where you use logic and reasoning to decode secret messages. Choose between three occupations to play as a hacker, a detective, or a spy.\\nEach game you get a random short encrypted phrase and it's your job to decrypt it. The phrase is encrypted with a simple substitution cipher, meaning that each letter in the alphabet is replaced with another letter (for example, all A's might be replaced with Q's). You start out with the ciphertext, and you decode it letter-by-letter until you reveal the plaintext message.\\nFeatures:\\n\\nThousands of phrases to decrypt\\nThree gritty graphical themes to choose from\\nUnlimited hints to curb frustration\\nLearn interesting facts and quotes about encryption, cypherpunks, and code-breaking\\n\\nLicense\\nThis game is licensed under the GNU General Public License (see gpl.txt). As the copyright owner, I hereby give anyone permission to re-license my GPL code under a non-GPL license for the purpose of distributing it in the iOS App Store or the Android Market.\\nInsurgent Games\\nInsurgent Games was founded in 2009 by Micah Lee and Crystal Mayer out of their San Francisco studio apartment. For a couple of years they happily made iPhone and Android games. They quickly realized that unless you’re incredibly lucky, it’s hard to make enough money developing indie mobile games to pay San Francisco rent. So Micah got a full time job and Crystal moved on to other things.\\nNow, several years later, Micah works for the Electronic Frontier Foundation defending internet users from evil (https://www.eff.org/files/xkcd_comic.png), and Crystal is a freelance web designer (http://moonsprocket.com/). But Insurgent Games is dormant.\\nSince they're not working on the games anymore, they decided to release them to the community. They hope their games will thrive and be reborn as bigger and better things. All of their games are licensed under the GNU General Public License.\\n\\nFollow Micah on Twitter: https://twitter.com/#!/micahflee\\nFollow Insurgent Games on Twitter: https://twitter.com/#!/insurgentgames\\n\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '60',\n",
       "  'forks': '28',\n",
       "  'commits': '2'},\n",
       " {'language': 'C++ 60.9',\n",
       "  'readme': 'Jedi-Academy\\nActivision and Raven released this code for people to learn from and play with.\\nThis code is copyright Activision 2003. This source was released under GNU GPLv2.\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '59',\n",
       "  'forks': '18',\n",
       "  'commits': '4'},\n",
       " {'language': 'C++ 86.1',\n",
       "  'readme': 'ebloom\\nOverview\\nTravis-CI :: \\nebloom is a NIF wrapper around a basic bloom filter.\\nQuick Start\\nYou must have Erlang/OTP R13B04 or later and a GNU-style build\\n  system to compile and run ebloom.\\ngit clone git://github.com/basho/ebloom.git\\ncd ebloom\\nmake\\nStart up an Erlang shell with the path to ebloom included.\\nerl -pa path/to/ebloom/ebin\\nCreate a new bloom filter, insert elements, and test for an\\n  elements presence.\\n1> PredictedElementCount=5.\\n5\\n2> FalsePositiveProbability=0.01.\\n\\n3> RandomSeed=123.\\n123\\n4> {ok, Ref} = ebloom:new(PredictedElementCount, FalsePositiveProbability, RandomSeed).\\n{ok,<<>>}\\n5> ebloom:insert(Ref, <<\"abcdef\">>).\\nok\\n6> true = ebloom:contains(Ref, <<\"abcdef\">>).\\ntrue\\n7> false = ebloom:contains(Ref, <<\"zzzzzz\">>).\\nfalse\\nContributing\\nWe encourage contributions to ebloom from the community.\\n\\nFork the ebloom repository on Github.\\nClone your fork or add the remote if you already have a clone of\\n    the repository.\\n\\ngit clone git@github.com:yourusername/ebloom.git\\n# or\\ngit remote add mine git@github.com:yourusername/ebloom.git\\n\\nCreate a topic branch for your change.\\n\\ngit checkout -b some-topic-branch\\n\\nMake your change and commit. Use a clear and descriptive commit\\n    message, spanning multiple lines if detailed explanation is\\n    needed.\\nPush to your fork of the repository and then send a pull-request\\n    through Github.\\n\\ngit push mine some-topic-branch\\n\\nA Basho engineer or community maintainer will review your patch\\n    and merge it into the main repository or send you feedback.\\n\\n',\n",
       "  'watchers': '93',\n",
       "  'stars': '59',\n",
       "  'forks': '29',\n",
       "  'commits': '68'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': '\\nOther Options\\nMaxArt2501 has started his own Object.observe polyfill, and a look at his commit history and reasoning makes me think it will probably be well supported.  If your looking for an Object.observe solution you should take a look at https://github.com/MaxArt2501/object-observe\\nNeeds a maintainer\\nI\\'d like to find someone who is willing to take this library over.  I\\'ve had no time to work with and/or maintain the library.  Honestly I have little need for Object.observe making it hard to justify time spent against it.\\nSo, if your using this shim, feel comfortable with the code, and would like to maintain it, let me know.\\nIf your curious why I don\\'t have much use for the library, this http://markdalgleish.github.io/presentation-a-state-of-change-object-observe/ pretty well sums it up.  I worked in native development when two way data binding caused all sorts of issues with application development, and I see the same coming out of Object.observe at the end of the day.\\nHopefully, someone will want to maintain this work in the future.\\nOh, and if you take it over, feel free to relicense it within reason as it seems no one likes unlicense :).  Also feel free to follow up on the polyfill-service integration if you so see fit (https://github.com/Financial-Times/polyfill-service/pull/81#issuecomment-66382432).\\nObject.observe Polyfill/shim\\nThanks to my new job I have a lot more time to devote to things like this library.  It has gone a REALLY long time without updates and there is a lot that can be done to make it more functional.  I hope to be spending more time on it soon, but for now I\\'ve fixed all the bugs that I know of and have been reported.  Thanks to everyone for their reports, and keep them coming if you find one.\\nStarted as GIST: https://gist.github.com/4173299\\nTested against Chromium build with Object.observe and acts EXACTLY the same for the basics, though Chromium build is MUCH faster.\\nTrying to stay as close to the spec as possible, this is a work in progress, feel free to comment/update\\nhttp://wiki.ecmascript.org/doku.php?id=harmony:observe\\nTODO\\nThe spec has changed a lot since I origionally wrote this, need to go back and add in a lot of things like custom update types and other fun.  For now though it seems to suffice.\\nLimits so far\\nBuilt using polling in combination with getter&setters.  This means that if you do something quick enough it won\\'t get caught.\\nExample:\\nvar myObject = {};\\nObject.observe(myObject, console.log);\\nmyObject.foo = \"bar\";\\ndelete myObject.foo;\\nThe observer function would never see the addition of foo since it was deleted so quickly.\\nTesting\\nTo run the tests first make sure you have Node.js installed.  Then use NPM to install Karam and all dependencies:\\nnpm install\\n\\nFinally run the tests with:\\nnpm test\\n\\nPlanned Updates\\nFor FireFox using Proxies will result in better performance.  Will look into this more.\\nExamples and Usage\\nSee examples folder\\nLatest Updates\\n* Memory leak fixed with PR #16\\n* Tests added to project thanks to mikeb1rd also part of PR #16\\n* Added Notifier.notify() with custom types support by klimlee\\n* Added accept list support by klimlee\\n* Stopped monitoring DOM nodes, Canary can\\'t do it and neither should the shim.\\n* Added in support for setImmediate if it is available.\\n* Memory leak fix by Moshemal\\n* Array length now reports as an update when it changes\\n* Added enumerable flag to defineProperty\\n\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '234',\n",
       "  'forks': '36',\n",
       "  'commits': '43'},\n",
       " {'language': 'JavaScript 96.0',\n",
       "  'readme': 'angular-modal \\nA modal factory service for AngularJS that makes it easy to add modals to your app.\\nInstall\\nnpm install angular-modal\\nUsage\\n\\nInclude the modal.js script provided by this component into your app.\\nOptional: Include the modal.css style provided by this component into your html.\\nAdd btford.modal as a module dependency to your app.\\n\\nExamples\\nPlunker demo\\nTypical Use\\n\\napp.js\\n\\nangular.module(\\'myApp\\', [\\'btford.modal\\']).\\n\\n// let\\'s make a modal called `myModal`\\nfactory(\\'myModal\\', function (btfModal) {\\n  return btfModal({\\n    controller: \\'MyModalCtrl\\',\\n    controllerAs: \\'modal\\',\\n    templateUrl: \\'my-modal.html\\'\\n  });\\n}).\\n\\n// typically you\\'ll inject the modal service into its own\\n// controller so that the modal can close itself\\ncontroller(\\'MyModalCtrl\\', function (myModal) {\\n  this.closeMe = myModal.deactivate;\\n}).\\n\\ncontroller(\\'MyCtrl\\', function (myModal) {\\n  this.showModal = myModal.activate;\\n});\\n\\nmy-modal.html\\n\\n<div class=\"btf-modal\">\\n  <h3>Hello {{name}}</h3>\\n  <p><a href ng-click=\"modal.closeMe()\">Close Me</a></p>\\n</div>\\n\\nindex.html\\n\\n<div ng-app=\"myApp\" ng-controller=\"MyCtrl as ctrl\">\\n  <a href ng-click=\"ctrl.showModal()\">Show the modal</a>\\n</div>\\nCleaning up\\nIf you add any listeners within the modal\\'s controller that are outside the modal\\'s scope,\\nyou should remove them with $scope.$on(\\'$destroy\\', fn () { ... }) to avoid creating a memory leak.\\nBuilding on the example above:\\n\\napp.js\\n\\n// ...\\ncontroller(\\'MyModalCtrl\\', function (myModal, $timeout) {\\n\\n  var ctrl = this,\\n      timeoutId;\\n\\n  ctrl.tickCount = 5;\\n\\n  ctrl.closeMe = function () {\\n    cancelTick();\\n    myModal.deactivate();\\n  };\\n\\n  function tick() {\\n    timeoutId = $timeout(function() {\\n      ctrl.tickCount -= 1;\\n      if (ctrl.tickCount <= 0) {\\n        ctrl.closeMe();\\n      } else {\\n        tick();\\n      }\\n    }, 1000);\\n  }\\n\\n  function cancelTick() {\\n    $timeout.cancel(timeoutId);\\n  }\\n\\n  $scope.$on(\\'$destroy\\', cancelTick);\\n\\n  tick();\\n}).\\n// ...\\nInline Options\\nNote: The best practice is to use a separate file for the template and a separate declaration for\\nthe controller, but inlining these options might be more pragmatic for cases where the template or\\ncontroller is just a couple lines.\\nangular.module(\\'myApp\\', []).\\n\\n// let\\'s make a modal called myModal\\nfactory(\\'myModal\\', function (btfModal) {\\n  return btfModal({\\n    controller: function () {\\n      this.name = \\'World\\';\\n    },\\n    controllerAs: \\'ctrl\\',\\n    template: \\'<div class=\"btf-modal\">Hello {{ctrl.name}}</div>\\'\\n  });\\n}).\\n\\ncontroller(\\'MyCtrl\\', function (myModal) {\\n  this.showModal = myModal.activate;\\n});\\n<div ng-app=\"myApp\" ng-controller=\"MyCtrl\">\\n  <a href ng-click=\"ctrl.showModal()\">Show the modal</a>\\n</div>\\nAPI\\nbtfModal\\nThe modal factory. Takes a configuration object as a parameter:\\nvar modalService = btfModal({\\n  /* options */\\n})\\nAnd returns a modalService object that you can use to show/hide the modal (described below).\\nThe config object must either have a template or a templateUrl option.\\nThese options work just like the route configuration in Angular\\'s\\n$routeProvider.\\nconfig.template\\nstring: HTML string of the template to be used for this modal.\\nUnless the template is very simple, you should probably use config.templateUrl instead.\\nconfig.templateUrl\\nstring (recommended): URL to the HTML template to be used for this modal.\\nconfig.controller\\nstring|function (optional): The name of a controller or a controller function.\\nconfig.controllerAs\\nstring (optional, recommended): Makes the controller available on the scope of the modal as the given name.\\nconfig.container\\nDOM Node (optional): DOM node to prepend . Defaults to document.body.\\nmodalService\\nA modalService has just two methods: activate and deactivate.\\nmodalService.activate\\nTakes a hash of objects to add to the scope of the modal as locals.\\nAdds the modal to the DOM by prepending it to the <body>.\\nReturns a promise that resolves once the modal is active.\\nmodalService.deactivate\\nRemoves the modal (DOM and scope) from the DOM.\\nReturns a promise that resolves once the modal is removed.\\nmodalService.active\\nReturns whether or not the modal is currently activated.\\nTests\\nYou can run the tests with karma:\\nkarma start karma.conf.js\\nLicense\\nMIT\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '234',\n",
       "  'forks': '62',\n",
       "  'commits': '35'},\n",
       " {'language': 'JavaScript 91.9',\n",
       "  'readme': 'TransformJS 1.0 Beta\\n2D and 3D transforms as regular CSS properties you can set using .css() and animate using .animate()\\nOverview\\nCSS Transforms  were first introduced in WebKit in 2007, and have now\\nreached mass-adoption by all the major browser vendors. This is great news\\nfor web developers, especially in the case of 3D transforms which are\\nhardware-accelerated, resulting in extremely smooth animations and\\nresponsive applications.\\nThe API for applying transforms however, does not scale to complex applications\\nwhich require intricate and complex management of transformations. TransformJS\\nattempts to identify and address these problems, allowing developers to\\nmake use of transforms without having to be encumbered by cross browser\\nissues, and low-level APIs.\\nHere\\'s a snippet of code that uses TransformJS to apply multiple 3d\\ntransformations to the same element, relative to their current value,\\nand animate the changes:\\n    $(\\'#test\\').animate({\\n      translateY:\\'+=150\\',\\n      scale:\\'+=2\\',\\n      rotateY: \\'+=6.24\\',\\n      rotateX: \\'+=3.15\\',\\n      rotateZ: \\'+=3.15\\'\\n    },1500);    \\nFor more detailed usage and overview information, please visit the\\nproject homepage at http://transformjs.strobeapp.com\\nLicense\\n  \\n    Copyright (c) 2011 Strobe Inc.\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n\\n\\nBack to top\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '231',\n",
       "  'forks': '23',\n",
       "  'commits': '16'},\n",
       " {'language': 'JavaScript 95.6',\n",
       "  'readme': 'DEPRECATED, I recommend you the tool SVGR\\n\\nreact-svg-inline\\n\\n\\nA react component to clean and display raw SVGs.\\n\\nInstall\\n$ npm install react-svg-inline\\nYou might also need to npm install raw-loader if you want to use this with\\nwebpack.\\nUsage\\nHere is an example of a usage in a React stateless component:\\nimport React from \"react\"\\nimport SVGInline from \"react-svg-inline\"\\n\\nexport default () => (\\n  <div>\\n    <SVGInline svg={\"<svg....</svg>\"} />\\n  </div>\\n)\\nWebpack to require() SVGs\\nUse the raw-loader to require() raw SVGs files and pass them to\\nreact-svg-inline.\\nmodule.exports = {\\n  loaders: [\\n    {\\n      test: /\\\\.svg$/,\\n      loader: \\'raw-loader\\'\\n    }\\n  ]\\n}\\nimport React from \"react\"\\nimport SVGInline from \"react-svg-inline\"\\nimport iconSVG from \"./myicon.svg\"\\n\\nexport default () => (\\n  <div>\\n    <SVGInline svg={ iconSVG } />\\n  </div>\\n)\\nOptions (props)\\nclassName\\nPropTypes.string\\nClass name used for the component that will wrap the SVG.\\nclassSuffix\\nPropTypes.string\\nThe class suffix that will be added to the svg className (default: \"-svg\").\\ncomponent\\nPropTypes.oneOfType([ PropTypes.string, PropTypes.func, ]),\\nThe component that will wrap the svg (default: span).\\nsvg\\nPropTypes.string.isRequired\\nfill\\nPropTypes.string\\nColor to use\\ncleanup\\nPropTypes.oneOfType([ PropTypes.bool, PropTypes.array, ])\\nThis allow you to cleanup (remove) some svg attributes.\\nHere are the supported value that can be removed:\\n\\ntitle\\ndesc\\ncomment\\ndefs\\nwidth\\nheight\\nfill\\nsketchMSShapeGroup\\nsketchMSPage\\nsketchMSLayerGroup\\n\\nIf cleanup === true, it will remove all the attributes above.\\ncleanupExceptions\\nPropTypes.array\\nThis allow you to whitelist some svg attributes to keep while cleaning some\\nothers.\\nwidth\\nPropTypes.string\\nheight\\nPropTypes.string\\naccessibilityLabel\\nPropTypes.string\\nThis value is added as an svg <title> element that is accessible to screen readers.\\n(Note: when this option is used, an SVG id attribute will be automatically injected).\\naccessibilityDesc\\nPropTypes.string\\nThis value is added as an svg <desc> element that is accessible to screen readers.\\n\\nCONTRIBUTING\\n\\n⇄ Pull requests and ★ Stars are always welcome.\\nFor bugs and feature requests, please create an issue.\\nPull requests must be accompanied by passing automated tests ($ npm test).\\n\\nCHANGELOG\\nLICENSE\\n',\n",
       "  'watchers': '1',\n",
       "  'stars': '231',\n",
       "  'forks': '21',\n",
       "  'commits': '24'},\n",
       " {'language': 'JavaScript 68.8',\n",
       "  'readme': 'datmusic\\nSearch and Download free music from VK.\\nDemo\\ndatmusic\\n\\nUsage wiki\\nAPI\\nVK disabled their public Audio API, so I had to write \"parser\" for their website to simulate api.\\nAPI repo\\nAndroid Version repo\\nAndroid Version repo\\nTelegram Bot repo\\nTelegram Bot repo\\nLicense\\nMIT - Alashov Berkeli\\n',\n",
       "  'watchers': '32',\n",
       "  'stars': '231',\n",
       "  'forks': '63',\n",
       "  'commits': '113'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Intelligist\\na jQuery plugin that makes it easy to share multiple, executable GitHub gists\\nDemo\\nSee Intelligist in action\\nRequirements\\njQuery\\nChosen (optional, it is auto-downloaded if not already included on the page)\\nHow to use\\nUsing Intelligist is easy. The general idea is:\\n$(selector).intelligist( gists, options );\\n\\nJust select the container, and pass in an object of gists that you\\'d like to display.\\n$(\"#demo\").intelligist({\\n      \"1973984\": \"Welcome to Intelligist\"\\n    , \"1973990\": \"Live CSS preview\"\\n    , \"1973575\": \"Live JS preview\"\\n}, { exec: true });\\n\\nThe object is made of keys (the gist ID) and values (titles for the drop-down menu).\\nNote: In this example, we are setting the \"exec\" option to true. This instructs Intelligist to execute the code after the gist is displayed. It is optional, and disabled by default.\\nImportant: If you are using the \"exec\" option, your Gist must be set to the correct language. Intelligist uses the Gist language to determine how it should execute the code.\\nOptions\\nexec\\nShould the gist be executed after it is loaded onto the page? Note: If you are using this option, your Gist must be set to the correct language (e.g. CSS or JavaScript) (default=false)\\nbefore\\nA function to be called before the gist is loaded onto the page. It is passed two variables, the ID of the previous gist and the ID of the newly selected gist. e.g. function(oldGistId, newGistId) {}\\nafter\\nA function to be called after the gist is loaded onto the page. It is passed one variable, the ID of the newly selected gist. e.g. function(newGistId) {}\\nThank yous\\n\\nGitHub: for creating the Gists service\\nHarvest: for creating Chosen\\nMartin Angelov: whose Shuffle Letters plugin is being used in the demo\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '230',\n",
       "  'forks': '18',\n",
       "  'commits': '6'},\n",
       " {'language': 'JavaScript 56.2',\n",
       "  'readme': \"Node-mysql-libmysqlclient \\nAsynchronous MySQL binding for Node.js using libmysqlclient.\\nThis module has been tested with Node.js versions 0.8.26, v0.10.25 and 0.11.10\\nOverview\\nThese bindings provides all general connection/querying functions from the MySQL C API,\\nand partial support for prepared statements. Connect, query and fetchAll are asynchronous.\\nThis module also includes support for asynchronous querySend from internals of libmysqlclient.\\nI started this project in 2010 when Node.js was growing. Ryan had plans to write this binding as part of GSoC.\\nIt is now used by many projects and has more than 10 contributors,\\nwho are listed in the AUTHORS file.\\nI also maintain the Node.js MySQL bindings benchmark which shows how mysql-libmysqlclient performs.\\nNode-mysql-libmysqlclient's source code is available in the Github repo and you can report issues at the project tracker.\\nVisit the module site for API docs and examples. You can also read some extra information in wiki.\\nDependencies\\nTo build this module you must install the libmysqlclient library and the development files for it.\\nmysql_config is used to determine the paths to the library and header files.\\nTo install these dependencies, execute the commands below for the OS you're running.\\nFor CentOS:\\n#> yum install mysql-devel\\n\\nFor openSUSE:\\n#> zypper install libmysqlclient-devel\\n\\nFor Debian-based systems/Ubuntu:\\n#> apt-get install libmysqlclient-dev\\n\\nAlternatively, you can use aptitude for Debian-based systems.\\nPlease refer to your system's documentation for more information and feel free to send me a patch for this readme.\\nInstallation\\nYou can install this module via NPM:\\n$> npm install mysql-libmysqlclient\\n\\nYou can also build latest source code from repository.\\nPlease refer to the developers documentation for more information.\\nContributing\\nThis module is written in collaboration with many peoples listed on GitHub contributors page.\\nList of authors ordered by first contribution also available.\\nIf you are interested in wide MySQL usage in Node.JS applications,\\nleave your comments to the code.\\nTo contribute any patches, simply fork this repository using GitHub\\nand send a pull request to me. Thanks!\\nAll information about development use and contribution is placed in the DEVELOPMENT file.\\nUsers and related projects\\nThis module is used by Taobao guys\\nfor their distributed MySQL proxy Myfox-query module.\\nThere is long time developed Node.js ORM library called noblerecord.\\nIt is inspired by Rails and widely used by Noblesamurai.\\nIf you are looking for lightweight Node.js ORM on top of this module,\\ntry mapper by Mario Gutierrez.\\nLicense\\nNode-mysql-libmysqlclient itself is published under MIT license.\\nSee license text in LICENSE file.\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '228',\n",
       "  'forks': '48',\n",
       "  'commits': '832'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'grunt-env \\nSpecify an ENV configuration as a task, e.g.\\ngrunt.registerTask(\\'dev\\', [\\'env:dev\\', \\'lint\\', \\'server\\', \\'watch\\']);\\ngrunt.registerTask(\\'build\\', [\\'env:build\\', \\'lint\\', \\'other:build:tasks\\']);\\n\\nGetting Started\\nInstall this grunt plugin next to your project\\'s grunt.js gruntfile with: npm install grunt-env\\nThen add this line to your project\\'s grunt.js gruntfile:\\ngrunt.loadNpmTasks(\\'grunt-env\\');\\nConfiguration\\n  env : {\\n    options : {\\n \\t//Shared Options Hash\\n    },\\n    dev : {\\n      NODE_ENV : \\'development\\',\\n      DEST     : \\'temp\\'\\n    },\\n    build : {\\n      NODE_ENV : \\'production\\',\\n      DEST     : \\'dist\\',\\n      concat   : {\\n        PATH     : {\\n          \\'value\\': \\'node_modules/.bin\\',\\n          \\'delimiter\\': \\':\\'\\n        }\\n      }\\n    },\\n    functions: {\\n      BY_FUNCTION: function() {\\n        var value = \\'123\\';\\n        grunt.log.writeln(\\'setting BY_FUNCTION to \\' + value);\\n        return value;\\n      }\\n    }\\n  }\\nUsing external files\\nYou can specify environment values in INI, JSON or YAML style and load them via the src option.\\n  env : {\\n    dev : {\\n      src : \"dev.json\"\\n    },\\n    prod: {\\n      src: \"settings.yaml\"\\n    }\\n    heroku : {\\n      src : \".env\"\\n    }\\n  }\\nUsing envdir\\nYou can specify files to read environment variables from, similar to the daemontools envdir utility.\\n  env : {\\n    dev : {\\n      src : [\"envdir/*\"],\\n      options: {\\n        envdir: true\\n      }\\n    }\\n  }\\nDynamic ENV configuration\\nThe following directives can be specified in the options to alter the environment in more specific ways\\n\\nadd\\n\\nThis will add the variables only if they don\\'t already exist\\n\\n\\nreplace\\n\\nWill replace the variable with the value specified\\n\\n\\nunshift\\n\\nWill prepend the value to the variable specified, optionally specifying a \\'delimiter\\'\\n\\n\\npush\\n\\nSame as unshift, but at the end of the value.\\n\\n\\nconcat\\n\\nFunctionally same as push, added for readability\\n\\n\\n\\nyourtask : {\\n  USER : \\'you\\',\\n  PATH : \\'/bin:/usr/bin\\'\\n\\n  options : {\\n    add : {\\n      VERBOSE : \\'1\\' // will only be added if VERBOSE isn\\'t already set\\n    },\\n    replace : {\\n      USER : \\'me\\'\\n    },\\n    push : {\\n      PATH : {\\n        value : \\'~/bin\\',\\n        delimiter : \\':\\'\\n      }\\n    },\\n    unshift : {\\n      PATH : \\'/sbin:\\'\\n    }\\n  }\\n}\\n\\nEnvironment-specific configuration\\nIn order to configure your tasks based on the environment, you need to define a task and use templates:\\n\\ngrunt.initConfig({\\n  env: {\\n    dev: {\\n      MY_CONST: \\'a\\'\\n    },\\n    prod: {\\n      MY_CONST: \\'b\\'\\n    }\\n  },\\n  myTask: {\\n    options: {\\n      myOpt: <%= MY_CONST %>\\n    }\\n  }\\n});\\n\\ngrunt.registerTask(\\'loadconst\\', \\'Load constants\\', function() {\\n    grunt.config(\\'MY_CONST\\', process.env.MY_CONST);\\n});\\n\\ngrunt.registerTask(\\'default\\', [\\n    \\'env:dev\\',\\n    \\'loadconst\\',\\n    \\'myTask\\'\\n]);\\n\\n\\nImportant note on data types\\nEnvironment variables are strings only. If you attempt to assign complex objects, they will be converted to strings.\\nContributing\\nIn lieu of a formal styleguide, take care to maintain the existing coding style. Add unit tests for any new or changed functionality. Lint and test your code using grunt.\\nRelease History\\n\\n0.4.0 Removed automatic parse, added ability to add ini or json style src files\\n0.3.0 Automatically parses .env files now\\n0.2.1 fixed npm install\\n0.2.0 grunt 0.4.0 support, simplified\\n0.1.0 Initial release\\n\\nLicense\\nLicensed under the Apache 2.0 license.\\nAuthor\\nJarrod Overson\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '226',\n",
       "  'forks': '39',\n",
       "  'commits': '66'},\n",
       " {'language': 'JavaScript 96.6',\n",
       "  'readme': 'Chart\\nAscii bar chart for nodejs.\\n\\nInstallation\\n$ npm install jstrace/chart\\n\\nExample\\nWhen data exceeds the available width the data will \"roll\" to the tail-end\\nof the array. This may become an option in the future, but that\\'s the default\\nbehaviour for now ;)\\nvar chart = require(\\'chart\\');\\nvar clear = require(\\'clear\\');\\n\\nvar data = [1, 2, ...];\\n\\nclear();\\nconsole.log(chart(data, {\\n  width: 130,\\n  height: 30,\\n  pointChar: \\'█\\',\\n  negativePointChar: \\'░\\'\\n}));\\nLicense\\nMIT\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '225',\n",
       "  'forks': '19',\n",
       "  'commits': '32'},\n",
       " {'language': 'JavaScript 99.1',\n",
       "  'readme': 'Deprecated\\nThis project is no longer maintained and is not representative of the current best practices in Marionette.\\n\\nBBCloneMail: A Backbone.Marionette Reference Application\\nNOTE: This code is currently in an experimental state. I don\\'t recommend\\nusing it as guidance for how to build Marionette apps at the moment. If you\\nwould like to see a better reference code base, check out the version of\\nBBCloneMail that Foxandxss is building.\\nSee It In Action\\nThis is a sample application, demonstrating how to use my\\nBackbone.Marionette\\nplugin for Backbone.js. You can see it in action at:\\nhttp://bbclonemail.heroku.com\\nRunning BBCloneMail On Your Computer\\nBBCloneMail is a NodeJS app built on Express.js. To run it on your\\ncomputer you\\'ll want to clone this repository to your machine somewhere,\\nand then follow these steps:\\n\\nInstall the latest http://nodejs.org if you don\\'t have it already\\nOpen a command prompt / terminal window in the BBCloneMail project folder\\nRun npm install to install all of the needed components\\nRun npm start to start the server\\nOpen http://localhost:3000 in your browser\\n\\nNote that step 1 through 3 only have to be done once. After you have\\ndone that, you just need to run step 4 and 5 any time you want to\\nsee the app running on your computer.\\nA Work In Progress\\nKeep in mind that this is always a work in progress. While the source code\\nand functionality do demonstrate all of the core features and capabilities\\nof Backbone.Marionette, the application itself is very limited in it\\'s\\nfunctionality.\\nAlso note that I haven\\'t optimized the JavaScript downloads in any way. There\\nis no minification, and no asset packaging to create a single download for the\\nentire application at this point. As a result, the app takes a moment or two\\nto download all of the JavaScript files and start up.\\nAs I continue working on functionality, I\\'ll also put in some optimizations for\\nthe JavaScript, so that it starts up faster.\\nLegal Mumbo Jumbo (MIT License)\\nCopyright (c) 2012 Derick Bailey, Muted Solutions, LLC\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '224',\n",
       "  'forks': '78',\n",
       "  'commits': '267'},\n",
       " {'language': 'Python 65.1',\n",
       "  'readme': 'GoogleClosureCompiler\\nMakes integrating the Google JavaScript Compiler with your Rails deployment process dead simple. Read why compressing your JavaScript is important\\nBoth the Google Closure Compiler API and Application are supported. Sensible defaults are provided.\\nInstalling the plugin\\nscript/plugin install git://github.com/mkelly12/google_closure_compiler.git\\n\\nRequirements\\nAny version of Rails 2.x; including Rails 2.3.4 and 2.1.2.\\nSo how does it work?\\nRead how this integrates with your workflow\\nThe plugin uses the Google Closure Compiler to optimize JavaScript files cached by Rails.\\nAnytime you use the javascript_include_tag with the :cache => true or :cache => \\'bundle_name\\' the resulting JavaScript file will be compiled. Read more about Rails asset caching\\nYou will also need this in your production.rb (and in your development.rb only when testing):\\nconfig.action_controller.perform_caching = true\\n\\nKeep in mind that cached files are saved to your public directory and only generated when needed. If you forget to delete them in the development environment they can lead to some serious headaches. It\\'s a good practice to use a naming scheme like \\'cache/bundle_name\\' so you can easily remove the cached files and add ignore rules to your version control.\\nHow does it work with the Google Closure Compiler?\\nThere are three ways to integrate with the Google Closure Compiler which are attempted in the following order:\\n\\nIf you have the Closure Compiler Application properly installed (yes we check this) then that is always used.\\nIf the Application is not detected then the API is used with your JavaScript embeded in the POST data.\\nIf your JavaScript file is larger then POST data will allow then a link to your JavaScript is sent to the API. If your host is not specified or not reachable by the Google service then no compilation is performed.\\n\\nApplication\\nThe preferred method is to use the compile.jar file which is included in this plugin.\\nYou will need the Java Runtime Environment version 6.\\nIf you are on OS X with the latest updates you will need to specify the path to the 1.6 JRE since /usr/bin/java still point to 1.5.\\nAdd the following to /config/google_closure_compiler.yml\\ndevelopment:\\n\\tjava_path: \\'/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Commands/java\\'\\n\\nAPI with code in request\\nYou don\\'t need anything besides an outgoing internet connection. However you JavaScript needs to fit in the POST data.\\nAPI with code urls\\nThis works well if your server is accessible to the world and you specified your host name using the following:\\nconfig.action_mailer.default_url_options = { :host => HOST_NAME }\\n\\nThe only limitation is that cached JavaScript files larger then 500k cannot be processed with this method.\\nUsing the Google Closure Library\\nIf you are using the Google Closure Library use the following view helper to include your Closure JavaScript:\\nclosure_include_tag \"wooly_zurbian.js\"\\n\\nIn this example wooly_zurbian.js is a JavaScript file in /public/javascripts/.\\nThis helper inserts the Google Closure Library base.js and events.js scripts when perform_caching is false. When perform_caching is true these files are not required since they are added using the calcdeps.py script.\\nThe Google Closure Library is required and is assumed to be in public/javascripts/closure. If you put it somewhere else specify the relative path from /public/javascripts/ using closure_library_path in google_closure_compiler.yml.\\nThe wooly_zurbian.js script is inserted and cached in /public/javascripts/cache/closure/wooly_zurbian.js.\\nUsing the Dependency Calculation Script\\nIf any of your cached JavaScript files contain a call to goog.require() then that cached file will be expanded using the calcdeps.py script. This requires Python 2.4 or greater and the Google Closure Library (see Using the Google Closure Library).\\ncalcdeps.py takes each goog.require() call and replaces it with the required libraries. Read more about calcdeps.py and why it is important when using the Google Closure Library.\\nADVANCED_OPTIMIZATIONS is used for the compilation of cached file that are expanded with calcdeps.py regardless of what is specified in the /config/google_closure_compiler.yml file.\\nFAQ\\nCan I change the compilation level?\\nYes, you can specify it in the config/google_closure_compiler.yml file.\\ndevelopment:\\n\\tcompilation_level: \\'ADVANCED_OPTIMIZATIONS\\'\\n\\nThe default is SIMPLE_OPTIMIZATIONS. Other options are WHITESPACE_ONLY and ADVANCED_OPTIMIZATIONS.\\nMake sure you read the documentation on Advanced Optimizations before enabling them.\\nWhat happens if there is an error or the API is down?\\nIf all compilation methods fail then the original JavaScripts are used in the bundles.\\nDoes this play nice with Smurf?\\nIt sure does. If you have Smurf installed then CSS minification works as expected and JavaScript files are processed by both Smurf and the Google Closure Compiler.\\nCopyright (c) 2009 Matt Kelly - ZURB, released under the MIT license\\n',\n",
       "  'watchers': '2',\n",
       "  'stars': '108',\n",
       "  'forks': '7',\n",
       "  'commits': '15'},\n",
       " {'language': 'Python 94.1',\n",
       "  'readme': \"Japont\\nDynamic Subsetting System for CJK fonts.\\nフォントを軽量化して配信するために，動的に必要文字を抽出したWebフォントを生成するシステム\\n比較的簡単に日本語Webフォントを導入できます\\n⚠️ Notice\\nThis branch is under developing.\\nIf you use Japont, please access with-fonttools branch.\\nDEMO\\nDEMO\\nInstallation\\nWIP\\nUsage\\nWIP\\nEnvironment variables\\n\\n\\n\\nENVS\\nDefault\\nNote\\n\\n\\n\\n\\nX_ROBOTS_TAG\\nnoindex, nofollow\\nX-Robots-Tag Header\\n\\n\\nSERVER_OWNER\\nAnonymous\\nServer owner's name\\n\\n\\nFONTS_DIR_PATH\\n./fonts\\nFolder path where fonts are\\n\\n\\nZIP_COMPRESSION_TYPE\\nZIP_STORED\\n\\n\\n\\nBIND_IP\\n0.0.0.0\\n\\n\\n\\nPORT\\n8000\\n\\n\\n\\n\\nContribution\\n\\nFork it ( http://github.com/Japont/Japont-core/fork )\\nCreate your feature branch ( git checkout -b my-new-feature )\\nCommit your changes ( git commit -am 'Add some feature' )\\nPush to the branch ( git push origin my-new-feature )\\nCreate new Pull Request\\n\\nLICENSE\\nApply the Apache License version 2.0.\\nApache License version 2.0 を適用します．\\nCopyright 2015- 3846masa\\nAuthor\\n 3846masa\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '106',\n",
       "  'forks': '6',\n",
       "  'commits': '22'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"What about running nose with a smarter interactive debugger?\\nUse this and never risk yourself forgetting import ipdb; ipdb.set_trace() in your code again!\\nThis plugin is 99.99% based on nose's builtin debug plugin.\\nIf you have any ideas about how to improve it, come and fork the code at http://github.com/flavioamieiro/nose-ipdb\\nInstall\\npip install ipdbplugin\\n\\nUsage\\nTo drop into ipdb on errors:\\nnosetests --ipdb\\n\\nTo drop into ipdb on failures:\\nnosetests --ipdb-failures\\n\\nLicense\\nGNU Lesser General Public License\\nAuthors\\n\\nBernardo Fontes (falecomigo@bernardofontes.net)\\nFlávio Amieiro (amieiro.flavio@gmail.com)\\nHenrique Bastos (henrique@bastos.net)\\n\\n\",\n",
       "  'watchers': '4',\n",
       "  'stars': '106',\n",
       "  'forks': '17',\n",
       "  'commits': '58'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"django-compat\\n\\n\\n\\n\\nForward and backwards compatibility layer for Django 1.4, 1.7, 1.8, 1.9, 1.10 and 1.11\\nConsider django-compat as an experiment based on the discussion on reddit. Let's see where it goes.\\nWhat started as an experiment based on this discussion on reddit has proven to be true in real life.\\ndjango-compat is under active development. To learn about other features, bug fixes, and changes, please refer to the changelog.\\nWho uses django-compat\\nTwo popular examples of open source reusable app that uses django-compat are django-hijack and django-background-tasks.\\nWant to have yours listed here? Send us a PR.\\nWhy use django-compat\\n\\nBe able to use the LTS versions of Django and support newer versions in your app\\nUse features from newer Django versions in an old one\\nManage and master the gap between different framework versions\\n\\nHow to use django-compat\\nInstall compat from the PyPI or download and install manually. All relevant  releases are listed here under releases.\\nUsing one of the compatible objects is easy. For example\\nfrom compat import patterns, url\\n\\nurlpatterns = patterns('ABC.views',\\n\\t\\turl(r'^abc/$', 'abc', name='abc-link'),\\n...\\n\\nSee a full example here.\\n\\n\\n\\ndjango-compat is free software. If you find it useful and would like to give back, please consider to make a donation using Bitcoin or PayPal. Thank you!\\n\\n\\n\\nCompatible objects\\n\\n\\n\\nCompatible object\\nSpecifically tested\\n1.8\\n1.9\\n1.10\\n1.11\\nNotes\\n\\n\\n\\n\\nBytesIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nDjangoJSONEncoder\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nEmailValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nGenericForeignKey\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nmodels.GenericForeignKey\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nHttpResponseBase\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nJsonResponse\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexProvider\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nNoReverseMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLPattern\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolver404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolverMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nSortedDict\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nStringIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nURLValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nVariableNode\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nView\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nadd_to_builtins\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nadmin_utils\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\natomic\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclean_manytomany_helptext\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclear_url_caches\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclose_connection\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit_on_success\\n✔️\\n✔️\\n✔️\\n✔️\\n`✔️\\ncommit_on_success replaced by atomic in Django >= 1.8\\n\\n\\nforce_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nformat_html\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_callable\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_current_site\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ident\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_mod_func\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model_name\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ns_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_template_loaders\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_user_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_username_field\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler500\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_module\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_string\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ninclude\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nis_valid_path\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nparse_qs\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\npatterns\\n✖️\\n✔️\\n✔️\\n❌\\n❌\\n\\n\\n\\npython_2_unicode_compatible\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrender_to_string\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nThe new function signature (https://docs.djangoproject.com/en/1.9/releases/1.8/#dictionary-and-context-instance-arguments-of-rendering-functions) is backported to pre-1.8.\\n\\n\\nresolve\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nresolve_url\\n✔️\\n✔️\\n✔️\\n⚠️\\n⚠️\\n1.10: Reversing by dotted path has been removed\\n\\n\\nreverse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nreverse_lazy\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrollback\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTransaction savepoint (sid) is required for Django < 1.8\\n\\n\\nset_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nset_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsimplejson\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nslugify\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsmart_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nunquote_plus\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurl\\n✖️\\n✔️\\n✔️\\n✖️\\n✖️\\nFunction used in urlpatterns\\n\\n\\ntempat.url\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n`uravy_multiplication_x:\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlresolvers\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlunparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nuser_model_label\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ntemplatetags.compat.verbatim\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTemplatetag; import with {% load verbatim from compat %}. 1.4: Does not allow specific closing tags, e.g. {% endverbatim myblock %}, and does not preserve whitespace inside tags.\\n\\n\\n\\nResources and references\\nResources\\n\\nhttps://github.com/ubernostrum/django-compat-lint\\nhttps://docs.djangoproject.com/en/dev/misc/api-stability/\\nhttps://docs.djangoproject.com/en/dev/topics/python3/\\nhttp://andrewsforge.com/presentation/upgrading-django-to-17/\\n\\ncompat.py\\nBits and bites of the following projects were re-used to build django-compat.\\n\\n https://github.com/lukaszb/django-guardian/blob/devel/guardian/compat.py\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/compat.py\\n https://github.com/toastdriven/django-tastypie/blob/master/tastypie/compat.py\\n https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/compat.py\\n\\n TODO: MinValueValidator, MaxValueValidator et al. (other relevant bits are included) Django 1.8\\n\\n\\n https://gist.github.com/theskumar/ff8de60ff6a33bdacaa8\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/templatetags/compat.py\\n https://github.com/kennethreitz/requests/blob/master/requests/compat.py\\n https://github.com/mitsuhiko/jinja2/blob/master/jinja2/_compat.py\\n https://github.com/jaraco/setuptools/blob/master/setuptools/compat.py\\n https://github.com/mariocesar/sorl-thumbnail/blob/master/sorl/thumbnail/compat.py\\n\\nChangelog\\n2017/04/07\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/08/02\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/06/01\\n\\nAdd get_current_site and admin_utils\\n\\n2016/05/11\\n\\nFix error when installing package under python 3.4\\n\\n###\\xa02015/11/12\\n\\nBackport new render_to_string function signature to Django < 1.8\\nBackport verbatim tag to Django 1.4\\nAdd get_template_loaders\\nAdd close_connection\\nImprove JsonResponse backport to Django 1.4\\nAdd tests for import_module, get_model and add_to_builtins\\nAnticipate renaming of django.core.urlresolvers to django.urls in 1.10\\nAvoid warnings in setup.py\\n\\n2015/11/11\\n\\n1.9 compatibility for existing objects with the following changes:\\n\\nadd_to_builtins was removed for Django >= 1.9\\nGenericForeignKey` was moved to compat.models`` for Django >= 1.9\\n\\n\\n\\n2015/07/15\\n\\nadd_to_builtins was added\\n\\n2015/07/08\\n\\nget_query_set/get_queryset support was dropped again (see #29)\\n\\n\",\n",
       "  'watchers': '5',\n",
       "  'stars': '106',\n",
       "  'forks': '21',\n",
       "  'commits': '215'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"django-compat\\n\\n\\n\\n\\nForward and backwards compatibility layer for Django 1.4, 1.7, 1.8, 1.9, 1.10 and 1.11\\nConsider django-compat as an experiment based on the discussion on reddit. Let's see where it goes.\\nWhat started as an experiment based on this discussion on reddit has proven to be true in real life.\\ndjango-compat is under active development. To learn about other features, bug fixes, and changes, please refer to the changelog.\\nWho uses django-compat\\nTwo popular examples of open source reusable app that uses django-compat are django-hijack and django-background-tasks.\\nWant to have yours listed here? Send us a PR.\\nWhy use django-compat\\n\\nBe able to use the LTS versions of Django and support newer versions in your app\\nUse features from newer Django versions in an old one\\nManage and master the gap between different framework versions\\n\\nHow to use django-compat\\nInstall compat from the PyPI or download and install manually. All relevant  releases are listed here under releases.\\nUsing one of the compatible objects is easy. For example\\nfrom compat import patterns, url\\n\\nurlpatterns = patterns('ABC.views',\\n\\t\\turl(r'^abc/$', 'abc', name='abc-link'),\\n...\\n\\nSee a full example here.\\n\\n\\n\\ndjango-compat is free software. If you find it useful and would like to give back, please consider to make a donation using Bitcoin or PayPal. Thank you!\\n\\n\\n\\nCompatible objects\\n\\n\\n\\nCompatible object\\nSpecifically tested\\n1.8\\n1.9\\n1.10\\n1.11\\nNotes\\n\\n\\n\\n\\nBytesIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nDjangoJSONEncoder\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nEmailValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nGenericForeignKey\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nmodels.GenericForeignKey\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nHttpResponseBase\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nJsonResponse\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexProvider\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nNoReverseMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLPattern\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolver404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolverMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nSortedDict\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nStringIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nURLValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nVariableNode\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nView\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nadd_to_builtins\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nadmin_utils\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\natomic\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclean_manytomany_helptext\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclear_url_caches\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclose_connection\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit_on_success\\n✔️\\n✔️\\n✔️\\n✔️\\n`✔️\\ncommit_on_success replaced by atomic in Django >= 1.8\\n\\n\\nforce_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nformat_html\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_callable\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_current_site\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ident\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_mod_func\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model_name\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ns_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_template_loaders\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_user_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_username_field\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler500\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_module\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_string\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ninclude\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nis_valid_path\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nparse_qs\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\npatterns\\n✖️\\n✔️\\n✔️\\n❌\\n❌\\n\\n\\n\\npython_2_unicode_compatible\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrender_to_string\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nThe new function signature (https://docs.djangoproject.com/en/1.9/releases/1.8/#dictionary-and-context-instance-arguments-of-rendering-functions) is backported to pre-1.8.\\n\\n\\nresolve\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nresolve_url\\n✔️\\n✔️\\n✔️\\n⚠️\\n⚠️\\n1.10: Reversing by dotted path has been removed\\n\\n\\nreverse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nreverse_lazy\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrollback\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTransaction savepoint (sid) is required for Django < 1.8\\n\\n\\nset_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nset_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsimplejson\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nslugify\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsmart_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nunquote_plus\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurl\\n✖️\\n✔️\\n✔️\\n✖️\\n✖️\\nFunction used in urlpatterns\\n\\n\\ntempat.url\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n`uravy_multiplication_x:\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlresolvers\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlunparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nuser_model_label\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ntemplatetags.compat.verbatim\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTemplatetag; import with {% load verbatim from compat %}. 1.4: Does not allow specific closing tags, e.g. {% endverbatim myblock %}, and does not preserve whitespace inside tags.\\n\\n\\n\\nResources and references\\nResources\\n\\nhttps://github.com/ubernostrum/django-compat-lint\\nhttps://docs.djangoproject.com/en/dev/misc/api-stability/\\nhttps://docs.djangoproject.com/en/dev/topics/python3/\\nhttp://andrewsforge.com/presentation/upgrading-django-to-17/\\n\\ncompat.py\\nBits and bites of the following projects were re-used to build django-compat.\\n\\n https://github.com/lukaszb/django-guardian/blob/devel/guardian/compat.py\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/compat.py\\n https://github.com/toastdriven/django-tastypie/blob/master/tastypie/compat.py\\n https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/compat.py\\n\\n TODO: MinValueValidator, MaxValueValidator et al. (other relevant bits are included) Django 1.8\\n\\n\\n https://gist.github.com/theskumar/ff8de60ff6a33bdacaa8\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/templatetags/compat.py\\n https://github.com/kennethreitz/requests/blob/master/requests/compat.py\\n https://github.com/mitsuhiko/jinja2/blob/master/jinja2/_compat.py\\n https://github.com/jaraco/setuptools/blob/master/setuptools/compat.py\\n https://github.com/mariocesar/sorl-thumbnail/blob/master/sorl/thumbnail/compat.py\\n\\nChangelog\\n2017/04/07\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/08/02\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/06/01\\n\\nAdd get_current_site and admin_utils\\n\\n2016/05/11\\n\\nFix error when installing package under python 3.4\\n\\n###\\xa02015/11/12\\n\\nBackport new render_to_string function signature to Django < 1.8\\nBackport verbatim tag to Django 1.4\\nAdd get_template_loaders\\nAdd close_connection\\nImprove JsonResponse backport to Django 1.4\\nAdd tests for import_module, get_model and add_to_builtins\\nAnticipate renaming of django.core.urlresolvers to django.urls in 1.10\\nAvoid warnings in setup.py\\n\\n2015/11/11\\n\\n1.9 compatibility for existing objects with the following changes:\\n\\nadd_to_builtins was removed for Django >= 1.9\\nGenericForeignKey` was moved to compat.models`` for Django >= 1.9\\n\\n\\n\\n2015/07/15\\n\\nadd_to_builtins was added\\n\\n2015/07/08\\n\\nget_query_set/get_queryset support was dropped again (see #29)\\n\\n\",\n",
       "  'watchers': '4',\n",
       "  'stars': '106',\n",
       "  'forks': '11',\n",
       "  'commits': '411'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Zippopotamus Cloud\\n\\nAPI Moved\\nOur full crowd-source zip-db can be found\\nhere\\nTrea has taken over Zippopotamus - you can find it at the ekotechnology fork\\nThanks\\nJeff & Samir\\n\\nThis is a repository for  Zippopotamus the global postal code API\\nZippopotamus is hosted by dotCloud. This repo is used to build and maintain the site.\\nIf you want to contribute to the improving the site, back-end, front-end etc. Just fork away and submit pull requests.\\nSample Implementations\\nCheckout the static/ folder to see some of the sample implementations of Zippopotamus for inspiration and examples for how to implement Zippopotamus API for use in your website etc.\\nIf you want to share an implementation, we would love to post example cases of Zippopotamus on our homepage.\\nResponse Format\\nOn May 1st Zippopotamus changed their JSON response format to work better with international postal codes.  Now we support a one-to-many format service. That is that one zip code may map to many regions, this is common in countries like Spain and France (but not in the US and Germany).\\nPostal Code Information\\nFor information our postal codes and countries supported, you should check out the zippopotamus crowd-sourcing project.  Here you can download the entire database dump, or fork and add changes that we will incorporate into our DB.\\nTechnical Information\\nWhat is Zippopotamus built on\\nAt the moment the zippopotamus is built on Python, MongoDB and bottle.py framework.\\nLocal Testing?\\nThe site is configured to run on dotCloud, if you want to test out the web interfaceyou can change the wsgi.py file to include the last commented line, which is used to run the site on your local host.\\nSuggestions and Comments?\\nHate it? Love it? Open an issue if you have a problem or contact\\nJeff Crowell or Samir Ahmed\\nAlso, we aren't bottle or python or mongo experts. So if you see a way that we can improve, please let us know. Additionally, if you have examples (translation corrects etc) of using Zippopotamus that you want to share let us know and we can feature your site / blog on the homepage.\\n\",\n",
       "  'watchers': '12',\n",
       "  'stars': '105',\n",
       "  'forks': '56',\n",
       "  'commits': '44'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Twitty Twister\\nTwitty twister allows you to access twitter-compatible APIs from pure twisted\\napps. It supports standard and OAuth authentication.\\nSee the example/ directory\\nfor example commandline tools that exercise various parts of the API.\\nWhat are People Saying About It?\\nNot much, but I consider this an excellent review:\\n\\nA winning project name if ever I've read one.\\n\\n— Alex Payne\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '105',\n",
       "  'forks': '32',\n",
       "  'commits': '207'},\n",
       " {'language': 'Python 66.8',\n",
       "  'readme': \"Uploadr.py\\nUploadr.py is a simple Python script for uploading your photos to Flickr. Unlike\\nmany GUI applications out there, it lends itself to automation; and because it's\\nfree and open source, you can just change it if you don't like it.\\n\\nAuthentication\\nTo use this application, you need to obtain your own Flickr API key and secret\\nkey. You can apply for keys on the Flickr website.\\nWhen you have got those keys, you need to set environment variables so that they\\ncan be used by this application. For example, if you use Bash, add the following\\nlines to your $HOME/.bash_profile:\\nexport FLICKR_UPLOADR_PY_API_KEY=0123456789abcdef0123456789abcdef\\nexport FLICKR_UPLOADR_PY_SECRET=0123456789abcdef\\n\\n\\nLicense\\nUploadr.py consists of code by Cameron Mallory, Martin Kleppmann, Aaron Swartz and\\nothers. See COPYRIGHT for details.\\n\",\n",
       "  'watchers': '8',\n",
       "  'stars': '105',\n",
       "  'forks': '54',\n",
       "  'commits': '13'},\n",
       " {'language': 'Python 99.3',\n",
       "  'readme': 'Spans\\n\\n   \\n \\n \\n\\nSpans is a pure Python implementation of PostgreSQL\\'s\\nrange types.\\nRange types are convenient when working with intervals of any kind. Every time\\nyou\\'ve found yourself working with date_start and date_end, an interval may have\\nbeen what you were actually looking for.\\nSpans has successfully been used in production since its first release\\n30th August, 2013.\\n\\nInstallation\\nSpans exists on PyPI.\\n$ pip install Spans\\nDocumentation is hosted on Read the\\nDocs.\\n\\nExample\\nImagine you are building a calendar and want to display all weeks that overlaps\\nthe current month. Normally you have to do some date trickery to achieve this,\\nsince the month\\'s bounds may be any day of the week. With Spans\\' set-like\\noperations and shortcuts the problem becomes a breeze.\\nWe start by importing date and daterange\\n>>> from datetime import date\\n>>> from spans import daterange\\nUsing daterange.from_month we can get range representing January in the year\\n2000\\n>>> month = daterange.from_month(2000, 1)\\n>>> month\\ndaterange(datetime.date(2000, 1, 1), datetime.date(2000, 2, 1))\\nNow we can calculate the ranges for the weeks where the first and last day of\\nmonth are\\n>>> start_week = daterange.from_date(month.lower, period=\"week\")\\n>>> end_week = daterange.from_date(month.last, period=\"week\")\\n>>> start_week\\ndaterange(datetime.date(1999, 12, 27), datetime.date(2000, 1, 3))\\n>>> end_week\\ndaterange(datetime.date(2000, 1, 31), datetime.date(2000, 2, 7))\\nUsing a union we can express the calendar view.\\n>>> start_week.union(month).union(end_week)\\ndaterange(datetime.date(1999, 12, 27), datetime.date(2000, 2, 7))\\nDo you want to know more? Head over to the\\ndocumentation.\\n\\nUse with Psycopg2\\nTo use these range types with Psycopg2 the\\nPsycoSpans.\\n\\nMotivation\\nFor a project of mine I started using PostgreSQL\\'s tsrange type and needed\\nan equivalent in Python. These range types attempt to mimick PostgreSQL\\'s\\nbehavior in every way. Deviating from it is considered as a bug and should be\\nreported.\\n\\nContribute\\nI appreciate all the help I can get! Some things to think about:\\n\\nIf it\\'s a simple fix, such as documentation or trivial bug fix, please file\\nan issue or submit a pull request. Make sure to only touch lines relevant to\\nthe issue. I don\\'t accept pull requests that simply reformat the code to be\\nPEP8-compliant. To me the history of the repository is more important.\\nIf it\\'s a feature request or a non-trivial bug, always open an issue first to\\ndiscuss the matter. It would be a shame if good work went to waste because a\\npull request doesn\\'t fit the scope of this project.\\n\\nPull requests are credited in the change log which is displayed on PyPI and the\\ndocumentaion on Read the Docs.\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '105',\n",
       "  'forks': '9',\n",
       "  'commits': '105'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Instadrop\\nAutomatically sync your Instagram photos to Dropbox\\nA demo application of the Instagram real-time API.\\nIt's live! http://instadrop.appspot.com\\nInstallation on Google App Engine\\n\\nSign up for an App Engine account.\\nDownload the App Engine SDK.\\nFork and modify our code! Need help? Read the Google App Engine Getting Started Guide\\n\\nFollow @instagramapi on Twitter\\nYou should follow @instagramapi on Twitter for announcements,\\nupdates, and news about the Instagram API.\\nJoin the mailing list!\\nhttps://groups.google.com/group/instagram-api-developers\\nDid you fork this app to create something cool?\\nAdd it to the apps wiki!\\nContributing\\nIn the spirit of free software, everyone is encouraged to help improve this project.\\nHere are some ways you can contribute:\\n\\nby using alpha, beta, and prerelease versions\\nby reporting bugs\\nby suggesting new features\\nby writing or editing documentation\\nby writing specifications\\nby writing code (no patch is too small: fix typos, add comments, clean up inconsistent whitespace)\\nby refactoring code\\nby closing issues\\nby reviewing patches\\n\\nSubmitting an Issue\\nWe use the GitHub issue tracker to track bugs and\\nfeatures. Before submitting a bug report or feature request, check to make sure it hasn't already\\nbeen submitted. You can indicate support for an existing issuse by voting it up. When submitting a\\nbug report, please include a Gist that includes a stack trace and any\\ndetails that may be necessary to reproduce the bug, including your Python version and\\noperating system. Ideally, a bug report should include a pull request with failing specs.\\nSubmitting a Pull Request\\n\\nFork the project.\\nCreate a topic branch.\\nImplement your feature or bug fix.\\nAdd documentation for your feature or bug fix.\\nCommit and push your changes.\\nSubmit a pull request.\\n\\nCopyright\\nCopyright (c) 2011 Instagram (Burbn, Inc).\\nSee LICENSE for details.\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '105',\n",
       "  'forks': '25',\n",
       "  'commits': '2'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'SimpleRecyclerView\\n\\n中文 README\\nAn enhancement to RecyclerView and SwipeRefreshLayout. Integrated with timehop/sticky-headers-recyclerview for sticky headers.\\nhttps://github.com/timehop/sticky-headers-recyclerview\\n\\nMain Characters:\\n1. Pull-To-Refresh\\nAn enhancement to SwipeRefreshLayout in 2 aspects:\\n\\n\\nIn some versions of android.support library, SwipeRefreshLayout has\\nsliding conflict with AppbarLayout. When you pull down RecyclerView, SwipeRefreshLayout will appear instantly, preventing you from pulling down the list.\\n\\n\\nNow you can invoke setRefreshing(true) to show loading progress in onCreate() while the official\\'s can not.\\n\\n\\n2.Load more\\n\\n\\nAutomatically load more data when there are still ${THRESHOLD} items to the bottom (THRESHOLD customizable)\\n\\n\\nIf user slides to the bottom and the loading process is still incomplete, the loading progress animation will be displayed.\\n\\n\\nLayoutManager irrelevant\\n\\n\\n2 indicator styles available: ProgressBar and SwipeRefreshLayout (The SwipeRefreshLayout style is independent of the Android version, with indicator color and background color Customizable. The ProgressBar style is dependent on the Android version. Its Material Design style is only available in API 21+.)\\n\\n\\n3.Loading View / Empty View / Error View\\n4.OnItemClickListener / OnItemLongClickListener\\n5.Sticky headers\\n\\nSupports any number of fixed header types\\n\\n6.Item divider support\\n\\n\\nCustomizable divider width and color\\n\\n\\nYou can customize the length of the blank area on the left / top / right / bottom of the divider (divider will not draw in blank area)\\n\\n\\nSupports horizontal / vertical LinearLayoutManager\\n\\n\\n7.Item animation\\n\\nMaterial Design animation when initing RecyclerView, adding / modifying / deleting items\\n\\n8.Support for group display\\n9.Get scrolled distance and distance to end\\nSimpleRecyclerView added 2 methods to get these distances;\\nMeanwhile, these two distances are passed as parameters in SimpleOnScrollListener.onScrolled.\\n//Scrolled distance (px)\\nint SimpleRecyclerView.getScrolledDistance();\\n\\n//Distance to end (px)\\nint SimpleRecyclerView.getDistanceToEnd();\\n\\nabstract class SimpleOnScrollListener extends RecyclerView.OnScrollListener {\\n  abstract void onScrollStateChanged(int scrolledDistance, int distanceToEnd, int newState);\\n  /**\\n  * @param scrolledDistance Scrolled distance (px)\\n  * @param distanceToEnd Distance to end (px)\\n  * @param velocity Current scroll velocity (positive or negative indicates the direction)\\n  */\\n  abstract void onScrolled(int scrolledDistance, int distanceToEnd, int velocity);\\n}\\n\\nImport\\n1.Add binary\\nIn build.gradle, add\\ncompile \\'com.xdandroid:simplerecyclerview:+\\'\\n\\n2.Basic usage is the same as official RecyclerView and SwipeRefreshLayout\\n\\n\\nSimple* classes inherit from the official widgets\\n\\n\\nPlease refer to the demo for usage\\n\\n\\nIf you have multiple viewType (Adapter inherits com.xdandroid.simplerecyclerview.Adapter), The ways of setting data to Adapter are the same as RecyclerView.Adapter. You can pass the data set by the constructor, or you can create a method called setList or so, setting data set to the Adapter and refresh UI using notifyDataSetChanged().\\n\\n\\nIf there is only one viewType (Adapter inherits SingleViewTypeAdapter<${JavaBean}>), the method of setting data list to Adapter is:\\n\\n\\n.\\nrecyclerView.setAdapter(adapter);\\nvoid Adapter.setList(List<${JavaBean}> list);\\n\\n3.Layout XML\\n<com.xdandroid.simplerecyclerview.SimpleSwipeRefreshLayout\\n    tools:context=\"com.xdandroid.sample.MainActivity\"\\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n    xmlns:tools=\"http://schemas.android.com/tools\"\\n    app:layout_behavior=\"@string/appbar_scrolling_view_behavior\"\\n    tools:showIn=\"@layout/activity_main\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\"\\n    android:id=\"@+id/swipe_container\">\\n\\n    <com.xdandroid.simplerecyclerview.SimpleRecyclerView\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"match_parent\"\\n        android:id=\"@+id/recycler_view\"/>\\n\\n</com.xdandroid.simplerecyclerview.SimpleSwipeRefreshLayout>\\n\\n<!--Empty View-->\\n<TextView\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\"\\n    android:id=\"@+id/empty_view\"\\n    android:visibility=\"gone\"/>\\n\\n<!--Error View-->\\n<TextView\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\"\\n    android:id=\"@+id/error_view\"\\n    android:visibility=\"gone\"/>\\n\\n<!--Loading View-->\\n<FrameLayout\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\"\\n    android:id=\"@+id/loading_view\">\\n\\n    <ProgressBar\\n        android:layout_width=\"wrap_content\"\\n        android:layout_height=\"wrap_content\"\\n        android:layout_gravity=\"center\"\\n        android:layout_marginBottom=\"20dp\"/>\\n\\n    <TextView\\n        android:layout_width=\"wrap_content\"\\n        android:layout_height=\"wrap_content\"\\n        android:layout_gravity=\"center\"\\n        android:text=\"Hello Loading View\"\\n        android:layout_marginTop=\"20dp\"\\n        android:textSize=\"20sp\"\\n        android:textColor=\"@android:color/black\"/>\\n</FrameLayout>\\n\\n4. Define Adapter abstract class\\n4.1 When you only have one viewType, subclass SingleViewTypeAdapter<${JavaBean}>\\n\\n\\nOverride onViewHolderCreate, corresponds to onCreateViewHolder in RecyclerView.Adapter\\n\\n\\nOverride onViewHolderBind, corresponds to onBindViewHolder in RecyclerView.Adapter\\n\\n\\nCreate ViewHolder\\n\\n\\nOverride int getItemSpanSize(int position, int viewType, int spanCount) if you use GridLayoutManager\\n\\n\\nDo not override onLoadMore or hasMoreElements, instead implement them when instantiat Adapter in Activity / Fragment.\\n\\n\\n4.2 Multiple viewType, subclass Adapter\\n\\n\\nOverride onViewHolderCreate, corresponds to onCreateViewHolder in RecyclerView.Adapter\\n\\n\\nOverride onViewHolderBind, corresponds to onBindViewHolder in RecyclerView.Adapter\\n\\n\\nOverride getViewType, corresponds to getItemViewType in RecyclerView.Adapter\\n\\n\\nOverride getCount, corresponds to getItemCount in RecyclerView.Adapter\\n\\n\\nCreate ViewHolder classes for each viewType\\n\\n\\nOverride int getItemSpanSize(int position, int viewType, int spanCount) if you use GridLayoutManager\\n\\n\\nDo not override onLoadMore or hasMoreElements, instead implement them when instantiat Adapter in Activity / Fragment.\\n\\n\\nPull-To-Refresh\\nResolve a sliding conflict with AppbarLayout: Assign an id to AppBarLayout named \"appbar\" (android:id=\"@+id/appbar\")\\nTo automatically leave the height for Toolbar and prevent from being blocked by the Toolbar, add app:layout_behavior=\"@string/appbar_scrolling_view_behavior\" to SwipeRefreshLayout or the layout under CoordinatorLayout.\\nLoad more\\nImplement 2 methods when instantiate Adapter:\\n1. void onLoadMore(Void please_make_your_adapter_class_as_abstract_class) :\\nFirst self-increment pageIndex varible by one, then invoke API to get more data. After new piece of data is parsed:\\nFor SingleViewTypeAdapter, Do NOT call List[E].addAll(Collection[? extends E]) on your data set, instead call void SingleViewTypeAdapter.addAll(List[E]) directly. SingleViewTypeAdapter will update the underlying data set automatically.\\nFor Adapter, first update your data set using List[E].addAll(Collection[? extends E]), then call void Adapter.onAddedAll(int newDataSize) to notify Adapter that some data has been added to the data set.\\n2. boolean hasMoreElements(Void let_activity_or_fragment_implement_these_methods) :\\nTell the Adapter whether there is more data to load.\\nif needed, you can call void Adapter.setLoadingFalse() to restore the status of not loading more.\\nCustomize style:\\nadapter.setUseMaterialProgress(true, new int[]{getResources().getColor(R.color.colorAccent)});\\n\\nWhen the first parameter (boolean useMaterialProgress) is true, SwipeRefreshLayout style is used, otherwise ProgressBar style is used. The second parameter (int[] colors) is available only if useMaterialProgress is true. You can pass an int[] in the method to set the color set of the loading indicator. If there are more than one color in the int[], the indicator will iterate the colors at the frequency of one color per turn.\\nadapter.setColorSchemeColors(new int[]{getResources().getColor(R.color.colorAccent)});\\n\\nCall the method above to change the indicator color at any time.\\nprogressView.setProgressBackgroundColor(Color.parseColor(\"#FAFAFA\"));\\n\\nSet the background color of indicator.\\nSet threshold: void Adapter.setThreshold(int threshold);\\nFor GridLayoutManager:\\nGridLayoutManager gridLayoutManager = new GridLayoutManager(context,SPAN_SIZE);\\ngridLayoutManager.setSpanSizeLookup(adapter.getSpanSizeLookup(SPAN_SIZE));\\nrecyclerView.setLayoutManager(gridLayoutManager);\\n\\nPlease refer to GridFragment and GridAdapter in the demo.\\nLoading View / Empty View / Error View\\nXML prepare (Please refer to Import - Layout XML)\\n\\n\\nPlace the LoadingView in parallel with the SwipeRefreshLayout element.\\n\\n\\nPlace the ErrorView / EmptyView in parallel with the SwipeRefreshLayout element. Set the visibility of ErrorView / EmptyView to \"gone\".\\n\\n\\nJava Code\\n/**\\n * Call this method to set the LoadingView before calling the setAdapter or notify* methods.\\n * LoadingView will automatically hide when the setAdapter or notify* methods is called.\\n * @param loadingView LoadingView got by findViewById.\\n */\\nvoid SimpleRecyclerView.setLoadingView(View loadingView);   //Call this method to set the LoadingView before calling the setAdapter or notify* methods.\\n//Example:\\nrecyclerView.setLoadingView(findViewById(R.id.loading_view));   //Set custom LoadingView layout.\\n\\nView SimpleRecyclerView.hideLoadingView();                  //Manually hide LoadingView. Generally needn\\'t.\\nView SimpleRecyclerView.showLoadingView();                  //Manually show LoadingView. Generally needn\\'t.\\n\\nrecyclerView.setEmptyView(findViewById(R.id.empty_view)); \\t//Set EmptyView\\nrecyclerView.showErrorView(findViewById(R.id.error_view));\\t//Set and show ErrorView\\nrecyclerView.hideErrorView();\\t\\t\\t\\t\\t\\t\\t\\t//Hide ErrorView\\n\\nOnItemClickListener/OnItemLongClickListener\\nadapter.setOnItemClickListener(new OnItemClickListener());\\nadapter.setOnItemLongClickListener(new OnItemLongClickListener());\\n\\nTo implement ripple effect on item clicks, add the code below to the root element of the item layout XML:\\nandroid:foreground=\"?android:attr/selectableItemBackground\"(for CardView, SimpleDraweeView)\\nandroid:background=\"?android:attr/selectableItemBackground\"(for general View)\\n\\nFor CardView, add the code above to the element of CardView, instead of adding to the root element.\\nDivider\\nInstantiate Divider:\\npublic Divider(\\n  @Px int width,           //The width of the divider\\n  @ColorInt int color,     //The color of the divider\\n  boolean isHorizontalList,   //Whether the LinearLayoutManager is horizontal\\n  @Px int leftOffset, @Px int topOffset, @Px int rightOffset, @Px int bottomOffset);\\n\\n\\n\\nleftOffset is the length of the blank area on the left of the divider (divider will not draw in blank area).\\n\\n\\ntopOffset / rightOffset / bottomOffset are the same.\\n\\n\\nUsage：\\nmRecyclerView.addItemDecoration(Divider divider);\\n\\nAnimation when initing RecyclerView, adding / modifying / deleting items\\nThe Adapter / SingleViewTypeAdapter encapsulates common methods for manipulating data sets. Using these methods, you will get animation effects and the correct loading state settings.\\nAdapter:\\n\\nvoid onAdded();\\nvoid onAdded(int position);\\nvoid onAddedAll(int newDataSize);\\nvoid onAddedAll(int position, int newDataSize);\\nvoid onListSet();\\nvoid onRemovedLast(); / void onRemoved();\\nvoid onRemoved(int position);\\nvoid onRemoveAll(int positionStart, int itemCount);\\nvoid onSet(int position);\\nvoid onSetAll(int positionStart, int itemCount);\\n\\nWhen you are using Adapter, you should update your data set first, then call the above methods.\\nSingleViewTypeAdapter :\\n\\nvoid setList(List<${JavaBean}> list);\\nvoid add(${JavaBean} javaBean);\\nvoid add(int position, ${JavaBean} javaBean);\\nvoid remove(int position);\\nvoid removeLast(); / void remove();\\nvoid removeAll(int positionStart, int itemCount);\\nvoid set(int position,${JavaBean} javaBean);\\nvoid setAll(int positionStart, int itemCount, ${JavaBean} javaBean);\\nvoid addAll(int position, List<${JavaBean}> newList);\\nvoid addAll(List<${JavaBean}> newList);\\n\\nWhen you are using SingleViewTypeAdapter, you should NOT update your data set, instead call the above methods directly.\\nYou only need to call the methods above, SingleViewTypeAdapter will update the underlying data set automatically.\\nIf the required method of operation on the data set is not listed above, you can operate on the data set List[${JavaBean}] first, then call adapter.notifyItem* mmethods to refresh UI, finally call setLoadingFalse() to restore the status of not loading more.\\nSticky headers\\n1.Make your Adapter class implements StickyRecyclerHeadersAdapter<RecyclerView.ViewHolder> interface;\\n2.Create your Header ViewHolder;\\n3.Implement 3 methods in the interface:\\n\\nlong getHeaderId(int position);\\n\\nThis method determines which header the item in ${position} is displayed under. One header corresponds to one headerId, so, For items that want to be displayed under the same header, this method should return the same headerId for the items\\' positions. The number of headerIds the method possibility returns is the number of fixed headers.\\nYou can call this method to get the headerId for the current adapterPosition in onViewHolderBind and onBindHeaderViewHolder.\\n\\n\\nRecyclerView.ViewHolder onCreateHeaderViewHolder(ViewGroup parent);\\n\\n\\nvoid onBindHeaderViewHolder(RecyclerView.ViewHolder holder, int adapterPosition);\\n\\n\\nExample:\\n@Override\\npublic long getHeaderId(int position) {\\n    //This example takes the elements 0-9 as a group, the 10-19 elements as a group, and so on, with every 10 elements belonging to the same group.\\n    //For actual use, you can base on position and the data got from List.get(position) to judge the fields inside, to decide which elements belong to which groups.\\n    return position / 10;\\n}\\n\\n@Override\\npublic HeaderVH onCreateHeaderViewHolder(ViewGroup parent) {\\n    return new HeaderVH(LayoutInflater.from(parent.getContext()).inflate(R.layout.item_header, parent, false));\\n}\\n\\n@Override\\npublic void onBindHeaderViewHolder(HeaderVH holder, int position) {\\n    holder.tvHeader.setText(\"Group \" + getHeaderId(position) /* The group the current header is in. */ +\\n        \": Adapter Position \" + String.valueOf(position + 1) + \" - \" + String.valueOf(position + 10));\\n}\\n\\nPlease refer to PinnedFragment and PinnedAdapter in the demo.\\nGroup display\\nType Group[Title, ChildItem] is required to represent a group. Each group contains one title and a number of subitems.\\n<Title, ChildItem> Group<Title, ChildItem> {\\n    Title title;  //Title of one group\\n    List<ChildItem> childItemList;  //Subitem list under one group\\n}\\n\\nTherefore, you need to convert the data to List[Group[Title, ChildItem]], that is, the list of groups.\\nYou need to subclass GroupAdapter, and override the following 4 methods:\\nViewHolder onTitleVHCreate(ViewGroup parent);\\n\\nViewHolder onChildItemVHCreate(ViewGroup parent);\\n\\n/**\\n* @param adapterPos The absolute position of the Title in the Adapter ( = holder.getAdapterPosition()).\\n* @param titleOrderInAllTitles The relative position of the Group of the current Title in List[Group].\\n*/\\nvoid onTitleVHBind(ViewHolder holder, int adapterPos, Title title, int titleOrderInAllTitles);\\n\\n/**\\n* @param adapterPos The absolute position of the ChildItem in the Adapter ( = holder.getAdapterPosition()).\\n* @param titleOrderInAllTitles The relative position of the Group of the current ChildItem in List[Group].\\n* @param childOrderInCurrentGroup The relative position of the current ChildItem in the Group.\\n* (The childOrder of the first ChildItem is 0, that is, the childOrder does not include the position of the Title.)\\n*/\\nvoid onChildItemVHBind(ViewHolder holder, int adapterPos, Title title,\\n    int titleOrderInAllTitles, ChildItem childItem, int childOrderInCurrentGroup);\\n\\nThe data List[Group] is passed in through the GroupAdapter.setList(List[Group] groupList) method.\\nIf you know the absolute position of the Title or ChildItem in the Adapter, and you need to get the Title object, the relative position of the Group of the current Title in List[Group], the ChildItem object and the relative position of the current ChildItem in the Group, You can use 2 methods below on the GroupAdapter:\\n/**\\n* According to the absolute position of the Title in the Adapter,\\n* get the Title object and the relative position of the Group of the current Title in List[Group].\\n* @param positionInRV_viewType_title The absolute position of the Title in the Adapter，\\n* @return TitleChildItemBean {Title title;  int titleOrder;}\\n*/\\nTitleChildItemBean<Title, Void> getTitleWithOrder(int positionInRV_viewType_title)\\n\\n/**\\n* According to the absolute position of the ChildItem in the Adapter,\\n* get the Title object, the relative position of the Group of the current Title in List[Group],\\n* the ChildItem object and the relative position of the current ChildItem in the Group.\\n* @param positionInRV_viewType_childItem The absolute position of the ChildItem in the Adapter.\\n* @return TitleChildItemBean {Title title;  int titleOrder;\\n    ChildItem childItem;  int childOrder;}\\n*/\\nTitleChildItemBean<Title, ChildItem> getTitleAndChildItem(int positionInRV_viewType_childItem)\\n\\nSet OnGroupItemClickListener and OnGroupItemLongClickListener:\\nGroupAdapter.setOnGroupItemClickListener(OnGroupItemClickListener l);\\n\\nvoid onGroupItemClick(ViewHolder holder, View v, int adapterPos, int viewType,\\n   Title title, int titleOrder, ChildItem childItem, int childOrder);\\n\\nGroupAdapter.setOnGroupItemLongClickListener(OnGroupItemLongClickListener l);\\n\\nboolean onGroupItemLongClick(ViewHolder holder, View v, int adapterPos, int viewType,\\n   Title title, int titleOrder, ChildItem childItem, int childOrder);\\n\\nAs with the SingleViewTypeAdapter, the GroupAdapter provides a set of methods for easily manipulating data sets held by the Adapter:\\n- void setList(List<Group<Title, ChildItem>> groupList);\\n- void add(Group<Title, ChildItem> group);\\n- void add(int position, Group<Title, ChildItem> group);\\n- void remove(int position);\\n- void removeLast(); / void remove();\\n- void removeAll(int positionStart, int itemCount);\\n- void set(int position, Group<Title, ChildItem> group);\\n- void setAll(int positionStart, int itemCount, Group group);\\n- void addAll(int position, List<Group> newGroupList);\\n- void addAll(List<Group<Title, ChildItem>> newGroupList);\\n\\nPlease refer to the codes and comments of GroupAdapter. There is also an example usage in GroupFragment and GroupRVAdapter of the demo.\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '132',\n",
       "  'forks': '42',\n",
       "  'commits': '102'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Morphos\\n\\n\\n\\n\\n They say a pic is worth a 1000 words. Is it true to admit a .gif is worth a 1000 pics? \\n  \\nInclude in your project\\n In your root/build.gradle\\nallprojects {\\n  repositories {\\n  ...\\n  maven { url \\'https://jitpack.io\\' }\\n  }\\n}  \\n In your app/build.gradle\\ndependencies {\\n  compile \\'com.github.rjsvieira:morphos:1.0.0\\'\\n}\\nIntroducing Morphos : an animation wrapper.\\nMorphos will take care of your views\\' animations without you having to write all that boring boilerplate code.\\nInitialization\\nMorphos are easy to interact with. Go ahead and create the following simple Morpho :\\nView viewToAnimate = ... ;\\nMorpho morph = new Morpho(viewToAnimate)\\n  .translate(50,50,0,1500) // will translate the view (x=50,y=50,z=0) in 1500 milliseconds, \\n  .rotationXY(45,45,2000); // will rotate the view by 45 degrees on both the X-axis and Y-axis in 2000 milliseconds\\nYou can then animate it by doing\\nmorph.animate(); \\nWhich will then animate the desired Morphos using the default animation type (SEQUENTIAL).\\nWhat if I want to reverse the animation? Sure, just do :\\nmorph.reverse();\\n Configuration \\nCreate a Morpho\\nMorpho morphoOne = new Morphos(view);\\nConfigure the Morphos\\' animations\\nAs of the first release, Morphos supports the 7 most basic and common animations.\\nSince Morphos has plenty of combinations for interpolation, animation type, duration, etc, the user is allowed to configure them according to his needs.\\nEvery animation configuration method returns the Morphos object itself, thus allowing the user to chain his preferred animations.\\nNote : If the user does not specify the duration and/or interpolator, the animation will assume a 0 second duration and the default interpolator.\\nalpha(double value)\\nalpha(double value, int duration)\\nalpha(double value, int duration, Interpolator interpolator)\\n\\nscale(double valueX, double valueY)\\nscale(double valueX, double valueY, int duration)\\nscale(double valueX, double valueY, int duration, Interpolator interpolator)\\n\\ntranslationX(AnimationTarget target, float valueX)\\ntranslationX(AnimationTarget target, float valueX, int duration)\\ntranslationX(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\\n\\ntranslationY(AnimationTarget target, float valueX)\\ntranslationY(AnimationTarget target, float valueX, int duration)\\ntranslationY(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\\n\\ntranslationZ(AnimationTarget target, float valueX)\\ntranslationZ(AnimationTarget target, float valueX, int duration)\\ntranslationZ(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\\n\\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ)\\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ, int duration)\\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ, int duration, Interpolator interpolator)\\n\\ndimensions(float width, float height)\\ndimensions(float width, float height, int duration)\\ndimensions(float width, float height, int duration, Interpolator interpolator)\\n\\nrotationXY(AnimationTarget target, double degreesX, double degreesY)\\nrotationXY(AnimationTarget target, double degreesX, double degreesY, int duration)\\nrotationXY(AnimationTarget target, double degreesX, double degreesY, int duration, Interpolator interpolator)\\n\\nrotation(AnimationTarget target, double degrees)\\nrotation(AnimationTarget target, double degrees, int duration)\\nrotation(AnimationTarget target, double degrees, int duration, Interpolator interpolator)\\nSet a Listener to track the Morphos\\' animation process\\nmorphoOne.setListener(new Animator.AnimatorListener() {\\n  @Override\\n  public void onAnimationStart(Animator animator) {\\n      System.out.println(\"Start\");\\n  }\\n\\n  @Override\\n  public void onAnimationEnd(Animator animator) {\\n      System.out.println(\"End\");\\n  }\\n\\n  @Override\\n  public void onAnimationCancel(Animator animator) {\\n\\n  }\\n\\n  @Override\\n  public void onAnimationRepeat(Animator animator) {\\n\\n  }\\n})  \\n Animate() the Morpho\\nThis can be done through one of the several methods created just to make the invocation easier.\\nThe animate() method\\'s default values are : \\n\\nAnimationType : SEQUENTIAL - All animations are executed one sequentially\\nDuration : -1 - Since no duration was specified, -1 is assumed thus executing every animation in its given duration. For example : morphoOne.translate(100,0,0,3000).scale(2,2,2000).animate() will execute the translation animation in 3 seconds, followed by an upscale animation of 2 seconds\\nInterpolator - The overall interpolator (LinearInterpolator)\\nanimate()\\nanimate(AnimationType type, int duration)\\nanimate(AnimationType type, int duration, Interpolator interpolator)\\nReverse\\nAfter executing animate(), the user can rollback the animation by invoking the reverse() method.\\nThe reverse method works just like the animate() method, having the same combinations and parameters.\\nreverse()\\nreverse(AnimationType type, int duration)\\nreverse(AnimationType type, int duration, Interpolator interpolator)\\nCancel\\nIf the user wishes to cancel the on-going animation by any reason, he can do so by invoking the cancel method :\\nmorphoOne.cancel();\\nupdateView\\nLike the method explicitly indicates, the user can update the view associated with the Morpho. This will clear every animation already configured for the given object.\\nupdateView(View v) also invokes reset();\\nmorphoOne.updateView(newView);\\nReset\\nIf by any chance the user wants to reset he Morpho and re-build it from scratch, he can do so by invoking the reset() method\\nmorphoOne.reset()\\nDispose\\nLast but not least, if they user wishes to discard the Morphos object, he can invoke the dispose() method, thus clearing and preparing the Morphos\\' inner variables for garbage collection.\\nmorphoOne.dispose();\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '131',\n",
       "  'forks': '14',\n",
       "  'commits': '10'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'This is library project with a custom view that implements concept of Submit Button (https://dribbble.com/shots/1426764-Submit-Button?list=likes&offset=3) made by Colin Garven.\\n###Demo###\\n\\n###Usage###\\n <com.tuesda.submit.SubmitView\\n        android:layout_centerInParent=\"true\"\\n        android:id=\"@+id/submit\"\\n        android:layout_width=\"200dp\"\\n        android:layout_height=\"200dp\" />\\nmSubmit.setOnProgressStart(new SubmitView.OnProgressStart() {\\n            @Override\\n            public void progressStart() {\\n                // do something when progress start\\n            }\\n        });\\n        \\n        mSubmit.setOnProgressDone(new SubmitView.OnProgressDone() {\\n            @Override\\n            public void progressDone() {\\n                // do something when progress is done\\n            }\\n        });\\n###public interface###\\n\\n\\n\\n函数名\\n作用\\n\\n\\n\\n\\nsetBackColor(int color)\\n设置图标背景色，默认是绿色(0xff00cd97)，上图Demo设置为蓝色(0xff0097cd)\\n\\n\\nsetText(String str)\\n设置按钮名字，默认是Submit\\n\\n\\nreset()\\n将按钮重置到初始状态\\n\\n\\nsetProgress(float progress)\\n设置正在执行工作的执行进程\\n\\n\\nisProgressDone()\\n正在执行工作是否完成\\n\\n\\nsetOnProgressStart(OnProgressStart listener)\\n设置progress开始回调\\n\\n\\nsetOnProgressDone(OnProgressDone listener)\\n设置progress完成回调\\n\\n\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '130',\n",
       "  'forks': '21',\n",
       "  'commits': '7'},\n",
       " {'language': 'Java 99.8',\n",
       "  'readme': 'OpenTripPlanner for Android  \\nAn Android app for multi-modal trip planning and navigation using any OpenTripPlanner server.\\nDownload the app via the Google Play store.\\n\\nSee more details on the wiki.\\nLike hacking things?  See our Developer Guide to get started.\\nBuild Setup\\nPrerequisites for both Android Studio and Gradle\\n\\nDownload and install the Android SDK.  Make sure to install the Google APIs for your API level (e.g., 17), the Android SDK Build-tools version for your buildToolsVersion version, and the Android Support Repository and Google Repository.\\nSet the \"ANDROID_HOME\" environmental variable to your Android SDK location.\\nSet the \"JAVA_HOME\" environmental variables to point to your JDK folder (e.g., \"C:\\\\Program Files\\\\Java\\\\jdk1.6.0_27\")\\n\\nBuilding in Android Studio\\n\\nDownload and install the latest version of Android Studio.\\nIn Android Studio, choose \"Import Project\" at the welcome screen.\\nBrowse to the location of the project, and double-click on the project directory.\\nIf prompted with options, check \"Use auto-import\", and select \"Use default gradle wrapper (recommended)\".  Click \"Ok\".\\nClick the green play button (or \\'Shift->F10\\') to run the project!\\n\\nBuilding from the command line using Gradle\\n\\nTo build and push the app to the device, run gradlew installDebug from the command line at the root of the project\\nTo start the app, run adb shell am start -n edu.usf.cutr.opentripplanner.android/.MyActivity (alternately, you can manually start the app)\\n\\nRelease builds\\nTo build a release build, you need to create a \"gradle.properties\" file that points to a \"secure.properties\" file, and a \"secure.properties\" file that points to your keystore and alias. The gradlew assembleRelease command will prompt for your keystore passphrase.\\nThe \"gradle.properties\" file is located in the opentripplanner-android directory and has the contents:\\nsecure.properties=<full_path_to_secure_properties_file>\\n\\nThe \"secure.properties\" file (in the location specified in gradle.properties) has the contents:\\nkey.store=<full_path_to_keystore_file>\\n\\nkey.alias=<key_alias_name>\\n\\nNote that the paths in these files always use the Unix path separator  /, even on Windows. If you use the Windows path separator \\\\ you will get the error No value has been specified for property \\'signingConfig.keyAlias\\'.\\nContributing\\nWe welcome contributions to the project!  Please see our Contributing Guide for details, including Code Style Guidelines and Template.\\nTroubleshooting\\nWhen importing to Android Studio, I get an error \"You are using an old, unsupported version of Gradle...\"\\nIf you\\'re using Android Studio v0.4.2 or lower, when importing, please be sure to select the \"settings.gradle\" file in the root, NOT the project directory.\\nYou will get the above error if you select the project directory / name of the project.\\nI get build errors for the Android Support libraries or Google APIs\\nOpen Android SDK Manager, and under the \"Extras\" category make sure you\\'ve installed both the \"Android Support Repository\" (in addition to the \"Android Support library\") as well as the\\n\"Google Repository\".  Also, make sure you have the Google API installed for the API level that you\\'re working with in the \"/build.gradle\" file,\\nincluding the \"Android SDK Build-tools\" version (at the top of the \"Tools\" category in the Android SDK Manager) that\\nmatches the compileSdkVersion and buildToolsVersion numbers in /opentripplanner-android/build.gradle.\\nI get the import gradle project error - “Cause: unexpected end of block data”\\nMake sure you have the Google API installed for the API level that you\\'re working with in the /build.gradle file,\\nincluding the \"Android SDK Build-tools\" version (at the top of the \"Tools\" category in the Android SDK Manager) that\\nmatches the compileSdkVersion and buildToolsVersion numbers in /opentripplanner-android/build.gradle.\\nAndroid Studio or Gradle can\\'t find my Android SDK, or the API Levels that I have installed\\nMake sure that you\\'re consistently using the same Android SDK throughout Android Studio and your environmental variables.\\nAndroid Studio comes bundled with an Android SDK, and can get confused if you\\'re pointing to this SDK within Android Studio\\nbut have your environmental variables pointed elsewhere.  Click \"File->Project Structure\", and then under \"Android SDK\"\\nmake sure you \"Android SDK Location\" is the correct location of your Android SDK.\\nAlso, make sure you\\'ve set the \"ANDROID_HOME\" environmental variable to your Android SDK location and\\nthe \"JAVA_HOME\" environmental variables to point to your JDK folder.\\nOpenTripPlanner Project\\nWant to learn more about the main OpenTripPlanner project? Read up here:\\nhttp://opentripplanner.org\\n',\n",
       "  'watchers': '26',\n",
       "  'stars': '125',\n",
       "  'forks': '91',\n",
       "  'commits': '803'},\n",
       " {'language': 'Java 88.2',\n",
       "  'readme': \"SugaredListAnimations\\nSugaredListAnimations is a library that pretends to add animations to your listview with minor changes to your already existing code.\\nCurrently the available animations are:\\n\\nGoogle Plus alike\\nGoogle Now alike\\n\\nVersion\\n0.9 - Yeah, it's still beta\\nDemo\\nSee demo here\\nLicense\\nMIT\\nFree Software, Fuck Yeah!\\n\",\n",
       "  'watchers': '14',\n",
       "  'stars': '124',\n",
       "  'forks': '42',\n",
       "  'commits': '2'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'ShareSDK for Android\\n\\nwebsite -- http://www.mob.com\\nwiki -- http://wiki.mob.com/Android_%E5%BF%AB%E9%80%9F%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97\\nbbs -- http://bbs.mob.com/forum-36-1.html\\n\\nStep One: Download the SDK\\nVisit our official website and download the latest version of ShareSDK. After extracting the downloaded file, you will find the following directory structure：\\n\\nOpen the ShareSDK for Android directory, you will find MainLibs and OnekeyShare. ShareSDK is stored in the MainLibs directory, and OnekeyShare is a GUI tool for developers to quickly complete the share feature by ShareSDK.\\nStep Two: Import ShareSDK to Your Project\\nThere are two ways to import ShareSDK into your project: reference to the ShareSDK project or copy the jars and resources into your project. If you select the second way, we provide the following tool to help you quickly finish these operations:\\n\\nExecute this tool and copy its products into your project.\\nShareSDK encourage you integrate ShareSDK by referencing its project, because this will be much simpler. Here are the steps:\\n(1) Copy the extracted SDK into your workspace of Eclipse\\n(2) Import the SDK projects:\\n\\nSelect MainLibs and OnekeyShare\\n\\n(3) Change dependency of your project to OnekeyShare (if you need this GUI tool) or MainLibs\\n\\nStep Three: Add Applications Information\\nThere are three ways to add your applications information into ShareSDK: register on the application console of ShareSDK, configurate the “assets/ShareSDK.xml” file, or modify by ShareSDK.setPlatformDevInfo(String, HashMap<String, Object>) method at runtime.\\nHere is the example of “assets/ShareSDK.xml” way:\\n<ShareSDK\\n   AppKey=\"add appkey you got from SahreSDK here\" />\\n\\n<Facebook\\n    Id=\"int field, custom value for developer to recognize this platform\"\\n    SortId=\"int field, the priority in the registered platforms\"\\n    ConsumerKey=\"consumer key you got from Facebook\"\\n    ConsumerSecret=\"consumer secret you got from Facebook\"\\n    Enable=\"Boolean field, false means to remove the platform from the registered platforms\" />\\n```\\n\\nAll applications information is registered in the “assets/ShareSDK.xml” of ShareSDK Sample project.\\n\\n# Step Four: Configurate AndroidManifest.xml\\n\\nAdd the following permissions into your AndroidMenifest.xml:\\n\\n```` xml\\n<uses-permission android:name=\"android.permission.GET_TASKS\" />\\n<uses-permission android:name=\"android.permission.INTERNET\" />\\n<uses-permission android:name=\"android.permission.ACCESS_WIFI_STATE\" />\\n<uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" />\\n<uses-permission android:name=\"android.permission.CHANGE_WIFI_STATE\" />\\n<uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" />\\n<uses-permission android:name=\"android.permission.READ_PHONE_STATE\" />\\n<uses-permission android:name=\"android.permission.MANAGE_ACCOUNTS\"/>\\n<uses-permission android:name=\"android.permission.GET_ACCOUNTS\"/>\\n```\\nYou should add the intent-filter in your  launchActivity if you want to use the KakaoTalk to share msg.\\n<!--\\n\\tIf you share msg in KakaoTalk, your share-params of executeUrl should set the value \"kakaoTalkTest://starActivity\"\\n\\tSo it do, when the user to click the share-msg, then startActivity of your app\\'s launch-activity. \\n\\tWhen you use the lib of onekeyshare, you can use the method of \\n    setExecuteUrl(\"kakaoTalkTest://starActivity\") to set executeUrl.\\n-->\\n    <intent-filter>\\n        <data android:scheme=\"kakaoTalkTest\" android:host=\"starActivity\"/>\\n        <action android:name=\"android.intent.action.VIEW\" />\\n        <category android:name=\"android.intent.category.BROWSABLE\" />\\n        <category android:name=\"android.intent.category.DEFAULT\" />\\n    </intent-filter>\\n```\\n\\t\\t\\nAnd the single Activity for GUIs of ShareSDK:\\n\\n```` xml\\n<activity\\n   android:name=\"cn.sharesdk.framework.ShareSDKUIShell\"\\n   android:theme=\"@android:style/Theme.Translucent.NoTitleBar\"\\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\\n   android:screenOrientation=\"portrait\"\\n   android:windowSoftInputMode=\"stateHidden|adjustResize\" />\\n```\\n\\nIf you integrate Wechat, add this callback activity:\\n\\n```` xml\\n<activity\\n   android:name=\".wxapi.WXEntryActivity\"\\n   android:theme=\"@android:style/Theme.Translucent.NoTitleBar\"\\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\\n   android:exported=\"true\"\\n   android:screenOrientation=\"portrait\" />\\n```\\n\\nAnd if you integrate Yixin, add this callback activity:\\n\\n```` xml\\n<activity\\n   android:name=\".yxapi.YXEntryActivity\"\\n   android:theme=\"@android:style/Theme.Translucent.NoTitleBar\"\\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\\n   android:exported=\"true\"\\n   android:screenOrientation=\"portrait\" />\\n```\\n\\n# Step Five: Add Codes\\n\\nAdd the following line in the **onCreate** method of **the entrance activity**:\\n\\n````java\\nShareSDK.initSDK(this);\\n```\\n\\nAnd add the following line int the **onDestroy** method of **the last activity**:\\n\\n````java\\nShareSDK.stopSDK(this);\\n```\\n\\n# Screenshots\\n![logo grid view of onekeyshare](http://wiki.sharesdk.cn/images/thumb/a/ad/p4.png/337px-p4.png)\\n![edit page of onekeyshare](http://wiki.sharesdk.cn/images/thumb/b/b1/p5.png/337px-p5.png)\\n![image preview](http://wiki.sharesdk.cn/images/thumb/8/88/p6.png/337px-p6.png)\\n![authorizes](http://wiki.sharesdk.cn/images/thumb/6/69/p7.png/337px-p7.png)\\n\\n# And the Next\\n\\nFor more information about how to integrate ShareSDK or how use ShareSDK to get your friends list, following someone, share statuses, etc. please visit our [official wiki](http://wiki.sharesdk.cn/Android_%E5%BF%AB%E9%80%9F%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97).\\n',\n",
       "  'watchers': '29',\n",
       "  'stars': '123',\n",
       "  'forks': '94',\n",
       "  'commits': '10'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'ShareSDK for Android\\n\\nwebsite -- http://www.mob.com\\nwiki -- http://wiki.mob.com/Android_%E5%BF%AB%E9%80%9F%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97\\nbbs -- http://bbs.mob.com/forum-36-1.html\\n\\nStep One: Download the SDK\\nVisit our official website and download the latest version of ShareSDK. After extracting the downloaded file, you will find the following directory structure：\\n\\nOpen the ShareSDK for Android directory, you will find MainLibs and OnekeyShare. ShareSDK is stored in the MainLibs directory, and OnekeyShare is a GUI tool for developers to quickly complete the share feature by ShareSDK.\\nStep Two: Import ShareSDK to Your Project\\nThere are two ways to import ShareSDK into your project: reference to the ShareSDK project or copy the jars and resources into your project. If you select the second way, we provide the following tool to help you quickly finish these operations:\\n\\nExecute this tool and copy its products into your project.\\nShareSDK encourage you integrate ShareSDK by referencing its project, because this will be much simpler. Here are the steps:\\n(1) Copy the extracted SDK into your workspace of Eclipse\\n(2) Import the SDK projects:\\n\\nSelect MainLibs and OnekeyShare\\n\\n(3) Change dependency of your project to OnekeyShare (if you need this GUI tool) or MainLibs\\n\\nStep Three: Add Applications Information\\nThere are three ways to add your applications information into ShareSDK: register on the application console of ShareSDK, configurate the “assets/ShareSDK.xml” file, or modify by ShareSDK.setPlatformDevInfo(String, HashMap<String, Object>) method at runtime.\\nHere is the example of “assets/ShareSDK.xml” way:\\n<ShareSDK\\n   AppKey=\"add appkey you got from SahreSDK here\" />\\n\\n<Facebook\\n    Id=\"int field, custom value for developer to recognize this platform\"\\n    SortId=\"int field, the priority in the registered platforms\"\\n    ConsumerKey=\"consumer key you got from Facebook\"\\n    ConsumerSecret=\"consumer secret you got from Facebook\"\\n    Enable=\"Boolean field, false means to remove the platform from the registered platforms\" />\\n```\\n\\nAll applications information is registered in the “assets/ShareSDK.xml” of ShareSDK Sample project.\\n\\n# Step Four: Configurate AndroidManifest.xml\\n\\nAdd the following permissions into your AndroidMenifest.xml:\\n\\n```` xml\\n<uses-permission android:name=\"android.permission.GET_TASKS\" />\\n<uses-permission android:name=\"android.permission.INTERNET\" />\\n<uses-permission android:name=\"android.permission.ACCESS_WIFI_STATE\" />\\n<uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" />\\n<uses-permission android:name=\"android.permission.CHANGE_WIFI_STATE\" />\\n<uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" />\\n<uses-permission android:name=\"android.permission.READ_PHONE_STATE\" />\\n<uses-permission android:name=\"android.permission.MANAGE_ACCOUNTS\"/>\\n<uses-permission android:name=\"android.permission.GET_ACCOUNTS\"/>\\n```\\nYou should add the intent-filter in your  launchActivity if you want to use the KakaoTalk to share msg.\\n<!--\\n\\tIf you share msg in KakaoTalk, your share-params of executeUrl should set the value \"kakaoTalkTest://starActivity\"\\n\\tSo it do, when the user to click the share-msg, then startActivity of your app\\'s launch-activity. \\n\\tWhen you use the lib of onekeyshare, you can use the method of \\n    setExecuteUrl(\"kakaoTalkTest://starActivity\") to set executeUrl.\\n-->\\n    <intent-filter>\\n        <data android:scheme=\"kakaoTalkTest\" android:host=\"starActivity\"/>\\n        <action android:name=\"android.intent.action.VIEW\" />\\n        <category android:name=\"android.intent.category.BROWSABLE\" />\\n        <category android:name=\"android.intent.category.DEFAULT\" />\\n    </intent-filter>\\n```\\n\\t\\t\\nAnd the single Activity for GUIs of ShareSDK:\\n\\n```` xml\\n<activity\\n   android:name=\"cn.sharesdk.framework.ShareSDKUIShell\"\\n   android:theme=\"@android:style/Theme.Translucent.NoTitleBar\"\\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\\n   android:screenOrientation=\"portrait\"\\n   android:windowSoftInputMode=\"stateHidden|adjustResize\" />\\n```\\n\\nIf you integrate Wechat, add this callback activity:\\n\\n```` xml\\n<activity\\n   android:name=\".wxapi.WXEntryActivity\"\\n   android:theme=\"@android:style/Theme.Translucent.NoTitleBar\"\\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\\n   android:exported=\"true\"\\n   android:screenOrientation=\"portrait\" />\\n```\\n\\nAnd if you integrate Yixin, add this callback activity:\\n\\n```` xml\\n<activity\\n   android:name=\".yxapi.YXEntryActivity\"\\n   android:theme=\"@android:style/Theme.Translucent.NoTitleBar\"\\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\\n   android:exported=\"true\"\\n   android:screenOrientation=\"portrait\" />\\n```\\n\\n# Step Five: Add Codes\\n\\nAdd the following line in the **onCreate** method of **the entrance activity**:\\n\\n````java\\nShareSDK.initSDK(this);\\n```\\n\\nAnd add the following line int the **onDestroy** method of **the last activity**:\\n\\n````java\\nShareSDK.stopSDK(this);\\n```\\n\\n# Screenshots\\n![logo grid view of onekeyshare](http://wiki.sharesdk.cn/images/thumb/a/ad/p4.png/337px-p4.png)\\n![edit page of onekeyshare](http://wiki.sharesdk.cn/images/thumb/b/b1/p5.png/337px-p5.png)\\n![image preview](http://wiki.sharesdk.cn/images/thumb/8/88/p6.png/337px-p6.png)\\n![authorizes](http://wiki.sharesdk.cn/images/thumb/6/69/p7.png/337px-p7.png)\\n\\n# And the Next\\n\\nFor more information about how to integrate ShareSDK or how use ShareSDK to get your friends list, following someone, share statuses, etc. please visit our [official wiki](http://wiki.sharesdk.cn/Android_%E5%BF%AB%E9%80%9F%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97).\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '123',\n",
       "  'forks': '10',\n",
       "  'commits': '31'},\n",
       " {'language': 'Java 98.5',\n",
       "  'readme': 'Compass\\nThis sample inserts a live card to the left of the Glass clock that displays a\\ncompass. Tapping the live card presents a menu with two options:\\n\\nRead aloud: read the compass\\'s current heading using text-to-speech\\nStop: remove the compass from the timeline\\n\\nThe compass also contains a small list of landmarks that will appear on the\\nscreen when the user is within 10 km of those locations. See the\\nres/raw/landmarks.json file to add your own.\\nGetting started\\nCheck out our documentation to learn how to get started on\\nhttps://developers.google.com/glass/gdk/index\\nRunning the sample on Glass\\nYou can use your IDE to compile and install the sample or use\\nadb\\non the command line:\\n$ adb install -r CompassSample.apk\\n\\nTo start the sample, say \"ok glass, show a compass\" from the Glass clock\\nscreen or use the touch menu.\\n',\n",
       "  'watchers': '32',\n",
       "  'stars': '122',\n",
       "  'forks': '100',\n",
       "  'commits': '9'},\n",
       " {'language': 'Java 45.1',\n",
       "  'readme': 'amap-running-app\\n\\nUse weex-amap to build a running app\\nHow to use\\n1.首先克隆这个项目(后面会写如何自己创建这样的项目). 确保你自己环境安装了weex-toolkit\\ngit clone https://github.com/weex-plugins/amap-running-app\\n2.进入克隆的项目目录，然后执行 npm install\\n3.测试你的需要运行的平台，比如android 或者 ios\\nweex platform add android\\n4.添加插件 weex-amap\\nweex plugin add weex-amap\\n这个时候你就可以运行命令看具体运行的效果了：\\nweex run android\\n如果你自己使用weex-toolkit创建项目，你只需要这样做：\\nweex create runningapp\\ncd runningapp && npm install\\nweex platform add android\\nweex plugin add weex-amap\\nweexpack run android\\n\\n运行demo截图\\niOS 版\\n\\nAndroid 版\\n\\n截图数据仅供演示\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '122',\n",
       "  'forks': '38',\n",
       "  'commits': '26'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'AndroidMVVM\\n\\nNote: issue the following in command line - gradlew build --refresh-dependencies in Windows or ./gradlew build --refresh-dependencies in Linux or Mac to force update to the latest RoboBinding snapshot when required.\\n\\n\\nA minimal Android app with MVVM pattern based on RoboBinding.\\n\\n\\nThe project can be directly imported into Android Studio.\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '120',\n",
       "  'forks': '41',\n",
       "  'commits': '17'},\n",
       " {'language': 'C++ 98.0',\n",
       "  'readme': 'dlib_for_arm\\nvery fast face detection for ARM platform.\\nThe code is based on dlib with the following enhancement\\n\\nreduce work load : only use 1 filter(front looking) instead of 5 filters in frontal_face_detector.h\\nthread level parallelism : use 3 threads to do face detection\\nSIMD : use arm neon to implement dlib/simd/\\n\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '85',\n",
       "  'forks': '27',\n",
       "  'commits': '2'},\n",
       " {'language': 'C++ 97.8',\n",
       "  'readme': 'injeqt\\nSimple dependency injection framework for Qt\\nDocumentation\\nDocumentation available at github.\\nExample\\nHere is example of what can be done using injeqt:\\n#include <injeqt/injector.h>\\n#include <injeqt/module.h>\\n\\n#include <QtCore/QObject>\\n#include <iostream>\\n#include <memory>\\n#include <string>\\n\\nclass hello_service : public QObject\\n{\\n\\tQ_OBJECT\\n\\npublic:\\n\\thello_service() {}\\n\\tvirtual ~hello_service() {}\\n\\n\\tstd::string say_hello() const\\n\\t{\\n\\t\\treturn {\"Hello\"};\\n\\t}\\n};\\n\\nclass world_service : public QObject\\n{\\n\\tQ_OBJECT\\n\\npublic:\\n\\tworld_service() {}\\n\\tvirtual ~world_service() {}\\n\\n\\tstd::string say_world() const\\n\\t{\\n\\t\\treturn {\"World\"};\\n\\t}\\n};\\n\\nclass hello_factory : public QObject\\n{\\n\\tQ_OBJECT\\n\\npublic:\\n\\tQ_INVOKABLE hello_factory() {}\\n\\tvirtual ~hello_factory() {}\\n\\n\\tQ_INVOKABLE hello_service * create_service()\\n\\t{\\n\\t\\treturn new hello_service{};\\n\\t}\\n};\\n\\nclass hello_client : public QObject\\n{\\n\\tQ_OBJECT\\n\\npublic:\\n\\tQ_INVOKABLE hello_client() : _s{nullptr}, _w{nullptr} {}\\n\\tvirtual ~hello_client() {}\\n\\n\\tstd::string say() const\\n\\t{\\n\\t\\treturn _s->say_hello() + \" \" + _w->say_world() + \"!\";\\n\\t}\\n\\nprivate slots:\\n\\tINJEQT_INIT void init()\\n\\t{\\n\\t\\tstd::cerr << \"all services set\" << std::endl;\\n\\t}\\n\\n\\tINJEQT_DONE void done()\\n\\t{\\n\\t\\tstd::cerr << \"ready for destruction\" << std::endl;\\n\\t}\\n\\n\\tINJEQT_SET void set_hello_service(hello_service *s)\\n\\t{\\n\\t\\t_s = s;\\n\\t}\\n\\n\\tINJEQT_SET void set_world_service(world_service *w)\\n\\t{\\n\\t\\t_w = w;\\n\\t}\\n\\nprivate:\\n\\thello_service *_s;\\n\\tworld_service *_w;\\n\\n};\\n\\nclass module : public injeqt::module\\n{\\npublic:\\n\\texplicit module()\\n\\t{\\n\\t\\t_w = std::unique_ptr<world_service>{new world_service{}};\\n\\n\\t\\tadd_type<hello_client>();\\n\\t\\tadd_type<hello_factory>();\\n\\t\\tadd_factory<hello_service, hello_factory>();\\n\\t\\tadd_ready_object<world_service>(_w.get());\\n\\t}\\n\\n\\tvirtual ~module() {}\\n\\nprivate:\\n\\tstd::unique_ptr<world_service> _w;\\n\\n};\\n\\nint main()\\n{\\n\\tauto modules = std::vector<std::unique_ptr<injeqt::module>>{};\\n\\tmodules.emplace_back(std::unique_ptr<injeqt::module>{new module{}});\\n\\n\\tauto injector = injeqt::injector{std::move(modules)};\\n\\tauto client = injector.get<hello_client>();\\n\\tauto hello = client->say();\\n\\n\\tstd::cout << hello << std::endl;\\n}\\n\\n#include \"hello-world.moc\"\\n\\nIn that example we can see two main services names hello_service and world_service. There\\nis also client of these names hello_client. In module class we configure how we create\\nand access these instances.\\nhello_client is added using add_type function. It means that injeqt will try to create it\\nusing default constructor. We provide that by declaration of Q_INVOKABLE hello_client()\\n(Q_INVOKABLE is requires by Qt\\'s meta object system).\\nhello_service is added using add_factory function. It means that injeqt will first try to\\ncreate a hello_factory object, then it will look for a method of that objet that returns\\nhello_service object. It will find Q_INVOKABLE hello_service * create_service() and use it.\\nTo be able to create hello_factory injeqt must know about it, so we also add it using add_type\\nmethod.\\nLast, world_service, is added as a ready object - provided from outside of injeqt scope.\\nIn main method list of conifguration modules are passed to newly created injector instance.\\nFrom that moment, we can use injector to create and manage our services. Just one line below\\nan hello_client instance is required. This is what happens next:\\n\\ninjeqt looks for dependencies of hello_client and found that it first needs to create hello_factory\\ninjeqt creates hello_factory without problems, as it does not have dependencies of its own\\ninjeqt adds new instance to object pool\\ninjeqt calls hello_factory::create_service() methods and adds its result to object pool\\nnow all dependencies of hello_client are available, so new instance of it is created with\\ndefault constructor and its added to objec tpool\\nall methods of hello_client marked with INJEQT_SET are called with proper objects from pool\\nall methods of hello_client marked with INJEQT_INIT are called\\nthis instance is returned to caller\\nbefore injector is destructed, all methods of hello_client marked with INJEQT_DONE are called\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '85',\n",
       "  'forks': '10',\n",
       "  'commits': '494'},\n",
       " {'language': 'C++ 98.4',\n",
       "  'readme': \"kinectable_pipe\\nkinectable_pipe is a command-line utility that dumps user skeleton data from a Microsoft Kinect device to a standard Unix pipe.\\nWhy?\\nTo bring Unix-y goodness to the world of Microsoft Kinect programming!\\nNo, really, why?\\nBecause Kinect programming is a pain in the neck, and by trivializing the device's output into a simple text format, it becomes infinitely easier to digest in the scripting language of your choice.\\nThis seems simple to the point of being almost useless\\nYes, that's the point. Do One Thing and Do It Well. There's an accompanying rubygem that will add all the smart stuff like advanced gesture recognition, events, etc.\\nUSAGE\\n% kinectable_pipe | ruby gesture_recognizer.rb | python play_light_show.py\\n\\nINSTALLATION (OS X / homebrew)\\n# must have universal binary for libusb\\nbrew uninstall libusb\\nbrew install libusb --universal\\n\\nbrew tap marshally/alt\\n\\nbrew install kinectable_pipe\\n\\n# now plug in your kinect\\n# and run this command in the terminal\\n\\nkinectable_pipe\\n\\n# step back from the sensor, wave your arms like a lunatic\\n# until it recognizes you and starts pumping out skeleton data\\n# to STDOUT\\n\\nOPTIONS\\n-r 15 # restrict output to 15fps, Kinect max is 30fps\\n\\nTODO\\n\\nLinux install instructions\\nRecognize all available xn::GestureGenerator\\nFix output to work with non-interactive terminal sessions.\\nImprove Ruby sample app.\\nAdd sample apps for other scripting languages.\\nAdd CLI argument for alternate output encodings (XML, msgpack, BERT, whatever).\\n\\n\",\n",
       "  'watchers': '2',\n",
       "  'stars': '84',\n",
       "  'forks': '7',\n",
       "  'commits': '31'},\n",
       " {'language': 'C++ 98.5',\n",
       "  'readme': '\\n\\nSingle file embedded C++ web server\\n===\\nHow to use in own project\\n#include <iostream>\\n#include <map>\\n\\n#include \"web++.hpp\"\\n\\nusing namespace WPP;\\n\\nvoid web(Request* req, Response* res) {\\n    std::cout << req->method << \" \" << req->path << std::endl;\\n\\n    std::cout << \"Headers:\" << std::endl;\\n\\n    std::map<std::string, std::string>::iterator iter;\\n    for (iter = req->headers.begin(); iter != req->headers.end(); ++iter) {\\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\\n    }\\n\\n    std::cout << \"Query:\" << std::endl;\\n\\n    for (iter = req->query.begin(); iter != req->query.end(); ++iter) {\\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\\n    }\\n\\n    std::cout << \"Cookies: \" << req->cookies.size() << std::endl;\\n\\n    for (iter = req->cookies.begin(); iter != req->cookies.end(); ++iter) {\\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\\n    }\\n\\n    res->body << \"HELLO\";\\n}\\n\\nint main(int argc, const char* argv[]) {\\n    try {\\n        std::cout << \"Listening on port 5000\" << std::endl;\\n\\n        WPP::Server server;\\n        server.get(\"/\", &web);\\n        server.all(\"/dir\", \"./\");\\n        server.start(5000);\\n    } catch(WPP::Exception e) {\\n        std::cerr << \"WebServer: \" << e.what() << std::endl;\\n    }\\n\\n    return EXIT_SUCCESS;\\n}\\nHow to compile\\ng++ demo.cpp -o demo\\n\\nSpecial requirements\\nNop\\nTested on\\n\\nMac OS X\\nLinux\\n\\nThe MIT License\\nCopyright (c) Alex Movsisyan\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \\'Software\\'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \\'AS IS\\', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '84',\n",
       "  'forks': '29',\n",
       "  'commits': '35'},\n",
       " {'language': 'C++ 100.0',\n",
       "  'readme': '#华软网络安全小组逆向工程训练营\\n###SNST Reverse Engineering Traning\\n各个训练程序解析\\n\\n###1.RE-50 Writeup\\n把代码导入到IDA ,用Hex-ray 把Main 函数汇编转到伪C 代码,结果如下\\n\\n程序代码意思是:先给本地的数组的每个位置赋值,然后接收我们输入的字符串,再通过字符串对比函数来对比输入的字符串是否和本地校验的字符串一样,于是在strcmp 处下断点,位置如下\\n\\n启动Ollydbg ,随意输入字符串\\n\\nOllydbg 会卡在0x40109D 这个地方(下断点的快捷键是F2 )\\n\\n查看寄存器窗口,可以看到flag\\n\\n\\n###2.RE-100 Writeup\\n同样是转换到伪C 代码分析\\n\\n程序原理:判断输入的字符串长度是否为19 ,符合的话把输入的字符串里面的每个字符的值加50 和byte_408030 里面的数据进行对比,于是我们把进去byte_408030 ,设置它的Array (数组)长度\\n\\n然后按几下d 键把这个串转换成char\\n\\n提取这些数据出来,拿到python 里面做逆运算(因为程序是把我们输入的数据加上50 再和这些数据做比较的,所以我们把这些数据减50 就可以获取到原来的字符串了)\\ncode=[0x9A,0xA4,0x95,0xA4,0x98,0xAD,0x84,0x77,0x63,0x62,0x62,0x91,0x75,0x93,0x9E,0x95,0xA7,0xAF]\\n\\nfor index in code :\\n    print chr(index-50),\\n\\n\\n\\n###3.RE-250 Writeup\\n在main 函数的入口处和之前所见的有点不同,我们直接关注jmp loc_401113\\n\\n跳到jmp loc_401113 中分析代码,可以看到这里有SEH 异常处理,异常回调的地址是0x401053\\n\\n往下分析,程序会执行到0x40115A ,这是一个无效的指令,于是会印发SEH 异常处理,程序跳转到0x401053\\n\\n于是找到0x401053 ,分析\\n\\n可以看到这里的对比运算:输入的字符串和byte_40B938 里面的数据做异或运算之后再对比两个字符是否相同,异或运算的key 是80 加上当前对比的偏移位置,于是可以写出解密python\\ncode=[0x38,0x23,0x31,0x27,0x32,0x2E,0x4,0x32,0x7,0x6B,0x6F,0x6B,0x3,0x25,0x31,0x2D,0x1D]\\nxor_number=80\\n\\nfor index in code :\\n    print chr(index^xor_number),\\n    xor_number+=1\\n\\n\\n\\n###4.RE-500 Writeup\\n第一步首先要绕过IDA 本身的bug ,因为IDA 默认是从第一区块开始解析数据的,但是在入口点中的跳转却是在第一区块之前的,所以IDA 无法从这个位置中获取数据,于是使用OllyDbg 跟踪\\n\\n跟踪到0x401041 处,继续往下单步\\n\\n可以看到这又是SEH 异常处理,直接跳到0x404100\\n\\n来到0x404100 ,先给它创建函数\\n\\n创建函数之后的变化\\n\\n然后用Hex-ray转换\\n\\n进去到0x401014\\n\\n切换回汇编窗口查看代码\\n\\n继续跳到0x40199E 去分析,这里创建一个线程,线程入口地址是0x401023 ,于是我们过去分析这个函数的功能\\n\\n发现这里一直死循环调用两个函数\\n\\nsub_40102D 的代码如下(为什么会显示sub_401400 呢?因为代码是从sub_40102D 直接jmp 到sub_401400 的):从PEB 中获取程序是否被调试\\n\\nSub_401028 的功能是直接调用IsDebuggerPresent() 获取是否被调试\\n\\n转换到汇编,可以发现,每次检测到程序被调试的话就会除0 ,产生异常退出程序,于是这个线程是用来反调试的\\n\\n回去主线代码继续分析,遇到代码混淆\\n\\n先到0x4019C8 中用c 键把代码转换成二进制数据\\n\\n再到0x4019C9 中用d 键把二进指数据转换成代码,转换之后的代码如下:\\n\\n继续往下分析,这里计算函数的真实地址,具体作用还不知道\\n\\n继续往下,发现程序又创建新线程\\n\\n于是我们查看线程里面的代码,这里是要让我们输入flag\\n\\n再往下,线程自己就关闭了\\n\\n主线代码里面在创建线程成功之后就阻塞等待它关闭,如果创建失败的话就关闭程序\\n\\n继续往下,发现又有新线程…\\n\\n在线程执行的代码最后有一处混淆\\n\\n因为Push EAX + Retn 等价于Jmp Eax ,eax 的值是ecx+40 (sub -40 等于add 40),ecx 的值是这个函数的入口地址,于是可以计算出接下来程序运行到这里的将会跳转到下面这个地址\\n0x401610+0x40=0x401650\\n跳转到此\\n\\nC 键转换代码\\n\\n然后又是过混淆,相信大家已经明白啦\\n\\n下面遇到switch 语句,先来分析下每个cases 的意义\\n\\nCase 0 是检测程序是否被调试\\n\\nCase 1 往缓冲区中写ERROR\\n\\nCase 2 输出刚才的缓冲区\\n\\nCase 3 把OK 写入缓冲区\\n\\nCase 4 是flag 对比,然后我们把精力主要集中在此\\n\\n这里调用了两个未知的函数,先忽略他们,继续往下分析\\n\\n发现下面有一段数据\\n\\n然后把这段数据和另一个缓冲区做对比\\n\\n往上寻找EBP-0x28 ,发现刚才忽略的函数有利用到这两个缓冲区\\n\\n在分析sub_401019 中发现,这个应该是加密函数\\n\\n函数的参数有两个,于是回去看看到底程序传哪两个参数进去\\n\\n可以看到,一个是0x10 ,一个是缓冲区地址,所以我们可以大胆的确定,a1 是要加密的数据地址,a2 是数据长度\\n\\n给加密函数中的变量写好命名,整体的加密流程也就一看明了\\n\\n于是获取到这段加密过后的字符串\\n\\n放到python 里面解码\\ndef decode(string) :\\n    for index in range(0,16,2) :\\n        string[index]=string[index]^16\\n        string[index+1]=string[index+1]^16\\n        bit_1_high=string[index]&0xF\\n        bit_2_low=(string[index]&0xF0)>>4\\n        bit_2_high=string[index+1]&0xF\\n        bit_1_low=(string[index+1]&0xF0)>>4\\n        source_bit_1=bit_1_high*16+bit_1_low\\n        source_bit_2=bit_2_high*16+bit_2_low\\n        print chr(source_bit_1),chr(source_bit_2),\\n    \\ncode=[0xE7,0x86,0xE7,0x45,0x06,0x26,0xE6,0xF5,0x06,0xC6,0x46,0xA6,0x05,0xE6,0xD6,0xD6]\\ndecode(code)\\n\\n\\n合起来就是hrctf:{you_can_make_all}\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '84',\n",
       "  'forks': '34',\n",
       "  'commits': '5'},\n",
       " {'language': 'C++ 50.5',\n",
       "  'readme': 'WebKit\\nWebKit is an open source web browser engine. QtWebKit is the Qt Port of WebKit.\\nThis code is based on the source found in webkit.org but it contains modifications introduced\\nby the isis-project.\\nFor more information see:\\nhttp://www.webkit.org\\nhttp://developer.qt.nokia.com/wiki/QtWebKit\\nhttp://en.wikipedia.org/wiki/Webkit\\nhttp://isis-project.org/\\nLicense\\nSome parts of WebKit are available under the GNU Lesser General Public License and others under\\na BSD-style license.\\n',\n",
       "  'watchers': '32',\n",
       "  'stars': '83',\n",
       "  'forks': '46',\n",
       "  'commits': '104,142'},\n",
       " {'language': 'C++ 55.3',\n",
       "  'readme': '丸の内MongoDB勉強会 Marunouchi.mongo\\n主に丸の内MongoDB勉強会の資料を管理しています。\\nThe guide of meetup MongoDB in Marunouchi Tokyo.\\n\\n開催予定\\n2015/7/16（木）19:00 - 20:30 丸の内MongoDB勉強会 #22 「初心者向け：MongoDBの基礎とハンズオン(ver3対応)」と「MongoDB World 2015レポート」 2015/7/16DoorKeeper\\nお知らせ\\ngihyo.jpで「MongoDBでゆるふわDB体験」連載終了！ありがとうございました！\\nご発表いただいた資料はOpenStandiaのWebサイトからリンクを貼らせていただく場合があります。ご了承ください。\\n\\nMongoDB最初の1歩\\n@fetarodcの初心者向けMongoDBのキホン！を一通り読むと良いと思います。\\nとりあえず手を動かしたい人は、丸の内MongoDB勉強会 #1のstep01をやってみてください。\\n\\nWiki\\n勉強会で出た質問を中心にノウハウ、Tipsをまとめていきます。\\nwiki\\n\\nお役立ちリンク集\\nmongodbソースコード(rpmもあるよ) - github\\n公式ドキュメント(日本語) マニュアル\\n公式ドキュメント(日本語) チュートリアル\\n新・公式ドキュメント(日本語) マニュアル\\nMongoDBの基礎(動画) - dotinstall\\n@doryokujinさんのスライド\\n@fetarodcの初心者向けMongoDBのキホン！\\nMongoDBでゆるふわDB体験\\n\\n開催記録\\n\\n丸の内MongoDB勉強会 #1 SQLと比較しながらクエリを学ぶ&RubyからMongoDBを触ってみる 2012/07/30 ATND\\n丸の内MongoDB勉強会 #2 みんなでシャーディング 2012/08/28 ATND\\n丸の内MongoDB勉強会 #3 2.2の新機能&レプリケーションハンズオン 2012/09/26 ATND\\n丸の内MongoDB勉強会 #4 「MongoDBでWebアプリを作ってみよう」と「Configパラメータ解説」 2012/11/06 ATND\\n丸の内MongoDB勉強会 #5 「ソースコードリーディング入門」と「MongoDBをカスタムビルドしてみよう」と「運用について」 2012/12/18 ATND\\n丸の内MongoDB勉強会 #6 「GridFSハンズオン」と「Shardingのコネクション数周りのチューニングについて」 2012/01/23 ATND\\n丸の内MongoDB勉強会 #7 「MongoDB 2.4 新機能紹介」と「MongoDBでXMLデータを扱うシステム開発」 2013/02/19 ATND\\n丸の内MongoDB勉強会 #8 in CookPad 「ghostsync and slaveDelay」、「権限によるACLのハンズオン」他 2013/03/27 ATND\\n丸の内MongoDB勉強会 #9 in 楽天 「MongoDB 2.4の注目の新機能ハンズオン」と「MongoDBの検証環境を作ろう」 2013/04/17 ATND\\n丸の内MongoDB勉強会 #10 「初心者向けMongoDB入門」と「MongoDBの運用」 2013/05/22 ATND\\n丸の内MongoDB勉強会 #11 「初心者向けレプリケーションハンズオン」「MongoDBでゲームを作る」 2013/06/26 ATND\\n丸の内MongoDB勉強会 #12 in 納涼もんご祭り 「Node.js+Mongoose+MongoDB で作るWebアプリ」「ホスティングサービスで始めるMongoDB」 2013/07/28 ATND\\n丸の内MongoDB勉強会 #13 「初心者向け：みんなでシャーディング」 2013/09/25 ATND\\n丸の内MongoDB勉強会 #14 「初心者向け：レプリケーション＋シャーディング」「もんごで自然言語処理！第一弾」 2013/10/31 ATND\\n丸の内MongoDB勉強会 #15 「初心者向け：レプリケーション＋シャーディング（第14回のリベンジ）」「MongoDBと私（事例紹介）」 2013/12/19 ATND\\n丸の内MongoDB勉強会 #16 「がっつり事例紹介」 2014/3/19 ATND\\n丸の内MongoDB勉強会 #17 「初心者向けMongoDBのキホン！」「初心者向けMongoDB入門」「月間10億PVを支えるMongoDBアーキテクチャ」 2014/5/14 DoorKeeper\\n丸の内MongoDB勉強会 #18 「初心者向け：レプリケーション＋シャーディング」「MongoDB World@NewYork 報告会」「LT：MongoDB World 2014 プチ報告」 2014/7/17 DoorKeeper\\n丸の内MongoDB勉強会 #19 in もんご祭り「初心者向け：SQLと比較しながらクエリを学ぶ」「MongoDBのindexについて」 2014/10/11 DoorKeeper\\n丸の内MongoDB勉強会 #20 「初心者向け：SQLと比較しながらクエリを学ぶハンズオン」「MMS、特に新機能『Automation』について」「LT:MongoDB 2.8RC0のストレージエンジンについて」 2014/11/26 DoorKeeper\\n丸の内MongoDB勉強会 #21「Couchbase Serverの紹介」「MongoDB の紹介とCouchbaseとの違い」 2015/03/05 DoorKeeper\\n\\n',\n",
       "  'watchers': '20',\n",
       "  'stars': '83',\n",
       "  'forks': '14',\n",
       "  'commits': '807'},\n",
       " {'language': 'C++ 81.5',\n",
       "  'readme': \"DEPRECATED: See https://www.mysensors.org/build/raspberry\\n#Wiring the NRF\\t24L01+ radio\\n\\n\\n\\nNRF24l01+\\nRpi Header Pin\\n\\n\\n\\n\\nGND\\n25\\n\\n\\nVCC\\n17\\n\\n\\nCE\\n22\\n\\n\\nCSN\\n24\\n\\n\\nSCK\\n23\\n\\n\\nMOSI\\n19\\n\\n\\nMISO\\n21\\n\\n\\nIRQ\\n--\\n\\n\\n\\n#Building & Installing\\n##RF24 library\\n\\nDownload the library from https://github.com/TMRh20/RF24\\nEither an official release(tested with 1.1.3) or clone the master branch.\\nDecompress(if needed) and change to the library directory\\nRun make all followed by sudo make install\\n\\n##Serial Gateway\\nThe standard configuration will build the Serial Gateway with a tty name of\\n'/dev/ttyMySensorsGateway' and PTS group ownership of 'tty' the PTS will be group read\\nand write. The default install location will be /usr/local/sbin. If you want to change\\nthat edit the variables in the head of the Makefile.\\n###Build the Gateway\\n\\nClone this repository\\nChange to the Raspberry directory\\nRun make all followed by sudo make install\\n(if you want to start daemon at boot) sudo make enable-gwserial\\n\\nFor some controllers a more recognisable name needs to be used: e.g. /dev/ttyUSB020 (check if this is free).\\nsudo ln -s /dev/ttyMySensorsGateway /dev/ttyUSB20\\nTo automatically create the link on startup, add ln -s /dev/ttyMySensorsGateway /dev/ttyUSB20 just before exit0 in /etc/rc.local\\n#Uninstalling\\n\\nChange to Raspberry directory\\nRun sudo make uninstall\\n\\nSupport: http://forum.mysensors.org\\n\",\n",
       "  'watchers': '54',\n",
       "  'stars': '83',\n",
       "  'forks': '72',\n",
       "  'commits': '57'},\n",
       " {'language': 'C++ 52.0',\n",
       "  'readme': 'PWP3D\\nThis is a cmake version of the PWP3D. The original VS version is availabled at:\\nhttp://www.robots.ox.ac.uk/~victor/code.html\\nFor more detail of the paper, see:\\nSee original paper from:\\nPWP3D: Real-time Segmentation and Tracking of 3D Objects\\nVictor Adrian Prisacariu, Ian Reid\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '83',\n",
       "  'forks': '47',\n",
       "  'commits': '76'},\n",
       " {'language': 'C++ 56.1',\n",
       "  'readme': 'Welcome to python-nlpir\\nThis open source project is a python wrapper for NLPIR.\\nNLPIR is a powerful tool for Chinese segmentation. This program supported Windows and Linux, both 32bit and 64bit platform.\\nCurrent Version: v3.0\\nChange Log\\nVersion 3.0\\n\\nUprgrade NLPIR to NPLIR2015(v20141230).\\nFix bug in installation script.\\n\\nVersion 2.0\\n\\nUprgrade NLPIR core library from NPLIR to NPLIR2014.\\nInstall SWIG automatically during PyNLPIR installation.\\nRefactor installation scripts.\\n\\nVersion 1.1\\n\\nAdd Windows 64bit and Linux 64bit supported.\\nCode constructure adjustment.\\nUpgrade NLPIR2013 core library.\\nAdd installation scripts for each platform.\\n\\nInstallation\\n\\nWindows: python install.py\\nLinux: sudo python install.py\\n\\nOthers\\nIf you want to know more details, you can access my blog.\\nLinks\\n\\nNLPIR Home: http://www.nlpir.org/\\n\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '83',\n",
       "  'forks': '44',\n",
       "  'commits': '56'},\n",
       " {'language': 'JavaScript 55.7',\n",
       "  'readme': '#jq-tiles\\nSlideshow with many cool css3 effects.\\nDemo: http://elclanrs.github.com/jq-tiles/ (Use Google Chrome for best experience)\\nSupport: Webkit, Firefox, Opera, IE10, IE9-8*\\nLicense: MIT\\nOptions:\\n{\\n  x              : 4, // # of tiles in x axis, 20 max\\n  y              : 4, // # of tiles in y axis, 20 max\\n  effect         : \\'default\\',\\n  fade           : false, // fade images in addition to the tiles effect\\n  random         : false, // animate tiles in random order\\n  reverse        : false, // start animation from opposite direction\\n  backReverse    : false, // reverse the animation when going back in the slideshow (useful for some effects)\\n  rewind         : false, // reverse animation at a certain percentage in time\\n  auto           : false, // Start the slideshow on load\\n  loop           : false, // Start slideshow again when it finishes\\n  slideSpeed     : 3500, // time between slides\\n  tileSpeed      : 800, // time to clear all tiles\\n  cssSpeed       : 300, // css3 transition speed [100,200,300,400,500,600,700,800,900,1000],\\n  nav            : true, // Add navigation\\n  navWrap        : null, // Add the navigation to an existing element\\n  bullets        : true, // Show bullets, if false the show pagination with numbers\\n  thumbs         : true, // Show thumbnails when hovering nav\\n  thumbSize      : 25, // Thumbnail size (percentage of the original image)\\n  timer          : true // show or hide the timer bar\\n  beforeChange   : function() {}, // Runs before changing the image\\n  afterChange    : function() {} // Runs after the tiles have cleared\\n  onSlideshowEnd : function() {} // Runs when the slideshow finishes ( \"loop\" must be set to false )\\n}\\nMethods:\\nstart\\n$(\\'.slider\\').tilesSlider(\\'start\\')\\nstop\\n$(\\'.slider\\').tilesSlider(\\'stop\\')\\nnext\\n$(\\'.slider\\').tilesSlider(\\'next\\', callback)\\nprev\\n$(\\'.slider\\').tilesSlider(\\'prev\\', callback)\\nUsage:\\nHTML:\\n<div class=\"slider\">\\n  <img src=\"img1.jpg\"/> <!-- No description -->\\n  <img src=\"img2.jpg\"/><p>Description image two</p>\\n  <img src=\"img3.jpg\"/><p>Description image three</p>\\n</div>\\nCSS:\\n.slider { width: 600px; height: 400px; }\\njQuery:\\n$(\\'.slider\\').tilesSlider({ random: true })\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '165',\n",
       "  'forks': '36',\n",
       "  'commits': '396'},\n",
       " {'language': 'JavaScript 99.6',\n",
       "  'readme': \"backbone-d3\\nWith backbone-d3 we aim to provide a simple interface to visualise with d3\\ndynamic data held in backbone collections. The simple visualisations provided\\n(pie, bar, line, scatter) are as much for demonstration or testing as for wider\\nuse. We've tried to connect two great packages without putting too much between\\nthem.\\nHopefully this means you can quickly create some basic plots of your data or\\nget into more sophisticated visualisations without having to fight with\\nasynchronous JavaScript or a lot of wrapper code.\\nExamples from the repo are also hosted on http://drsm79.github.com/Backbone-d3/\\nso you can see the library in action.\\nPlotCollection\\nAny backbone collection of any backbone model can be used by the plotting view.\\nThe collection containing them can have a caption variable set which will be\\nrendered (using Markdown if pagedown is available) under the plot. This is all\\nthe backbone.d3.PlotCollection does.\\nSome simple collections are provided for use with the canned views. If your\\ndata maps onto these models/collections well you can just reuse them. If you\\nalready have model/collections in use you should be able to reuse them\\ntrivially.\\nPlotView\\nPlotView is where the magic happens. It deals with the captioning of plots and\\nmakes sure the appropriate actions are taken when data in the collection\\nchanges. The PlotView defines how the data is extracted from the collection\\n(through the plotdata() method) and how it is rendered to the browser via the\\nplot() method.\\nThe PlotView is responsible for both formatting and rendering the plot to allow\\none collection to be visualised in multiple ways. The view can also hold a\\ncaption for the visualisation, overriding the one set on the data collection.\\nStreaming\\nAs your collection changes so should your visualisation. The change/add/remove\\ntriggers are bound to the redraw method of the plot view (reset redraws the\\nvisualisation from scratch. This triggers a d3 transformation, updating the\\nplot in place with your new data. Tasty!\\nThe canned plot views\\nEach type of visualisation (should be careful about referring to them as\\nplots...) is a Backbone view. There are canned views for some common\\nvisualisations, or ones we've needed ourselves, that are available in\\nindividual files (to minimise what gets loaded) under the Backbone.d3.Canned\\nnamespace.\\nIf you want to create more interesting visualisations you'll be subclassing the\\nPlotView baseclass (please send pull requests if you make something nice!).\\n\",\n",
       "  'watchers': '11',\n",
       "  'stars': '165',\n",
       "  'forks': '29',\n",
       "  'commits': '49'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'CMND\\nCommand Line Interface Utility for Node.js\\nCMND is a package that lets you easily create CLI tools in Node.js using\\nidiomatic ES6 syntax (Node 4+). It\\'s also simple to create associated manual\\n(help) pages for each command.\\nThis module was initially built for Nodal, but\\ncan be used anywhere you\\'d like.\\nUsage\\nTo use CMND, first install it in your Node project with npm install cmnd --save.\\nNext, modify your project\\'s package.json to include:\\n\"bin\": {\\n  \"mycli\": \"./cli.js\"\\n}\\n\\nWhere mycli is the intended name of your command in the CLI.\\nNow create a file: ./cli.js and folder ./commands:\\n#!/usr/bin/env node\\n\\n\\'use strict\\';\\n\\nconst CommandLineInterface = require(\\'cmnd\\').CommandLineInterface;\\nconst CLI = new CommandLineInterface();\\n\\nCLI.load(__dirname, \\'./commands\\');\\nCLI.run(process.argv.slice(2));\\nFinally, populate your commands directory with your commands, here\\'s an example\\nfile: ./commands/example.js\\nmodule.exports = (() => {\\n\\n  \\'use strict\\';\\n\\n  const Command = require(\\'cmnd\\').Command;\\n\\n  class ExampleCommand extends Command {\\n\\n    constructor() {\\n\\n      super(\\'example\\');\\n\\n    }\\n\\n    help() {\\n\\n      return {\\n        description: \\'An example command\\',\\n        args: [\\'example_arg1\\', \\'example_arg2\\'],\\n        flags: {flag: \\'An example flag\\'},\\n        vflags: {vflag: \\'An example verbose flag\\'}\\n      };\\n\\n    }\\n\\n    run(args, flags, vflags, callback) {\\n\\n      // Run code here.\\n      // To throw an error, use: callback(new Error(msg))\\n      // To optionally return a result, use: callback(null, result)\\n\\n      callback(null);\\n\\n    }\\n\\n  }\\n\\n  return ExampleCommand;\\n\\n})();\\nCreating manual pages (help)\\nView all the commands available to your CLI with mycli help where mycli is\\nthe intended name of your command in the CLI. To modify help information,\\nchange the return value of the help() method for each command.\\nCreating Subcommands\\nTo subclass a command (i.e. mycli command_name:sub_name) simply change the contructor()\\nmethod in your command to the following:\\nconstructor() {\\n\\n  super(\\'command_name\\', \\'sub_name\\');\\n\\n}\\n\\nRunning your commands\\nEach command has a run() method which takes three arguments: args, flags,\\nand vflags.\\nargs\\nargs is the array of arguments, passed before any flags.\\ni.e. mycli command alpha beta would populate args with [\\'alpha\\', \\'beta\\']\\nflags\\nflags is an object containing any flags (prefixed with -), where each entry\\nis an array of values passed after the flag\\ni.e. mycli command -f my flag would populate vflags with {f: [\\'my\\', \\'flag\\']}\\nvflags\\nvflags works identically to flags, but with \"verbose flags\" (prefixed\\nwith --).\\nAdditional notes\\nAll argument arrays passed to args or any flags or vflags options will\\nbe separated by spaces, except in the case of quotation marks. Use\\nquotation marks to specify an argument with spaces in it.\\ni.e. mycli command -f \"argument one\" argument_two\\nAcknowledgements\\nThanks for checking out the library! Feel free to submit issues or PRs if you\\'d\\nlike to see more features.\\nFollow me on Twitter, @keithwhor.\\nFeel free to check out more of my GitHub projects.\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '165',\n",
       "  'forks': '12',\n",
       "  'commits': '17'},\n",
       " {'language': 'JavaScript 72.3',\n",
       "  'readme': 'connect-prerenderer\\n\\n\\nExpress/connect middleware to pre-render ajax page for non-ajax browsers, especially using angular.js\\nHow to use\\n$ npm install connect-prerenderer\\n\\nIn app.js:\\nvar express = require(\\'express\\');\\nvar prerenderer = require(\\'connect-prerenderer\\');\\nvar app = express();\\napp.use(prerenderer());\\n...\\n\\nOptions\\n\\ntargetGenerator: a name or a function to generate a new one for HTTP request.\\n\\n\"default\" --- check \"/PRERENDER-\" prefix which will be removed, and replace \"HASH-\" to \"#/\" and all \"-\"s to \"/\"s to make a target URL. (see the source code for more options.)\\n\"googlebot\" --- follow https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\\na function that returns a target URL for prerendering or null.\\n\\n\\ntimeout: an integer in milliseconds to specify how long it waits to prerender.\\ncookieDomain: a domain name to allow passing cookies.\\nattachConsole: when truthy, attach global.console to window.console when prerendering files\\n(useful for debugging)\\nsubprocess: if truthy, do the rendering in a subprocess (it helps to prevent possible memory leaks in jsdom). You can also set the RENDERER_USE_SUBPROCESS environment variable to any value (except an empty string) to achieve the same result. Try it if you experience memory leaks.\\n\\nCoding conventions (client-side)\\n\\nIf an html is prerendered, the body is like: <body data-prerendered=\"true\">\\nWhen JavaScript code finishes prerendering, it should call: document.onprerendered()\\n\\nNotes for AngularJS\\nThe following snippet would help AngularJS to work:\\n<script>\\n  angular.element(document).ready(function() {\\n    if (document.body.getAttribute(\\'data-prerendered\\')) {\\n      document.addEventListener(\\'click\\', function() {\\n        document.removeEventListener(\\'click\\', arguments.callee, true);\\n        angular.bootstrap(document.documentElement, []);\\n        return true;\\n      }, true);\\n    } else {\\n      angular.bootstrap(document.documentElement, []);\\n    }\\n  });\\n</script>\\n\\nTo keep templates for interpolation in a prerendered html,\\nuse the modified version of angular.js (v1.2.5)\\nlocated in test/server/public/.\\nLimitations\\n\\nstyle attributes are not preserved by jsdom (use class).\\n\\nAngularJS only limitations:\\n\\nng-repeat workaround only works with ng-repeat attributes.\\nng-repeat-(start|end) is not supported.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '165',\n",
       "  'forks': '6',\n",
       "  'commits': '116'},\n",
       " {'language': 'JavaScript 71.8',\n",
       "  'readme': \"Stormpath is Joining Okta\\nWe are incredibly excited to announce that Stormpath is joining forces with Okta. Please visit the Migration FAQs for a detailed look at what this means for Stormpath users.\\nWe're available to answer all questions at support@stormpath.com.\\nWhat does this mean for developers who are using this library?\\n\\nIf you have upgraded to the 2.x series from 1.x, you should downgrade to 1.1.1.  Why?  The 2.x series depends on the Stormpath Client API, which will not be migrated to the Okta platform.\\nWhen downgrading to 1.1.1 you will need to use one of our backend framework integrations to serve the APIs that the 1.x series depends on.\\nThese backend integrations are being patched to work with Okta:\\n\\n\\nJava Spring\\nJava Spring Boot\\nNode Express\\nASP.NET 4.x\\nASP.NET Core\\n\\n\\nIf you are using the Express integration, please see the Express-Stormpath Angular Sample Project, it can be used to test your migration to Okta.\\n\\nREADME\\nIf you are actively using this library, you can find the old readme in OLD-README.md. It is not possible to register for new Stormpath tenants at this time, so you must already have a Stormpath tenant if you wish to use this library during the migration period.\\n\",\n",
       "  'watchers': '45',\n",
       "  'stars': '165',\n",
       "  'forks': '64',\n",
       "  'commits': '403'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Employee Directory\\nSample Application built with Backbone.js and TopCoat\\n\"Backbone Directory\" is a simple Employee Directory application built with Backbone.js and [TopCoat] (http://topcoat.io).\\nRefer to this blog post for more information about the application.\\nThe application runs out-of-the-box with an in-memory data store.\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '165',\n",
       "  'forks': '86',\n",
       "  'commits': '1'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"Synopsis\\nCustom command line tab completion for node.js applications.\\nExample\\n#!/usr/bin/env node\\n\\nvar complete = require('complete'); // get the `complete` module.\\n\\n//\\n// list of items to complete on.\\n//\\ncomplete.list = ['apple', 'orange', 'pear', 'lemon', 'mango'];\\n\\ncomplete.callback = function(lastSelection, userInput, reducedList) {\\n\\n  if (lastSelection === 'apple') {\\n\\tcomplete.add('sauce');\\n  }\\n};\\n\\ncomplete.init();\\n\\n//\\n// continue with the application...\\n//\\nconsole.log('program started with the following arguments:', process.argv[2] || 'none provided');\\nDistribution and Installation\\nYour installment procedure should place your CLI program in a location made accessible by the PATH variable. If users install your program with the NPM -g option, your program will be in the path.\\n/usr/local/bin/myprogram -> /usr/local/lib/node_modules/myprogram/bin/myprogram\\nAPI\\nlist\\nCreate a list of commands that you want to autocomplete with.\\ncomplete.list = ['apple', 'orange', 'pear', 'lemon', 'mango'];\\ncallback\\nOptionally you can define a callback that will get called when the match when the completion happens.\\ncomplete.callback = function(lastSelection, userInput, reducedList) {\\n\\n  //\\n  // do something if this is an `orange`. Note that anything that\\n  // you `process.stdout.write()` will be added to the auto complete\\n  // list.\\n  //\\n};\\ninit()\\nInitialize the auto completion behavior.\\ncomplete.init();\\nHigher Level Example\\nvar complete = require('complete');\\n\\ncomplete({\\n  program: 'my-program',\\n  // Commands\\n  commands: {\\n    'hello': function(words, prev, cur) {\\n      complete.output(cur, ['abc', 'def']);\\n    },\\n    'world': {\\n      'hi': function(words, prev, cur) {\\n        complete.echo('next');\\n      }\\n    }\\n  },\\n  // Position-independent options.\\n  // These will attempted to be\\n  // matched if `commands` fails\\n  // to match.\\n  options: {\\n    '--help': {},\\n    '-h': {},\\n    '--version': {},\\n    '-v': {}\\n  }\\n});\\nThe above results in\\n$ my-program he<TAB>\\n$ my-program hello\\n$ my-program hello a<TAB>\\n$ my-program hello abc\\nLicense\\n(The MIT License)\\nCopyright (c) 2010 hij1nx http://www.twitter.com/hij1nx\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '164',\n",
       "  'forks': '13',\n",
       "  'commits': '70'},\n",
       " {'language': 'JavaScript 65.7',\n",
       "  'readme': 'New Relic Boxes\\n \\nNew Relic Boxes is an html5 dashboard for your R&D team. Boxes lets you monitor your applications and servers on New Relic.\\nAlarm is played when one of your application / servers health status is changed.\\n\\nInstallation\\n\\nEnsure node ^0.10.33 is installed and on your path.\\nRun npm install bower to install bower\\nRun bower install to install libraries\\nRun npm install to install dependencies\\nRun index.html (the easiest way) or run npm start to start a static server\\n\\nSetup\\npress settings and add the following information:\\nYour new relic apikey (mandatory).\\nYour company name (optional).\\nYour favicon url (optional).\\nYour new relic account id (optional) - to open a new tab with your specific box on new relic app.\\n',\n",
       "  'watchers': '20',\n",
       "  'stars': '163',\n",
       "  'forks': '12',\n",
       "  'commits': '91'},\n",
       " {'language': 'JavaScript 98.4',\n",
       "  'readme': 'angular-flash\\n\\n\\nA flash service and directive for setting and displaying flash messages in Angular JS.  Specifically, the flash service is a publisher of flash messages and the flash directive is a subscriber to flash messages.  The flash directive leverages the Twitter Bootstrap Alert component.\\nInstallation\\nDownload angular-flash.min.js or install with bower.\\n$ bower install angular-flash --save\\nLoad the angular-flash.service and the angular-flash.flash-alert-directive modules in your app.\\nangular.module(\\'app\\', [\\'angular-flash.service\\', \\'angular-flash.flash-alert-directive\\']);\\nConfigure\\nangular.module(\\'app\\', [\\'angular-flash.service\\', \\'angular-flash.flash-alert-directive\\'])\\n        .config(function (flashProvider) {\\n        \\n            // Support bootstrap 3.0 \"alert-danger\" class with error flash types\\n            flashProvider.errorClassnames.push(\\'alert-danger\\');\\n\\n            /**\\n             * Also have...\\n             *\\n             * flashProvider.warnClassnames\\n             * flashProvider.infoClassnames\\n             * flashProvider.successClassnames\\n             */\\n\\n        })\\nUsage\\nUse the flash service to publish a flash messages...\\nvar FooController = function(flash){\\n    // Publish a success flash\\n    flash.success = \\'Do it live!\\';\\n\\n    // Publish a error flash\\n    flash.error = \\'Fail!\\';\\n\\n    // Publish an info flash to the `alert-1` subscriber\\n    flash.to(\\'alert-1\\').info = \\'Only for alert 1\\';\\n\\n    // The `flash-alert` directive hides itself when if receives falsey flash messages of any type\\n    flash.error = \\'\\';\\n\\n};\\n\\nFooController.$inject = [\\'flash\\'];\\nUse the flash-alert directive to subscribe to flash messages...\\n<!-- Subscribe to success flash messages. -->\\n<div flash-alert=\"success\" active-class=\"in\" class=\"alert fade\">\\n    <strong class=\"alert-heading\">Congrats!</strong>\\n    <span class=\"alert-message\">{{flash.message}}</span>\\n</div>\\n\\n<!-- Subscribe to error flash messages. -->\\n<div flash-alert=\"error\" active-class=\"in\" class=\"alert fade\">\\n    <strong class=\"alert-heading\">Boo!</strong>\\n    <span class=\"alert-message\">{{flash.message}}</span>\\n</div>\\n\\n<!-- Subscribe to all flash messages. -->\\n<div flash-alert active-class=\"in\" class=\"alert fade\">\\n    <strong class=\"alert-heading\">Boo!</strong>\\n    <span class=\"alert-message\">{{flash.message}}</span>\\n</div>\\n\\n<!-- Subscribe to all flash messages sent to `alert-1`. -->\\n<div id=\"alert-1\" flash-alert active-class=\"in\" class=\"alert fade\">\\n    <strong class=\"alert-heading\">Boo!</strong>\\n    <span class=\"alert-message\">{{flash.message}}</span>\\n</div>\\n\\n<!-- Set the display duration in milli-secs.  Default is 5000, 0 disables the fade-away. -->\\n<div flash-alert active-class=\"in\" class=\"alert fade\" duration=\"0\">\\n    <!-- Manually hide the alert with `hide()` -->\\n    <button type=\"button\" class=\"close\" ng-click=\"hide()\">&times;</button>\\n    <strong class=\"alert-heading\">Boo!</strong>\\n    <span class=\"alert-message\">{{flash.message}}</span>\\n</div>\\nWhen a flash message is published, the flash-alert directive will add a class of the form alert-<type> and also add classes specified in active-class.  Then after 5 seconds it will remove them.\\nThe example above leverages Twitter Bootstrap CSS3 transitions: fade and in.\\nStyling Considerations\\nBootstrap 2 has a few styling quirks with the .alert and .fade classes.\\nVisible or not\\nSome folks may want hidden alerts to take up visible space others may not.  Fortunately, each case is easy to achieve by declaring .alert as indicated below...\\nTakes up no visible space when hidden\\n<div flash-alert active-class=\"in alert\" class=\"fade\">\\n...\\n</div>\\nTakes up visible space when hidden\\n<div flash-alert active-class=\"in\" class=\"fade alert\">\\n...\\n</div>\\nCSS Transition Quirks\\nThe .fade class only transitions opacity and the base .alert class has a background color and background border used for alert warnings.  Together these styling attributes can make it challenging to achieve smooth transitions.\\nFortunately, its easy to replace the .alert class and move the warning colors to .alert-warn as illustrated below...\\nStyling\\n/* Remove colors and add transition property */\\n.alert-flash {\\n    padding: 8px 35px 8px 14px;\\n    margin-bottom: 20px;\\n    text-shadow: 0 1px 0 rgba(255, 255, 255, 0.5);\\n    border: 1px solid transparent;\\n    -webkit-border-radius: 4px;\\n    -moz-border-radius: 4px;\\n    border-radius: 4px;\\n\\n    /* change transition property to all */\\n    -webkit-transition-property: all;\\n    transition-property: all;\\n}\\n\\n.alert-flash h4 {\\n    margin: 0;\\n}\\n\\n.alert-flash .close {\\n    position: relative;\\n    top: -2px;\\n    right: -21px;\\n    line-height: 20px;\\n}\\n\\n/* add warning colors to warn class */\\n.alert-warn {\\n    background-color: #fcf8e3;\\n    border: 1px solid #fbeed5;\\n}\\n\\n.alert-warn,\\n.alert-warn h4 {\\n    color: #c09853;\\n}\\nTemplate:\\n<div flash-alert=\"info\" active-class=\"in\" class=\"alert-flash fade\">\\n    <i class=\"icon-info-sign\"></i>\\n    <strong class=\"alert-heading\">Ahem...</strong>\\n    <span class=\"alert-message\">{{flash.message}}</span>\\n</div>\\nFlashProvider API\\nflashProvider.errorClassnames\\nflashProvider.warnClassnames\\nflashProvider.infoClassnames\\nflashProvider.successClassnames\\nFlash Service API\\nProperties\\nSet and get flash messages with the following flash properties...\\n\\nsuccess\\ninfo\\nwarn\\nerror\\n\\nMethods\\nsubscribe(listener, [type])\\nRegister a subscriber callback function to be notified of flash messages.  The subscriber function has two arguments: message and type.\\nclean()\\nClear all subscribers and flash messages.\\nContributing\\nPrerequisites\\nThe project requires Bower, Grunt, and PhantomJS.  Once you have installed them, you can build, test, and run the project.\\nBuild & Test\\nTo build and run tests, run either...\\n$ make install\\nor\\n$ npm install\\n$ bower install\\n$ grunt install\\nDemo & Develop\\nTo run a live demo or do some hackery, run...\\n$ grunt server\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '163',\n",
       "  'forks': '52',\n",
       "  'commits': '80'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"DEPRECATED\\nYou should use Facebook's module -- https://github.com/facebook/jscodeshift\\nSYNOPSIS\\nBuild a subset or superset of a code base.\\nDESCRIPTION\\nRather than breaking apart files at design-time for the purpose of targeted\\ndistribution, you can break them apart using a filter at build-time.\\nEXAMPLES\\nAs the Javascript file is traversed and syntax is discovered, you will have the\\nopportunity to interject, for example:\\nInput\\nvar a = 10\\nvar b = 11\\nvar c = 12\\nCode\\nvar cs = require('codesurgeon')\\n\\nvar filter = cs.filter()\\n\\nfilter.inclusive = true\\n\\nfilter.syntax.VariableDeclaration = function(name, item) {\\n  if (name === 'b') {\\n    return true\\n  }\\n}\\n\\nfstream\\n  .Reader('example.js')\\n  .pipe(filter)\\n  .pipe(process.stdout)\\nOutput\\nvar b = 11\\nA filter supports a streaming style API, but most operations block processing so\\nthis is purely for convenience.\\nSyntax Reference\\nAssignmentExpression\\nArrayExpression\\nArrayPattern\\nBlockStatement\\nBinaryExpression\\nBreakStatement\\nCallExpression\\nCatchClause\\nComprehensionBlock\\nComprehensionExpression\\nConditionalExpression\\nContinueStatement\\nDirectiveStatement\\nDoWhileStatement\\nDebuggerStatement\\nEmptyStatement\\nExpressionStatement\\nForStatement\\nForInStatement\\nFunctionDeclaration\\nFunctionExpression\\nIdentifier\\nIfStatement\\nLiteral\\nLabeledStatement\\nLogicalExpression\\nMemberExpression\\nNewExpression\\nObjectExpression\\nObjectPattern\\nProgram\\nProperty\\nReturnStatement\\nSequenceExpression\\nSwitchStatement\\nSwitchCase\\nThisExpression\\nThrowStatement\\nTryStatement\\nUnaryExpression\\nUpdateExpression\\nVariableDeclaration\\nVariableDeclarator\\nWhileStatement\\nWithStatement\\nYieldExpression\\nLICENSE\\n(The MIT License)\\nCopyright (c) 2010 hij1nx http://www.twitter.com/hij1nx\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\",\n",
       "  'watchers': '4',\n",
       "  'stars': '163',\n",
       "  'forks': '5',\n",
       "  'commits': '111'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"django-compat\\n\\n\\n\\n\\nForward and backwards compatibility layer for Django 1.4, 1.7, 1.8, 1.9, 1.10 and 1.11\\nConsider django-compat as an experiment based on the discussion on reddit. Let's see where it goes.\\nWhat started as an experiment based on this discussion on reddit has proven to be true in real life.\\ndjango-compat is under active development. To learn about other features, bug fixes, and changes, please refer to the changelog.\\nWho uses django-compat\\nTwo popular examples of open source reusable app that uses django-compat are django-hijack and django-background-tasks.\\nWant to have yours listed here? Send us a PR.\\nWhy use django-compat\\n\\nBe able to use the LTS versions of Django and support newer versions in your app\\nUse features from newer Django versions in an old one\\nManage and master the gap between different framework versions\\n\\nHow to use django-compat\\nInstall compat from the PyPI or download and install manually. All relevant  releases are listed here under releases.\\nUsing one of the compatible objects is easy. For example\\nfrom compat import patterns, url\\n\\nurlpatterns = patterns('ABC.views',\\n\\t\\turl(r'^abc/$', 'abc', name='abc-link'),\\n...\\n\\nSee a full example here.\\n\\n\\n\\ndjango-compat is free software. If you find it useful and would like to give back, please consider to make a donation using Bitcoin or PayPal. Thank you!\\n\\n\\n\\nCompatible objects\\n\\n\\n\\nCompatible object\\nSpecifically tested\\n1.8\\n1.9\\n1.10\\n1.11\\nNotes\\n\\n\\n\\n\\nBytesIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nDjangoJSONEncoder\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nEmailValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nGenericForeignKey\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nmodels.GenericForeignKey\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nHttpResponseBase\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nJsonResponse\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexProvider\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nNoReverseMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLPattern\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolver404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolverMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nSortedDict\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nStringIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nURLValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nVariableNode\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nView\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nadd_to_builtins\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nadmin_utils\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\natomic\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclean_manytomany_helptext\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclear_url_caches\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclose_connection\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit_on_success\\n✔️\\n✔️\\n✔️\\n✔️\\n`✔️\\ncommit_on_success replaced by atomic in Django >= 1.8\\n\\n\\nforce_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nformat_html\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_callable\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_current_site\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ident\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_mod_func\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model_name\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ns_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_template_loaders\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_user_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_username_field\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler500\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_module\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_string\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ninclude\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nis_valid_path\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nparse_qs\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\npatterns\\n✖️\\n✔️\\n✔️\\n❌\\n❌\\n\\n\\n\\npython_2_unicode_compatible\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrender_to_string\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nThe new function signature (https://docs.djangoproject.com/en/1.9/releases/1.8/#dictionary-and-context-instance-arguments-of-rendering-functions) is backported to pre-1.8.\\n\\n\\nresolve\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nresolve_url\\n✔️\\n✔️\\n✔️\\n⚠️\\n⚠️\\n1.10: Reversing by dotted path has been removed\\n\\n\\nreverse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nreverse_lazy\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrollback\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTransaction savepoint (sid) is required for Django < 1.8\\n\\n\\nset_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nset_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsimplejson\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nslugify\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsmart_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nunquote_plus\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurl\\n✖️\\n✔️\\n✔️\\n✖️\\n✖️\\nFunction used in urlpatterns\\n\\n\\ntempat.url\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n`uravy_multiplication_x:\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlresolvers\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlunparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nuser_model_label\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ntemplatetags.compat.verbatim\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTemplatetag; import with {% load verbatim from compat %}. 1.4: Does not allow specific closing tags, e.g. {% endverbatim myblock %}, and does not preserve whitespace inside tags.\\n\\n\\n\\nResources and references\\nResources\\n\\nhttps://github.com/ubernostrum/django-compat-lint\\nhttps://docs.djangoproject.com/en/dev/misc/api-stability/\\nhttps://docs.djangoproject.com/en/dev/topics/python3/\\nhttp://andrewsforge.com/presentation/upgrading-django-to-17/\\n\\ncompat.py\\nBits and bites of the following projects were re-used to build django-compat.\\n\\n https://github.com/lukaszb/django-guardian/blob/devel/guardian/compat.py\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/compat.py\\n https://github.com/toastdriven/django-tastypie/blob/master/tastypie/compat.py\\n https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/compat.py\\n\\n TODO: MinValueValidator, MaxValueValidator et al. (other relevant bits are included) Django 1.8\\n\\n\\n https://gist.github.com/theskumar/ff8de60ff6a33bdacaa8\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/templatetags/compat.py\\n https://github.com/kennethreitz/requests/blob/master/requests/compat.py\\n https://github.com/mitsuhiko/jinja2/blob/master/jinja2/_compat.py\\n https://github.com/jaraco/setuptools/blob/master/setuptools/compat.py\\n https://github.com/mariocesar/sorl-thumbnail/blob/master/sorl/thumbnail/compat.py\\n\\nChangelog\\n2017/04/07\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/08/02\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/06/01\\n\\nAdd get_current_site and admin_utils\\n\\n2016/05/11\\n\\nFix error when installing package under python 3.4\\n\\n###\\xa02015/11/12\\n\\nBackport new render_to_string function signature to Django < 1.8\\nBackport verbatim tag to Django 1.4\\nAdd get_template_loaders\\nAdd close_connection\\nImprove JsonResponse backport to Django 1.4\\nAdd tests for import_module, get_model and add_to_builtins\\nAnticipate renaming of django.core.urlresolvers to django.urls in 1.10\\nAvoid warnings in setup.py\\n\\n2015/11/11\\n\\n1.9 compatibility for existing objects with the following changes:\\n\\nadd_to_builtins was removed for Django >= 1.9\\nGenericForeignKey` was moved to compat.models`` for Django >= 1.9\\n\\n\\n\\n2015/07/15\\n\\nadd_to_builtins was added\\n\\n2015/07/08\\n\\nget_query_set/get_queryset support was dropped again (see #29)\\n\\n\",\n",
       "  'watchers': '5',\n",
       "  'stars': '106',\n",
       "  'forks': '21',\n",
       "  'commits': '215'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"django-compat\\n\\n\\n\\n\\nForward and backwards compatibility layer for Django 1.4, 1.7, 1.8, 1.9, 1.10 and 1.11\\nConsider django-compat as an experiment based on the discussion on reddit. Let's see where it goes.\\nWhat started as an experiment based on this discussion on reddit has proven to be true in real life.\\ndjango-compat is under active development. To learn about other features, bug fixes, and changes, please refer to the changelog.\\nWho uses django-compat\\nTwo popular examples of open source reusable app that uses django-compat are django-hijack and django-background-tasks.\\nWant to have yours listed here? Send us a PR.\\nWhy use django-compat\\n\\nBe able to use the LTS versions of Django and support newer versions in your app\\nUse features from newer Django versions in an old one\\nManage and master the gap between different framework versions\\n\\nHow to use django-compat\\nInstall compat from the PyPI or download and install manually. All relevant  releases are listed here under releases.\\nUsing one of the compatible objects is easy. For example\\nfrom compat import patterns, url\\n\\nurlpatterns = patterns('ABC.views',\\n\\t\\turl(r'^abc/$', 'abc', name='abc-link'),\\n...\\n\\nSee a full example here.\\n\\n\\n\\ndjango-compat is free software. If you find it useful and would like to give back, please consider to make a donation using Bitcoin or PayPal. Thank you!\\n\\n\\n\\nCompatible objects\\n\\n\\n\\nCompatible object\\nSpecifically tested\\n1.8\\n1.9\\n1.10\\n1.11\\nNotes\\n\\n\\n\\n\\nBytesIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nDjangoJSONEncoder\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nEmailValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nGenericForeignKey\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nmodels.GenericForeignKey\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nHttpResponseBase\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nJsonResponse\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexProvider\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nLocaleRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nNoReverseMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLPattern\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nRegexURLResolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolver404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nResolverMatch\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nSortedDict\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nStringIO\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nURLValidator\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nVariableNode\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nView\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nadd_to_builtins\\n✖️\\n✔️\\n❌\\n❌\\n❌\\n\\n\\n\\nadmin_utils\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\natomic\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclean_manytomany_helptext\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclear_url_caches\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nclose_connection\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ncommit_on_success\\n✔️\\n✔️\\n✔️\\n✔️\\n`✔️\\ncommit_on_success replaced by atomic in Django >= 1.8\\n\\n\\nforce_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nformat_html\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_callable\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_current_site\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ident\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_mod_func\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_model_name\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_ns_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_resolver\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_template_loaders\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_user_model\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nget_username_field\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler404\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nhandler500\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_module\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nimport_string\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ninclude\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nis_valid_path\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nparse_qs\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\npatterns\\n✖️\\n✔️\\n✔️\\n❌\\n❌\\n\\n\\n\\npython_2_unicode_compatible\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrender_to_string\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nThe new function signature (https://docs.djangoproject.com/en/1.9/releases/1.8/#dictionary-and-context-instance-arguments-of-rendering-functions) is backported to pre-1.8.\\n\\n\\nresolve\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nresolve_url\\n✔️\\n✔️\\n✔️\\n⚠️\\n⚠️\\n1.10: Reversing by dotted path has been removed\\n\\n\\nreverse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nreverse_lazy\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nrollback\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTransaction savepoint (sid) is required for Django < 1.8\\n\\n\\nset_script_prefix\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nset_urlconf\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsimplejson\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nslugify\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nsmart_text\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nunquote_plus\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurl\\n✖️\\n✔️\\n✔️\\n✖️\\n✖️\\nFunction used in urlpatterns\\n\\n\\ntempat.url\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n`uravy_multiplication_x:\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlresolvers\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nurlunparse\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\nuser_model_label\\n✖️\\n✔️\\n✔️\\n✔️\\n✔️\\n\\n\\n\\ntemplatetags.compat.verbatim\\n✔️\\n✔️\\n✔️\\n✔️\\n✔️\\nTemplatetag; import with {% load verbatim from compat %}. 1.4: Does not allow specific closing tags, e.g. {% endverbatim myblock %}, and does not preserve whitespace inside tags.\\n\\n\\n\\nResources and references\\nResources\\n\\nhttps://github.com/ubernostrum/django-compat-lint\\nhttps://docs.djangoproject.com/en/dev/misc/api-stability/\\nhttps://docs.djangoproject.com/en/dev/topics/python3/\\nhttp://andrewsforge.com/presentation/upgrading-django-to-17/\\n\\ncompat.py\\nBits and bites of the following projects were re-used to build django-compat.\\n\\n https://github.com/lukaszb/django-guardian/blob/devel/guardian/compat.py\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/compat.py\\n https://github.com/toastdriven/django-tastypie/blob/master/tastypie/compat.py\\n https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/compat.py\\n\\n TODO: MinValueValidator, MaxValueValidator et al. (other relevant bits are included) Django 1.8\\n\\n\\n https://gist.github.com/theskumar/ff8de60ff6a33bdacaa8\\n https://github.com/evonove/django-oauth-toolkit/blob/master/oauth2_provider/templatetags/compat.py\\n https://github.com/kennethreitz/requests/blob/master/requests/compat.py\\n https://github.com/mitsuhiko/jinja2/blob/master/jinja2/_compat.py\\n https://github.com/jaraco/setuptools/blob/master/setuptools/compat.py\\n https://github.com/mariocesar/sorl-thumbnail/blob/master/sorl/thumbnail/compat.py\\n\\nChangelog\\n2017/04/07\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/08/02\\n\\nUpdate existing patches for Django 1.10\\n\\n2016/06/01\\n\\nAdd get_current_site and admin_utils\\n\\n2016/05/11\\n\\nFix error when installing package under python 3.4\\n\\n###\\xa02015/11/12\\n\\nBackport new render_to_string function signature to Django < 1.8\\nBackport verbatim tag to Django 1.4\\nAdd get_template_loaders\\nAdd close_connection\\nImprove JsonResponse backport to Django 1.4\\nAdd tests for import_module, get_model and add_to_builtins\\nAnticipate renaming of django.core.urlresolvers to django.urls in 1.10\\nAvoid warnings in setup.py\\n\\n2015/11/11\\n\\n1.9 compatibility for existing objects with the following changes:\\n\\nadd_to_builtins was removed for Django >= 1.9\\nGenericForeignKey` was moved to compat.models`` for Django >= 1.9\\n\\n\\n\\n2015/07/15\\n\\nadd_to_builtins was added\\n\\n2015/07/08\\n\\nget_query_set/get_queryset support was dropped again (see #29)\\n\\n\",\n",
       "  'watchers': '4',\n",
       "  'stars': '106',\n",
       "  'forks': '11',\n",
       "  'commits': '411'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Zippopotamus Cloud\\n\\nAPI Moved\\nOur full crowd-source zip-db can be found\\nhere\\nTrea has taken over Zippopotamus - you can find it at the ekotechnology fork\\nThanks\\nJeff & Samir\\n\\nThis is a repository for  Zippopotamus the global postal code API\\nZippopotamus is hosted by dotCloud. This repo is used to build and maintain the site.\\nIf you want to contribute to the improving the site, back-end, front-end etc. Just fork away and submit pull requests.\\nSample Implementations\\nCheckout the static/ folder to see some of the sample implementations of Zippopotamus for inspiration and examples for how to implement Zippopotamus API for use in your website etc.\\nIf you want to share an implementation, we would love to post example cases of Zippopotamus on our homepage.\\nResponse Format\\nOn May 1st Zippopotamus changed their JSON response format to work better with international postal codes.  Now we support a one-to-many format service. That is that one zip code may map to many regions, this is common in countries like Spain and France (but not in the US and Germany).\\nPostal Code Information\\nFor information our postal codes and countries supported, you should check out the zippopotamus crowd-sourcing project.  Here you can download the entire database dump, or fork and add changes that we will incorporate into our DB.\\nTechnical Information\\nWhat is Zippopotamus built on\\nAt the moment the zippopotamus is built on Python, MongoDB and bottle.py framework.\\nLocal Testing?\\nThe site is configured to run on dotCloud, if you want to test out the web interfaceyou can change the wsgi.py file to include the last commented line, which is used to run the site on your local host.\\nSuggestions and Comments?\\nHate it? Love it? Open an issue if you have a problem or contact\\nJeff Crowell or Samir Ahmed\\nAlso, we aren't bottle or python or mongo experts. So if you see a way that we can improve, please let us know. Additionally, if you have examples (translation corrects etc) of using Zippopotamus that you want to share let us know and we can feature your site / blog on the homepage.\\n\",\n",
       "  'watchers': '12',\n",
       "  'stars': '105',\n",
       "  'forks': '56',\n",
       "  'commits': '44'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Twitty Twister\\nTwitty twister allows you to access twitter-compatible APIs from pure twisted\\napps. It supports standard and OAuth authentication.\\nSee the example/ directory\\nfor example commandline tools that exercise various parts of the API.\\nWhat are People Saying About It?\\nNot much, but I consider this an excellent review:\\n\\nA winning project name if ever I've read one.\\n\\n— Alex Payne\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '105',\n",
       "  'forks': '32',\n",
       "  'commits': '207'},\n",
       " {'language': 'Python 66.8',\n",
       "  'readme': \"Uploadr.py\\nUploadr.py is a simple Python script for uploading your photos to Flickr. Unlike\\nmany GUI applications out there, it lends itself to automation; and because it's\\nfree and open source, you can just change it if you don't like it.\\n\\nAuthentication\\nTo use this application, you need to obtain your own Flickr API key and secret\\nkey. You can apply for keys on the Flickr website.\\nWhen you have got those keys, you need to set environment variables so that they\\ncan be used by this application. For example, if you use Bash, add the following\\nlines to your $HOME/.bash_profile:\\nexport FLICKR_UPLOADR_PY_API_KEY=0123456789abcdef0123456789abcdef\\nexport FLICKR_UPLOADR_PY_SECRET=0123456789abcdef\\n\\n\\nLicense\\nUploadr.py consists of code by Cameron Mallory, Martin Kleppmann, Aaron Swartz and\\nothers. See COPYRIGHT for details.\\n\",\n",
       "  'watchers': '8',\n",
       "  'stars': '105',\n",
       "  'forks': '54',\n",
       "  'commits': '13'},\n",
       " {'language': 'Python 93.1',\n",
       "  'readme': '\\n\\n\\nDjango OAuth2 Server\\nImplementation of OAuth2 Server for Django. Feel free to fork this repository and contribute.\\nWritten for Django 1.9 :)\\n\\nGrant Types\\n\\nAuthorization Code\\nImplicit\\nClient Credentials\\nUser Credentials\\nRefresh Token\\n\\n\\nScope\\nAuthentication\\nContributing\\n\\nInstallation\\nConfiguration\\nRunning Tests\\n\\n\\n\\nGrant Types\\nAuthorization Code\\nhttp://tools.ietf.org/html/rfc6749#section-4.1\\nInsert test data:\\n$ python oauth2server/manage.py loaddata test_credentials\\n$ python oauth2server/manage.py loaddata test_scopes\\n\\nRun the development web server:\\n$ python oauth2server/manage.py runserver\\n\\nAnd you can now go to this page in your web browser:\\nhttp://localhost:8000/web/authorize/?response_type=code&client_id=testclient&redirect_uri=https://www.example.com&state=somestate\\n\\nYou should see a screen like this:\\n\\nClick yes, you will be redirected to the redirect_uri and the authorization code will be in the query string. For example:\\nhttps://www.example.com/?code=cd45169cf6575f76d789f55764cb751b4d08274d&state=somestate\\n\\nYou can use it to get access token:\\nhttp://tools.ietf.org/html/rfc6749#section-4.1.3\\n$ curl -u testclient:testpassword localhost:8080/api/v1/tokens/ -d \\'grant_type=authorization_code&code=cd45169cf6575f76d789f55764cb751b4d08274d\\'\\n\\nYou should get a response like:\\n{\\n    \"id\": 1,\\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\\n    \"expires_in\": 3600,\\n    \"token_type\": \"Bearer\",\\n    \"scope\": \"foo bar qux\",\\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\\n}\\nImplicit\\nhttp://tools.ietf.org/html/rfc6749#section-4.2\\nVery similar to the authorization code but the token is returned in URL fragment.\\nInsert test data:\\n$ python oauth2server/manage.py loaddata test_credentials\\n$ python oauth2server/manage.py loaddata test_scopes\\n\\nRun the development web server:\\n$ python oauth2server/manage.py runserver\\n\\nAnd you can now go to this page in your web browser:\\nhttp://localhost:8080/web/authorize/?response_type=token&client_id=testclient&redirect_uri=https://www.example.com&state=somestate\\n\\nYou should see a screen like this:\\n\\nClick yes, you will be redirected to the redirect_uri and the access token code will be in the URL fragment. For example:\\nhttps://www.example.com#access_token=66b80fb9d6630705bcea1c9be0df2a5f7f7a52bf&expires_in=3600&token_type=Bearer&state=somestate\\n\\nUser Credentials\\nhttp://tools.ietf.org/html/rfc6749#section-4.3\\nInsert test data:\\n$ python oauth2server/manage.py loaddata test_credentials\\n$ python oauth2server/manage.py loaddata test_scopes\\n\\nRun the development web server:\\n$ python oauth2server/manage.py runserver\\n\\nAnd you can now get a new access token:\\n$ curl -u testclient:testpassword localhost:8080/api/v1/tokens/ -d \\'grant_type=password&username=testuser@example.com&password=testpassword\\'\\n\\nYou should get a response like:\\n{\\n    \"id\": 1,\\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\\n    \"expires_in\": 3600,\\n    \"token_type\": \"Bearer\",\\n    \"scope\": \"foo bar qux\",\\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\\n}\\nClient Credentials\\nhttp://tools.ietf.org/html/rfc6749#section-4.4\\nInsert test data:\\n$ python oauth2server/manage.py loaddata test_credentials\\n$ python oauth2server/manage.py loaddata test_scopes\\n\\nRun the development web server:\\n$ python oauth2server/manage.py runserver\\n\\nAnd you can now get token either using HTTP Basic Authentication:\\n$ curl -u testclient:testpassword localhost:8080/api/v1/tokens/ -d \\'grant_type=client_credentials\\'\\n\\nOr using POST body:\\n$ curl localhost:8000/api/v1/tokens/ -d \\'grant_type=client_credentials&client_id=testclient&client_secret=testpassword\\'\\n\\nYou should get a response like:\\n{\\n    \"id\": 1,\\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\\n    \"expires_in\": 3600,\\n    \"token_type\": \"Bearer\",\\n    \"scope\": \"foo bar qux\",\\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\\n}\\nRefresh Token\\nLet\\'s say you have created a new access token using the user credentials grant type. The response included a refresh token which you can use to get a new access token before your current access token expires.\\n$ curl -u testclient:testpassword localhost:8080/api/v1/tokens/ -d \\'grant_type=refresh_token&refresh_token=55697efd4b74c980f2c638602556115bc14ca931\\'\\n\\nAnd you get a new access token:\\n{\\n    \"id\": 1,\\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\\n    \"expires_in\": 3600,\\n    \"token_type\": \"Bearer\",\\n    \"scope\": \"foo bar qux\",\\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\\n}\\nScope\\nhttp://tools.ietf.org/html/rfc6749#section-3.3\\nScope is quite arbitrary. Basically it is a space delimited case-sensitive string where each part defines a specific access range.\\nYou can define your scopes and insert them into tokens_oauthscope table, is_default flag can be used to specify default scope.\\nAuthentication\\nNow that you have obtained an access token, you can make requests to protected resources.\\nIn order to require authentication for a view, wrap it in the authentication_required decorator:\\nfrom apps.tokens.decorators import authentication_required\\n\\n@authentication_required(\"some_scope\")\\ndef some_view(request, *args, **kwargs):\\n    ...\\nContributing\\nIn order to contribute to this project, fork it and make a pull request. I will review and accept it.\\nAll tests must be passing in order for the pull request to be accepted.\\nInstallation\\nClone the repository:\\n$ git clone https://github.com/RichardKnop/django-oauth2-server.git\\n\\nCreate a virtual environment and install requirements:\\n$ virtualenv venv\\n$ source venv/bin/activate\\n$ pip install -r requirements.txt\\n\\nCreate a local.py file and insert correct configuration details:\\n$ cp oauth2server/proj/settings/local.example.py oauth2server/proj/settings/local.py\\n$ nano cp oauth2server/proj/settings/local.py\\n\\nSync the database:\\n$ python oauth2server/manage.py syncdb\\n\\nConfiguration\\nThese are the current configuration options:\\nOAUTH2_SERVER = {\\n    \\'ACCESS_TOKEN_LIFETIME\\': 3600,\\n    \\'AUTH_CODE_LIFETIME\\': 3600,\\n    \\'REFRESH_TOKEN_LIFETIME\\': 1209600,\\n    \\'IGNORE_CLIENT_REQUESTED_SCOPE\\': False,\\n}\\n\\nACCESS_TOKEN_LIFETIME: lifetime of an access token in seconds\\nAUTH_CODE_LIFETIME: lifetime of an authorization code in seconds\\nREFRESH_TOKEN_LIFETIME: lifetime of a refresh token in seconds\\nIGNORE_CLIENT_REQUESTED_SCOPE: if true, client requested scope will be ignored\\n\\nRunning Tests\\n$ python oauth2server/manage.py test\\n\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '105',\n",
       "  'forks': '18',\n",
       "  'commits': '47'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': '\\n\\n\\nDjango OAuth2 Server\\nImplementation of OAuth2 Server for Django. Feel free to fork this repository and contribute.\\nWritten for Django 1.9 :)\\n\\nGrant Types\\n\\nAuthorization Code\\nImplicit\\nClient Credentials\\nUser Credentials\\nRefresh Token\\n\\n\\nScope\\nAuthentication\\nContributing\\n\\nInstallation\\nConfiguration\\nRunning Tests\\n\\n\\n\\nGrant Types\\nAuthorization Code\\nhttp://tools.ietf.org/html/rfc6749#section-4.1\\nInsert test data:\\n$ python oauth2server/manage.py loaddata test_credentials\\n$ python oauth2server/manage.py loaddata test_scopes\\n\\nRun the development web server:\\n$ python oauth2server/manage.py runserver\\n\\nAnd you can now go to this page in your web browser:\\nhttp://localhost:8000/web/authorize/?response_type=code&client_id=testclient&redirect_uri=https://www.example.com&state=somestate\\n\\nYou should see a screen like this:\\n\\nClick yes, you will be redirected to the redirect_uri and the authorization code will be in the query string. For example:\\nhttps://www.example.com/?code=cd45169cf6575f76d789f55764cb751b4d08274d&state=somestate\\n\\nYou can use it to get access token:\\nhttp://tools.ietf.org/html/rfc6749#section-4.1.3\\n$ curl -u testclient:testpassword localhost:8080/api/v1/tokens/ -d \\'grant_type=authorization_code&code=cd45169cf6575f76d789f55764cb751b4d08274d\\'\\n\\nYou should get a response like:\\n{\\n    \"id\": 1,\\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\\n    \"expires_in\": 3600,\\n    \"token_type\": \"Bearer\",\\n    \"scope\": \"foo bar qux\",\\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\\n}\\nImplicit\\nhttp://tools.ietf.org/html/rfc6749#section-4.2\\nVery similar to the authorization code but the token is returned in URL fragment.\\nInsert test data:\\n$ python oauth2server/manage.py loaddata test_credentials\\n$ python oauth2server/manage.py loaddata test_scopes\\n\\nRun the development web server:\\n$ python oauth2server/manage.py runserver\\n\\nAnd you can now go to this page in your web browser:\\nhttp://localhost:8080/web/authorize/?response_type=token&client_id=testclient&redirect_uri=https://www.example.com&state=somestate\\n\\nYou should see a screen like this:\\n\\nClick yes, you will be redirected to the redirect_uri and the access token code will be in the URL fragment. For example:\\nhttps://www.example.com#access_token=66b80fb9d6630705bcea1c9be0df2a5f7f7a52bf&expires_in=3600&token_type=Bearer&state=somestate\\n\\nUser Credentials\\nhttp://tools.ietf.org/html/rfc6749#section-4.3\\nInsert test data:\\n$ python oauth2server/manage.py loaddata test_credentials\\n$ python oauth2server/manage.py loaddata test_scopes\\n\\nRun the development web server:\\n$ python oauth2server/manage.py runserver\\n\\nAnd you can now get a new access token:\\n$ curl -u testclient:testpassword localhost:8080/api/v1/tokens/ -d \\'grant_type=password&username=testuser@example.com&password=testpassword\\'\\n\\nYou should get a response like:\\n{\\n    \"id\": 1,\\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\\n    \"expires_in\": 3600,\\n    \"token_type\": \"Bearer\",\\n    \"scope\": \"foo bar qux\",\\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\\n}\\nClient Credentials\\nhttp://tools.ietf.org/html/rfc6749#section-4.4\\nInsert test data:\\n$ python oauth2server/manage.py loaddata test_credentials\\n$ python oauth2server/manage.py loaddata test_scopes\\n\\nRun the development web server:\\n$ python oauth2server/manage.py runserver\\n\\nAnd you can now get token either using HTTP Basic Authentication:\\n$ curl -u testclient:testpassword localhost:8080/api/v1/tokens/ -d \\'grant_type=client_credentials\\'\\n\\nOr using POST body:\\n$ curl localhost:8000/api/v1/tokens/ -d \\'grant_type=client_credentials&client_id=testclient&client_secret=testpassword\\'\\n\\nYou should get a response like:\\n{\\n    \"id\": 1,\\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\\n    \"expires_in\": 3600,\\n    \"token_type\": \"Bearer\",\\n    \"scope\": \"foo bar qux\",\\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\\n}\\nRefresh Token\\nLet\\'s say you have created a new access token using the user credentials grant type. The response included a refresh token which you can use to get a new access token before your current access token expires.\\n$ curl -u testclient:testpassword localhost:8080/api/v1/tokens/ -d \\'grant_type=refresh_token&refresh_token=55697efd4b74c980f2c638602556115bc14ca931\\'\\n\\nAnd you get a new access token:\\n{\\n    \"id\": 1,\\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\\n    \"expires_in\": 3600,\\n    \"token_type\": \"Bearer\",\\n    \"scope\": \"foo bar qux\",\\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\\n}\\nScope\\nhttp://tools.ietf.org/html/rfc6749#section-3.3\\nScope is quite arbitrary. Basically it is a space delimited case-sensitive string where each part defines a specific access range.\\nYou can define your scopes and insert them into tokens_oauthscope table, is_default flag can be used to specify default scope.\\nAuthentication\\nNow that you have obtained an access token, you can make requests to protected resources.\\nIn order to require authentication for a view, wrap it in the authentication_required decorator:\\nfrom apps.tokens.decorators import authentication_required\\n\\n@authentication_required(\"some_scope\")\\ndef some_view(request, *args, **kwargs):\\n    ...\\nContributing\\nIn order to contribute to this project, fork it and make a pull request. I will review and accept it.\\nAll tests must be passing in order for the pull request to be accepted.\\nInstallation\\nClone the repository:\\n$ git clone https://github.com/RichardKnop/django-oauth2-server.git\\n\\nCreate a virtual environment and install requirements:\\n$ virtualenv venv\\n$ source venv/bin/activate\\n$ pip install -r requirements.txt\\n\\nCreate a local.py file and insert correct configuration details:\\n$ cp oauth2server/proj/settings/local.example.py oauth2server/proj/settings/local.py\\n$ nano cp oauth2server/proj/settings/local.py\\n\\nSync the database:\\n$ python oauth2server/manage.py syncdb\\n\\nConfiguration\\nThese are the current configuration options:\\nOAUTH2_SERVER = {\\n    \\'ACCESS_TOKEN_LIFETIME\\': 3600,\\n    \\'AUTH_CODE_LIFETIME\\': 3600,\\n    \\'REFRESH_TOKEN_LIFETIME\\': 1209600,\\n    \\'IGNORE_CLIENT_REQUESTED_SCOPE\\': False,\\n}\\n\\nACCESS_TOKEN_LIFETIME: lifetime of an access token in seconds\\nAUTH_CODE_LIFETIME: lifetime of an authorization code in seconds\\nREFRESH_TOKEN_LIFETIME: lifetime of a refresh token in seconds\\nIGNORE_CLIENT_REQUESTED_SCOPE: if true, client requested scope will be ignored\\n\\nRunning Tests\\n$ python oauth2server/manage.py test\\n\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '105',\n",
       "  'forks': '19',\n",
       "  'commits': '228'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': '\\nCreated by Stephen McDonald\\n\\nIntroduction\\nA Django reusable app providing the overextends template tag, a\\ndrop-in replacement for Django\\'s extends tag, which allows you to\\nuse circular template inheritance.\\nThe primary use-case for overextends is to simultaneously override\\nand extend templates from other reusable apps, in your own Django project.\\n\\nExample\\nConsider the following settings module and templates, with the apps\\napp1 and app2 bundled in the project, for example\\'s sake:\\n# settings.py\\nINSTALLED_APPS = (\\n    \"app1\",\\n    \"app2\",\\n    \"overextends\",\\n)\\nTEMPLATE_LOADERS = (\\n    \"django.template.loaders.filesystem.Loader\",\\n    \"django.template.loaders.app_directories.Loader\",\\n)\\nPROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))\\nTEMPLATE_DIRS = (os.path.join(PROJECT_ROOT, \"templates\"),)\\n\\n<!-- myproject/app1/templates/pages/page.html -->\\n<h1>Title</h1>\\n{% block main %}\\n<p>A paragraph in app1</p>\\n{% enblock %}\\n<footer>Copyright 2012</footer>\\n\\n<!-- myproject/app2/templates/pages/page.html -->\\n{% overextends \"pages/page.html\" %}\\n{% block main %}\\n<p>A paragraph in app2, that wants to be on top of app1\\'s main block</p>\\n{{ block.super }}\\n{% enblock %}\\n\\n<!-- myproject/templates/pages/page.html -->\\n{% overextends \"pages/page.html\" %}\\n{% block main %}\\n{{ block.super }}\\n<p>A paragraph in the project\\'s template directory, under the other main blocks</p>\\n{% enblock %}\\n\\nThe resulting HTML rendered when pages/page.html was loaded would be:\\n<h1>Title</h1>\\n<p>A paragraph in app2, that wants to be on top of app1\\'s main block</p>\\n<p>A paragraph in app1</p>\\n<p>A paragraph in the project\\'s template directory, under the other main blocks</p>\\n<footer>Copyright 2012</footer>\\n\\nFor a detailed analysis of why you would use this approach, how it works,\\nand alternative approaches, read my initial blog post:\\nCircular Template Inheritance for Django\\n\\nInstallation\\nThe easiest way to install django-overextends is directly from PyPi\\nusing pip by running the following command:\\n$ pip install -U django-overextends\\n\\nOtherwise you can download django-overextends and install it directly\\nfrom source:\\n$ python setup.py install\\n\\n\\nProject Configuration\\nOnce installed you can configure your project to use\\ndjango-overextends by adding the overextends app to the\\nINSTALLED_APPS in your project\\'s settings module:\\nINSTALLED_APPS = (\\n    # ... other apps here ...\\n    \\'overextends\\',\\n)\\n\\nFor Django 1.9+ you must add overextends to the builtins key of your TEMPLATES setting:\\nTEMPLATES = [\\n    {\\n        \\'BACKEND\\': \\'django.template.backends.django.DjangoTemplates\\',\\n        \\'APP_DIRS\\': True,\\n        \\'OPTIONS\\': {\\n            \\'builtins\\': [\\'overextends.templatetags.overextends_tags\\'],\\n        }\\n    },\\n]\\n\\nNote that while the overextends tag is provided by the package\\noverextends.templatetags.overextends_tags, it is unnecessary to use\\n{% load overextends_tags %} in your templates. Like the extends\\ntag, overextends must be the first tag in your template, so it is\\nautomatically added to Django\\'s built-in template tags, removing the\\nneed to load its tag library in each template.\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '105',\n",
       "  'forks': '25',\n",
       "  'commits': '72'},\n",
       " {'language': 'Python 99.3',\n",
       "  'readme': 'Spans\\n\\n   \\n \\n \\n\\nSpans is a pure Python implementation of PostgreSQL\\'s\\nrange types.\\nRange types are convenient when working with intervals of any kind. Every time\\nyou\\'ve found yourself working with date_start and date_end, an interval may have\\nbeen what you were actually looking for.\\nSpans has successfully been used in production since its first release\\n30th August, 2013.\\n\\nInstallation\\nSpans exists on PyPI.\\n$ pip install Spans\\nDocumentation is hosted on Read the\\nDocs.\\n\\nExample\\nImagine you are building a calendar and want to display all weeks that overlaps\\nthe current month. Normally you have to do some date trickery to achieve this,\\nsince the month\\'s bounds may be any day of the week. With Spans\\' set-like\\noperations and shortcuts the problem becomes a breeze.\\nWe start by importing date and daterange\\n>>> from datetime import date\\n>>> from spans import daterange\\nUsing daterange.from_month we can get range representing January in the year\\n2000\\n>>> month = daterange.from_month(2000, 1)\\n>>> month\\ndaterange(datetime.date(2000, 1, 1), datetime.date(2000, 2, 1))\\nNow we can calculate the ranges for the weeks where the first and last day of\\nmonth are\\n>>> start_week = daterange.from_date(month.lower, period=\"week\")\\n>>> end_week = daterange.from_date(month.last, period=\"week\")\\n>>> start_week\\ndaterange(datetime.date(1999, 12, 27), datetime.date(2000, 1, 3))\\n>>> end_week\\ndaterange(datetime.date(2000, 1, 31), datetime.date(2000, 2, 7))\\nUsing a union we can express the calendar view.\\n>>> start_week.union(month).union(end_week)\\ndaterange(datetime.date(1999, 12, 27), datetime.date(2000, 2, 7))\\nDo you want to know more? Head over to the\\ndocumentation.\\n\\nUse with Psycopg2\\nTo use these range types with Psycopg2 the\\nPsycoSpans.\\n\\nMotivation\\nFor a project of mine I started using PostgreSQL\\'s tsrange type and needed\\nan equivalent in Python. These range types attempt to mimick PostgreSQL\\'s\\nbehavior in every way. Deviating from it is considered as a bug and should be\\nreported.\\n\\nContribute\\nI appreciate all the help I can get! Some things to think about:\\n\\nIf it\\'s a simple fix, such as documentation or trivial bug fix, please file\\nan issue or submit a pull request. Make sure to only touch lines relevant to\\nthe issue. I don\\'t accept pull requests that simply reformat the code to be\\nPEP8-compliant. To me the history of the repository is more important.\\nIf it\\'s a feature request or a non-trivial bug, always open an issue first to\\ndiscuss the matter. It would be a shame if good work went to waste because a\\npull request doesn\\'t fit the scope of this project.\\n\\nPull requests are credited in the change log which is displayed on PyPI and the\\ndocumentaion on Read the Docs.\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '105',\n",
       "  'forks': '9',\n",
       "  'commits': '105'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Instadrop\\nAutomatically sync your Instagram photos to Dropbox\\nA demo application of the Instagram real-time API.\\nIt's live! http://instadrop.appspot.com\\nInstallation on Google App Engine\\n\\nSign up for an App Engine account.\\nDownload the App Engine SDK.\\nFork and modify our code! Need help? Read the Google App Engine Getting Started Guide\\n\\nFollow @instagramapi on Twitter\\nYou should follow @instagramapi on Twitter for announcements,\\nupdates, and news about the Instagram API.\\nJoin the mailing list!\\nhttps://groups.google.com/group/instagram-api-developers\\nDid you fork this app to create something cool?\\nAdd it to the apps wiki!\\nContributing\\nIn the spirit of free software, everyone is encouraged to help improve this project.\\nHere are some ways you can contribute:\\n\\nby using alpha, beta, and prerelease versions\\nby reporting bugs\\nby suggesting new features\\nby writing or editing documentation\\nby writing specifications\\nby writing code (no patch is too small: fix typos, add comments, clean up inconsistent whitespace)\\nby refactoring code\\nby closing issues\\nby reviewing patches\\n\\nSubmitting an Issue\\nWe use the GitHub issue tracker to track bugs and\\nfeatures. Before submitting a bug report or feature request, check to make sure it hasn't already\\nbeen submitted. You can indicate support for an existing issuse by voting it up. When submitting a\\nbug report, please include a Gist that includes a stack trace and any\\ndetails that may be necessary to reproduce the bug, including your Python version and\\noperating system. Ideally, a bug report should include a pull request with failing specs.\\nSubmitting a Pull Request\\n\\nFork the project.\\nCreate a topic branch.\\nImplement your feature or bug fix.\\nAdd documentation for your feature or bug fix.\\nCommit and push your changes.\\nSubmit a pull request.\\n\\nCopyright\\nCopyright (c) 2011 Instagram (Burbn, Inc).\\nSee LICENSE for details.\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '105',\n",
       "  'forks': '25',\n",
       "  'commits': '2'},\n",
       " {'language': 'Java 91.1',\n",
       "  'readme': 'Big O\\nA big collection of data structures and algorithms puzzles.\\nAs on May 2013, the repository has roughly 130 problems with solutions.\\nSolutions are primarily in Java (under java/). Some solutions are in Ruby (under ruby/).\\nProblem statements are specified as Javadoc at the top of every solution. I have tried to be as descriptive as I can, but if something is not clear then please do let me know!\\nProblems are broadly classified into the following categories which are self explanatory.\\nPlease feel free to contribute!\\n\\narrays/\\nbinarytrees/\\nbitsandbytes/\\ncache/\\ncollections/\\nconcurrency/\\ndp/\\ngeneral/\\ngraphs/\\nlinkedlists/\\nsorting/\\nstrings/\\ntrees/\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '91',\n",
       "  'forks': '25',\n",
       "  'commits': '293'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '此项目已废弃\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '91',\n",
       "  'forks': '17',\n",
       "  'commits': '7'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'RecyclerViewSample\\n[Android]使用RecyclerView替代ListView（一）：http://www.cnblogs.com/tiantianbyconan/p/4232560.html\\n[Android]使用RecyclerView替代ListView（二）：http://www.cnblogs.com/tiantianbyconan/p/4242541.html\\n[Android]使用RecyclerView替代ListView（三）：http://www.cnblogs.com/tiantianbyconan/p/4268097.html\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '90',\n",
       "  'forks': '57',\n",
       "  'commits': '5'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'HipChat plugin for Jenkins\\nStarted with a fork of the Campfire plugin:\\nhttps://github.com/jgp/hudson_campfire_plugin\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '90',\n",
       "  'forks': '201',\n",
       "  'commits': '65'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Carma-Public/swagger-jaxrs-doclet has been doing more recent development on a fork of this project. Please consider checking that fork out first.\\nSwagger Doclet \\nA JavaDoc Doclet that can be used to generate a Swagger resource listing suitable for feeding to\\nswagger-ui.\\nUsage\\nTo use the Swagger Doclet in your Maven project, add the following to your POM file.\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\\n    <modelVersion>4.0.0</modelVersion>\\n\\n    <groupId>…</groupId>\\n    <artifactId>…</artifactId>\\n    <version>…</version>\\n    \\n    <dependencies>\\n        …\\n    </dependencies>\\n\\n    <build>\\n        <plugins>\\n            <plugin>\\n                <groupId>org.apache.maven.plugins</groupId>\\n                <artifactId>maven-javadoc-plugin</artifactId>\\n                <version>2.9.1</version>\\n                <executions>\\n                    <execution>\\n                        <id>generate-service-docs</id>\\n                        <phase>generate-resources</phase>\\n                        <configuration>\\n                            <doclet>com.hypnoticocelot.jaxrs.doclet.ServiceDoclet</doclet>\\n                            <docletArtifact>\\n                                <groupId>com.hypnoticocelot</groupId>\\n                                <artifactId>jaxrs-doclet</artifactId>\\n                                <version>0.0.4-SNAPSHOT</version>\\n                            </docletArtifact>\\n                            <reportOutputDirectory>${project.build.outputDirectory}</reportOutputDirectory>\\n                            <useStandardDocletOptions>false</useStandardDocletOptions>\\n                            <additionalparam>-apiVersion 1 -docBasePath /apidocs -apiBasePath /</additionalparam>\\n                        </configuration>\\n                        <goals>\\n                            <goal>javadoc</goal>\\n                        </goals>\\n                    </execution>\\n                </executions>\\n            </plugin>\\n        </plugins>\\n    </build>\\n</xml>\\nExample\\nAn example project using Dropwizard is included in jaxrs-doclet-sample-dropwizard. To get it running, run the following commands.\\n$ cd jaxrs-doclet-sample-dropwizard\\n$ mvn package\\n$ java -jar target/jaxrs-doclet-sample-dropwizard-0.0.4-SNAPSHOT.jar server sample.yml\\n\\nThe example server should be running on port 8080:\\n$ curl localhost:8080/apidocs/service.json\\n{\\n  \"apiVersion\" : \"1\",\\n  \"basePath\" : \"/apidocs/\",\\n  \"apis\" : [ {\\n    \"path\" : \"/Auth.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/HttpServletRequest.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/ModelResource_modelid.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/Recursive.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/Response.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/greetings_name.{format}\",\\n    \"description\" : \"\"\\n  } ],\\n  \"swaggerVersion\" : \"1.1\"\\n}\\n$\\n\\nOverride Swagger UI\\nTo override the swagger ui included with the doclet, create your own swagger-ui.zip file and add a swaggerUiZipPath to the additionalparam attribute in the pom file.\\n<additionalparam>-apiVersion 1 -docBasePath /apidocs -apiBasePath / -swaggerUiZipPath ../../../src/main/resources/swagger-ui.zip</additionalparam>\\n\\n',\n",
       "  'watchers': '20',\n",
       "  'stars': '90',\n",
       "  'forks': '139',\n",
       "  'commits': '162'},\n",
       " {'language': 'Java 68.7',\n",
       "  'readme': 'Spock for Android\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThis library allows for the Spock Framework mocks to\\nbe used on Android. As well as add some helpful features to make Android\\ntesting easier.\\n\\n\\n\\n\\nUsage\\n\\n\\nSetup Groovy For Android\\n\\n\\nbuildscript {\\n  repositories {\\n    jcenter()\\n  }\\n\\n  dependencies {\\n     classpath \\'com.android.tools.build:gradle:2.3.2\\'\\n     classpath \\'org.codehaus.groovy:groovy-android-gradle-plugin:1.2.0\\'\\n  }\\n}\\n\\napply plugin: \\'com.android.application\\'\\napply plugin: \\'groovyx.android\\'\\n\\n\\n\\nSee groovy-android-gradle-plugin for more\\ninformation on setting this plugin up.\\n\\n\\n\\nSetup Dependencies\\n\\n\\ndependencies {\\n  ...\\n  androidTestCompile \"com.andrewreitz:spock-android:2.0\"\\n  ...\\n}\\n\\n\\n\\n\\nSetup Android Plugin\\n\\n\\nandroid {\\n  ...\\n\\n  defaultConfig {\\n    ...\\n    testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\"\\n  }\\n\\n  packagingOptions {\\n    exclude \\'META-INF/LICENSE\\'\\n  }\\n}\\n\\n\\n\\n\\nWrite your tests\\n\\nTests must be placed in the ./src/androidTest/groovy directory.\\n\\n\\nWrite your tests like you would regular spock tests. See the spock-android-sample project and\\nSpock Framework for more details.\\n\\n\\n\\nMocking\\n\\nObjenesis and cglib do not work with Android. But that’s okay, using dexmaker we can still create\\nmock objects in spock fashion. The only difference is instead of your test classes inheriting from\\nSpecification, you need to inherit from AndroidSpecification.\\n\\n\\nNote: You can not use mocked automatic getters and setters. Example mocked.getName() will work\\nwhere as mocked.name will not. This is due to limitations of Android not having the java.bean\\npackage available.\\n\\n\\n\\nAnnotations\\n\\nUseActivity is an field annotation to get access to your Activity during tests. You can even\\nprovide bundle arguments by supplying a BundleCreator.\\n\\n\\nEx.\\n\\n\\n\\n@UseActivity(MyActivity) def myActivity\\n\\n\\n\\nUseApplication is a field annotation that supplies your Application.\\n\\n\\nEx.\\n\\n\\n\\n@UseApplication(MyApplication) def myApplication\\n\\n\\n\\nWithContext is a field annotation that supplies you with a context. This is not an implementation of\\nyour application.\\n\\n\\nEx.\\n\\n\\n\\n@WithContext def context\\n\\n\\n\\nAll field annotations will be set during the setup fixture.\\n\\n\\n\\nOther Notes\\n\\n\\n\\nAndroid Studio can not run tests with spaces in the name. This is due to a limitation\\nwith how it tells the Android device how to run them. To get around this you can\\nname tests without spaces. EX: \"this_is_my_test\" or just run them in gradle with gradle connectedAndroidTest.\\n\\n\\nSpock includes some references to Java packages that are not included in Android. You may need to use\\nlintOptions { disable \\'InvalidPackage\\' } to avoid lint failures.\\n\\n\\n\\n\\n\\n\\n\\nLicense\\n\\n\\n\\n\\nCopyright 2017 Andrew Reitz\\n\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\n\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\n\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '90',\n",
       "  'forks': '7',\n",
       "  'commits': '95'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Expandable-RecyclerView\\nA RecyclerView that behaves like an ExpandableListView, also includes a RecyclerView with header/footer support.\\nDownload\\nDownload the latest AAR via Maven:\\n<dependency>\\n  <groupId>com.levelupstudio</groupId>\\n  <artifactId>expandable-recyclerview</artifactId>\\n  <version>1.0.1</version>\\n</dependency>\\nor Gradle:\\ncompile \\'com.levelupstudio:expandable-recyclerview:1.0.1\\'\\nLicense\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '89',\n",
       "  'forks': '30',\n",
       "  'commits': '9'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'SpringStudy\\n对Spring框架的学习，包括官方文档的翻译，Spring框架的应用，Spring源码的剖析。。。\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '89',\n",
       "  'forks': '61',\n",
       "  'commits': '52'},\n",
       " {'language': 'Java 41.5',\n",
       "  'readme': 'fxosstub\\nBuilding an Open Web App on Firefox, step by step.\\n\\nAll explanations are found\\nhere\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '89',\n",
       "  'forks': '27',\n",
       "  'commits': '54'},\n",
       " {'language': 'C++ 62.8',\n",
       "  'readme': 'Important Notes\\nnode-webcl will be soon superseeded by https://github.com/mikeseven/node-opencl. Node-opencl is a reimplementation of node.js bindings to OpenCL:\\n\\ntrue wrapper of OpenCL, not an object-oriented model as WebCL. This makes it easier to develop higher-level API\\nsupports OpenCL 1.1, 1.2, 2.0\\nsupport for Nan2 and Node.JS 4\\nno dependency on external packages such as node-webgl, which was a huge issue to install for many users\\nLots of unit tests\\n\\nIntroduction\\nThis is an implementation of Khronos WebCL specification using NodeJS.\\nThis implementation solely relies on OpenCL 1.1 C headers.\\nDislaimer\\nWhile we are close to the WebCL specification, some features in this code may or may not be available in the final specification. As such, this implementation should be considered for\\n\\nexperimental development of WebCL/WebGL content,\\nexperimenting with new constructs that may not be available in browsers,\\ncoupled with Node.JS awesome capabilities, this software opens the door to non-browser applications (e.g. fast server-side image processing).\\n\\nThis implementation is not secure nor robust, we will update as the standard progresses in these areas. So beware that hacking your GPU may crash your computer; don\\'t blame us!\\nLicense\\nnode-webcl is distributed under BSD license\\nCopyright (c) 2011-2012, Motorola Mobility, Inc.\\nAll rights reserved.\\nSee LICENSES in this distribution for all licenses used in samples from other companies.\\nDependencies\\n\\n\\nNAN must be installed first to support all versions of v8\\n\\n\\nnode-webgl. This module is used for samples using WebGL interoperability with WebCL.\\nIn turns, node-webgl relies on node-glfw that relies on GLFW, GLEW, AntTweakBar, and FreeImage. See node-webgl and node-glfw for instructions on how to install these modules.\\n\\n\\nOpenCL 1.1 must be installed on your machine. Typically, this means your machine has a not too old graphic card (maybe not more than 3 years old) and its latest graphic drivers installed.\\n\\n\\nOn Mac, we recommend using OSX 10.7 \"Lion\" since OSX 10.6 \"Snow Leopard\" only supports OpenCL 1.0 and is buggy.\\nOn Windows, use Windows 7. Note that if your machine is 64-bit, you should use node.js 64-bit distribution, not the 32-bit default to avoid mismatch between node libraries and these native dependencies when node-gyp build the modules.\\nOn Linux, make sure you use the latest AMD or NVidia drivers. This module has been tested with Ubuntu 10.10, 11.04 and 11.10 64-bit.\\nPre-built binaries are available in submodule deps. Don\\'t forget to do:\\ngit submodule init\\ngit submodule udpate\\n\\nif you need these binaries.\\nInstallation\\nMake sure GLEW, GLFW, AntTweakBar, and FreeImage libraries are in your path.\\n\\n\\non Windows, put DLLs in Windows\\\\System32. Put headers in \\\\include and static librairies in \\\\lib for 32-bit libraries (if you use node.js in 32-bit) or \\\\lib\\\\x64 (if you use 64-bit node.js).\\n\\n\\non Mac, use homebrew\\nbrew install freeimage anttweakbar glfw3 glew\\n\\n\\non Linux use you package manager to install these libraries\\n\\n\\nNow install the usual way:\\nnpm install node-webcl\\n\\nthis will also install https://github.com/mikeseven/node-webgl, https://github.com/mikeseven/node-glfw, https://github.com/mikeseven/node-image, and https://github.com/rvagg/nan.\\nIf you want to use the latest code, retrieve each repo (node-webcl, node-webgl, node-glfw, and node-image) from github and simply do\\nnode-gyp rebuild\\nnpm link\\n\\nA crash course on WebCL\\nWebCL is a JavaScript representation of OpenCL 1.1. It is not a straight mapping of OpenCL C methods but rather an object-oriented representation of OpenCL object model. Knowledge of OpenCL is therefore mandatory to be able to develop WebCL programs. See the Books section and/or jump directly into Specifications references at the end of this page.\\nThere are few steps in creating a WebCL program:\\n\\nInit WebCL\\n\\nfind a suitable platform/device (cl.getPlatform(), cl.getDevices())\\ncreate a context (context = cl.createContext())\\ncreate a command queue (queue = context.createCommandQueue())\\ncompile a program (program = context.createProgram(), program.build())\\nset up a kernel and its arguments (kernel = program.createKernel(), kernel.setArg())\\nset up commands (queue.enqueueXXX())\\n\\n\\nRun computation\\n\\nset up kernel arguments (kernel.setArg())\\nset up commands (queue.enqueueXXX())\\nlaunch the computation (queue.enqueueTask() or queue.enqueueNDRange())\\n\\n\\n\\nWhen used with WebGL, WebGL context must be created first because WebCL context is created with sharing the WebGL context. Remember that WebCL allows computations on WebGL buffers. WebCL doesn\\'t do any rendering. By using WebCL and WebGL together, data remains in the device and this avoids expensive data transfer to/from CPU memory.\\nSo the sequence becomes:\\n\\ninit WebGL\\n\\ninit buffers\\ninit shaders\\ninit textures\\n\\n\\ninit WebCL (context = cl.createContext({ sharedObject: gl }))\\ninit shared objects (e.g. textures/array/pixel/render buffers)\\nlaunch WebGL rendering loop\\n\\n... execute GL commands\\nacquire GL objects (queue.enqueueAcquireGLObjects())\\nlaunch CL computation (queue.enqueueNDRange())\\nrelease GL objects (queue.enqueueReleaseGLObjects())\\n... execute GL commands\\n\\n\\n\\nReferences\\nSpecifications\\n\\nKhronos OpenCL specification, http://www.khronos.org/registry/cl/\\nKhronos WebCL working draft, https://cvs.khronos.org/svn/repos/registry/trunk/public/webcl/spec/latest/index.html\\nKhronos WebGL specification, http://www.khronos.org/webgl/\\n\\nBooks\\n\\nHeterogeneous Computing with OpenCL, Benedict Gaster, Lee Howes, David R. Kaeli and Perhaad Mistry, Morgan Kaufmann, August 2011\\nOpenCL Programming Guide by Aaftab Munshi, Benedict Gaster, Timothy G. Mattson and James Fung, Addison-Wesley Professional, July 2011\\nOpenCL in Action: How to Accelerate Graphics and Computations, Matthew Scarpino, Manning Publications, November 2011\\nThe OpenCL Programming Book, Ryoji Tsuchiyama, Takashi Nakamura, Takuro Iizuka and Akihiro Asahara, Fixstars Corporation, April 2010, http://www.fixstars.com/en/opencl/book/OpenCLProgrammingBook/contents.html\\nOpenCL Programming Guide for Mac OS X, http://developer.apple.com/library/mac/documentation/Performance/Conceptual/OpenCL_MacProgGuide/OpenCL_MacProgGuide.pdf\\n\\nOpenCL SDKs (use latest!)\\n\\nNVidia graphic cards: NVidia CUDA/OpenCL SDK, http://developer.nvidia.com/opencl\\nAMD/ATI graphics cards: AMD Accelerated Parallel Processing SDK, http://developer.amd.com/zones/openclzone/Pages/default.aspx\\nIntel CPUs: Intel OpenCL SDK, http://software.intel.com/en-us/articles/vcsource-tools-opencl-sdk/\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '63',\n",
       "  'forks': '11',\n",
       "  'commits': '286'},\n",
       " {'language': 'C++ 95.7',\n",
       "  'readme': 'TextureMovie Plugin for Unreal Engine 4\\nAdds the ability to import AVI/WMV videos and use them as textures in UE4. Currently only for Windows as it uses the Media Foundation as the backend for playing the videos. Currently working on a cross platform solution.\\n\\nCurrent version: v0.0.1\\nInstallation\\n\\nDownload the source code and place into /Engine/Plugins/Runtime directory (or your own projects plugin directory) and compile.\\nRun editor, you should now have the ability to import AVI/WMV videos.\\n\\nKnown Issues\\n\\nOption \"Reset on Last Frame\" currently does nothing.\\nThere is no way to stop or play the video during gameplay.\\n\\nLegal info\\nUnreal® is a trademark or registered trademark of Epic Games, Inc. in the United States of America and elsewhere.\\nUnreal® Engine, Copyright 1998 – 2014, Epic Games, Inc. All rights reserved.\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '63',\n",
       "  'forks': '43',\n",
       "  'commits': '6'},\n",
       " {'language': 'C++ 52.1',\n",
       "  'readme': 'TextureMovie Plugin for Unreal Engine 4\\nAdds the ability to import AVI/WMV videos and use them as textures in UE4. Currently only for Windows as it uses the Media Foundation as the backend for playing the videos. Currently working on a cross platform solution.\\n\\nCurrent version: v0.0.1\\nInstallation\\n\\nDownload the source code and place into /Engine/Plugins/Runtime directory (or your own projects plugin directory) and compile.\\nRun editor, you should now have the ability to import AVI/WMV videos.\\n\\nKnown Issues\\n\\nOption \"Reset on Last Frame\" currently does nothing.\\nThere is no way to stop or play the video during gameplay.\\n\\nLegal info\\nUnreal® is a trademark or registered trademark of Epic Games, Inc. in the United States of America and elsewhere.\\nUnreal® Engine, Copyright 1998 – 2014, Epic Games, Inc. All rights reserved.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '63',\n",
       "  'forks': '23',\n",
       "  'commits': '1'},\n",
       " {'language': 'C++ 71.4',\n",
       "  'readme': 'mangos-zero \\nmangos-zero is a full featured server, including authentication, client updates,\\nand world content serving compatible with the classic World of Warcaft\\nexperience for World of Warcraft Client Patch 1.12 - Drums of War.\\nmangos-zero is released under the GPL v2.  The file (LICENSE.mdown) must\\nbe a part of any redistributable packages made from this software.  No licenses\\nshould be removed from this software if you are making redistributable copies.\\nDevelopment\\nThe develop branch is where the development of mangos-zero server is done.\\nAny of the commits submitted here may or may not become part of the next\\nrelease.\\nIt is recommended to use the master branch for stable systems, and only use\\nthe develop branch if you intend to test commits and submit issues and/or\\nreports.\\nCompatibility\\nmangos-zero is compatible with scriptdev0 revision 464, and the game\\ncontent database 2012-07-15, and newer revisions.\\nAnd if something goes wrong?\\nIf you feel like submitting an issue, please do so only if you are willing\\nto provide a detailed report, and are available to verify any solution to the\\nissue provided by the developers of this repository.\\nCredits\\nMaNGOS has originally been written by Team Python and WoW Daemon Team. Many\\npeople further contributed to MaNGOS by reporting problems, suggesting various\\nimprovements or submitting actual code.\\nSpecial thanks should also go out to the WowwoW team. We have gained help from\\nthem many times in the creation of this project.\\nThanks should also go out to the Ludmilla team, who are also providing the\\ncommunity with a great server. We have not gained too much help from them,\\nbut we have recieved some.\\nWarning\\nThe Massive Network Game Object Server (MaNGOS) has been built with education\\nas the main purpose and the MaNGOS team would like to keep it that way.\\nSince any public and/or commercial use of this software is considered illegal\\nin many countries (please refer to your local law), the MaNGOS team will not\\nprovide any help nor support with such usage in any way.  Every user of this\\nsoftware is encouraged to make sure no law is being broken on his/her side.\\nBoth the MaNGOS team and MaNGOS foundation will not take any responsibility\\nfor any kind of usage of this software by the end users.\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '63',\n",
       "  'forks': '2',\n",
       "  'commits': '3,495'},\n",
       " {'language': 'C++ 82.0',\n",
       "  'readme': 'mangos-zero \\nmangos-zero is a full featured server, including authentication, client updates,\\nand world content serving compatible with the classic World of Warcaft\\nexperience for World of Warcraft Client Patch 1.12 - Drums of War.\\nmangos-zero is released under the GPL v2.  The file (LICENSE.mdown) must\\nbe a part of any redistributable packages made from this software.  No licenses\\nshould be removed from this software if you are making redistributable copies.\\nDevelopment\\nThe develop branch is where the development of mangos-zero server is done.\\nAny of the commits submitted here may or may not become part of the next\\nrelease.\\nIt is recommended to use the master branch for stable systems, and only use\\nthe develop branch if you intend to test commits and submit issues and/or\\nreports.\\nCompatibility\\nmangos-zero is compatible with scriptdev0 revision 464, and the game\\ncontent database 2012-07-15, and newer revisions.\\nAnd if something goes wrong?\\nIf you feel like submitting an issue, please do so only if you are willing\\nto provide a detailed report, and are available to verify any solution to the\\nissue provided by the developers of this repository.\\nCredits\\nMaNGOS has originally been written by Team Python and WoW Daemon Team. Many\\npeople further contributed to MaNGOS by reporting problems, suggesting various\\nimprovements or submitting actual code.\\nSpecial thanks should also go out to the WowwoW team. We have gained help from\\nthem many times in the creation of this project.\\nThanks should also go out to the Ludmilla team, who are also providing the\\ncommunity with a great server. We have not gained too much help from them,\\nbut we have recieved some.\\nWarning\\nThe Massive Network Game Object Server (MaNGOS) has been built with education\\nas the main purpose and the MaNGOS team would like to keep it that way.\\nSince any public and/or commercial use of this software is considered illegal\\nin many countries (please refer to your local law), the MaNGOS team will not\\nprovide any help nor support with such usage in any way.  Every user of this\\nsoftware is encouraged to make sure no law is being broken on his/her side.\\nBoth the MaNGOS team and MaNGOS foundation will not take any responsibility\\nfor any kind of usage of this software by the end users.\\n',\n",
       "  'watchers': '28',\n",
       "  'stars': '63',\n",
       "  'forks': '49',\n",
       "  'commits': '8,265'},\n",
       " {'language': 'C++ 80.8',\n",
       "  'readme': 'mangos-zero \\nmangos-zero is a full featured server, including authentication, client updates,\\nand world content serving compatible with the classic World of Warcaft\\nexperience for World of Warcraft Client Patch 1.12 - Drums of War.\\nmangos-zero is released under the GPL v2.  The file (LICENSE.mdown) must\\nbe a part of any redistributable packages made from this software.  No licenses\\nshould be removed from this software if you are making redistributable copies.\\nDevelopment\\nThe develop branch is where the development of mangos-zero server is done.\\nAny of the commits submitted here may or may not become part of the next\\nrelease.\\nIt is recommended to use the master branch for stable systems, and only use\\nthe develop branch if you intend to test commits and submit issues and/or\\nreports.\\nCompatibility\\nmangos-zero is compatible with scriptdev0 revision 464, and the game\\ncontent database 2012-07-15, and newer revisions.\\nAnd if something goes wrong?\\nIf you feel like submitting an issue, please do so only if you are willing\\nto provide a detailed report, and are available to verify any solution to the\\nissue provided by the developers of this repository.\\nCredits\\nMaNGOS has originally been written by Team Python and WoW Daemon Team. Many\\npeople further contributed to MaNGOS by reporting problems, suggesting various\\nimprovements or submitting actual code.\\nSpecial thanks should also go out to the WowwoW team. We have gained help from\\nthem many times in the creation of this project.\\nThanks should also go out to the Ludmilla team, who are also providing the\\ncommunity with a great server. We have not gained too much help from them,\\nbut we have recieved some.\\nWarning\\nThe Massive Network Game Object Server (MaNGOS) has been built with education\\nas the main purpose and the MaNGOS team would like to keep it that way.\\nSince any public and/or commercial use of this software is considered illegal\\nin many countries (please refer to your local law), the MaNGOS team will not\\nprovide any help nor support with such usage in any way.  Every user of this\\nsoftware is encouraged to make sure no law is being broken on his/her side.\\nBoth the MaNGOS team and MaNGOS foundation will not take any responsibility\\nfor any kind of usage of this software by the end users.\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '63',\n",
       "  'forks': '5',\n",
       "  'commits': '5'},\n",
       " {'language': 'C++ 70.3',\n",
       "  'readme': 'SCC — Simple C++\\n\\nSCC is C++  snippets evaluator at shell prompt\\n\\n\\nSee full docs at http://volnitsky.com/project/scc\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '62',\n",
       "  'forks': '4',\n",
       "  'commits': '843'},\n",
       " {'language': 'C++ 67.9',\n",
       "  'readme': 'UACElevator\\nPassive UAC elevation using dll infection\\nThe shellcode binary is in the shellcode[] char array in UACElevator.cpp and the raw code is in shellcode.asm\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '62',\n",
       "  'forks': '43',\n",
       "  'commits': '4'},\n",
       " {'language': 'C++ 77.7',\n",
       "  'readme': 'Grape is a stepping stone to building data processing systems on top of Elliptics routing and server-side code execution.\\nIts main goal is to provide an active example of elliptics data processing capabilities and also to provide ready-to-use building blocks for such systems.\\nGrape, as for now, consist of 2 components:\\n\\nfault-tolerant persistent queue\\nand a connector that allows to direct queue output into user application running on elliptics cluster (see event driver concept in Cocaine docs)\\n\\nQueue\\nQueue is a cocaine application running on elliptics node. Its deployment process follows general process for cocaine applications.\\nOnce deployed and started queue accepts data entries pushed into it, stores them among nodes of elliptics cluster its working on, and gives data entries back on consumer request, maintaining entries original order.\\nQueue supports fault-tolerance by using data replication and by implementing fault-replay mechanics: consumer must acknowledge processing status of every data entry that it retrieved from the queue - failing to do so will result in entry \"replay\", over and over again up until it\\'ll be confirmed.\\n(For further details about how this works internally see TODO: How queue works.)\\nAPI\\nQueue\\'s API basically consist of three methods: push, peek, ack:\\n\\npush pushes data entry to the top of the queue\\npeek gets data entry from the bottom of the queue\\nack confirms that entry has been processed and could be dropped\\n\\n(Latter two are combined in additional short-circuit method pop.)\\nThese methods are implemented in two sets: simple one operates in single queue entries and more complex one operates in multi-entry blocks.\\nqueue.push\\ndnet_id key;\\nsession->exec(&key, \"queue@push\", ioremap::elliptics::data_pointer::from_raw(\"abcd\")).wait();\\n\\nPushes data entry (\"abcd\") to the queue running under the base name \"queue\" at node responsible for the specified dnet_id.\\nThere is no multi-entry variant for this method.\\nqueue.peek\\ndnet_id key;\\nioremap::elliptics::exec_context context = session->exec(\\n        &key, \"queue@peek\", ioremap::elliptics::data_pointer()\\n        ).get_one().context();\\nioremap::elliptics::data_pointer entry_data = context.data();\\nioremap::grape::entry_id entry_id = ioremap::grape::entry_id::from_dnet_raw_id(context.src_id());\\n\\nPeeks data entry from the queue running under the base name \"queue\" at node responsible for the specified dnet_id.\\nReturns entry id embedded in src_id field of the response. Also returns queue\\'s supplemental subid in the src_key field (that subid makes possible to acknowledge entry back and thus must be preserved). Both fields are accessible through exec_context.\\n(Details of the TODO: request and response fields of the exec command explained separately.)\\nqueue.ack\\nsession->exec(context, \"queue@ack\", ioremap::elliptics::data_pointer()).wait();\\n\\nor equivalent:\\nsession->exec(context.src_id(), context.src_key(), \"queue@ack\", ioremap::elliptics::data_pointer()).wait();\\n\\nAcknowledges entry received by a previous peek.\\nEntry id must be sent embedded in dnet_id of the request. src_key must be set to that received by a previous peek.\\nqueue.peek-multi\\ndnet_id key;\\nioremap::elliptics::exec_context context = session->exec(\\n        &key, \"queue@peek-multi\", ioremap::elliptics::data_pointer(\"100\")\\n        ).get_one().context();\\nauto array = ioremap::grape::deserialize<ioremap::grape::data_array>(context.data());\\nioremap::elliptics::data_pointer d = array.data();\\nsize_t offset = 0;\\nfor (size_t i = 0; i < array.sizes().size(); ++i) {\\n    int bytesize = array.sizes()[i];\\n    // process data: (d.data() + offset, bytesize)\\n    offset += bytesize;\\n}\\n\\nPeeks multiple data entries from the queue running under the base name \"queue\" at node responsible for the specified dnet_id.\\nPeek-multi has an argument: hint about number of entries, which must be presented in a string form.\\nReturns serialized ioremap::grape::data_array structure which holds entries\\' data packed into byte array and array with entries\\' byte sizes and array with entries\\' ids.\\nioremap::grape::data_array is declared in a header file include/grape/data_array.hpp.\\nqueue.ack-multi\\nioremap::grape::data_array array = ...;\\nsession->exec(context, \"queue@ack-multi\", ioremap::grape::serialize(array.ids())).wait();\\n\\nAcknowledges entries received by a previous peek (may be several).\\nqueue.pop and queue.pop-multi\\nShort circuit methods pop and pop-multi has a combined effect of peek and ack called in one go. They are simple to use but also lose acking and replaying properties.\\nAdditional methods\\nQueue also implements few techical methods (in addition to common TODO: Cocaine and Elliptics app managment capabilities):\\n\\nping can be used to see if queue is currently active (or activate it for that matter)\\nstats shows internal state and statistics queue gathers about itself\\n\\nConfiguration\\nQueue reads its configuration from the file queue.conf. This file must be included in deployment tarball along with an app executable (see following section on Deployment).\\nqueue.conf must contain configuration for the elliptics client (used to return replies on inbound events) and can include queue configuration options.\\nThere is only one configuration option for now:\\n\\nchunk-max-size (int) - specifies how many entries will contain single chunk in the queue (default value: 10000)\\n\\nDeployment\\nDeployment process of the queue follows general process for cocaine applications. For launching the queue user needs three files:\\n\\nqueue application file (which is an executable)\\nqueue.conf config file (which is also a manifest file)\\nqueue.profile execution profile file\\n\\nqueue app could be taken from the binary package grape-components or built from the sources. Config and profile files also exist both in source repository and included in the same package.\\nHere we presume that user have installation of elliptics running on localhost:1025 in group 2 (how to do it see Elliptics: Server setup tutorial).\\nqueue.conf content:\\n{\\n    \"type\": \"binary\",\\n    \"slave\": \"queue\",\\n\\n    \"remotes\": [\\n        \"localhost:1025:2\"\\n     ],\\n    \"groups\": [2]\\n}\\n\\nqueue.profile content:\\n{\\n    \"heartbeat-timeout\" : 60,\\n    \"pool-limit\" : 1,\\n    \"queue-limit\" : 1000,\\n    \"grow-threshold\" : 1,\\n    \"concurrency\" : 10,\\n    \"idle-timeout\": 0\\n}\\n\\nSteps to launch a queue:\\n\\nCreate tarball with queue executable and config files:\\n\\ntar cvjf queue.tar.bz2 queue queue.conf\\n\\n\\nUpload tarball, manifest (same as config) and profile\\n\\ncocaine-tool -n queue -m queue.conf -p queue.tar.bz2 app:upload\\ncocaine-tool -n queue -m queue.profile profile:upload\\n\\n\\nDeploy the app (get it ready to run)\\n\\ndnet_ioclient -r localhost:1025:2 -g 2 -c \"queue@start-multiple-task local\"\\n\\n(More details about what these commands do exactly see in TODO: Cocaine: application deployment and TODO: Elliptics task management.)\\nNow queue is deployed (on every node that this elliptics installation includes, most possible that would be a single node here) and will actually start as soon as it\\'ll receive its first command (or event).\\nActivate the queue:\\ndnet_ioclient -r localhost:1025:2 -g 2 -c \"queue@ping\"\\n\\nQueue is up and running if reply would be:\\n127.0.0.1:1025: queue@ping \"ok\"\\n\\n\\nLinks:\\n\\nElliptics: http://www.reverbrain.com/elliptics/\\nCocaine: https://github.com/cocaine/cocaine-core\\nGoogle group: https://groups.google.com/forum/?fromgroups=#!forum/reverbrain\\n\\n',\n",
       "  'watchers': '15',\n",
       "  'stars': '62',\n",
       "  'forks': '8',\n",
       "  'commits': '386'},\n",
       " {'language': 'C++ 63.4',\n",
       "  'readme': 'ZeroNights2017 (c) 2017 James Forshaw\\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\\nWindows access tokens.\\nConsists of two parts.\\n\\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\\n\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '62',\n",
       "  'forks': '16',\n",
       "  'commits': '2'},\n",
       " {'language': 'JavaScript 64.7',\n",
       "  'readme': 'Stp.pediff\\n\\nA simple set of tools for visually comparing web pages built on top of\\nCasperjs and ImageMagick\\ncompare tool.\\n\\nView a sample report here.\\nTable of Contents\\n\\nHow it works\\nWhy to use it\\nDependencies\\nUsage\\nVerbose mode\\nReports\\nCoverage\\nMocks\\n\\nHow it works\\nBasically, Pediff executes a set of user defined tasks over two different versions of a website,\\ntakes screenshots at desired moments and scans the output for differences.\\nThen it generates human-friendly report containing all the inconsistencies ordered by\\nrelative number of differences. Optionally, it can check whether all the routes of your web application\\nare tested.\\nWhy to use it\\nPediff enables developers to detect entire class of visual problems invisible to\\nclassic unit tests and only occasionally catchable by manual review. For more details on the topic\\nsee this great talk by Brett Slatkin at Air Mozilla:\\nhttps://air.mozilla.org/continuous-delivery-at-google/\\nDependencies\\n\\nUnix-like operating system with Bash shell\\nCasperjs 1.1.0 or newer\\nImageMagick compare tool\\n\\nUsage\\n\\n\\nDownload the project to a place of your convenience\\n\\n\\nRename config.js-dist to config.js and set the environments values so that candidate key\\npoints to candidate version of your app and current key points to, well... current version of\\nyour app. The options object will be passed to Casperjs instance upon every Pediff run.\\n\\n\\nCreate as many task files as needed. They all must be placed inside the tasks/ subdirectory.\\nExample task file:\\nmodule.exports = {\\n    config: {\\n        path: \\'PATH/TO/RESOURCE\\'\\n        options: {\\n            viewportSize: [{width: 1440, height: 900}]\\n        },\\n        media: {\\n            print: false\\n        },\\n        package: \"homepage\"\\n    },\\n    execute: function(){\\n        this.save(\\'home\\');\\n    }\\n};\\nEvery task must contain two sections: config and execute. The config part will be merged\\nwith global config file before task execution. It serves the purpose of \"creating a sandbox\" for\\nthe task: subpage to open, a set of screen resolutions, user agent, media types,\\nresponse mocks - things like that.\\n###Some config keys explained:\\n\\npackage is the label under which all the output for given task will be grouped by in the report.\\nmedia is the key that enables you to force PhantomJS to render page as different media (ex.\\nprint)\\n\\nThe execute function will be run after the page has been loaded by Casperjs. Everything from\\nCasperjs API is acceptable, however you should use\\nsave(filename) instead of built in capture method for taking screenshots to make sure the\\nfiles are saved in correct directories for comparison.\\n\\n\\nAfter creating your tasks, type:\\n$ ./pediff.sh\\ninto terminal and wait for the tool to finish.\\nPediff runs every task in a subshell to speed things up. You can limit number of tasks\\nto be run parallelly by providing single positive integer as a parameter:\\n$ ./pediff.sh 4\\nThis way, tasks will be run in sets of 4 at a time.\\n\\n\\nAt this point index.html file should be sitting in project\\'s report/ subdirectory. Open it with your\\nbrowser and review results.\\n\\n\\nVerbose mode\\nWhile working on tasks it is useful to see what is going on during execution.\\nYou can force pediff to output more data by setting verbose option to true in your config.js\\nmodule.exports = {\\n    options: {\\n        verbose: true,\\n        ...\\nReports\\nPediff generates convenient reports by default. Just open report/index.html file with your browser.\\n\\nExample: Diff view between consecutive deployments of VGTV (difference in video impressions picked up).\\nReport\\'s anatomy\\nGeneral\\n\\nOn the left you can see a list of all the tasks executed on your page along with label marking\\nlevel of compatibility between the two versions. The list is sorted by number of differences\\n(most different first).\\nOptionally, you can toggle visibility of matching tasks (100% compability) with \"Show only\\ndiffering\" button below.\\nBy default, the tool renders lighter (and uglier) jpg images to reduce download times. You can change\\nthis behavior with the \"HQ\" button, which forces pediff to load much heavier png images.\\nIn the bottom left corner there\\'s \"Test coverage\" link. More on coverage.\\n\\nTask view\\n\\nIn the top right corner you can see a list of different screen resulutions that selected task was\\nexecuted on.\\nIn the central area of report the actual screenshots are displayed. You can switch between\\ndiff, current and candidate versions using both arrow keys and mouse clicks (left click forwards,\\nright click backwards).\\n\\nMocks\\nSometimes you may want to alter browser state by ensuring a request ends with a certain response.\\nYou can do that by providing mocks object to your task\\'s config section:\\n// ...\\nconfig: {\\n    options: {\\n        viewportSize: [{width: 1100, height: 2500}]\\n    },\\n    mocks: {\\'modernizer.js\\': \\'modernizr-notouch.js\\'}\\n}\\n// ...\\nwhere you map request to mock. Mocks must be located in mocks/ subdirectory.\\nCoverage\\nGeneral\\nThe tests coverage feature, allows one to easily check, whether all possible subpages of a project are being tested.\\nPediff does this by comparing a given set of routes, placed in routes.json, with test paths defined in individual tasks.\\nYou can disable the tests coverage feature by setting coverage property of pediff config file to false.\\n// ...\\ncoverage: {\\n    routes: \\'routes.json\\',\\n    skipOptional: true,\\n    exceptions: [\\n        {\\n            route: \\'\\',\\n            mapTo: \\'\\'\\n        }\\n    ]\\n}\\n// ...\\nCoverage configuration allows you to set a different path to the routes file (e.g. when this file is generated\\nautomatically), and define exceptions by explicitly mapping a path from routes definition to a path in a task file.\\nYou may also choose to force checking of routes\\' optional fragments by setting the skipOptional flag to false.\\nReport\\nResults of the test coverage check are presented in the pediff report, with an indication of tested viewport sizes,\\nmedia and files in which specific tasks are defined. Routes not tested with any tasks will be marked with red color,\\nand be included in the total coverage percentage.\\nExtending\\nCurrently coverage check is possible only for routes defined according to\\nBackbone.js routes specification. Support for custom routing setups can be\\nadded by extending coverage Route prototype (located in coverage/route.js), and specifically by overriding the\\nfollowing methods: buildVariants, isFragmentOptional, isFragmentVariable. More details can be found in comments\\nfor these methods.\\n',\n",
       "  'watchers': '28',\n",
       "  'stars': '157',\n",
       "  'forks': '7',\n",
       "  'commits': '67'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': '    \\nInline Manifest Webpack Plugin\\nThis is a webpack plugin that inline your manifest.js with a script tag to save http request. Cause webpack\\'s runtime always change between every build, it\\'s better to split the runtime code out for long-term caching.\\nInstallation\\nInstall the plugin with npm:\\n$ npm i inline-manifest-webpack-plugin -D\\nBasic Usage\\nThis plugin need to work with webpack v4 (for webpack v3 and below, pls use version 3) and HtmlWebpackPlugin v3 :\\nStep1: split out the runtime code\\n// the default name is \"runtime\"\\noptimization: {\\n    runtimeChunk: \\'single\\'\\n }\\n\\n// or specify another name\\noptimization: {\\n    runtimeChunk: {\\n        name: \\'another name\\'\\n    }\\n }\\nStep2: add plugins:\\n// this plugin need to put after HtmlWebpackPlugin\\n[\\n    new HtmlWebpackPlugin(),\\n    new InlineManifestWebpackPlugin()\\n]\\n\\nor\\n\\n[\\n    new HtmlWebpackPlugin(),\\n    // if you changed the runtimeChunk\\'s name, you need to sync it here\\n    new InlineManifestWebpackPlugin(\\'another name\\')\\n]\\nDone! This will replace the external script with inline code.\\nOne more thing\\nif you use inject: false in your HtmlWebpackPlugin, you can access the runtime code like this:\\n<%= htmlWebpackPlugin.files.runtime %>\\n\\n<% for (var chunk in htmlWebpackPlugin.files.chunks) { %>\\n<script src=\"<%= htmlWebpackPlugin.files.chunks[chunk].entry %>\"></script>\\n<% } %>\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '157',\n",
       "  'forks': '18',\n",
       "  'commits': '25'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Dockunit\\nContainerized unit testing across any platform and programming language.\\nPurpose\\nWe all want to test our applications on as many relevant platforms as possible. Sometimes this is easy.\\nSometimes it\\'s not. Dockunit let\\'s you define a set of Docker containers to run your tests against. You can run your\\ntest framework of choice in your language of choice on any type of environment. In the past many developers, myself\\nincluded, have relied on Travis CI to run tests in environments that aren\\'t setup locally (i.e. PHP 5.2). With\\nDockunit you don\\'t need to do this anymore.\\nRequirements\\n\\nOSX or a Linux Distribution (Windows not yet tested or officially supported)\\nNode.js\\nnpm\\nDocker\\n\\nInstallation\\n\\nMake sure you have Node.js, Docker, and npm install\\nInstall via npm:\\n\\nnpm install -g dockunit\\nUsage\\nDockunit relies on Dockunit.json files. Each of your projects should have their own Dockunit.json file.\\nDockunit.json defines what test commands should be run on what type of containers for any given project. Here is an\\nexample Dockunit.json:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"PHP 5.2 on Ubuntu\",\\n      \"image\": \"user/my-php-image\",\\n      \"beforeScripts\": [],\\n      \"testCommand\": \"phpunit\"\\n    },\\n    {\\n      \"prettyName\": \"PHP 5.6 FPM on Ubuntu\",\\n      \"image\": \"user/my-php-image2\",\\n      \"beforeScripts\": [],\\n      \"testCommand\": \"phpunit\"\\n    }\\n  ]\\n}\\ncontainers contains an array of container objects. Each container object can contain the following properties:\\n\\nprettyName (required) - This is used in output to help you identify your container.\\nimage (required) - This is a valid Docker container image located in the Docker registry. We have a number of handy prebuilt Docker images for use in your Dockunit.json files.\\nbeforeScripts (optional) - This is a string array of bash scripts to be run in order.\\ntestCommand (required) - This is the actual test command to be run on each container i.e. phpunit or qunit.\\n\\nThe Dockunit command is:\\ndockunit <path-to-project-directory> [--du-verbose] [--du-container] [--help] [--version] ...\\nNote: sudo is usually required when run within a Linux distribution since Dockunit runs Docker commands which require special permissions.\\n\\n<path-to-project-directory> (optional) - If you run dockunit in a folder with a Dockunit.json folder, it will detect it\\nautomatically.\\n[--du-verbose] (optional) - This will print out light verbose Dockunit output. [--du-verbose=2] will output even more verbose Dockunit output.\\n[--du-container] (optional) - Run only one container in your Dockunit.json file by specifying the index of that container in the containers array .i.e --du-container=1.\\n[--help] (optional) - This will display usage information for the dockunit command.\\n[--version] (optional) - This will display the current installed version of Dockunit.\\n... - Any additional arguments and options passed to the command will be passed to your test command. For example,\\nif you wanted to pass a few extra options to PHPUnit, you could append them to the end of your dockunit command.\\n\\nYou can simply run dockunit in any folder with a Dockunit.json to run Dockunit.\\nDockunit.json Examples\\nEach of your projects should have a Dockunit.json file in the project root. You should define your containers to fit\\nyour application\\'s unique needs. Here\\'s a few example Dockunit.json files for a variety of different programming languages and\\nenvironments. Feel free to use any of our prebuilt Docker images in your Dockunit.json files or create your own.\\nPHP and WordPress\\nDockunit and WordPress work well together. WordPress is backwards compatible with PHP 5.2. It\\'s very difficult to test\\napplications on PHP 5.2 without some sort of containerized workflow. Here is an example Dockunit.json file that you\\ncan use to test your WordPress plugins in PHP 5.2, 5.6, and PHP 7.0 RC 1 (make sure to replace PLUGIN-FILE.php with your plugins main file):\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"PHP-FPM 5.2 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-5.2-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp-install latest\"\\n      ],\\n      \"testCommand\": \"wp-activate-plugin PLUGIN-FILE.php\"\\n    },\\n    {\\n      \"prettyName\": \"PHP-FPM 5.6 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-5.6-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp core download --path=/temp/wp --allow-root\",\\n        \"wp core config --path=/temp/wp --dbname=test --dbuser=root --allow-root\",\\n        \"wp core install --url=http://localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=/temp/wp --allow-root\",\\n        \"mkdir /temp/wp/wp-content/plugins/test\",\\n        \"cp -r . /temp/wp/wp-content/plugins/test\"\\n      ],\\n      \"testCommand\": \"wp plugin activate test --allow-root --path=/temp/wp\"\\n    },\\n    {\\n      \"prettyName\": \"PHP-FPM 7.0 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-7.0-rc-1-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp core download --path=/temp/wp --allow-root\",\\n        \"wp core config --path=/temp/wp --dbname=test --dbuser=root --allow-root\",\\n        \"wp core install --url=http://localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=/temp/wp --allow-root\",\\n        \"mkdir /temp/wp/wp-content/plugins/test\",\\n        \"cp -r . /temp/wp/wp-content/plugins/test\"\\n      ],\\n      \"testCommand\": \"wp plugin activate test --allow-root --path=/temp/wp\"\\n    }\\n  ]\\n}\\nHere is an example Dockunit.json file that you can use to test your WordPress themes in PHP 5.2, 5.6, and PHP 7.0 RC 1:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"PHP-FPM 5.2 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-5.2-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp-install latest\"\\n      ],\\n      \"testCommand\": \"wp-activate-theme test\"\\n    },\\n    {\\n      \"prettyName\": \"PHP-FPM 5.6 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-5.6-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp core download --path=/temp/wp --allow-root\",\\n        \"wp core config --path=/temp/wp --dbname=test --dbuser=root --allow-root\",\\n        \"wp core install --url=http://localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=/temp/wp --allow-root\",\\n        \"mkdir /temp/wp/wp-content/themes/test\",\\n        \"cp -r . /temp/wp/wp-content/themes/test\"\\n      ],\\n      \"testCommand\": \"wp theme activate test --allow-root --path=/temp/wp\"\\n    },\\n    {\\n      \"prettyName\": \"PHP-FPM 7.0 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-7.0-rc-1-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp core download --path=/temp/wp --allow-root\",\\n        \"wp core config --path=/temp/wp --dbname=test --dbuser=root --allow-root\",\\n        \"wp core install --url=http://localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=/temp/wp --allow-root\",\\n        \"mkdir /temp/wp/wp-content/themes/test\",\\n        \"cp -r . /temp/wp/wp-content/themes/test\"\\n      ],\\n      \"testCommand\": \"wp theme activate test --allow-root --path=/temp/wp\"\\n    }\\n  ]\\n}\\nPHP and WordPress Unit Tests\\nHere are some more advanced WordPress examples. That assume you have unit tests setup via WP-CLI.\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"PHP 5.2 FPM WordPress 4.1\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-5.2-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"bash bin/install-wp-tests.sh wordpress_test root \\'\\' localhost 4.1\"\\n      ],\\n      \"testCommand\": \"phpunit\"\\n    },\\n    {\\n      \"prettyName\": \"PHP 5.6 FPM WordPress 4.0\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-5.6-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"bash bin/install-wp-tests.sh wordpress_test2 root \\'\\' localhost 4.0\"\\n      ],\\n      \"testCommand\": \"phpunit\"\\n    },\\n    {\\n      \"prettyName\": \"PHP 7.0 RC-1\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-7.0-rc-1-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"bash bin/install-wp-tests.sh wordpress_test3 root \\'\\' localhost 3.9\"\\n      ],\\n      \"testCommand\": \"phpunit\"\\n    }\\n  ]\\n}\\ndockunit/prebuilt-images:php-mysql-phpunit-5.6-fpm, dockunit/prebuilt-images:php-mysql-phpunit-5.6-fpm, and dockunit/prebuilt-images:php-mysql-phpunit-7.0-rc-1-fpm are valid Docker images available for use in any Dockerfile.json.\\nNode.js\\nIt is super easy to test your Node.js applications with Dockunit. Here is a simple Dockunit.json file that tests\\nan application in Node.js 0.10.x and 0.12.0 using mocha:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"Node 0.10.x\",\\n      \"image\": \"google/nodejs:latest\",\\n      \"beforeScripts\": [\\n        \"npm install -g mocha\"\\n      ],\\n      \"testCommand\": \"mocha\"\\n    },\\n    {\\n      \"prettyName\": \"Node 0.12\",\\n      \"image\": \"tlovett1/nodejs:0.12\",\\n      \"beforeScripts\": [\\n        \"npm install -g mocha\"\\n      ],\\n      \"testCommand\": \"mocha\"\\n    }\\n  ]\\n}\\ngoogle/nodejs is a valid Docker image available for use in any Dockerfile.json.\\nPython\\nDockunit works great with Python. This Dockunit.json file tests in Python 2.7.9 and the latest Python version using nose:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"Python 2.7.9\",\\n      \"image\": \"python:2.7.9\",\\n      \"beforeScripts\": [\\n        \"easy_install nose\"\\n      ],\\n      \"testCommand\": \"nosetests tests\"\\n    },\\n    {\\n      \"prettyName\": \"Python Latest\",\\n      \"image\": \"python:latest\",\\n      \"beforeScripts\": [\\n        \"easy_install nose\"\\n      ],\\n      \"testCommand\": \"nosetests tests\"\\n    }\\n  ]\\n}\\nRuby\\nYou can use Dockunit to test your Ruby scripts. This Dockunit.json file tests a project in Ruby 2.1 and the latest\\nstable Ruby version using test-unit:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"Latest version of Ruby\",\\n      \"image\": \"ruby:latest\",\\n      \"beforeScripts\": [\\n        \"bundle install\"\\n      ],\\n      \"testCommand\": \"bundle exec rake test\"\\n    },\\n    {\\n      \"prettyName\": \"Ruby version 2.1\",\\n      \"image\": \"ruby:2.1\",\\n      \"beforeScripts\": [\\n        \"bundle install\"\\n      ],\\n      \"testCommand\": \"bundle exec rake test\"\\n    }\\n  ]\\n}\\nruby is a valid Docker image available for use in any Dockerfile.json.\\nLicense\\nDockunit is free software; you can redistribute it and/or modify it under the terms of the GNU General\\nPublic License as published by the Free Software Foundation; either version\\n2 of the License, or (at your option) any later version.\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '157',\n",
       "  'forks': '11',\n",
       "  'commits': '118'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Dockunit\\nContainerized unit testing across any platform and programming language.\\nPurpose\\nWe all want to test our applications on as many relevant platforms as possible. Sometimes this is easy.\\nSometimes it\\'s not. Dockunit let\\'s you define a set of Docker containers to run your tests against. You can run your\\ntest framework of choice in your language of choice on any type of environment. In the past many developers, myself\\nincluded, have relied on Travis CI to run tests in environments that aren\\'t setup locally (i.e. PHP 5.2). With\\nDockunit you don\\'t need to do this anymore.\\nRequirements\\n\\nOSX or a Linux Distribution (Windows not yet tested or officially supported)\\nNode.js\\nnpm\\nDocker\\n\\nInstallation\\n\\nMake sure you have Node.js, Docker, and npm install\\nInstall via npm:\\n\\nnpm install -g dockunit\\nUsage\\nDockunit relies on Dockunit.json files. Each of your projects should have their own Dockunit.json file.\\nDockunit.json defines what test commands should be run on what type of containers for any given project. Here is an\\nexample Dockunit.json:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"PHP 5.2 on Ubuntu\",\\n      \"image\": \"user/my-php-image\",\\n      \"beforeScripts\": [],\\n      \"testCommand\": \"phpunit\"\\n    },\\n    {\\n      \"prettyName\": \"PHP 5.6 FPM on Ubuntu\",\\n      \"image\": \"user/my-php-image2\",\\n      \"beforeScripts\": [],\\n      \"testCommand\": \"phpunit\"\\n    }\\n  ]\\n}\\ncontainers contains an array of container objects. Each container object can contain the following properties:\\n\\nprettyName (required) - This is used in output to help you identify your container.\\nimage (required) - This is a valid Docker container image located in the Docker registry. We have a number of handy prebuilt Docker images for use in your Dockunit.json files.\\nbeforeScripts (optional) - This is a string array of bash scripts to be run in order.\\ntestCommand (required) - This is the actual test command to be run on each container i.e. phpunit or qunit.\\n\\nThe Dockunit command is:\\ndockunit <path-to-project-directory> [--du-verbose] [--du-container] [--help] [--version] ...\\nNote: sudo is usually required when run within a Linux distribution since Dockunit runs Docker commands which require special permissions.\\n\\n<path-to-project-directory> (optional) - If you run dockunit in a folder with a Dockunit.json folder, it will detect it\\nautomatically.\\n[--du-verbose] (optional) - This will print out light verbose Dockunit output. [--du-verbose=2] will output even more verbose Dockunit output.\\n[--du-container] (optional) - Run only one container in your Dockunit.json file by specifying the index of that container in the containers array .i.e --du-container=1.\\n[--help] (optional) - This will display usage information for the dockunit command.\\n[--version] (optional) - This will display the current installed version of Dockunit.\\n... - Any additional arguments and options passed to the command will be passed to your test command. For example,\\nif you wanted to pass a few extra options to PHPUnit, you could append them to the end of your dockunit command.\\n\\nYou can simply run dockunit in any folder with a Dockunit.json to run Dockunit.\\nDockunit.json Examples\\nEach of your projects should have a Dockunit.json file in the project root. You should define your containers to fit\\nyour application\\'s unique needs. Here\\'s a few example Dockunit.json files for a variety of different programming languages and\\nenvironments. Feel free to use any of our prebuilt Docker images in your Dockunit.json files or create your own.\\nPHP and WordPress\\nDockunit and WordPress work well together. WordPress is backwards compatible with PHP 5.2. It\\'s very difficult to test\\napplications on PHP 5.2 without some sort of containerized workflow. Here is an example Dockunit.json file that you\\ncan use to test your WordPress plugins in PHP 5.2, 5.6, and PHP 7.0 RC 1 (make sure to replace PLUGIN-FILE.php with your plugins main file):\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"PHP-FPM 5.2 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-5.2-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp-install latest\"\\n      ],\\n      \"testCommand\": \"wp-activate-plugin PLUGIN-FILE.php\"\\n    },\\n    {\\n      \"prettyName\": \"PHP-FPM 5.6 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-5.6-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp core download --path=/temp/wp --allow-root\",\\n        \"wp core config --path=/temp/wp --dbname=test --dbuser=root --allow-root\",\\n        \"wp core install --url=http://localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=/temp/wp --allow-root\",\\n        \"mkdir /temp/wp/wp-content/plugins/test\",\\n        \"cp -r . /temp/wp/wp-content/plugins/test\"\\n      ],\\n      \"testCommand\": \"wp plugin activate test --allow-root --path=/temp/wp\"\\n    },\\n    {\\n      \"prettyName\": \"PHP-FPM 7.0 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-7.0-rc-1-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp core download --path=/temp/wp --allow-root\",\\n        \"wp core config --path=/temp/wp --dbname=test --dbuser=root --allow-root\",\\n        \"wp core install --url=http://localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=/temp/wp --allow-root\",\\n        \"mkdir /temp/wp/wp-content/plugins/test\",\\n        \"cp -r . /temp/wp/wp-content/plugins/test\"\\n      ],\\n      \"testCommand\": \"wp plugin activate test --allow-root --path=/temp/wp\"\\n    }\\n  ]\\n}\\nHere is an example Dockunit.json file that you can use to test your WordPress themes in PHP 5.2, 5.6, and PHP 7.0 RC 1:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"PHP-FPM 5.2 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-5.2-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp-install latest\"\\n      ],\\n      \"testCommand\": \"wp-activate-theme test\"\\n    },\\n    {\\n      \"prettyName\": \"PHP-FPM 5.6 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-5.6-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp core download --path=/temp/wp --allow-root\",\\n        \"wp core config --path=/temp/wp --dbname=test --dbuser=root --allow-root\",\\n        \"wp core install --url=http://localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=/temp/wp --allow-root\",\\n        \"mkdir /temp/wp/wp-content/themes/test\",\\n        \"cp -r . /temp/wp/wp-content/themes/test\"\\n      ],\\n      \"testCommand\": \"wp theme activate test --allow-root --path=/temp/wp\"\\n    },\\n    {\\n      \"prettyName\": \"PHP-FPM 7.0 WordPress Latest\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-wordpress-7.0-rc-1-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"wp core download --path=/temp/wp --allow-root\",\\n        \"wp core config --path=/temp/wp --dbname=test --dbuser=root --allow-root\",\\n        \"wp core install --url=http://localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=/temp/wp --allow-root\",\\n        \"mkdir /temp/wp/wp-content/themes/test\",\\n        \"cp -r . /temp/wp/wp-content/themes/test\"\\n      ],\\n      \"testCommand\": \"wp theme activate test --allow-root --path=/temp/wp\"\\n    }\\n  ]\\n}\\nPHP and WordPress Unit Tests\\nHere are some more advanced WordPress examples. That assume you have unit tests setup via WP-CLI.\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"PHP 5.2 FPM WordPress 4.1\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-5.2-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"bash bin/install-wp-tests.sh wordpress_test root \\'\\' localhost 4.1\"\\n      ],\\n      \"testCommand\": \"phpunit\"\\n    },\\n    {\\n      \"prettyName\": \"PHP 5.6 FPM WordPress 4.0\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-5.6-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"bash bin/install-wp-tests.sh wordpress_test2 root \\'\\' localhost 4.0\"\\n      ],\\n      \"testCommand\": \"phpunit\"\\n    },\\n    {\\n      \"prettyName\": \"PHP 7.0 RC-1\",\\n      \"image\": \"dockunit/prebuilt-images:php-mysql-phpunit-7.0-rc-1-fpm\",\\n      \"beforeScripts\": [\\n        \"service mysql start\",\\n        \"bash bin/install-wp-tests.sh wordpress_test3 root \\'\\' localhost 3.9\"\\n      ],\\n      \"testCommand\": \"phpunit\"\\n    }\\n  ]\\n}\\ndockunit/prebuilt-images:php-mysql-phpunit-5.6-fpm, dockunit/prebuilt-images:php-mysql-phpunit-5.6-fpm, and dockunit/prebuilt-images:php-mysql-phpunit-7.0-rc-1-fpm are valid Docker images available for use in any Dockerfile.json.\\nNode.js\\nIt is super easy to test your Node.js applications with Dockunit. Here is a simple Dockunit.json file that tests\\nan application in Node.js 0.10.x and 0.12.0 using mocha:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"Node 0.10.x\",\\n      \"image\": \"google/nodejs:latest\",\\n      \"beforeScripts\": [\\n        \"npm install -g mocha\"\\n      ],\\n      \"testCommand\": \"mocha\"\\n    },\\n    {\\n      \"prettyName\": \"Node 0.12\",\\n      \"image\": \"tlovett1/nodejs:0.12\",\\n      \"beforeScripts\": [\\n        \"npm install -g mocha\"\\n      ],\\n      \"testCommand\": \"mocha\"\\n    }\\n  ]\\n}\\ngoogle/nodejs is a valid Docker image available for use in any Dockerfile.json.\\nPython\\nDockunit works great with Python. This Dockunit.json file tests in Python 2.7.9 and the latest Python version using nose:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"Python 2.7.9\",\\n      \"image\": \"python:2.7.9\",\\n      \"beforeScripts\": [\\n        \"easy_install nose\"\\n      ],\\n      \"testCommand\": \"nosetests tests\"\\n    },\\n    {\\n      \"prettyName\": \"Python Latest\",\\n      \"image\": \"python:latest\",\\n      \"beforeScripts\": [\\n        \"easy_install nose\"\\n      ],\\n      \"testCommand\": \"nosetests tests\"\\n    }\\n  ]\\n}\\nRuby\\nYou can use Dockunit to test your Ruby scripts. This Dockunit.json file tests a project in Ruby 2.1 and the latest\\nstable Ruby version using test-unit:\\n{\\n  \"containers\": [\\n    {\\n      \"prettyName\": \"Latest version of Ruby\",\\n      \"image\": \"ruby:latest\",\\n      \"beforeScripts\": [\\n        \"bundle install\"\\n      ],\\n      \"testCommand\": \"bundle exec rake test\"\\n    },\\n    {\\n      \"prettyName\": \"Ruby version 2.1\",\\n      \"image\": \"ruby:2.1\",\\n      \"beforeScripts\": [\\n        \"bundle install\"\\n      ],\\n      \"testCommand\": \"bundle exec rake test\"\\n    }\\n  ]\\n}\\nruby is a valid Docker image available for use in any Dockerfile.json.\\nLicense\\nDockunit is free software; you can redistribute it and/or modify it under the terms of the GNU General\\nPublic License as published by the Free Software Foundation; either version\\n2 of the License, or (at your option) any later version.\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '155',\n",
       "  'forks': '15',\n",
       "  'commits': '56'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"require-analyzer\\nDetermine dependencies for a given node.js file, directory tree, or module in code or on the command line\\nStatus\\n\\nInstallation\\nInstalling npm (node package manager)\\n  curl http://npmjs.org/install.sh | sh\\n\\nInstalling require-analyzer\\n  [sudo] npm install require-analyzer\\n\\nNOTE: If you're using npm >= 1.0 then you need to add the -g parameter to install require-analyzer globally.\\nUsage\\nThere are two distinct ways to use the require-analyzer library: from the command line or through code. The command line tool is designed to work with package.json files so make sure that you have created one for your project first. Checkout jitsu for a quick and easy way to create a package.json.\\nFor more information read our blog post at blog.nodejitsu.com.\\nCommand-line usage\\nUsing require-analyzer from the command line is easy. The binary will attempt to read the package.json file in the current directory, then analyze the dependencies and cross reference the result.\\n  $ require-analyzer --help\\n  usage: require-analyzer [options] [directory]\\n\\n  Analyzes the node.js requirements for the target directory. If no directory\\n  is supplied then the current directory is used\\n\\n  options:\\n    --update     Update versions for existing dependencies\\n    -h, --help   You're staring at it\\n\\nHere's a sample of require-analyzer analyzing its own dependencies:\\n  $ require-analyzer\\n  info:  require-analyzer starting in /Users/Charlie/Nodejitsu/require-analyzer\\n  warn:  No dependencies found\\n  info:  Analyzing dependencies...\\n  info:  Done analyzing raw dependencies\\n  info:  Retrieved packages from npm\\n  info:  Additional dependencies found\\n  data:  {\\n  data:    findit: '>= 0.0.3',\\n  data:    npm: '>= 0.3.18'\\n  data:  }\\n  info:  Updating /Users/Charlie/Nodejitsu/require-analyzer/package.json\\n  info:  require-analyzer updated package.json dependencies\\n\\nProgrammatic usage\\nThe easiest way to use require-analyzer programmatically is through the .analyze() method. This method will use fs.stat() on the path supplied and attempt one of three options:\\n\\nIf it is a directory that has a package.json, analyze require statements from package.main\\nIf it is a directory with no package.json analyze every .js or .coffee file in the directory tree\\nIf it is a file, then analyze require statements from that individual file.\\n\\nLets dive into a quick sample usage:\\n  var analyzer = require('require-analyzer');\\n\\n  var options = {\\n    target: 'path/to/your/dependency' // e.g /Users/some-user/your-package\\n    reduce: true\\n  };\\n\\n  var deps = analyzer.analyze(options, function (err, pkgs) {\\n    //\\n    // Log all packages that were discovered\\n    //\\n    console.dir(pkgs);\\n  });\\n\\n  //\\n  // The call the `.analyze()` returns an `EventEmitter` which outputs\\n  // data at various stages of the analysis operation.\\n  //\\n  deps.on('dependencies', function (raw) {\\n    //\\n    // Log the raw list of dependencies (no versions)\\n    //\\n    console.dir(raw);\\n  });\\n\\n  deps.on('search', function (pkgs) {\\n    //\\n    // Log the results from the npm search operation with the current\\n    // active version for each dependency\\n    //\\n    console.dir(pkgs);\\n  });\\n\\n  deps.on('reduce', function (reduced) {\\n    //\\n    // Logs the dependencies after they have been cross-referenced with\\n    // sibling dependencies. (i.e. if 'foo' requires 'bar', 'bar' will be removed).\\n    //\\n    console.dir(reduced);\\n  });\\nFurther analyzing dependencies\\nSometimes when dealing with dependencies it is necessary to further analyze the dependencies that are returned. require-analyzer has a convenience method for doing just this:\\n  var analyzer = require('require-analyzer');\\n\\n  var current = {\\n    'foo': '>= 0.1.0'\\n  };\\n\\n  var updated = {\\n    'foo': '>= 0.2.0',\\n    'bar': '>= 0.1.0'\\n  };\\n\\n  var updates = analyzer.updates(current, updated);\\n\\n  //\\n  // This will return an object literal with the differential\\n  // updates between the two sets of dependencies:\\n  //\\n  // {\\n  //   added: { 'bar': '>= 0.1.0' },\\n  //   updated: { 'foo': '>= 0.2.0' }\\n  // }\\n  //\\nTests\\n  npm test\\n\\nAuthor: Charlie Robbins\\n\",\n",
       "  'watchers': '12',\n",
       "  'stars': '155',\n",
       "  'forks': '10',\n",
       "  'commits': '206'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'walkabout.js\\nWalkabout.js is an automatic web application tester.  It randomly\\nsimulates a user.\\nThis code figures out what your application is paying attention to,\\nand does it.  It fills in fields with random values.  It finds the\\nevents you listen for and fires those events.  It finds internal links\\n(like <a href=\"#foo\">) and clicks them.  It\\'s a little like a fuzz\\ntester for your app.\\nIt has special support for jQuery, and uses source code rewriting to\\ndetect what your application is listening for in the absence of\\njQuery.  (jQuery leaves evidence on DOM nodes about what is being\\nlistened for.)\\nYou can use it like so (if you are using jQuery):\\n// This makes something like $(\\'#some-input\\').val() return random values:\\njQuery.fn.val.patch();\\n\\n// Now, fiddle around, do 100 random things:\\nWalkabout.runManyActions({\\n  times: 100\\n});\\nYou can also give Walkabout hints about what\\'s a valid input; this\\nlets it guess application-progressing values more often.\\nYou can add data-walkabout-disable=\"1\" to any element to suppress\\nactivation of that element or any of its children.\\nYou can use data-walkabout-eventname=\"...\" to set attributes on the\\nevent that is created, such as data-walkabout-keyup=\"{which: 13}\"\\nYou can use data-walkabout-options=\"[\\'a\\', \\'b\\']\" to give the valid inputs\\nfor a field.\\nYou can use data-walkabout-edit-value=\"type paste delete move\" to\\nhave Walkabout do edit operations inside a textarea or input type=text.  You give a space-separated list of the things to do: type\\na key at the cursor (and fire keyup), paste at the cursor (and fire\\npaste), delete the selection or delete before or after the cursor, or\\nmove the cursor around.\\nBookmarklet\\nIf you want to try walkabout.js on some random jQuery site, you can\\nuse the bookmarklet.  This will\\nload Walkabout and also start a simple UI to start the runs.  In\\naddition to starting a run it\\'ll also track any uncaught errors and\\nany calls to console.warn() or console.error().\\nNon-jQuery Support\\nThere\\'s some support for working with code that doesn\\'t use jQuery.\\nIt might work with other frameworks (by catching their calls to\\naddEventListener), but is more intended to work with code that\\ndoesn\\'t use a framework.  (Note: if you know equivalent ways to\\ndetect what other frameworks are listening for, patches to Walkabout\\nwould be welcome, or issues with details and examples that can be\\ntested on.)\\nThis technique uses code rewriting to capture calls to\\n.addEventListener() and .value.  The code can\\'t be weirdly tricky,\\nit needs to actually use those literal names -- which is usually fine\\nin \"normal\" code, but might not be in fancy or optimized code.  E.g.,\\nel[\"addEventListener\"](...) would not be found, nor would\\na=\"addEventListener\";el[a](...)\\nYou can use Walkabout.rewriteListeners(code) to rewrite the code.\\nThe code transformation looks like this:\\ndocument.getElementById(\"el\").addEventListener(\"click\", function () {}, true);\\nvar value = document.getElementById(\"textarea\").value;\\n\\n// Becomes:\\nWalkabout.addEventListener(document.getElementById(\"el\"), \"click\", function () {}, true);\\nvar value = Walkabout.value(document.getElementById(\"textarea\"));\\nAnd to find actions you use Walkabout.findActions(element).\\nSee Proxy Server for an option to use this.\\nOptions\\nBy default Walkabout will only go to links on the current page (i.e.,\\nhref=\"#something\").  This is because it will lose its context and\\npotentially not load on the next page.  But if you want Walkabout to\\nmove to other pages you can use Walkabout.options.anyLocalLinks = true\\nOr set it to something like \"/dir/\" which will only follow links\\nunder /dir/.\\nIf you use Walkabout.options.loadPersistent = true then it will save\\nthe state in localStorage and continue where it left of when another\\npage is loaded.\\n\\nProxy Server\\nAn application could include walkabout.js on its own, and if it\\ndoesn\\'t use jQuery it could also run Walkabout.rewriteListeners() on\\nall its code.  But maybe you don\\'t feel like doing that, maybe you\\nwant to try it out without all that work.  Also, by default Walkabout\\nwill try to stay on the current page, but in the proxy mode because it\\nknows Walkabout will load on each page, it will freely move about the\\nsite.\\nThere is a proxy server node-proxy.js that makes this easier.  As\\nyou might guess, you have to install Node.js to use it.\\nThe server expects to receive requests for the website you want to\\nactually access.  To do this you have to edit /etc/hosts to point\\nthe request locally, e.g.:\\n127.0.0.1 site-to-test.com\\n\\nThen when you access http://site-to-test.com it will connect\\nlocally, and the proxy server in turn will forward the request to the\\nactual server.  Any HTML responses will have the Walkabout Javascript\\nadded, and Javascript will be rewritten.\\nNote that it binds to port 80, and so you must run it as root.  This\\nisn\\'t awesome, pull requests to drop root welcome.  A tool like\\nauthbind also could help.\\nWhen you are done you should definitely stop the server, and undo the\\n/etc/hosts entry.\\nNote many live sites seem to notice the proxy, though I don\\'t know\\nhow.  My IP got blocked for a week from news.ycombinator.com after\\nusing it in my own testing.  Any ideas welcome.\\nYou can control the proxy with environmental variables: $PORT for\\nthe port to bind to (default 80), $BIND for the interface to bind to\\n(default 127.0.0.1; 0.0.0.0 means all interfaces), and $PORT_ALIASES\\nwhich looks like domain1:8088;domain2:8080 which lets you proxy from\\none port to another (helpful sometimes when you need to proxy to a\\nserver running locally).\\nLicense\\nThis is available under the\\nMozilla Public License or the GPL.\\nTo Do\\nLots of stuff, of course.  But:\\n\\n\\nSometimes the validation options (like data-walkabout-options)\\nshould be obeyed, and sometimes they should be ignored.  Obeying\\nthem progresses you through the site, disobeying them does some fuzz\\ntesting.\\n\\n\\nNot all form controls get triggered, I think.  E.g., checkboxes\\ndon\\'t get checked.\\n\\n\\n.val() should figure out better what\\'s a reasonable return value.\\nE.g., a textarea returns multi-line strings, a checkbox returns\\ntrue/false.\\n\\n\\nProbably .val() hacking should just go away, and only do \"real\"\\nediting of the fields.\\n\\n\\nThe generator could have smarts about what scripts (i.e., action\\nsequences) are successful and which are not.  Starting with a\\nsuccessful script (that progresses you through the application), and\\nthen tweaking that script and increasing randomness at the end of\\nthe script.\\n\\n\\nWe could look at code coverage as a score.  Or we could even just\\nlook at event coverage - hidden elements often have events bound,\\nbut we know they have to be visible to be triggered.  We\\'d like to\\nexplore paths where they are visible.\\n\\n\\nSomething more Gaussian with the values generated.  Like, sometimes\\nyou should make a 100 character input.  But maybe not as often as a\\n10 character input.  And sometimes a 1 character input.  The values\\nshould be biased towards \"fencepost\" values.  E.g., a bug that isn\\'t\\ntriggered when you enter 43 is unlike to be triggered by 42, but\\nmore likely with 0, -1, or 1000000.\\n\\n\\ndata-walkabout-options should be treated more as additional\\nsuggestions instead of the only suggestions.  They represent valid\\ninputs that will keep the application going, but invalid inputs are\\nstill sometimes interesting.  Still the given options should be\\ngenerally preferred when picking random options.\\n\\n\\nOnce a path has shown itself to get us to \"new\" code (via code\\ncoverage or that we see new action options) we might want to explore\\nthat specific code path further.  Maybe instead of a single random\\nnumber seed, we could have a sequence of seeds: [[30491, 10], 50492] meaning use seed 30491 for 10 numbers, then reseed,\\nexploring a different path but with the same start as a\\nknown-successful path.\\n\\n\\nA corollary of the above: we should have a signature for a set of\\nactions, so we can quickly see if a set is unique.\\n\\n\\nConfiguration should be possible externally, not just on attributes\\non the DOM.  This would be fairly simple stuff, like:\\nWalkabout.options.elements = {\\n  \"#name\": {\\n    suggest: \"Bob\"\\n  },\\n};\\n\\n\\n\\nSome way of indicating a bug-detector.  Like, you know in some\\ncircumstances some element doesn\\'t show when it should or something.\\nThese would be functions that would be run in-between actions.\\nSimple assertions might work okay, but some tests are expensive or\\ntricky.  But in cases when you don\\'t get an \"error\" but have noticed\\nthat invalid things can happen (without an exception) a detector\\nmight help create a reproducable case.  This could also be a kind of\\n\"pause when\" detector.\\n\\n\\nQuickCheck has this notion of making a test as small as possible --\\nbasically seeing if you can get to the same error case with a\\nsmaller set of actions.  This could be pretty straight forward for\\nWalkabout as well.  Though in cases like \"do something, cancel\\nsomething, do something again, do error thing\" it might be tricky:\\nwe want to trim the first two actions, but trimming either one alone\\nwon\\'t make the path possible.  So we have to look for chunks we can\\npull out.  With action signatures it might be easier to determine\\nsuch chunks.\\n\\n\\nWith code coverage it would be interesting to see which scripts had\\n\"changed\" by seeing if any of the code they touch \"changed\".  Then\\nthose scripts could be re-run.\\n\\n\\nMaking \"screencaps\" at various points would be interesting.  These\\nwould be a freeze of the visible DOM (BrowserMirror-style).  We\\ncould do them only when we\\'ve determined a path that is\\n\"interesting\" (i.e., maybe shows an error), by rerunning from the\\nstart.\\n\\n\\nHaving a \"reset app\" hook would be good, so we can truly rerun.\\n\\n\\nCombining signatures, we could try to determine if the app wasn\\'t\\ndeterministic despite our efforts.\\n\\n\\nWe should patch Math.random()\\n\\n\\nWe could consider timeouts an actionable thing.  We could keep our\\nown fake internal clock (mocking Date and setTimeout/setInterval),\\nand moving it ahead in a predictable way.  We could simulate\\nconditions like a computer going to sleep this way.\\n\\n\\nSimulate pagehide and pageview.\\n\\n\\nSimulate WebAPIs,\\nat least the non-deterministic ones.  This involves mocking each one\\nout (except event ones, which work kind of like other events).\\n\\n\\nMaybe hook up to\\nWebDriver so we\\ncan issue events that we aren\\'t generally allowed to issue.\\n.dispatchEvent() doesn\\'t work with keyboard events, for instance.\\nOTOH, we could also reimplement the event dispatch process.\\n\\n\\nPut in touch events on mobile, and suppress hover events.\\n\\n\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '155',\n",
       "  'forks': '9',\n",
       "  'commits': '42'},\n",
       " {'language': 'JavaScript 98.0',\n",
       "  'readme': 'Entwine – Support for Concrete UI style programming in jQuery\\nBy Hamish Friedlander, with thanks to SilverStripe\\nEntwine tries to provide a new model of code organisation – a replacement for Object Oriented programming that is\\nfocused on adding functions to groups of DOM elements based on the structure and contents of those DOM elements. It’s a\\nmerging of the model and view layer that initially seems weird, but can give very powerful results.\\nWe’re standing on the shoulders of giants here – combining ideas from Prototype’s behaviour & lowpro and jQuery’s effen\\n& livequery (who themselves stole ideals from Self’s Morphic UI and others), but extending & combining the concepts\\npresented in those tools to provide a complete alternative to traditional OO concepts – self-aware methods, inheritance,\\npolymorphisim and namespacing without a single class definition.\\nRequirements\\nCurrently Entwine layers itself on top of jQuery. Any patch level of jQuery from 1.7 through 1.11 or 2.0 through 2.1 should work.\\nBecause we patch some internal jQuery APIs there can be a delay between a new version of jQuery being released\\nand Entwine providing support.\\nGetting Started\\n\\nWalk through the Tutorial\\nWatch the Screencast (shot during a introductory developer meeting at SilverStripe)\\nJoin the Google Group and let us know what you think, or what other features you’d like to see\\n\\nName change\\njQuery Entwine used to be called jQuery Concrete. The name was changed to avoid confusion with another product. The concrete function remains as an alias, but all new code should use entwine\\nBasic use\\nFirst intro\\nTo attach methods to DOM nodes, call the `entwine` function on a jQuery selector object, passing a hash listing the method names and bodys\\n\\n  $(\\'div\\').entwine({\\n    foo: function(..){..},\\n    bar: function(..){..}\\n  });\\n\\nYou can then call those methods on any jQuery object.\\n\\n  $(\\'#a\\').foo();\\n\\nAny elements in the jQuery selection that match the selector used during definition (‘div’ in this example) will have foo called with that element\\nset as this. Any other objects are skipped. The return value will be the return value of foo() for the last matched DOM object in the set\\nA proper example\\nGiven this DOM structure:\\n\\n  <body>\\n    <div class=\"internal_text\">Internal text</div>\\n    <div class=\"attribute_text\" rel=\"Attribute text\"></div>\\n    <div>Nonsense</div>\\n  </body>\\n\\nAnd this entwine definition\\n\\n  $(\\'.internal_text\\').entwine({\\n    foo: function(){ console.log(this.text()); }\\n  });\\n  $(\\'.attribute_text\\').entwine({\\n    foo: function(){ console.log(this.attr(\\'rel\\')); }\\n  });\\n\\nThen this call\\n\\n  $(\\'div\\').foo();\\n\\nWill log this to the console\\n\\n  Internal text\\n  Attribute text  \\n\\nLimitations\\nWhen defining methods, the jQuery object that entwine is called on must be a plain selector, without context. These examples will not work\\n\\n  $(\\'div\\', el).entwine(...)\\n  $([ela, elb, elc]).entwine(...)\\n  $(\\'<div id=\"a\"></div>\\').entwine(...)\\n\\nLive\\nThe definitions you provide are not bound to the elements that match at definition time. You can declare behaviour prior to the DOM existing in any\\nform (i.e. prior to DOMReady) and later calls will function correctly.\\nSelector specifity\\nWhen there are two definitions for a particular method on a particular DOM node, the function with the most specific selector is used. \\nSpecifity is calculated as defined by the CSS 2/3 spec. This can be seen as subclassing applied to behaviour.\\nAnother example. Given this DOM structure\\n\\n  <body>\\n    <div>Internal text</div>\\n    <div class=\"attribute_text\" rel=\"Attribute text\"></div>\\n    <div>Nonsense</div>\\n  </body>\\n\\nAnd this entwine definition\\n\\n  $(\\'div\\').entwine({\\n    foo: function(){ console.log(this.text()); }\\n  });\\n  $(\\'.attribute_text\\').entwine({\\n    foo: function(){ console.log(this.attr(\\'rel\\')); }\\n  });\\n\\nThen this call\\n\\n  $(\\'div\\').foo();\\n\\nWill log this to the console\\n\\n  Internal text\\n  Attribute text\\n  Nonsense\\n\\nEvents\\nIf you declare a function with a name starting with ‘on’, then instead of defining that function, it will be bound to an event of that\\nname. Just like other functions this binding will be live, and only the most specific definition will be used\\n\\n  <head>\\n    <script type=\\'text/javascript\\'>\\n      /* No need for onready wrapper. Events are bound as needed */\\n      $(\\'div\\').entwine({\\n        onclick: function(){ this.css({backgroundColor: \\'blue\\'}); }\\n      });\\n      $(\\'.green\\').entwine({\\n        onclick: function(){ this.css({color: \\'green\\'}); }\\n      });\\n    </script>\\n  <body>\\n    <div>Background will turn blue when clicked on</div>\\n    <div>Will also have blue background when clicked on</div>\\n    <div class=\\'green\\'>Will have green text when clicked on. Background color will not change</div>\\n  </body>\\n\\nConstructors / Destructors\\nDeclaring a function with the name `onmatch` will create a behavior that is called on each object when it matches. Likewise, `onunmatch` will\\nbe called when an object that did match this selector stops matching it (because it is removed, or because you’ve changed its properties).\\nNote that an onunmatch block must be paired with an onmatch block – an onunmatch without an onmatch in the same entwine definition block is illegal\\nLike other functions, only the most specific definition will be used. However, because property changes are not atomic, this may not work as you\\nexpect.\\nNamespaces\\nTo avoid name clashes, to allow multiple bindings to the same event, and to generally seperate a set of functions from other code you can use namespaces\\n\\n  $.entwine(\\'foo.bar\\', function($){\\n    $(\\'div\\').entwine({\\n      baz: function(){}\\n    });\\n  });\\n\\nYou can then call these functions like this:\\n\\n  $(\\'div\\').entwine(\\'foo.bar\\').baz()\\n\\nNamespaced functions work just like regular functions (`this` is still set to a matching DOM Node). However, specifity is calculated per namespace.\\nThis is particularly useful for events, because given this:\\n\\n  $(\\'div\\').entwine({\\n    onclick: function(){ this.css({backgroundColor: \\'blue\\'}); }\\n  });\\n  \\n  $.entwine(\\'foo\\', function($){\\n    $(\\'div\\').entwine({\\n      onclick: function(){ this.css({color: \\'green\\'}); }\\n    });\\n  });\\n\\nClicking on a div will change the background and foreground color.\\nThis is particularly important when writing reusable code, since otherwise you can’t know before hand whether your event handler will be called or not\\nAlthough a namespace can be any string, best practise is to name them with dotted-identifier notation.\\nNamespaces and scope (or What the hell’s up with that ugly function closure)\\nInside a namespace definition, functions remember the namespace they are in, and calls to other functions will be looked up inside that namespace first. \\nWhere they don’t exist, they will be looked up in the base namespace\\n\\n  $.entwine(\\'foo\\', function($){\\n    $(\\'div\\').entwine({\\n      bar: function() { this.baz(); this.qux(); }\\n      baz: function() { console.log(\\'baz\\'); }\\n    })\\n  })\\n  \\n  $(\\'div\\').entwine({\\n    qux: function() { console.log(\\'qux\\'); }\\n  })\\n\\nWill print baz, qux to the console\\nNote that ‘exists’ means that a function is declared in this namespace for any selector, not just a matching one. Given the dom\\n\\n  <div>Internal text</div>\\n\\nAnd the entwine definitions\\n\\n  $.entwine(\\'foo\\', function($){\\n    $(\\'div\\').entwine({\\n      bar: function() { this.baz(); }\\n    });\\n    $(\\'span\\').entwine({\\n      baz: function() { console.log(\\'a\\'); }\\n    });\\n  })\\n  \\n  \\n  $(\\'div\\').entwine({\\n    baz: function() { console.log(\\'b\\'); }\\n  })\\n\\nThen doing $(‘div’).bar(); will not display b. Even though the span rule could never match a div, because baz is defined for some rule in the foo namespace, the base namespace will never be checked\\nNesting namespace blocks\\nYou can also nest declarations. In this next example, we’re defining the functions $().entwine(‘zap’).bar() and $().entwine(‘zap.pow’).baz()\\n\\n  $.entwine(\\'zap\\', function($){\\n    $(\\'div\\').entwine({\\n      bar: function() { .. }\\n    })\\n    $.entwine(\\'pow\\', function($){\\n      $(\\'div\\').entwine({\\n        baz: function() { .. }\\n      })\\n    })\\n  })\\n\\nCalling to another namespace (and forcing base)\\nInside a namespace, namespace lookups are by default relative to the current namespace.\\nIn some situations (such as the last example) you may want to force using the base namespace. In this case you can call entwine with the first argument being the base namespace code ‘.’. For example, if the first definition in the previous example was\\n\\n  $.entwine(\\'foo\\', function($){\\n    $(\\'div\\').entwine({\\n      bar: function() { this.entwine(\\'.\\').baz(); }\\n    })\\n  })\\n\\nThen b would be output to the console.\\nUsing\\nSometimes a block outside of a namespace will need to refer to that namespace repeatedly. By passing a function to the entwine function, you can change the looked-up namespace\\n\\n  $.entwine(\\'foo\\', function($){\\n    $(\\'div\\').entwine({\\n      bar: function() { console.log(\\'a\\'); }\\n    })\\n  })\\n  \\n  $(\\'div\\').entwine(\\'foo\\', function(){\\n    this.bar();\\n    this.bar();\\n    this.bar();\\n  });\\n\\nThis equivalent to /with/ in javascript, and just like /with/, care should be taken to only use this construct in situations that merit it.\\nTests\\nSpecs are written using the awesome JSpec library.  You can run them in two ways\\nAd-hoc\\nOpen the file `spec/spec.html` in any modern browser\\nContinuous testing\\nJSpec has a command line client which can be used for continuous testing. Make sure ruby is installed and enabled the Gemcutter gem hosting service, like so:\\n\\n\\tsudo gem install gemcutter\\n    sudo gem tumble\\n\\nThen install the jspec binary:\\n\\n    sudo gem install jspec\\n\\nThe JSpec command line tool should now be installed. This command will re-run the specs whenever you edit a file:\\n\\n    jspec spec/spec.html -p src,spec\\n\\nLicense\\nCopyright © 2009 Hamish Friedlander (hamish@silverstripe.com) and SilverStripe Limited (www.silverstripe.com). All rights reserved.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\nNeither the name of Hamish Friedlander nor SilverStripe nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '155',\n",
       "  'forks': '13',\n",
       "  'commits': '115'},\n",
       " {'language': 'JavaScript 80.7',\n",
       "  'readme': 'jquery.resizeend\\n\\n\\n\\n\\n\\n\\nA jQuery plugin that allows for window resize-end event handling.\\nInstallation\\nWith yarn\\nyarn add jquery.resizeend\\nWith npm\\nnpm install jquery.resizeend\\nIn the browser\\nReference your local script:\\n<script src=\"node_modules/jquery.resizeend/lib/jquery.resizeend.min.js\"></script>\\nOr load the script via jsdelivr:\\n<script src=\"https://cdn.jsdelivr.net/npm/jquery.resizeend@latest/lib/jquery.resizeend.min.js\"></script>\\nUsage\\n$(window).on(\\'resizeend\\', function(e) {\\n  // ...\\n});\\nContributing\\nFork the repo and clone locally, then run:\\nyarn install\\nThis will install the devDependencies packages and build the dist folder.\\nOnce you\\'ve made your desired changes, make sure to write any new tests for\\nyour feature and run the tests:\\nyarn run lint # lints js\\n\\nyarn test     # runs test suite\\nIf all tests pass, create a pull request.\\nLicense\\nThis plugin is licensed under the MIT license.\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '155',\n",
       "  'forks': '19',\n",
       "  'commits': '47'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'redux-await\\n\\n\\n\\n\\nManage async redux actions sanely\\nBreaking Changes!!\\nredux-await now takes control of a branch of your state/reducer tree similar to redux-form, and also like redux-form you need to use this module\\'s version of connect and not react-redux\\'s\\nInstall\\nnpm install --save redux-await\\nUsage\\nThis module exposes a middleware, reducer, and connector to take care of async state in a redux\\napp. You\\'ll need to:\\n\\n\\nApply the middleware:\\nimport { middleware as awaitMiddleware } from \\'redux-await\\';\\nlet createStoreWithMiddleware = applyMiddleware(\\n  awaitMiddleware\\n)(createStore);\\n\\n\\nInstall the reducer into the await path of your combineReducers\\nimport reducers from \\'./reducers\\';\\n\\n// old code\\n// const store = applyMiddleware(thunk)(createStore)(reducers);\\n\\n// new code\\nimport { reducer as awaitReducer } from \\'redux-await\\';\\nconst store = applyMiddleware(thunk, awaitMiddleware)(createStore)({\\n  ...reducers,\\n  await: awaitReducer,\\n});\\n\\n\\nUse the connect function from this module and not react-redux\\'s\\n// old code\\n// import { connect } from \\'react-redux\\';\\n\\n// new code\\nimport { connect } from \\'redux-await\\';\\n\\nclass FooPage extends Component {\\n  render() { /* ... */ }\\n}\\n\\nexport default connect(state => state.foo)(FooPage)\\n\\n\\nNow your action payloads can contain promises, you just need to add AWAIT_MARKER to the\\naction like this:\\n// old code\\n//export const getTodos = () => ({\\n//  type: GET_TODOS,\\n//  payload: {\\n//    loadedTodos: localStorage.todos,\\n//  },\\n//});\\n//export const addTodo = todo => ({\\n//  type: ADD_TODO,\\n//  payload: {\\n//    savedTodo: todo,\\n//  },\\n//});\\n\\n// new code\\nimport { AWAIT_MARKER } from \\'redux-await\\';\\nexport const getTodos = () => ({\\n  type: GET_TODOS,\\n  AWAIT_MARKER,\\n  payload: {\\n    loadedTodos: api.getTodos(), // returns promise\\n  },\\n});\\nexport const addTodo = todo => ({\\n  type: ADD_TODO,\\n  AWAIT_MARKER,\\n  payload: {\\n    savedTodo: api.saveTodo(todo), // returns promise\\n  },\\n});\\nNow your containers barely need to change:\\nclass Container extends Component {\\n  render() {\\n    const { todos, statuses, errors } = this.props;\\n\\n    // old code\\n    //return <div>\\n    //  <MyList data={todos} />\\n    //</div>;\\n\\n    // new code\\n    return <div>\\n      { statuses.loadedTodos === \\'pending\\' && <div>Loading...</div> }\\n      { statuses.loadedTodos === \\'success\\' && <MyList data={loadedTodos} /> }\\n      { statuses.loadedTodos.status === \\'failure\\' && <div>Oops: {errors.loadedTodos.message}</div> }\\n      { statuses.savedTodo === \\'pending\\' && <div>Saving new savedTodo</div> }\\n      { statuses.savedTodo === \\'failure\\' && <div>There was an error saving</div> }\\n    </div>;\\n  }\\n}\\n\\n//old code\\n// import { connect } from \\'react-redux\\';\\n\\n// new code\\nimport { connect } from \\'redux-await\\'; // it just spreads state.await on props\\n\\nexport default connect(state => state.todos)(Container)\\nWhy\\nRedux is mostly concerned about how to manage state in a synchronous setting. Async apps create\\nchallenges like keeping track of the async status and dealing with async errors.\\nWhile it is possible to build an app this way using\\nredux-thunk\\nand/or\\nredux-promise\\nit tends to bloat the app and it makes unit testing needlessly verbose\\nredux-await tries to solve all of these problems by keeping track of async payloads by means\\nof a middleware and a reducer keeping track of payload properties statuses. Let\\'s walk\\nthrough the development of a TODO app (App 1) that starts without any async and then needs to\\nstart converting action from sync to async. We\\'ll first try only using redux-thunk to solve\\nthis (App 2), and then see how to solve this with redux-await (App 3)\\nFor the first version of the app we\\'re going to store the todos in localStorage. Here\\'s a simple way we would do it:\\nApp1 demo\\nApp 1\\nimport React, { Component } from \\'react\\';\\nimport ReactDOM from \\'react-dom\\';\\nimport { Provider, connect } from \\'react-redux\\';\\nimport { applyMiddleware, createStore, combineReducers } from \\'redux\\';\\nimport thunk from \\'redux-thunk\\';\\nimport createLogger from \\'redux-logger\\';\\n\\nconst GET_TODOS = \\'GET_TODOS\\';\\nconst ADD_TODO = \\'ADD_TODO\\';\\nconst SAVE_APP = \\'SAVE_APP\\';\\nconst actions = {\\n  getTodos() {\\n    const todos = JSON.parse(localStorage.todos || \\'[]\\');\\n    return { type: GET_TODOS, payload: { todos } };\\n  },\\n  addTodo(todo) {\\n    return { type: ADD_TODO, payload: { todo } };\\n  },\\n  saveApp() {\\n    return (dispatch, getState) => {\\n      localStorage.todos = JSON.stringify(getState().todos.todos);\\n      dispatch({ type: SAVE_APP });\\n    }\\n  },\\n};\\nconst initialState = { isAppSynced: false, todos: [] };\\nconst todosReducer = (state = initialState, action = {}) => {\\n  if (action.type === GET_TODOS) {\\n    return { ...state, isAppSynced: true, todos: action.payload.todos };\\n  }\\n  if (action.type === ADD_TODO) {\\n    return { ...state, isAppSynced: false, todos: state.todos.concat(action.payload.todo) };\\n  }\\n  if (action.type === SAVE_APP) {\\n    return { ...state, isAppSynced: true };\\n  }\\n  return state;\\n};\\nconst reducer = combineReducers({\\n  todos: todosReducer,\\n})\\nconst store = applyMiddleware(thunk, createLogger())(createStore)(reducer);\\n\\nclass App extends Component {\\n  componentDidMount() {\\n    this.props.dispatch(actions.getTodos());\\n  }\\n  render() {\\n    const { dispatch, todos, isAppSynced } = this.props;\\n    const { input } = this.refs;\\n    return <div>\\n      {isAppSynced && \\'app is synced up\\'}\\n      <ul>{todos.map(todo => <li>{todo}</li>)}</ul>\\n      <input ref=\"input\" type=\"text\" onBlur={() => dispatch(actions.addTodo(input.value))} />\\n      <button onClick={() => dispatch(actions.saveApp())}>Sync</button>\\n      <br />\\n      <pre>{JSON.stringify(store.getState(), null, 2)}</pre>\\n    </div>;\\n  }\\n}\\nconst ConnectedApp = connect(state => state.todos)(App);\\n\\nReactDOM.render(<Provider store={store}><ConnectedApp /></Provider>, document.getElementById(\\'root\\'));\\nLooks cool (it\\'s a POC so it\\'s purposely minimal), but let\\'s say you want to start using an API\\nwhich is async to store the state, now your app will look something like App 2:\\nApp2 demo\\nApp 2\\nimport React, { Component } from \\'react\\';\\nimport ReactDOM from \\'react-dom\\';\\nimport { Provider, connect } from \\'react-redux\\';\\nimport { applyMiddleware, createStore, combineReducers } from \\'redux\\';\\nimport thunk from \\'redux-thunk\\';\\nimport createLogger from \\'redux-logger\\';\\n\\n// this not an API, this is a tribute\\nconst api = {\\n  save(data) {\\n    return new Promise(resolve => {\\n      setTimeout(() => {\\n        localStorage.todos = JSON.stringify(data);\\n        resolve(true);\\n      }, 2000);\\n    });\\n  },\\n  get() {\\n    return new Promise(resolve => {\\n      setTimeout(() => {\\n        resolve(JSON.parse(localStorage.todos || \\'[]\\'));\\n      }, 1000);\\n    });\\n  }\\n}\\n\\nconst GET_TODOS_PENDING = \\'GET_TODOS_PENDING\\';\\nconst GET_TODOS = \\'GET_TODOS\\';\\nconst GET_TODOS_ERROR = \\'GET_TODOS_ERROR\\';\\nconst ADD_TODO = \\'ADD_TODO\\';\\nconst SAVE_APP_PENDING = \\'SAVE_APP_PENDING\\'\\nconst SAVE_APP = \\'SAVE_APP\\';\\nconst SAVE_APP_ERROR = \\'SAVE_APP_ERROR\\';\\nconst actions = {\\n  getTodos() {\\n    return dispatch => {\\n      dispatch({ type: GET_TODOS_PENDING });\\n      api.get()\\n        .then(todos => dispatch({ type: GET_TODOS, payload: { todos } }))\\n        .catch(error => dispatch({ type: GET_TODOS_ERROR, payload: error, error: true }))\\n      ;\\n      ;\\n    }\\n  },\\n  addTodo(todo) {\\n    return { type: ADD_TODO, payload: { todo } };\\n  },\\n  saveApp() {\\n    return (dispatch, getState) => {\\n      dispatch({ type: SAVE_APP_PENDING });\\n      api.save(getState().todos.todos)\\n        .then(() => dispatch({ type: SAVE_APP }))\\n        .catch(error => dispatch({ type: SAVE_APP_ERROR, payload: error, error: true }))\\n      ;\\n    }\\n  },\\n};\\nconst initialState = {\\n  isAppSynced: false,\\n  isFetching: false,\\n  fetchingError: null,\\n  isSaving: false,\\n  savingError: null,\\n  todos: [],\\n};\\nconst todosReducer = (state = initialState, action = {}) => {\\n  if (action.type === GET_TODOS_PENDING) {\\n    return { ...state, isFetching: true, fetchingError: null };\\n  }\\n  if (action.type === GET_TODOS) {\\n    return {\\n      ...state,\\n      isAppSynced: true,\\n      isFetching: false,\\n      fetchingError: null,\\n      todos: action.payload.todos,\\n    };\\n  }\\n  if (action.type === GET_TODOS_ERROR) {\\n    return { ...state, isFetching: false, fetchingError: action.payload.message };\\n  }\\n  if (action.type === ADD_TODO) {\\n    return { ...state, isAppSynced: false, todos: state.todos.concat(action.payload.todo) };\\n  }\\n  if (action.type === SAVE_APP_PENDING) {\\n    return { ...state, isSaving: true, savingError: null };\\n  }\\n  if (action.type === SAVE_APP) {\\n    return { ...state, isAppSynced: true, isSaving: false, savingError: null };\\n  }\\n  if (action === SAVE_APP_ERROR) {\\n    return { ...state, isSaving: false, savingError: action.payload.message }\\n  }\\n  return state;\\n};\\nconst reducer = combineReducers({\\n  todos: todosReducer,\\n})\\nconst store = applyMiddleware(thunk, createLogger())(createStore)(reducer);\\n\\nclass App extends Component {\\n  componentDidMount() {\\n    this.props.dispatch(actions.getTodos());\\n  }\\n  render() {\\n    const { dispatch, todos, isAppSynced, isFetching, fetchingError, isSaving, savingError } = this.props;\\n    const { input } = this.refs;\\n    return <div>\\n      {isAppSynced && \\'app is synced up\\'}\\n      {isFetching && \\'getting todos\\'}\\n      {fetchingError && \\'there was an error getting todos: \\' + fetchingError}\\n      {isSaving && \\'saving todos\\'}\\n      {savingError && \\'there was an error saving todos: \\' + savingError}\\n      <ul>{todos.map(todo => <li>{todo}</li>)}</ul>\\n      <input ref=\"input\" type=\"text\" onBlur={() => dispatch(actions.addTodo(input.value))} />\\n      <button onClick={() => dispatch(actions.saveApp())}>Sync</button>\\n      <br />\\n      <pre>{JSON.stringify(store.getState(), null, 2)}</pre>\\n    </div>;\\n  }\\n}\\n\\nconst ConnectedApp = connect(state => state.todos)(App);\\n\\nReactDOM.render(<Provider store={store}><ConnectedApp /></Provider>, document.getElementById(\\'root\\'));\\nAs you can see there\\'s a lot of async logic and state we don\\'t want to have to deal with.\\nThis is 62 more LOC than the first version. Here\\'s how you would do it in App 3 with\\nredux-await:\\nApp3 demo\\nApp 3\\nimport React, { Component } from \\'react\\';\\nimport ReactDOM from \\'react-dom\\';\\nimport { Provider } from \\'react-redux\\';\\nimport { applyMiddleware, createStore, combineReducers } from \\'redux\\';\\nimport thunk from \\'redux-thunk\\';\\nimport createLogger from \\'redux-logger\\';\\nimport {\\n  AWAIT_MARKER,\\n  createReducer,\\n  connect,\\n  reducer as awaitReducer,\\n  middleware as awaitMiddleware,\\n} from \\'redux-await\\';\\n\\n// this not an API, this is a tribute\\nconst api = {\\n  save(data) {\\n    return new Promise(resolve => {\\n      setTimeout(() => {\\n        localStorage.todos = JSON.stringify(data);\\n        resolve(true);\\n      }, 2000);\\n    });\\n  },\\n  get() {\\n    return new Promise(resolve => {\\n      setTimeout(() => {\\n        resolve(JSON.parse(localStorage.todos || \\'[]\\'));\\n      }, 1000);\\n    });\\n  }\\n}\\n\\nconst GET_TODOS = \\'GET_TODOS\\';\\nconst ADD_TODO = \\'ADD_TODO\\';\\nconst SAVE_APP = \\'SAVE_APP\\';\\nconst actions = {\\n  getTodos() {\\n    return {\\n      type: GET_TODOS,\\n      AWAIT_MARKER,\\n      payload: {\\n        todos: api.get(),\\n      },\\n    };\\n  },\\n  addTodo(todo) {\\n    return { type: ADD_TODO, payload: { todo } };\\n  },\\n  saveApp() {\\n    return (dispatch, getState) => {\\n      dispatch({\\n        type: SAVE_APP,\\n        AWAIT_MARKER,\\n        payload: {\\n          save: api.save(getState().todos.todos),\\n        },\\n      });\\n    }\\n  },\\n};\\nconst initialState = { isAppSynced: false, todos: [] };\\nconst todosReducer = (state = initialState, action = {}) => {\\n  if (action.type === GET_TODOS) {\\n    return { ...state, isAppSynced: true, todos: action.payload.todos };\\n  }\\n  if (action.type === ADD_TODO) {\\n    return { ...state, isAppSynced: false, todos: state.todos.concat(action.payload.todo) };\\n  }\\n  if (action.type === SAVE_APP) {\\n    return { ...state, isAppSynced: true };\\n  }\\n  return state;\\n};\\nconst reducer = combineReducers({\\n  todos: todosReducer,\\n  await: awaitReducer,\\n})\\n\\nconst store = applyMiddleware(thunk, awaitMiddleware, createLogger())(createStore)(reducer);\\n\\nclass App extends Component {\\n  componentDidMount() {\\n    this.props.dispatch(actions.getTodos());\\n  }\\n  render() {\\n    const { dispatch, todos, isAppSynced, statuses, errors } = this.props;\\n    const { input } = this.refs;\\n    return <div>\\n      {isAppSynced && \\'app is synced up\\'}\\n      {statuses.todos === \\'pending\\' && \\'getting todos\\'}\\n      {statuses.todos === \\'failure\\' && \\'there was an error getting todos: \\' + errors.todos.message}\\n      {statuses.save === \\'pending\\' && \\'saving todos\\'}\\n      {errors.save && \\'there was an error saving todos: \\' + errors.save.message}\\n      <ul>{todos.map(todo => <li>{todo}</li>)}</ul>\\n      <input ref=\"input\" type=\"text\" onBlur={() => dispatch(actions.addTodo(input.value))} />\\n      <button onClick={() => dispatch(actions.saveApp())}>Sync</button>\\n      <br />\\n      <pre>{JSON.stringify(store.getState(), null, 2)}</pre>\\n    </div>;\\n  }\\n}\\n\\n\\nconst ConnectedApp = connect(state => state.todos)(App);\\n\\nReactDOM.render(<Provider store={store}><ConnectedApp /></Provider>, document.getElementById(\\'root\\'));\\nThis version is very easy to reason about, in fact you can completely ignore the fact that the app is async at all. The todosReducer didn\\'t need to have a single line changed!\\nNote that this is 107 LOC compared to app2\\'s 125 LOC\\nSome pitfalls to watch out for\\nYou must either use this modules connect or manually spread the await part of the tree over\\nmapStateToProps, you can also choose to name it something other than await and spread that\\nyourself too.\\nredux-await will name the statuses and errors prop the same as the payload prop so try to be\\nas descriptive as possible when naming payload props since any payload props collision will\\noverwrite the statuses/errors value. For a CRUD app don\\'t always name it something like\\nrecords because when you\\'re loading users.records the app will also think you\\'re loading\\ntodos.records\\nHow it works:\\nThe middleware checks to see if the AWAIT_MARKER was set on the action\\nand if it was then dispatches three events with a [AWAIT_META_CONTAINER]\\nproperty on the meta property of the action.\\nThe reducer listens for actions with a meta of [AWAIT_META_CONTAINER] and\\nwhen found will set the await property of the state accordingly.\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '154',\n",
       "  'forks': '4',\n",
       "  'commits': '38'},\n",
       " {'language': 'JavaScript 99.4',\n",
       "  'readme': 'Conductor.js\\n\\nConductor.js is a library for creating sandboxed, re-usable apps that\\ncan be embedded inside a host application.\\nThe advantage of using Conductor.js over a standard <iframe> is that\\nit uses a well-defined set of events that allow the app and its host\\nenvironment to communicate. Because of this, apps from different vendors\\ncan be embedded securely, yet still interact in meaningful ways.\\nUnderstanding Conductor.js\\nTo understand the benefit of architecting your application using\\nConductor, let\\'s look at an example use case.\\nImagine that you are authoring a next-generation banking web\\napplication. When your customers visit their account online, you would\\nlike to present them with a list of their most recent transactions:\\n\\nThis is fine, but is the same as every other bank. You, being a\\nnext-generation, technology-driven bank, want to make this data more\\nmeaningful to your customers. Most transactions have some metadata\\nassociated with them. What if you could allow merchants to customize how\\ntheir transactions appeared?\\n\\nObviously, you could write custom code for each merchant that might have\\na transaction go through your bank. But there are many merchants in the\\nworld, and you would most likely only have the resources to customize\\nthe most popular.\\nThe better option is to have each merchant write and maintain their own\\ntransaction card. But how do you run that code in a way that doesn\\'t\\nmake your customers\\' private financial data vulnerable to attack?\\nRunning the code directly is out of the question. Normally, you might\\njust create an <iframe> for each item in the transaction. That\\'s fine,\\nbut how do provide information about the transaction to each transaction\\ncard? And what if you want to get information out of the card?\\nFor example, imagine we wanted to group transactions by type. Instead of\\nguessing, we could ask each transaction what category it belongs to,\\nthen show the user only travel-related transactions, for example.\\nConductor.js allows you to define interfaces between a host environment\\nand the cards that run inside of it, without having to write messy and\\nbrittle transports over postMessage.\\nCards are also designed to be run on the server, so they can be indexed\\nand otherwise manipulated without having to create a memory- and\\nCPU-intensive virtual browser.\\nAnd, because cards are run inside either iframes or Web Workers, they\\ncannot get access to any data that is not explicitly provided.\\nThe above example is just one use for Conductor.js. There are many\\ndifferent scenarios where being able to embed secure, third-party code\\ncan allow you to build a platform of re-usable components that can\\ninteract with their host environment.\\nAuthoring Conductor.js Cards\\nA card is an application that can be embedded in a parent environment\\nusing Conductor.js. The entry point to a card is an HTML page that loads\\nConductor.js.\\nAt its most basic, your card should call the Conductor.card method.\\nThe object passed to Conductor.card defines the behavior of the card,\\nand contains callbacks and other hooks that will be invoked in response\\nto requests from the containing environment.\\nFor example, the activate hook is invoked automatically once all\\ndependencies have finished loading, and communication with the parent\\nenvironment has been set up:\\nConductor.card({\\n  activate: function() {\\n    this.alert(\"Hello!\")\\n  }\\n});\\nLoading Dependencies\\nA card can load additional javascript dependencies by using the\\nConductor.require method:\\nConductor.require(\\'alert.js\\');\\nNote that files loaded via Conductor.require are loaded\\nasynchronously. That means that, assuming the Model class was\\ndefined in models.js, this would not work:\\nConductor.require(\\'models.js\\');\\nvar person = new Model();\\nMake sure you only access dependencies once the card\\'s activate method\\nhas been called:\\nConductor.require(\\'alert.js\\');\\n\\nfunction createModel() {\\n  var person = new Model();\\n}\\n\\nConductor.card({\\n  activate: function() {\\n    createModel();\\n  }\\n});\\nYou can include CSS files in your card using the Conductor.requireCSS method:\\nConductor.requireCSS(\\'card.css\\');\\nThe path is relative to the card.\\nTutorial\\nLearn more about authoring cards in the tutorial.\\nBuild Tools\\nConductor.js uses Grunt to automate building and\\ntesting.\\nSetup\\nBefore you use any of the commands below, make sure you have\\ninstalled node.js, which includes npm, the node package manager.\\nIf you haven\\'t before, install the grunt CLI tool:\\n$ npm install -g grunt-cli\\nThis will put the grunt command in your system path, allowing it to be\\nrun from any directory.\\nNext, install Conductor\\'s dependencies:\\n$ npm install\\nThis will install all of the packages that Conductor\\'s Gruntfile relies\\non into the local node_modules directory.\\nBuilding\\nConductor is available as either as an AMD module, or as a more\\ntraditional distribution that exports the global variable Conductor.\\nTo build both versions, run:\\ngrunt build\\n\\nYou can find the built versions of the library in the dist directory.\\nTests\\nRun the Conductor tests by starting a test server:\\ngrunt server\\n\\nOnce the server is running, visit http://localhost:8000 in your\\nbrowser. Conductor will automatically be rebuilt if you make any changes\\nto its constituent files while the server is running.\\n',\n",
       "  'watchers': '20',\n",
       "  'stars': '154',\n",
       "  'forks': '14',\n",
       "  'commits': '236'},\n",
       " {'language': 'Python 92.8',\n",
       "  'readme': '\\n\\n\\n\\nStoring settings in the database\\nNot all settings belong in settings.py, as it has some particular\\nlimitations:\\n\\n\\nSettings are project-wide. This not only requires apps to clutter up\\nsettings.py, but also increases the chances of naming conflicts.\\nSettings are constant throughout an instance of Django. They cannot be\\nchanged without restarting the application.\\nSettings require a programmer in order to be changed. This is true even\\nif the setting has no functional impact on anything else.\\n\\n\\nMany applications find need to overcome these limitations, and dbsettings\\nprovides a convenient way to do so.\\nThe main goal in using this application is to define a set of placeholders that\\nwill be used to represent the settings that are stored in the database. Then,\\nthe settings may be edited at run-time using the provided editor, and all Python\\ncode in your application that uses the setting will receive the updated value.\\n\\nRequirements\\n\\n\\nDbsettings\\nPython\\nDjango\\n\\n\\n\\n==0.10\\n3.4 - 3.5\\n1.7 - 1.10\\n\\n3.2 - 3.3\\n1.7 - 1.8\\n\\n2.7\\n1.7 - 1.10\\n\\n==0.9\\n3.4 - 3.5\\n1.7 - 1.9\\n\\n3.2 - 3.3\\n1.7 - 1.8\\n\\n2.7\\n1.7 - 1.9\\n\\n==0.8\\n3.2\\n1.5 - 1.8\\n\\n2.7\\n1.4 - 1.8\\n\\n2.6\\n1.4 - 1.6\\n\\n==0.7\\n3.2\\n1.5 - 1.7\\n\\n2.7\\n1.3 - 1.7\\n\\n2.6\\n1.3 - 1.6\\n\\n==0.6\\n3.2\\n1.5\\n\\n2.6 - 2.7\\n1.3 - 1.5\\n\\n<=0.5\\n2.6 - 2.7\\n1.2* - 1.4\\n\\n\\n\\n* Possibly version below 1.2 will work too, but not tested.\\n\\nInstallation\\nTo install the dbsettings package, simply place it anywhere on your\\nPYTHONPATH.\\n\\nProject settings\\nIn order to setup database storage, and to let Django know about your use of\\ndbsettings, simply add it to your INSTALLED_APPS setting, like so:\\nINSTALLED_APPS = (\\n    ...\\n    \\'dbsettings\\',\\n    ...\\n)\\n\\nIf your Django project utilizes sites framework, all setting would be related\\nto some site. If sites are not present, settings won\\'t be connected to any site\\n(and sites framework is no longer required since 0.8.1).\\nYou can force to do (not) use sites via DBSETTINGS_USE_SITES = True / False\\nconfiguration variable (put it in project\\'s settings.py).\\nBy default, values stored in database are limited to 255 characters per setting.\\nYou can change this limit with DBSETTINGS_VALUE_LENGTH configuration variable.\\nIf you change this value after migrations were run, you need to manually alter\\nthe dbsettings_setting table schema.\\n\\nURL Configuration\\nIn order to edit your settings at run-time, you\\'ll need to configure a URL to\\naccess the provided editors. You\\'ll just need to add a single line, defining\\nthe base URL for the editors, as dbsettings has its own URLconf to handle\\nthe rest. You may choose any location you like:\\nurlpatterns = patterns(\\'\\',\\n    ...\\n    (r\\'^settings/\\', include(\\'dbsettings.urls\\')),\\n    ...\\n)\\n\\n\\nA note about caching\\nThis framework utilizes Django\\'s built-in cache framework, which is used to\\nminimize how often the database needs to be accessed. During development,\\nDjango\\'s built-in server runs in a single process, so all cache backends will\\nwork just fine.\\nMost productions environments, including mod_python, FastCGI or WSGI, run multiple\\nprocesses, which some backends don\\'t fully support. When using the simple\\nor locmem backends, updates to your settings won\\'t be reflected immediately\\nin all workers, causing your application to ignore the new changes.\\nNo other backends exhibit this behavior, but since simple is the default,\\nmake sure to specify a proper backend when moving to a production environment.\\nAlternatively you can disable caching of settings by setting\\nDBSETTINGS_USE_CACHE = False in settings.py. Beware though: every\\naccess of any setting will result in database hit.\\n\\nUsage\\nThese database-backed settings can be applied to any model in any app, or even\\nin the app itself. All the tools necessary to do so are available within the\\ndbsettings module. A single import provides everything you\\'ll need:\\nimport dbsettings\\n\\n\\nDefining a group of settings\\nSettings are be defined in groups that allow them to be referenced together\\nunder a single attribute. Defining a group uses a declarative syntax similar\\nto that of models, by declaring a new subclass of the Group class and\\npopulating it with values.\\nclass ImageLimits(dbsettings.Group):\\n    maximum_width = dbsettings.PositiveIntegerValue()\\n    maximum_height = dbsettings.PositiveIntegerValue()\\n\\nYou may name your groups anything you like, and they may be defined in any\\nmodule. This allows them to be imported from common applications if applicable.\\n\\nDefining individual settings\\nWithin your groups, you may define any number of individual settings by simply\\nassigning the value types to appropriate names. The names you assign them to\\nwill be the attribute names you\\'ll use to reference the setting later, so be\\nsure to choose names accordingly.\\nFor the editor, the default description of each setting will be retrieved from\\nthe attribute name, similar to how the verbose_name of model fields is\\nretrieved. Also like model fields, however, an optional argument may be provided\\nto define a more fitting description. It\\'s recommended to leave the first letter\\nlower-case, as it will be capitalized as necessary, automatically.\\nclass EmailOptions(dbsettings.Group):\\n    enabled = dbsettings.BooleanValue(\\'whether to send emails or not\\')\\n    sender = dbsettings.StringValue(\\'address to send emails from\\')\\n    subject = dbsettings.StringValue(default=\\'SiteMail\\')\\n\\nFor more descriptive explanation, the help_text argument can be used. It\\nwill be shown in the editor.\\nThe default argument is very useful - it specify an initial value of the\\nsetting.\\nIn addition, settings may be supplied with a list of available options, through\\nthe use of of the choices argument. This works exactly like the choices\\nargument for model fields, and that of the newforms ChoiceField.\\nThe widget used for a value can be overriden using the widget keyword. For example:\\npayment_instructions = dbsettings.StringValue(\\n    help_text=\"Printed on every invoice.\",\\n    default=\"Payment to Example XYZ\\\\nBank name here\\\\nAccount: 0123456\\\\nSort: 01-02-03\",\\n    widget=forms.Textarea\\n)\\n\\nA full list of value types is available later in this document, but the process\\nand arguments are the same for each.\\n\\nAssigning settings\\nOnce your settings are defined and grouped properly, they must be assigned to a\\nlocation where they will be referenced later. This is as simple as instantiating\\nthe settings group in the appropriate location. This may be at the module level\\nor within any standard Django model.\\nGroup instance may receive one optional argument: verbose name of the group.\\nThis name will be displayed in the editor.\\nemail = EmailOptions()\\n\\nclass Image(models.Model):\\n    image = models.ImageField(upload_to=\\'/upload/path\\')\\n    caption = models.TextField()\\n\\n    limits = ImageLimits(\\'Dimension settings\\')\\n\\nMultiple groups may be assigned to the same module or model, and they can even\\nbe combined into a single group by using standard addition syntax:\\noptions = EmailOptions() + ImageLimits()\\n\\nTo separate and tag settings nicely in the editor, use verbose names:\\noptions = EmailOptions(\\'Email\\') + ImageLimits(\\'Dimesions\\')\\n\\n\\nDatabase setup\\nA single model is provided for database storage, and this model must be\\ninstalled in your database before you can use the included editors or the\\npermissions that will be automatically created. This is a simple matter of\\nrunning manage.py syncdb or manage.py migrate now that your settings\\nare configured.\\nThis step need only be repeate when settings are added to a new application,\\nas it will create the appropriate permissions. Once those are in place, new\\nsettings may be added to existing applications with no impact on the database.\\n\\nUsing your settings\\nOnce the above steps are completed, you\\'re ready to make use of database-backed\\nsettings.\\n\\nEditing settings\\nWhen first defined, your settings will default to None (or False in\\nthe case of BooleanValue), so their values must be set using one of the\\nsupplied editors before they can be considered useful (however, if the setting\\nhad the default argument passed in the constructor, its value is already\\nuseful - equal to the defined default).\\nThe editor will be available at the URL configured earlier.\\nFor example, if you used the prefix of \\'settings/\\', the URL /settings/\\nwill provide an editor of all available settings, while /settings/myapp/\\nwould contain a list of just the settings for myapp.\\nURL patterns are named: \\'site_settings\\' and \\'app_settings\\', respectively.\\nThe editors are restricted to staff members, and the particular settings that\\nwill be available to users is based on permissions that are set for them. This\\nmeans that superusers will automatically be able to edit all settings, while\\nother staff members will need to have permissions set explicitly.\\n\\nAccessing settings in Python\\nOnce settings have been assigned to an appropriate location, they may be\\nreferenced as standard Python attributes. The group becomes an attribute of the\\nlocation where it was assigned, and the individual values are attributes of the\\ngroup.\\nIf any settings are referenced without being set to a particular value, they\\nwill default to None (or False in the case of BooleanValue, or\\nwhatever was passed as default). In the\\nfollowing example, assume that EmailOptions were just added to the project\\nand the ImageLimits were added earlier and already set via editor.\\n>>> from myproject.myapp import models\\n\\n# EmailOptions are not defined\\n>>> models.email.enabled\\nFalse\\n>>> models.email.sender\\n>>> models.email.subject\\n\\'SiteMail\\'  # Since default was defined\\n\\n# ImageLimits are defined\\n>>> models.Image.limits.maximum_width\\n1024\\n>>> models.Image.limits.maximum_height\\n768\\n\\nThese settings are accessible from any Python code, making them especially\\nuseful in model methods and views. Each time the attribute is accessed, it will\\nretrieve the current value, so your code doesn\\'t need to worry about what\\nhappens behind the scenes.\\ndef is_valid(self):\\n    if self.width > Image.limits.maximum_width:\\n        return False\\n    if self.height > Image.limits.maximum_height:\\n        return False\\nreturn True\\n\\nAs mentioned, views can make use of these settings as well.\\nfrom myproject.myapp.models import email\\n\\ndef submit(request):\\n\\n    ...\\n    # Deal with a form submission\\n    ...\\n\\n    if email.enabled:\\n        from django.core.mail import send_mail\\n    send_mail(email.subject, \\'message\\', email.sender, [request.user.email])\\n\\nSettings can be not only read, but also written. The admin editor is more\\nuser-friendly, but in case code need to change something:\\nfrom myproject.myapp.models import Image\\n\\ndef low_disk_space():\\n    Image.limits.maximum_width = Image.limits.maximum_height = 200\\n\\nEvery write is immediately commited to the database and proper cache key is deleted.\\n\\nA note about model instances\\nSince settings aren\\'t related to individual model instances, any settings that\\nare set on models may only be accessed by the model class itself. Attempting to\\naccess settings on an instance will raise an AttributeError.\\n\\nValue types\\nThere are several various value types available for database-backed settings.\\nSelect the one most appropriate for each individual setting, but all types use\\nthe same set of arguments.\\n\\nBooleanValue\\nPresents a checkbox in the editor, and returns True or False in Python.\\n\\nDurationValue\\nPresents a set of inputs suitable for specifying a length of time. This is\\nrepresented in Python as a timedelta object.\\n\\nFloatValue\\nPresents a standard input field, which becomes a float in Python.\\n\\nIntegerValue\\nPresents a standard input field, which becomes an int in Python.\\n\\nPercentValue\\nSimilar to IntegerValue, but with a limit requiring that the value be\\nbetween 0 and 100. In addition, when accessed in Python, the value will be\\ndivided by 100, so that it is immediately suitable for calculations.\\nFor instance, if a myapp.taxes.sales_tax was set to 5 in the editor,\\nthe following calculation would be valid:\\n>>> 5.00 * myapp.taxes.sales_tax\\n0.25\\n\\n\\nPositiveIntegerValue\\nSimilar to IntegerValue, but limited to positive values and 0.\\n\\nStringValue\\nPresents a standard input, accepting any text string up to 255\\n(or DBSETTINGS_VALUE_LENGTH) characters. In\\nPython, the value is accessed as a standard string.\\n\\nDateTimeValue\\nPresents a standard input field, which becomes a datetime in Python.\\nUser input will be parsed according to DATETIME_INPUT_FORMATS setting.\\nIn code, one can assign to field string or datetime object:\\n# These two statements has the same effect\\nmyapp.Feed.next_feed = \\'2012-06-01 00:00:00\\'\\nmyapp.Feed.next_feed = datetime.datetime(2012, 6, 1, 0, 0, 0)\\n\\n\\nDateValue\\nPresents a standard input field, which becomes a date in Python.\\nUser input will be parsed according to DATE_INPUT_FORMATS setting.\\nSee DateTimeValue for the remark about assigning.\\n\\nTimeValue\\nPresents a standard input field, which becomes a time in Python.\\nUser input will be parsed according to TIME_INPUT_FORMATS setting.\\nSee DateTimeValue for the remark about assigning.\\n\\nImageValue\\n(requires PIL or Pillow imaging library to work)\\nAllows to upload image and view its preview.\\nImageValue has optional upload_to keyword, which specify path\\n(relative to MEDIA_ROOT), where uploaded images will be stored.\\nIf keyword is not present, files will be saved directly under\\nMEDIA_ROOT.\\n\\nPasswordValue\\nPresents a standard password input. Retain old setting value if not changed.\\n\\nSetting defaults for a distributed application\\nDistributed applications often have need for certain default settings that are\\nuseful for the common case, but which may be changed to suit individual\\ninstallations. For such cases, a utility is provided to enable applications to\\nset any applicable defaults.\\nLiving at dbsettings.utils.set_defaults, this utility is designed to be used\\nwithin the app\\'s management.py. This way, when the application is installed\\nusing syncdb/migrate, the default settings will also be installed to the database.\\nThe function requires a single positional argument, which is the models\\nmodule for the application. Any additional arguments must represent the actual\\nsettings that will be installed. Each argument is a 3-tuple, of the following\\nformat: (class_name, setting_name, value).\\nIf the value is intended for a module-level setting, simply set class_name\\nto an empty string. The value for setting_name should be the name given to\\nthe setting itself, while the name assigned to the group isn\\'t supplied, as it\\nisn\\'t used for storing the value.\\nFor example, the following code in management.py would set defaults for\\nsome of the settings provided earlier in this document:\\nfrom django.conf import settings\\nfrom dbsettings.utils import set_defaults\\nfrom myproject.myapp import models as myapp\\n\\nset_defaults(myapp,\\n    (\\'\\', \\'enabled\\', True)\\n    (\\'\\', \\'sender\\', settings.ADMINS[0][1]) # Email of the first listed admin\\n    (\\'Image\\', \\'maximum_width\\', 800)\\n    (\\'Image\\', \\'maximum_height\\', 600)\\n)\\n\\n\\n\\nChangelog\\n\\n0.10.0 (25/09/2016)\\n\\nAdded compatibility with Django 1.10\\n\\n\\n0.9.3 (02/06/2016)\\n\\nFixed (hopefully for good) problem with ImageValue in Python 3 (thanks rolexCoder)\\n\\n\\n0.9.2 (01/05/2016)\\n\\nFixed bug when saving non-required settings\\nFixed problem with ImageValue in Python 3 (thanks rolexCoder)\\n\\n\\n0.9.1 (10/01/2016)\\n\\nFixed Sites app being optional (thanks rolexCoder)\\n\\n\\n0.9.0 (25/12/2015)\\n\\nAdded compatibility with Django 1.9 (thanks Alonso)\\nDropped compatibility with Django 1.4, 1.5, 1.6\\n\\n\\n0.8.2 (17/09/2015)\\n\\nAdded migrations to distro\\nAdd configuration option to change max length of setting values from 255 to whatever\\nAdd configuration option to disable caching (thanks nwaxiomatic)\\nFixed PercentValue rendering (thanks last-partizan)\\n\\n\\n0.8.1 (21/06/2015)\\n\\nMade django.contrib.sites framework dependency optional\\nAdded migration for app\\n\\n\\n0.8.0 (16/04/2015)\\n\\nSwitched to using django.utils.six instead of standalone six.\\nAdded compatibility with Django 1.8\\nDropped compatibility with Django 1.3\\n\\n\\n0.7.4 (24/03/2015)\\n\\nAdded default values for fields.\\nFixed Python 3.3 compatibility\\nAdded creation of folders with ImageValue\\n\\n\\n0.7.3, 0.7.2\\npypi problems\\n0.7.1 (11/03/2015)\\n\\nFixed pypi distribution.\\n\\n\\n0.7 (06/07/2014)\\n\\nAdded PasswordValue\\nAdded compatibility with Django 1.6 and 1.7.\\n\\n\\n0.6 (16/09/2013)\\n\\nAdded compatibility with Django 1.5 and python3, dropped support for Django 1.2.\\nFixed permissions: added permission for editing non-model (module-level) settings\\nMake PIL/Pillow not required in setup.py\\n\\n\\n0.5 (11/10/2012)\\n\\nFixed error occuring when test are run with LANGUAGE_CODE different than \\'en\\'\\nAdded verbose_name option for Groups\\nCleaned code\\n\\n\\n0.4.1 (02/10/2012)\\n\\nFixed Image import\\n\\n\\n0.4 (30/09/2012)\\n\\nNamed urls\\nAdded polish translation\\n\\n\\n0.3 (04/09/2012)\\nIncluded testrunner in distribution\\n0.2 (05/07/2012)\\n\\nFixed errors appearing when module-level and model-level settings have\\nsame attribute names\\nCorrected the editor templates admin integration\\nUpdated README\\n\\n\\n0.1 (29/06/2012)\\nInitial PyPI release\\n\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '100',\n",
       "  'forks': '84',\n",
       "  'commits': '181'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'django-predicate\\ndjango-predicate provides a Q like object to facilitate the question: \"would\\nthis model instance be part of a query\" but without running the query or even\\nsaving the object.\\n\\nQuickstart\\nInstall django-predicate:\\npip install django-predicate\\nThen use the P object just as you would Q objects:\\nfrom predicate import P\\n\\np = P(some_field__startswith=\"hello\", age__gt=20)\\nYou can then call the eval method with a model instance to check whether it\\npasses the conditions:\\nmodel_instance = MyModel(some_field=\"hello there\", age=21)\\nother_model_instance = MyModel(some_field=\"hello there\", age=10)\\np.eval(model_instance)\\n>>> True\\np.eval(other_model_instance)\\n>>> False\\nor you can use Python\\'s in operator.\\nmodel_instance in p\\n>>> True\\nEven though a predicate is not a true container class - it can be used as (and\\nwas designed as being) a virtual \"set\" of objects that meets some condiiton.\\nLike Q objects, P objects can be &\\'ed  and |\\'ed together to form more\\ncomplex logic groupings.\\nIn fact, P objects are actually a subclass of Q objects, so you can use them in\\nqueryset filter statements:\\nqs = MyModel.objects.filter(p)\\nP objects also support QuerySet-like filtering operations that can be\\napplied to an arbitrary iterable: P.get(iterable), P.filter(iterable),\\nand P.exclude(iterable):\\nmodel_instance = MyModel(some_field=\"hello there\", age=21)\\nother_model_instance = MyModel(some_field=\"hello there\", age=10)\\np.filter([model_instance, other_model_instance]) == [model_instance]\\n>>> True\\np.get([model_instance, other_model_instance]) == model_instance\\n>>> True\\np.exclude([model_instance, other_model_instance]) == [other_model_instance]\\n>>> True\\nIf you have a situation where you want to use querysets and predicates based on\\nthe same conditions, it is far better to start with the predicate. Because of\\nthe way querysets assume a SQL context, it is non-trivial to reverse engineer\\nthem back into a predicate. However as seen above, it is very straightforward\\nto create a queryset based on a predicate.\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '100',\n",
       "  'forks': '4',\n",
       "  'commits': '140'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'NO LONGER SUPPORTED: This project will receive no more updates and pull\\nrequests and issues will no longer be monitored. I recommend everyone begin to\\nadopt the migrations support that is in Django 1.7 and use South until you are\\non Django 1.7.\\n\\nNashvegas\\nThe purpose of this app is to enable a plug and play method for managing\\ndatabase changes.\\nDatabase migrations is a large topic with a lot of different approaches.  This\\napproach worked well for my needs and maybe it will for you as well.\\n\\nDocumentation\\nYou can find the documentation in the docs/ folder of the repo or online at:\\n\\nhttp://nashvegas.readthedocs.org\\n\\nInstallation\\nInstallation is simple:\\n$ pip install nashvegas\\n\\nThen add nashvegas to your INSTALLED_APPS in your Django settings.py\\nfile.\\n\\nSupport\\nYou can either log issues on the Github issue tracker for this project or pop\\ninto #nashvegas on Freenode.\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '99',\n",
       "  'forks': '18',\n",
       "  'commits': '191'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'dmc\\ndmc is python library for date and time manipulation.\\nUsage of standard library modules such as datetime, time, pytz etc is dangerous and error prone.\\nThis library is very opinionated about how to treat dates, times and intervals\\nso as to prevent developers from shooting themselves in the foot. Also, the API\\nis quote a bit more convinient for most use cases.\\nOverview\\ndmc is really just a wrapper around several other python libraries, but does so\\nin a way that makes using these libraries safe.\\nSome things to keep in mind:\\n\\nThere is no such thing as a naive time. ALL times involve a timezone, and that timezone is UTC.\\nThe only time we deal with non-UTC timezones is when we parse or display a time.\\nWe support math with dates, and math with times, but you can\\'t do date math with times.\\nFor example: You can\\'t add \"1 day\" to 2014-03-28 02:00:00, because that\\ndoesn\\'t actually make sense. Do you mean add 24 hours? Then do that. Did\\nyou mean you want 02:00:00 on 3/29, sure we can do that, but you have to\\ndecide.\\n\\nWe have a few primary objects:\\n\\nTime - Represents a precise time in UTC.\\nTimeInterval - Represents a change in time, like 10 minutes, or 4000 hours.\\nTimeSpan - a range of Times of some TimeInterval length\\nTimeIterator - generates time instances based on a TimeSpan and a TimeInterval\\nDate - Represents a calendar date. Only the modern Gregorian calendar is supported.\\nDateInterval - Represents a change in calendar date, like \\'2 days\\', \\'next month\\', or \\'next monday\\'\\nDateSpan - a range of dates of some DateInterval.\\nDateIterator - genreates dates based on DateSpan and an DateInterval\\n\\nUsage\\n>> start_t = dmc.Time.now()\\n>> print start_t\\n\"2014-03-29T20:16:58.265249Z\"\\n\\n>> print start.to_str(tz=\\'Americas/Los_Angeles\\')\\n\"2014-03-29T12:16:58.265249-7:00\"\\n\\n>> print start.to_str(local=True)\\n\"2014-03-29T12:16:58.265249-7:00\"\\n\\n>> start.to_str(\"YYYY-MM-DD HH:MM\", tz=\\'Americas/Los_Angeles\\')\\n\"2014-03-29 12:16\"\\n\\n>> print start_t.to_timestamp()\\n1396120755.748726\\n\\n>> print start_t.to_human()\\n\"10 minutes ago\"\\n\\n>> start_t = dmc.Time.from_timestamp(1396120755.748726)\\n\\n>> print start_t.to_datetime()\\ndatetime.datetime(2014, 3, 29, 12, 16, 58, 0, tzinfo=<UTC>)\\n\\n>> d = dmc.Date(2014, 3, 28)\\n>> print d\\n\"2014-03-28\"\\n\\n>> dmc.Date.from_str(\"3/28/2014\")\\ndmc.Date(2014, 28, 3)\\n\\n>> start_t, _ = dmc.TimeSpan.from_date(d)\\n>> print start_t\\n\"2014-03-28T00:00:00Z\"\\n\\n# 3 weeks from now\\n>> d += dmc.DateInterval(weeks=3)\\n\\n# Next Sunday\\n>> d += dmc.DateInterval(weekday=0)\\n\\n>> start_t, end_t = dmc.TimeSpan.from_date(d)\\n\\n>> t = dmc.Time.now()\\n>> t += dmc.TimeInterval(minutes=30)\\n\\n>> today_span = dmc.Date.today().to_timespan()\\n>> for t in dmc.TimeIterator(today_span, dmc.TimeInterval(hours=1)):\\n>>    print t\\n\"2014-03-28T00:00:00Z\"\\n\"2014-03-28T01:00:00Z\"\\n\"2014-03-28T02:00:00Z\"\\n....\\n\\nTesting\\nWhen you\\'re working with date and time sensitive code, it\\'s often very helpful\\nto be able to mock out the current time or date. dmc makes this easy:\\n>> dmc.set_mock_time(dmc.Time().now() - dmc.TimeInterval(hours=1))\\n>> dmc.clear_mock_time()\\n\\nOr, with a friendly context manager:\\n>> with dmc.MockTime(...):\\n.... pass\\n\\nWhat\\'s wrong with datetime\\nThis fun old wiki page is enlightening: https://wiki.python.org/moin/WorkingWithTime\\nDatetime was supposed to be the solution. I still remember when I stumbed\\nacross mx.Datetime. Mind blown. Such a better world. (datetime was added in\\npython 2.3, in 2003, over 10 years ago). You\\'d think that dealing with dates\\nand times would be solved problem. But twice a year I wake up to the collective\\n\"oh shit\" as developers remember daylight savings time.\\nThere is also PEP-431 that\\nattempts to fix datetime timezones. Or is it just patching over how insane it\\nis to handle timezones in this way?\\nThere are also a lot of glaring holes in the API datetime provides us. We fill\\nthose holes with a cornocopia of several other modules.  Basically, if you\\'re\\ndoing anything remotely complex with dates and times you need to understand and\\nmake us of:\\n\\ndatetime\\ntime\\ncalendar\\npytz\\ndateutil\\n\\ndmc attempts to combine all these together into a consistent interface so you don\\'t have to.\\nFor specific examples, try these:\\ntimestamps\\nIt\\'s easy to create a a datetime from a timestamp:\\n>> d = datetime.datetime.fromtimestamp(12312412.0)\\n\\nOh wait, what timezone was that in? What we should have done was\\n>> d = datetime.datetime.utcfromtimestamp(12312412.0)\\n\\nPop, quiz, how do you convert back?\\n>> import time\\n>> time.mktime(d.timetuple())\\n\\nOr wait, did I mean:\\n>> time.mktime(d.utctimetuple())\\n\\nformatting / parsing\\n>> d.isoformat()\\n\\'2014-04-17T15:32:01.219333\\'\\n\\nNow how do I parse an isoformat? There are several solutions on stackoverflow.\\n>> datetime.datetime.strptime(\"2014-04-17T15:32:01\", \"%Y-%m-%dT%H:%M:%S\" )\\n\\nBut of course parsing iso8601 is more complicated than that:\\n>> import re\\n>> s = \"2008-09-03T20:56:35.450686Z\"\\n>> d = datetime.datetime(*map(int, re.split(\\'[^\\\\d]\\', s)[:-1]))\\n\\nOk, there are some 3rd party libraries:\\n>> dateutil.parser.parse(\\'2014-04-17T15:32:01.219333Z\\')\\n\\nOf course you need to be real careful with this library, if there is something\\nit doesn\\'t recognize you\\'ll just get a datetime filled in with values from the\\ncurrent time (!!!! wtf)\\nOh good, there is a python module for JUST THIS ONE FORMAT:\\n>> import iso8601\\n>> iso8601.parse_date(\"2007-01-25T12:00:00Z\")\\n   datetime.datetime(2007, 1, 25, 12, 0, tzinfo=<iso8601.iso8601.Utc ...>)\\n\\nI don\\'t even want to think about whether iso8601.iso8601.Utc == pytz.UTC.\\ntimezones\\n>> datetime.datetime.now()\\n\\nWhat timezone is this in?\\nOh right, what I actually need to do is:\\n>> import pytz\\n>> d = pytz.UTC.localize(datetime.datetime.utcnow())\\n\\nHow about some arithmetic:\\n>> now() + datetime.timedelta(days=1)\\n\\nIs this 24 hours from now? Is this the same number hours since midnight but the\\nnext calendar day? Unfortunately the difference between these interpretations\\nonly becomes obvious twice a year, in only some political regions of the world.\\nWhat if I wanted to enumerate time ranges for a day in localtime, and then run\\nsome queries that are in UTC.\\n>> import pytz\\n>> tz = pytz.timezone(\\'US/Pacific\\')\\n>> start_dt = datetime.datetime(2014, 3, 9, 0, 0, 0, tzinfo=tz)\\n>> end_dt = start_dt + datetime.timedelta(days=1)\\n>> d = start_dt\\n\\nwhile d < end_dt:\\n    run_query(d.astimezone(pytz.UTC), d.astimezone(pytz.UTC) + datetime.timedelta(hours=1))\\n    d += datetime.timedelta(hours=1)\\n\\nHow many errors can you spot?\\nOh, my favorite:\\n>> import pytz\\n>> tz = pytz.timezone(\\'US/Pacific\\')\\n>> start_dt = datetime.datetime(2014, 3, 6, 0, 0, 0, tzinfo=tz)\\n\\n>> d = start_dt\\n>> for _ in range(7):\\n...   print d\\n...   d += datetime.timedelta(days=1\\n2014-03-06 00:00:00-08:00\\n2014-03-07 00:00:00-08:00\\n2014-03-08 00:00:00-08:00\\n2014-03-09 00:00:00-08:00\\n2014-03-10 00:00:00-08:00\\n2014-03-11 00:00:00-08:00\\n2014-03-12 00:00:00-08:00\\n\\nSee anything wrong here? 2014-03-10 00:00:00-08:00 doesn\\'t make any sense.\\nBecause on the 2014-03-10, the timezone should be -07:00. There are fun\\nswitcheroo ways around this:\\n>> d = start_dt\\n>> for _ in range(7):\\n...   print d.astimezone(pytz.UTC).astimezone(pytz.timezone(\\'US/Pacific\\'))\\n...   d += datetime.timedelta(days=1)\\n\\nThis is common enough that pytz actually provides a function for it:\\n>> print pytz.timezone(\\'US/Pacific\\').normalize(d)\\n\\nProject Status\\nExperimental. Still estabilishing interfaces.\\nBasic Date and Time formatting and parsing are written and tested.\\nIntervals and Span are being actively developed.\\nDMC still requires some real world use before APIs are firmly set.\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '99',\n",
       "  'forks': '3',\n",
       "  'commits': '12'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'dmc\\ndmc is python library for date and time manipulation.\\nUsage of standard library modules such as datetime, time, pytz etc is dangerous and error prone.\\nThis library is very opinionated about how to treat dates, times and intervals\\nso as to prevent developers from shooting themselves in the foot. Also, the API\\nis quote a bit more convinient for most use cases.\\nOverview\\ndmc is really just a wrapper around several other python libraries, but does so\\nin a way that makes using these libraries safe.\\nSome things to keep in mind:\\n\\nThere is no such thing as a naive time. ALL times involve a timezone, and that timezone is UTC.\\nThe only time we deal with non-UTC timezones is when we parse or display a time.\\nWe support math with dates, and math with times, but you can\\'t do date math with times.\\nFor example: You can\\'t add \"1 day\" to 2014-03-28 02:00:00, because that\\ndoesn\\'t actually make sense. Do you mean add 24 hours? Then do that. Did\\nyou mean you want 02:00:00 on 3/29, sure we can do that, but you have to\\ndecide.\\n\\nWe have a few primary objects:\\n\\nTime - Represents a precise time in UTC.\\nTimeInterval - Represents a change in time, like 10 minutes, or 4000 hours.\\nTimeSpan - a range of Times of some TimeInterval length\\nTimeIterator - generates time instances based on a TimeSpan and a TimeInterval\\nDate - Represents a calendar date. Only the modern Gregorian calendar is supported.\\nDateInterval - Represents a change in calendar date, like \\'2 days\\', \\'next month\\', or \\'next monday\\'\\nDateSpan - a range of dates of some DateInterval.\\nDateIterator - genreates dates based on DateSpan and an DateInterval\\n\\nUsage\\n>> start_t = dmc.Time.now()\\n>> print start_t\\n\"2014-03-29T20:16:58.265249Z\"\\n\\n>> print start.to_str(tz=\\'Americas/Los_Angeles\\')\\n\"2014-03-29T12:16:58.265249-7:00\"\\n\\n>> print start.to_str(local=True)\\n\"2014-03-29T12:16:58.265249-7:00\"\\n\\n>> start.to_str(\"YYYY-MM-DD HH:MM\", tz=\\'Americas/Los_Angeles\\')\\n\"2014-03-29 12:16\"\\n\\n>> print start_t.to_timestamp()\\n1396120755.748726\\n\\n>> print start_t.to_human()\\n\"10 minutes ago\"\\n\\n>> start_t = dmc.Time.from_timestamp(1396120755.748726)\\n\\n>> print start_t.to_datetime()\\ndatetime.datetime(2014, 3, 29, 12, 16, 58, 0, tzinfo=<UTC>)\\n\\n>> d = dmc.Date(2014, 3, 28)\\n>> print d\\n\"2014-03-28\"\\n\\n>> dmc.Date.from_str(\"3/28/2014\")\\ndmc.Date(2014, 28, 3)\\n\\n>> start_t, _ = dmc.TimeSpan.from_date(d)\\n>> print start_t\\n\"2014-03-28T00:00:00Z\"\\n\\n# 3 weeks from now\\n>> d += dmc.DateInterval(weeks=3)\\n\\n# Next Sunday\\n>> d += dmc.DateInterval(weekday=0)\\n\\n>> start_t, end_t = dmc.TimeSpan.from_date(d)\\n\\n>> t = dmc.Time.now()\\n>> t += dmc.TimeInterval(minutes=30)\\n\\n>> today_span = dmc.Date.today().to_timespan()\\n>> for t in dmc.TimeIterator(today_span, dmc.TimeInterval(hours=1)):\\n>>    print t\\n\"2014-03-28T00:00:00Z\"\\n\"2014-03-28T01:00:00Z\"\\n\"2014-03-28T02:00:00Z\"\\n....\\n\\nTesting\\nWhen you\\'re working with date and time sensitive code, it\\'s often very helpful\\nto be able to mock out the current time or date. dmc makes this easy:\\n>> dmc.set_mock_time(dmc.Time().now() - dmc.TimeInterval(hours=1))\\n>> dmc.clear_mock_time()\\n\\nOr, with a friendly context manager:\\n>> with dmc.MockTime(...):\\n.... pass\\n\\nWhat\\'s wrong with datetime\\nThis fun old wiki page is enlightening: https://wiki.python.org/moin/WorkingWithTime\\nDatetime was supposed to be the solution. I still remember when I stumbed\\nacross mx.Datetime. Mind blown. Such a better world. (datetime was added in\\npython 2.3, in 2003, over 10 years ago). You\\'d think that dealing with dates\\nand times would be solved problem. But twice a year I wake up to the collective\\n\"oh shit\" as developers remember daylight savings time.\\nThere is also PEP-431 that\\nattempts to fix datetime timezones. Or is it just patching over how insane it\\nis to handle timezones in this way?\\nThere are also a lot of glaring holes in the API datetime provides us. We fill\\nthose holes with a cornocopia of several other modules.  Basically, if you\\'re\\ndoing anything remotely complex with dates and times you need to understand and\\nmake us of:\\n\\ndatetime\\ntime\\ncalendar\\npytz\\ndateutil\\n\\ndmc attempts to combine all these together into a consistent interface so you don\\'t have to.\\nFor specific examples, try these:\\ntimestamps\\nIt\\'s easy to create a a datetime from a timestamp:\\n>> d = datetime.datetime.fromtimestamp(12312412.0)\\n\\nOh wait, what timezone was that in? What we should have done was\\n>> d = datetime.datetime.utcfromtimestamp(12312412.0)\\n\\nPop, quiz, how do you convert back?\\n>> import time\\n>> time.mktime(d.timetuple())\\n\\nOr wait, did I mean:\\n>> time.mktime(d.utctimetuple())\\n\\nformatting / parsing\\n>> d.isoformat()\\n\\'2014-04-17T15:32:01.219333\\'\\n\\nNow how do I parse an isoformat? There are several solutions on stackoverflow.\\n>> datetime.datetime.strptime(\"2014-04-17T15:32:01\", \"%Y-%m-%dT%H:%M:%S\" )\\n\\nBut of course parsing iso8601 is more complicated than that:\\n>> import re\\n>> s = \"2008-09-03T20:56:35.450686Z\"\\n>> d = datetime.datetime(*map(int, re.split(\\'[^\\\\d]\\', s)[:-1]))\\n\\nOk, there are some 3rd party libraries:\\n>> dateutil.parser.parse(\\'2014-04-17T15:32:01.219333Z\\')\\n\\nOf course you need to be real careful with this library, if there is something\\nit doesn\\'t recognize you\\'ll just get a datetime filled in with values from the\\ncurrent time (!!!! wtf)\\nOh good, there is a python module for JUST THIS ONE FORMAT:\\n>> import iso8601\\n>> iso8601.parse_date(\"2007-01-25T12:00:00Z\")\\n   datetime.datetime(2007, 1, 25, 12, 0, tzinfo=<iso8601.iso8601.Utc ...>)\\n\\nI don\\'t even want to think about whether iso8601.iso8601.Utc == pytz.UTC.\\ntimezones\\n>> datetime.datetime.now()\\n\\nWhat timezone is this in?\\nOh right, what I actually need to do is:\\n>> import pytz\\n>> d = pytz.UTC.localize(datetime.datetime.utcnow())\\n\\nHow about some arithmetic:\\n>> now() + datetime.timedelta(days=1)\\n\\nIs this 24 hours from now? Is this the same number hours since midnight but the\\nnext calendar day? Unfortunately the difference between these interpretations\\nonly becomes obvious twice a year, in only some political regions of the world.\\nWhat if I wanted to enumerate time ranges for a day in localtime, and then run\\nsome queries that are in UTC.\\n>> import pytz\\n>> tz = pytz.timezone(\\'US/Pacific\\')\\n>> start_dt = datetime.datetime(2014, 3, 9, 0, 0, 0, tzinfo=tz)\\n>> end_dt = start_dt + datetime.timedelta(days=1)\\n>> d = start_dt\\n\\nwhile d < end_dt:\\n    run_query(d.astimezone(pytz.UTC), d.astimezone(pytz.UTC) + datetime.timedelta(hours=1))\\n    d += datetime.timedelta(hours=1)\\n\\nHow many errors can you spot?\\nOh, my favorite:\\n>> import pytz\\n>> tz = pytz.timezone(\\'US/Pacific\\')\\n>> start_dt = datetime.datetime(2014, 3, 6, 0, 0, 0, tzinfo=tz)\\n\\n>> d = start_dt\\n>> for _ in range(7):\\n...   print d\\n...   d += datetime.timedelta(days=1\\n2014-03-06 00:00:00-08:00\\n2014-03-07 00:00:00-08:00\\n2014-03-08 00:00:00-08:00\\n2014-03-09 00:00:00-08:00\\n2014-03-10 00:00:00-08:00\\n2014-03-11 00:00:00-08:00\\n2014-03-12 00:00:00-08:00\\n\\nSee anything wrong here? 2014-03-10 00:00:00-08:00 doesn\\'t make any sense.\\nBecause on the 2014-03-10, the timezone should be -07:00. There are fun\\nswitcheroo ways around this:\\n>> d = start_dt\\n>> for _ in range(7):\\n...   print d.astimezone(pytz.UTC).astimezone(pytz.timezone(\\'US/Pacific\\'))\\n...   d += datetime.timedelta(days=1)\\n\\nThis is common enough that pytz actually provides a function for it:\\n>> print pytz.timezone(\\'US/Pacific\\').normalize(d)\\n\\nProject Status\\nExperimental. Still estabilishing interfaces.\\nBasic Date and Time formatting and parsing are written and tested.\\nIntervals and Span are being actively developed.\\nDMC still requires some real world use before APIs are firmly set.\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '99',\n",
       "  'forks': '21',\n",
       "  'commits': '78'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Valor\\n \\nPython HTTP clients for APIs represented by JSON Schema.\\nThis is still super-early days yet, many things probably don't work. Use at your own risk.\\nAmong most other things, docs aren't done, but check this out:\\n$ heroku auth:whoami\\njacob@heroku.com\\n$ heroku apps\\nancient-thicket-4976\\narcane-reef-4005\\n...\\n\\n$ python -i heroku.py\\n>>> heroku.account.info().email\\nu'jacob@heroku.com'\\n>>> [app.name for app in heroku.app.list()]\\n[u'ancient-thicket-4976', u'arcane-reef-4005', ...]\\nThen see heroku.py as an example of how this works.\\n\\n\\nWhat's with the name? The Ruby version of the same thing is Heroics. Heroics. Valor. See what I did there?\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '99',\n",
       "  'forks': '2',\n",
       "  'commits': '27'},\n",
       " {'language': 'Python 82.0',\n",
       "  'readme': 'django-pdfutils\\nA simple django app to generate PDF documents.\\n\\nInstallation\\n\\nIn your settings.py, add pdfutils to your INSTALLED_APPS.\\n(r\\'^reports/\\', include(pdfutils.site.urls)), to your urls.py\\nAdd pdfutils.autodiscover() to your urls.py\\nCreate a report.py file in any installed django application.\\nCreate your report(s)\\nProfit!\\n\\nNote: If you are using buildout, don\\'t forget to put pdfutils\\nin your eggs section or else the django-pdfutils dependencies wont\\nbe installed.\\n\\nExample report\\nReports are basically views with custom methods and properties.\\n# -*- coding: utf-8 -*-\\n\\nfrom django.contrib.auth.models import User\\nfrom django.core.urlresolvers import reverse\\nfrom django.utils.translation import ugettext as _\\n\\nfrom pdfutils.reports import Report\\nfrom pdfutils.sites import site\\n\\n\\nclass MyUserReport(Report):\\n    title = _(\\'Users\\')\\n    template_name = \\'myapp/reports/users-report.html\\'\\n    slug = \\'users-report\\'\\n    orientation = \\'portrait\\'\\n\\n    def get_users(self):\\n        return User.objects.filter(is_staff=True)\\n\\n    def get_styles(self):\\n        \"\"\"\\n        It is possible to add or override style like so\\n        \"\"\"\\n        self.add_styles(\\'myapp/css/users-report.css\\')\\n        return super(AccountStatementReport, self).get_styles()\\n\\n    def filename(self):\\n        \"\"\"\\n        The filename can be generated dynamically and translated\\n        \"\"\"\\n        return _(\\'Users-report-%(count)s.pdf\\') % {\\'count\\': self.get_users().count() }\\n\\n    def get_context_data(self):\\n        \"\"\"\\n        Context data is injected just like a normal view\\n        \"\"\"\\n        context = super(AccountStatementReport, self).get_context_data()\\n        context[\\'user_list\\'] = self.get_users()\\n        return context\\n\\nsite.register(MyUserReport)\\nThe slug should obviously be unique since it is used to build the report URL.\\nFor example, with the default settings and URLs, the URL for report above would be /reports/users-report/.\\n\\nExample template\\n<html>\\n    <head>\\n        {{ STYLES|safe }}\\n    </head>\\n    <body class=\"{% if landscape %}landscape{% else %}portrait{% endif %}\">\\n        <ul>\\n            {% for user in user_list %}\\n            <li>{{ user }}</li>\\n            {% endfor %}\\n        </ul>\\n        <a href=\"{% url \\'pdfutils:your_report_slug\\' %}?format=html\">Add ?format=html for easy template debug</a>\\n    </body>\\n</html>\\nSome template variables are injected by default in reports:\\n\\ntitle\\nslug\\norientation\\nMEDIA_URL\\nSTATIC_URL\\nSTYLES\\n\\n\\nOverriding default CSS\\nSince the default CSS (base.css, portrait.css, landscape.css) are normal static files, they can be overrided\\nfrom any other django app which has a pdfutils folder in their static folder.\\nNote: Be sure your applications are listed in the right order in INSTALLED_APPS !\\n\\nDependencies\\n\\ndjango >=1.4, < 1.5.99\\ndecorator == 3.4.0, <= 3.9.9\\nPIL == 1.1.7\\nreportlab == 2.5\\nhtml5lib == 0.90\\nhttplib2 == 0.9\\npyPdf == 1.13\\nxhtml2pdf == 0.0.4\\ndjango-xhtml2pdf == 0.0.3\\n\\nNote: dependencies versions are specified in setup.py. The amount of time required to find the right\\ncombination of dependency versions is largely to blame for the creation of this project.\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '98',\n",
       "  'forks': '12',\n",
       "  'commits': '89'},\n",
       " {'language': 'Python 99.0',\n",
       "  'readme': \"PokemonGo-SlackBot\\nReceive Slack notification whenever a Pokémon spawns near a given location!\\nFor instructions please check:\\nhttps://www.themarketingtechnologist.co/pokemon-go-slack-notifications/\\n####UPDATE #1: Now also works in Japanese thanks to https://github.com/ttymsd!\\n####UPDATE #2: Thanks to Bart persoons, you can also hook up to Homey (https://www.athom.com/en/)\\nInstructions are in UPDATE #2 below the blog post: https://www.themarketingtechnologist.co/pokemon-go-slack-notifications/\\n####Update #3: Languages, stability and lured Pokémon!\\nSeveral updates:\\nMessage are now available in French and German thanks to Vincent. They can be set by setting the locale. The locale that needs to be used for the emojis can be set separately with -iL, default is English.\\nAs we also see in the app and at Pokevision, server quite often seem to have troubles. Therefore, the script now has a reconnect features instead of stopping the script in cases of issues. From experience, it's advisable to use a Google account, since it's more stable than PTC.\\nAnd last but not least, lured Pokémon! Pokevision doesn't show these (yet), so many thanks go to the tip by Daniel.\\n\",\n",
       "  'watchers': '14',\n",
       "  'stars': '97',\n",
       "  'forks': '29',\n",
       "  'commits': '32'},\n",
       " {'language': 'Python 90.7',\n",
       "  'readme': 'django-filepicker\\n\\n\\nA django plugin to make integrating with Filepicker.io even easier\\n##Installation\\n\\n\\nInstall the python package:\\npip install django-filepicker\\n\\n\\n\\nAdd your file picker api key to your settings.py file. You api key can be\\nfound in the developer portal.\\nFILEPICKER_API_KEY = <your api key>\\n\\n\\n\\nConfigure your media root.\\nCWD = os.getcwd()\\nMEDIA_ROOT = os.path.join(CWD, \\'media\\')\\n\\n\\n\\nAdd a filepicker field to your model and set the upload_to value.\\n# *Please note, that FPFileField handle only one file*\\n# In demo you can see how to handle multiple files upload.\\nfpfile = django_filepicker.models.FPFileField(upload_to=\\'uploads\\')\\n\\n\\n\\nModify your view to accept the uploaded files along with the post data.\\nform = models.TestModelForm(request.POST, request.FILES)\\nif form.is_valid():\\n    #Save will read the data and upload it to the location\\n    # defined in TestModel\\n    form.save()\\n\\n\\n\\nAdd the form.media variable above your other JavaScript calls.\\n<head>\\n    <title>Form Template Example</title>\\n    <!--  Normally this would go into a block defined in base.html that\\n          occurs before other JavaScript calls. -->\\n    {{ form.media }}\\n</head>\\n\\n<body>\\n    <form method=\"POST\" action=\"/\" enctype=\"multipart/form-data\">\\n        {{ form.as_p }}\\n        <input type=\"submit\" />\\n    </form>\\n</body>\\n\\n\\n\\n##Demo\\nTo see how all the pieces come together, see the example code in demo/, which you can run with the standard\\npython manage.py runserver command\\n###models.py\\nimport django_filepicker\\nclass TestModel(models.Model):\\n#FPFileField is a field that will render as a filepicker dragdrop widget, but\\n#When accessed will provide a File-like interface (so you can do fpfile.read(), for instance)\\nfpfile = django_filepicker.models.FPFileField(upload_to=\\'uploads\\')\\n###views.py\\n#building the form - automagically turns the uploaded fpurl into a File object\\nform = models.TestModelForm(request.POST, request.FILES)\\nif form.is_valid():\\n#Save will read the data and upload it to the location defined in TestModel\\nform.save()\\nBe sure to also provide your Filepicker.io api key, either as a parameter to the FPFileField or in settings.py as FILEPICKER_API_KEY\\n##Components\\n###Models\\nThe filepicker django library defines the FPFileField model field so you can get all the benefits of using Filepicker.io as a drop-in replacement for the standard django FileField. No need to change any of your view logic.\\n###Forms\\nSimilarly with the FPFileField for models, the filepicker django library defines a FPFileField for forms as well, that likewise serves as a drop-in replacement for the standard django FileField. There is also the FPUrlField if you want to store the Filepicker.io URL instead\\n###Middleware\\nAlso included is a middleware library that will take any Filepicker.io urls passed to the server, download the contents, and place the result in request.FILES. This way, you can keep your backend code for handling file uploads the same as before while adding all the front-end magic that Filepicker.io provides\\nIf you have any questions, don\\'t hesitate to reach out at contact@filepicker.io. For more information, see https://filepicker.io\\nOpen-sourced under the MIT License. Pull requests encouraged!\\n',\n",
       "  'watchers': '31',\n",
       "  'stars': '97',\n",
       "  'forks': '30',\n",
       "  'commits': '99'},\n",
       " {'language': 'Python 87.7',\n",
       "  'readme': 'Powerhose\\nPowerhose turns your CPU-bound tasks into I/O-bound tasks so your Python applications\\nare easier to scale.\\n\\nPowerhose is an implementation of the\\nRequest-Reply Broker\\npattern in ZMQ.\\nSee http://powerhose.readthedocs.org for a full documentation.\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '97',\n",
       "  'forks': '11',\n",
       "  'commits': '205'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Loop View for Android\\nEnglish 中文\\nAndroid LoopView is a powerful widget for unlimited rotation picture, It provides some configuration options and good control the appearance and operational requirements.\\nSimple usage picture:\\n\\nCustom layout usage picture:\\n\\nUsing LoopView in your application\\nIf you are building with Gradle, simply add the following line to the dependencies section of your build.gradle file:\\nsupport\\nimplementation \\'com.kevin:loopview:1.5.6\\'\\n\\nandroidX\\nimplementation \\'com.kevin:loopview:2.0.1\\'\\n\\nSimple Usage\\nConfigured as View in layout.xml\\nTo add the LoopView to your application, specify <com.kevin.loopview.BannerView in your layout XML.\\n<com.kevin.loopview.BannerView\\n    android:id=\"@+id/main_act_banner\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"192dp\">\\n</com.kevin.loopview.BannerView>\\n\\nConfigured Programmatically\\nBannerView mBannerView = (BannerView) this.findViewById(R.id.main_act_banner);\\n// set image loader, can set up any image engine.\\nmBannerView.setImageLoader(new ImageLoader() {\\n    @Override\\n    public void loadImage(ImageView imageView, String url, int placeholder) {\\n        Glide.with(imageView.getContext()).load(url).into(imageView);\\n    }\\n});\\nString json = LocalFileUtils.getStringFormAsset(this, \"loopview_date.json\");\\nLoopData loopData = new Gson().fromJson(json, LoopData.class);\\nmBannerView.setData(loopData);\\n// begin to loop\\nmBannerView.startAutoLoop();\\n\\nmBannerView.setOnItemClickListener(new BaseLoopAdapter.OnItemClickListener() {\\n    @Override\\n    public void onItemClick(View view, LoopData.ItemData itemData, int position) {\\n        // Open connection with browser\\n        Intent intent = new Intent();\\n        intent.setData(Uri.parse(itemData.link));\\n        intent.setAction(Intent.ACTION_VIEW);\\n        startActivity(intent);\\n    }\\n});\\n\\nMore configuration Usage\\nXML Usage\\nIf you decide to use BannerView as a view, you can define it in your xml layout like this:\\nxmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n\\n<com.kevin.loopview.BannerView\\n    android:id=\"@+id/adloop_act_adloopview\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"192dp\"\\n    app:loop_interval=\"5000\"\\n    app:loop_scrollDuration=\"2000\"\\n    app:loop_dotMargin=\"5dp\"\\n    app:loop_autoLoop=\"[true|false]\"\\n    app:loop_alwaysShowDot=\"[true|false]\"\\n    app:loop_dotSelector=\"@drawable/ad_dots_selector\"\\n    app:loop_placeholder=\"@mipmap/ic_launcher\"\\n    app:loop_layout=\"@layout/ad_loopview_layout\">\\n</com.kevin.loopview.BannerView>\\n\\nProgramme Usage\\n// Set page switching transition time\\nmLoopView.setScrollDuration(1000);\\n// Set time interval\\nmLoopView.setInterval(3000);\\n// To initialize the data in a collection\\nmLoopView.setData(List<Map<String, String>> data);\\n// Initialized data in entity mode\\nmLoopView.setData(LoopData rotateData);\\n// Initialized data in a collection mode\\nmLoopView.setData(List<String> images);\\n// Initialized data in a collection mode\\nmLoopView.setData(List<String> images, List<String> links);\\n// Initialized data in a collection mode\\nmLoopView.setData(List<String> images, List<String> descs, List<String> links);\\n// Get the running loop date\\nmLoopView.getData();\\n// Begin to auto Loop\\nmLoopView.startAutoLoop();\\n// Begin to auto Loop delay\\nmLoopView.startAutoLoop(long delayTimeInMills);\\n// Stop to auto Loop\\nmLoopView.stopAutoLoop();\\n// Set a custom loop layout\\nmLoopView.setLoopLayout(int layoutResId);\\n\\nNotes:\\nIn custom layout you must to use those ids loop_view_pager in ViewPager loop_view_dots in indicate point parent LinearLayout and loop_view_desc in description TextView;\\nMake sure you at least have loop_view_pager.\\nLicense\\nCopyright 2015 Kevin zhou\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '103',\n",
       "  'forks': '51',\n",
       "  'commits': '119'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'CircleImageView\\nCircleImageView is a component which display circle image with customization options\\n\\n\\nRelease notes\\n1.2.0\\n\\nImplemented animation for pressed event\\nAdded new attribute pressedRingWidth, pressedRingColor\\n\\n1.1.4\\n\\nFix bug (issue #3 on github)\\n\\n1.1.3\\n\\nImprove component performance\\nOptimization of image drawing\\n\\n1.1.2\\n\\nSupport padding parameters (paddingLeft, paddingTop, paddingRight, paddingBottom).\\n\\n1.1.1\\n\\nFix bug (issue #1 on github).\\n\\n1.1.0\\n\\nshows shadow for image;\\nshows shadow for border.\\n\\n1.0.0\\n\\nchange background for *.png images;\\nshows images;\\nrealize click listener for this component;\\nshows border (component is selected or component is unselected).\\n\\nUsage\\nTo make a circular ImageView, add this CircleImageView library to your project and add CircleImageView in your layout XML.\\nYou can also grab it via Gradle:\\n      compile \\'com.alexzh:circleimageview:1.2.0\\'\\nor Maven:\\n<dependency>\\n    <groupId>com.alexzh</groupId>\\n    <artifactId>circleimageview</artifactId>\\n    <version>1.2.0</version>\\n    <type>pom</type>\\n</dependency>\\nXML\\n    <com.alexzh.circleimageview.CircleImageView\\n        android:id=\"@+id/imageView\"\\n        android:layout_width=\"192dp\"\\n        android:layout_height=\"192dp\"\\n        android:clickable=\"true\"\\n        android:src=\"@drawable/android_logo\"\\n        android:layout_alignParentTop=\"true\"\\n        android:layout_centerHorizontal=\"true\"\\n        android:layout_marginTop=\"10dp\"\\n        app:view_backgroundColor=\"@color/colorPrimary\"\\n        app:view_shadowRadius=\"4dp\"\\n        app:view_shadowDx=\"2dp\"\\n        app:view_shadowDy=\"0dp\"\\n        app:view_shadowColor=\"@color/grey\"\\n        app:view_borderWidth=\"4dp\"\\n        app:view_selectedColor=\"@color/blue\"\\n        app:view_borderColor=\"@android:color/darker_gray\"/>\\nYou may use the following properties in your XML to customize your CircularImageView.\\nProperties:\\n\\napp:view_backgroundColor    (color)\\napp:view_borderColor        (color)\\napp:view_borderWidth        (dimension)\\napp:view_selectedColor      (color)\\napp:view_shadowRadius       (dimension)\\napp:view_shadowDx           (dimension)\\napp:view_shadowDy           (dimension)\\napp:view_shadowColor        (color)\\n\\nJAVA\\n\\n\\nsetOnItemSelectedClickListener(ItemSelectedListener listener) - let\\'s handle onSelected(View view) and onUnselected(View view)\\n\\n\\nonSelected(View view) - view is selected\\n\\n\\nonUnselected(View view) - view is unselected\\n\\n\\nDeveloped By\\nAliaksandr Zhukovich - http://alexzh.com\\nLicense\\nCopyright (C) 2016 Aliaksandr Zhukovich (http://alexzh.com)\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n     http://www.apache.org/licenses/LICENSE-2.0\\t     \\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '103',\n",
       "  'forks': '27',\n",
       "  'commits': '58'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'CircleImageView\\nCircleImageView is a component which display circle image with customization options\\n\\n\\nRelease notes\\n1.2.0\\n\\nImplemented animation for pressed event\\nAdded new attribute pressedRingWidth, pressedRingColor\\n\\n1.1.4\\n\\nFix bug (issue #3 on github)\\n\\n1.1.3\\n\\nImprove component performance\\nOptimization of image drawing\\n\\n1.1.2\\n\\nSupport padding parameters (paddingLeft, paddingTop, paddingRight, paddingBottom).\\n\\n1.1.1\\n\\nFix bug (issue #1 on github).\\n\\n1.1.0\\n\\nshows shadow for image;\\nshows shadow for border.\\n\\n1.0.0\\n\\nchange background for *.png images;\\nshows images;\\nrealize click listener for this component;\\nshows border (component is selected or component is unselected).\\n\\nUsage\\nTo make a circular ImageView, add this CircleImageView library to your project and add CircleImageView in your layout XML.\\nYou can also grab it via Gradle:\\n      compile \\'com.alexzh:circleimageview:1.2.0\\'\\nor Maven:\\n<dependency>\\n    <groupId>com.alexzh</groupId>\\n    <artifactId>circleimageview</artifactId>\\n    <version>1.2.0</version>\\n    <type>pom</type>\\n</dependency>\\nXML\\n    <com.alexzh.circleimageview.CircleImageView\\n        android:id=\"@+id/imageView\"\\n        android:layout_width=\"192dp\"\\n        android:layout_height=\"192dp\"\\n        android:clickable=\"true\"\\n        android:src=\"@drawable/android_logo\"\\n        android:layout_alignParentTop=\"true\"\\n        android:layout_centerHorizontal=\"true\"\\n        android:layout_marginTop=\"10dp\"\\n        app:view_backgroundColor=\"@color/colorPrimary\"\\n        app:view_shadowRadius=\"4dp\"\\n        app:view_shadowDx=\"2dp\"\\n        app:view_shadowDy=\"0dp\"\\n        app:view_shadowColor=\"@color/grey\"\\n        app:view_borderWidth=\"4dp\"\\n        app:view_selectedColor=\"@color/blue\"\\n        app:view_borderColor=\"@android:color/darker_gray\"/>\\nYou may use the following properties in your XML to customize your CircularImageView.\\nProperties:\\n\\napp:view_backgroundColor    (color)\\napp:view_borderColor        (color)\\napp:view_borderWidth        (dimension)\\napp:view_selectedColor      (color)\\napp:view_shadowRadius       (dimension)\\napp:view_shadowDx           (dimension)\\napp:view_shadowDy           (dimension)\\napp:view_shadowColor        (color)\\n\\nJAVA\\n\\n\\nsetOnItemSelectedClickListener(ItemSelectedListener listener) - let\\'s handle onSelected(View view) and onUnselected(View view)\\n\\n\\nonSelected(View view) - view is selected\\n\\n\\nonUnselected(View view) - view is unselected\\n\\n\\nDeveloped By\\nAliaksandr Zhukovich - http://alexzh.com\\nLicense\\nCopyright (C) 2016 Aliaksandr Zhukovich (http://alexzh.com)\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n     http://www.apache.org/licenses/LICENSE-2.0\\t     \\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '17',\n",
       "  'stars': '102',\n",
       "  'forks': '54',\n",
       "  'commits': '152'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '模仿墨迹天气3.0引导界面\\n\\n屏幕录制使用的licecap\\nhttp://www.cockos.com/licecap/\\nREADME.md编辑参考\\nhttp://www.tuicool.com/articles/zIJrEjn\\n@author 张兴业\\nhttp://blog.csdn.net/xyz_lmn\\n我的新浪微博：@张兴业TBOW\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '102',\n",
       "  'forks': '48',\n",
       "  'commits': '14'},\n",
       " {'language': 'Java 99.2',\n",
       "  'readme': 'CHelper is distrubuted under LGPL\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '102',\n",
       "  'forks': '52',\n",
       "  'commits': '256'},\n",
       " {'language': 'Java 70.9',\n",
       "  'readme': 'CHelper is distrubuted under LGPL\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '102',\n",
       "  'forks': '36',\n",
       "  'commits': '77'},\n",
       " {'language': 'Java 81.2',\n",
       "  'readme': 'multi-column-list-adapter  \\xa0\\xa0 \\nMultiColumnListAdapter is a cursor adapter that enables you to make a ListView that looks like a GridView. One important benefit is that you can add headers and footers to your grid-looking list.\\n\\nSample App\\nThe sample app demonstrates how to use the MultiColumnListAdapter to create a GridView-looking layout with a header. You can build it from source or install it from the Play Store.\\nDownload\\nGrab the library from Maven central\\n<dependency>\\n    <groupId>com.twotoasters.multicolumnlistadapter</groupId>\\n    <artifactId>library</artifactId>\\n    <version>1.0.0</version>\\n</dependency>\\nor Gradle:\\ncompile \\'com.twotoasters.multicolumnlistadapter:library:1.0.+\\'\\nCredit\\nMultiColumnListAdapter was created by Two Toasters in development with Ebates.\\nLicense\\nCopyright 2014 Two Toasters\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '102',\n",
       "  'forks': '19',\n",
       "  'commits': '15'},\n",
       " {'language': 'Java 97.6',\n",
       "  'readme': 'cleo-primer\\nThe cleo-primer package provides a basic RESTful implementation of partial, out-of-order and real-time typeahead services.\\nIt is built on Cleo, an open source package from LinkedIn.\\nHomepage\\nFind out more about Cleo at http://sna-projects.com/cleo.\\nLicense\\nApache Public License (APL) 2.0\\nArtifacts\\ncleo-primer.war\\nMaven\\ngroupId: com.sna-projects.cleo\\nartifactId: cleo-primer\\nversion: 1.0\\nBuild the war\\nmvn clean package\\n\\nLaunch WebApp\\nLaunch the cleo-primer web application using the next command from the\\nmain folder:\\nMAVEN_OPTS=\"-Xms1g -Xmx1g\" mvn jetty:run -Dcleo.instance.name=Company -Dcleo.instance.type=cleo.primer.GenericTypeaheadInstance -Dcleo.instance.conf=src/main/resources/config/generic-typeahead\\n\\nYou can customize your web application by choosing different values for parameters\\ncleo.instance.name, cleo.instance.type and cleo.instance.conf. Depending on the size\\nof your data sets, you may need to specify a different JVM heap size.\\nPost a list of new elements\\n./scripts/post-element-list.sh dat/nasdaq-company-list.xml\\n\\nPost a new element\\n./scripts/post-element.sh dat/nasdaq-google.xml dat/nasdaq-intel.xml\\n\\nSearch\\nVisit the URL below to try out cleo-primer.\\nhttp://localhost:8080/cleo-primer\\n\\nhttp://localhost:8080/cleo-primer/rest/elements/search?query=goo\\n\\nEclipse\\nSet up Eclipse by executing the command below:\\nmvn eclipse:eclipse\\n\\nInside Eclipse, select Preferences > Java > Build Path > Classpath Variables. Define a new classpath variable M2_REPO and assign maven repository.\\nFor more information, check out http://maven.apache.org/guides/mini/guide-ide-eclipse.html\\nContribute\\nFor help please see the discussion group.  Bugs and feature requests can be filed here.\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '102',\n",
       "  'forks': '26',\n",
       "  'commits': '54'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Geo Cluster Facet Plugin for elasticsearch\\nThis plugin provides a facet for elasticsearch to aggregate documents with geo_point fields.\\nA naive, distance-based algorithm is used to build rectangular (and potentially overlapping) clusters with a weighted center.\\nTo install the plugin, run:\\nbin/plugin --url https://github.com/zenobase/geocluster-facet/releases/download/0.0.11/geocluster-facet-0.0.11.jar --install geocluster-facet\\n\\nVersions\\n\\n\\n\\ngeocluster-facet\\nelasticsearch\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t0.0.12 -> master\\n        \\t\\t\\n\\n1.4.x -> 1.7.x\\n\\n\\n0.0.11\\n1.2.x, 1.3.x\\n\\n\\n0.0.10\\n1.0.x, 1.1.x\\n\\n\\n0.0.9\\n0.90.6, 0.90.7\\n\\n\\n0.0.8\\n0.90.5\\n\\n\\n0.0.7\\n0.90.3\\n\\n\\n0.0.6\\n0.90.2\\n\\n\\n0.0.5\\n0.90.0, 0.90.1\\n\\n\\n0.0.2 -> 0.0.4\\n0.20.x\\n\\n\\n0.0.1\\n0.19.x\\n\\n\\n\\nParameters\\n\\n\\n\\nfield\\nThe name of a field of type `geo_point`.\\n\\n\\nfactor\\nControls the amount of clustering, from 0.0 (don\\'t cluster any points) to 1.0 (create a single cluster containing all points). \\n\\t\\t\\tDefaults to 0.1. This value is relative to the size of the area that contains points, so it does not need to be adjusted e.g. when \\n\\t\\t\\tzooming in on a map.\\n\\n\\n\\nExample\\nQuery:\\n{\\n    \"query\" : { ... }\\n    \"facets\" : {\\n        \"places\" : { \\n            \"geo_cluster\" : {\\n                \"field\" : \"location\",\\n                \"factor\" : 0.5\\n            }\\n        }\\n    }\\n}\\nSearchSourceBuilder search = ...\\nsearch.facet(new GeoClusterFacetBuilder(\"places\", \"location\", 0.5));\\nResult:\\n{\\n    ...\\n    \"facets\" : {\\n        \"geo_cluster\" : [ {\\n        \\t\"count\" : 1,\\n        \\t\"lat\" : 36.08,\\n        \\t\"lon\" : -115.17\\n        }, {\\n            \"count\" : 3,\\n            \"lat\" : 39.75,\\n            \"lon\" : -104.87,\\n            \"lat_min\" : 37.00,\\n            \"lat_max\" : 41.00,\\n            \"lon_min\" : -109.05,\\n            \"lon_max\" : -102.04\\n        } ]\\n    }\\n}\\nLicense\\nThis software is licensed under the Apache 2 license, quoted below.\\n\\nCopyright 2012-2014 Zenobase LLC\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not\\nuse this file except in compliance with the License. You may obtain a copy of\\nthe License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\nLicense for the specific language governing permissions and limitations under\\nthe License.\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '101',\n",
       "  'forks': '29',\n",
       "  'commits': '51'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Geo Cluster Facet Plugin for elasticsearch\\nThis plugin provides a facet for elasticsearch to aggregate documents with geo_point fields.\\nA naive, distance-based algorithm is used to build rectangular (and potentially overlapping) clusters with a weighted center.\\nTo install the plugin, run:\\nbin/plugin --url https://github.com/zenobase/geocluster-facet/releases/download/0.0.11/geocluster-facet-0.0.11.jar --install geocluster-facet\\n\\nVersions\\n\\n\\n\\ngeocluster-facet\\nelasticsearch\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t0.0.12 -> master\\n        \\t\\t\\n\\n1.4.x -> 1.7.x\\n\\n\\n0.0.11\\n1.2.x, 1.3.x\\n\\n\\n0.0.10\\n1.0.x, 1.1.x\\n\\n\\n0.0.9\\n0.90.6, 0.90.7\\n\\n\\n0.0.8\\n0.90.5\\n\\n\\n0.0.7\\n0.90.3\\n\\n\\n0.0.6\\n0.90.2\\n\\n\\n0.0.5\\n0.90.0, 0.90.1\\n\\n\\n0.0.2 -> 0.0.4\\n0.20.x\\n\\n\\n0.0.1\\n0.19.x\\n\\n\\n\\nParameters\\n\\n\\n\\nfield\\nThe name of a field of type `geo_point`.\\n\\n\\nfactor\\nControls the amount of clustering, from 0.0 (don\\'t cluster any points) to 1.0 (create a single cluster containing all points). \\n\\t\\t\\tDefaults to 0.1. This value is relative to the size of the area that contains points, so it does not need to be adjusted e.g. when \\n\\t\\t\\tzooming in on a map.\\n\\n\\n\\nExample\\nQuery:\\n{\\n    \"query\" : { ... }\\n    \"facets\" : {\\n        \"places\" : { \\n            \"geo_cluster\" : {\\n                \"field\" : \"location\",\\n                \"factor\" : 0.5\\n            }\\n        }\\n    }\\n}\\nSearchSourceBuilder search = ...\\nsearch.facet(new GeoClusterFacetBuilder(\"places\", \"location\", 0.5));\\nResult:\\n{\\n    ...\\n    \"facets\" : {\\n        \"geo_cluster\" : [ {\\n        \\t\"count\" : 1,\\n        \\t\"lat\" : 36.08,\\n        \\t\"lon\" : -115.17\\n        }, {\\n            \"count\" : 3,\\n            \"lat\" : 39.75,\\n            \"lon\" : -104.87,\\n            \"lat_min\" : 37.00,\\n            \"lat_max\" : 41.00,\\n            \"lon_min\" : -109.05,\\n            \"lon_max\" : -102.04\\n        } ]\\n    }\\n}\\nLicense\\nThis software is licensed under the Apache 2 license, quoted below.\\n\\nCopyright 2012-2014 Zenobase LLC\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not\\nuse this file except in compliance with the License. You may obtain a copy of\\nthe License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\nLicense for the specific language governing permissions and limitations under\\nthe License.\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '101',\n",
       "  'forks': '22',\n",
       "  'commits': '337'},\n",
       " {'language': 'C++ 64.4',\n",
       "  'readme': \"Libral\\n\\nLibral is a systems management library that makes it possible to query and\\nmodify system resources (files, packages, services, etc.) through a\\ndesired-state API. Its goals are to:\\n\\nProvide a management API for UNIX-like systems to query and modify\\nsystem resources\\nMake managing new kinds of resources very simple; in particular, there is\\nno need to write native code to add new providers and they can be\\narbitrary scripts that adhere to one of a number of simple calling\\nconventions\\nExpress both current state and desired state in a simple, unified form\\nGuarantee that all changes are done idempotently by enforcing desired\\nstate only when changes are needed\\nHave a very small default footprint that enables use of libral in\\nresource-constrained environments such as devices or containers\\nBe versatile enough to serve as the basis of more extensive configuration\\nmanagement systems, such as\\nPuppet, without being directly\\ndependent on any one of them.\\n\\nGetting in touch\\n\\nMailing list: libral\\nIRC: irc.freenode.net:#libral\\n\\nBuilding and installation\\nIf you just want to quickly try out libral, you can download a\\nprecompiled tarball\\n(GPG signature)\\nthat should work on any Linux machine that has glibc 2.12 or later. (It\\nmight actually work with glibc 2.8 or later.) If you succeed in that, please\\nlet us know.\\nWe have reports of the precompiled binaries working on 64-bit Fedora 21-25,\\nRHEL 5-7 (and related distributions like CentOS), Debian 6-9,\\nUbuntu 10.04-16.10, and SLES 11-12. The binaries will not work on\\n32-bit and pre-glibc 2.8 systems: RHEL 4, SLES 10, ...\\nIn case you do need to build from source, which is not required for\\nprovider development, only if you want to work on the core libral library,\\nthis document contains instructions on building libral.\\nDocker\\nYou can also try out libral in the context of a Docker container. The\\nDockerfile in the repository allows for building an image quickly based\\non the above mentioned precompiled tarball.\\ndocker build -t puppet/libral .\\n\\nRunning this can then be done with Docker, for instance the following\\ninvocation will launch ralsh in the context of the container.\\ndocker run --rm -t puppet/libral\\n\\nThis is intended for exploring the CLI and experimenting with providers.\\nUsage\\nAfter you build libral, or after you download and unpack the\\nprecompiled tarball,\\nyou can try things out by running ralsh:\\n    # If you downloaded the precompiled tarball\\n    alias ralsh=$TARBALL_LOCATION/ral/bin/ralsh\\n\\n    # Only if you built libral from source\\n    export RALSH_DATA_DIR=$LIBRAL_CHECKOUT/data\\n    alias ralsh=$LIBRAL_CHECKOUT/bin/ralsh\\n\\n    # list available types\\n    ralsh\\n    # list all instances of a type\\n    ralsh mount\\n    # list a specific instance\\n    ralsh service crond\\n    # make a change for the better\\n    ralsh service crond ensure=stopped\\n\\n    # Do the same against a remote system to which you have ssh access\\n    ralsh -t ahost\\n    ralsh -t ahost package sudo\\nThe default output from ralsh is meant for human consumption and looks a\\nlot like Puppet. It is also possible to have ralsh produce\\nJSON output by passing the --json flag.\\nMany of the providers that libral knows about are separate\\nscripts. ralsh searches them in the following order. In each case, the\\nproviders must be executable scripts in a subdirectory providers in the\\nmentioned directory ending in .prov:\\n\\nif the environment variable RALSH_DATA_DIR is set, look in the\\ndirectory this variable is set to\\nif the --include option to ralsh is given, look in that directory\\n(the option can be given multiple times)\\ndefault to a directory determined at build time, by default\\n/usr/share/libral/data\\n\\nRunning inside a container\\nThe\\nprecompiled tarball\\ncontains a statically linked ralsh and all supporting files. After\\nunpacking the tarball, you can copy it into a container and run it like\\nthis:\\n    CONTAINER=<some_container>\\n    docker cp $TARBALL_LOCATION/ral $CONTAINER:/tmp\\n    docker exec $CONTAINER /bin/sh -c /tmp/ral/bin/ralsh\\n    docker exec $CONTAINER /bin/sh -c '/tmp/ral/bin/ralsh user root'\\nWriting providers\\nWhat resources libral can manage is determined by what providers are\\navailable. Some providers are built in and implemented in C++, but doing\\nthat is of course labor intensive and should only be done for good\\nreason. It is much simpler, and recommended, that new providers first be\\nimplemented as external providers. External providers are nothing more than\\nscripts or other executables that follow one of libral's calling\\nconventions. The different calling conventions trade off implementation\\ncomplexity for expressive power.\\nThe following calling conventions are available. If you are just getting\\nstarted with libral, you should write your first providers using the\\nsimple or json calling convention:\\n\\nsimple\\njson: input/output via JSON\\njson_batch (planned,maybe): input/output via JSON, can operate on multiple resources at once\\nnative\\n\\nFor all of these, you will also want to read up on the\\nmetadata that each provider needs to produce to describe\\nitself.\\nTo start a new provider, follow these steps:\\n\\nDecide on some working directory DIR, run mkdir $DIR/providers, and\\nCreate a file $DIR/providers/myprovider.prov and make it executable\\nMake sure that running myprovider.prov ral_action=describe returns\\nvalid provider metadata, especially a valid type. For\\nnow it doesn't matter what the type is, let's call it MYTYPE\\nRun ralsh -I $DIR $MYTYPE. This will ask the provider to list all\\ninstances of the type using the list action. Get that working\\nRun ralsh -I $DIR $MYTYPE NAME. This will ask the provider to find a\\nresource with name NAME using the find action. Get that working,\\ntoo.\\nRun ralsh -I $DIR $MYTYPE NAME ATTR=VALUE.... This will ask the\\nprovider to update the resource NAME using the update action.\\nYou're done; your provider is ready for a pull request here ;)\\n\\nTodo list\\n\\n finish mount provider\\n add a shell provider\\n event reporting on update\\n simple type system and checking for provider attributes\\n produce help/details about providers\\n add a json calling convention and use it in a provider\\nmore core providers\\n\\n cron\\n file\\n group (groupXXX)\\n host\\n package (besides dnf and yum)\\n service (besides systemd, upstart and sysv)\\n user (userXXX)\\n\\n\\neven more core providers\\n\\n interface\\n k5login\\n mailalias\\n selboolean\\n selmodule\\n sshkey\\n ssh-authorized-key\\n vlan\\n yumrepo\\n\\n\\n add a remote provider (using an HTTP API)\\n adapt providers to multiple OS (maybe using mount)\\n add support for running providers over ssh (see this PR for details)\\n noop mode\\n expand the type system to cover as much of Puppet 4 as is reasonable\\n\\nSome language\\nNaming things is hard; here's the terms libral uses:\\n\\nType: the abstract description of an entity we need to manage; this is\\nthe external interface through which entities are managed. I am not\\nentirely convinced this belongs in libral at all\\nProvider: something that knows how to manage a certain kind of thing\\nwith very specific means; for example something that knows how to manage\\nusers with the user* commands, or how to manage mounts on Linux\\nResource: an instance of something that a provider manages. This is\\nclosely tied both to what is being managed and how it is being\\nmanaged. The important thing is that resources expose a desired-state\\ninterface and therefore abstract away the details of how changes are made\\n\\nFIXME: we need some conventions around some special resource\\nproperties; especially, namevar should always be 'name' and the primary key\\nfor all resources from this provider, and 'ensure' should have a special\\nmeaning (or should it?)\\nOpen questions\\n\\nDo we need types at all at this level ?\\nCan we get away without any provider selection logic ? There are two\\nreasons why provider selection is necessary:\\n\\nadjust to system differences. Could we push those to compile time ?\\nmanage different things that are somewhat similar, like system packages\\nand gems, or local users and LDAP users ? This would push this\\nmodelling decision into a layer above libral.\\n\\n\\nWould it be better to make providers responsible for event generation\\nrather than doing that in the framework ?\\n\\n\",\n",
       "  'watchers': '115',\n",
       "  'stars': '62',\n",
       "  'forks': '14',\n",
       "  'commits': '347'},\n",
       " {'language': 'C++ 63.5',\n",
       "  'readme': 'DSST: Discriminative Scale Space Tracker\\nDSST is part of the Visual Object Tracking Repository,\\nwhich aims at providing a central repository for state-of-the-art tracking algorithms that are freely available.\\nThe source code for this tracker was submitted for the VOT2014 challenge.\\nA minor adaption was made by using MATLAB\\'s imresize() function\\ninstead of a custom mex file to increase portability.\\nDependencies:\\n\\nMatlab\\nImage Processing Toolbox\\nSignal Processing Toolbox\\n\\nThe following description was copied literally from the original author.\\nABSTRACT\\nThe Discriminative Scale Space Tracker (DSST), proposed in [2], extends the Minimum Output Sum of Squared\\nErrors (MOSSE) tracker [1] with robust scale estimation. The MOSSE tracker works by training a discriminative\\ncorrelation filter on a set of observed sample grayscale patches. This correlation filter is then applied to estimate the\\ntarget translation in the next frame. The DSST additionally learns a one-dimensional discriminative scale filter, that\\nis used to estimate the target size. The scale filter is trained by extracting several sample patches at different scales\\naround the current target position in the image. Each sample is represented by a fixed-length feature vector based on\\nHOG. These samples are used to learn a multi-channel one-dimensional discriminative filter for scale estimation. This\\nscale filter is generic and can be combined with any tracker that is limited to only estimating the target translation.\\nGiven a new image, the DSST first applies a translation filter to obtain the most probable target location. The scale\\nfilter is then applied at this location to estimate the target size. The tracking model is then updated with the new\\ninformation information of the target and background appearance. For the translation filter, we combine the intensity\\nfeatures employed in the MOSSE tracker with a pixel-dense representation of HOG-features.\\n[1] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui. Visual object tracking using adaptive correlation filters. In CVPR,\\n2010.\\n[2] M. Danelljan, G. Häger, F. Shahbaz Khan, and M. Felsberg. Accurate scale estimation for robust visual tracking. In Proceedings\\nof the British Machine Vision Conference (BMVC), 2014.\\nREADME\\nThis MATLAB code integrates the Discriminative Scale Space Tracker (DSST) [1] into the VOT 2014 evaluation kit. The implementation is built upon the code provided by [2]. The code provided by [3] is used for computing the HOG features.\\nInstructions:\\nUse the wrapper function \"wrapper.m\" without trax.\\nThe code may be publicly available from the VOT homepage.\\nContact:\\nMartin Danelljan\\nmartin.danelljan@liu.se\\n[1] Martin Danelljan, Gustav Häger, Fahad Shahbaz Khan and Michael Felsberg.\\n\"Accurate Scale Estimation for Robust Visual Tracking\".\\nProceedings of the British Machine Vision Conference (BMVC), 2014.\\n[2] J. Henriques, R. Caseiro, P. Martins, and J. Batista.\\n\"Exploiting the circulant structure of tracking-by-detection with kernels.\"\\nIn ECCV, 2012.\\n[3] Piotr Dollár.\\n\"Piotr\\'s Image and Video Matlab Toolbox (PMT).\"\\nhttp://vision.ucsd.edu/~pdollar/toolbox/doc/index.html.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '62',\n",
       "  'forks': '28',\n",
       "  'commits': '8'},\n",
       " {'language': 'C++ 80.4',\n",
       "  'readme': '\\n\\n\\nLibrary: Virgil Crypto\\nLibrary features | Supported algorithms | Build | Benchmark | Docs | Support\\nIntroduction\\nWelcome to Virgil Security!\\nVirgil Security guides software developers into the forthcoming security world in which everything will be encrypted (and passwords will be eliminated).  In this world, the days of developers having to raise millions of dollars to build a secure chat, secure email, secure file-sharing, or a secure anything have come to an end.  Now developers can instead focus on building features that give them a competitive market advantage while end-users can enjoy the privacy and security they increasingly demand.\\nVirgil Security offers this security via a stack of security libraries (ECIES with Crypto Agility wrapped in Virgil Cryptogram) and all the necessary infrastructure to enable seamless, end-to-end encryption for any application, platform or device. End-to-end encryption can be used for a variety of important reasons: compliance with regulations like HIPAA and GDPR, the transfer and storage of PII, general user privacy as a feature, breach risk mitigation and more.\\nVirgil Crypto also has all required cryptographic functions and primitives to perform an implementation of Pythia technology.\\nSee below for currently available languages and platforms. Get in touch with us to get beta access to our Key infrastructure.\\nLibrary Features\\nSupported crypto operations\\n\\nGenerate keys;\\nEncrypt data;\\nDecrypt data;\\nSign data;\\nVerify data;\\nPythia protocol.\\n\\nSupported platforms\\nCrypto Library is suitable for the following platforms:\\n\\nDesktop (Windows, Linux, MacOS);\\nMobile (iOS, Android, watchOS, tvOS);\\nWeb (WebAssembly, AsmJS)\\n\\nSupported languages\\nCrypto Library is written in C++ [CDN] and supports bindings for the following programming languages:\\n\\nGo [CDN]\\nPHP [CDN]\\nPython [CDN]\\nRuby [CDN]\\nJava [CDN]\\nC# [CDN]\\nAsmJS [CDN]\\nNodeJS [CDN]\\nWebAssembly [CDN]\\n\\nSwift/Objective_C language can use the Virgil Crypto Library directly, without any bind.\\nAvailable Wrappers\\nVirgil also has special wrappers for simplifying Crypto Library implementation in your digital solutions. We support wrappers for the following programming languages:\\n\\nGo\\nObjective-C/Swift\\nC#/.NET\\nRuby\\nPython\\nPHP\\nJS\\n\\nSupported algorithms\\n\\n\\n\\nPurpose\\nAlgorithm, Source\\n\\n\\n\\n\\nKey Generation, PRNG\\nNIST SP 800-90A\\n\\n\\nKey Derivation\\nKDF2*, HKDF\\n\\n\\nKey Exchange\\nX25519*, ECDH, RSA\\n\\n\\nHashing\\nSHA-2 (256/384*/512), Blake2\\n\\n\\nDigital Signature\\nEd25519*, ECDSA, RSASSA-PSS\\n\\n\\nEntropy Source\\nLinux /dev/urandom, Windows CryptGenRandom()\\n\\n\\nSymmetric Algorithms\\nAES GCM*, AES CBC\\n\\n\\nElliptic Curves\\nX25519, Ed25519*, Koblitz (secp192k1, secp224k1, secp256k1), Brainpool (bp256r1, bp384r1, bp512r1),  NIST (secp256r1, secp192r1, secp224r1, secp384r1, secp521r1)\\n\\n\\n\\n\\n* - used by default.\\n\\nBuild\\nPrerequisites\\nThe page lists the prerequisite packages which need to be installed on the different platforms to be able to configure and to build Virgil Crypto Library.\\n\\nCompiler:\\n\\ng++ (version >= 4.9), or\\nclang++ (version >= 3.6), or\\nmsvc++ (version >= 14.0)\\n\\n\\nBuild tools:\\n\\ncmake (version >= 3.10)\\nmake\\n\\n\\nOther tools:\\n\\ngit\\nswig (version >= 3.0.12), optional for C++ build\\ndoxygen (optional)\\n\\n\\n\\nBuild the Library\\nThis section describes how to build Virgil Crypto Library for а particular OS.\\nStep 1 - Get source code\\n\\nOpen Terminal.\\nGet the source code:\\n\\n> git clone https://github.com/VirgilSecurity/virgil-crypto.git\\nStep 2 - Run a build Script\\nUnix-like OS:\\n> cd virgil-crypto\\n> ./utils/build.sh\\nWindows OS:\\n> cd virgil-crypto\\n> ./utils/build.bat\\nRun the build script with the option -h to get help on how to build a library for a necessary OS, Platforms or languages.\\nBuild command has the following syntax:\\n ./utils/build.sh [--target=<target>] [--feature=<feature>] [--src=<src_dir>] [--build=<build_dir>] [--install=<install_dir>]\\nwhere the command options are:\\n\\n<target> - (default = cpp) target to build which contains two parts <name>[-<version>], where <name>:\\n\\n\\n\\n\\n<name>\\nbuild information\\n\\n\\n\\n\\ncpp\\nbuild C++ library\\n\\n\\nmacos\\nbuild framework for Apple macOSX, requirements: OS X, Xcode\\n\\n\\nios\\nbuild framework for Apple iOS, requirements: OS X, Xcode\\n\\n\\nwatchos\\nbuild framework for Apple WatchOS, requirements: OS X, Xcode\\n\\n\\ntvos\\nbuild framework for Apple TVOS, requirements: OS X, Xcode\\n\\n\\nphp\\nbuild PHP library, requirements: php-dev\\n\\n\\npython\\nbuild Python library\\n\\n\\nruby\\nbuild Ruby library\\n\\n\\njava\\nbuild Java library, requirements: $JAVA_HOME\\n\\n\\njava_android\\nbuild Java library under Android platform, requirements: $ANDROID_NDK\\n\\n\\nnet\\nbuild .NET library, requirements: .NET or Mono\\n\\n\\nnet_macos\\nbuild .NET library under Apple macOSX platform, requirements: Mono, OS X, Xcode\\n\\n\\nnet_ios\\nbuild .NET library under Apple iOS platform, requirements: Mono, OS X, Xcode\\n\\n\\nnet_applewatchos\\nbuild .NET library under WatchOS platform, requirements: Mono, OS X, Xcode\\n\\n\\nnet_appletvos\\nbuild .NET library under TVOS platform, requirements: Mono, OS X, Xcode\\n\\n\\nnet_android\\nbuild .NET library under Android platform, requirements: Mono, $ANDROID_NDK\\n\\n\\nasmjs\\nbuild AsmJS library, requirements: $EMSDK_HOME\\n\\n\\nwebasm\\nbuild WebAssembly library, requirements: $EMSDK_HOME\\n\\n\\nnodejs\\nbuild NodeJS module\\n\\n\\ngo\\nbuild Golang library\\n\\n\\n\\n\\n<feature> - available features:\\n\\npythia - ask to enable feature Pythia. Some targets enable this feature by default.\\n\\n\\n<src_dir> - a path to the directory where a root CMakeLists.txt file is located (default = .).\\n<build_dir> - a path to the directory where temp files will be stored (default = build/<target>).\\n<install_dir> - a path to the directory where library files will be installed (default = install/<target>).\\n\\n\\nAll available Crypto Library versions you can find here.\\n\\nBenchmark\\nYou can find out benchmark of the Crypto Library in the benchmark.md file.\\nDocs\\nWe always try to make cryptography accessible for programmers, and the documentation below can get you started today.\\n\\nCrypto Library API\\nLibrary usage examples\\n\\nGenerate a key pair\\nimport and export keys\\ngenerate and verify signature\\nencrypt and decrypt data\\n\\n\\nVirgil CLI for the Crypto Library\\n\\nSupport\\nOur developer support team is here to help you. Find out more information on our Help Center.\\nYou can find us on Twitter or send us email support@VirgilSecurity.com.\\nAlso, get extra help from our support team on Slack.\\nLicense\\nBSD 3-Clause. See LICENSE for details.\\n',\n",
       "  'watchers': '31',\n",
       "  'stars': '62',\n",
       "  'forks': '22',\n",
       "  'commits': '1,493'},\n",
       " {'language': 'C++ 63.4',\n",
       "  'readme': 'ZeroNights2017 (c) 2017 James Forshaw\\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\\nWindows access tokens.\\nConsists of two parts.\\n\\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\\n\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '62',\n",
       "  'forks': '16',\n",
       "  'commits': '2'},\n",
       " {'language': 'C++ 89.0',\n",
       "  'readme': 'ZeroNights2017 (c) 2017 James Forshaw\\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\\nWindows access tokens.\\nConsists of two parts.\\n\\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '62',\n",
       "  'forks': '6',\n",
       "  'commits': '34'},\n",
       " {'language': 'C++ 70.1',\n",
       "  'readme': 'ZeroNights2017 (c) 2017 James Forshaw\\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\\nWindows access tokens.\\nConsists of two parts.\\n\\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\\n\\n',\n",
       "  'watchers': '17',\n",
       "  'stars': '62',\n",
       "  'forks': '22',\n",
       "  'commits': '2,109'},\n",
       " {'language': 'C++ 66.5',\n",
       "  'readme': \"ssh\\nCreate ssh servers in node.js!\\nstatus\\nThis module has all kinds of problems since the node.js thread pool does not\\nplay nicely with libssh.\\nexample\\nsimple echo shell\\nvar ssh = require('ssh');\\n\\nssh.createServer(function (session) {\\n    session.on('password', function (user, pass, cb) {\\n        cb(user === 'foo' && pass === 'bar');\\n    });\\n    \\n    session.on('shell', function (sh) {\\n        sh.pipe(sh); // echo on\\n    });\\n}).listen(2222);\\n\\ninstallation\\nYou'll need a version of libssh with my patches:\\ngit clone git://github.com/substack/libssh.git master\\ncd libssh && mkdir build && cd build\\ncmake -DCMAKE_INSTALL_PREFIX=$PREFIX -DCMAKE_BUILD_TYPE=Debug ..\\nmake && make install\\n\\nThat should install libssh.pc, which is used by pkg-config during the\\nwscript installation. Make sure libssh.pc got installed into your\\n$PKG_CONFIG_PATH someplace.\\nYou can then install with npm:\\nnpm install ssh\\n\\n\",\n",
       "  'watchers': '8',\n",
       "  'stars': '61',\n",
       "  'forks': '13',\n",
       "  'commits': '44'},\n",
       " {'language': 'C++ 56.2',\n",
       "  'readme': \"\\nShark\\nA framework for controlling, recording, training, and running a self driving robotic car. Focused now on an implementation that runs on 1/10th scale rc cars with pwm servo steering and ESC throttle controller driven by an 9865 Servo board with a RaspberryPi 3 or Jetson TX2.\\n\\n\\nGoals\\n\\nUse ZeroMQ and C/Python to create a mesh of components that are flexible and fast\\nRun all code on the robot ( pi 3 / jetson tx2 )\\nManage things with a mobile device via web page and joystick controller\\nTrain in the cloud\\n\\nBuild your bot\\n\\ncheck docs/bot_setup.md\\n\\nOnce you are ready, you can install them as services.\\nCheck shark.service and sharkweb.service to run on startup.\\n\\nWorkflow\\nCheck camera output:\\n\\nNavigate to web page\\nSelect 'robot'. You should see a live image from camera\\n\\nLogging:\\n\\npress X on PS3 Sixaxis controls to enable recording\\nleft analog to steer, right analog forward to throttle\\nonly recording when throttle is non zero\\npress X to disable recording\\n\\nEdit logs:\\n\\nNavigate to web page\\nSelect 'log'\\nSelect 'view/edit logs'\\nobserve recorded logs\\nif unwanted frames, use slider, 'set trim start', and 'set trim end' to set range\\nuse 'trim log' button to remove unwanted frames\\n\\nManual Training:\\n\\ncopy logs to your PC: scp me@pi.local:~/projects/shark/log/*.jpg ~/projects/shark/log\\ntrain model on your PC: python train.py mymodel\\ncp mymodel to pi: scp mymodel me@pi.local:~/projects/shark/model/\\non the pi: python shark.py --model mymodel\\n\\nWeb EC2 Based Training:\\n\\ncheck docs/aws_setup.md\\nNavigate to web page\\nSelect 'ec2' button\\nSelect 'start ec2'\\nWait for 1 to 2 minutes and select 'check ec2' until the machine is ready.\\nSelect 'prepare host' to copy code to remote host\\nFrom the home menu select 'log'\\nSelect 'upload logs'\\nFrom the home menu select 'train'\\nSelect 'model' and name the new model\\nOptionally select 'epochs' and set upper limit of epochs\\nSelect 'train' to start training. Feedback varies depending on browser. Better luck from a desktop browser. Tested mostly on Firefox.\\nWhen complete, select 'push model' to tell robot predict loop to load that trained result.\\nReady to test self driving\\nWhen you are done with server, select 'release ec2' to shutdown remote machine.\\n\\nSelf Driving:\\n\\nhit triangle toggle start self driving.\\nuse dpad up and down to modify throttle scale\\n\\n\",\n",
       "  'watchers': '11',\n",
       "  'stars': '61',\n",
       "  'forks': '16',\n",
       "  'commits': '39'},\n",
       " {'language': 'C++ 73.7',\n",
       "  'readme': \"SPDY daemon\\nThis is a wrapper around the original Google's SPDY Framer.\\nIt includes a standalone server (spdyd) which can act as a SPDY-HTTP proxy (or use yet another HTTP proxy)\\nas well as a Rack adapter.\\nThe server is built around Eventmachine, and should be pretty fast.\\nInstallation:\\nGem\\n\\ngem build spdy.gemspec\\nsudo gem install ./spdy-0.1.gem\\n\\nManual\\n\\ngem install em-http-request -v 0.3.0\\nOptional, for daemonization: gem install daemons\\ncd ext; ruby extconf.rb; make\\n\\nRunning standalone server:\\nRunning it standalone is as simple as:\\nbin/spdyd\\n\\nCheck bin/spdyd -h for options.\\nRack:\\nYou can also run it as a rack server:\\nrackup -s Spdy examples/local.ru\\n\\nor for Rails application:\\nrackup -s Spdy config.ru\\n\\nTODO:\\n\\nIntegrate with npn-enabled openssl which can be built using these steps:\\nhttps://gist.github.com/944386\\n\\nCopyright 2010 (c) Roman Shterenzon, released under the AGPLv3 license.\\n\",\n",
       "  'watchers': '1',\n",
       "  'stars': '61',\n",
       "  'forks': '6',\n",
       "  'commits': '9'},\n",
       " {'language': 'C++ 87.1',\n",
       "  'readme': \"SPDY daemon\\nThis is a wrapper around the original Google's SPDY Framer.\\nIt includes a standalone server (spdyd) which can act as a SPDY-HTTP proxy (or use yet another HTTP proxy)\\nas well as a Rack adapter.\\nThe server is built around Eventmachine, and should be pretty fast.\\nInstallation:\\nGem\\n\\ngem build spdy.gemspec\\nsudo gem install ./spdy-0.1.gem\\n\\nManual\\n\\ngem install em-http-request -v 0.3.0\\nOptional, for daemonization: gem install daemons\\ncd ext; ruby extconf.rb; make\\n\\nRunning standalone server:\\nRunning it standalone is as simple as:\\nbin/spdyd\\n\\nCheck bin/spdyd -h for options.\\nRack:\\nYou can also run it as a rack server:\\nrackup -s Spdy examples/local.ru\\n\\nor for Rails application:\\nrackup -s Spdy config.ru\\n\\nTODO:\\n\\nIntegrate with npn-enabled openssl which can be built using these steps:\\nhttps://gist.github.com/944386\\n\\nCopyright 2010 (c) Roman Shterenzon, released under the AGPLv3 license.\\n\",\n",
       "  'watchers': '7',\n",
       "  'stars': '60',\n",
       "  'forks': '41',\n",
       "  'commits': '32'},\n",
       " {'language': 'JavaScript 99.0',\n",
       "  'readme': \"--> Demos, Examples, Playground, Docu\\n--> d3-react-squared-c3-loader\\n--> New live example\\n--> New blog post, based on live example\\nNotes\\n--> v0.6.0 and later require d3 v4!\\n--> v0.3.0 and later require React 0.14!\\nc3\\nDocumentation is still missing, sorry!\\nv0.3.6 and newer\\nStarting in 0.3.6, c3 charts are loaded using d3-react-squared-c3-loader\\nv0.2.7 through v0.3.5:\\nPlease note that this is still 'beta'. So far, there is no docu on the docu page.\\n--> please check out the c3example.js in the source (./examples).\\nd3-react-squared\\n\\nFeedback, ideas, PRs, etc. very welcome!\\nWhy yet another d3-react component?\\nThere are already some great solutions out there, combining React and D3, e.g.:\\nA gist with some links here\\nMost of these articles/code aims to combine/add d3 into the lifecycle methods to\\ngenerate charts that way. Have a look at them, great ideas there.\\nSee docu page for some details about my approach. I don't want to bore you with details here -\\njust contact us (contacts on docu page). I am very happy to discuss ideas/concepts!\\nSome keywords:\\n\\nUse D3 charts 'directly', maybe very limited adjustments needed (just think examples!)\\nProvide viewboxes etc. to get responsive graphs\\nMake chart modular (a.k.a. reusable)\\nProvide a clean API to create and update charts (from ANY component!).\\nParametrize charts\\nBe lightweight\\nProvide a way to share events between charts (and using a wrapper: any component!)\\nProvide access to a charts library (we currently offer c3js, as of v0.2.7)\\nProvide a limited set of examples in this repo and make it easy to the users to add their own custom charts\\n\\nWe believe that especially the last bullet is helpful to teams separate concerns and have maintainable solutions.\\nWhy? The chart generating code is in its own module and the interaction designer doesn't really have to care about React (maybe he should, but that's another story...).\\nDetails?\\nSee also here\\n(click on DR2 in top right navigation!)\\nDocumentation\\n--> See here\\n(click on DR2 in top right navigation!)\\nThe documentation is still somewhat basic. Definitely check out the examples in the repo!\\nBut hey, writing docu is sooooo time consuming...\\nStand-alone example\\nThis repo now includes a stand-alone example. Simply:\\nnpm install\\n\\nand then\\nnpm run dev\\n\\nand it should be running on localhost:8080.\\nRequirements\\nAs far as I know, you shouldn't need anything fancy.\\nWe run it in a babel/webpack/react setup, plain vanilla, so to speak (plenty of setup guides out there),\\nand it works.\\nAlso: we have bootstrap, no other css/sass/... (actually: we love react-bootstrap)\\n(Note: you could, if you wanted, to use SASS to style your graphs, must require the files where and when needed; you know how.).\\nThanks\\nHuge thanks to all the people involved in providing awesome tools such as:\\n\\nReactJS\\nD3\\nwebpack\\nBabelJS\\nReflux (no longer using it, thanks anyway!)\\nredux (replaces Reflux)\\nc3.js\\n\\nand many others...\\nSome screenshots\\nNew Docu-Page\\n\\nPlayground (--> See here) to learn about parameters:\\n\\n\",\n",
       "  'watchers': '18',\n",
       "  'stars': '183',\n",
       "  'forks': '13',\n",
       "  'commits': '219'},\n",
       " {'language': 'JavaScript 82.9',\n",
       "  'readme': \"Terminus\\nTerminus is an experimental\\nCapybara driver for real browsers. It\\nlets you control your application in any browser on any device (including\\nPhantomJS), without needing browser plugins. This\\nallows several types of testing to be automated:\\n\\nCross-browser testing\\nHeadless testing\\nMulti-browser interaction e.g. messaging apps\\nTesting on remote machines, phones, iPads etc\\n\\nSince it is experimental, this project is sporadically maintained. Usage is\\nentirely at your own risk.\\nInstallation\\n$ gem install terminus\\n\\nRunning the example\\nInstall the dependencies and boot the Terminus server, then open\\nhttp://localhost:70004/ in your browser.\\n$ bundle install\\n$ bundle exec bin/terminus\\n\\nWith your browser open, start an IRB session and begin controlling the app:\\n$ irb -r ./example/app\\n>> extend Capybara::DSL\\n>> visit '/'\\n>> click_link 'Sign up!'\\n>> fill_in 'Username', :with => 'jcoglan'\\n>> fill_in 'Password', :with => 'hello'\\n>> choose 'Web scale'\\n>> click_button 'Go!'\\n\\nLicense\\n(The MIT License)\\nCopyright (c) 2010-2013 James Coglan\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the 'Software'), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\\nof the Software, and to permit persons to whom the Software is furnished to do\\nso, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\",\n",
       "  'watchers': '1',\n",
       "  'stars': '183',\n",
       "  'forks': '5',\n",
       "  'commits': '361'},\n",
       " {'language': 'JavaScript 88.1',\n",
       "  'readme': \"generator-material-app\\nYet another yeoman generator for scaffolding a simple MEAN stack application using some material design elements.\\n\\nThis project is still under heavy development.\\nFeatures\\nThe generator supports group based ACL's and socket.io communication with the server API.\\nInstall\\n$ npm install generator-material-app\\nQuick Start\\nTo quickly scaffold an application use the following commands (Answer all questions with hitting the enter key):\\n$ mkdir app && cd $_\\n$ yo material-app\\n$ gulp build\\n$ npm start\\nWill generate something like this:\\n\\nCreate user dialog\\nNavigate to http://localhost:9001 to see the generated application where you can administer the users of your application\\nAdd API\\nThis will add a server API and a corresponding client route to manage your cats including test stubs and documentation:\\n$ yo material-app:api cat\\n$ yo material-app:apiroute cat\\n$ gulp build\\n$ npm start\\nGenerate Documentation\\nThe documentation is by now generated for server side code only:\\ngulp jsdoc\\nRun Tests\\nFor running the generated test start the following gulp tasks\\ngulp unit:server\\ngulp unit:client\\nDeploy\\nFor now, run commands:\\nNODE_ENV=production gulp build\\nNODE_ENV=production npm start\\n\\nAnd manually seed database with NODE_ENV=production npm run seed if you choosed to auth your app.\\nYou can't sign into app without any users. You can specify users and seed data in server/config/seed.js.\\nEnvironment Variables\\n\\nDATABASE_NAME\\nMONGO_URI || 'mongodb://localhost/' + process.env.DATABASE_NAME\\nMONGO_OPTIONS\\n\\nModelDefinition\\nModefy factory YourResourceDefinition in your-resource.service.js.\\nFor example:\\nModelDefinitions({\\n  name: {type: 'text', required: true},\\n  info: 'text',\\n  nested: {\\n    name: {\\n      type: 'text',\\n      desc: 'Nested Name'\\n    }\\n  }\\n})\\n\\nFor detail options.\\nList of Generators\\n\\n\\nApplication scaffold\\n\\nmaterial-app (alias for material-app:app) - The directory name will be used as the application name\\n\\n\\n\\nServer API\\n\\nmaterial-app:api - Pass the name of the API items as an argument\\n\\n\\n\\nClient Generators\\n\\nmaterial-app:apiroute - Pass the name of the route as an argument\\nmaterial-app:decorator - Pass the name of the decorator as an argument\\nmaterial-app:directive - Pass the name of the directive as an argument\\nmaterial-app:route - Pass the name of the route as an argument\\nmaterial-app:controller - Pass the name of the controller as an argument\\nmaterial-app:filter - Pass the name of the filter as an argument\\nmaterial-app:service - Pass the name of the service as an argument\\nmaterial-app:provider - Pass the name of the provider as an argument\\nmaterial-app:factory - Pass the name of the factory as an argument\\n\\n\\n\\nAPIs\\nPropDefintion\\n\\ntypes supported now:\\n\\ntypes for input(corresponding Mongoose type in ()), like 'text'(String), 'url'(String), 'number'(Number), 'date'(Date), 'password'(String),\\n'select' - type values in options array correspond to mongoose type\\n'select/resource' - mongoose ObjectId, use resource to simulate mongoose ref\\n\\n\\ncommon options\\n\\ntype - different types of property\\nNotice: name: {type: 'text'} can be short in name: 'text' but type: {type: 'text'} can't\\ndesc - name of prop displayed in form, detail and list\\n\\ndefault is capitalized last name of nested name\\n\\n\\ndisplayKey - key to display in md-select\\n\\ndefault is 'name' when type === 'select/resource'\\n\\n\\ndisplayPriority - when set to 'low', field in list will auto-hide when\\n\\nwhen screen width is less than 1200 because of css class .hide-in-narrow\\nnarrowMode in ModelViewGroup is true\\n\\n\\n\\n\\nvalidation options\\n\\nrequired - ng-required\\nformat - regex for ng-validate\\nremoteUnique - resource name to check unique from server\\nrepeatInput - force repeat field, usually for type=password\\nvalidators - for ng-messages, above validations can be written in validators uniformly\\n\\nrequired\\npattern\\nremote-unique\\nrepeat-input\\n\\n\\n\\n\\nspecial config options for 'select'\\n\\n\\noptions - static options for select\\n\\n\\nvalueKey - key to value in md-select\\n\\ndefault is '_id' when type === 'select/resource'\\n\\n\\n\\nresource - static resource for 'select/resource'\\n\\nonly work when type === ''\\n\\n\\n\\nparams - static params for 'select/resource'\\n\\n\\ngetOptions - async function returns a promise to load options upon md-select is open, resource and params can be dynamic with this\\n\\n\\n\\n\\nBelow is an example with all options, which can be generated into app with the demo option\\nvar typeMap = {\\n  User: User,\\n  ClientModelDoc: ClientModelDoc\\n};\\n\\nreturn ModelDefinitions({\\n  name: {\\n    type: 'text',\\n    format: {\\n      value: /^[a-zA-Z]{6,18}$/,\\n      error: 'Should be 6-18 letters.'\\n    },\\n    required: true,\\n    remoteUnique: 'ClientModelDoc'\\n  },\\n  repeatName: {\\n    type: 'text',\\n    repeatInput: 'name',\\n    desc: 'Repeat Name',\\n    displayPriority: 'low'\\n  },\\n  wholeName: {\\n    type: 'text',\\n    validators: {\\n      required: true,\\n      'remote-unique': {\\n        value: 'ClientModelDoc',\\n        error: 'Override default error'\\n      },\\n      pattern: /^[a-zA-Z0-9]{8,12}$/\\n    }\\n  },\\n  user: {\\n    type: 'select/resource',\\n    required: true,\\n    resource: User\\n  },\\n  rootUser: {\\n    type: 'select/resource',\\n    resource: User,\\n    params: {\\n      role: 'root'\\n    },\\n    displayKey: '_id'\\n  },\\n  anyType: {\\n    type: 'select',\\n    options: ['User', 'ClientModelDoc']\\n  },\\n  anyTypeRef: {\\n    type: 'select',\\n    getOptions: function(model) {\\n      var resource = typeMap[model.anyType];\\n      if (!resource || !resource.query) return $q.when([]);\\n      return resource.query().$promise;\\n    },\\n    displayKey: 'name',\\n    valueKey: '_id'\\n  },\\n  important: 'text',\\n  notImportant: {\\n    type: 'text',\\n    desc: 'Not Important',\\n    displayPriority: 'low'\\n  },\\n  nested: {\\n    name: 'text',\\n    repeatName: {\\n      type: 'text',\\n      repeatInput: 'nested.name',\\n      desc: 'Nested Repeat Name',\\n      displayPriority: 'low'\\n    },\\n    wholeName: {\\n      type: 'text',\\n      desc: 'Whole Name',\\n      remoteUnique: 'ClientModelDoc',\\n      auto: function (nestedModel) {\\n        return nestedModel.nested.firstName + ' ' + nestedModel.nested.secondName;\\n      }\\n    },\\n    firstName: {\\n      type: 'text',\\n      desc: 'First Name',\\n      displayPriority: 'low'\\n    },\\n    secondName: {\\n      type: 'text',\\n      desc: 'Second Name',\\n      displayPriority: 'low'\\n    },\\n  },\\n  info: 'text',\\n  //active: 'boolean'\\n})\\n\\nPurpose\\nThis generator is suited for prototyping simple CRUD applications. The generated code is somehow following  John Papa's Styleguide for Angular applications. Every generator generates a test stub\\nfor easily adding tests to your application. Note that there is not much material design in the layout yet, despite the use of\\nthe Angular Material Design components.\\nUsed Technologies\\n\\nNode.js\\nMongoDB\\nExpress\\nmongoose\\nsocket.io\\nAngular.js\\nui.router\\nSASS\\nGulp\\n\\nScreenshots\\n\\nSet password dialog\\n\",\n",
       "  'watchers': '22',\n",
       "  'stars': '183',\n",
       "  'forks': '52',\n",
       "  'commits': '169'},\n",
       " {'language': 'JavaScript 72.5',\n",
       "  'readme': 'Really Simple Color Picker\\nThis is a very minimal, yet robust Color Picker based on jQuery.\\nFor more details check the introductory blog post - http://laktek.com/2008/10/27/really-simple-color-picker-in-jquery/\\nUsage\\nYou can either clone this repo or download the latest build as a zip from here - http://github.com/laktek/really-simple-color-picker/zipball/master\\nColor Picker requires jQuery 1.2.6 or higher. Make sure to load it before Color Picker (there\\'s no other dependencies!).\\nFor default styles of the color picker load the CSS file that comes with the plugin.\\n  <script language=\"javascript\" type=\"text/javascript\" src=jquery.min.js\"></script>\\n  <script language=\"javascript\" type=\"text/javascript\" src=\"jquery.colorPicker.min.js\"/></script>\\n\\n  <link rel=\"stylesheet\" href=\"colorPicker.css\" type=\"text/css\" />\\nAdd a text field to take the color input.\\n  <div><label for=\"color1\">Color 1</label> <input id=\"color1\" type=\"text\" name=\"color1\" value=\"#333399\" /></div>\\nThen call \\'colorPicker\\' method on the text field when document loads.\\n  <script language=\"javascript\">\\n    jQuery(document).ready(function($) {\\n      $(\\'#color1\\').colorPicker();\\n    }\\n  </script>\\nOptions\\nThere are several options you can set at the time of binding.\\nSelected color\\nColor Picker will use the value of the input field, which the picker is attached to as the selected color. If not, it will use the color passed with pickerDefault property.\\n  $(\\'#color1\\').colorPicker({pickerDefault: \"ffffff\"});\\nColor Palette\\nOverrides the default color palette by passing an array of color values.\\n  $(\\'#color1\\').colorPicker({colors: [\"333333\", \"111111\"]});\\nTransparency\\nEnable transparency value as an option.\\n  $(\\'#color1\\').colorPicker({transparency: true});\\nColor Change Callback\\nRegisters a callback that can be used to notify the calling code of a color change.\\n  $(\\'#color1\\').colorPicker( { onColorChange : function(id, newValue) { console.log(\"ID: \" + id + \" has been changed to \" + newValue); } } );\\nIf you want to set an option gloablly (to apply for all color pickers), use:\\n  $.fn.colorPicker.defaults.colors = [\\'151337\\', \\'111111\\']\\nDefault text on picker field\\nYou can set some text to show on the picker field. For example, you could show a user\\'s initials.\\n  <input id=\"color4\" type=\"text\" name=\"color4\" value=\"#FF0000\" data-text=\"AG\" />\\n  $(\\'#color4\\').colorPicker();\\nDemo\\nDemo can be found at http://laktek.github.com/really-simple-color-picker/demo.html\\nReal-world Examples\\n\\nCurdBee\\nReadability\\n\\nLet us know how you are using Really Simple Color Picker...\\nContributors\\n\\nLakshan Perera - http://laktek.com\\nDaniel Lacy  - http://daniellacy.com\\n\\nIssues & Suggestions\\nPlease report any bugs or feature requests here:\\nhttps://github.com/laktek/really-simple-color-picker/issues\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '180',\n",
       "  'forks': '66',\n",
       "  'commits': '72'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Installation and usage\\nVia bower:\\nbower install jquery.serialScroll\\nVia npm:\\nnpm install jquery.serialscroll\\nUsing a public CDN\\nCDN provided by jsdelivr\\n<script src=\"//cdn.jsdelivr.net/npm/jquery.serialscroll@1.3.0/jquery.serialScroll.min.js\"></script>\\nDownloading Manually\\nIf you want the latest stable version, get the latest release from the releases page.\\njQuery.scrollTo\\nThis plugin requires jQuery.scrollTo.\\nIn order to use jQuery.scrollTo 2.0 you need to update jQuery.localScroll to 1.3.0 and above.\\nNotes\\n\\n\\nThe hash of settings is passed in to jQuery.scrollTo, so, in addition to jQuery.localScroll\\'s settings, you can use any of jQuery.scrollTo\\'s. Check that plugin\\'s documentation for further information.\\n\\n\\nMost of this plugin\\'s defaults, belong to jQuery.scrollTo, check it\\'s demo for an example of each option.\\n\\n\\n',\n",
       "  'watchers': '21',\n",
       "  'stars': '180',\n",
       "  'forks': '53',\n",
       "  'commits': '28'},\n",
       " {'language': 'JavaScript 76.1',\n",
       "  'readme': \"Phantom-Jasmine\\nPhantom-Jasmine is a simple set of two scripts for running your Jasmine Tests via Phantom.js (http://www.phantomjs.org/).\\nThe first script lib/console-runner.js is a plugin to Jasmine that outputs test results (with ansi color codes) via console.log (included with a script tag inside TestRunner.html).\\nThe second script lib/run_jasmine_test.coffee takes an html file as it's first and only argument and then executes any Jasmine tests\\nthat file loads. See below for more detail.\\nInstallation\\nAssuming you have PhantomJs setup and installed...\\nsudo npm install phantom-jasmine -g\\n\\nRunning Tests\\nphantom-jasmine examples/TestRunner.html\\n\\nOn some running OS X you might have to pass in the full url, ex:\\nphantom-jasmine file://localhost/Users/bob/phantom-jasmine/examples/TestRunner.html \\n\\nIf everything works you should see output like this in your terminal:\\nUtil : should fail for the example\\nError: Expected false to be truthy.\\n\\nFinished\\n-----------------\\n3 specs, 1 failure in 0.024s.\\n\\nTo run your own tests with Phantom-Jasmine just look at TestRunner.html and modify/copy the script tags accordingly.\\n\",\n",
       "  'watchers': '11',\n",
       "  'stars': '178',\n",
       "  'forks': '60',\n",
       "  'commits': '15'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'An Unofficial Google+ JavaScript API\\nIt has been 6 months since we have seen any Circle/Posts/Followers\\nWrite/Read API for Google+. Since Google+ is by nature asynchronous\\nwe could tap into their XHR calls and imitate the requests.\\nWho uses this API:\\n My Hangouts Chrome Extension https://plus.google.com/116935358560979346551/about\\n Circle Management Chrome Extension https://plus.google.com/100283465826629314254/about\\n Map My Circles for Google+™ https://chrome.google.com/webstore/detail/mcfifkeppchbjlepfbepfhjpkhfalcoa\\n Nuke Comments on Google+ https://chrome.google.com/webstore/detail/nfgaadooldinkdjpjbnbgnoaepmajdfh\\n Do Share on Google+ https://chrome.google.com/webstore/detail/oglhhmnmdocfhmhlekfdecokagmbchnf\\nWho made this:\\nCore Contributors\\n\\nMohamed Mansour (Maintainer, Core Dev), - https://github.com/mohamedmansour\\nTzafrir Rehan (Core Dev) - https://github.com/tzafrir\\n\\nContributors\\n\\nJingqin Lynn (Contributed newPost)\\nRyan Peggs (Contributed Bug fixes)\\n\\nWhat is it about:\\nI provide you a very basic asynchronous Google+ API, in the current\\nrelease, you can do the following:\\n\\nCreate, Modify, Sort, Query, Remove Circles\\nQuery, Modify your Profile Information\\nAdd, Remove, Move People from and to Circles\\nReal-Time Search\\nLookup Posts, Manage comments by reporting and deleting.\\n\\nThis is a fully read and write API.\\nNative Examples:\\n// Create an instance of the API.\\nvar plus = new GooglePlusAPI();\\n\\n// Initialize the API so we could get a new session if it exists we reuse it.\\nplus.init();\\n\\n// Refresh your circle information.\\nplus.refreshCircles(function() {\\n\\n   // Let us see who added me to their circle.\\n   plus.getPeopleWhoAddedMe(function(people) {\\n     console.log(people);\\n   });\\n\\n   // Let us see who is in my circles.\\n   plus.getPeopleInMyCircles(function(people) {\\n     console.log(people);\\n   });\\n\\n   // Let us see who is in our circles but didn\\'t add us to theirs.\\n   plus.getDatabase().getPersonEntity().find({in_my_circle: \\'Y\\', added_me: \\'N\\'}, function(people) {\\n     console.log(people);\\n   });\\n});\\n\\nAs you see, it is pretty easy to query everything. The possibilities are inifinite\\nsince the data is backed up in a WebSQL DataStore, so querying, reporting, etc, would\\nbe super quick.\\nIf you want to place that in an extension, I have created a bridge, so you can use\\nthis in a content script context and extension context safely. To do so, you send a\\nmessage as follows:\\n// Initialize the API so we get the authorization token.\\nchrome.extension.sendRequest({method: \\'PlusAPI\\', data: {service: \\'Plus\\', method: \\'init\\'}}, function(initResponse) {\\n  chrome.extension.sendRequest({method: \\'PlusAPI\\', data: {service: \\'Plus\\', method: \\'refreshCircles\\'}}, function() {\\n    // etc ... The method is the same method we defined previously in the raw example.\\n  });\\n});\\n\\nAnother example, lets say we want to search for a hash tag:\\n// Initialize the Google Plus API Wrapper.\\nvar api = new GooglePlusAPI();\\n\\n// Lets initialize it so we can get the current logged in users session.\\napi.init();\\n\\n// Search for the API. You have the following enums to choose from for searching:\\n//\\n//   GooglePlusAPI.SearchType.EVERYTHING\\n//   GooglePlusAPI.SearchType.PEOPLE_PAGES;\\n//   GooglePlusAPI.SearchType.POSTS\\n//   GooglePlusAPI.SearchType.SPARKS\\n//   GooglePlusAPI.SearchType.HANGOUTS\\n//   GooglePlusAPI.SearchType.HASHTAGS\\n//\\n// So lets search for hashtags that have Microsoft inside them.\\napi.search(function(resp) {\\n  console.log(\"Hash results for Microsoft: \" + resp.data.join(\", \"));\\n}, \"microsoft\", { type: GooglePlusAPI.SearchType.HASHTAGS });\\n\\nA full blown example will be released by the end of this week showing how powerful this could be.\\nAs you already know, I am creating a simple circle management utility so you can manage your circles.\\nAPI Documentation\\nAbstractEntity Members:\\n\\nString getName() - The table name that this entity holds.\\nvoid tableDefinition() - Abstract method that you override to describe the table.\\nvoid initialize() - Private method that creates the DDL to execute from tableDefinition\\nvoid drop(Function:doneCallback) - Drops the table from cache including the definition.\\nvoid clear(Function:doneCallback) - Removes all rows from the table, keeps the definition.\\nvoid create(Object[]:obj, Function<Object{status, data}>:callback) - Inserts object(s) into the table.\\nvoid destroy(String[]:id, Function<Object{status, data}>:callback) - Deletes object(s) into the table.\\nvoid update(Object[]:obj, Function<Object{status, data}>:callback) - Updates object(s) into the table.\\nvoid find(Object:obj, Function<Object{status, data}>:callback) - Find a specific object(s) in the table.\\nvoid findAll(Function<Object{status, data}>:callback) - Queries for everthing, all the data.\\nvoid count(Object:obj, Function<Object{status, data}>:callback) - Counts the number of rows in the table.\\nvoid save(Object[]:obj, Function<Object{status, data}>:callback) - Updates otherwise it creates.\\n\\nPlusDB Entities:\\n\\nvoid open() - Opens the database\\nvoid clearAll(Function:doneCallback) - Drops all tables from the database.\\nAbstractEntity getPersonEntity() - Returns the PersonEntity\\nAbstractEntity getCircleEntity() - Returns the CircleEntity\\nAbstractEntity getPersonCircleEntity() - Returns the PersonCircleEntity\\n\\nNative querying:\\n\\nPlusDB getDatabase() - Returns the native Database to do advanced queries\\n\\nInitialization, fill up the database:\\n\\nvoid init(Function:doneCallback) - Initializes session and data, you can call it at app start.\\nvoid refreshCircles(Function:doneCallback, boolean:opt_onlyCircles) - Queries G+ Service for all circles and people information.\\nvoid refreshFollowers(Function:doneCallback) - Queries G+ Service for everyone who is following me.\\nvoid refreshFindPeople(Function:doneCallback) - Queries G+ Services for discovering similar people like me.\\nvoid refreshInfo(Function:doneCallback(data)) - Refresh my information. Rarely used.\\n\\nPersistence:\\n\\nvoid addPeople(Function:doneCallback, String:circleName, Array<String>:usersToAdd) - Adding people to a circle.\\nvoid removePeople(Function:doneCallback, String:circleName, Array<String>:usersToRemove) - Removing people from a circle\\nvoid createCircle(Function:doneCallback, String:circleName, String:optionalDescription) - Creating a circle.\\nvoid removeCircle(Function:doneCallback, String:circleID) - Removing a circle.\\nvoid sortCircle(Function:doneCallback, String:circleID, Number:index) - Sort a circle to the given index, G+ will deal with the order\\nvoid modifyCircle(Function:doneCallback, String:circleID, String:optionalName, String:optionalDescription) - Modifying circle meta.\\nvoid modifyBlocked(Function:doneCallback, Array<String>:usersToModify, boolean:opt_block) - Modify the blocked state of people. Allows blocking and unblocking.\\nvoid modifyMute(Function:doneCallback, String:activityID, Boolean:muteStatus) - Sets the mute status for a specific item.\\nvoid modifyLockPost(Function:doneCallback, String:activityID, Boolean:toLock) - Sets the mute status for a specific item.\\nvoid modifyDisableComments(Function:doneCallback, String:activityID, Boolean:toDisable) - Sets the mute status for a specific item.\\nvoid addComment(Function:doneCallback, String: postId, String: content) - Adds a comment.\\nvoid deleteComment(Function:doneCallback, String:commentId) - Deleting a comment.\\nvoid deleteActivity(Function:doneCallback, String:activityId) - Deleting a post.\\nvoid saveProfile(Function:doneCallback, String:introduction) - Save a new introduction.\\nvoid reportProfile(Function:doneCallback, String:userId, opt_abuseReason) - Report an abusive profile.\\nvoid newPost(Function:doneCallback, String:content) - Creates a new post on the stream.\\nvoid fetchLinkMedia(Function:doneCallback(data), String:url - Fetches media items describing a URL such as images, title and description.\\n\\nRead:\\n\\nboolean isAuthenticated()\\nvoid getProfile(Function({introduction}):callback, String:googleProfileID)\\nvoid getInfo(Function({id, name, email, acl}:callback)\\nvoid getCircles(Function(CircleEntity[]):callback)\\nvoid getCircle(Function(String:circleID, CircleEntity):callback)\\nvoid getPeople(Object:obj, Function(PersonEntity[]):callback)\\nvoid getPerson(Function(String:googleProfileID, PersonEntity):callback)\\nvoid getPeopleInMyCircles(Function(PersonEntity[]):callback)\\nvoid getPersonInMyCircles(String:googleProfileID, Function(PersonEntity):callback)\\nvoid getPeopleWhoAddedMe(Function(PersonEntity[]):callback)\\nvoid getPersonWhoAddedMe(String:googleProfileID, Function(PersonEntity):callback)\\nvoid search(Function(data):callback, String:query, Object:{category, precache, burst, burst_size})\\nvoid lookupUsers(Function(data):callback, Array<String:googleProfileID>)\\nvoid lookupPost(Function(data):callback, String:googleProfileID, String:postProfileID)\\nvoid lookupActivities(Function(data):callback, String:circleID, String:personID, String:pageToken)\\nvoid getPages(Function(data):callback)\\nvoid getCommunities(Function(data):callback)\\nvoid getCommunity(Function(data):callback, String:communityId)\\n\\nPrivate Members (only for internal API):\\n\\nObject _parseJSON(String:input) - Parses the Google Irregular JSON\\nXMLHttpRequest _requestService(Function:callback, String:url, String:postData - Sends an XHR request to Google Service\\nString _getSession()- Unique user session that authenticates to persist to your account.\\n\\nWatch this space!\\n',\n",
       "  'watchers': '24',\n",
       "  'stars': '178',\n",
       "  'forks': '39',\n",
       "  'commits': '159'},\n",
       " {'language': 'JavaScript 96.1',\n",
       "  'readme': 'grunt-contrib-stylus v1.2.0  \\n\\nCompile Stylus files to CSS\\n\\nGetting Started\\nIf you haven\\'t used Grunt before, be sure to check out the Getting Started guide, as it explains how to create a Gruntfile as well as install and use Grunt plugins. Once you\\'re familiar with that process, you may install this plugin with this command:\\nnpm install grunt-contrib-stylus --save-dev\\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\\ngrunt.loadNpmTasks(\\'grunt-contrib-stylus\\');\\nThis plugin was designed to work with Grunt 0.4.x. If you\\'re still using grunt v0.3.x it\\'s strongly recommended that you upgrade, but in case you can\\'t please use v0.3.1.\\nStylus task\\nRun this task with the grunt stylus command.\\nTask targets, files and options may be specified according to the grunt Configuring tasks guide.\\nThis task comes preloaded with nib.\\nOptions\\ncompress\\nType: Boolean\\nDefault: true\\nSpecifies if we should compress the compiled css. Compression is always disabled when --debug flag is passed to grunt.\\nlinenos\\nType: Boolean\\nDefault: false\\nSpecifies if the generated CSS file should contain comments indicating the corresponding stylus line.\\nfirebug\\nType: Boolean\\nDefault: false\\nSpecifies if the generated CSS file should contain debug info that can be used by the FireStylus Firebug plugin.\\npaths\\nType: Array\\nSpecifies directories to scan for @import directives when parsing.\\ndefine\\nType: Object\\nAllows you to define global variables in Gruntfile that will be accessible in Stylus files.\\nrawDefine\\nType: Boolean|Array|String\\nIf set to true, defines global variables in Gruntfile without casting objects to Stylus lists. Allows using a JavaScript object in Gruntfile to be accessible as a Stylus Hash. See Stylus\\'s issue tracker for details. stylus/stylus#1286\\nAllows passing an array or string to specify individual keys to define \"raw\", casting all other keys as default Stylus behavior.\\nurlfunc\\nType: String|Object\\nIf String: specifies function name that should be used for embedding images as Data URI.\\nIf Object:\\n\\nname - Type: String. Function name that should be used for embedding images as Data URI.\\n[ limit ] - Type: Number|Boolean Default: 30000. Bytesize limit defaulting to 30Kb (30000), use false to disable the limit.\\n[ [paths ] - Type: Array, Default: []. Image resolution path(s).\\n\\nSee url() for details.\\nuse\\nType: Array\\nAllows passing of stylus plugins to be used during compile.\\nimport\\nType: Array\\nImport given stylus packages into every compiled .styl file, as if you wrote @import \\'...\\'\\nin every single one of said files.\\ninclude css\\nType: Boolean\\nDefault: false\\nWhen including a css file in your app.styl by using @import \"style.css\", by default it will not include the full script, use true to compile into one script.\\n( NOTICE: the object key contains a space \"include css\" )\\nresolve url\\nType: Boolean\\nDefault: false\\nTelling Stylus to generate url(\"bar/baz.png\") in the compiled CSS files accordingly from @import \"bar/bar.styl\" and url(\"baz.png\"), which makes relative pathes work in Stylus.\\nAll urls are resolved relatively to position of resulting .css file\\n( NOTICE: the object key contains a space \"resolve url\" and Stylus resolves the url only if it finds the provided file )\\nresolve url nocheck\\nType: Boolean\\nDefault: false\\nLike resolve url, but without file existence check. Fixing some current issues.\\n( NOTICE: the object key contains two spaces \"resolve url nocheck\" )\\nbanner\\nType: String\\nDefault: \\'\\'\\nThis string will be prepended to the beginning of the compiled output.\\nrelativeDest\\nType: String\\nDefault: \\'\\'\\nPath to be joined and resolved with each file dest to get new one. Mostly useful for files specified using wildcards:\\noptions: {\\n  relativeDest: \\'out\\'\\n},\\nfiles: [{\\n  src: [\\'src/components/*/*.styl\\'],\\n  ext: \\'.css\\'\\n}]\\nwill generate src/components/*/out/*.css files.\\nExamples\\nstylus: {\\n  compile: {\\n    options: {\\n      paths: [\\'path/to/import\\', \\'another/to/import\\'],\\n      relativeDest: \\'../out\\', //path to be joined and resolved with each file dest to get new one.\\n                              //mostly useful for files specified using wildcards\\n      urlfunc: \\'data-uri\\', // use data-uri(\\'test.png\\') in our code to trigger Data URI embedding\\n      use: [\\n        function () {\\n          return testPlugin(\\'yep\\'); // plugin with options\\n        },\\n        require(\\'fluidity\\') // use stylus plugin at compile time\\n      ],\\n      import: [      //  @import \\'foo\\', \\'bar/moo\\', etc. into every .styl file\\n        \\'foo\\',       //  that is compiled. These might be findable based on values you gave\\n        \\'bar/moo\\'    //  to `paths`, or a plugin you added under `use`\\n      ]\\n    },\\n    files: {\\n      \\'path/to/result.css\\': \\'path/to/source.styl\\', // 1:1 compile\\n      \\'path/to/another.css\\': [\\'path/to/sources/*.styl\\', \\'path/to/more/*.styl\\'] // compile and concat into single file\\n    }\\n  }\\n}\\nRelease History\\n\\n2016-03-08\\u2003\\u2003\\u2003v1.2.0\\u2003\\u2003\\u2003Stylus ^0.54.\\n2016-03-04\\u2003\\u2003\\u2003v1.1.0\\u2003\\u2003\\u2003Add \"resolve url nocheck\". Fixes #146. Remove peerDeps. Point main script to task.\\n2016-01-19\\u2003\\u2003\\u2003v1.0.0\\u2003\\u2003\\u2003Bump stylus to 0.53.\\n2015-07-21\\u2003\\u2003\\u2003v0.22.0\\u2003\\u2003\\u2003Add relativeDest option. Bump stylus to 0.52.\\n2015-03-09\\u2003\\u2003\\u2003v0.21.0\\u2003\\u2003\\u2003Bump stylus to 0.50.\\n2014-09-22\\u2003\\u2003\\u2003v0.20.0\\u2003\\u2003\\u2003Bump stylus to 0.49.\\n2014-08-26\\u2003\\u2003\\u2003v0.19.0\\u2003\\u2003\\u2003Bump dependencies. Bump stylus to 0.48.\\n2014-07-02\\u2003\\u2003\\u2003v0.18.0\\u2003\\u2003\\u2003Bump stylus to 0.47.\\n2014-06-04\\u2003\\u2003\\u2003v0.17.0\\u2003\\u2003\\u2003Bump stylus to 0.46.\\n2014-05-12\\u2003\\u2003\\u2003v0.16.0\\u2003\\u2003\\u2003Bump stylus to 0.45.\\n2014-05-01\\u2003\\u2003\\u2003v0.15.1\\u2003\\u2003\\u2003Add support for rawDefine.\\n2014-04-23\\u2003\\u2003\\u2003v0.15.0\\u2003\\u2003\\u2003Bump stylus to 0.44.\\n2014-04-08\\u2003\\u2003\\u2003v0.14.0\\u2003\\u2003\\u2003Bump stylus to 0.43.\\n2014-03-01\\u2003\\u2003\\u2003v0.13.2\\u2003\\u2003\\u2003Fix limit option for urlfunc. Update copyright to 2014.\\n2014-02-27\\u2003\\u2003\\u2003v0.13.1\\u2003\\u2003\\u2003grunt.template.process is not needed.\\n2014-02-22\\u2003\\u2003\\u2003v0.13.0\\u2003\\u2003\\u2003Adds Data URI Image Inlining options. Fix \"resolve url\" option. Use chalk module to colorize terminal output. Emphasize spaces in object keys in the README.\\n2014-01-08\\u2003\\u2003\\u2003v0.12.0\\u2003\\u2003\\u2003Update to stylus 0.42.0.\\n2013-12-02\\u2003\\u2003\\u2003v0.11.0\\u2003\\u2003\\u2003Update to stylus 0.41.0.\\n2013-11-07\\u2003\\u2003\\u2003v0.10.0\\u2003\\u2003\\u2003Update to stylus 0.40.0 and nib 1.0.1.\\n2013-10-20\\u2003\\u2003\\u2003v0.9.0\\u2003\\u2003\\u2003Update to stylus 0.38.0.\\n2013-08-20\\u2003\\u2003\\u2003v0.8.0\\u2003\\u2003\\u2003Update to stylus 0.37.0 and nib to 1.0.0.\\n2013-07-31\\u2003\\u2003\\u2003v0.7.0\\u2003\\u2003\\u2003Update to stylus 0.35.\\n2013-07-11\\u2003\\u2003\\u2003v0.6.0\\u2003\\u2003\\u2003Update to stylus 0.33.\\n2013-03-10\\u2003\\u2003\\u2003v0.5.0\\u2003\\u2003\\u2003Upgrade to stylus 0.32.1.\\n2013-02-22\\u2003\\u2003\\u2003v0.4.1\\u2003\\u2003\\u2003Support stylus define option.\\n2013-02-15\\u2003\\u2003\\u2003v0.4.0\\u2003\\u2003\\u2003First official release for Grunt 0.4.0.\\n2013-01-23\\u2003\\u2003\\u2003v0.4.0rc7\\u2003\\u2003\\u2003Updating grunt/gruntplugin dependencies to rc7. Changing in-development grunt/gruntplugin dependency versions from tilde version ranges to specific versions.\\n2013-01-09\\u2003\\u2003\\u2003v0.4.0rc5\\u2003\\u2003\\u2003Updating to work with grunt v0.4.0rc5. Switching to this.file API.\\n2012-12-15\\u2003\\u2003\\u2003v0.4.0a\\u2003\\u2003\\u2003Conversion to grunt v0.4 conventions. Remove Node.js v0.6 and grunt v0.3 support. Merge grunt-stylus features (plugin loading, embedding). Remove experimental destination wildcards.\\n2012-10-12\\u2003\\u2003\\u2003v0.3.1\\u2003\\u2003\\u2003Rename grunt-contrib-lib dep to grunt-lib-contrib.\\n2012-09-24\\u2003\\u2003\\u2003v0.3.0\\u2003\\u2003\\u2003Options no longer accepted from global config key. Individually compile into dest, maintaining folder structure.\\n2012-09-17\\u2003\\u2003\\u2003v0.2.2\\u2003\\u2003\\u2003Tests refactored, better watch integration.\\n2012-09-10\\u2003\\u2003\\u2003v0.2.0\\u2003\\u2003\\u2003Refactored from grunt-contrib into individual repo.\\n\\n\\nTask submitted by Eric Woroshow\\nThis file was generated on Fri Mar 18 2016 21:06:50.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '177',\n",
       "  'forks': '66',\n",
       "  'commits': '221'},\n",
       " {'language': 'JavaScript 49.3',\n",
       "  'readme': 'Feeling RESTful Theme\\nVersion 2\\nA WordPress React JS theme updated with a design for A Day of Rest Boston 2017\\n\\nPurpose-built theme for the A Day of Rest Boston conference.\\nThis is an update of the FeelingRestful v1 theme.\\nDifferences between v1 and v2\\nThere are some differences between the two.\\nPlease note that version 1 has been release tagged.\\nChanges\\n\\n\\nAdded dynamic menu capabilities. Works with WordPress Menu settings. Has a built in menu walker in React.\\n\\n\\nUpdated design modifications: ADOR Boston website which was a design change.\\nThis theme has been visually redesigned for ADOR Boston. Therefore, there are CSS and HTML changes to enable that.\\n\\n\\nRemoval of Preview_Postmeta\\n\\n\\nBug fixes\\n\\n\\nRequirements\\nRequired homepage setup:\\nA page with the slug \"home-page\" is used for the homepage. Please create this page if using a fresh installation.\\nRequired Plugins\\n\\nWordPress version 4.7+ or WordPress REST API\\nModular Page Builder\\nWP-API Menus\\n\\nAdded support for\\n\\nTestimonials by WooThemes\\n\\nBuilding\\nnpm install\\ngrunt build\\n\\nDeveloping\\nnpm install\\ngrunt webpack:watch-dev # build javascript\\ngrunt watch sass #\\xa0compile sass (watch does not trigger LiveReload on sass changes)\\n\\n',\n",
       "  'watchers': '24',\n",
       "  'stars': '177',\n",
       "  'forks': '28',\n",
       "  'commits': '154'},\n",
       " {'language': 'JavaScript 99.7',\n",
       "  'readme': 'Demopack\\nFire up a pre-configured, live reloading Webpack server that supports React, (S)CSS and files for quick demos and tutorials.\\nLike create-react-app, but without needing to create a new project and npm install all the things.\\nSupports JavaScript, JSX, Sass, CSS Modules and images out of the box. 0 configuration required!\\nCatch this quick demo on Youtube to learn more :).\\nBut why?\\nI love Webpack, and I love tools like create-react-app, but they come with some set up cost.\\n\\nFor Webpack, it takes a reasonable amount of time to configure and set it up. I love this for full projects, but for quick experiments/demos, it\\'s pretty slow.\\nSimilarly create-react-app is great but the setup time in generating a new project can be relatively slow, especially on slow networks when you have to install all the dependencies!\\n\\nThe idea behind demopack is that it\\'s a pre-packaged Webpack. It includes and configures everything you need, so all you have to do is install demopack once, and then when you run it there\\'s no more setup.\\nDemopack is not designed for fully featured projects, but for small demos or one off projects where you need no extra configuration. For \"proper\" projects I recommend either Webpack from scratch or a tool like create-react-app.\\nInstallation\\nPlease note that demopack is very new and I\\'m sure it might not work right for all people on all machines. Please open an issue if you have any problems or suggestions!\\nYou should install demopack globally so you can easily run it in a directory.\\nUnfortunately demopack is taken on npm, so you need to install the scoped version:\\nnpm install --global @jackfranklin/demopack\\nyarn global add @jackfranklin/demopack\\n\\nYou\\'ll need to be running Node 8 or higher.\\nUsage\\nGiven a folder with the following file in it:\\n// index.js\\nconsole.log(\\'Hello World!\\');\\nRunning demopack in that directory will:\\n\\nFire up a Webpack server on port 8080 (or another free port).\\nGenerate an index.html file that loads your JavaScript.\\nParses any JavaScript (including JSX support) and SCSS for you.\\n\\nWhat Demopack supports\\nDemopack has the following Webpack loaders configured out of the box:\\n\\nbabel-loader which parses .js and .jsx files. It runs using babel-preset-env, babel-preset-react and babel-preset-stage-0. This should mean any fancy JS you want to write should be supported :)\\nsass-loader, css-loader and style-loader to load your CSS and SASS (but regular CSS works just fine). If you pass --css-modules CSS Modules will be supported.\\nfile-loader to load gif, png, jpg and svg files.\\n\\nDemopack will run a small local server and will refresh automatically when you change files. Any errors are shown in the terminal and in the page.\\nIf there\\'s anything you think should be supported out of the box, feel free to open an issue and we can discuss it.\\nProduction build\\nThe primary goal of demopack is to be used for development, but you can generate a production build if you like with the --build flag.\\nThis will run all the same plugins, with some extras (CSS is pulled into a separate file, JS & CSS is minified).\\nThe build will output into ./demopack-built and will contain an index.html file alongside any assets.\\nConfiguration\\nThe goal of demopack is to be preconfigured out of the box, but there are some options you can configure. Running demopack --help will show you these:\\nOptions:\\n  --help          Show help                                            [boolean]\\n\\n  --version       Show version number                                  [boolean]\\n\\n  --open-browser  Automatically open the browser when you run demopack. [default: true]\\n\\n  --css-modules   Enable CSS Modules support.                   [default: false]\\n\\n  --entry         The JavaScript file that demopack should build from. [default: \"index.js\"]\\n\\n  --build         Output static files into a directory. JS and CSS will be minified [default: false]\\n\\nChangelog\\n0.2.0\\n\\nInitial release :)\\n\\nPrior art / credit\\n\\nThanks to Eduard and his work on create-elm-app which I took a bunch of Webpack code from to make the Demopack terminal output super tidy.\\nThanks also to everyone behind create-react-app which was the inspiration behind Demopack initially.\\n\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '176',\n",
       "  'forks': '11',\n",
       "  'commits': '20'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': '🚨 NO LONGER ACTIVE 🚨\\nTrolly\\nA Python wrapper around the Trello API. Provides a group of Python classes to represent Trello Objects. None of the\\nclasses cache values as they are designed to be inherited and extended to suit the needs of each user. Each class\\nincludes a basic set of methods based on general use cases. This library was based on work done by\\nsarumont. Very little was kept from this code, but still props on the initial\\nwork.\\nChanges\\n0.2\\nTrolly has recently been updated to start the deprecation of non-pythonic conventions and improve PEP8 compliance.\\nAll the old pascal cased methods are now deprecated, they have been left in and point to the new method names. There\\nare no breaking changes in this release but it is recommended you change your code to use the pythonic naming conventions.\\nGetting Started\\nDependencies\\nThis library requires python 2.5+ or 3+.\\nBefore getting stated with this library you will need a few extra things:\\n\\nhttplib2\\nAn API key for your Trello user\\nUser authorisation token ( see below for how to obtain )\\n\\nInstallation\\nTo install for python 2 or 3 you can either download the source and run:\\nsudo python setup.py install\\n\\nAlternatively you can use:\\nsudo pip install Trolly\\n\\nAuthorisation\\nUser Authorisation Token\\nA user authorisation token isn\\'t too hard to get hold of. There are instruction on how to get one on the\\nTrello Documentation. For those too\\nlazy there is a python class in the library called Authorise(). To use this class simply navigate to authorise.py in a\\nterminal and type:\\npython authorise.py -a API_KEY APPLICATION_NAME WHEN_TO_EXPIRE\\n\\nThe API key and application names are required but the \"WHEN_TO_EXPIRE\" will default to 1day if not specified. Running\\nthis file will return a URL. Copy and paste it into your browser and away you go. You might want to store this somewhere\\nfor future use, especially if you have set it to never expire.\\nOauth\\nThis library (currently) has no Oauth support however the code this was based on includes Oauth support. So for\\ninspiration on how to extend the Client class to include this check out the link above.\\nOverview\\nThere are a number of methods for each of the Trello objects. Some accept query parameters, these are for API methods\\nthat will accept a wide range of values and return a lot of information. For example\\nGET Boards will take a lot of query parameters to\\nallow you to whittle down the information to the bar minimum. This is extremely useful for extending the classes without\\nmuch extra programming.\\nTrello Client\\nThis class holds the bulk of all the methods for communicating with the trello API and returning the Trello objects.\\nA client instance is required by every Trello object, because of this it makes extending and overiding methods in this\\nclass very effective. This is where you would override the creating of an object, e.g. a Card, with your own object.\\nExample usage of the client:\\nclient = trolly.client.Client(settings.API_KEY, settings.TOKEN)\\n\\nprint(\\'Member: %s\\' % client.get_member())\\n\\nprint(\\'Organisations:\\')\\nfor organisation in client.get_organisations():\\n    print(\\' - %s\\' % organisation)\\n\\nprint(\\'Organisations:\\')\\nfor organisation in client.get_organisations():\\n    print(\\' - %s\\' % organisation)\\n\\nprint(\\'Boards:\\')\\nfor board in client.get_boards():\\n    print(\\' - %s\\' % board)\\n\\nprint(\\'Cards:\\')\\nfor card in client.get_cards():\\n    print(\\' - %s\\' % card)\\n\\n# Get all information from a card (works for boards, lists, etc. too):\\nprint(\\'Detailed cards:\\')\\nfor card in client.get_cards(actions=\\'all\\'):\\n    print(\\' - %s: %s\\' % (card, card.data))\\n\\nTrello Object\\nThis class is inhereted my all Trello object classes ( Board, List, Card, etc ). The class takes only one argument, a\\nTrello client instance. It masks calls to the client as belonging to the class ( for the purposes of modularity ).\\nThere are also a number of methods for fetching ( Board, List, etc ) JSON from the API. Since the only thing that\\ndiffers from class to class is the base URI, this is taken as the only argument.\\nThere are a number of methods for creating Trello objects. The createOBJECTNAME methods in this class can be extended\\neasily by passing them keyword arguments.\\nExtending Trello Classes\\nExtending these classes is the premise on which they were built. Below outlines an example of how this can be acheived.\\nIf for example we wanted to pass extra variables to our a Trello Card object then we can do the below:\\nclass MyList( List ):\\n\\n    def __init__( self, trello_client, list_id, name = \\'\\' ):\\n        super( MyList, self ).__init__( trello_client, list_id, name )\\n\\n    def getCards( self ):\\n        cards = self.getCardsJson( self.base_uri )\\n        return self.createCard( card_json = cards[0], test = \\'this is a test argument\\' )\\n\\nThis MyList class overrides the getCards method to add the extra variable we need. This would need to be done to any\\nTrello object that will return a custom card.\\nWe declare and pass the extra (\\'test\\') variable as a keyword argument here. We then need to extend the card class to\\nallow for the extra variables:\\nclass MyCard( Card ):\\n\\n    def __init__( self, trello_client, card_id, test, name = \\'\\' ):\\n        super( MyCard, self ).__init__( trello_client, card_id, name )\\n        self.test_arg = test\\n\\nFinally, we extend and override the Client. Overriding the client means that any object that calls createCard will\\ncreate one of our new client classes.\\nclass MyClient( Client ):\\n\\n    def __init__( self, api_key, user_auth_token ):\\n        super( MyClient, self ).__init__( api_key, user_auth_token )\\n\\n    def createCard( self, card_json, **kwargs ):\\n\\n        return MyCard( \\n                trello_client = self,\\n                card_id = card_json[\\'id\\'],\\n                name = card_json[\\'name\\'],\\n                test = kwargs[\\'test\\']\\n            )\\n\\nThe above client will fail though if you fail to pass a \"test\" keyword argument. To get around this you could use:\\nkwargs.get(\\'test\\',\"default value\")\\n\\nThis will help avoid a value not being passed. You could also, instead of extending the object creation, add\\na method to cache the details you want using the objects getObjectInformation method.\\nHope this helps and happy Trelloing!\\nRunning Test\\nIn order to run the tests you will need:\\n\\nAPI Key\\nUser authorisation token\\nAn organisation ID\\nA board ID\\nA list ID\\nA card ID\\nA checklist ID\\nA member ID\\n\\nIt\\'s quite a lot of information to get hold of (sorry). If you don\\'t need everything you can just comment out\\nthe tests you don\\'t need.\\nTo run the tests navigate to the Trolly in a terminal and run:\\nPYTHONPATH=. python test/tests.py\\n\\nLicence\\nThis code is licenced under the MIT Licence\\nContributors\\n\\nRick van Hatten\\n\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '100',\n",
       "  'forks': '31',\n",
       "  'commits': '139'},\n",
       " {'language': 'Python 92.8',\n",
       "  'readme': '\\n\\n\\n\\nStoring settings in the database\\nNot all settings belong in settings.py, as it has some particular\\nlimitations:\\n\\n\\nSettings are project-wide. This not only requires apps to clutter up\\nsettings.py, but also increases the chances of naming conflicts.\\nSettings are constant throughout an instance of Django. They cannot be\\nchanged without restarting the application.\\nSettings require a programmer in order to be changed. This is true even\\nif the setting has no functional impact on anything else.\\n\\n\\nMany applications find need to overcome these limitations, and dbsettings\\nprovides a convenient way to do so.\\nThe main goal in using this application is to define a set of placeholders that\\nwill be used to represent the settings that are stored in the database. Then,\\nthe settings may be edited at run-time using the provided editor, and all Python\\ncode in your application that uses the setting will receive the updated value.\\n\\nRequirements\\n\\n\\nDbsettings\\nPython\\nDjango\\n\\n\\n\\n==0.10\\n3.4 - 3.5\\n1.7 - 1.10\\n\\n3.2 - 3.3\\n1.7 - 1.8\\n\\n2.7\\n1.7 - 1.10\\n\\n==0.9\\n3.4 - 3.5\\n1.7 - 1.9\\n\\n3.2 - 3.3\\n1.7 - 1.8\\n\\n2.7\\n1.7 - 1.9\\n\\n==0.8\\n3.2\\n1.5 - 1.8\\n\\n2.7\\n1.4 - 1.8\\n\\n2.6\\n1.4 - 1.6\\n\\n==0.7\\n3.2\\n1.5 - 1.7\\n\\n2.7\\n1.3 - 1.7\\n\\n2.6\\n1.3 - 1.6\\n\\n==0.6\\n3.2\\n1.5\\n\\n2.6 - 2.7\\n1.3 - 1.5\\n\\n<=0.5\\n2.6 - 2.7\\n1.2* - 1.4\\n\\n\\n\\n* Possibly version below 1.2 will work too, but not tested.\\n\\nInstallation\\nTo install the dbsettings package, simply place it anywhere on your\\nPYTHONPATH.\\n\\nProject settings\\nIn order to setup database storage, and to let Django know about your use of\\ndbsettings, simply add it to your INSTALLED_APPS setting, like so:\\nINSTALLED_APPS = (\\n    ...\\n    \\'dbsettings\\',\\n    ...\\n)\\n\\nIf your Django project utilizes sites framework, all setting would be related\\nto some site. If sites are not present, settings won\\'t be connected to any site\\n(and sites framework is no longer required since 0.8.1).\\nYou can force to do (not) use sites via DBSETTINGS_USE_SITES = True / False\\nconfiguration variable (put it in project\\'s settings.py).\\nBy default, values stored in database are limited to 255 characters per setting.\\nYou can change this limit with DBSETTINGS_VALUE_LENGTH configuration variable.\\nIf you change this value after migrations were run, you need to manually alter\\nthe dbsettings_setting table schema.\\n\\nURL Configuration\\nIn order to edit your settings at run-time, you\\'ll need to configure a URL to\\naccess the provided editors. You\\'ll just need to add a single line, defining\\nthe base URL for the editors, as dbsettings has its own URLconf to handle\\nthe rest. You may choose any location you like:\\nurlpatterns = patterns(\\'\\',\\n    ...\\n    (r\\'^settings/\\', include(\\'dbsettings.urls\\')),\\n    ...\\n)\\n\\n\\nA note about caching\\nThis framework utilizes Django\\'s built-in cache framework, which is used to\\nminimize how often the database needs to be accessed. During development,\\nDjango\\'s built-in server runs in a single process, so all cache backends will\\nwork just fine.\\nMost productions environments, including mod_python, FastCGI or WSGI, run multiple\\nprocesses, which some backends don\\'t fully support. When using the simple\\nor locmem backends, updates to your settings won\\'t be reflected immediately\\nin all workers, causing your application to ignore the new changes.\\nNo other backends exhibit this behavior, but since simple is the default,\\nmake sure to specify a proper backend when moving to a production environment.\\nAlternatively you can disable caching of settings by setting\\nDBSETTINGS_USE_CACHE = False in settings.py. Beware though: every\\naccess of any setting will result in database hit.\\n\\nUsage\\nThese database-backed settings can be applied to any model in any app, or even\\nin the app itself. All the tools necessary to do so are available within the\\ndbsettings module. A single import provides everything you\\'ll need:\\nimport dbsettings\\n\\n\\nDefining a group of settings\\nSettings are be defined in groups that allow them to be referenced together\\nunder a single attribute. Defining a group uses a declarative syntax similar\\nto that of models, by declaring a new subclass of the Group class and\\npopulating it with values.\\nclass ImageLimits(dbsettings.Group):\\n    maximum_width = dbsettings.PositiveIntegerValue()\\n    maximum_height = dbsettings.PositiveIntegerValue()\\n\\nYou may name your groups anything you like, and they may be defined in any\\nmodule. This allows them to be imported from common applications if applicable.\\n\\nDefining individual settings\\nWithin your groups, you may define any number of individual settings by simply\\nassigning the value types to appropriate names. The names you assign them to\\nwill be the attribute names you\\'ll use to reference the setting later, so be\\nsure to choose names accordingly.\\nFor the editor, the default description of each setting will be retrieved from\\nthe attribute name, similar to how the verbose_name of model fields is\\nretrieved. Also like model fields, however, an optional argument may be provided\\nto define a more fitting description. It\\'s recommended to leave the first letter\\nlower-case, as it will be capitalized as necessary, automatically.\\nclass EmailOptions(dbsettings.Group):\\n    enabled = dbsettings.BooleanValue(\\'whether to send emails or not\\')\\n    sender = dbsettings.StringValue(\\'address to send emails from\\')\\n    subject = dbsettings.StringValue(default=\\'SiteMail\\')\\n\\nFor more descriptive explanation, the help_text argument can be used. It\\nwill be shown in the editor.\\nThe default argument is very useful - it specify an initial value of the\\nsetting.\\nIn addition, settings may be supplied with a list of available options, through\\nthe use of of the choices argument. This works exactly like the choices\\nargument for model fields, and that of the newforms ChoiceField.\\nThe widget used for a value can be overriden using the widget keyword. For example:\\npayment_instructions = dbsettings.StringValue(\\n    help_text=\"Printed on every invoice.\",\\n    default=\"Payment to Example XYZ\\\\nBank name here\\\\nAccount: 0123456\\\\nSort: 01-02-03\",\\n    widget=forms.Textarea\\n)\\n\\nA full list of value types is available later in this document, but the process\\nand arguments are the same for each.\\n\\nAssigning settings\\nOnce your settings are defined and grouped properly, they must be assigned to a\\nlocation where they will be referenced later. This is as simple as instantiating\\nthe settings group in the appropriate location. This may be at the module level\\nor within any standard Django model.\\nGroup instance may receive one optional argument: verbose name of the group.\\nThis name will be displayed in the editor.\\nemail = EmailOptions()\\n\\nclass Image(models.Model):\\n    image = models.ImageField(upload_to=\\'/upload/path\\')\\n    caption = models.TextField()\\n\\n    limits = ImageLimits(\\'Dimension settings\\')\\n\\nMultiple groups may be assigned to the same module or model, and they can even\\nbe combined into a single group by using standard addition syntax:\\noptions = EmailOptions() + ImageLimits()\\n\\nTo separate and tag settings nicely in the editor, use verbose names:\\noptions = EmailOptions(\\'Email\\') + ImageLimits(\\'Dimesions\\')\\n\\n\\nDatabase setup\\nA single model is provided for database storage, and this model must be\\ninstalled in your database before you can use the included editors or the\\npermissions that will be automatically created. This is a simple matter of\\nrunning manage.py syncdb or manage.py migrate now that your settings\\nare configured.\\nThis step need only be repeate when settings are added to a new application,\\nas it will create the appropriate permissions. Once those are in place, new\\nsettings may be added to existing applications with no impact on the database.\\n\\nUsing your settings\\nOnce the above steps are completed, you\\'re ready to make use of database-backed\\nsettings.\\n\\nEditing settings\\nWhen first defined, your settings will default to None (or False in\\nthe case of BooleanValue), so their values must be set using one of the\\nsupplied editors before they can be considered useful (however, if the setting\\nhad the default argument passed in the constructor, its value is already\\nuseful - equal to the defined default).\\nThe editor will be available at the URL configured earlier.\\nFor example, if you used the prefix of \\'settings/\\', the URL /settings/\\nwill provide an editor of all available settings, while /settings/myapp/\\nwould contain a list of just the settings for myapp.\\nURL patterns are named: \\'site_settings\\' and \\'app_settings\\', respectively.\\nThe editors are restricted to staff members, and the particular settings that\\nwill be available to users is based on permissions that are set for them. This\\nmeans that superusers will automatically be able to edit all settings, while\\nother staff members will need to have permissions set explicitly.\\n\\nAccessing settings in Python\\nOnce settings have been assigned to an appropriate location, they may be\\nreferenced as standard Python attributes. The group becomes an attribute of the\\nlocation where it was assigned, and the individual values are attributes of the\\ngroup.\\nIf any settings are referenced without being set to a particular value, they\\nwill default to None (or False in the case of BooleanValue, or\\nwhatever was passed as default). In the\\nfollowing example, assume that EmailOptions were just added to the project\\nand the ImageLimits were added earlier and already set via editor.\\n>>> from myproject.myapp import models\\n\\n# EmailOptions are not defined\\n>>> models.email.enabled\\nFalse\\n>>> models.email.sender\\n>>> models.email.subject\\n\\'SiteMail\\'  # Since default was defined\\n\\n# ImageLimits are defined\\n>>> models.Image.limits.maximum_width\\n1024\\n>>> models.Image.limits.maximum_height\\n768\\n\\nThese settings are accessible from any Python code, making them especially\\nuseful in model methods and views. Each time the attribute is accessed, it will\\nretrieve the current value, so your code doesn\\'t need to worry about what\\nhappens behind the scenes.\\ndef is_valid(self):\\n    if self.width > Image.limits.maximum_width:\\n        return False\\n    if self.height > Image.limits.maximum_height:\\n        return False\\nreturn True\\n\\nAs mentioned, views can make use of these settings as well.\\nfrom myproject.myapp.models import email\\n\\ndef submit(request):\\n\\n    ...\\n    # Deal with a form submission\\n    ...\\n\\n    if email.enabled:\\n        from django.core.mail import send_mail\\n    send_mail(email.subject, \\'message\\', email.sender, [request.user.email])\\n\\nSettings can be not only read, but also written. The admin editor is more\\nuser-friendly, but in case code need to change something:\\nfrom myproject.myapp.models import Image\\n\\ndef low_disk_space():\\n    Image.limits.maximum_width = Image.limits.maximum_height = 200\\n\\nEvery write is immediately commited to the database and proper cache key is deleted.\\n\\nA note about model instances\\nSince settings aren\\'t related to individual model instances, any settings that\\nare set on models may only be accessed by the model class itself. Attempting to\\naccess settings on an instance will raise an AttributeError.\\n\\nValue types\\nThere are several various value types available for database-backed settings.\\nSelect the one most appropriate for each individual setting, but all types use\\nthe same set of arguments.\\n\\nBooleanValue\\nPresents a checkbox in the editor, and returns True or False in Python.\\n\\nDurationValue\\nPresents a set of inputs suitable for specifying a length of time. This is\\nrepresented in Python as a timedelta object.\\n\\nFloatValue\\nPresents a standard input field, which becomes a float in Python.\\n\\nIntegerValue\\nPresents a standard input field, which becomes an int in Python.\\n\\nPercentValue\\nSimilar to IntegerValue, but with a limit requiring that the value be\\nbetween 0 and 100. In addition, when accessed in Python, the value will be\\ndivided by 100, so that it is immediately suitable for calculations.\\nFor instance, if a myapp.taxes.sales_tax was set to 5 in the editor,\\nthe following calculation would be valid:\\n>>> 5.00 * myapp.taxes.sales_tax\\n0.25\\n\\n\\nPositiveIntegerValue\\nSimilar to IntegerValue, but limited to positive values and 0.\\n\\nStringValue\\nPresents a standard input, accepting any text string up to 255\\n(or DBSETTINGS_VALUE_LENGTH) characters. In\\nPython, the value is accessed as a standard string.\\n\\nDateTimeValue\\nPresents a standard input field, which becomes a datetime in Python.\\nUser input will be parsed according to DATETIME_INPUT_FORMATS setting.\\nIn code, one can assign to field string or datetime object:\\n# These two statements has the same effect\\nmyapp.Feed.next_feed = \\'2012-06-01 00:00:00\\'\\nmyapp.Feed.next_feed = datetime.datetime(2012, 6, 1, 0, 0, 0)\\n\\n\\nDateValue\\nPresents a standard input field, which becomes a date in Python.\\nUser input will be parsed according to DATE_INPUT_FORMATS setting.\\nSee DateTimeValue for the remark about assigning.\\n\\nTimeValue\\nPresents a standard input field, which becomes a time in Python.\\nUser input will be parsed according to TIME_INPUT_FORMATS setting.\\nSee DateTimeValue for the remark about assigning.\\n\\nImageValue\\n(requires PIL or Pillow imaging library to work)\\nAllows to upload image and view its preview.\\nImageValue has optional upload_to keyword, which specify path\\n(relative to MEDIA_ROOT), where uploaded images will be stored.\\nIf keyword is not present, files will be saved directly under\\nMEDIA_ROOT.\\n\\nPasswordValue\\nPresents a standard password input. Retain old setting value if not changed.\\n\\nSetting defaults for a distributed application\\nDistributed applications often have need for certain default settings that are\\nuseful for the common case, but which may be changed to suit individual\\ninstallations. For such cases, a utility is provided to enable applications to\\nset any applicable defaults.\\nLiving at dbsettings.utils.set_defaults, this utility is designed to be used\\nwithin the app\\'s management.py. This way, when the application is installed\\nusing syncdb/migrate, the default settings will also be installed to the database.\\nThe function requires a single positional argument, which is the models\\nmodule for the application. Any additional arguments must represent the actual\\nsettings that will be installed. Each argument is a 3-tuple, of the following\\nformat: (class_name, setting_name, value).\\nIf the value is intended for a module-level setting, simply set class_name\\nto an empty string. The value for setting_name should be the name given to\\nthe setting itself, while the name assigned to the group isn\\'t supplied, as it\\nisn\\'t used for storing the value.\\nFor example, the following code in management.py would set defaults for\\nsome of the settings provided earlier in this document:\\nfrom django.conf import settings\\nfrom dbsettings.utils import set_defaults\\nfrom myproject.myapp import models as myapp\\n\\nset_defaults(myapp,\\n    (\\'\\', \\'enabled\\', True)\\n    (\\'\\', \\'sender\\', settings.ADMINS[0][1]) # Email of the first listed admin\\n    (\\'Image\\', \\'maximum_width\\', 800)\\n    (\\'Image\\', \\'maximum_height\\', 600)\\n)\\n\\n\\n\\nChangelog\\n\\n0.10.0 (25/09/2016)\\n\\nAdded compatibility with Django 1.10\\n\\n\\n0.9.3 (02/06/2016)\\n\\nFixed (hopefully for good) problem with ImageValue in Python 3 (thanks rolexCoder)\\n\\n\\n0.9.2 (01/05/2016)\\n\\nFixed bug when saving non-required settings\\nFixed problem with ImageValue in Python 3 (thanks rolexCoder)\\n\\n\\n0.9.1 (10/01/2016)\\n\\nFixed Sites app being optional (thanks rolexCoder)\\n\\n\\n0.9.0 (25/12/2015)\\n\\nAdded compatibility with Django 1.9 (thanks Alonso)\\nDropped compatibility with Django 1.4, 1.5, 1.6\\n\\n\\n0.8.2 (17/09/2015)\\n\\nAdded migrations to distro\\nAdd configuration option to change max length of setting values from 255 to whatever\\nAdd configuration option to disable caching (thanks nwaxiomatic)\\nFixed PercentValue rendering (thanks last-partizan)\\n\\n\\n0.8.1 (21/06/2015)\\n\\nMade django.contrib.sites framework dependency optional\\nAdded migration for app\\n\\n\\n0.8.0 (16/04/2015)\\n\\nSwitched to using django.utils.six instead of standalone six.\\nAdded compatibility with Django 1.8\\nDropped compatibility with Django 1.3\\n\\n\\n0.7.4 (24/03/2015)\\n\\nAdded default values for fields.\\nFixed Python 3.3 compatibility\\nAdded creation of folders with ImageValue\\n\\n\\n0.7.3, 0.7.2\\npypi problems\\n0.7.1 (11/03/2015)\\n\\nFixed pypi distribution.\\n\\n\\n0.7 (06/07/2014)\\n\\nAdded PasswordValue\\nAdded compatibility with Django 1.6 and 1.7.\\n\\n\\n0.6 (16/09/2013)\\n\\nAdded compatibility with Django 1.5 and python3, dropped support for Django 1.2.\\nFixed permissions: added permission for editing non-model (module-level) settings\\nMake PIL/Pillow not required in setup.py\\n\\n\\n0.5 (11/10/2012)\\n\\nFixed error occuring when test are run with LANGUAGE_CODE different than \\'en\\'\\nAdded verbose_name option for Groups\\nCleaned code\\n\\n\\n0.4.1 (02/10/2012)\\n\\nFixed Image import\\n\\n\\n0.4 (30/09/2012)\\n\\nNamed urls\\nAdded polish translation\\n\\n\\n0.3 (04/09/2012)\\nIncluded testrunner in distribution\\n0.2 (05/07/2012)\\n\\nFixed errors appearing when module-level and model-level settings have\\nsame attribute names\\nCorrected the editor templates admin integration\\nUpdated README\\n\\n\\n0.1 (29/06/2012)\\nInitial PyPI release\\n\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '100',\n",
       "  'forks': '84',\n",
       "  'commits': '181'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'django-predicate\\ndjango-predicate provides a Q like object to facilitate the question: \"would\\nthis model instance be part of a query\" but without running the query or even\\nsaving the object.\\n\\nQuickstart\\nInstall django-predicate:\\npip install django-predicate\\nThen use the P object just as you would Q objects:\\nfrom predicate import P\\n\\np = P(some_field__startswith=\"hello\", age__gt=20)\\nYou can then call the eval method with a model instance to check whether it\\npasses the conditions:\\nmodel_instance = MyModel(some_field=\"hello there\", age=21)\\nother_model_instance = MyModel(some_field=\"hello there\", age=10)\\np.eval(model_instance)\\n>>> True\\np.eval(other_model_instance)\\n>>> False\\nor you can use Python\\'s in operator.\\nmodel_instance in p\\n>>> True\\nEven though a predicate is not a true container class - it can be used as (and\\nwas designed as being) a virtual \"set\" of objects that meets some condiiton.\\nLike Q objects, P objects can be &\\'ed  and |\\'ed together to form more\\ncomplex logic groupings.\\nIn fact, P objects are actually a subclass of Q objects, so you can use them in\\nqueryset filter statements:\\nqs = MyModel.objects.filter(p)\\nP objects also support QuerySet-like filtering operations that can be\\napplied to an arbitrary iterable: P.get(iterable), P.filter(iterable),\\nand P.exclude(iterable):\\nmodel_instance = MyModel(some_field=\"hello there\", age=21)\\nother_model_instance = MyModel(some_field=\"hello there\", age=10)\\np.filter([model_instance, other_model_instance]) == [model_instance]\\n>>> True\\np.get([model_instance, other_model_instance]) == model_instance\\n>>> True\\np.exclude([model_instance, other_model_instance]) == [other_model_instance]\\n>>> True\\nIf you have a situation where you want to use querysets and predicates based on\\nthe same conditions, it is far better to start with the predicate. Because of\\nthe way querysets assume a SQL context, it is non-trivial to reverse engineer\\nthem back into a predicate. However as seen above, it is very straightforward\\nto create a queryset based on a predicate.\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '100',\n",
       "  'forks': '4',\n",
       "  'commits': '140'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'dmc\\ndmc is python library for date and time manipulation.\\nUsage of standard library modules such as datetime, time, pytz etc is dangerous and error prone.\\nThis library is very opinionated about how to treat dates, times and intervals\\nso as to prevent developers from shooting themselves in the foot. Also, the API\\nis quote a bit more convinient for most use cases.\\nOverview\\ndmc is really just a wrapper around several other python libraries, but does so\\nin a way that makes using these libraries safe.\\nSome things to keep in mind:\\n\\nThere is no such thing as a naive time. ALL times involve a timezone, and that timezone is UTC.\\nThe only time we deal with non-UTC timezones is when we parse or display a time.\\nWe support math with dates, and math with times, but you can\\'t do date math with times.\\nFor example: You can\\'t add \"1 day\" to 2014-03-28 02:00:00, because that\\ndoesn\\'t actually make sense. Do you mean add 24 hours? Then do that. Did\\nyou mean you want 02:00:00 on 3/29, sure we can do that, but you have to\\ndecide.\\n\\nWe have a few primary objects:\\n\\nTime - Represents a precise time in UTC.\\nTimeInterval - Represents a change in time, like 10 minutes, or 4000 hours.\\nTimeSpan - a range of Times of some TimeInterval length\\nTimeIterator - generates time instances based on a TimeSpan and a TimeInterval\\nDate - Represents a calendar date. Only the modern Gregorian calendar is supported.\\nDateInterval - Represents a change in calendar date, like \\'2 days\\', \\'next month\\', or \\'next monday\\'\\nDateSpan - a range of dates of some DateInterval.\\nDateIterator - genreates dates based on DateSpan and an DateInterval\\n\\nUsage\\n>> start_t = dmc.Time.now()\\n>> print start_t\\n\"2014-03-29T20:16:58.265249Z\"\\n\\n>> print start.to_str(tz=\\'Americas/Los_Angeles\\')\\n\"2014-03-29T12:16:58.265249-7:00\"\\n\\n>> print start.to_str(local=True)\\n\"2014-03-29T12:16:58.265249-7:00\"\\n\\n>> start.to_str(\"YYYY-MM-DD HH:MM\", tz=\\'Americas/Los_Angeles\\')\\n\"2014-03-29 12:16\"\\n\\n>> print start_t.to_timestamp()\\n1396120755.748726\\n\\n>> print start_t.to_human()\\n\"10 minutes ago\"\\n\\n>> start_t = dmc.Time.from_timestamp(1396120755.748726)\\n\\n>> print start_t.to_datetime()\\ndatetime.datetime(2014, 3, 29, 12, 16, 58, 0, tzinfo=<UTC>)\\n\\n>> d = dmc.Date(2014, 3, 28)\\n>> print d\\n\"2014-03-28\"\\n\\n>> dmc.Date.from_str(\"3/28/2014\")\\ndmc.Date(2014, 28, 3)\\n\\n>> start_t, _ = dmc.TimeSpan.from_date(d)\\n>> print start_t\\n\"2014-03-28T00:00:00Z\"\\n\\n# 3 weeks from now\\n>> d += dmc.DateInterval(weeks=3)\\n\\n# Next Sunday\\n>> d += dmc.DateInterval(weekday=0)\\n\\n>> start_t, end_t = dmc.TimeSpan.from_date(d)\\n\\n>> t = dmc.Time.now()\\n>> t += dmc.TimeInterval(minutes=30)\\n\\n>> today_span = dmc.Date.today().to_timespan()\\n>> for t in dmc.TimeIterator(today_span, dmc.TimeInterval(hours=1)):\\n>>    print t\\n\"2014-03-28T00:00:00Z\"\\n\"2014-03-28T01:00:00Z\"\\n\"2014-03-28T02:00:00Z\"\\n....\\n\\nTesting\\nWhen you\\'re working with date and time sensitive code, it\\'s often very helpful\\nto be able to mock out the current time or date. dmc makes this easy:\\n>> dmc.set_mock_time(dmc.Time().now() - dmc.TimeInterval(hours=1))\\n>> dmc.clear_mock_time()\\n\\nOr, with a friendly context manager:\\n>> with dmc.MockTime(...):\\n.... pass\\n\\nWhat\\'s wrong with datetime\\nThis fun old wiki page is enlightening: https://wiki.python.org/moin/WorkingWithTime\\nDatetime was supposed to be the solution. I still remember when I stumbed\\nacross mx.Datetime. Mind blown. Such a better world. (datetime was added in\\npython 2.3, in 2003, over 10 years ago). You\\'d think that dealing with dates\\nand times would be solved problem. But twice a year I wake up to the collective\\n\"oh shit\" as developers remember daylight savings time.\\nThere is also PEP-431 that\\nattempts to fix datetime timezones. Or is it just patching over how insane it\\nis to handle timezones in this way?\\nThere are also a lot of glaring holes in the API datetime provides us. We fill\\nthose holes with a cornocopia of several other modules.  Basically, if you\\'re\\ndoing anything remotely complex with dates and times you need to understand and\\nmake us of:\\n\\ndatetime\\ntime\\ncalendar\\npytz\\ndateutil\\n\\ndmc attempts to combine all these together into a consistent interface so you don\\'t have to.\\nFor specific examples, try these:\\ntimestamps\\nIt\\'s easy to create a a datetime from a timestamp:\\n>> d = datetime.datetime.fromtimestamp(12312412.0)\\n\\nOh wait, what timezone was that in? What we should have done was\\n>> d = datetime.datetime.utcfromtimestamp(12312412.0)\\n\\nPop, quiz, how do you convert back?\\n>> import time\\n>> time.mktime(d.timetuple())\\n\\nOr wait, did I mean:\\n>> time.mktime(d.utctimetuple())\\n\\nformatting / parsing\\n>> d.isoformat()\\n\\'2014-04-17T15:32:01.219333\\'\\n\\nNow how do I parse an isoformat? There are several solutions on stackoverflow.\\n>> datetime.datetime.strptime(\"2014-04-17T15:32:01\", \"%Y-%m-%dT%H:%M:%S\" )\\n\\nBut of course parsing iso8601 is more complicated than that:\\n>> import re\\n>> s = \"2008-09-03T20:56:35.450686Z\"\\n>> d = datetime.datetime(*map(int, re.split(\\'[^\\\\d]\\', s)[:-1]))\\n\\nOk, there are some 3rd party libraries:\\n>> dateutil.parser.parse(\\'2014-04-17T15:32:01.219333Z\\')\\n\\nOf course you need to be real careful with this library, if there is something\\nit doesn\\'t recognize you\\'ll just get a datetime filled in with values from the\\ncurrent time (!!!! wtf)\\nOh good, there is a python module for JUST THIS ONE FORMAT:\\n>> import iso8601\\n>> iso8601.parse_date(\"2007-01-25T12:00:00Z\")\\n   datetime.datetime(2007, 1, 25, 12, 0, tzinfo=<iso8601.iso8601.Utc ...>)\\n\\nI don\\'t even want to think about whether iso8601.iso8601.Utc == pytz.UTC.\\ntimezones\\n>> datetime.datetime.now()\\n\\nWhat timezone is this in?\\nOh right, what I actually need to do is:\\n>> import pytz\\n>> d = pytz.UTC.localize(datetime.datetime.utcnow())\\n\\nHow about some arithmetic:\\n>> now() + datetime.timedelta(days=1)\\n\\nIs this 24 hours from now? Is this the same number hours since midnight but the\\nnext calendar day? Unfortunately the difference between these interpretations\\nonly becomes obvious twice a year, in only some political regions of the world.\\nWhat if I wanted to enumerate time ranges for a day in localtime, and then run\\nsome queries that are in UTC.\\n>> import pytz\\n>> tz = pytz.timezone(\\'US/Pacific\\')\\n>> start_dt = datetime.datetime(2014, 3, 9, 0, 0, 0, tzinfo=tz)\\n>> end_dt = start_dt + datetime.timedelta(days=1)\\n>> d = start_dt\\n\\nwhile d < end_dt:\\n    run_query(d.astimezone(pytz.UTC), d.astimezone(pytz.UTC) + datetime.timedelta(hours=1))\\n    d += datetime.timedelta(hours=1)\\n\\nHow many errors can you spot?\\nOh, my favorite:\\n>> import pytz\\n>> tz = pytz.timezone(\\'US/Pacific\\')\\n>> start_dt = datetime.datetime(2014, 3, 6, 0, 0, 0, tzinfo=tz)\\n\\n>> d = start_dt\\n>> for _ in range(7):\\n...   print d\\n...   d += datetime.timedelta(days=1\\n2014-03-06 00:00:00-08:00\\n2014-03-07 00:00:00-08:00\\n2014-03-08 00:00:00-08:00\\n2014-03-09 00:00:00-08:00\\n2014-03-10 00:00:00-08:00\\n2014-03-11 00:00:00-08:00\\n2014-03-12 00:00:00-08:00\\n\\nSee anything wrong here? 2014-03-10 00:00:00-08:00 doesn\\'t make any sense.\\nBecause on the 2014-03-10, the timezone should be -07:00. There are fun\\nswitcheroo ways around this:\\n>> d = start_dt\\n>> for _ in range(7):\\n...   print d.astimezone(pytz.UTC).astimezone(pytz.timezone(\\'US/Pacific\\'))\\n...   d += datetime.timedelta(days=1)\\n\\nThis is common enough that pytz actually provides a function for it:\\n>> print pytz.timezone(\\'US/Pacific\\').normalize(d)\\n\\nProject Status\\nExperimental. Still estabilishing interfaces.\\nBasic Date and Time formatting and parsing are written and tested.\\nIntervals and Span are being actively developed.\\nDMC still requires some real world use before APIs are firmly set.\\n',\n",
       "  'watchers': '3',\n",
       "  'stars': '99',\n",
       "  'forks': '3',\n",
       "  'commits': '12'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'dmc\\ndmc is python library for date and time manipulation.\\nUsage of standard library modules such as datetime, time, pytz etc is dangerous and error prone.\\nThis library is very opinionated about how to treat dates, times and intervals\\nso as to prevent developers from shooting themselves in the foot. Also, the API\\nis quote a bit more convinient for most use cases.\\nOverview\\ndmc is really just a wrapper around several other python libraries, but does so\\nin a way that makes using these libraries safe.\\nSome things to keep in mind:\\n\\nThere is no such thing as a naive time. ALL times involve a timezone, and that timezone is UTC.\\nThe only time we deal with non-UTC timezones is when we parse or display a time.\\nWe support math with dates, and math with times, but you can\\'t do date math with times.\\nFor example: You can\\'t add \"1 day\" to 2014-03-28 02:00:00, because that\\ndoesn\\'t actually make sense. Do you mean add 24 hours? Then do that. Did\\nyou mean you want 02:00:00 on 3/29, sure we can do that, but you have to\\ndecide.\\n\\nWe have a few primary objects:\\n\\nTime - Represents a precise time in UTC.\\nTimeInterval - Represents a change in time, like 10 minutes, or 4000 hours.\\nTimeSpan - a range of Times of some TimeInterval length\\nTimeIterator - generates time instances based on a TimeSpan and a TimeInterval\\nDate - Represents a calendar date. Only the modern Gregorian calendar is supported.\\nDateInterval - Represents a change in calendar date, like \\'2 days\\', \\'next month\\', or \\'next monday\\'\\nDateSpan - a range of dates of some DateInterval.\\nDateIterator - genreates dates based on DateSpan and an DateInterval\\n\\nUsage\\n>> start_t = dmc.Time.now()\\n>> print start_t\\n\"2014-03-29T20:16:58.265249Z\"\\n\\n>> print start.to_str(tz=\\'Americas/Los_Angeles\\')\\n\"2014-03-29T12:16:58.265249-7:00\"\\n\\n>> print start.to_str(local=True)\\n\"2014-03-29T12:16:58.265249-7:00\"\\n\\n>> start.to_str(\"YYYY-MM-DD HH:MM\", tz=\\'Americas/Los_Angeles\\')\\n\"2014-03-29 12:16\"\\n\\n>> print start_t.to_timestamp()\\n1396120755.748726\\n\\n>> print start_t.to_human()\\n\"10 minutes ago\"\\n\\n>> start_t = dmc.Time.from_timestamp(1396120755.748726)\\n\\n>> print start_t.to_datetime()\\ndatetime.datetime(2014, 3, 29, 12, 16, 58, 0, tzinfo=<UTC>)\\n\\n>> d = dmc.Date(2014, 3, 28)\\n>> print d\\n\"2014-03-28\"\\n\\n>> dmc.Date.from_str(\"3/28/2014\")\\ndmc.Date(2014, 28, 3)\\n\\n>> start_t, _ = dmc.TimeSpan.from_date(d)\\n>> print start_t\\n\"2014-03-28T00:00:00Z\"\\n\\n# 3 weeks from now\\n>> d += dmc.DateInterval(weeks=3)\\n\\n# Next Sunday\\n>> d += dmc.DateInterval(weekday=0)\\n\\n>> start_t, end_t = dmc.TimeSpan.from_date(d)\\n\\n>> t = dmc.Time.now()\\n>> t += dmc.TimeInterval(minutes=30)\\n\\n>> today_span = dmc.Date.today().to_timespan()\\n>> for t in dmc.TimeIterator(today_span, dmc.TimeInterval(hours=1)):\\n>>    print t\\n\"2014-03-28T00:00:00Z\"\\n\"2014-03-28T01:00:00Z\"\\n\"2014-03-28T02:00:00Z\"\\n....\\n\\nTesting\\nWhen you\\'re working with date and time sensitive code, it\\'s often very helpful\\nto be able to mock out the current time or date. dmc makes this easy:\\n>> dmc.set_mock_time(dmc.Time().now() - dmc.TimeInterval(hours=1))\\n>> dmc.clear_mock_time()\\n\\nOr, with a friendly context manager:\\n>> with dmc.MockTime(...):\\n.... pass\\n\\nWhat\\'s wrong with datetime\\nThis fun old wiki page is enlightening: https://wiki.python.org/moin/WorkingWithTime\\nDatetime was supposed to be the solution. I still remember when I stumbed\\nacross mx.Datetime. Mind blown. Such a better world. (datetime was added in\\npython 2.3, in 2003, over 10 years ago). You\\'d think that dealing with dates\\nand times would be solved problem. But twice a year I wake up to the collective\\n\"oh shit\" as developers remember daylight savings time.\\nThere is also PEP-431 that\\nattempts to fix datetime timezones. Or is it just patching over how insane it\\nis to handle timezones in this way?\\nThere are also a lot of glaring holes in the API datetime provides us. We fill\\nthose holes with a cornocopia of several other modules.  Basically, if you\\'re\\ndoing anything remotely complex with dates and times you need to understand and\\nmake us of:\\n\\ndatetime\\ntime\\ncalendar\\npytz\\ndateutil\\n\\ndmc attempts to combine all these together into a consistent interface so you don\\'t have to.\\nFor specific examples, try these:\\ntimestamps\\nIt\\'s easy to create a a datetime from a timestamp:\\n>> d = datetime.datetime.fromtimestamp(12312412.0)\\n\\nOh wait, what timezone was that in? What we should have done was\\n>> d = datetime.datetime.utcfromtimestamp(12312412.0)\\n\\nPop, quiz, how do you convert back?\\n>> import time\\n>> time.mktime(d.timetuple())\\n\\nOr wait, did I mean:\\n>> time.mktime(d.utctimetuple())\\n\\nformatting / parsing\\n>> d.isoformat()\\n\\'2014-04-17T15:32:01.219333\\'\\n\\nNow how do I parse an isoformat? There are several solutions on stackoverflow.\\n>> datetime.datetime.strptime(\"2014-04-17T15:32:01\", \"%Y-%m-%dT%H:%M:%S\" )\\n\\nBut of course parsing iso8601 is more complicated than that:\\n>> import re\\n>> s = \"2008-09-03T20:56:35.450686Z\"\\n>> d = datetime.datetime(*map(int, re.split(\\'[^\\\\d]\\', s)[:-1]))\\n\\nOk, there are some 3rd party libraries:\\n>> dateutil.parser.parse(\\'2014-04-17T15:32:01.219333Z\\')\\n\\nOf course you need to be real careful with this library, if there is something\\nit doesn\\'t recognize you\\'ll just get a datetime filled in with values from the\\ncurrent time (!!!! wtf)\\nOh good, there is a python module for JUST THIS ONE FORMAT:\\n>> import iso8601\\n>> iso8601.parse_date(\"2007-01-25T12:00:00Z\")\\n   datetime.datetime(2007, 1, 25, 12, 0, tzinfo=<iso8601.iso8601.Utc ...>)\\n\\nI don\\'t even want to think about whether iso8601.iso8601.Utc == pytz.UTC.\\ntimezones\\n>> datetime.datetime.now()\\n\\nWhat timezone is this in?\\nOh right, what I actually need to do is:\\n>> import pytz\\n>> d = pytz.UTC.localize(datetime.datetime.utcnow())\\n\\nHow about some arithmetic:\\n>> now() + datetime.timedelta(days=1)\\n\\nIs this 24 hours from now? Is this the same number hours since midnight but the\\nnext calendar day? Unfortunately the difference between these interpretations\\nonly becomes obvious twice a year, in only some political regions of the world.\\nWhat if I wanted to enumerate time ranges for a day in localtime, and then run\\nsome queries that are in UTC.\\n>> import pytz\\n>> tz = pytz.timezone(\\'US/Pacific\\')\\n>> start_dt = datetime.datetime(2014, 3, 9, 0, 0, 0, tzinfo=tz)\\n>> end_dt = start_dt + datetime.timedelta(days=1)\\n>> d = start_dt\\n\\nwhile d < end_dt:\\n    run_query(d.astimezone(pytz.UTC), d.astimezone(pytz.UTC) + datetime.timedelta(hours=1))\\n    d += datetime.timedelta(hours=1)\\n\\nHow many errors can you spot?\\nOh, my favorite:\\n>> import pytz\\n>> tz = pytz.timezone(\\'US/Pacific\\')\\n>> start_dt = datetime.datetime(2014, 3, 6, 0, 0, 0, tzinfo=tz)\\n\\n>> d = start_dt\\n>> for _ in range(7):\\n...   print d\\n...   d += datetime.timedelta(days=1\\n2014-03-06 00:00:00-08:00\\n2014-03-07 00:00:00-08:00\\n2014-03-08 00:00:00-08:00\\n2014-03-09 00:00:00-08:00\\n2014-03-10 00:00:00-08:00\\n2014-03-11 00:00:00-08:00\\n2014-03-12 00:00:00-08:00\\n\\nSee anything wrong here? 2014-03-10 00:00:00-08:00 doesn\\'t make any sense.\\nBecause on the 2014-03-10, the timezone should be -07:00. There are fun\\nswitcheroo ways around this:\\n>> d = start_dt\\n>> for _ in range(7):\\n...   print d.astimezone(pytz.UTC).astimezone(pytz.timezone(\\'US/Pacific\\'))\\n...   d += datetime.timedelta(days=1)\\n\\nThis is common enough that pytz actually provides a function for it:\\n>> print pytz.timezone(\\'US/Pacific\\').normalize(d)\\n\\nProject Status\\nExperimental. Still estabilishing interfaces.\\nBasic Date and Time formatting and parsing are written and tested.\\nIntervals and Span are being actively developed.\\nDMC still requires some real world use before APIs are firmly set.\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '99',\n",
       "  'forks': '21',\n",
       "  'commits': '78'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Valor\\n \\nPython HTTP clients for APIs represented by JSON Schema.\\nThis is still super-early days yet, many things probably don't work. Use at your own risk.\\nAmong most other things, docs aren't done, but check this out:\\n$ heroku auth:whoami\\njacob@heroku.com\\n$ heroku apps\\nancient-thicket-4976\\narcane-reef-4005\\n...\\n\\n$ python -i heroku.py\\n>>> heroku.account.info().email\\nu'jacob@heroku.com'\\n>>> [app.name for app in heroku.app.list()]\\n[u'ancient-thicket-4976', u'arcane-reef-4005', ...]\\nThen see heroku.py as an example of how this works.\\n\\n\\nWhat's with the name? The Ruby version of the same thing is Heroics. Heroics. Valor. See what I did there?\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '99',\n",
       "  'forks': '2',\n",
       "  'commits': '27'},\n",
       " {'language': 'Python 84.4',\n",
       "  'readme': 'Ansible Role: blockinfile\\nThis role contains no tasks, but provides blockinfile module\\nwhich might be useful when you want to maintain multi-line snippets\\nin config files in /etc.\\nAnsible Galaxy Page: https://galaxy.ansible.com/list#/roles/1475\\nRequest for review:\\nThe pull request to ansible-modules-extras\\nhas been made to include blockinfile module\\nin the official distribution of Ansible,\\nwhich enables you to use blockinfile as a standard module without this role!\\nIf you use this module and feel it\\'s useful,\\nplease leave some endorsement comments on the PR.\\nI greatly appreciate if you\\'re\\nan eligible reviewer (existing module author)\\nand could take some time to review the PR,\\notherwise if you could ask reviewers of your acquiaintance for the review.\\nIt needs two +1 votes from reviewers in order to be nominated for inclusion.\\nblockinfile Module\\nThis module will insert/update/remove a block of multi-line text\\nsurrounded by the marker lines.\\nExample task:\\n- blockinfile:\\n    dest: /etc/network/interfaces\\n    block: |\\n      iface eth0 inet static\\n          address 192.168.0.1\\n          netmask 255.255.255.0\\nText inserted/updated by the task in /etc/network/interfaces:\\n# BEGIN ANSIBLE MANAGED BLOCK\\niface eth0 inet static\\n    address 192.168.0.1\\n    netmask 255.255.255.0\\n# END ANSIBLE MANAGED BLOCK\\n\\nIt uses marker lines # {BEGIN/END} ANSIBLE MANAGED BLOCK as default.\\nYou can specify alternative marker lines by marker option\\nwhen you need to update files in other formats like HTML,\\nor run multiple blockinfile tasks on the same file.\\nOptions\\nIf this section doesn\\'t show nicely in Ansible Galaxy Page,\\nplease refer to equivalent in\\nGitHub Page.\\n\\n\\n\\nparameter\\nrequired\\ndefault\\nchoices\\ncomments\\n\\n\\nbackup\\nno\\nno\\nyesno\\nCreate a backup file including the timestamp information so you can get the original file back if you somehow clobbered it incorrectly.\\n\\nblock\\nno\\n\\n\\nThe text to insert inside the marker lines. If it\\'s missing or an empty string, the block will be removed as if state were specified to absent.\\naliases: content\\n\\ncreate\\nno\\nno\\nyesno\\nCreate a new file if it doesn\\'t exist.\\n\\ndest\\nyes\\n\\n\\nThe file to modify.\\naliases: name, destfile\\n\\nfollow (added in 1.8)\\nno\\nno\\nyesno\\nThis flag indicates that filesystem links, if they exist, should be followed.\\n\\ngroup\\nno\\n\\n\\nname of the group that should own the file/directory, as would be fed to chown\\n\\ninsertafter\\nno\\nEOF\\nEOF*regex*\\nIf specified, the block will be inserted after the last match of specified regular expression. A special value is available; EOF for inserting the block at the end of the file.  If specified regular expresion has no matches, EOF will be used instead.\\n\\ninsertbefore\\nno\\n\\nBOF*regex*\\nIf specified, the block will be inserted before the last match of specified regular expression. A special value is available; BOF for inserting the block at the beginning of the file.  If specified regular expresion has no matches, the block will be inserted at the end of the file.\\n\\nmarker\\nno\\n# {mark} ANSIBLE MANAGED BLOCK\\n\\nThe marker line template. \"{mark}\" will be replaced with \"BEGIN\" or \"END\".\\n\\nmode\\nno\\n\\n\\nmode the file or directory should be. For those used to /usr/bin/chmod remember that modes are actually octal numbers (like 0644). Leaving off the leading zero will likely have unexpected results. As of version 1.8, the mode may be specified as a symbolic mode (for example, u+rwx or u=rw,g=r,o=r).\\n\\nowner\\nno\\n\\n\\nname of the user that should own the file/directory, as would be fed to chown\\n\\nselevel\\nno\\ns0\\n\\nlevel part of the SELinux file context. This is the MLS/MCS attribute, sometimes known as the range. _default feature works as for seuser.\\n\\nserole\\nno\\n\\n\\nrole part of SELinux file context, _default feature works as for seuser.\\n\\nsetype\\nno\\n\\n\\ntype part of SELinux file context, _default feature works as for seuser.\\n\\nseuser\\nno\\n\\n\\nuser part of SELinux file context. Will default to system policy, if applicable. If set to _default, it will use the user portion of the policy if available\\n\\nstate\\nno\\npresent\\npresentabsent\\nWhether the block should be there or not.\\n\\nvalidate\\nno\\nNone\\n\\nThe validation command to run before copying into place. The path to the file to validate is passed in via \\'%s\\' which must be present as in the example below. The command is passed securely so shell features like expansion and pipes won\\'t work.\\n\\nExamples\\n- name: insert/update \"Match User\" configuation block in /etc/ssh/sshd_config\\n  blockinfile:\\n    dest: /etc/ssh/sshd_config\\n    block: |\\n      Match User ansible-agent\\n      PasswordAuthentication no\\n- name: insert/update eth0 configuration stanza in /etc/network/interfaces\\n        (it might be better to copy files into /etc/network/interfaces.d/)\\n  blockinfile:\\n    dest: /etc/network/interfaces\\n    block: |\\n      iface eth0 inet static\\n          address 192.168.0.1\\n          netmask 255.255.255.0\\n- name: insert/update HTML surrounded by custom markers after <body> line\\n  blockinfile:\\n    dest: /var/www/html/index.html\\n    marker: \"<!-- {mark} ANSIBLE MANAGED BLOCK -->\"\\n    insertafter: \"<body>\"\\n    content: |\\n      <h1>Welcome to {{ansible_hostname}}</h1>\\n      <p>Last updated on {{ansible_date_time.iso8601}}</p>\\n- name: remove HTML as well as surrounding markers\\n  blockinfile:\\n    dest: /var/www/html/index.html\\n    marker: \"<!-- {mark} ANSIBLE MANAGED BLOCK -->\"\\n    content: \"\"\\nRequirements\\nNone.\\nRole Variables\\nNone.\\nDependencies\\nNone.\\nExample Playbook\\nComplete playbook\\nthat makes SSH password authentication for specific user prohibited,\\nthen restarts sshd if needed.\\n---\\n- hosts: all\\n  remote_user: ansible-agent\\n  sudo: yes\\n  roles:\\n    - yaegashi.blockinfile\\n  tasks:\\n    - name: Prohibit SSH password authentication for $SUDO_USER\\n      blockinfile:\\n        dest: /etc/ssh/sshd_config\\n        backup: yes\\n        content: |\\n          Match User {{ansible_env.SUDO_USER}}\\n          PasswordAuthentication no\\n      notify: Restart sshd\\n  handlers:\\n    - name: Restart sshd\\n      service\\n        name: ssh\\n        state: restarted\\nLicense\\nGPLv3+\\nAuthor Information\\nYAEGASHI Takeshi\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '99',\n",
       "  'forks': '20',\n",
       "  'commits': '34'},\n",
       " {'language': 'Python 82.0',\n",
       "  'readme': 'django-pdfutils\\nA simple django app to generate PDF documents.\\n\\nInstallation\\n\\nIn your settings.py, add pdfutils to your INSTALLED_APPS.\\n(r\\'^reports/\\', include(pdfutils.site.urls)), to your urls.py\\nAdd pdfutils.autodiscover() to your urls.py\\nCreate a report.py file in any installed django application.\\nCreate your report(s)\\nProfit!\\n\\nNote: If you are using buildout, don\\'t forget to put pdfutils\\nin your eggs section or else the django-pdfutils dependencies wont\\nbe installed.\\n\\nExample report\\nReports are basically views with custom methods and properties.\\n# -*- coding: utf-8 -*-\\n\\nfrom django.contrib.auth.models import User\\nfrom django.core.urlresolvers import reverse\\nfrom django.utils.translation import ugettext as _\\n\\nfrom pdfutils.reports import Report\\nfrom pdfutils.sites import site\\n\\n\\nclass MyUserReport(Report):\\n    title = _(\\'Users\\')\\n    template_name = \\'myapp/reports/users-report.html\\'\\n    slug = \\'users-report\\'\\n    orientation = \\'portrait\\'\\n\\n    def get_users(self):\\n        return User.objects.filter(is_staff=True)\\n\\n    def get_styles(self):\\n        \"\"\"\\n        It is possible to add or override style like so\\n        \"\"\"\\n        self.add_styles(\\'myapp/css/users-report.css\\')\\n        return super(AccountStatementReport, self).get_styles()\\n\\n    def filename(self):\\n        \"\"\"\\n        The filename can be generated dynamically and translated\\n        \"\"\"\\n        return _(\\'Users-report-%(count)s.pdf\\') % {\\'count\\': self.get_users().count() }\\n\\n    def get_context_data(self):\\n        \"\"\"\\n        Context data is injected just like a normal view\\n        \"\"\"\\n        context = super(AccountStatementReport, self).get_context_data()\\n        context[\\'user_list\\'] = self.get_users()\\n        return context\\n\\nsite.register(MyUserReport)\\nThe slug should obviously be unique since it is used to build the report URL.\\nFor example, with the default settings and URLs, the URL for report above would be /reports/users-report/.\\n\\nExample template\\n<html>\\n    <head>\\n        {{ STYLES|safe }}\\n    </head>\\n    <body class=\"{% if landscape %}landscape{% else %}portrait{% endif %}\">\\n        <ul>\\n            {% for user in user_list %}\\n            <li>{{ user }}</li>\\n            {% endfor %}\\n        </ul>\\n        <a href=\"{% url \\'pdfutils:your_report_slug\\' %}?format=html\">Add ?format=html for easy template debug</a>\\n    </body>\\n</html>\\nSome template variables are injected by default in reports:\\n\\ntitle\\nslug\\norientation\\nMEDIA_URL\\nSTATIC_URL\\nSTYLES\\n\\n\\nOverriding default CSS\\nSince the default CSS (base.css, portrait.css, landscape.css) are normal static files, they can be overrided\\nfrom any other django app which has a pdfutils folder in their static folder.\\nNote: Be sure your applications are listed in the right order in INSTALLED_APPS !\\n\\nDependencies\\n\\ndjango >=1.4, < 1.5.99\\ndecorator == 3.4.0, <= 3.9.9\\nPIL == 1.1.7\\nreportlab == 2.5\\nhtml5lib == 0.90\\nhttplib2 == 0.9\\npyPdf == 1.13\\nxhtml2pdf == 0.0.4\\ndjango-xhtml2pdf == 0.0.3\\n\\nNote: dependencies versions are specified in setup.py. The amount of time required to find the right\\ncombination of dependency versions is largely to blame for the creation of this project.\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '98',\n",
       "  'forks': '12',\n",
       "  'commits': '89'},\n",
       " {'language': 'Python 99.0',\n",
       "  'readme': \"PokemonGo-SlackBot\\nReceive Slack notification whenever a Pokémon spawns near a given location!\\nFor instructions please check:\\nhttps://www.themarketingtechnologist.co/pokemon-go-slack-notifications/\\n####UPDATE #1: Now also works in Japanese thanks to https://github.com/ttymsd!\\n####UPDATE #2: Thanks to Bart persoons, you can also hook up to Homey (https://www.athom.com/en/)\\nInstructions are in UPDATE #2 below the blog post: https://www.themarketingtechnologist.co/pokemon-go-slack-notifications/\\n####Update #3: Languages, stability and lured Pokémon!\\nSeveral updates:\\nMessage are now available in French and German thanks to Vincent. They can be set by setting the locale. The locale that needs to be used for the emojis can be set separately with -iL, default is English.\\nAs we also see in the app and at Pokevision, server quite often seem to have troubles. Therefore, the script now has a reconnect features instead of stopping the script in cases of issues. From experience, it's advisable to use a Google account, since it's more stable than PTC.\\nAnd last but not least, lured Pokémon! Pokevision doesn't show these (yet), so many thanks go to the tip by Daniel.\\n\",\n",
       "  'watchers': '14',\n",
       "  'stars': '97',\n",
       "  'forks': '29',\n",
       "  'commits': '32'},\n",
       " {'language': 'Python 90.7',\n",
       "  'readme': 'django-filepicker\\n\\n\\nA django plugin to make integrating with Filepicker.io even easier\\n##Installation\\n\\n\\nInstall the python package:\\npip install django-filepicker\\n\\n\\n\\nAdd your file picker api key to your settings.py file. You api key can be\\nfound in the developer portal.\\nFILEPICKER_API_KEY = <your api key>\\n\\n\\n\\nConfigure your media root.\\nCWD = os.getcwd()\\nMEDIA_ROOT = os.path.join(CWD, \\'media\\')\\n\\n\\n\\nAdd a filepicker field to your model and set the upload_to value.\\n# *Please note, that FPFileField handle only one file*\\n# In demo you can see how to handle multiple files upload.\\nfpfile = django_filepicker.models.FPFileField(upload_to=\\'uploads\\')\\n\\n\\n\\nModify your view to accept the uploaded files along with the post data.\\nform = models.TestModelForm(request.POST, request.FILES)\\nif form.is_valid():\\n    #Save will read the data and upload it to the location\\n    # defined in TestModel\\n    form.save()\\n\\n\\n\\nAdd the form.media variable above your other JavaScript calls.\\n<head>\\n    <title>Form Template Example</title>\\n    <!--  Normally this would go into a block defined in base.html that\\n          occurs before other JavaScript calls. -->\\n    {{ form.media }}\\n</head>\\n\\n<body>\\n    <form method=\"POST\" action=\"/\" enctype=\"multipart/form-data\">\\n        {{ form.as_p }}\\n        <input type=\"submit\" />\\n    </form>\\n</body>\\n\\n\\n\\n##Demo\\nTo see how all the pieces come together, see the example code in demo/, which you can run with the standard\\npython manage.py runserver command\\n###models.py\\nimport django_filepicker\\nclass TestModel(models.Model):\\n#FPFileField is a field that will render as a filepicker dragdrop widget, but\\n#When accessed will provide a File-like interface (so you can do fpfile.read(), for instance)\\nfpfile = django_filepicker.models.FPFileField(upload_to=\\'uploads\\')\\n###views.py\\n#building the form - automagically turns the uploaded fpurl into a File object\\nform = models.TestModelForm(request.POST, request.FILES)\\nif form.is_valid():\\n#Save will read the data and upload it to the location defined in TestModel\\nform.save()\\nBe sure to also provide your Filepicker.io api key, either as a parameter to the FPFileField or in settings.py as FILEPICKER_API_KEY\\n##Components\\n###Models\\nThe filepicker django library defines the FPFileField model field so you can get all the benefits of using Filepicker.io as a drop-in replacement for the standard django FileField. No need to change any of your view logic.\\n###Forms\\nSimilarly with the FPFileField for models, the filepicker django library defines a FPFileField for forms as well, that likewise serves as a drop-in replacement for the standard django FileField. There is also the FPUrlField if you want to store the Filepicker.io URL instead\\n###Middleware\\nAlso included is a middleware library that will take any Filepicker.io urls passed to the server, download the contents, and place the result in request.FILES. This way, you can keep your backend code for handling file uploads the same as before while adding all the front-end magic that Filepicker.io provides\\nIf you have any questions, don\\'t hesitate to reach out at contact@filepicker.io. For more information, see https://filepicker.io\\nOpen-sourced under the MIT License. Pull requests encouraged!\\n',\n",
       "  'watchers': '31',\n",
       "  'stars': '97',\n",
       "  'forks': '30',\n",
       "  'commits': '99'},\n",
       " {'language': 'Java 66.4',\n",
       "  'readme': 'FragmentStack\\nA helper class for managing a stack of Fragments in a single container.\\nLicense\\nCopyright 2013 Simon Vig Therkildsen\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '105',\n",
       "  'forks': '19',\n",
       "  'commits': '2'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': \"DragArea\\nThis is an android library project that provides drag and drop operations\\non top of android-10.\\nStarting in android-11 (Android 3.0 / Honeycomb) Google provided drag and\\ndrop API's of their own. This library is based loosely on those API's\\nand so may be useful to people wishing to develop for Android 2.0 who\\nwant an easyish upgrade path.\\nDocumentation is available at\\nhttp://doffm.github.com/android-dragarea/.\\nA small example using this library can be found at\\nhttps://github.com/doffm/android-dragexample.\\n\",\n",
       "  'watchers': '8',\n",
       "  'stars': '104',\n",
       "  'forks': '31',\n",
       "  'commits': '9'},\n",
       " {'language': 'Java 99.5',\n",
       "  'readme': 'IMClient\\n基于XMPP协议的 Android 即时通讯客户端\\n即时通讯公开课\\n1.地址：http://pan.baidu.com/s/1qWnYNyG\\n2.密码：h7yu\\n3.交流群：324632947\\n\\n如何使用\\n更改IM.java里的HOST为你的openfire地址\\npublic static final String HOST = \"192.168.1.123\";\\n##截图\\n\\n\\n\\n',\n",
       "  'watchers': '24',\n",
       "  'stars': '103',\n",
       "  'forks': '69',\n",
       "  'commits': '50'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Loop View for Android\\nEnglish 中文\\nAndroid LoopView is a powerful widget for unlimited rotation picture, It provides some configuration options and good control the appearance and operational requirements.\\nSimple usage picture:\\n\\nCustom layout usage picture:\\n\\nUsing LoopView in your application\\nIf you are building with Gradle, simply add the following line to the dependencies section of your build.gradle file:\\nsupport\\nimplementation \\'com.kevin:loopview:1.5.6\\'\\n\\nandroidX\\nimplementation \\'com.kevin:loopview:2.0.1\\'\\n\\nSimple Usage\\nConfigured as View in layout.xml\\nTo add the LoopView to your application, specify <com.kevin.loopview.BannerView in your layout XML.\\n<com.kevin.loopview.BannerView\\n    android:id=\"@+id/main_act_banner\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"192dp\">\\n</com.kevin.loopview.BannerView>\\n\\nConfigured Programmatically\\nBannerView mBannerView = (BannerView) this.findViewById(R.id.main_act_banner);\\n// set image loader, can set up any image engine.\\nmBannerView.setImageLoader(new ImageLoader() {\\n    @Override\\n    public void loadImage(ImageView imageView, String url, int placeholder) {\\n        Glide.with(imageView.getContext()).load(url).into(imageView);\\n    }\\n});\\nString json = LocalFileUtils.getStringFormAsset(this, \"loopview_date.json\");\\nLoopData loopData = new Gson().fromJson(json, LoopData.class);\\nmBannerView.setData(loopData);\\n// begin to loop\\nmBannerView.startAutoLoop();\\n\\nmBannerView.setOnItemClickListener(new BaseLoopAdapter.OnItemClickListener() {\\n    @Override\\n    public void onItemClick(View view, LoopData.ItemData itemData, int position) {\\n        // Open connection with browser\\n        Intent intent = new Intent();\\n        intent.setData(Uri.parse(itemData.link));\\n        intent.setAction(Intent.ACTION_VIEW);\\n        startActivity(intent);\\n    }\\n});\\n\\nMore configuration Usage\\nXML Usage\\nIf you decide to use BannerView as a view, you can define it in your xml layout like this:\\nxmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n\\n<com.kevin.loopview.BannerView\\n    android:id=\"@+id/adloop_act_adloopview\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"192dp\"\\n    app:loop_interval=\"5000\"\\n    app:loop_scrollDuration=\"2000\"\\n    app:loop_dotMargin=\"5dp\"\\n    app:loop_autoLoop=\"[true|false]\"\\n    app:loop_alwaysShowDot=\"[true|false]\"\\n    app:loop_dotSelector=\"@drawable/ad_dots_selector\"\\n    app:loop_placeholder=\"@mipmap/ic_launcher\"\\n    app:loop_layout=\"@layout/ad_loopview_layout\">\\n</com.kevin.loopview.BannerView>\\n\\nProgramme Usage\\n// Set page switching transition time\\nmLoopView.setScrollDuration(1000);\\n// Set time interval\\nmLoopView.setInterval(3000);\\n// To initialize the data in a collection\\nmLoopView.setData(List<Map<String, String>> data);\\n// Initialized data in entity mode\\nmLoopView.setData(LoopData rotateData);\\n// Initialized data in a collection mode\\nmLoopView.setData(List<String> images);\\n// Initialized data in a collection mode\\nmLoopView.setData(List<String> images, List<String> links);\\n// Initialized data in a collection mode\\nmLoopView.setData(List<String> images, List<String> descs, List<String> links);\\n// Get the running loop date\\nmLoopView.getData();\\n// Begin to auto Loop\\nmLoopView.startAutoLoop();\\n// Begin to auto Loop delay\\nmLoopView.startAutoLoop(long delayTimeInMills);\\n// Stop to auto Loop\\nmLoopView.stopAutoLoop();\\n// Set a custom loop layout\\nmLoopView.setLoopLayout(int layoutResId);\\n\\nNotes:\\nIn custom layout you must to use those ids loop_view_pager in ViewPager loop_view_dots in indicate point parent LinearLayout and loop_view_desc in description TextView;\\nMake sure you at least have loop_view_pager.\\nLicense\\nCopyright 2015 Kevin zhou\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '103',\n",
       "  'forks': '51',\n",
       "  'commits': '119'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'CircleImageView\\nCircleImageView is a component which display circle image with customization options\\n\\n\\nRelease notes\\n1.2.0\\n\\nImplemented animation for pressed event\\nAdded new attribute pressedRingWidth, pressedRingColor\\n\\n1.1.4\\n\\nFix bug (issue #3 on github)\\n\\n1.1.3\\n\\nImprove component performance\\nOptimization of image drawing\\n\\n1.1.2\\n\\nSupport padding parameters (paddingLeft, paddingTop, paddingRight, paddingBottom).\\n\\n1.1.1\\n\\nFix bug (issue #1 on github).\\n\\n1.1.0\\n\\nshows shadow for image;\\nshows shadow for border.\\n\\n1.0.0\\n\\nchange background for *.png images;\\nshows images;\\nrealize click listener for this component;\\nshows border (component is selected or component is unselected).\\n\\nUsage\\nTo make a circular ImageView, add this CircleImageView library to your project and add CircleImageView in your layout XML.\\nYou can also grab it via Gradle:\\n      compile \\'com.alexzh:circleimageview:1.2.0\\'\\nor Maven:\\n<dependency>\\n    <groupId>com.alexzh</groupId>\\n    <artifactId>circleimageview</artifactId>\\n    <version>1.2.0</version>\\n    <type>pom</type>\\n</dependency>\\nXML\\n    <com.alexzh.circleimageview.CircleImageView\\n        android:id=\"@+id/imageView\"\\n        android:layout_width=\"192dp\"\\n        android:layout_height=\"192dp\"\\n        android:clickable=\"true\"\\n        android:src=\"@drawable/android_logo\"\\n        android:layout_alignParentTop=\"true\"\\n        android:layout_centerHorizontal=\"true\"\\n        android:layout_marginTop=\"10dp\"\\n        app:view_backgroundColor=\"@color/colorPrimary\"\\n        app:view_shadowRadius=\"4dp\"\\n        app:view_shadowDx=\"2dp\"\\n        app:view_shadowDy=\"0dp\"\\n        app:view_shadowColor=\"@color/grey\"\\n        app:view_borderWidth=\"4dp\"\\n        app:view_selectedColor=\"@color/blue\"\\n        app:view_borderColor=\"@android:color/darker_gray\"/>\\nYou may use the following properties in your XML to customize your CircularImageView.\\nProperties:\\n\\napp:view_backgroundColor    (color)\\napp:view_borderColor        (color)\\napp:view_borderWidth        (dimension)\\napp:view_selectedColor      (color)\\napp:view_shadowRadius       (dimension)\\napp:view_shadowDx           (dimension)\\napp:view_shadowDy           (dimension)\\napp:view_shadowColor        (color)\\n\\nJAVA\\n\\n\\nsetOnItemSelectedClickListener(ItemSelectedListener listener) - let\\'s handle onSelected(View view) and onUnselected(View view)\\n\\n\\nonSelected(View view) - view is selected\\n\\n\\nonUnselected(View view) - view is unselected\\n\\n\\nDeveloped By\\nAliaksandr Zhukovich - http://alexzh.com\\nLicense\\nCopyright (C) 2016 Aliaksandr Zhukovich (http://alexzh.com)\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n     http://www.apache.org/licenses/LICENSE-2.0\\t     \\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '103',\n",
       "  'forks': '27',\n",
       "  'commits': '58'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'JGet - Download files from anywhere\\n\\n\\nGetting Started\\nThis is basic java downloader library, analog to wget\\nInstalling\\nStep 1: Add to maven dependencies\\n        <dependency>\\n            <groupId>com.github.elvinmahmudov</groupId>\\n            <artifactId>jget</artifactId>\\n            <version>1.1</version>\\n        </dependency>\\n\\nStep 2: Create JGet instance and add task to it\\npublic class Main {\\n\\n    private static String MODEL_NAME = \"com.elvinmahmudov.dynocom.MathTeacher\";\\n\\n    public static void main(String[] args) throws IOException {\\n        JGet jget = JGet.getInstance();\\n        String url = \"https://file-examples-com.github.io/uploads/2017/04/file_example_MP4_480_1_5MG.mp4\";\\n        String saveDirectory = \"download\";\\n\\n        JTask task = new JTask(url, saveDirectory, \"test.mp4\");\\n        jget.addTask(task);\\n        jget.start();\\n    }\\n\\n}\\n\\n',\n",
       "  'watchers': '1',\n",
       "  'stars': '103',\n",
       "  'forks': '0',\n",
       "  'commits': '13'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': '模仿墨迹天气3.0引导界面\\n\\n屏幕录制使用的licecap\\nhttp://www.cockos.com/licecap/\\nREADME.md编辑参考\\nhttp://www.tuicool.com/articles/zIJrEjn\\n@author 张兴业\\nhttp://blog.csdn.net/xyz_lmn\\n我的新浪微博：@张兴业TBOW\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '102',\n",
       "  'forks': '48',\n",
       "  'commits': '14'},\n",
       " {'language': 'Java 99.2',\n",
       "  'readme': 'CHelper is distrubuted under LGPL\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '102',\n",
       "  'forks': '52',\n",
       "  'commits': '256'},\n",
       " {'language': 'Java 70.9',\n",
       "  'readme': 'CHelper is distrubuted under LGPL\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '102',\n",
       "  'forks': '36',\n",
       "  'commits': '77'},\n",
       " {'language': 'Java 81.2',\n",
       "  'readme': 'multi-column-list-adapter  \\xa0\\xa0 \\nMultiColumnListAdapter is a cursor adapter that enables you to make a ListView that looks like a GridView. One important benefit is that you can add headers and footers to your grid-looking list.\\n\\nSample App\\nThe sample app demonstrates how to use the MultiColumnListAdapter to create a GridView-looking layout with a header. You can build it from source or install it from the Play Store.\\nDownload\\nGrab the library from Maven central\\n<dependency>\\n    <groupId>com.twotoasters.multicolumnlistadapter</groupId>\\n    <artifactId>library</artifactId>\\n    <version>1.0.0</version>\\n</dependency>\\nor Gradle:\\ncompile \\'com.twotoasters.multicolumnlistadapter:library:1.0.+\\'\\nCredit\\nMultiColumnListAdapter was created by Two Toasters in development with Ebates.\\nLicense\\nCopyright 2014 Two Toasters\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '13',\n",
       "  'stars': '102',\n",
       "  'forks': '19',\n",
       "  'commits': '15'},\n",
       " {'language': 'C++ 52.2',\n",
       "  'readme': 'THIS DOESN\\'T WORK anymore with newer Catalyst drivers (since 15.7)\\nOpenEncodeVFW\\nVFW encoder for AMD VCE h264 encoder. Usable with Virtualdub, Dxtory etc.\\nhttps://github.com/jackun/openencodevfw/archive/master.zip\\nExtra settings are saved to registry under HKCU\\\\Software\\\\OpenEncodeVFW\\nAs OpenEncode has been deprecated by AMD for a long time already, it appears the support has been finally dropped from Catalyst 15.7\\nYou may need to install MSVC++ 2013 runtimes.\\nLast MSVC++2010 commit.\\nNOTE: You need to install x86 version for 32bit codec even if your Windows is 64 bit.\\nNOTE: VCE on cards/APUs prior to Tonga only go up to 1080p and solid 1080p60 recording can not be guaranteed (yet) unfortunately.\\nCompatible hardware\\nAMD\\'s GCN based cards and APUs.\\nFrom AMD\\'s blog:\\n\\n\\n\\nVCE Version\\nProduct Family\\nDistinguishing Features\\n\\n\\n\\n\\nVCE 1.0\\nRadeon HD 7900 series/Radeon R9 280X dGPU\\nFirst release: AVC – I,P and DEM\\n\\n\\n\\nRadeon HD 7800 series dGPU\\n\\n\\n\\n\\nRadeon R9 270X/270 dGPU\\n\\n\\n\\n\\nRadeon HD 7700 series/Radeon R7 250X dGPU\\n\\n\\n\\n\\nA10 – 58XX (and other variations) APU\\n\\n\\n\\n\\nA10 – 68XX APU\\n\\n\\n\\n\\n\\n\\n\\n\\nVCE 2.0\\nRadeon R9 390x/390/290x/290 dGPU\\nSVC (temporal) + B-pictures + DEM improvements\\n\\n\\n\\nRadeon R7 260X/260 dGPU\\n\\n\\n\\n\\nA10 – 7850K APU\\n\\n\\n\\n\\nA4-5350, A4-3850, or E1-2650 APU\\n\\n\\n\\n\\nA4-1200/A6-1450 APU\\n\\n\\n\\n\\n\\n\\n\\n\\nVCE 3.0\\nRadeon R9 Fury/285 dGPU\\n4K\\n\\n\\n\\nInstalling\\n\\nUnpack the archive somewhere, right click on install.bat and click Run as Administrator.\\n\\nIf it complains about missing files, try the more manual version:\\n\\nUnpack the archive somewhere, open command prompt as administrator by typing cmd to start menu or \"Metro\" and press SHIFT+CTRL+Enter or right click on the icon and click Run as Administrator.\\nGo to unpacked folder by typing into opened command prompt cd some\\\\where\\\\OpenEncodeVFW-bin.\\nType install.bat and press enter to run the installer.\\n\\nUninstalling\\nIf uninstaller fails its job, manually remove these registry keys:\\nHKLM\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\MediaResources\\\\icm\\\\VIDC.H264\\nHKLM\\\\Software\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\\\\drivers.desc\\\\OPENENCODEVFW.DLL\\nHKLM\\\\Software\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\\\\Drivers32\\\\VIDC.H264\\nHKLM\\\\Software\\\\Wow6432Node\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\\\\drivers.desc\\\\OPENENCODEVFW.DLL\\nHKLM\\\\Software\\\\Wow6432Node\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\\\\Drivers32\\\\VIDC.H264\\n\\nand OPENENCODEVFW.DLL in %WINDIR%\\\\syswow64 or %WINDIR%\\\\system32\\nRecommended usage\\n\\n32 bit input format\\nwidth/height multiples of 2\\n\\nSome setting descriptions\\n\\nFixed QP basically keeps picture quality constant across all frames.\\nCBR keeps constant bitrate so picture quality gets worse if there is frequently fast motion in video and bitrate is too low or wastes harddrive space if frame could have been compressed more. Seems to fluctuate too much though.\\nVBR uses variable bitrate, tries to keep in target bitrate but rises bitrate a little bit if needed or lowers if frame can be compressed more.\\nCABAC is more efficient and resource intensive encoding option.\\nSearch range is motion vector range. Specifies how wide the codec looks for moved pixels so it can just say that these pixels moved to x,y and just save that. Higher (max 36?) is better and more resource intensive encoding option.\\nProfiles / levels: start from http://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles . Colorspace is limited to Y\\'UV420.\\n\\nProbably not very accurate descriptions :P\\nAlso:\\n\\nSend FPS sets encoder framerate properties to video framerate, but not all framerates are supported by encoder. Untick to treat all videos as having 30 fps, but this may make encoding inefficient and increase bitrate more than necessary.\\nSpeedy Math tries to speed up OpenCL floating point math by making it less accurate, but should be good enough.\\nSwitch byte order : for the rare case when input bitmap is RGB(A) instead of BGR(A).\\nHeader insertion : adds SPS/PPS to every frame, may make cutting/splitting video easier. More of a \\'debug\\' feature.\\n\\nQuickset buttons for speed vs quality:\\n\\nSpeed : encodes 1080p at 60+ fps (theoretical max 80+)\\nBalanced : encodes 1080p at 40+ fps\\nQuality : encodes 1080p at 30+ fps (can probably do 720p@60)\\n\\nWith newer AMD cards (hawaii+), seem to support B-frames, though VCE may not actually generate B-frames with OpenVideo, and AVI kinda sucks with these (see). You may need to remux to MKV/MP4 for better audio/video sync.\\n(Also maybe ffmpeg -fflags +genpts)\\n',\n",
       "  'watchers': '18',\n",
       "  'stars': '56',\n",
       "  'forks': '11',\n",
       "  'commits': '132'},\n",
       " {'language': 'C++ 85.5',\n",
       "  'readme': 'OctaForge\\nTHIS REPOSITORY IS ARCHIVED AND DEPRECATED.\\nDevelopment continues on https://git.octaforge.org.\\nFor installation, read INSTALL.md.\\nRunning depends on the platform.\\nOn Unix-like systems (OS X only when compiled with the provided Makefile), run\\nit using bin_unix/client_YOUROS_YOURARCH from either the root directory or\\nbin_unix.\\nOn OS X, you can use the way above and you can also run it from within the\\nXcode project (development builds) or you can run the generated app bundle\\nafter deploying (the .dmg file also contains gamedata and is ready to ship).\\nOn Windows, execute bin_winYOURARCH\\\\client_win_YOURARCH.exe either from\\nthe root directory or from the directory with the binary.\\nSamee goes for server, just change \"client\" to \"server\". Using the -d2 option\\nto client binary you can get the same dedicated server too (-d1 will start a\\nlistenserver).\\nIn case of problems, delete contents of your OF home directory\\n($HOME/.octaforge on Unix-like operating systems,\\nDocuments\\\\My Games\\\\OctaForge on Windows)\\nThe variable \"game\" influences what gamescript will be run. In local sessions\\nit\\'s used directly, but when connecting to a server is involved, the server\\nwill send the gamescript name to the client (on the server the same variable\\nis used to specify it; modify server-init.cfg appropriately).\\nExample:\\n/game drawing.basic\\n\\nThis results in media/scripts/gamescripts/drawing/basic.oct being run.\\nThere is just one map, \"test\", bundled by default. Use /newmap to create a\\nnew map, just like in Tesseract.\\nIf a problem persists, report it into our\\nissue tracker: https://github.com/OctaForge/OF-Engine/issues\\nhttp://octaforge.org/\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '55',\n",
       "  'forks': '11',\n",
       "  'commits': '2,720'},\n",
       " {'language': 'C++ 42.5',\n",
       "  'readme': 'OctaForge\\nTHIS REPOSITORY IS ARCHIVED AND DEPRECATED.\\nDevelopment continues on https://git.octaforge.org.\\nFor installation, read INSTALL.md.\\nRunning depends on the platform.\\nOn Unix-like systems (OS X only when compiled with the provided Makefile), run\\nit using bin_unix/client_YOUROS_YOURARCH from either the root directory or\\nbin_unix.\\nOn OS X, you can use the way above and you can also run it from within the\\nXcode project (development builds) or you can run the generated app bundle\\nafter deploying (the .dmg file also contains gamedata and is ready to ship).\\nOn Windows, execute bin_winYOURARCH\\\\client_win_YOURARCH.exe either from\\nthe root directory or from the directory with the binary.\\nSamee goes for server, just change \"client\" to \"server\". Using the -d2 option\\nto client binary you can get the same dedicated server too (-d1 will start a\\nlistenserver).\\nIn case of problems, delete contents of your OF home directory\\n($HOME/.octaforge on Unix-like operating systems,\\nDocuments\\\\My Games\\\\OctaForge on Windows)\\nThe variable \"game\" influences what gamescript will be run. In local sessions\\nit\\'s used directly, but when connecting to a server is involved, the server\\nwill send the gamescript name to the client (on the server the same variable\\nis used to specify it; modify server-init.cfg appropriately).\\nExample:\\n/game drawing.basic\\n\\nThis results in media/scripts/gamescripts/drawing/basic.oct being run.\\nThere is just one map, \"test\", bundled by default. Use /newmap to create a\\nnew map, just like in Tesseract.\\nIf a problem persists, report it into our\\nissue tracker: https://github.com/OctaForge/OF-Engine/issues\\nhttp://octaforge.org/\\n',\n",
       "  'watchers': '99',\n",
       "  'stars': '55',\n",
       "  'forks': '10',\n",
       "  'commits': '1,372'},\n",
       " {'language': 'C++ 98.0',\n",
       "  'readme': 'rdis\\nC++ code for the RDIS algorithm from \"Recursive Decomposition for Nonconvex Optimization.\" Friesen and Domingos, IJCAI 2015.\\nMain platform this was tested on: Mac OS X 10.9 64 bit\\nOther platforms this hopefully works on: Mac OS X 32 bit, Linux 32 bit & 64 bit\\nBinaries are included for Mac OS X 64 bit (built on 10.9 Yosemite) in rdis/bin/macosx64, otherwise you will have to build them (see below).\\nTo build (if all dependencies are accessible from default system paths), simply call the following:\\ncmake .\\nmake -j8\\nRDIS builds in <BASE_DIR>/build and there should be three executables: testRDIS, optBA, and optSinusoid.\\n\\ntestRDIS: run the debug tests (if no parameters are specified on the command line), or optimize a polynomial specified in a file.\\ne.g., run ./build/testRDIS\\ne.g., run ./build/testRDIS -f data/testpoly.txt\\n\\nsee the notes in data/testpoly.txt for writing your own polynomial for RDIS to optimize\\n\\n\\noptBA: optimize a bundle adjustment function;\\ne.g., run ./build/optBA -f data/ladybug-problem-49-7776-pre.txt --ncams 5 --npts 30\\noptSinusoid: optimize a high-dimensional sinusoid\\ne.g., run ./build/optSinusoid\\ncalling ./build/<executable> --help will print the command line options\\n\\nDependencies:\\n\\nNOTE: cmake vars EXT_INCL_DIR and EXT_LIB_DIR specify paths in which the build should look for the following libraries (if they\\'re enabled) except for boost, instead of specifying each one for each library.\\nBoost 1.55.0 (required, not included) (or potentially later, but 1.55.0 is tested) from http://www.boost.org/\\n\\nRequired boost libs: system, filesystem, program_options, and chrono.\\nYou must build these yourself. If the paths to the boost include folder and libs are non-standard (e.g., not in /usr/local/include or /usr/include), then you can specify the paths when calling cmake as cmake -D BOOST_ROOT=/path/to/boost . or you can specify BOOST_INCLUDEDIR and BOOST_LIBRARYDIR separately.\\n\\nBuilding these is easy; simply follow the Getting Started instructions at www.boost.org.\\n\\n\\n\\n\\nPaToH (not required, included, used by default) hypergraph partitioning library from http://bmi.osu.edu/umit/software.html.\\n\\nPre-built libs are included in the repo already, so this should (ideally) just work.\\nCan be disabled when calling cmake with cmake -D USE_PATOH=FALSE .\\nYou can change the search path by setting the cmake variable PATOH_DIR.\\n\\n\\nHMetis (not required, not included) hypergraph partitioning library from http://glaros.dtc.umn.edu/gkhome/metis/hmetis/overview.\\n\\nNothing is included, but once you download it you can specify the path in cmake variable HMETIS_DIR.\\n\\n\\nlevmar (not required, not included) Levenberg-Marquardt optimizer from http://users.ics.forth.gr/~lourakis/levmar/.\\n\\nNothing is included, but once you download it you can specify the path in cmake variable LEVMAR_DIR.\\nNote that this library requires that LAPACK (and BLAS) are installed and work on your system.\\n\\n\\n\\nKnown Issues:\\n\\nIf the minimum is on the border of the domain, RDIS will do poorly because it does not handle constraints properly (a non-infinite domain constitutes box constraints on the function).\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '55',\n",
       "  'forks': '18',\n",
       "  'commits': '17'},\n",
       " {'language': 'C++ 60.2',\n",
       "  'readme': 'rdis\\nC++ code for the RDIS algorithm from \"Recursive Decomposition for Nonconvex Optimization.\" Friesen and Domingos, IJCAI 2015.\\nMain platform this was tested on: Mac OS X 10.9 64 bit\\nOther platforms this hopefully works on: Mac OS X 32 bit, Linux 32 bit & 64 bit\\nBinaries are included for Mac OS X 64 bit (built on 10.9 Yosemite) in rdis/bin/macosx64, otherwise you will have to build them (see below).\\nTo build (if all dependencies are accessible from default system paths), simply call the following:\\ncmake .\\nmake -j8\\nRDIS builds in <BASE_DIR>/build and there should be three executables: testRDIS, optBA, and optSinusoid.\\n\\ntestRDIS: run the debug tests (if no parameters are specified on the command line), or optimize a polynomial specified in a file.\\ne.g., run ./build/testRDIS\\ne.g., run ./build/testRDIS -f data/testpoly.txt\\n\\nsee the notes in data/testpoly.txt for writing your own polynomial for RDIS to optimize\\n\\n\\noptBA: optimize a bundle adjustment function;\\ne.g., run ./build/optBA -f data/ladybug-problem-49-7776-pre.txt --ncams 5 --npts 30\\noptSinusoid: optimize a high-dimensional sinusoid\\ne.g., run ./build/optSinusoid\\ncalling ./build/<executable> --help will print the command line options\\n\\nDependencies:\\n\\nNOTE: cmake vars EXT_INCL_DIR and EXT_LIB_DIR specify paths in which the build should look for the following libraries (if they\\'re enabled) except for boost, instead of specifying each one for each library.\\nBoost 1.55.0 (required, not included) (or potentially later, but 1.55.0 is tested) from http://www.boost.org/\\n\\nRequired boost libs: system, filesystem, program_options, and chrono.\\nYou must build these yourself. If the paths to the boost include folder and libs are non-standard (e.g., not in /usr/local/include or /usr/include), then you can specify the paths when calling cmake as cmake -D BOOST_ROOT=/path/to/boost . or you can specify BOOST_INCLUDEDIR and BOOST_LIBRARYDIR separately.\\n\\nBuilding these is easy; simply follow the Getting Started instructions at www.boost.org.\\n\\n\\n\\n\\nPaToH (not required, included, used by default) hypergraph partitioning library from http://bmi.osu.edu/umit/software.html.\\n\\nPre-built libs are included in the repo already, so this should (ideally) just work.\\nCan be disabled when calling cmake with cmake -D USE_PATOH=FALSE .\\nYou can change the search path by setting the cmake variable PATOH_DIR.\\n\\n\\nHMetis (not required, not included) hypergraph partitioning library from http://glaros.dtc.umn.edu/gkhome/metis/hmetis/overview.\\n\\nNothing is included, but once you download it you can specify the path in cmake variable HMETIS_DIR.\\n\\n\\nlevmar (not required, not included) Levenberg-Marquardt optimizer from http://users.ics.forth.gr/~lourakis/levmar/.\\n\\nNothing is included, but once you download it you can specify the path in cmake variable LEVMAR_DIR.\\nNote that this library requires that LAPACK (and BLAS) are installed and work on your system.\\n\\n\\n\\nKnown Issues:\\n\\nIf the minimum is on the border of the domain, RDIS will do poorly because it does not handle constraints properly (a non-infinite domain constitutes box constraints on the function).\\n\\n',\n",
       "  'watchers': '24',\n",
       "  'stars': '54',\n",
       "  'forks': '33',\n",
       "  'commits': '1,075'},\n",
       " {'language': 'C++ 89.7',\n",
       "  'readme': 'Purpose\\nExpose opencv to the node environment.\\nFeatures\\n- cv::Point, cv::Size, cv::Rect, etc replaced by object notation:\\n\\tPoint -> {x: 0, y:0}, Size -> {width: 33, height: 33}\\n\\n- Checks the types of parameters as well as ranges of values on each native call (harder to crash app from script)\\n- Friendly exception messages on invalid parameters \\n\\nExample\\nSee scripts/effects.coffee for a full demo.\\nInstallation - MacOS X\\n$ brew install opencv --build32\\n$ npm install -g opencv-node\\n$ coffee scripts/tests\\n\\n(tested with GCC 4.2.1 and node 0.8.0)\\nTroubleshooting\\nIf brew complains \"SHA1 mismatch\" error you may find that updating homebrew fixes the issue:\\n$ brew update\\nIf brew complains \"No available formula\" you will need to tap the science repository:\\n$ brew tap homebrew/science\\nInstallation - Windows\\n\\n\\nDownload OpenCV from http://sourceforge.net/projects/opencvlibrary/files/latest/download\\n\\n\\nExtract it to a folder, eg C:\\\\OpenCV\\n\\n\\nOpen a Visual Studio command prompt and type\\n set OPENCV_ROOT=C:/OpenCV\\n npm install -g opencv-node\\n\\n\\n\\n(tested with Visual Studio 2010 and node 0.8.8)\\nAPI Differences\\nSome functions have a more js-friendly API/syntax.\\nThe void functions which return their output in an argument passed by reference, return the result directly:\\ncv::split returns an Array and takes only 1 argument\\ncv::HoughCircles returns an Array\\ncv::HoughLines returns an Array\\ncv::cornerSubPix returns an Array\\n\\n* others?\\n\\nLicense\\nBSD\\nDisclaimer\\nPlease report any bugs or missing functions. This module has never been used in production and is generally\\nmeant to be used for experimentation.\\n',\n",
       "  'watchers': '7',\n",
       "  'stars': '54',\n",
       "  'forks': '8',\n",
       "  'commits': '48'},\n",
       " {'language': 'C++ 96.8',\n",
       "  'readme': 'Purpose\\nExpose opencv to the node environment.\\nFeatures\\n- cv::Point, cv::Size, cv::Rect, etc replaced by object notation:\\n\\tPoint -> {x: 0, y:0}, Size -> {width: 33, height: 33}\\n\\n- Checks the types of parameters as well as ranges of values on each native call (harder to crash app from script)\\n- Friendly exception messages on invalid parameters \\n\\nExample\\nSee scripts/effects.coffee for a full demo.\\nInstallation - MacOS X\\n$ brew install opencv --build32\\n$ npm install -g opencv-node\\n$ coffee scripts/tests\\n\\n(tested with GCC 4.2.1 and node 0.8.0)\\nTroubleshooting\\nIf brew complains \"SHA1 mismatch\" error you may find that updating homebrew fixes the issue:\\n$ brew update\\nIf brew complains \"No available formula\" you will need to tap the science repository:\\n$ brew tap homebrew/science\\nInstallation - Windows\\n\\n\\nDownload OpenCV from http://sourceforge.net/projects/opencvlibrary/files/latest/download\\n\\n\\nExtract it to a folder, eg C:\\\\OpenCV\\n\\n\\nOpen a Visual Studio command prompt and type\\n set OPENCV_ROOT=C:/OpenCV\\n npm install -g opencv-node\\n\\n\\n\\n(tested with Visual Studio 2010 and node 0.8.8)\\nAPI Differences\\nSome functions have a more js-friendly API/syntax.\\nThe void functions which return their output in an argument passed by reference, return the result directly:\\ncv::split returns an Array and takes only 1 argument\\ncv::HoughCircles returns an Array\\ncv::HoughLines returns an Array\\ncv::cornerSubPix returns an Array\\n\\n* others?\\n\\nLicense\\nBSD\\nDisclaimer\\nPlease report any bugs or missing functions. This module has never been used in production and is generally\\nmeant to be used for experimentation.\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '54',\n",
       "  'forks': '16',\n",
       "  'commits': '596'},\n",
       " {'language': 'C++ 69.5',\n",
       "  'readme': 'cocos2dx-extensions\\nExtensions for Cocos2dx:\\n1.Dynamic CCLabelTTF - 动态显示数字\\n2.Gray Sprite - 创建灰色图\\n3.Turn Card - 翻牌效果\\n4.Zoom Controller - 场景多点聚焦缩放\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '54',\n",
       "  'forks': '39',\n",
       "  'commits': '13'},\n",
       " {'language': 'C++ 84.6',\n",
       "  'readme': 'Motorola SMI\\nXT890|Scorpion Mini Intel|Razr i\\nMotorola Razr i Intel-atom\\nSource from Turl, Oxavelar, Motorola, Intel, OPENSuse, Google(android) and HazouPH\\n',\n",
       "  'watchers': '25',\n",
       "  'stars': '53',\n",
       "  'forks': '23',\n",
       "  'commits': '611'},\n",
       " {'language': 'C++ 64.2',\n",
       "  'readme': 'This repository is part of the codebender.cc maker and artist web platform.\\nAnd what\\'s that?\\ncodebender comes to fill the need for reliable and easy to use tools for makers. A need that from our own experience could not be totally fulfilled by any of the existing solutions. Things like installing libraries, updating the software or installing the IDE can be quite a painful process.\\nIn addition to the above, the limited features provided (e.g. insufficient highlighting, indentation and autocompletion) got us starting building codebender, a completely web-based IDE, that requires no installation and offers a great code editor. It also stores your sketches on the cloud.\\nThat way, you can still access your sketches safely even if your laptop is stolen or your hard drive fails! codebender also takes care of compilation, giving you extremely descriptive warnings on terrible code. On top of that, when you are done, you can upload your code to your Arduino straight from the browser without installing anything.\\nCurrently codebender.cc is running its beta and we are trying to fix issues that may (will) come up so that we can launch and offer our services to everyone!\\nIf you like what we do you can also support our campaign on indiegogo to also get early access to codebender!\\nHow do these \"arduino-files\" come into the picture?\\ncodebender.cc runs on PHP Fog, a PaaS that helps us run our PHP projects in a fast and scallable manner. However, we need to run our compiler on some VPS.\\nThe compiler repository includes all the necessary files needed to run the compiler as a service. This includes the arduino examples, libraries, etc. This is a repository that includes all of these necessary files, which the compiler repository then includes as a submodule.\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '53',\n",
       "  'forks': '306',\n",
       "  'commits': '30'},\n",
       " {'language': 'JavaScript 61.4',\n",
       "  'readme': \"adventures-reactive-web-dev\\nAdventures in Reactive Web Development.\\nExploring various libraries, frameworks, and techniques for reactive web development, including:\\n\\nAngular 2\\nCycle.js\\nElm\\nReact\\n\\nwith Redux\\nwith RxJS\\n\\n\\nYolk\\n\\nAlso part of the adventure:\\n\\nsnabbdom\\nmost and most-subject\\n\\nRequirements\\nYou need to have Node.js installed to run the examples.\\nThere are several ways to install Node.js, including Homebrew and nvm.\\nOne-time setup\\nYou only need to run this command once:\\nnpm i -g bower\\n\\nThis installs Bower. It is used to install the client-side CSS used in the examples - namely, Bootstrap. Technically, it's not required to run the examples, but it does make them look nicer.\\nThe same server is used for every example. You only need to run this once:\\n# from the top-level directory\\nnpm i\\nbower i\\n\\nOne-time setup per example\\nFor each example that you wish to run, you also need to run these commands once:\\n# from the top-level directory\\ncd client-<example> # replace with directory of the example you want to run\\nnpm i\\n\\nRunning the example\\nTo run an example, open two terminal windows.\\nIn the first terminal window:\\n# from the top-level directory\\nnpm start\\n\\nIn the second terminal window:\\n# from the top-level directory\\ncd client-<example> # replace with directory of the example you want to run\\nnpm run dev\\n\\nViewing the example and making changes\\nOpen http://localhost:3013 to view the example.\\nYou can experiment with making changes by editing the code under the client-<example> directory. You only have to save the file to see your changes. Recompilation and browser page reload happen automatically.\\nBranches\\nThe master branch contains the most up-to-date code. However, you may be interested in other branches.\\nSimply run git checkout <branchname> to check out a branch, replacing <branchname> with one of the following:\\n\\nng2-webpack : Angular 2 example using RxJS instead of Redux\\nng2-systemjs : Angular 2 example using SystemJS instead of Webpack\\nreact-router : React example using React-Router\\nclient-elm branches : there are 4 branches for the\\n4-part article\\nabout the Elm example.\\n\\nFinally, please note that the examples use Webpack. I left the ng2-systemjs branch there for reference, but I had a lot of trouble using SystemJS and I did not like having to add a bunch of <script> tags. I prefer the Webpack setup, which includes automatic recompilation and page refresh, so once I had that set up and working, I went with it for the rest of the examples.\\n\",\n",
       "  'watchers': '6',\n",
       "  'stars': '152',\n",
       "  'forks': '5',\n",
       "  'commits': '270'},\n",
       " {'language': 'JavaScript 92.3',\n",
       "  'readme': 'Radial Bubble Tree\\n![bubble tree screenshot](http://driven-by-data.net/wp/wp-content/uploads/2011/06/Bildschirmfoto-2011-07-02-um-23.02.50.png bubbletree screenshot right)\\nBubbleTree is a library for interactive visualization of hierarchical data. Originally developed mainly for spending data, the library is now completely independent from the OpenSpending platform. BubbleTree is built on top of jQuery and RaphaelJS.\\nDocumentation\\nPlease refer to the docs in the wiki pages.\\nCopyright and License\\nCopyright (c) 2011,2012 Open Knowledge Foundation\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nAttribution Request\\nIf you use Bubbletree to make a visualization we\\'d love it (but it\\'s not required!) if you added a small credit text and link e.g.\\nBuilt with <a href=\"http://okfn.org/\">Open Knowledge Foundation\\'s</a> <a href=\"https://github.com/okfn/bubbletree\">Bubbletree library</a>\\n\\nAuthors\\nGregor Aisch with a very small amount of input from Rufus Pollock as part of work on the OpenSpending project and with financial support from Publish What You Fund and the Shuttleworth Foundation.\\nUseful commands\\n\\nnpm run build to rebuild library and demos\\nnpm run review to check code style\\n\\nInstallation\\nnpm install bubbletree\\nor\\nbower install bubbletree\\n',\n",
       "  'watchers': '35',\n",
       "  'stars': '152',\n",
       "  'forks': '72',\n",
       "  'commits': '185'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': \"Yeoman Generator for jQuery Boilerplate \\n\\nA Yeoman generator that provides a functional boilerplate to easily create jQuery plugins out of the box.\\n\\nUsage\\n\\nInstall the generator by running: npm install -g generator-jquery-boilerplate\\nRun: yo jquery-boilerplate\\nStart writing your jQuery plugin :)\\n\\nWanna know more?\\nCheck jQuery Boilerplate's repository.\\nHistory\\nCheck Releases for detailed changelog.\\nLicense\\nMIT License © Zeno Rocha\\n\",\n",
       "  'watchers': '5',\n",
       "  'stars': '152',\n",
       "  'forks': '28',\n",
       "  'commits': '68'},\n",
       " {'language': 'JavaScript 53.3',\n",
       "  'readme': 'see the homepage\\nEasy WebSocket\\nlike WebSocket but no server setup and available in any browser\\nEasyWebSocket aims to make realtimes webapps in every browser without the trouble\\nto setup Websocket servers.\\nHow to use it\\nInclude the following in your webpage and it just works.\\n<script src=\"http://EasyWebsocket.org/easyWebSocket.min.js\"></script>\\t\\n<script>\\n    var socket = new EasyWebSocket(\"ws://example.com/resource\");\\n    socket.onopen\\t= function(){\\n        socket.send(\"hello world.\")\\n    }\\n    socket.onmessage= function(event){\\n        alert(\"received\"+ event.data)\\n    }\\n</script>\\n\\nStep 1: You connect the socket to a given url\\nStep 2: What you send() thru this socket is sent to all sockets connected the same url\\nSee this code live. No server\\nsetup, no cross-origin issue to care about... It is that easy!\\n',\n",
       "  'watchers': '6',\n",
       "  'stars': '152',\n",
       "  'forks': '24',\n",
       "  'commits': '121'},\n",
       " {'language': 'JavaScript 99.6',\n",
       "  'readme': 'AngularJS - Animation Article\\nRun with\\n# run this from the root of the repo\\nnpm install http-server\\nhttp-server -p 8000 app/\\nYou can then access the website via:\\nhttp://localhost:8000/\\nBlog Article\\nThis article will teach you everything you need to know:\\nhttp://www.yearofmoo.com/2013/04/animation-in-angularjs.html\\nDemo\\nThe application can be demoed via:\\nhttp://yearofmoo-articles.github.com/angularjs-animation-article/app/\\n',\n",
       "  'watchers': '14',\n",
       "  'stars': '151',\n",
       "  'forks': '61',\n",
       "  'commits': '11'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'grunt-protractor-runner\\n\\n\\nA Grunt plugin for running Protractor runner.\\n\\nGetting Started\\nThis plugin requires Grunt >=0.4.1.\\nFor Protractor 5.x.x, please use version v5.x.x of this plugin.\\nFor Protractor 4.x.x, please use version v4.x.x of this plugin.\\nFor Protractor 3.x.x, please use version v3.x.x of this plugin.\\nFor Protractor 2.x.x, please use version v2.x.x of this plugin.\\nIf you haven\\'t used Grunt before, be sure to check out the Getting Started guide, as it explains how to create a Gruntfile as well as install and use Grunt plugins. Once you\\'re familiar with that process, you may install this plugin with this command:\\nnpm install grunt-protractor-runner --save-dev\\nThis plugin will install protractor module locally as a normal dependency.\\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\\ngrunt.loadNpmTasks(\\'grunt-protractor-runner\\');\\nFinally you need a Selenium server. If you don\\'t have one set up already, you can install a local standalone version with this command:\\n./node_modules/grunt-protractor-runner/scripts/webdriver-manager-update\\nThe \"protractor\" task\\nOverview\\nIn your project\\'s Gruntfile, add a section named protractor to the data object passed into grunt.initConfig().\\ngrunt.initConfig({\\n  protractor: {\\n    options: {\\n      configFile: \"node_modules/protractor/example/conf.js\", // Default config file\\n      keepAlive: true, // If false, the grunt process stops when the test fails.\\n      noColor: false, // If true, protractor will not use colors in its output.\\n      args: {\\n        // Arguments passed to the command\\n      }\\n    },\\n    your_target: {   // Grunt requires at least one target to run so you can simply put \\'all: {}\\' here too.\\n      options: {\\n        configFile: \"e2e.conf.js\", // Target-specific config file\\n        args: {} // Target-specific arguments\\n      }\\n    },\\n  },\\n})\\nOptions\\noptions.configFile\\nType: String\\nDefault value: No default value\\nA protractor config file.\\noptions.keepAlive\\nType: Boolean\\nDefault value: false (true before v1.0.0)\\nIf true, grunt process continues even if the test fails. This option is useful when using with grunt watch.\\nIf false, grunt process stops when the test fails.\\noptions.noColor\\nType: Boolean\\nDefault value: false\\nIf true, protractor will not give colored output.\\nIf false, protractor will give colored output, as it does by default.\\noptions.debug\\nType: Boolean\\nDefault value: false\\nIf true, grunt will pass \\'debug\\' as second argument to protractor CLI to enable node CLI debugging as described in Protractor Debugging documentation.\\noptions.args\\nType: Object\\nDefault value: {}\\nArguments passed to the command. These arguments can also be supplied via command-line too. Ex.grunt protractor --specs=specs/some-test.js  or for object options grunt protractor --cucumberOpts={\\\\\"tags\\\\\":\\\\\"@quick\\\\\"} or --params=\\'{ \"location\" : { \"href\" : \"some url\" } }\\'\\nPassing object argument with --params.xxx.yyy=zzz is not supported at the moment. If you need this behaviour, please join the discussion in #148 .\\nSupported arguments are below.\\n\\nseleniumAddress string: A running selenium address to use\\nseleniumServerJar string: Location of the standalone selenium server .jar file\\nseleniumPort string: Optional port for the standalone selenium server\\nbaseUrl string: URL to prepend to all relative paths\\nrootElement string: Element housing ng-app, if not html or body\\nspecs array: Array of spec files to test. Ex. [\"spec1.js\",\"spec2.js\"]\\nexclude array: Array of files to exclude from testing. Ex. [\"spec2.js\"]\\nsuite string or array: Suite or Array of suites to run. Ex. [\"suite1\", \"suite2\"]\\nincludeStackTrace boolean: Print stack trace on error\\nverbose boolean: Print full spec names\\nbrowser string: Browser name, e.g. chrome or firefox\\nparams object: Param object to be passed to the test as browser.params\\nchromeDriver string: Location of chrome driver overridng the property in config file\\ndirectConnect boolean: To connect directly to the browser Drivers. This option is only available for Firefox and Chrome.\\nsauceUser string: Username for a SauceLabs account\\nsauceKey string: Access Key for a SauceLabs account\\nsauceSeleniumAddress string: Customize the URL Protractor uses to connect to sauce labs (for example, if you are tunneling selenium traffic through a sauce connect tunnel). Default is ondemand.saucelabs.com:80/wd/hub\\ncapabilities object: Capabilities object to be passed to the test, e.g. browserName, platform and version\\nframework string: Limited support for using mocha as the test framework instead of jasmine.\\nframeworkPath string: When framework is set to custom, set this path relative to the config file or absolute\\ncucumberOpts object: Cucumber framework options object to be passed to the test, e.g. require, tags and format\\nmochaOpts object: Mocha test framework options object to be passed\\nbeforeLaunch string: You can specify a file containing code to run once configs are read but before any environment setup. This will only run once, and before onPrepare.\\nonPrepare string: You can specify a file containing code to run once protractor is ready and available, and before the specs are executed. If multiple capabilities are being run, this will run once per capability.\\nwebDriverProxy string: WebDriver proxy configuration to run remote tests\\n\\noptions.output\\nType: String\\nDefault value: false\\nThe file that the task should output the results to.\\noptions.outputOptions\\nType: Object\\nDefault value: {}\\nOptions for output file. For details see: fs.createWriteStream\\'s options\\noptions.nodeBin\\nType: String\\nDefault value: node\\nPath to the node binary file. Useful if node is not on the PATH.\\noptions.webdriverManagerUpdate\\nType: Boolean\\nDefault value: false\\nIf true, webdriver-manager update will run and install/update selenium driver.\\nTests\\nRun npm install to install dependencies.\\nThen run grunt or npm test to test the module. You will encounter these.\\n\\nRuns unit and e2e tests\\nIt opens chrome a couple of times without warnings or errors.\\nA test task fails but the test process keeps alive and continues to the next test tasks.\\n\\nContributing\\nIn lieu of a formal styleguide, take care to maintain the existing coding style. Add unit tests for any new or changed functionality. Lint and test your code using Grunt.\\nFAQ\\nQ: Want to global installed protractor?\\nThis plugin installs protractor module locally as a normal dependency.\\nIn case you want to use the plugin with the global installed protractor command. You can do it with these steps below.\\n\\nRemove local install protractor by rm -rf node_modules/protractor\\nInstall protractor globally  with npm install -g protractor\\nMake sure that node can resolve the module with require() mechanism. See Module loading from the global folders for more information.\\nRun webdriver-manager update to install/update selenium driver for global install protractor.\\n\\nQ: Error: Could not find chromedriver at....\\nYou need to install/update selenium webdriver for protractor.\\n\\nRun webdriver-manager update or node scripts/webdriver-manager-update or node ./node_modules/protractor/bin/webdriver-manager update\\n\\nRelease History\\n\\n\\n5.0.0\\n\\nUpgrade protractor to version 5 (#185)\\n\\n\\n\\n4.0.0\\n\\nAccept array for suite argument (#172)\\nUpgrade protractor to version 4 (#168)\\n\\n\\n\\n3.2.0\\n\\nSupport --frameworkPath in options.args (#155, #156)\\nSupport grunt version >=0.4.0\" (#154)\\n\\n\\n\\n3.1.0\\n\\nAdd options.outputOptions (#143)\\nSupport webDriverProxy in options.args (#147)\\nRemove referenceConf.js as default value of options.configFile because it does not exist anymore\\n\\n\\n\\n3.0.0\\n\\nUpdate protractor to version 3\\nUpdate other dependencies including through2 and split to latest version\\n\\n\\n\\n2.1.2\\n\\nFix boolean parameters in object.args.params (#130)\\nModify unit tests to run nodeunit test faster and after protractor task\\n\\n\\n\\n2.1.1\\n\\nFix EINVAL error when run in git bash shell (#134)\\n\\n\\n\\n2.1.0\\n\\nAdd options.webdriverManagerUpdate option (#125)\\nFix support for object option via command-line (#116)\\n\\n\\n\\n2.0.0\\n\\nUpgrade protractor to ^2.0.0 (#114)\\nchromeOnly in options.args is deprecated. Replaced by directConnect (#114)\\nSupport beforeLaunch and onPrepare in options.args (#110)\\nWhen one of the tests fails, throw warning instead of fatal error so that grunt can still use --force to continue. (#103)\\n\\n\\n\\n1.2.1\\n\\nMove split and through2 from devDependencies to dependencies (#104)\\n\\n\\n\\n1.2.0\\n\\nAdd options.nodeBin to specify node binary (#96)\\nSupport --directConnect and --sauceSeleniumAddress in options.args (#95, #101)\\nAdd options.output (#80)\\nMerge README.md PRs (#89, #91)\\nFix plugin test for protractor>=v1.5.0\\nFix TravisCI test\\n\\n\\n\\n1.1.4\\n\\nMove webdriver-manager update step from problematic postinstall to pretest\\n\\n\\n\\n1.1.3\\n\\nAttempt to fix webdriver-manager postinstall problem with webdriver-manager script (#83)\\n\\n\\n\\n1.1.2\\n\\nAttempt to fix webdriver-manager path in package.json postinstall\\nAdd Travis CI build configuration\\n\\n\\n\\n1.1.1\\n\\nRun webdriver-manager update on postinstall (#41)\\n\\n\\n\\n1.1.0\\n\\nUpdate protractor to version 1.x.x\\n\\n\\n\\n1.0.1\\n\\nPass specified command line params to the subprocess (#68)\\nMake npm test to run and handle interactive debugger by itself (#66)\\nFixed argsTest\\n\\n\\n\\n1.0.0\\n\\nChange default value of options.keepAlive to false (#50)\\n\\n\\n\\n0.2.5\\n\\nSupport --mochaOpts, --suite and --exclude in options.args (#52, #53, #57)\\n\\n\\n\\n0.2.4\\n\\nSupport --cucumberOpts in options.args (#46)\\n\\n\\n\\n0.2.3\\n\\nTemporarily remove automatically download/update webdriver-manager because it fails in some environment such as Windows (#41)\\n\\n\\n\\n0.2.2\\n\\nAdd protractor module as a normal dependency and automatically download/update webdriver with webdriver-manager after installed (#29, #39)\\nSupport --framework in options.args (#36)\\n\\n\\n\\n0.2.1\\n\\nSupport --capabilities in options.args (#33)\\n\\n\\n\\n0.2.0\\n\\nAble to use either local or global install protractor the same way as how require() function works (#29)\\nMove protractor from peerDependencies to devDependencies. These changes might break some user modules. (See FAQ above for explanation) (#29)\\n\\n\\n\\n0.1.11 - Support SauceLabs account config in options.args (#27)\\n\\n\\n0.1.10\\n\\nSupport --chromeOnly in options.args (#23)\\nSupport options.noColor to turn color off in protractor output (#24)\\n\\n\\n\\n0.1.9\\n\\nAble to supply options.args via command-line arguments (#20)\\nFixed merging task-level and target-level options\\n\\n\\n\\n0.1.8 - Support --chromeDriver in options.args (#17)\\n\\n\\n0.1.7 - Support --browser and --params arguments passed to the protractor command using config in options.args (#12)\\n\\n\\n0.1.6 - Change protractor(peerDependencies) to support version to 0.x (#8, #9, #10)\\n\\n\\n0.1.5 - Added options.debug (#7)\\n\\n\\n0.1.4 - Change protractor(peerDependencies) to support version to 0.10.x - 0.11.x (#6)\\n\\n\\n0.1.3 - Fixed Windows command\\n\\n\\n0.1.2 - Added keepAlive option.\\n\\n\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '151',\n",
       "  'forks': '128',\n",
       "  'commits': '238'},\n",
       " {'language': 'JavaScript 53.6',\n",
       "  'readme': \"jquery.tweetable.js\\nA simple li'l plugin that lets you make site content easily tweetable.\\nInspired (and by inspired I mean I stole this) from a recent New York Times article doing the exact same thing.\\nYou can see a demo of the plugin in action here.\\nUsage\\nDependencies\\nI mean, jquery is in the name and everything ...\\nThe Basics\\nUsing the plugin is super duper easy:\\n$('[data-tweetable]').tweetable();\\nThat will grab all elements with the data-tweetable attribute and create clickable links out of them.  If the attribute in question has a value (e.g. data-tweetable='I love lamp'), the tweet's text will be set to that value; otherwise, it is set to whatever text is within the given element.\\n(If you're using a selector which is not an attribute, then be sure to set dataAttr so it knows where to pick up the text.)\\nThe links are unstyled by default, but those links also are created with a given class (by default tweetable) so you can style them to your heart's content.\\nOptions\\nOptions are pretty simple, and you can pass them as so:\\n$('.awesome-text').tweetable({\\n\\tvia: 'justinmduke'\\n});\\nThe defaults:\\n// Defaults\\n{\\n\\tdataAttr: 'data-tweetable',\\n\\tlinkClass: 'tweetable',\\n\\tvia: null,\\n\\trelated: null,\\n\\turl: window.location.pathname\\n}\\nThe first two should be fairly obvious: via, related, and url all correspond to what you're passing to Twitter in terms of data.\\n\",\n",
       "  'watchers': '11',\n",
       "  'stars': '151',\n",
       "  'forks': '11',\n",
       "  'commits': '16'},\n",
       " {'language': 'JavaScript 77.0',\n",
       "  'readme': \"react-native-smart-scroll-view\\n\\n\\nA pure JS React Native Component for IOS.\\nA wrapper around react-native ScrollView to handle keyboard events and auto adjust input fields to be visible above keyboard on focus.\\nTakes in your components and recursively searches for any component (i.e. TextInput) that is given smartScrollOptions as a prop. Further props are added to these components to ensure they are always visible above the keyboard and within the ScrollView when focused.\\nThere is also the option to autofocus the next component with smartScrollOptions on TextInput submission, and the ability to autofocus any component by setting the smartScrollOptionsprops appropriately and specifying the index of the component (more info below) .\\nGreat for use with forms which have multiple TextInput fields!\\nGetting Started\\n\\nInstallation\\nProperties\\nExample Usage\\nTODO\\n\\nInstallation\\n$ npm i react-native-smart-scroll-view --save\\nProperties\\nIn wrapping around the ScrollView and using the TextInput to control keyboard we have used their native properties to create our functionality. You can still add most props to TextInputs and we will allow you to pass some props to the ScrollView but do so with care.\\nSmartScrollView Props\\n\\n\\n\\nProp\\nDefault\\nType\\nDescription\\n\\n\\n\\n\\nforceFocusField\\nundefined\\nnumber or string\\nForce scroll the view to the TextInput field at the specified index (smart children indexed in order from 0) or 'scrollRef' you have given to your smart child (see smartScrollOptions below)\\n\\n\\nscrollContainerStyle\\n{flex: 1}\\nnumber\\nStyle options for the View that wraps the ScrollView, the ScrollView will take up all available space.\\n\\n\\nscrollPadding\\n5\\nnumber\\nPadding between the top of the keyboard/ScrollView and the focused TextInput field\\n\\n\\ncontentContainerStyle\\n{flex: 1}\\nnumber\\nSet to the ScrollView contentContainerStyle prop\\n\\n\\nzoomScale\\n1\\nnumber\\nSet to the ScrollView zoomScale prop\\n\\n\\nshowsVerticalScrollIndicator\\ntrue\\nbool\\nSet to the ScrollView showsVerticalScrollIndicator prop\\n\\n\\ncontentInset\\n{top: 0, left: 0, bottom: 0, right: 0}\\nobject\\nSet to the ScrollView contentInset prop\\n\\n\\nonScroll\\n() => {}\\nfunc\\nSet to the ScrollView onScroll function. It will be called alongside our own\\n\\n\\nonRefFocus\\n()=>{}\\nfunc\\nGives back the 'ref' of the node whenever a smart component is focused\\n\\n\\n\\nSmart Component Props\\nSmart components can be the native 'TextInput' s, other component like 'View' s or your own custom components.\\nFor each component that you would like to use, provide the prop smartScrollOptions alongside the normal props. Beware* some props of native components like TextInputs may be modified by the Smart Scroll View (see below).\\nsmartScrollOptions - An object with the following keys:\\n\\n\\n\\nKey\\nType\\nDescription\\n\\n\\n\\n\\ntype\\nenum (text,custom)\\nFor type 'text' the 'moveToNext' and 'onSubmitEditing' options can be set (see below). For type 'custom' further scrolling must be done by forcing the index\\n\\n\\nmoveToNext\\nbool\\nIf true, the next TextInput field will be focused when the submit button on the keyboard is pressed. Should be set to false or omitted for the last input field on the page. Warning this will not work if keyboardType for the TextInput is set to 'number-pad', 'decimal-pad', 'phone-pad' or 'numeric' as they do not have a return key\\n\\n\\nonSubmitEditing(next)\\nfunc\\nOptional function that takes a callback.  When invoked, the callback will focus the next TextInput field. If no function is specified the next TextInput field is focused. Example: (next) => { if (condition) { next() } }\\n\\n\\nscrollRef\\nstring\\nTo be used in conjunction with the 'forceFocusField' prop of the 'SmartScrollView'. Use 'scrollRef' to reference a particular component which can then be set to forceFocusField to have control where the focus is\\n\\n\\n\\nHow We Modify TextInput Props\\nFor any component which has 'smartScrollOptions.type = text', it is inferred that it is either a 'TextInput' component or contains a 'TextInput' component. The props of the enclosing 'TextInput' component are modified in the following way.\\n\\nWe attach our own onFocus function and will call yours alongside.\\nIf moveToNext in smartScrollOptions is true:\\n\\nTheonSubmitEditing is replaced with our own. See above.\\nblurOnSubmit is set to false\\n\\n\\n\\nExample Usage\\nCode for the above gif is found here\\nHere is another example of the smart-scroll-view in action.\\n\\nTo run the code yourself and play around, open and run the Xcode project.\\nopen SuperScrollingFormExample/ios/SuperScrollingFormExample.xcodeproj\\nTODO\\n\\nAllow for more types other than text input to have smart scroll functionality.\\n\\ni.e. a customisable picker component that can be used to replace keyboard to allow the user to select a value from a picker.\\nAny image, button, slider....\\n\\n\\nAllow for header/banner above keyboard.\\nBetter animations....\\nYour issues/suggestions!\\n\\nFeel free to comment, question, create issues, submit PRs... to make this view even smarter\\n\",\n",
       "  'watchers': '4',\n",
       "  'stars': '151',\n",
       "  'forks': '33',\n",
       "  'commits': '71'},\n",
       " {'language': 'JavaScript 98.4',\n",
       "  'readme': 'highlight\\nA simple, pluggable API for syntax highlighting.\\nSyntax highlighters tend to have pretty opinionated APIs, both in terms of when to highlight, and how to determine the language. And lots bundle the languages directly into the core library, which makes it much harder to reason about them individually, or to have the smallest possible file size if you don\\'t need the esoteric ones.\\nSo... we made this one. The API is very simple, yet still gives you full control. The language definitions are all separate plugins, so you get the smallest possible build size, and so that they\\'re simpler for everyone to contribute to. Because regexes are already hard enough to read as it is!\\nInstallation\\n$ component install segmentio/highlight\\n\\nExample\\nvar Highlight = require(\\'highlight\\')\\nvar html = require(\\'highlight-xml\\');\\nvar js = require(\\'highlight-javascript\\');\\n\\nvar highlight = new Highlight()\\n  .use(html)\\n  .use(js);\\n\\nvar el = document.querySelector(\\'.code-sample\\');\\nhighlight.element(el);\\n...or if you\\'re lazy, you can just pass a selector string:\\nhighlight.element(\\'.code-sample\\');\\n...or if you\\'re incredibly lazy, you can just highlight everything:\\nhighlight.all();\\nLanguages\\n\\nBash\\nCSS\\nC#\\nGo\\nHTML\\nJava\\nJavascript\\nJSON\\n.NET\\nObjective-C\\nPHP\\nPython\\nRuby\\nSQL\\nXML\\nYAML\\n\\nAPI\\nnew Highlight()\\nCreate a new Highlight instance.\\n#use(plugin)\\nApply a plugin function, for example language syntaxes.\\n#string(string, language)\\nHighlight a string of code of a given language.\\n#element(el, [language])\\nHighlight an el. If you don\\'t pass a language, it will use the data-language attribute:\\n<pre data-language=\"css\"><code>YOUR CODE HERE</code></pre>\\n#elements(els, [language])\\nHighlight a series of els.\\n#all()\\nHighlight all of the elements in the DOM that have a data-language attribute.\\n#prefix(string)\\nSet the CSS class name prefix string.\\n#language(name, grammar)\\nDefine a new language by name with a grammar.\\n#parse(string, language)\\nReturn an AST for a given string and language.\\n#stringify(ast)\\nConvert an AST into a string of HTML.\\nLicense\\nThe MIT License\\nCopyright © 2014 Segment.io\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n',\n",
       "  'watchers': '43',\n",
       "  'stars': '150',\n",
       "  'forks': '9',\n",
       "  'commits': '29'},\n",
       " {'language': 'JavaScript 97.1',\n",
       "  'readme': 'es6-react-mixins\\n \\nes6-react-mixins is a module that lets you augment your ES6 React component classes with any number of custom ES6 mixins. You can also use it to merge traditional pre-ES6 React mixin objects into your ES6 React classes.\\nInspired by this gist by Sebastian Markbåge the strategy is transient class hierarchies – instead of locking classes into permanent is a roles, class realtionships are assembled and re-assembled at will.\\nES6 mixins are functions that return classes. The base parameter is used internally to construct the mixin chain. There\\'s no need to extend React.component, you get that for free.\\nconst es6Mixin = base => class extends base {\\n  componentWillMount() {\\n    console.log(\"augmented componentWillMount\");\\n  }\\n  render() {\\n    console.log(\"augmented render\");\\n  }\\n};\\nReact components invoke mixins with a call to super.\\nimport mixin from \\'es6-react-mixins\\';\\nimport React from \\'react\\';\\n\\nclass MyComponent extends mixin(es6Mixin) {\\n  componentWillMount() {\\n    super.componentWillMount();\\n  }\\n  render() {\\n    super.render();\\n    return <div>hello/div>;\\n  }\\n}\\n\\nReact.render(<MyComponent>, document.body);\\nThe API works with any number of mixins. Obviously order matters with multiple mixins – each super call works its way up the mixin hierarchy.\\nconst mixin1 = base => class extends base {\\n  componentWillMount() {\\n    super.componentWillMount();\\n    console.log(\"mixin1 componentWillMount\");\\n  }\\n  render() {\\n    super.render();\\n    console.log(\"mixin1 render\");\\n  }\\n};\\n\\nconst mixin2 = base => class extends base {\\n  componentWillMount() {\\n    super.componentWillMount();\\n    console.log(\"mixin2 componentWillMount\");\\n  }\\n  render() {\\n    super.render();\\n    console.log(\"mixin2 render\");\\n  }\\n};\\n\\nclass MyComponent extends mixin(mixin1, mixin2) {\\n  componentWillMount() {\\n    super.componentWillMount();\\n  }\\n  render() {\\n    super.render();\\n    return <div>hello/div>;\\n  }\\n}\\nNotice that any mixin can call super, even though there may be no other mixins to hear it. Every mixin descends from a base mixin with no-op implementations of the react lifecycle methods.\\nes6-react-mixins also accepts traditional plain object mixins (a la pre-ES6 React), adapting them to ES6 style mixins internally.\\nvar reactMixin = {\\n  componentWillMount: function () {\\n    console.log\\n  },\\n  render: function () {\\n    this.reactMixin_rendered = true;\\n  }\\n};\\n\\nclass MyComponent extends mixin(reactMixin) {\\n  componentWillMount() {\\n    super.componentWillMount();\\n  }\\n  render() {\\n    super.render();\\n    return <div>hello/div>;\\n  }\\n}\\nInstallation\\nnpm install es6-react-mixins\\n\\nThe source is written in es6 but there\\'s an npm prebublish step which transpiles to es5 and dumps into lib directory - which is the default import.\\nTesting\\nnpm test\\n\\nContributions\\nYes please!\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '150',\n",
       "  'forks': '15',\n",
       "  'commits': '64'},\n",
       " {'language': 'Python 87.7',\n",
       "  'readme': 'Powerhose\\nPowerhose turns your CPU-bound tasks into I/O-bound tasks so your Python applications\\nare easier to scale.\\n\\nPowerhose is an implementation of the\\nRequest-Reply Broker\\npattern in ZMQ.\\nSee http://powerhose.readthedocs.org for a full documentation.\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '97',\n",
       "  'forks': '11',\n",
       "  'commits': '205'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'dm_bot\\n** This bot has been decomissioned **\\nThis bot is more of a learning exercise than anything else, no harm intended.\\nIt works by using phantom.js and selenium to grab a screenshot of the whole page and mark some points for reference. The next step is to crop just the article out of the page by using pillow, and then save it as a jpg to save bandwidth. Once it has done that it can post a comment on the reddit post with a link to the image.\\nNeeds all this installed to work:\\n\\npraw        - get reddit posts and comment on them\\nselenium    - get screenshot through phantom.js\\npillow      - crop screenshot to just the article\\nlibjpeg8    - allows pillow to save as jpeg\\nphantom.js  - renders the screenshot\\n\\n\\nLink to /u/DailMail_Bot. See him in action!\\nI was originally using imgur, however imgur would compress images over 1MB so they were unreadable. I now use a.pomf.se as it allows for the full image to be shown.\\nI am aware the code is not in good shape, I keep just patching it manually to keep up with new image hosts, it is just intended to be a fun script, not a work of art!\\nI have been banned from:\\n\\n/r/sydney\\n/r/texas\\n/r/imagesofthe1970s\\n/r/ImagesOfThe2010s\\n/r/ImagesOfEngland\\n/r/DoctorWhumour\\n/r/historyblogs\\n/r/nufcirclejerk\\n/r/nufcirclejerk\\n/r/The_Donald\\n/r/Mr_Trump\\n/r/Minecraft\\n/r/ireland\\n/r/youranonnews\\n/r/army\\n/r/Conservatives_R_Us\\n/r/conservatives\\n/r/skeptic\\n/r/hipsterhuskies\\n/r/science\\n/r/SandersForPresident\\n/r/politota\\n/r/UKIP\\n/r/ukipparty\\n/r/gaming\\n/r/Romania\\n/r/worldnews\\n/r/photoshopbattles\\n/r/unitedkingdom\\n/r/funfacts\\n/r/travel\\n/r/China\\n/r/interestingasfuck\\n/r/conspiratard\\n/r/inthenews\\n/r/funny\\n/r/politics\\n/r/NaziHunting\\n/r/Celebs\\n/r/Conservative\\n/r/UpliftingNews\\n/r/WTF\\n/r/news\\n/r/AnyMore\\n/r/necrodancer\\n/r/celebnsfw\\n/r/space\\n/r/reactiongifs\\n/r/nottheonion\\n/r/ukpolitics\\n/r/WhiteRights\\n/r/unitedkingdom\\n/r/european\\n/r/ebola\\n/r/soccer\\n\\nList of potential image hosts\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '97',\n",
       "  'forks': '16',\n",
       "  'commits': '56'},\n",
       " {'language': 'Python 81.9',\n",
       "  'readme': 'notice\\nSince I don\\'t work on this any more for 2 years and it may not work, I don\\'t have time to fix issues yet.\\nIf anyone want to use a free version of Slack , just go here: https://workjoy.online\\nslack-backup\\nSlack is great, it would be better if you can easily back-up all Slack chat history into your secured host automatically and help browsing history easily .\\nPreinstall Requiments:\\n\\npython 2.7\\nvirtualenv\\n\\nIf you want to test locally, you should install all modules in requirements.txt into your virtualenv.\\nYou can use a free hosted service here:\\nhttp://slackbk.herokuapp.com\\nInstallation in Heroku\\nClone the source code into your local machine\\ngit clone git@github.com:suoinguon/slack-backup.git\\n\\nInstall heroku toolbelt\\nhttps://toolbelt.heroku.com\\nCreate an heroku app\\ncd slack-backup\\n\\nheroku create    \\n\\nPush source code into heroku\\ngit push heroku master    \\n\\nCreate database schema\\nheroku run python manage.py migrate\\n\\nGet your Slack client_id and client_secret\\nhttps://api.slack.com/applications\\nSet heroku environment variables with your client_id & client_secret\\nheroku config:set SLACK_CLIENT_ID=[your_client_id]    \\n\\nheroku config:set SLACK_CLIENT_SECRET=[your_client_secret]    \\n\\nAdd Heroku Sendgrid and Scheduler\\nheroku addons:add sendgrid:starter    \\n\\nheroku addons:add scheduler    \\n\\nSet cron job for to automatically back-up your slack history\\nOpen the scheduler\\nheroku addons:open scheduler\\n\\nhttps://scheduler.heroku.com/dashboard\\nAdd follow in command into your heroku schedule, set frequency to \"Every 10 minutes\"\\npython manage.py parse_channels    \\n\\nOpen and start using it\\nheroku open    \\n\\nDon\\'t know heroku & don\\'t have time? I can help you to deploy.\\nhong (at) vietnamdevelopers.com    \\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '96',\n",
       "  'forks': '26',\n",
       "  'commits': '29'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Wasserstein GAN\\n\\nCode for the paper\\n実装について\\nSee also:\\n\\nUnrolled Generative Adversarial Networks\\nChainer implementation of Unrolled GAN\\n\\n\\n\\nRequirements\\n\\nChainer\\n\\nMixture of Gaussians Dataset\\n\\nMNIST\\ngenerated images\\n\\nAnimeface Dataset\\nloss\\n\\ngenerated images\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '95',\n",
       "  'forks': '9',\n",
       "  'commits': '29'},\n",
       " {'language': 'Python 98.1',\n",
       "  'readme': 'ObjC2RubyMotion Converter for Sublime Text 2/3\\n\\nA command plugin that enables to convert Objective-C code to Ruby Motion.\\nScreenshot:\\n\\nUsing iShowU, KeyCastr, GIFBrewery\\nHow It Works\\nCode in the line of the cursor or selection are converted:\\n// original\\n_window = [[UIWindow alloc] initWithFrame:[[UIScreen mainScreen] bounds]];\\nself.window.rootViewController = self.myNavController;\\n[self.window makeKeyAndVisible];\\n\\n// select lines and run \"objc_to_ruby_motion\"\\n_window = UIWindow.alloc.initWithFrame(UIScreen.mainScreen.bounds)\\nself.window.rootViewController = self.myNavController\\nself.window.makeKeyAndVisible\\nInstall\\nPackage Control\\nInstall the ObjC2RubyMotion package from Package Control.\\nManual\\nClone this repository from your Sublime packages directory:\\nMacosx\\n$ cd ~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2/Packages\\n$ git clone https://github.com/kyamaguchi/SublimeObjC2RubyMotion.git ObjC2RubyMotion\\n\\nKey Binding\\nBy default,\\nFor Conversion\\nsuper+ctrl+i objc_to_ruby_motion\\nConversions\\nIn internal order\\n\\nReplace NSString @\"String\" -> \"String\"\\nRemove inline comments //\\nConvert blocks (may be not perfect)\\nConvert square brackets expression  [[Obj alloc] init] -> Obj.alloc.init\\nRemove semicolon ; at the end of line\\nRemove autorelease at the end\\nRemove type declaration for Object Type * before =\\nYES/NO\\nFloat 100.0f -> 100\\nCGRectMake CGRectMake(10, 10, 20, 20) -> [[10, 10], [20, 20]]\\n\\nNOT supported\\n\\nComplex block\\nif else conditions etc.\\nactions action:@selector(tapped:)\\nMethod name and args conversion - (NSInteger)tableView:(UITableView *)tableView numberOfRowsInSection:(NSInteger)section\\nOthers\\n\\nNote\\nThis converter is not intended to convert perfectly. This is intended to help the conversion of Objective-C code snippets.\\nSome complex expression may not be converted correctly.\\nTests\\n😄 Fortunately ObjC2RubyMotion has tests.\\nRun test from command line\\n$ cd ~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2/Packages/ObjC2RubyMotion\\n$ python tests/all_test.py\\n\\nOR\\n🐎 Use guard\\n# Requirement: ruby\\n$ gem install guard\\n$ gem install guard-shell\\n$ guard\\n\\nCustomize\\n\\n\\nFork it\\n\\n\\nRemove original ObjC2RubyMotion and clone yours OR add your repository as another git remote\\n\\n\\n$ cd ~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2/Packages\\n$ git clone git@github.com:yourname/SublimeObjC2RubyMotion.git ObjC2RubyMotion\\n\\n\\nCopy test file and write new test\\n\\ncp tests/test_basic.py tests/test_custom.py\\n\\n🐍 Change and Test\\n\\nNormally, you should change CodeConverter.py and test_*.py.\\n$ guard is recommended.\\nNote\\nProbably most users of this plugin are rubyist, not pythonista.\\nThis converter is mostly composed of regular expression for now.\\nIf we try to improve this to convert more complex expressions, probably we need to replace the converter with the one using parser/tokenizer/scanner.\\nFork is welcomed.\\nCare about unexpected string replacements. They could happen and they will be problems.\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '95',\n",
       "  'forks': '7',\n",
       "  'commits': '84'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"DJ Skeletor\\nDJ Skeletor is a skeleton Django project handy for quick bootstrapping of new\\nempty Django projects. It will help you get up and running with your project\\nin seconds.\\nThe repository contains an empty, relocatable Django project with a selection\\nof useful Django application and setup for development, production and\\n(automated) test settings and environments.\\nQuickstart\\n# prepare the virtual environment\\nmkvirtualenv --no-site-packages myenv\\n\\n# get the skeleton project\\ngit clone https://github.com/senko/dj-skeletor.git myproject\\ncd myproject\\n\\n# set up the development environment\\nmake dev-setup\\n\\n# run your fully operational Django project\\npython manage.py runserver_plus\\n\\nBatteries included\\nThe development environment by default includes:\\n\\nSouth\\nfor database migrations (both development and production use it)\\nDjango Debug Toolbar\\nfor displaying extra information about view execution\\nSQLite database (dev.db in the project root directory)\\nIntegrated view debugger making it easy to debug crashes directly from the\\nbrowser (Werkzeug and django-extension's\\nrunserver_plus)\\nFull SQL statement logging\\nBeefed-up Django shell with model auto-loading and\\nIPython REPL\\nFlake8 source code checker\\n(style, passive code analysis)\\nConsole E-mail backend set by default in dev for simple E-mail send testing\\nAutomated testing all set-up with\\nnose, optionally creating test\\ncoverage reports, and using the in-memory SQLite database (and disabled\\nSouth) to speed up test execution\\nDisabled cache for easier debugging\\n\\nThe production environment by default includes:\\n\\nSouth for database migrations (both development and production use it)\\nGunicorn integration\\nDjango Compressor\\nfor CSS/JS asset minification and compilation\\nDatabase auto-discovery via environment settings, compatible with Heroku\\nSentry client (raven_compat)\\nfor exception logging (used only if SENTRY_DSN variable is set in\\nsettings or environment)\\nLocal-memory cache (although memcached is strongly recommended if available)\\n\\nThe extended tour\\nAfter setting up your new Django project (see Quickstart above), try these:\\n# make sure all tests pass (you'll need to write them first, though :)\\nmake test\\n\\n# get a test coverage report (outputs to stdout, saves HTML format in\\n# cover/index.html and produces Cobertura report compatible with Jenkins)\\nmake coverage\\n\\n# clean up test artifacts, *.pyc files and cached compressed assets\\nmake clean\\n\\n# check if the code follows PEP8 and is free of obvious errors\\n# this also includes cyclomatic complexity check and will complain if your\\n# code is too complex (configurable by editing the Makefile)\\nmake lint\\n\\n# update the environment (eg. after pulling in new code)\\nmake dev-update\\n\\n# open up the new and improved Django shell\\npython manage.py shell_plus\\n\\nYearn for more? Django-extension comes with tons of useful management commands,\\nrun python manage.py help to get an overview.\\nThe production setup\\nThe production environment can't be set up automatically (at it may require\\nsetting up databaes details and other per-server settings manually), but there\\nare some helper Makefile tasks to speed it up.\\nTo set up the production environment for a DJ Skeletor-powered project, loosely\\nfollow this procedure:\\n# prepare the virtual environment\\nmkvirtualenv --no-site-packages myenv\\n\\n# get your project\\ngit clone <myproject-url>\\ncd myproject\\n\\n# install the requirements\\nmake reqs/prod\\n\\n# create a project/settings/local.py settings file with per-server config\\nvim project/settings/local.py\\n\\n# run automatic update (db sync/migrations, collectstatic)\\nmake prod-update\\n\\n# your production environment is now ready\\npython manage.py run_gunicorn\\n\\nOf course, your mileage may vary.\\nThe settings files\\nThe settings files base (base settings used in all environments),\\nprod (production settings), dev (local development settings) and\\ntest (settings used when running automated tests) should contain only the\\nsettings used by all developers/servers.\\nPer-server (or per-developer) settings should go into local module\\n(ie. project/settings/local.py). The usual pattern for this module is to\\nfirst import everything from the settings variant that best matches your\\nenvironment (prod for servers, dev for local development), and then\\noverride/add settings as needed.\\nExample production settings just specifying the production database:\\n# file: project/settings/local.py\\nfrom .prod import *\\n\\nDATABASES = {\\n    'default': { ... }\\n}\\n\\nYou shouldn't need to add local.py to the repository (in fact, git is\\nalready set up to ignore it). If some setting needs to be shared by everyone,\\nit should probably be added to base, dev or prod.\\nThe local settings file isn't required. If it doesn't exist, the production\\nsetup will be used by default. This is useful if you don't have per-server\\nsettings or they're deployed via Unix environment (as they are on eg. Heroku\\nand similar cloud hosting providers).\\nEnvironment settings\\nIn either production or develoment mode, settings can also be set via\\nthe environment variables. The following variables are supported:\\n\\nDATABASE_URL - Heroku-compatible database URL\\nDEBUG - String true enables DEBUG, any other disables\\nTEMPLATE_DEBUG - String true enables TEMPLATE_DEBUG, any other disables\\nCOMPRESS_ENABLED - String true enables django-compressor, any other\\ndisables\\nSQL_DEBUG - String true enables SQL statement logging, any other\\ndisables (disabled by default, available only if using dev.py)\\nCACHE_BACKEND - String value to put into CACHES['default']['BACKEND']\\nEMAIL_BACKEND - String value for EMAIL_BACKEND (only if using dev.py)\\n\\nNote that values from local.py override environment settings! You probably\\nwant to use either the local settings file or the environment settings, not\\nmix them.\\nHeroku support\\nThe production setup uses database autodiscovery so if you have a (promoted)\\ndatabase in Heroku, it will automatically get picked up.\\nFor Heroku, you'll probably want to add the Procfile file with contents\\nsimilar to this:\\nweb: python manage.py run_gunicorn --workers=4 --bind=0.0.0.0:$PORT\\n\\nIf your web app supports uploading of media (eg. images, videos or other\\nfiles) by users, you'll probably need the django-storages app to\\nautomatically host them somewhere else (eg on Amazon S3). When\\ndjango-storages is set up, the collecstatic management command (run as\\npart of make prod-update) will copy the static assets to the specified\\nservice as well.\\nAfter pushing the new code to Heroku for update, you should make sure to run\\nall the needed management commands to migrate the database, etc:\\nheroku run make prod-update\\n\\nDjango Debug Toolbar\\nDjango Debug Toolbar is set up so it's always visible in the dev\\nenvironment, no matter what the client IP is, and always hidden in\\nthe production environment.\\nSentry / Raven\\nTo use the Sentry client, you'll need a server to point it to. Installing\\nSentry server is easy as:\\n# mkvirtualenv --no-site-packages sentry-env\\n# pip install sentry\\n# sentry init\\n# sentry start\\n\\nYou'll want to install Sentry into its own environment as it requires\\nDjango 1.2 or 1.3 at the moment.\\nIf you don't want to install Sentry yourself, you can use a hosted\\nversion at http://getsentry.com/.\\nWhen you connect to your (or hosted) Sentry server and create a new project\\nthere, you'll be given Sentry DSN which you need to put into production\\nsettings to activate Sentry exception logging.\\nCompressor\\nDjango Compressor can minify and compile your CSS and JS assets. DJ Skeletor\\ncomes with Compressor support, but to make use of it, you need to use\\n{% compress %} tags in your templates.\\nBy default Compressor runs in online mode, and files are compressed\\nand cached (if needed) when the template that uses them is first served.\\nOptionally, it can also use offline mode (COMPRESSOR_OFFLINE) in which\\nthe static files are pre-compressed in deployment phase. To activate this,\\nyou'll need to activate the COMPRESSOR_OFFLINE setting (it's commented\\nout in settings/prod.py by default) and update Makefile to run the\\ncompressor in the deployment phase.\\nNote that if you enable offline mode, you will need to run compress after\\nevery template or static file change, so it's recommended to only use it\\nfor deployed/production environments.\\nTest code coverage\\nDJ Skeletor comes with support for nose test runner and code coverage\\nreporting through coverage.py.\\nTo run a normal test without code coverage report, run make test.\\nTo run a test with a coverage report, run make coverage. The report\\nis generated in HTML format in the cover/ subdirectory, and in the\\nCobertura format in coverage.xml file (useful for integrating with\\nContinuous Integration systems, such as Jenkins). The test run also produces\\nnosetests.xml file in the standard JUnit format, also useful for integration\\nwith Jenkins or other CI systems.\\nDeployments via git\\nIf deployments are done via git (and not fabric, see below), it's\\nrecommended to create another Makefile target that will do the deploy, for\\nexample:\\ndeploy:\\n  git pull\\n  $(MAKE) update\\n  # command to restart the service(s) as neccessary\\n\\nFabric\\nA fabfile is provided with common tasks for rsyncing local directory to\\nthe server for use while developing the project, and for deploying the\\nproject using git clone/pull.\\nUseful commands:\\n\\nserver - host to connect to (same as -H, but accepts only one argument)\\nenv - virtualenv name on the server, as used with virtualenvwrapper/workon\\nproject_path - full path to the project directory on the server\\nrsync - use rsync to copy the local folder to the project directory on the server\\nsetup - set up the project instance on the server (clones the origin\\nrepository, creates a virtual environment, initialises the database and\\nruns the tests)\\ndeploy - deploy a new version of project on the server using git pull\\ncollecstatic, syncdb, migrate, runserver - run manage.py command\\nupdate - combines collecstatic, syncdb, migrate\\ntest - run manage.py test with the test settings enabled\\n\\nFor all the commands, run 'fab -l' or look at the source.\\nExamples:\\nCopy local directory to the server, update database and static files, and\\nrun tests (only files changed from last copy are going to be copied):\\nfab server:my.server.com env:myenv project_path:/path/to/project rsync update test\\n\\nDeploy a new instance of a project on a server ('myenv' will be newly created,\\ncode will be cloned into /path/to/project):\\nfab server:my.server.com env:myenv project_path:/path/to/project \\\\\\n    setup:origin=http://github.com/senko/dj-skeletor\\n\\nDeploy a new version of the project on the server (a new git tag will be\\ncreated for each deployment, so it's easy to roll-back if needed):\\nfab server:my.server.com env:myenv project_path:/path/to/project deploy\\n\\nCustomization\\nEveryone has a slightly different workflow, so you'll probably want to\\ncustomize the default fabric tasks or combine them. You can either customize\\nfabfile.py and commit the changes to your repository, or you can create\\nlocal_fabfile.py, which will be loaded if it exists. The latter can be useful\\nif you have per-team-member fabric customizations you don't want to commit\\nto the repository.\\nRenaming the project\\nBy default, DJ Skeletor names the project project, so it's generic enough\\nto not requiring the change for each project, so the initial setup is\\na bit faster (and the manage.py logic is simpler).\\nIf you do want to change the project name though, there's couple of things\\nyou need to do. For example, if you want to rename the project to foo:\\n\\nrename the folder: git mv project foo\\nupdate Makefile, manage.py and˛fabfile to set PROJECT_NAME to foo\\ncommit the changes to your git repository and you're done!\\n\\n\",\n",
       "  'watchers': '10',\n",
       "  'stars': '95',\n",
       "  'forks': '26',\n",
       "  'commits': '94'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'spec\\nWhat is it?\\nspec is a Python (2.6+ and 3.3+) testing tool that turns this:\\n\\ninto this:\\n\\nSpecifically, spec provides:\\n\\nColorized, specification style output\\nColorized tracebacks and summary\\nOptional timing display for slow tests\\nTest-running CLI tool which enables useful non-default options and implements\\nrelaxed test discovery for less test_annoying.py:TestBoilerplate.test_code\\nand more readable.py:Classes.and_methods.\\n\\nSpec-style output\\nspec is a BDD-esque\\nnose plugin designed to provide \"specification\"\\nstyle test output (similar to Java\\'s\\nTestDox or Ruby\\'s\\nRSpec). Spec-style output provides a more\\nstructured view of what your tests assert, compared to nose/unittest\\'s\\ndefault \"flat\" mode of operation.\\nFor example, this nose-style test module:\\nclass TestShape(object):\\n    def test_has_sides(self):\\n        pass\\n\\n    def test_can_calculate_its_perimeter(self):\\n        pass\\n\\nclass TestSquare(object):\\n    def test_is_a_shape(self):\\n        pass\\n\\n    def test_has_four_sides(self):\\n        pass\\n\\n    def test_has_sides_of_equal_length(self):\\n        pass\\nnormally tests like so, in a single flat list:\\nTestShape.test_has_sides ... ok\\nTestShape.test_can_calculate_its_perimeter ... ok\\nTestSquare.test_has_four_sides ... ok\\nTestSquare.test_has_sides_of_equal_length ... ok\\nTestSquare.test_is_a_shape ... ok\\n\\nWith spec enabled (--with-spec), the tests are visually grouped by class,\\nand the member names are tweaked to read more like regular English:\\nShape\\n- has sides\\n- can calculate its perimeter\\n\\nSquare\\n- has four sides\\n- has sides of equal length\\n- is a shape\\n\\nIn other words:\\n\\nClass-based tests are arranged with the class name as the subject, and the\\nmethods as the specifications;\\nAny module-level tests are arranged with the module name as the subject;\\nAll objects\\' docstrings are used as their descriptions, if found. Otherwise:\\n\\nCamelCaseNames (typically classes) have any leading/trailing Test\\nstripped, as well as any trailing underscore;\\nCamelCaseNames also get turned into sentences if necessary, so e.g.\\nCamelCaseNames becomes Camel case names;\\nunderscored_names have any leading/trailing test (with its attached\\nunderscore) stripped;\\nunderscored_names have underscores turned into spaces;\\n\\n\\n\\nTest runner\\nspec ships with a same-name command-line tool which may be used as a more\\nliberal nosetests. In addition to toggling a number of useful default options\\n(such as nose\\'s builtin --detailed-errors) spec-the-program will honor\\nany and all public objects defined within your project\\'s tests directory,\\nmeaning any file, function or class whose name does not begin with an\\nunderscore (\\'_\\') and which is defined locally.\\nFor example, given the following code inside tests/feature_name.py:\\nfrom external_module import a_function, AClass\\n\\ndef _helper_function(args):\\n    return a_function(args)\\n\\nclass _Parent(object):\\n    def this_will_not_get_tested():\\n        pass\\n\\nclass Feature(_Parent):\\n    def should_have_some_attribute(self):\\n        _helper_function(AClass)\\n\\n    def does_something_awesome(self):\\n        self._helper_method()\\n\\n    def _helper_method(self):\\n        pass\\n\\ndef something_tested_by_itself_outside_a_class():\\n    pass\\nonly the following items will be picked up as test cases:\\n\\nFeature.should_have_some_attribute\\nFeature.does_something_awesome\\nsomething_tested_by_itself_outside_a_class\\n\\nThe imported function and class, the underscored functions/methods, and the\\nmethods inherited from a parent class, are all ignored.\\nEnhanced output via the Spec class\\nAs with some other spec-style tools, spec provides a means for nesting your\\ntest \"contexts\" so they display nicely during test runs. Just use the Spec\\nclass as your primary superclass and inner classes will get parsed\\nautomatically.\\nFor example:\\nfrom spec import Spec\\n\\nclass ClassUnderTest(Spec):\\n    def it_behaves_like_this(self):\\n        # ...\\n\\n    class init:\\n        \"__init__\"\\n        def takes_arg1(self):\\n            # ...\\n\\n        def takes_arg2(self):\\n            # ...\\nThe above results in output like so:\\nClass under test\\n- it behaves like this\\n\\n    __init__\\n    - takes arg1\\n    - takes arg2\\n\\nThis indentation makes output even easier to follow & helps keep things\\norganized.\\nAccessing outer classes from inner ones\\nFrequently, you may have a useful setup method in your outer class, and wish\\nto access objects attached to self from inner classes. As of Spec 0.11.0 this\\nis now possible and is quite transparent; failed attribute lookups will check\\nan instantiated + setup\\'d copy of the outer class:\\nclass MainClass(Spec):\\n    def setup(self):\\n        self.x = \\'y\\'\\n\\n    def outer_test(self):\\n        assert self.x == \\'y\\'\\n\\n    class some_inner_class:\\n        def inner_test(self):\\n            # Here, because some_inner_class has no real \\'x\\' attribute, we end\\n            # up seeing the outer class\\' value.\\n            assert self.x == \\'y\\'\\nRight now this support is pretty basic and assumes your setup methods are\\nliterally named setup, not setUp or whatnot. This will likely improve in\\nthe future.\\nUsage tips\\nFollowing from spec-the-tool\\'s discovery algorithm, and spec-the-plugin\\'s\\nname transformation, we suggest the following for both readable code and\\nreadable test output:\\n\\nStore tests in tests/, with whatever file-by-file organization you like\\nbest;\\nWithin files, import the classes under test normally, e.g. from mymodule import MyClass;\\nName the test classes identically, but with a trailing underscore to avoid\\nname collisions, and inheriting from the Spec class, e.g. class MyClass_(Spec): [...]\\nName their methods like English sentences, e.g. def has_attribute_X(self): [...].\\nTests not yet filled out should call the skip function, which raises a\\n\"skip this test\" exception Nose handles correctly.\\n\\nFor example:\\nfrom spec import Spec, skip\\nfrom mypackage import MyClass, MyOtherClass\\n\\nclass MyClass_(Spec):\\n    def has_attribute_A(self):\\n        skip()\\n\\nclass MyOtherClass_(Spec):\\n    def also_has_attribute_A(self):\\n        skip()\\n\\n    def has_attribute_B(Spec):\\n        skip()\\ntests as:\\nMyClass\\n- has attribute A\\n\\nMyOtherClass\\n- also has attribute A\\n- has attribute B\\n\\nActivation / command-line use\\nAfter installation via setup.py, pip or what have you, nosetests will\\nexpose these new additional options/flags:\\n\\n--with-spec: enables the plugin and prints out your tests in specification\\nformat. Also automatically sets --verbose (i.e. the spec output is a\\nverbose format.)\\n--no-spec-color: disables color output. Normally, successes are green,\\nfailures/errors are red, and\\nskipped tests are\\nyellow.\\n--spec-doctests: enables (experimental) support for doctests.\\n--with-timing: enables timing display for tests that take >=\\nTIMING_THRESHOLD (see below) to run.\\n--timing-threshold: configuration of timing threshold (in seconds) for\\n--with-timing. Defaults to 0.1.\\n\\nWhy would I want to use it?\\nSpecification-style output can make large test suites easier to read, and like\\nany other BDD tool, it\\'s more about framing the way we think about (and view)\\nour tests, and less about introducing new technical methods for writing them.\\nWhere did it come from?\\nspec is heavily based on the spec plugin for Titus Brown\\'s\\npinocchio\\nset of Nose extensions. Said plugin was originally written by Michal\\nKwiatkowski. Both pinocchio and its spec plugin are copyright © 2007\\nin the above two gentlemen\\'s names, respectively.\\nThis version of the plugin was created and distributed by Jeff Forcier, ©\\n2011. It tweaks the original source to be Python 2.7 compatible, based on\\nsimilar\\nchanges.\\nIt also fixes a handful of bugs such as broken\\nSkipTest\\ncompatibility under Nose 1.x, and then adds some additional functionality on\\ntop (most notably the spec command-line tool.)\\nWhat\\'s the license?\\nBecause this is heavily derivative of pinocchio, spec is licensed the same\\nway -- under the MIT\\nlicense.\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '95',\n",
       "  'forks': '17',\n",
       "  'commits': '185'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Django-Pluggables\\nDjango-Pluggables provides a design pattern for making reusable apps \"pluggable\" so that apps can exist at multiple URL locations and be presented by other apps or models.\\n\\nOverview\\nDjango-Pluggables is a design pattern that endows reusable applications with a few additional features:\\n\\nApplications can exist at multiple URL locations (e.g. http://example.com/foo/app/ and http://example.com/bar/app/).\\nApplications can be \"parented\" to other applications or objects which can then deliver specialized context information.\\nPosting form data and error handling can happen in locations that make sense to the user, as opposed to the common practice of using templatetags and standalone error or preview pages for form data processing.\\nViews and templates remain generic and reusable.\\n\\n\\nInstallation\\nDjango-Pluggables can be added to your Django project like any other reusable app. To see the examples in action, fun the following commands:\\nFirst off, the native Mac implementation of sed is not as robust as we expect. Therefore install Super Sed  Then run\\n$ easy_install Fabric==0.1.1 pip virtualenv\\n$ fab bootstrap\\n$ ./examples/sillywalks/manage.py runserver\\n\\nVoila! You now have a fully functional site running at http://127.0.0.1:8000/\\n\\nUsage\\nTo utilize Django-Pluggables, you will need to do a little bit of configuration. First you must have a two applications:\\n\\nA reusable app that will be made Pluggable.\\nAn app that will expose an instance of the Pluggable app in its URL hierarchy (this app does not need to be reusable).\\n\\nInstallation and development of these two apps can begin normally. The individual functions of the apps should be standalone. The Django-Pluggables pattern delivers needed context and parameters to the app that is being made Pluggable when it is called from the URL hierarchy of the other app.\\nTo set up the relationship between these two apps, it is necessary to define two items:\\n\\nThe URLconf that defines the URLs that will invoke the Pluggable app instance.\\nThe pluggable definition that will supply the Pluggable app with its required context and parameters.\\n\\n\\nNote\\nIn the following examples we will use two imaginary apps: sillywalks and complaints. These would be imported in your modules like so:\\nfrom sillywalks.models import Walk\\nfrom complaints.models import Complaint\\n\\nIn the examples we discuss below, the sillywalks app lives at the URL:\\nhttp://example.com/sillywalks/\\n\\nThe sillywalks app utilizes the complaints app to field complaints about various types of walk. To provide an excellent user experience, the complaints forms and functionality should be presented at a URL beneath the sillywalks URL:\\nhttp://example.com/sillywalks/<walk_name>/complaints/\\n\\nAlso, the presentation of complaints functionality beneath the sillywalks app should adhere to the sillywalks templates pattern, including various walk-specific information that must be supplied through additional, non- complaints app context.\\n\\n\\nDefining the URLconf\\nIn order to make the complaints app pluggable, we first must define a pluggable URLs file for it:\\nclass ComplaintsPluggable(PluggableApp):\\n    urlpatterns = patterns(\\'\\',\\n        url(r\\'^$\\', \\'complaints.views.view_complaint\\', name=\\'complaints_view_complaint\\'),\\n        url(r\\'^activity_feed/$\\', \\'complaints.views.submit_complaint\\', name=\\'complaints_submit_complaint\\'),\\n\\n...\\n\\n\\nNote\\nOnly one pluggable URLs file is required for an app that you wish to make pluggable. This URLconf may be subclassed for additional flexibility when using the pluggable app in conjunction with other apps, but it is likely that one definition of pluggable URLs will be sufficient for most use cases.\\n\\n\\nDefining the Pluggable Config and Context\\nOnce the complaints app has been set up with the pluggable URLs, it is necessary to define the context that will be delivered to the complaints app and add these URLs to the URLconf for the sillywalks app:\\nfrom complaints.urls import ComplaintsPluggable\\n\\nclass SillyWalkComplaints(ComplaintsPluggable):\\n    def pluggable_view_context(self, request, complaint_id):\\n        return dict(complaint_id=complaint_id)\\n\\n    def pluggable_template_context(self, request, slug):\\n        return dict(silly_walk=SillyWalk.objects.get(slug=slug))\\n\\n    def pluggable_config(self):\\n        return dict(base_template=\\'sillywalks/base.html\\')\\n\\nurlpatterns = SillyWalkComplaints(\\'sillywalk\\')\\n\\n\\nMore\\nThe primary repository for Django-Pluggables is located at:\\nhttp://github.com/nowells/django-pluggables/\\nDjango-Pluggables was created by Nowell Strite.\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '94',\n",
       "  'forks': '7',\n",
       "  'commits': '79'},\n",
       " {'language': 'Python 85.9',\n",
       "  'readme': 'Sublime IPython Notebook\\nThis is a Sublime Text 3 plugin that emulates IPython notebook interface inside Sublime.\\nNOTE: this plugin does not work with the latest versions of Ipython Notebook! (2.0 or greater)\\nDisclaimer\\nWhile the plugin looks stable so far and I am trying to preserve as much of the notebook data as possible, there are no guarantees that you data will be safe. Do not use it for the notebooks that contain valuable data without doing a backup.\\nHow to use\\n\\nConnect to the notebook server using \"Open IPython Notebook\" command. Choose a notebook you want to open and it will open in a separate buffer.\\nI am trying to support keyboard shortcuts from the web version of the notebook. Currently you can use:\\n\\nshift+enter - execute current cell\\nctrl+enter - execute current cell inplace\\nctrl+m, d - delete current cell\\nctrl+m, a - add cell above current\\nctrl+m, b - add cell below current\\nctrl+m, n - select next cell\\nctlr+m, p - select previous cell\\nctrl+m, y - code cell\\nctrl+m, m - markdown cell\\nctrl+m, t - raw cell\\nctrl+m, s - save notebook (ctrl+s and super+s will work too)\\n\\n\\n\\nNotes\\n\\nYou can use %pylab inline. You will not be able to see the plots, but they will be saved in the notebook and available when viewing it through the web interface.\\nI am using websocket-client library from https://github.com/liris/websocket-client and (slightly patched) subset of the IPython. You do not have to install them separately.\\nST3 port was contributed by chirswl\\nDark theme, support for password-protected servers and nicer last-used-server picker was contributed by z-m-k\\n\\nVintage Mode\\nIn Vintage mode, for the navigation keys to work as expected in IPython Notebook buffer, you need to modify some keybindings. Add the following to your Key Bindings - User.\\n\\n\\nAdd a context key to Shift+Enter so you can run a cell with Shift+Enter in the command mode:\\n{ \"keys\": [\"shift+enter\"], \"command\": \"set_motion\", \"args\": {\\n    \"motion\": \"move\",\\n    \"motion_args\": {\"by\": \"lines\", \"forward\": true, \"extend\": true }},\\n    \"context\": [\\n        { \"key\": \"setting.command_mode\"},\\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": false },\\n        ]\\n},\\n\\n\\n\\nCommand mode Up/Down navigation keys:\\n{ \"keys\": [\"j\"], \"command\": \"set_motion\", \"args\": {\\n    \"motion\": \"move\",\\n    \"motion_args\": {\"by\": \"lines\", \"forward\": true, \"extend\": true },\\n    \"linewise\": true },\\n    \"context\": [\\n        { \"key\": \"setting.command_mode\"},\\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": false },\\n        ]\\n},\\n\\n{\\n    \"keys\": [\"j\"], \"command\": \"inb_move_up\",\\n    \"context\" : [\\n        { \"key\": \"setting.command_mode\"}\\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": true },\\n        { \"key\": \"auto_complete_visible\", \"operator\": \"equal\", \"operand\": false },\\n        ]\\n},\\n\\n{ \"keys\": [\"k\"], \"command\": \"set_motion\", \"args\": {\\n    \"motion\": \"move\",\\n    \"motion_args\": {\"by\": \"lines\", \"forward\": false, \"extend\": true },\\n    \"linewise\": true },\\n    \"context\": [\\n        { \"key\": \"setting.command_mode\"},\\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": false },\\n        ]\\n},\\n\\n{\\n    \"keys\": [\"k\"], \"command\": \"inb_move_down\",\\n    \"context\" : [\\n        { \"key\": \"setting.command_mode\"},\\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": true },\\n        { \"key\": \"auto_complete_visible\", \"operator\": \"equal\", \"operand\": false },\\n        ]\\n},\\n\\n\\n\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '94',\n",
       "  'forks': '9',\n",
       "  'commits': '89'},\n",
       " {'language': 'Python 51.2',\n",
       "  'readme': \"Live Blog\\nSourcefabric's Live Blog is an open source web app that enables journalists to provide immediate and ongoing coverage on evolving news events. Find out more here: https://www.sourcefabric.org/en/liveblog/.\\nSee an example of Live Blog in action here: http://www.zeit.de/politik/deutschland/2013-09/bundestagswahl-2013-live\\nLicense: AGPLv3\\nCopyright: Sourcefabric z.ú.\\nFeatures\\n\\nAccess to sources: Drag and drop search results from social media and other content networks into your Live Blog timeline.\\nThemes: Customise Live Blog to match your site’s design and make it look great on mobile and tablet.\\nUser roles: Build a team workﬂow for multiple collaborators with appropriate sign-offs and permissions.\\nEasy embedding: Simply copy and paste an embed code from the Live Blog interface and start spreading the news.\\nBlog chaining: Connect several blogs to syndicate your content to other media outlets.\\nSEO solution: With our new embed plugin you can integrate an html version of the blogs into your website: https://github.com/liveblog/plugin-liveblog-embed-server/tree/master\\n\\nInstallation\\nPlease check the installation guide: https://github.com/superdesk/Live-Blog/blob/master/INSTALL.md\\nDocumentation\\n\\nLive Blog RESTful API documentation: http://docs.sourcefabric.org/projects/live-blog-restful-api/en/latest/\\n\\nManuals and Tutorials\\n\\nManual for journalists: https://www.sourcefabric.org/en/liveblog/manuals/\\nHow to create your own themes: http://www.sourcefabric.org/en/community/blog/2097/Building-themes-for-Live-Blog.htm\\n\\nSuperdesk\\nLive Blog is based on Superdesk technology.\\nHow To Contribute\\nCommit messages\\nEvery commit has to have a meaningful commit message in the form:\\n[JIRA ref] [JIRA Title] or [Title]\\n<empty line>\\n[Description]\\n\\nWhere JIRA ref is the Issue code eg. LB-13.\\nFor trivial changes you can ommit JIRA ref or Description or both eg. Add travis.yml files\\nPull requests\\nEvery pull request has to have a meaningful message and if not specified in the commits, a good description of what has been done.\\n\",\n",
       "  'watchers': '25',\n",
       "  'stars': '93',\n",
       "  'forks': '44',\n",
       "  'commits': '13,037'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Carma-Public/swagger-jaxrs-doclet has been doing more recent development on a fork of this project. Please consider checking that fork out first.\\nSwagger Doclet \\nA JavaDoc Doclet that can be used to generate a Swagger resource listing suitable for feeding to\\nswagger-ui.\\nUsage\\nTo use the Swagger Doclet in your Maven project, add the following to your POM file.\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\\n    <modelVersion>4.0.0</modelVersion>\\n\\n    <groupId>…</groupId>\\n    <artifactId>…</artifactId>\\n    <version>…</version>\\n    \\n    <dependencies>\\n        …\\n    </dependencies>\\n\\n    <build>\\n        <plugins>\\n            <plugin>\\n                <groupId>org.apache.maven.plugins</groupId>\\n                <artifactId>maven-javadoc-plugin</artifactId>\\n                <version>2.9.1</version>\\n                <executions>\\n                    <execution>\\n                        <id>generate-service-docs</id>\\n                        <phase>generate-resources</phase>\\n                        <configuration>\\n                            <doclet>com.hypnoticocelot.jaxrs.doclet.ServiceDoclet</doclet>\\n                            <docletArtifact>\\n                                <groupId>com.hypnoticocelot</groupId>\\n                                <artifactId>jaxrs-doclet</artifactId>\\n                                <version>0.0.4-SNAPSHOT</version>\\n                            </docletArtifact>\\n                            <reportOutputDirectory>${project.build.outputDirectory}</reportOutputDirectory>\\n                            <useStandardDocletOptions>false</useStandardDocletOptions>\\n                            <additionalparam>-apiVersion 1 -docBasePath /apidocs -apiBasePath /</additionalparam>\\n                        </configuration>\\n                        <goals>\\n                            <goal>javadoc</goal>\\n                        </goals>\\n                    </execution>\\n                </executions>\\n            </plugin>\\n        </plugins>\\n    </build>\\n</xml>\\nExample\\nAn example project using Dropwizard is included in jaxrs-doclet-sample-dropwizard. To get it running, run the following commands.\\n$ cd jaxrs-doclet-sample-dropwizard\\n$ mvn package\\n$ java -jar target/jaxrs-doclet-sample-dropwizard-0.0.4-SNAPSHOT.jar server sample.yml\\n\\nThe example server should be running on port 8080:\\n$ curl localhost:8080/apidocs/service.json\\n{\\n  \"apiVersion\" : \"1\",\\n  \"basePath\" : \"/apidocs/\",\\n  \"apis\" : [ {\\n    \"path\" : \"/Auth.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/HttpServletRequest.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/ModelResource_modelid.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/Recursive.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/Response.{format}\",\\n    \"description\" : \"\"\\n  }, {\\n    \"path\" : \"/greetings_name.{format}\",\\n    \"description\" : \"\"\\n  } ],\\n  \"swaggerVersion\" : \"1.1\"\\n}\\n$\\n\\nOverride Swagger UI\\nTo override the swagger ui included with the doclet, create your own swagger-ui.zip file and add a swaggerUiZipPath to the additionalparam attribute in the pom file.\\n<additionalparam>-apiVersion 1 -docBasePath /apidocs -apiBasePath / -swaggerUiZipPath ../../../src/main/resources/swagger-ui.zip</additionalparam>\\n\\n',\n",
       "  'watchers': '20',\n",
       "  'stars': '90',\n",
       "  'forks': '139',\n",
       "  'commits': '162'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Expandable-RecyclerView\\nA RecyclerView that behaves like an ExpandableListView, also includes a RecyclerView with header/footer support.\\nDownload\\nDownload the latest AAR via Maven:\\n<dependency>\\n  <groupId>com.levelupstudio</groupId>\\n  <artifactId>expandable-recyclerview</artifactId>\\n  <version>1.0.1</version>\\n</dependency>\\nor Gradle:\\ncompile \\'com.levelupstudio:expandable-recyclerview:1.0.1\\'\\nLicense\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '89',\n",
       "  'forks': '30',\n",
       "  'commits': '9'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'SpringStudy\\n对Spring框架的学习，包括官方文档的翻译，Spring框架的应用，Spring源码的剖析。。。\\n',\n",
       "  'watchers': '19',\n",
       "  'stars': '89',\n",
       "  'forks': '61',\n",
       "  'commits': '52'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'SpringStudy\\n对Spring框架的学习，包括官方文档的翻译，Spring框架的应用，Spring源码的剖析。。。\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '88',\n",
       "  'forks': '84',\n",
       "  'commits': '335'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': \"Docker/Vagrant build runners for TeamCity\\nFeatures\\nPlugin detects and reports installed vagrant and docker\\nUse the dedicated build runner to run your build script under virtualized environment\\nwith help of Docker/Vagrant build runner\\nLicense\\nApache 2.0\\nsee LICENSE.txt for details\\nDownloading Build\\nDownload the latest build from TeamCity\\nInstallation\\nTeamCity Server and TeamCity Build Agents are required to be runnung under JRE 1.7+\\nTo install plugin, put downloaded plugin .zip file into <TeamCity Data Directory>/plugins folder and restart TeamCity Server.\\nMake sure downloaded .zip file is not corrupted and is not sources .zip from GitHub.\\nFor more details, there is documentation\\nSupported Versions\\nPlugin is tested to work with TeamCity 8.1.\\nIt should work with 8.0 (and maybe 7.1.x)\\nAgent and server are expected to run JRE 1.7\\nBuilding\\n\\ncall ant -f fetch.xml fetch\\nopen the project in IntelliJ IDEA 13.1\\nmake all artifacts\\n\\nIn this repo you will find\\n\\nTeamCity server and agent plugin bundle\\nPlugin version will be patched if building with IDEA build runner in TeamCity\\nRun configuration server to run/debug plugin under TeamCity (use http://localhost:8111/bs)\\npre-configured IDEA settings to support references to TeamCity\\nUses $TeamCityDistribution$ IDEA path variable as path to TeamCity home (unpacked .tar.gz or .exe distribution)\\nBunch of libraries for most recent needed TeamCity APIs\\nModule with TestNG tests that uses TeamCity Tests API\\n\\nFor details see https://github.com/jonnyzzz/TeamCity.PluginTemplate\\nDependencies fetch via simple-maven\\nThis is simple possible ant script that could be used to fetch\\nmaven dependencies in Intellij IDEA based projects.\\nCall fetch.xml to fetch all missing dependencies\\nThe script also updates IDEA library files to include new-ly added libraries to IDEA project.\\nIt's expected you'll no changed files on script re-run. If you do => post the issue with diff\\nWe use  Maven Ant Tasks\\ninside. So <dependency> element under <maven-fetch> is the same\\nelement as under Maven Ant Tasks\\nFor details see https://github.com/jonnyzzz/intellij-ant-maven\\nNotice\\nSome code of this plugin was borrowed from NuGet plugin\\nwhich is licensed under Apache 2.0\\nNote\\nThis plugin was created with [https://github.com/jonnyzzz/TeamCity.PluginTemplate](TeamCity Plugin Template)\\nThis is my (Eugene Petrenko) private home project\\n\",\n",
       "  'watchers': '10',\n",
       "  'stars': '88',\n",
       "  'forks': '28',\n",
       "  'commits': '189'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Lifecycle Sorter for Android Studio\\nNote: This project is no longer maintained. Feel free to fork and fix issues/add enhancements.\\nThis plugin sorts the lifecycle methods of an Activity or Fragment in the order that they are called in an application.\\n\\n Activity Lifecycle \\n Fragment Lifecycle \\n\\nUsage\\nYou can find the option to sort the lifecycle methods under the Code > \"Sort Lifecycle Methods\".\\nAll you need to do is place the cursor inside of your class and select \"Sort Lifecycle Methods\" or use \\nthe keyboard shortcut control+alt+k. It\\'s as simple as that!\\nActivity before sorting\\n\\nActivity after sorting\\n\\nThis is my first plugin and thought it would be a handy tool, not only for myself but other Android developers. \\nHope you guys enjoy!\\n',\n",
       "  'watchers': '4',\n",
       "  'stars': '88',\n",
       "  'forks': '22',\n",
       "  'commits': '36'},\n",
       " {'language': 'Java 55.3',\n",
       "  'readme': \"Docker with SpringBoot, tomcat and MySQL\\nRead how to build this project from scratch here -> tutorial\\n(It's very messy right now)\\nHow to run this demo?\\nStep 0 - Requirements\\nHere are the tools you need:\\n\\n\\nDocker (duh)\\nEclipse (I'm using Eclipse Photon)\\nJava10\\n\\n\\nStep 1 - Clone this repo\\ngit clone https://github.com/HechengLi/Docker_SpringBoot_Tomcat_MySQL_Demo.git\\nStep 2 - Import TianMiao into eclipse as a maven project\\n\\nOpen your eclipse\\nOn the taskbar click File -> Import -> Maven -> Existing Maven Project\\nSelect the folder TianMiao the Root Directory (You should see /pom.xml com.example:TianMiao:0.0.1-SNAPSHOT:war in Projects)\\nClick Finish\\n\\nStep 3 - Build TianMiao as a war file\\n\\nRight click TianMiao in Package Explorer -> Run As -> Run Configurations\\nType 'clean install -Dmaven.test.skip=true' in Goals\\nClick Apply then Run (you should have TianMiao.war under Docker_SpringBoot_Tomcat_MySQL\\\\TianMiao\\\\target)\\n\\nStep 4 - Run the project with docker\\n\\nOpen your commandline, cd to the git directory\\nMake sure you have docker app running\\nRun 'docker-compose -f stack.yml up' (add -d if you want it to run in background)\\n\\nStep 5 - Rerun if there's an error on first run\\n\\nIf you get an error while starting tomcat, it probably is because the docker container running Tomcat doesn't wait for MySQL to finish running it's setup script.\\nWait for MySQL to finish running its script (it will log ...ready for connections...)\\nStop all containers and start again should fix the problem.\\n\\nStep 6 - Check if it works (suggestion - use postman)\\n\\nSend Get Request to 'http://localhost:8080/TianMiao/api/users' to retrive data\\nSend Post Request to 'http://localhost:8080/TianMiao/api/users' with json {'username': 'anyusername'} to add data\\n\\n\",\n",
       "  'watchers': '1',\n",
       "  'stars': '88',\n",
       "  'forks': '19',\n",
       "  'commits': '24'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'Litho - Picasso\\nLitho Picasso is a Litho compatible library, which provides an Image Component compatible with Picasso. Litho-PicassoX supports a wide part of Picasso\\'s functionality and is compatible with Android X. If you notice that anything is missing ping me :)\\nHow to use\\nAdd the following dependency on your app\\'s build.gradle:\\nimplementation \\'com.github.charbgr:litho-picassox:1.0\\'\\nAdd PicassoImage component on your own Component :)\\nPicassoImage.create(componentContext)\\n    .imageUrl(image)\\n    .fit(true)\\n    .centerCrop(true)\\n    .buildWithLayout();\\nHere you can find a sample ComponentSpec that uses PicassoImage component.\\nOther\\nLitho repository: https://github.com/facebook/litho\\nLitho documentation: http://fblitho.com/docs/getting-started\\nLicense\\nMIT License\\n\\nCopyright (c) 2017 Vasilis Charalampakis & Pavlos-Petros Tournaris\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '88',\n",
       "  'forks': '7',\n",
       "  'commits': '15'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'VolleySample\\nネットワーク処理を高速に、簡単に実装できる Volley ライブラリを使ったサンプルです。\\nこちらのブログの参考にお使いください。\\nAndroid Tips #51 ネットワーク通信・キャッシュ処理をより速く、簡単に実装できるライブラリ “Volley” を使ってみた | Developers.IO\\n',\n",
       "  'watchers': '10',\n",
       "  'stars': '88',\n",
       "  'forks': '69',\n",
       "  'commits': '4'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': 'MyGank\\n描述\\n一个具有用户登录注册，干货收藏功能的干货集中营第三方客户端。个人学习项目，前端使用Android展示，后端使用javaee中转，数据来源干货集中营。\\n截图\\n\\n\\n\\n\\n主要技术\\nAndroid:\\n\\nMvp\\nRxJava   ,  RxAndroid\\nRetrofit\\nBufferKnife\\nGlide\\nBAVH\\nPhotoView\\nAndroid-skin-support\\n\\njavaee:\\n\\nSpring\\nMybatis\\nMybatis.spring\\n\\n下载地址\\nhttps://fir.im/skq1\\n最后\\n\\n感谢干货集中营提供的数据\\n\\n',\n",
       "  'watchers': '2',\n",
       "  'stars': '88',\n",
       "  'forks': '16',\n",
       "  'commits': '18'},\n",
       " {'language': 'C++ 56.6',\n",
       "  'readme': 'DoubleCommand\\nA keyboard remap tool for Mac OS X.\\nContributors\\nDoubleCommand is open source, free software (GPL) developed by a team of contributors including:\\n\\nMichael Baltaks\\nTyler Bunnell\\nAdam Strzelecki\\nGuillaume Outters\\nGerolf Seitz\\nRyan Walklin\\nMike Ter Louw\\nand others...\\n\\n',\n",
       "  'watchers': '11',\n",
       "  'stars': '58',\n",
       "  'forks': '6',\n",
       "  'commits': '249'},\n",
       " {'language': 'C++ 47.7',\n",
       "  'readme': 'DoubleCommand\\nA keyboard remap tool for Mac OS X.\\nContributors\\nDoubleCommand is open source, free software (GPL) developed by a team of contributors including:\\n\\nMichael Baltaks\\nTyler Bunnell\\nAdam Strzelecki\\nGuillaume Outters\\nGerolf Seitz\\nRyan Walklin\\nMike Ter Louw\\nand others...\\n\\n',\n",
       "  'watchers': '47',\n",
       "  'stars': '58',\n",
       "  'forks': '47',\n",
       "  'commits': '5,983'},\n",
       " {'language': 'C++ 94.5',\n",
       "  'readme': \"Warning: This project is unmaintained (due to MaxMind API deprecation) and currently uses an old database from 2017\\ngeoloc is a command line tool for bulk geolocation queries written in C++.\\nOnce its binary database has been built, geoloc performs geolocation\\nqueries offline.\\nExamples:\\nBulk lookup from apache access.log:\\n$ cat access.log | awk '{print $1}' | geoloc -f - | column -t\\n\\n10.172.47.117  AU  02  Sydney         -33.8001  151.3123   AS1610581   BIGCableCo\\n10.36.87.70    AU  07  Melbourne      -37.8266  144.7834   AS1370775   Micronode+PTY+LTD\\n10.88.81.165   US  CA  San+Francisco  37.6777   -122.2221  AS49335653  Big+Flare,+Inc\\n\\nQuery some IPs:\\n$ geoloc -q 8.8.8.8 192.30.252.131 --headers | column -t\\n\\nip              country  region  city           latitude  longitude  as_num   as_text\\n8.8.8.8         US       CA      Mountain+View  37.3860   -122.0838  AS15169  Google+Inc.\\n192.30.252.131  US       CA      San+Francisco  37.7697   -122.3933  AS36459  GitHub,+Inc.\\n\\ngeoloc is designed to run fast and load fast:\\n$ wc -l /tmp/ip_list\\n\\n 1000000 /tmp/ip_list\\n\\n$ time geoloc -f /tmp/ip_list > /tmp/res1\\n\\nreal    0m6.131s\\nuser    0m5.662s\\nsys     0m0.369s\\n\\n$ time geoloc -q 8.8.8.8 192.30.252.131 > /tmp/res2\\n\\nreal    0m0.010s\\nuser    0m0.002s\\nsys     0m0.005s\\n\\n\\nInstallation\\nThe program is designed as a portable application, to run out of ~/bin,\\nwith the database stored in ~/var/db/geoloc/geodata.bin.\\nTo install:\\n$ git clone https://github.com/loadzero/geoloc.git && cd geoloc\\n$ ./configure\\n$ make\\n$ make install\\n\\nThe configure script will check for these dependencies:\\n\\niconv\\nunzip\\nwget\\nmake\\nc++\\n\\nDuring installation, data will be downloaded from\\nMaxMind to create the database.\\nAn update script will be installed into ~/bin/_geoloc_update.sh. Run\\nthis script when you would like to update your geolocation database. MaxMind\\nupdates their source data once a month.\\nI have tested on OSX 10.9.5 and Ubuntu 14.04. Other unices are likely to work\\nwith minimal or no changes. It is unlikely to work on windows, due to the use\\nof mmap.\\nDesign and Implementation\\nThe code operates in two phases, packing and query. The packing phase is all\\nabout converting the data into a machine optimal format, namely relocatable\\nsorted vectors. The query phase simply mmaps that data, and performs a\\nstd::upper_bound binary search on it to find the IPs.\\nThere is an outline of the code, roughly in topological order\\nhere, that contains a summary of each module.\\nAttribution\\nThis software includes GeoLite data created by MaxMind available from\\nhttp://www.maxmind.com\\n\",\n",
       "  'watchers': '3',\n",
       "  'stars': '58',\n",
       "  'forks': '15',\n",
       "  'commits': '13'},\n",
       " {'language': 'C++ 100.0',\n",
       "  'readme': \"Warning: This project is unmaintained (due to MaxMind API deprecation) and currently uses an old database from 2017\\ngeoloc is a command line tool for bulk geolocation queries written in C++.\\nOnce its binary database has been built, geoloc performs geolocation\\nqueries offline.\\nExamples:\\nBulk lookup from apache access.log:\\n$ cat access.log | awk '{print $1}' | geoloc -f - | column -t\\n\\n10.172.47.117  AU  02  Sydney         -33.8001  151.3123   AS1610581   BIGCableCo\\n10.36.87.70    AU  07  Melbourne      -37.8266  144.7834   AS1370775   Micronode+PTY+LTD\\n10.88.81.165   US  CA  San+Francisco  37.6777   -122.2221  AS49335653  Big+Flare,+Inc\\n\\nQuery some IPs:\\n$ geoloc -q 8.8.8.8 192.30.252.131 --headers | column -t\\n\\nip              country  region  city           latitude  longitude  as_num   as_text\\n8.8.8.8         US       CA      Mountain+View  37.3860   -122.0838  AS15169  Google+Inc.\\n192.30.252.131  US       CA      San+Francisco  37.7697   -122.3933  AS36459  GitHub,+Inc.\\n\\ngeoloc is designed to run fast and load fast:\\n$ wc -l /tmp/ip_list\\n\\n 1000000 /tmp/ip_list\\n\\n$ time geoloc -f /tmp/ip_list > /tmp/res1\\n\\nreal    0m6.131s\\nuser    0m5.662s\\nsys     0m0.369s\\n\\n$ time geoloc -q 8.8.8.8 192.30.252.131 > /tmp/res2\\n\\nreal    0m0.010s\\nuser    0m0.002s\\nsys     0m0.005s\\n\\n\\nInstallation\\nThe program is designed as a portable application, to run out of ~/bin,\\nwith the database stored in ~/var/db/geoloc/geodata.bin.\\nTo install:\\n$ git clone https://github.com/loadzero/geoloc.git && cd geoloc\\n$ ./configure\\n$ make\\n$ make install\\n\\nThe configure script will check for these dependencies:\\n\\niconv\\nunzip\\nwget\\nmake\\nc++\\n\\nDuring installation, data will be downloaded from\\nMaxMind to create the database.\\nAn update script will be installed into ~/bin/_geoloc_update.sh. Run\\nthis script when you would like to update your geolocation database. MaxMind\\nupdates their source data once a month.\\nI have tested on OSX 10.9.5 and Ubuntu 14.04. Other unices are likely to work\\nwith minimal or no changes. It is unlikely to work on windows, due to the use\\nof mmap.\\nDesign and Implementation\\nThe code operates in two phases, packing and query. The packing phase is all\\nabout converting the data into a machine optimal format, namely relocatable\\nsorted vectors. The query phase simply mmaps that data, and performs a\\nstd::upper_bound binary search on it to find the IPs.\\nThere is an outline of the code, roughly in topological order\\nhere, that contains a summary of each module.\\nAttribution\\nThis software includes GeoLite data created by MaxMind available from\\nhttp://www.maxmind.com\\n\",\n",
       "  'watchers': '16',\n",
       "  'stars': '58',\n",
       "  'forks': '22',\n",
       "  'commits': '116'},\n",
       " {'language': 'C++ 67.3',\n",
       "  'readme': 'A Cocos2d-x Asynchronous Http Request Libray That Let You Make Asynchrounous Http Requests From Cocos2d-x A FUN!\\nYou can find the library file:\\n\\nCCHttpRequest.h\\nCCHttpRequest.cpp\\n\\nUnder CCHttpRequestExample/Classes\\nThis library had been written as a cocos2d-x extension, Use it is very simple:\\nCCHttpRequest *gateway = CCHttpRequest::sharedHttpRequest();\\n\\n//Get Request\\nstd::string url = \"http://www.baidu.com\";\\ngateway->addGetTask(url, NULL, NULL);\\n\\n//Post Request\\nstd::string postData = \"key=value\";\\ngateway->addPostTask(url, postData, NULL, NULL);\\n\\n//Download File\\nstd::vector<std::string> downloads;\\ndownloads.push_back(\"http://www.baidu.com/index.php\");\\ngateway->addDownloadTask(urls, NULL, NULL);\\n\\nYou can specify a callback just by simply replace the NULL parameter with your selector, So you can process the response data in your callback method.\\n',\n",
       "  'watchers': '17',\n",
       "  'stars': '58',\n",
       "  'forks': '26',\n",
       "  'commits': '4'},\n",
       " {'language': 'C++ 91.1',\n",
       "  'readme': 'A Cocos2d-x Asynchronous Http Request Libray That Let You Make Asynchrounous Http Requests From Cocos2d-x A FUN!\\nYou can find the library file:\\n\\nCCHttpRequest.h\\nCCHttpRequest.cpp\\n\\nUnder CCHttpRequestExample/Classes\\nThis library had been written as a cocos2d-x extension, Use it is very simple:\\nCCHttpRequest *gateway = CCHttpRequest::sharedHttpRequest();\\n\\n//Get Request\\nstd::string url = \"http://www.baidu.com\";\\ngateway->addGetTask(url, NULL, NULL);\\n\\n//Post Request\\nstd::string postData = \"key=value\";\\ngateway->addPostTask(url, postData, NULL, NULL);\\n\\n//Download File\\nstd::vector<std::string> downloads;\\ndownloads.push_back(\"http://www.baidu.com/index.php\");\\ngateway->addDownloadTask(urls, NULL, NULL);\\n\\nYou can specify a callback just by simply replace the NULL parameter with your selector, So you can process the response data in your callback method.\\n',\n",
       "  'watchers': '8',\n",
       "  'stars': '58',\n",
       "  'forks': '10',\n",
       "  'commits': '412'},\n",
       " {'language': 'C++ 53.4',\n",
       "  'readme': 'Setup instruction (OS X)\\n\\n\\nGet dependencies (96.9 MB) and extract them to ofxPCL folder.\\n $ curl -O -L http://cl.ly/1D1Q3G072q3D/download/ofxpcl_16_libs.zip\\n $ unzip ofxpcl_16_libs.zip\\n\\n\\n\\nChange Project.xcconfig like\\n OFXPCL_PATH = $(OF_PATH)/addons/ofxPCL\\n\\n OFXPCL_OTHER_LDFLAGS = -L$(OFXPCL_PATH)/libs/pcl/lib/osx -lpcl_common -lpcl_features -lpcl_filters -lpcl_geometry -lpcl_io -lpcl_io_ply -lpcl_kdtree -lpcl_keypoints -lpcl_octree -lpcl_registration -lpcl_sample_consensus -lpcl_search -lpcl_segmentation -lpcl_surface -lpcl_tracking -lqhull\\n\\n OFXPCL_HEADER_SEARCH_PATHS = $(OFXPCL_PATH)/libs/pcl/include/ $(OFXPCL_PATH)/libs/pcl/include/eigen3 $(OFXPCL_PATH)/libs/pcl/include/pcl-1.6\\n\\n OFXPCL_LD_RUNPATH_SEARCH_PATHS = @executable_path/../../../../../../../addons/ofxPCL/libs/pcl/lib/osx @executable_path/../../../data/pcl/lib\\n\\n LD_RUNPATH_SEARCH_PATHS = $(OFXPCL_LD_RUNPATH_SEARCH_PATHS)\\n\\n OTHER_LDFLAGS = $(OF_CORE_LIBS) $(OFXPCL_OTHER_LDFLAGS)\\n HEADER_SEARCH_PATHS = $(OF_CORE_HEADERS) $(OFXPCL_HEADER_SEARCH_PATHS)\\n\\n\\n\\nAdd ofxPCL/src filder to Xcode project.\\n\\n\\nCopy libraries to data folder\\n $ python copyfiles.py PATH_TO_YOUR_PROJECT\\n\\n\\n\\n',\n",
       "  'watchers': '5',\n",
       "  'stars': '58',\n",
       "  'forks': '22',\n",
       "  'commits': '28'},\n",
       " {'language': 'C++ 86.2',\n",
       "  'readme': 'Launchy\\nLaunchy is a free utility designed to help you forget about your start menu, your desktop icons,\\nand your file manager. Launchy indexes and launches your applications, documents, project files,\\nfolders, and bookmarks with just a few keystrokes!\\nThis is a fork from SourceForge project https://sourceforge.net/projects/launchy/\\n',\n",
       "  'watchers': '12',\n",
       "  'stars': '58',\n",
       "  'forks': '9',\n",
       "  'commits': '728'},\n",
       " {'language': 'C++ 58.0',\n",
       "  'readme': 'Launchy\\nLaunchy is a free utility designed to help you forget about your start menu, your desktop icons,\\nand your file manager. Launchy indexes and launches your applications, documents, project files,\\nfolders, and bookmarks with just a few keystrokes!\\nThis is a fork from SourceForge project https://sourceforge.net/projects/launchy/\\n',\n",
       "  'watchers': '9',\n",
       "  'stars': '57',\n",
       "  'forks': '18',\n",
       "  'commits': '3'},\n",
       " {'language': 'C++ 81.9',\n",
       "  'readme': '\\n  \\nOKCash OK\\n<iframe src=\"https://discordapp.com/widget?id=213747404745211904&theme=dark\" width=\"350\" height=\"500\" allowtransparency=\"true\" frameborder=\"0\"></iframe>\\nJoin the new communications server\\n\\n\\n\\n\\nDownload Supported Platforms\\n  \\n  \\nDownload the Instant OK-Blockchain (Fast Sync for first time users):\\n https://github.com/okcashpro/ok-blockchain/releases\\n\\nOKCash is digital cash. OK is the official symbol.\\nTransactions have real fast confirmations, making them virtually instant.\\nYou can send OKCash to family or friends, or pay for goods or services, anywhere in the world.\\nThe OKCash network is decentralized and free from middlemen, giving you back control of your finances and providing a secure network for all of your payments.\\nSome of the OKCash features: Fast, Efficient, Social, Community based (like Bitcoin), Transparency, Private Messages, friendly to use, Universal name, Multicultural, Multi-platform, IoT(Internet of Things), Tor and i2p compatible.\\n\\nVisit OK Smart Links and join the different channels\\nWhat is Okcash?  http://okcash.co\\nOK videos: Youtube\\nOkcashTalk Forums *new:  http://okcashtalk.org\\nBitcoinTalk Ann: https://bitcointalk.org/index.php?topic=1028368.0\\nOK Twitter community: https://twitter.com/OkcashCrypto\\nOK Facebook community: https://www.facebook.com/OKCashCrypto/\\nOK Reddit community: https://reddit.com/r/okcash\\nOK Communications Channels: https://discord.gg/grvpc8c\\n\\nOKCash Development process\\nDevelopers work in their own forks, then submit pull requests when\\nthey think their feature or bug fix is ready.\\nThe patch will be accepted if there is broad consensus that it is a\\ngood thing.  Developers should expect to rework and resubmit patches\\nif they don\\'t match the project\\'s coding conventions (see coding.txt)\\nor are controversial.\\nThe master branch is regularly built and tested, but is not guaranteed\\nto be completely stable. Tags are regularly created to indicate new\\nstable release versions of OKCash.\\nFeature branches are created when there are major new features being\\nworked on by several people.\\nFrom time to time a pull request will become outdated. If this occurs, and\\nthe pull is no longer automatically merge able; a comment on the pull will\\nbe used to issue a warning of closure. The pull will be closed 15 days\\nafter the warning if action is not taken by the author. Pull requests closed\\nin this manner will have their corresponding issue labelled \\'stagnant\\'.\\nIssues with no commits will be given a similar warning, and closed after\\n15 days from their last activity. Issues closed in this manner will be\\nlabelled \\'stale\\'.\\n',\n",
       "  'watchers': '42',\n",
       "  'stars': '57',\n",
       "  'forks': '60',\n",
       "  'commits': '177'}]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>readme</th>\n",
       "      <th>watchers</th>\n",
       "      <th>stars</th>\n",
       "      <th>forks</th>\n",
       "      <th>commits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JavaScript 89.8</td>\n",
       "      <td>Kalei - Style guide\\nThis project aims at maki...</td>\n",
       "      <td>39</td>\n",
       "      <td>683</td>\n",
       "      <td>110</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JavaScript 84.8</td>\n",
       "      <td>bui\\n基于jQuery的富客户端控件库\\n\\n文档库地址\\n应用代码\\nAPI代码\\nL...</td>\n",
       "      <td>96</td>\n",
       "      <td>659</td>\n",
       "      <td>374</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JavaScript 98.2</td>\n",
       "      <td>JQuery EasyTabs Plugin\\nTabs with(out) style.\\...</td>\n",
       "      <td>34</td>\n",
       "      <td>564</td>\n",
       "      <td>211</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JavaScript 55.9</td>\n",
       "      <td>\\nVisit the Mimosa Website for all sorts of Mi...</td>\n",
       "      <td>23</td>\n",
       "      <td>532</td>\n",
       "      <td>37</td>\n",
       "      <td>1,636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JavaScript 100.0</td>\n",
       "      <td>Sketch Mate\\nThese plugins will make you best ...</td>\n",
       "      <td>25</td>\n",
       "      <td>509</td>\n",
       "      <td>32</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>C++ 91.1</td>\n",
       "      <td>A Cocos2d-x Asynchronous Http Request Libray T...</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>C++ 53.4</td>\n",
       "      <td>Setup instruction (OS X)\\n\\n\\nGet dependencies...</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>C++ 86.2</td>\n",
       "      <td>Launchy\\nLaunchy is a free utility designed to...</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>C++ 58.0</td>\n",
       "      <td>Launchy\\nLaunchy is a free utility designed to...</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>C++ 81.9</td>\n",
       "      <td>\\n  \\nOKCash OK\\n&lt;iframe src=\"https://discorda...</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             language                                             readme  \\\n",
       "0     JavaScript 89.8  Kalei - Style guide\\nThis project aims at maki...   \n",
       "1     JavaScript 84.8  bui\\n基于jQuery的富客户端控件库\\n\\n文档库地址\\n应用代码\\nAPI代码\\nL...   \n",
       "2     JavaScript 98.2  JQuery EasyTabs Plugin\\nTabs with(out) style.\\...   \n",
       "3     JavaScript 55.9  \\nVisit the Mimosa Website for all sorts of Mi...   \n",
       "4    JavaScript 100.0  Sketch Mate\\nThese plugins will make you best ...   \n",
       "..                ...                                                ...   \n",
       "473          C++ 91.1  A Cocos2d-x Asynchronous Http Request Libray T...   \n",
       "474          C++ 53.4  Setup instruction (OS X)\\n\\n\\nGet dependencies...   \n",
       "475          C++ 86.2  Launchy\\nLaunchy is a free utility designed to...   \n",
       "476          C++ 58.0  Launchy\\nLaunchy is a free utility designed to...   \n",
       "477          C++ 81.9  \\n  \\nOKCash OK\\n<iframe src=\"https://discorda...   \n",
       "\n",
       "    watchers stars forks commits  \n",
       "0         39   683   110     133  \n",
       "1         96   659   374     610  \n",
       "2         34   564   211     111  \n",
       "3         23   532    37   1,636  \n",
       "4         25   509    32      81  \n",
       "..       ...   ...   ...     ...  \n",
       "473        8    58    10     412  \n",
       "474        5    58    22      28  \n",
       "475       12    58     9     728  \n",
       "476        9    57    18       3  \n",
       "477       42    57    60     177  \n",
       "\n",
       "[478 rows x 6 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('readme_data_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
