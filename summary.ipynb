{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project\n",
    "### README Repository Language Classification\n",
    "\n",
    "---\n",
    "## Executive Summary\n",
    "\n",
    "1. Scrapped README's from GitHub profiles\n",
    "2. Built models to predict if primary language is:\n",
    "    - Java\n",
    "    - JavaScript  \n",
    "    - Python\n",
    "    - C++\n",
    "3. Gradient Boost Classification model performed the best with a 76% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.0%}'.format\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import main\n",
    "import acquire\n",
    "import prepare\n",
    "import preprocessing\n",
    "import model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire\n",
    "\n",
    "- Manually explore sites in a web browser, and identify the relevant HTML elements from sports, data engineering, artificial intelligence, space exporation and biology written in Javascript, Python, Java and C++.\n",
    "- Use the requests module to obtain the HTML from each repository.\n",
    "- Use BeautifulSoup to parse the HTML and obtain the text/data that we want.\n",
    "- Parse the text of README's, language, watchers, stars, forks and commits.\n",
    "- Saved raw data as a json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "\n",
    "- Cleaned the `language`, `watchers`, `stars`, `forks` and `commits` columns of uneccessary text and changed data types as needed.\n",
    "- Normalized, tokenized, stemmed, lemmatized, and removed stop words from the README column.\n",
    "- Split into Train, Validate and Test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "- Turn the readme column into a panda series and create a dataframe with the word counts for each language\n",
    "- Visualize the proportion of the most frequent words by language\n",
    "- Create bigrams and trigrams\n",
    "- Create word clouds to visualize the most frequent words used\n",
    "- Find the most frequent named entities in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration Visual 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration Visual 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration Visual 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modeling\n",
    "\n",
    "\n",
    "60-20-20 Split\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "<strong>TF-IDF</strong><br>\n",
    "Term Frequency - Inverse Document Frequency\n",
    "\n",
    "<strong>MinMaxScaler</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = model.model_data('data/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline accuracy is 26%\n",
      "JavaScript\n"
     ]
    }
   ],
   "source": [
    "baseline_prediction = y_train.value_counts().nlargest(1).index[0]\n",
    "\n",
    "baseline_accuracy = (y_train == baseline_prediction).mean()\n",
    "print(f\"The baseline accuracy is {baseline_accuracy:.0%}\")\n",
    "print(f\"{baseline_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ridge Classifier\n",
    "\n",
    "clf = RidgeClassifierCV()\n",
    "clf.fit(X_train, y_train)\n",
    "clf_train_acc = clf.score(X_train, y_train)\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "tree = RandomForestClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "tree_train_acc = tree.score(X_train, y_train)\n",
    "\n",
    "### Gradient Boost\n",
    "\n",
    "ml = GradientBoostingClassifier()\n",
    "ml.fit(X_train, y_train)\n",
    "ml_train_acc = ml.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = model.model_scores(clf_train_acc,\n",
    "                                  tree_train_acc,\n",
    "                                  ml_train_acc,\n",
    "                                  modeling_set='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ridge</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>gradient_boost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ridge  random_forest  gradient_boost\n",
       "train   100%           100%            100%"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ridge Classifier\n",
    "clf_val_acc = clf.score(X_validate, y_validate)\n",
    "\n",
    "### Random Forest\n",
    "tree_val_acc = tree.score(X_validate, y_validate)\n",
    "\n",
    "### Gradient Boost\n",
    "ml_val_acc = ml.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_scores = model.model_scores(clf_val_acc,\n",
    "                                     tree_val_acc,\n",
    "                                     ml_val_acc,\n",
    "                                     modeling_set='validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ridge</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>gradient_boost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>validate</th>\n",
       "      <td>74%</td>\n",
       "      <td>72%</td>\n",
       "      <td>78%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ridge  random_forest  gradient_boost\n",
       "validate    74%            72%             78%"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient Boost\n",
    "\n",
    "ml_test_acc = ml.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "\n",
    "Model Accuracy\n",
    "- Confusion matrix\n",
    "- Classification report\n",
    "- ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "validate = pd.DataFrame(dict(actual=y_validate))\n",
    "test = pd.DataFrame(dict(actual=y_test))\n",
    "\n",
    "train['predicted'] = ml.predict(X_train)\n",
    "validate['predicted'] = ml.predict(X_validate)\n",
    "test['predicted'] = ml.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "\n",
      "Accuracy: 99.72%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual      C++  Java  JavaScript  Python\n",
      "predicted                                \n",
      "C++          90     0           0       1\n",
      "Java          0    87           0       0\n",
      "JavaScript    0     0          91       0\n",
      "Python        0     0           0      84\n",
      "---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C++       0.99      1.00      0.99        90\n",
      "        Java       1.00      1.00      1.00        87\n",
      "  JavaScript       1.00      1.00      1.00        91\n",
      "      Python       1.00      0.99      0.99        85\n",
      "\n",
      "    accuracy                           1.00       353\n",
      "   macro avg       1.00      1.00      1.00       353\n",
      "weighted avg       1.00      1.00      1.00       353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"\")\n",
    "print('Accuracy: {:.2%}'.format(accuracy_score(train.actual, train.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(train.predicted, train.actual))\n",
    "print('---')\n",
    "print(classification_report(train.actual, train.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set\n",
      "\n",
      "Accuracy: 77.97%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual      C++  Java  JavaScript  Python\n",
      "predicted                                \n",
      "C++          22     4           5       3\n",
      "Java          4    22           1       1\n",
      "JavaScript    3     2          24       0\n",
      "Python        1     1           1      24\n",
      "---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C++       0.65      0.73      0.69        30\n",
      "        Java       0.79      0.76      0.77        29\n",
      "  JavaScript       0.83      0.77      0.80        31\n",
      "      Python       0.89      0.86      0.87        28\n",
      "\n",
      "    accuracy                           0.78       118\n",
      "   macro avg       0.79      0.78      0.78       118\n",
      "weighted avg       0.79      0.78      0.78       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Set\")\n",
    "print(\"\")\n",
    "print('Accuracy: {:.2%}'.format(accuracy_score(validate.actual, validate.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(validate.predicted, validate.actual))\n",
    "print('---')\n",
    "print(classification_report(validate.actual, validate.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n",
      "\n",
      "Accuracy: 76.27%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual      C++  Java  JavaScript  Python\n",
      "predicted                                \n",
      "C++          22     3           3       1\n",
      "Java          3    21           0       1\n",
      "JavaScript    3     4          25       4\n",
      "Python        2     1           3      22\n",
      "---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C++       0.76      0.73      0.75        30\n",
      "        Java       0.84      0.72      0.78        29\n",
      "  JavaScript       0.69      0.81      0.75        31\n",
      "      Python       0.79      0.79      0.79        28\n",
      "\n",
      "    accuracy                           0.76       118\n",
      "   macro avg       0.77      0.76      0.76       118\n",
      "weighted avg       0.77      0.76      0.76       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Set\")\n",
    "print(\"\")\n",
    "print('Accuracy: {:.2%}'.format(accuracy_score(test.actual, test.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(test.predicted, test.actual))\n",
    "print('---')\n",
    "print(classification_report(test.actual, test.predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training observation 1: L2 Norm --- Euclidian distance = 1\n",
      "Training observation 2: L2 Norm --- Euclidian distance = 1\n",
      "Training observation 3: L2 Norm --- Euclidian distance = 1\n"
     ]
    }
   ],
   "source": [
    "# tf-idf L2 Normalization\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Training observation {i+1}: L2 Norm --- Euclidian distance = {sum(X_train.iloc[i]**2):.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
