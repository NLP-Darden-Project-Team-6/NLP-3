{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests import get\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of urls using a list of topics\n",
    "pages = acquire.get_url_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 unique search topics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://github.com/search?l=JavaScript&q=sports&type=Repositories',\n",
       " 'https://github.com/search?l=JavaScript&q=data+engineering&type=Repositories',\n",
       " 'https://github.com/search?l=JavaScript&q=artificial+intelligence&type=Repositories',\n",
       " 'https://github.com/search?l=JavaScript&q=space+exploration&type=Repositories',\n",
       " 'https://github.com/search?l=JavaScript&q=biology&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=sports&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=data+engineering&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=artificial+intelligence&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=space+exploration&type=Repositories',\n",
       " 'https://github.com/search?l=Python&q=biology&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=sports&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=data+engineering&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=artificial+intelligence&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=space+exploration&type=Repositories',\n",
       " 'https://github.com/search?l=Java&q=biology&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=sports&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=data+engineering&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=artificial+intelligence&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=space+exploration&type=Repositories',\n",
       " 'https://github.com/search?l=C%2B%2B&q=biology&type=Repositories']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {len(pages)} unique search topics')\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_links = acquire.get_url_links(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://github.com/freeCodeCamp/league-for-good'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_soup = acquire.make_soup(repository_links[0])\n",
    "\n",
    "for sentence in repo_soup.findAll(class_=\"markdown-body entry-content container-lg\"):\n",
    "    readme = ''.join(sentence.findAll(text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = repo_soup.find('span', class_=\"Progress-item\", itemprop='keywords', attrs='aria-label')['aria-label']\n",
    "repo_info = {'language' : language,\n",
    "              'readme': readme}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "data = acquire.scrape_repos(repository_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'language': 'JavaScript 98.7',\n",
       "  'readme': 'FCC League-For-Good\\nThis is a free, open-source web application designed to help sports leagues track their player and team stats, and simplify the other day-to-day tasks involved with team management. It was designed to accommodate many popular sports.\\nJoin Us On Slack!\\nYou can now join us on slack. Get Invite Here\\nGetting Started\\nPrerequisites\\n\\nNodeJS\\nMongoDB\\n\\nIn order for the authorization component of this app to work, it needs to be registered with Google. Here is a helpful walkthrough of that process: https://developers.google.com/identity/sign-in/web/devconsole-project.\\nYou will want to register Type as Web application, set Authorized JavaScript origins to\\nhttp://localhost:4000 (if you\\'re running the application locally) and set the Authorized\\nredirect URI to http://localhost:4000/auth/google/callback (this can be set through the Google API\\nConsole Dashboard under Credentials if not offered as an option during setup).\\nYou will also need to enable the \"Google+ API\" on the Google API Console Dashboard - if you forget,\\nGoogle will display an error message (with a link to the API) the first time you try to log in.\\nSteps\\n\\nFork and clone the repo\\nRun npm install\\nIn the root directory, create a .env file and place the following:\\n\\nMONGO_URI = Your database uri - typically mongodb://localhost:27017/your_project_name if your MongoDB is local\\nGOOGLE_CLIENT_ID = Client id assigned by Google\\nGOOGLE_CLIENT_SECRET = Client secret assigned by Google\\nSESSION_SECRET = Any random string of characters\\nGOOGLE_CALLBACK_URL = http://localhost:4000/auth/google/callback (Use for the callback url in the Google API console)\\n\\n\\nOpen a new terminal session and run mongod if mongodb is on the local machine\\nRun npm run dev\\nNavigate to localhost:4000 in your browser\\n\\nAvailable Scripts\\nIn the project directory, the following commands are available:\\nnpm install\\nInstalls all the dependencies\\nnpm run dev\\nBuilds the app for development. It is watched by webpack for any changes in the front end.\\nHow to Contribute\\nWe warmly welcome contributions from anyone. Check out our how to contribute section to find out how you can do so.\\n'},\n",
       " {'language': 'JavaScript 97.4',\n",
       "  'readme': 'nba\\nNode.js client for nba.com API endpoints\\nnpm install nba\\nNOTES:\\nBROWSER USAGE\\nThis package can\\'t be used from the browser because of CORS restrictions imposed by nba.com. Currently the hostnames are hardcoded so this package can\\'t be used with a proxy host but if you want support for this use case please open an issue!\\nBLACKLISTED IP ADDRESSES:\\nIt appears as though the NBA has blacklisted certain blocks of IP addresses, specifically those of cloud hosting providers including AWS. As such, you may hit a situation where an application using this package works fine on your local machine, but doesn\\'t work at all when deployed to a cloud server. Annoyingly, requests from these IPs seem to just hang. More information here and here -- the second issue has a curl command somewhere which will quickly tell you if NBA is accepting requests from your IP. (Incidentally, this is also the same reason the TravisCI build is always \"broken\" but tests all pass locally). There is a simple pass-through server in scripts/proxy that can be used to get around this restriction; you can put the proxy server somewhere that can reach NBA.com (e.g. not on AWS or Heroku or similar) and host your actual application on a cloud provider.\\nNBA API\\nThe stats.nba.com uses a large number of undocumented JSON endpoints to provide the statistics tables and charts displayed on that website. This library provides a JavaScript client for interacting with many of those API endpoints.\\nGetting Started\\nNBA.findPlayer(str) will return an object with a player\\'s name, their ID, and their team information. This method is built into the package.\\nAll methods in the NBA.stats namespace require an object to be passed in as a parameter. The keys to the object are in the docs for the stats namespace here\\nconst NBA = require(\"nba\");\\nconst curry = NBA.findPlayer(\\'Stephen Curry\\');\\nconsole.log(curry);\\n/* logs the following:\\n{\\n  firstName: \\'Stephen\\',\\n  lastName: \\'Curry\\',\\n  playerId: 201939,\\n  teamId: 1610612744,\\n  fullName: \\'Stephen Curry\\',\\n  downcaseName: \\'stephen curry\\'\\n}\\n*/\\nNBA.stats.playerInfo({ PlayerID: curry.playerId }).then(console.log);\\nFor more example API calls, see /test/integration/stats.js\\nStability Warning\\nThis is a client for an unstable and undocumented API. While I try to follow semver for changes to the JavaScript API this library exposes, the underlying HTTP API can (and has) changed without warning. In particular, the NBA has repeatedly deprecated endpoints, or added certain required headers without which requests will fail. Further, this library comes bundled with a (relatively) up-to-date list of current NBA players which is subject to change at any time -- the specific contents of it should not be considered part of this library\\'s API contract.\\nUsability\\nTo put it nicely, the NBA\\'s API endpoints are a little clunky to work with. This library tries to strike a balance between being usable but not making assumptions about how the data will be used. Specifically, the NBA sends data in a concise \"table\" form where the column headers come first then each result is an array of values that need to be matched with the proper header. This library does a simple transformation to zip the header and values arrays into a header-keyed object. Beyond that, it tries to not do too much. This is important to note because sometimes the various \"result sets\" that come back on a single endpoint seem sort of arbitrary. The underlying HTTP API doesn\\'t seem to follow standard REST practices; rather it seems the endpoints are tied directly to the data needed by specific tables and charts displayed on stats.nba.com. This is what I mean by \"clunky\" to work with -- it can be tricky to assemble the data you need for a specific analysis from the various endpoints available.\\nDocumentation\\nstill lots to do here...\\nThere are four primary parts of this library\\n\\nTop-level methods\\nstats namespace — docs\\nsportVu namespace\\nsynergy namespace\\n\\nTransport Layer\\nIn some cases you will want to use a different transport layer to handle HTTP requests. Perhaps you have an HTTP client library you like better than what I used here. Better yet, you want to get stats for the WNBA or the G League. The following code snippet shows how to use the withTransport method to create a new client with your own transport function.\\n// here we are getting stats for the WNBA!\\n\\nconst nba = require(\"nba\");\\nconst getJSON = require(\"nba/src/get-json\");\\n\\n// for the G League, try \"stats.gleague.nba.com\"\\nconst newHost = \"stats.wnba.com\";\\n\\nconst transport = (url, params, options) => {\\n  // simply swap the host and then defer the rest to the built in getJSON function\\n  const fixedURL = url.replace(\"stats.nba.com\", \"stats.wnba.com\");\\n  return getJSON(fixedURL, params, options);\\n};\\n\\n// create a new stats client here with our WNBA transport\\nconst wnbaStats = nba.stats.withTransport(transport);\\n\\n(async () => {\\n  const result = await wnbaStats.playerInfo({ PlayerID: \"1628886\" });\\n  console.log(result);\\n})();\\n\"I don\\'t use Node.js\"\\nPlease take a look at nba-client-template. The relevant part of the repo is a single JSON document from which many programming languages can dynamically generate an API client. The repo contains (sloppy) examples in Ruby and Python. Compiled languages can use code generation techniques to the same effect -- there\\'s a (again, sloppy) example in Go. If you\\'d like me to publish it to a specific registry so you can install it with your language\\'s package manager, please open an issue. Please note, however, that package only includes  the endpoints exposed by this library under the stats namespace -- sportvu and synergy endpoints aren\\'t yet included in it. I also plan to add a command-line interface to this library so that it can be easily driven as a child process by another program.\\n\\n'},\n",
       " {'language': 'JavaScript 60.0',\n",
       "  'readme': \"SportsLeague: Laravel 5.4 based system for various sports leagues to manage their teams/players/games.\\nIt is a demo project for demonstrating what can be generated with QuickAdminPanel tool.\\nSportsLeague is all generated with QuickAdmin except for front-end code.\\nClickable live-demo\\ndemo-sportsleague.quickadminpanel.com\\n\\nEmail: admin@admin.com\\nPass: password\\n\\n\\n\\nHow to use\\n\\nClone the repository with git clone\\nCopy .env.example file to .env and edit database credentials there\\nRun composer install\\nRun php artisan key:generate\\nRun php artisan migrate --seed (it has some seeded data for your testing)\\nThat's it: launch the main URL or go to /login and login with default credentials admin@admin.com - password\\n\\nLicense\\nBasically, feel free to use and re-use any way you want.\\n\\nMore from our LaravelDaily Team\\n\\nCheck out our adminpanel generator QuickAdminPanel\\nRead our Blog with Laravel Tutorials\\nFREE E-book: 50 Laravel Quick Tips (and counting)\\nSubscribe to our YouTube channel Laravel Business\\nEnroll in our Laravel Online Courses\\n\\n\"},\n",
       " {'language': 'JavaScript 63.6',\n",
       "  'readme': 'Team Colors\\n\\nTeam Colors is a reference of HEX, RGB, CMYK, and Pantone color values of major league sports teams.\\nHow-To\\nInstall: yarn install\\nDevelopment: yarn start\\nBuild: yarn build\\nHow It Works\\nSite is built on the react framework. index.html is the shell container for react app. If javascript is not supported, a link is shown to the raw JSON data which has all color information.\\nColor data is housed in a single .json file src/teams.json. Any changes to team colors can be done there. Note on colors: Color definitions for each team are in arrays and grouped by color mode. Color values should match index position in the array across color modes, for example:\\ncolors:\\n  rgb: TEAMS-RGB-BLUE, TEAMS-RGB-RED\\n  hex: TEAMS-HEX-BLUE, TEAMS-HEX-RED\\n\\nSource artwork for each team is grouped by league in sketch. Production versions of these logo should be in .svg format in public/img.\\nEdit Team Color or Name\\nFind teams .json file in src/teams.json, and edit the info you need.\\nAdd a Team\\n\\nDetermine the team’s league\\nFollowing the established pattern, add the team’s name and colors the .json file\\nAdd a vector logo for the team in its corresponding .sketch league file in sketch with the team’s name (as referenced in its .json file) in lowercase with hyphens, i.e. \"utah-jazz\"\\nExport the team’s .svg logo to public/img/\\nPreferably, optimize the svg (with a tool like SVGO)\\nRun yarn build, commit, push\\n\\nOfficial Color References\\nNBA\\nAll NBA colors are official (source user & pass: nbamedia).\\nThe NBA only provides RGB, CMYK, and Pantone colors for each team, so the HEX color is a programmatic conversion of the RGB color.\\nNFL\\nAll NFL colors are official (see sources below).\\nThe NFL provides official RGB, HEX, CMYK, and Pantone colors (so none of the colors you see on Team Colors are conversions).\\nThe NFL has logo slicks which detail team color values. These are provided on a per-conference basis. Note: each of these source links are over 100MB in size, so they take a while to download.\\n\\nAFC North\\nAFC South\\nAFC East\\nAFC West\\nNFC North\\nNFC South\\nNFC East\\nNFC West\\n\\nMLB\\nMLB colors have been extracted from the official “RGB Digital Art” spot color logo slicks provided at MLB Press Box (user account required). They were not explicitly stated values, but they are color values pulled from individual team logos in an official MLB document.\\nThe extracted colors are in HEX form and their RGB counterparts are generated programmatically.\\n\\nAmerican League logo slick\\nNational League logo slick\\n\\nNHL\\nNHL colors are official. As per Michael Sharer of the NHL.\\nMLS\\nMLS colors are currently approximations, with the exceptions listed below. I am working on getting official colors of the remaining teams.\\n\\nPhiladelphia Union\\n\\nEPL\\nThese leagues’ teams and colors are currently approximations. I am working on getting official colors. If you know how/where to find them, please open an issue here in Github.\\nTo-Dos\\n\\n Switch to flex for layout\\n Improve filtering with fuzzy string search\\n Improve error states for when data doesn\\'t render\\n Consider alternatives to no-js users rather than just \"here\\'s the raw data\" (something that doesn\\'t required a build if a single color in the JSON file is changed)\\n Possibly add team id manually to JSON file ??\\n\\n'},\n",
       " {'language': 'JavaScript 39.3',\n",
       "  'readme': 'vue-sports\\n\\nA Vue.js project\\n\\n仿凤凰新闻体育板块+赛事数据\\n体育新闻板块已经完成(暂时未用到vuex) ，正在弄体育赛事数据接口(计划加入vuex)\\n\\n\\n\\nBuild Setup\\n# install dependencies\\nnpm install\\n\\n# serve with hot reload at localhost:8080\\nnpm run dev\\n\\n# build for production with minification\\nnpm run build\\n\\n# build for production and view the bundle analyzer report\\nnpm run build --report\\n目标功能\\n\\n 体育新闻 -- 完成\\n 比赛数据 -- 未完成\\n\\n'},\n",
       " {'language': 'JavaScript 91.4',\n",
       "  'readme': '\\nCourtside: pick up sports app.\\nMake game plans and let all your friends know about it.\\n\\nInstallation\\n\\n\\nTo install requirements:\\n$ pip install -r requirements.txt\\n\\n\\nYou will need to create a keys file in keys module for the twitter and fb app keys, ones there are old and not used.\\n\\n\\n\\n\\nContributions\\nAll software contributions are welcome and encouraged.\\n\\nTwitter\\n\\nMahdi Yusuf @myusuf3\\nOmar Shammas @omarshammas\\nSerena Ngai @serenangai\\n\\n'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': '微信小程序 Sports News(体育新闻) 持续更新\\n小程序预览\\n\\n\\n\\n使用步骤\\n\\n\\n将仓库克隆到本地：\\n$ git clone https://github.com/havenxie/weapp-sportsnews.git weapp-sportsnews --depth 1\\n\\n$ cd weapp-sportsnews\\n\\n\\n打开微信Web开发者工具\\n\\n\\n\\n我用的是0.11.122100版本\\n不需要所谓的破解，网上所谓的破解只是针对之前的0.9.092100版本，新的官方版本不需要破解！\\n下载链接：\"https://mp.weixin.qq.com/debug/wxadoc/dev/devtools/download.html\"\\n\\n\\n选择添加项目，填写或选择相应信息\\n\\n\\nAppID：点击右下角无AppID（我也没有资格拿到）\\n项目名称：随便填写，因为不涉及到部署，所以无所谓\\n项目目录：选择刚刚克隆的文件夹\\n点击添加项目\\n\\n\\n\\n你可以选择在微信Web开发者工具中编码（也可选择你喜欢的编辑器，我用的是sublime,现在发现vs code比sublime好用多了）\\n\\n\\n通过左下角重启按钮，刷新编码过后的预览\\n\\n\\n代码中用到了大量ES6的语法，可能需要node环境，请自行安装\\n\\n\\n剩下的可以自由发挥了\\n\\n\\n设置页没有做，因为不知道要做些什么，假如你有兴趣的话可以自己发挥\\n\\n\\n下载后将images中的GIF5.jpg删除。\\n\\n\\n有什么不明白或者想找我交流的朋友可以和我联系\\n\\n\\n\\n个人微信\\n个人公众号\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n至此告一段落，啥时候再有兴趣再来继续添加功能吧。\\n有兴趣的小伙伴可以一起来提交代码。。。\\n许可\\nMIT © havenxie\\n'},\n",
       " {'language': 'JavaScript 72.0',\n",
       "  'readme': 'TableChamp\\nTablesports leaderboard app\\nTrack each ping pong, pool, foosball, air hockey, or shuffleboard game that\\'s played. Find out who really is number one (in your office, or out of your group of friends).\\nWhat is it?\\nWith TableChamp, you can add players, track every game that is played, and always know who\\'s #1.\\n\\nYou can view stats on each player, including their 20 most recent games:\\n\\nYou can manage all of the settings of the app in one convenient sidebar:\\n\\nYou can even select from one of 14 languages:\\n\\nHow does it work?\\nTableChamp is written entirely in JS/HTML/CSS. There is no back-end code (like python, or PHP). It uses FireBase as a back-end real-time DB to store all of the data, and manage the user authentication.\\nInstallation\\n1) You\\'ll need a hosting account for the JS/HTML/CSS files\\nNOTE: you can run a FireBase app locally, but you\\'ll need to follow these instructions to get set up with FireBase CLI.\\nJust clone this entire project to your server. Once you\\'ve done that, move on to step 2.\\n2) You\\'ll need to sign up for a free FireBase account\\n\\nEven if you have a large team, the free FireBase account should offer plenty of resources.\\nOnce you\\'ve signed up for a free FireBase account, move on to the next step.\\n3) Create a new FireBase app\\n\\nGo through the process of creating a new FireBase Project. You can name it \"Table Champ\", or anything you\\'d like.\\n\\nFind the \"Add to your web app\" option, and click it:\\n\\nYou now have all of the information you need to connect to connect the app to FireBase:\\n\\nOnce you have your FireBase API info, move on to the next step\\n4) Copy your FireBase info to the /js/firebase-key.js file\\nOpen up /js/firebase-key.js:\\n\\nPaste in the FireBase apiKey, authDomain, and databaseURL from step 3 above:\\n\\nOnce you\\'ve done this, save your changes, and move on to the next step.\\n5) Add your first FireBase user\\nFireBase handles storing all of your data, as well as authentication. We\\'ll need to set up a user in the FireBase admin, so that you can log into your app. I\\'ll walk you through how to add a single user, but you can add as many login users as you\\'d like.\\nNOTE: Users are separate from players. Users are set up in the FireBase admin, and have an email & password attached to them so that you can log in. Players are managed from the settings section once you\\'ve logged into your app.\\n\\nAll you need to enter to set up a user is an email, and a password.\\nOnce you\\'ve added your first user, continue to the next step.\\n6) Create a database instance\\nFrom your FireBase console, click into the Database section:\\n\\nCreate a new \"Real-time database\" (not a Firestore DB - note: they try and get you to create a Firestore DB by default).\\nOnce you\\'ve created your real-time DB, you\\'ll need to change the security rules. Click the \"Rules\" tab and and replace what\\'s there with the following:\\n{\\n  \"rules\": {\\n    \".read\": true,\\n    \".write\": true\\n  }\\n}\\n\\nHere\\'s what it should look like:\\n\\n7) Login, and add your players\\nNow you can log into your app for the first time. Go to the index.html file (wherever it\\'s being hosted from step 1 above). You should see:\\n\\nOnce you\\'ve logged in, you should see:\\n\\nEnter your organizations name, and the game you\\'ll be tracking:\\n\\nThen click on the Players tab:\\n\\nClick \"Add Players\" and enter the names of your players (one name per line):\\n\\nYou\\'re all set\\nYou should be ready to start tracking games:\\n\\n'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Yahoo! Fantasy API Node Module\\nThis is a node module created to wrap the Yahoo! Fantasy Sports API (link). At the moment, not all subresources are available, nor are any of the \\'collection\\' elements. I do hope to add them, and they have been added to the code, but as of now this project is very much in an open beta phase.\\nThe API is designed to act as a helper for those interacting with the Y! Fantasy API. The goal is for ease of use for the user, both in terms of querying endpoints and parsing responses. I\\'ve noticed that in working with the API, the data is not always the easiest to understand, so hopefully what I have created here will help people out.\\nInstallation\\nYou can install the module via npm by running:\\n$ npm install yahoo-fantasy\\n\\nLicence\\nThis module is available under the MIT Licence\\nDocumentation\\nMore complete documentation can be found using the application sandbox. This sandbox is always a work in progress, if I\\'ve learned anything it\\'s that nothing is ever complete.\\nThe API can be used by simply importing the module and querying data, since version 4.0 the authentication flow has been built into the library to make things easier for users.\\n// import the library\\nconst YahooFantasy = require(\\'yahoo-fantasy\\');\\n\\n// you can get an application key/secret by creating a new application on Yahoo!\\nconst yf = new YahooFantasy(\\n  Y!APPLICATION_KEY, // Yahoo! Application Key\\n  Y!APPLICATION_SECRET, // Yahoo! Application Secret\\n  tokenCallbackFunction, // callback function when user token is refreshed (optional)\\n  redirectUri // redirect endpoint when user authenticates (optional)\\n);\\n\\n// you can authenticate a user by setting a route to call the auth function\\n// note: from v4.0 on, public queries are now supported; that is, you can query\\n// public resources without authenticating a user (ie/ game meta, player meta,\\n// and information from public leagues)\\nyf.auth(\\n  response // response object to redirect the user to the Yahoo! login screen\\n)\\n\\n// you also need to set up the callback route (defined as the redirect uri above)\\n// note: this will automatically set the user and refresh token if the request is\\n// successful, but you can also call them manually, described below\\nyf.authCallback(\\n  request, // the request will contain the auth code from Yahoo!\\n  callback // callback function that will be called after the token has been retrieved\\n)\\n\\n// if you\\'re not authenticating via the library you\\'ll need to set the Yahoo!\\n// token for the user\\nyf.setUserToken(\\n  Y!CLIENT_TOKEN\\n);\\n\\n// you can do the same for the refresh token...\\n// if you set this and the token expires (lasts an hour) then the token will automatically\\n// refresh and call the above \"tokenCallbackFunction\" that you\\'ve specified to persist the\\n// token elsewhere\\nyf.setRefreshToken(\\n  Y!CLIENT_REFRESH_TOKEN\\n);\\n\\n// query a resource/subresource\\nyf.{resource}.{subresource} (\\n  {possible argument(s)},\\n  function cb(err, data) {\\n    // handle error\\n    // callback function\\n    // do your thing\\n  }\\n);\\n\\nStarting with v3.1.0 you can also use a promise chain to query resources and subresources\\nyf.{resource}.{subresource} (\\n  {possible argument(s)}\\n)\\n.then(data => // do your thing)\\n.catch(err => // handle error)\\n\\nThis also opens the door to use async/await in version of node that support it\\ntry {\\n  let data = await yf.{resource}.{subresource} (\\n    {possible argument(s)}\\n  )\\n\\n  // do your thing\\n} catch(err) {\\n  // handle error\\n}\\n\\nBugs & Issues\\nThis project is very much still a work in progress, please report any issues via the GitHub issues page.\\nChangelog\\n4.1.1\\n\\nSmall change to the way the resource and collection files are being imported as it was causing issues on some hosts...\\n\\n4.1.0\\n\\nMaybe would have made sense as a 5.0.0 as there may be breaking changes, but I haven\\'t been able to find any yet...\\nthe authCallback() function will now return an object with the user\\'s access_token and refresh_token\\nthe auth() function will accept a \"state\" string, allowing for state persistence through the authentication process\\nre-enabled the transactions.fetch() collection call\\ncleaned up the \"wavier_days\" and \"stat_categories\" objects on league resources\\nadded deprecation warnings to the game.leagues and game.players functions as they\\'re not very useful in that context\\n\\n4.0.0\\n\\nAdded auth(), authCallback, setRefreshToken() functions to the library\\nAutomatically handle refreshing of the token and call a user defined function when the token has expired\\nAdded support for public queries\\nGeneral cleanup\\n\\n3.2.0\\n\\nAdded \"players\" subresource to \"league\" in order to obtain weekly / season stats for a player based on league settings\\nFixed a bug where the starting status wasn\\'t properly being returned due to a shift in how the data was being returned\\nRemoved use of \"request\" library for size and performance reasons\\nGeneral code optimizations and improvements\\n\\n3.1.2\\n\\nUpdated outdated dependencies\\n\\n3.1.1\\n\\nResolve error when no team logo is present (Issue #42)\\n\\n3.1.0\\n\\nIntroduced promise based flow for all endpoints as an alternative to callbacks. Thanks Marois!\\n\\n3.0.4\\n\\nFixed a bug in the players.league collection call where it was trying to use split on an array... (Issue #46).\\nFixed similar bugs in other places...\\n\\n3.0.3\\n\\nAdded the ability to specify a date or week when querying the team.stats resource.\\nUnit test fixes (Issue #42). Thanks Marios!\\nUpdated \"vulnerable\" dependencies.\\n\\n3.0.2\\n\\nFixed an issue with the user.game_leagues resource, where the data was not at all user friendly (renamed leagues to games at the top level of the return object)\\n\\n3.0.1\\n\\nFixed some typos in some import statements which caused issues on some servers\\n\\n3.0.0\\n\\nMajor refactor to use ES6?... 2015? ...2018? Whatever the hell they\\'re calling it now...\\nUsing ES Modules (mjs) files where possible\\nRemoved transactions collections (they\\'ll be back!)\\n\\n2.0.4\\n\\nAdded a fix to give a cleaner value for the new \"batting order\" attribute in the player oject.\\n\\n2.0.3\\n\\nFixed a bug where the league players collection was not properly parsing the ownership subresource\\n\\n2.0.2\\n\\nFixed a bug where \"mapTeamPoints\" helper function was not defining \"self\". Thanks platky!\\n\\n2.0.1\\n\\nRemoved the code that added a \"reason\" to errors coming from Yahoo! as it was breaking other errors. Retry notifications should now be handled within the application using the module.\\n\\n2.0.0\\n\\nMoved to Yahoo!\\'s OAuth2.0 authentication mechanism.\\n\\n1.0.2\\n\\nFixed game resource roster postions callback bug.\\n\\n1.0.1\\n\\nFixed a typo that was breaking team mapping.\\n\\n1.0.0\\n\\nBreaking changes\\nFixed NFL scoreboard/matchups bug (Issue #19)\\nIn fixing this bug I realized that my \"team\" set up was really only useful for MLB fantasy, so I rewrote team mapping to work better across all sports and give additional details that weren\\'t previously reported. This will cause errors if you are using the team.manager attribute in your code.\\n\\n0.5.3\\n\\nFixed a bug where leagueFetch was throwing an error, thanks danielspector!\\n\\n0.5.2\\n\\nFixed a bug where player stats by week url was not being created properly, thanks withsmilo!\\n\\n0.5.1\\n\\nFixed a bug where collections that contained subresources would return no data.\\n\\n0.5.0\\n\\nAdded \"Transactions\" collection with functionality to add players, drop players, and add/drop players, thanks again githubsmilo!\\n\\n0.4.4\\n\\nFixed a bug in player.draft_analysis, thanks githubsmilo!\\n\\n0.4.3\\n\\nAdded weeks param for league.scoreboard\\nAdded weeks param for team.matchups\\nFixed a bug where individual players weren\\'t mapping properly\\nMinor code cleanup\\n\\n0.4.2\\n\\nAdded the ability to specify a date or week when querying the roster resource.\\nCleaned up the player normalization model\\nFixed a bug where the team.roster call was erroring\\n\\n0.4.1\\n\\nFixes to how POST data is handled\\n\\n0.4.0\\n\\nSignificantly restructured the code to have more consistency and set it up better for future plans, namely POST methods and proper unit testing\\nRemoved the \"refresh user token\" and instead return the error to the user who can handle the refresh within their application.\\n\\n0.3.1\\n\\nAdditional player attributes added, thanks ryus08!\\n\\n0.3.0\\n\\nAdded a method to refresh the user\\'s token if it has expired.\\n\\n0.2.2\\n\\nHotfix to fix \"Teams\" collection - use error first convention\\n\\n0.2.0\\n\\nMade helper classes more consistent\\nAdded collections for games, leagues, players, and teams\\nMoved to error first convention because JavaScript\\n\\n0.1.2\\n\\nAdded \\'Team Matchups\\' subresource\\nAdded \\'League Scoreboard\\' subresource\\nMinor code cleanup and improvements\\n\\n0.1.1\\n\\nRefactored module to fix a bug where user sessions were not necessarily unique because of require caching.\\n\\n0.1\\n\\nInitial release.\\n\\n'},\n",
       " {'language': 'JavaScript 97.0',\n",
       "  'readme': '#Angular Sports Ticker Directive\\n##What is this?\\nAn Angular directive that approximates a responsive (for screen sizes greater than 767px) sports news ticker (similar to a popular sports news network\\'s \"bottomline\").\\n##Why?\\nThis was a fun-side project I used to teach myself more about Angular and CSS animations. There are definitely some flaws and it\\'s certainly not as polished as it could be, but I\\'m putting it out there anyway.\\n##How do I use it?\\n\\nInclude the sportsTicker.js and sportsTicker.css in your document\\nAdd the \"sportsTicker\" module as a dependency in your Angular app\\nAdd the <sportsticker> tag to your page\\'s markup:\\n<sportsticker feed=\"feed\" message-delay=\"4000\" scroll-speed-factor=\"6.25\"></sportsticker>\\nHave an Angular controller provide a JSON \"feed\" to the directive (see feed.json for examples of all item types)\\n\\nSee the demo app for a complete example.\\n##Caveats:\\nThe ticker is hidden on mobile devices (i.e., for devices with a max-width of 767px), and performance on tablets is likely shaky at best.  I just didn\\'t think I could provide a nice mobile experience with the limited screen real estate, and this code isn\\'t optimized for the non-desktop experience (e.g., no hardware-acceleration on animations).  In fact, the code isn\\'t really optimized at all.  As I stated, this was just a side-project, and should not be viewed as battle-tested, production-ready code.\\nI don\\'t have any immediate plans to address these limitations, so feel free to clone and do with this what you wish, if anything.  Code is MIT licensed, so go crazy.\\n##Acknowledgements\\n\\nmodernizr\\nbackstretch\\nprefix-free\\nnormalize.css\\nvery cool faux NFL logos\\n\\n'},\n",
       " {'language': 'JavaScript 98.0',\n",
       "  'readme': 'Statistics and Data Analysis\\n\\n\\nMini javascript statistics library for nodejs or the browser.\\nNo production dependencies.\\nCurrent Library Coverage\\n\\nStandard Deviation\\nMean\\nMedian (sorts before calculating)\\nMedian Absolute Deviation (MAD)\\nOutlier Detection & Filtering using Iglewicz and Hoaglin\\'s method (MAD) - Use this if the order of your data does not matter.\\nOutlier Detection & Filtering using Median Differencing (Default method) - Use this if the order of your data matters. This looks at the difference between adjacent points best for time series data.\\n\\nNode.js / Browserify / ES6 module\\n$ npm install stats-analysis\\n\\nvar stats = require(\"./stats-analysis\") // include statistics library\\nBrowser\\n<script src=\"https://unpkg.com/stats-analysis\"></script>\\nwindow.stats\\nUsage\\nvar arr = [-2, 1, 2, 3, 3, 4, 15]\\n\\n//standard deviation\\nstats.stdev(arr).toFixed(2) * 1 // Round to 2dp and convert to number\\n> 4.98\\n\\n//mean\\nstats.mean(arr).toFixed(2) * 1\\n> 3.57\\n\\n//median\\nstats.median(arr)\\n> 2\\n\\n//median absolute deviation\\nstats.MAD(arr)\\n> 1\\n\\n// Outlier detection. Returns indexes of outliers\\nstats.indexOfOutliers(arr)  // Default theshold of 3\\n> [6]\\n\\nstats.indexOfOutliers(arr, 6) // Supply higher threshold to allow more outliers.\\n\\n// Outlier filtering. Returns array with outliers removed.\\nstats.filterOutliers(arr)\\n> [-2, 1, 2, 3, 3, 4]\\nTo use different outlier methods:\\nstats.filterOutliers(arr, stats.outlierMethod.medianDiff)\\nstats.filterOutliers(arr, stats.outlierMethod.medianDiff, 6) // Different threshold\\nstats.filterOutliers(arr, stats.outlierMethod.MAD) // Default\\n\\nstats.indexOfOutliers(arr, stats.outlierMethod.medianDiff)\\nstats.indexOfOutliers(arr, stats.outlierMethod.medianDiff, 6) // Different threshold\\nstats.indexOfOutliers(arr, stats.outlierMethod.MAD) // Default\\nDevelopment\\nMocha is used as the testing framework.\\nIstanbul and codecov used for code coverage.\\nCommands:\\n$ npm install   // Grab mocha\\n$ npm run lint  // Ensure code consistency with standard\\n$ npm test      // Run tests\\n$ npm run cov   // Run code coverage. (Ensure 100%)\\nResources\\nEngineering statistics handbook:\\nhttp://www.itl.nist.gov/div898/handbook/index.htm\\nContribute to the library\\n\\nFork it!\\nCreate your feature branch: git checkout -b my-new-feature\\nMake changes and ensure tests and code coverage all pass.\\nCommit your changes: git commit -m \\'Add some feature\\'\\nPush to the branch: git push origin my-new-feature\\nSubmit a pull request :D\\n\\nLicense\\nMIT\\n'},\n",
       " {'language': 'JavaScript 61.9',\n",
       "  'readme': 'Online-Courses-Learning\\nComputer Science\\nData Science\\n\\nIBM Data Science Professional Certificate - Coursera\\n\\nWhat is Data Science? - Coursera - Github\\nOpen Source tools for Data Science - Coursera - Github\\nData Science Methodology - Coursera - Github\\nPython for Data Science - Coursera - Github\\nDatabases and SQL for Data Science - Coursera - Github\\nData Analysis with Python - Coursera - Github\\nData Visualization with Python - Coursera - Github\\n\\n\\n\\nMachine Learning\\n\\nMachine Learning with TensorFlow on Google Cloud Platform - Google Cloud\\n\\nHow Google does Machine Learning\\nLaunching into Machine Learning\\nIntro to TensorFlow\\nFeature Engineering\\nArt and Science of Machine Learning\\n\\n\\n\\nProgramming Language\\n\\n\\nPython Programming Language\\n\\nPython for Everody - University of Michigan - Coursera\\n\\nProgramming for Everybody (Getting Started with Python) - Coursera - Github\\nPython Data Structures - Coursera - Github\\nUsing Python to Access Web Data - Coursera - Github\\nUsing Databases with Python - Coursera - Github\\nCapstone: Retrieving, Processing, and Visualizing Data with Python - Coursera - Github\\n\\n\\n\\n\\n\\nGo Programming Language\\n\\nProgramming with Google Go - University of California, Irvine - Coursera\\n\\nGetting Started with Go - Coursera - Github\\nFunctions, Methods, and Interfaces in Go - Coursera - Github\\nConcurrency in Go - Coursera - Github\\n\\n\\n\\n\\n\\nMATLAB Programming Language\\n\\nIntroduction to Programming with MATLAB - Vanderbilt University - Coursera - Github\\n\\n\\n\\nJavaScript Programming Language\\n\\nIntroduction to Computer Programming - University of London - Coursera - Github\\n\\n\\n\\nOperating System\\n\\nOpen Source Software Development, Linux and Git - The Linux Foundation - Coursera\\n\\nOpen Source Software Development Methods - Coursera - Github\\nLinux for Developers - Coursera - Github\\nLinux Tools for Developers - Coursera - Github\\nUsing Git for Distributed Development - Coursera - Github\\n\\n\\n\\nMechanical Engineering\\n\\nMechanics of Materials - Georgia Institute of Technology\\n\\nMechanics of Materials I: Fundamentals of Stress & Strain and Axial Loading - Coursera - Github\\nMechanics of Materials II: Thin-Walled Pressure Vessels and Torsion - Coursera - Github\\n\\n\\n\\nMathematics\\n\\nCalculus: Single Variable - University of Pennsylvania\\n\\nCalculus: Single Variable Part 1 - Functions - Coursera - Github\\nCalculus: Single Variable Part 2 - Differentiation - Coursera - Github\\nCalculus: Single Variable Part 3 - Integration - Coursera - Github\\nCalculus: Single Variable Part 4 - Applications - Coursera - Github\\n\\n\\n\\nRobotics\\n\\nAn Introduction to Programming the Internet of Things (IOT) - University of California - Coursera\\n\\nIntroduction to the Internet of Things and Embedded Systems - Coursera - Github\\nThe Arduino Platform and C Programming - Coursera - Github\\nInterfacing with the Arduino - Coursera - Github\\nThe Raspberry Pi Platform and Python Programming for the Raspberry Pi - Coursera - Github\\nInterfacing with the Raspberry Pi - Coursera - Github\\nProgramming for the Internet of Things Project - Coursera - Github\\n\\n\\n\\n'},\n",
       " {'language': 'JavaScript 74.5',\n",
       "  'readme': 'OnDataEngineering.net\\nWelcome to the content for the http://OnDataEngineering.net site\\nFor details on how to contribute please see http://OnDataEngineering.net/site/contributing/\\nThis content is licensed under a Creative Commons Attribution 4.0 International License. For more details please please see http://OnDataEngineering.net/site/content-license.\\nAll code in this repository (primarily to be found in the _includes and _layouts directory) is licensed under the Apache 2.0 licence.\\nServing this content locally\\nThis is entirely optional, but if you\\'d like to view the content in a browser and check pages have been correctly created and metadata correctly set then you can do this under either Windows, Linux or Mac as follows:\\n\\n\\nInstall the latest stable version of Ruby v2 from http://www.ruby-lang.org/en/downloads/.  Under Windows, you\\'ll also need to install the appropriate development kit - see http://jekyll-windows.juthilo.com/1-ruby-and-devkit/.\\n\\n\\nInstall the latest stable version of RubyGems from http://rubygems.org/pages/download\\n\\n\\nInstall the required ruby gems (including Jekyll) by running \"gem install bundler\" and then \"bundle install\"\\n\\n\\nStart the jekyll server via \"bundle exec jekyll serve\"\\n\\n\\nGo to http://localhost:4000 to view the site\\n\\n\\nNote that this is not the full OnDataEngineering site, but a cut down simplified version for the purposes of creating and checking content.\\nThere\\'s some basic checking of metadata included, which will show up as error messages at the top of the relevent page.  These can also be searched for with \"grep -r ERROR _site\"\\n'},\n",
       " {'language': 'JavaScript 95.7',\n",
       "  'readme': '\\n ****\\n\\n\\n\\n\\n\\n\\n\\nDexcalibur\\nDexcalibur is an Android reverse engineering platform focus on instrumentation automation. Its particularity is to use dynamic analysis to improve static analysis heuristics. It aims automate boring tasks related to dynamic instrumentation, such as :\\n\\nDecompile/disass intercepted bytecode at runtime\\nWrite hook code and Manage lot of hook message\\nSearch interesting pattern / things to hook\\nProcess data gathered by hook (dex file, invoked method, class loader, ...)\\nand so ...\\nBut not only that, because Dexcalibur has own static analysis engine and it is able to execute partial piece of smali.\\n\\nOfficial documentation is available here (website - work in progress).\\nSee the latest news here : http://docs.dexcalibur.org/News.html\\nShow Dexcalibur demo videos : Demo: Less than 1 minute to hook 61 methods ? Not a problem. (youtube)\\nHow to support Dexcalibur ?\\nContribute !\\nDon\\'t hesitate ! There are several ways to contribute :\\n\\nMake a pull request related to a fix or a new feature\\nCreate an issue to help me to patch/involves tools\\nHelp me to develop UI\\nSend me a mail with your feedback\\netc ...\\n\\nA. Installation\\nA.1 New install\\nEnsure following dependencies are installed :\\n\\nNodeJS 12.x LTS (upper non-LTS versions can cause issues during installation if there is not prebuilt binaries for frida-node, see Issue #27. Else, you can rebuild frida-node)\\nJava >= 8\\n\\nRun command:\\n$  npm install -g dexcalibur\\n\\nAnd start Dexcalibur with:\\n$  dexcalibur\\n\\nVisit http://127.0.0.1:8000 and follow instruction.\\nYour default port number 8000 is already in use ? Specify a custom port by using --port=<number> like $  dexcalibur --port=9999�\\nFill the form with the location of your workspace and default listening port, and submit it.\\nThe workspace will contain a folder for each application you reverse using Dexcalibur.\\n\\nDexcalibur will create the workspace if the folder not exists. A standalone version of android platform tools, APKtool, and more will be downloaded into this workspace.\\n\\nOnce install is done, restart Dexcalibur by killing it and doing (again)\\n$ dexcalibur\\n\\nFor more information, please visit intallation guide\\nOr use Docker (DEPRECATED) (See docker guide):\\nA.2 Update\\nFrom version <= 0.6.x\\nYou are using a previous version of Dexcalibur ?\\nFollow same steps than a new install, and when you should enter workspace path, enter your current workspace location.\\nFrom version >= 0.7\\nJust by doing:\\n$  npm install -g dexcalibur\\n\\nExisting configuration and workspace will be detected automatically.\\nC. Screenshots\\nFollowing screenshots illustrate the automatic update of xrefs at runtime.\\n\\n\\nD. Features and limitations\\nActually, the biggest limitation is Dexcalibur is not able to generate source code of hook targeting native function (into JNI library). However, you can declare manually a Frida\\'s Interceptor by editing a hook.\\nAssuming Dexcalibur does not provide (for the moment) features to analyse native part such as JNI library or JNA, only features and limitations related to Java part have been detailled.\\nAnalysis accuracy depends of the completeness of the Android API image used during early steps of the analysis. That means, if you use a DEX file generated from the Android.jar file from Android SDK, some references to internal methods, fields, or classes from Android java API could be missing. Better results are obtained when the analysis start from a \"boot.oat\" file extracted directly from a real device running the expected Android version.\\nD.1 Features\\nD.1.A Static analyzer\\nTODO : write text\\nD.1.B Hook manager\\nTODO : write text\\nD.1.C Dexcalibur\\'s smali VM\\nTracked behaviors\\nStatic analyzer involved into \"Run smali (VM)\" action is able to discover and accept but track following behaviors :\\n\\nOut-of-bound destination register (register out of v0 - v255)\\nOut-of-bound source register (register out of v0 - v65535)\\nDetect invalid instruction throwing implicitely an internal exception\\nDetect some piece of valid bytecode non-compliant with Android specification\\nCompute length of undefined array\\nFill undefined array\\nand more ...\\n\\nActually, handlers/listeners for such invalid instruction are not supported but events are tracked and rendered.\\nDexcalibur IR\\nThe VM produces a custom and simplified Intermediate Representation (IR) which is displayed only to help analyst to perform its analysis.\\nDepending of the value of the callstack depth and configuration, IR can include or not instruction executed into called function. If the execution enters into a try block and continues to return, but never excute catch, then the catch block will not be rendered. In fact the purpose of Dexcalibur IR is to render only \"what is executed\" or \"what  could be executed depending of some symbol\\'s value\" into VM context.\\nDexcalibur IR helps to read a cleaned version of bytcode by removing useless goto and opaque predicate. Dexcalibur IR can be generated by the VM with 2 simplifying levels :\\n1st level IR, could be used if you don\\'t trust 2th level IR  :\\n\\nno CFG simplifying : conditions and incondtionnal jumps are rendered.\\nevery move into a register are rendered\\n\\n2th level :\\n\\nHide assign if the register is not modified with an unknown value before its use.\\nAlways TRUE/FALSE predicate are removed\\nInconditional jump such goto are removed under certain conditions : single predecessor of targeted basic block, etc ...\\nResolve & replace Method.inoke() call by called method if possible.\\nInstructions into a Try block are not rendered if an exception is thrown before\\n...\\n\\nAndroid API mock\\nTODO\\nDetails\\nSmali VM follows steps :\\n\\nInit VM : stack memory, heap, classloaders, method area, ...\\nThe VM load class declaring the method.\\n(Optionnal) If the class has static blocks, clinit() is executed.  It helps to solve concrete value stored into static properties\\nLoad method metadata\\nExecute method\\'s instructions, if PseudoCodeMaker is enable, Dexcalibur IR is generated.\\n\\nHow VM handles invoke-* instruction ?\\n\\nWhen an invoke-* happens, the local symbol table is saved, and the invoked method is loaded.\\nIf the class declaring the invoked method  has never been loaded, the class is loaded\\nIf the method has never been loaded, the method is loaded (by MethodArea) and its local symbol table initialized by importing symbols of arguments from caller\\'s symbol table.\\nInvoked method is push into callstack (StackMemory).\\nMethod instruction are executed.\\nReturn is push into stack memory\\nCaller give flow control\\n\\nD.1.D Application Topology  analyzers\\nManifest analysis (LIMITED)\\nBefore the first run, the Android manifest of the application is parsed. Actually, anomalies into the manifest\\nsuch insecure configuration are really detected at this level.\\nThe only purpose of Android manifest parsing is to populate other kind of analyzers.\\nPermission analysis\\nEvery permissions extracted from the Manifest are listed and identified and compared to Android specification of the target Android API version.\\nDexcalibur provides - only in some case - a description of the permission purpose, the minimal Android API version, ...\\nActivities analysis\\nProviders analysis\\nServices analysis\\nReceivers analysis\\nD.1.E Runtime monitoring (not implemented)\\nNetwork monitoring\\nIntent monitoring\\nFile access monitoring\\nD.1.F Collaborating features\\nYou cannot find multi-user menu ? Not a probleme, there is not a menu but minimalistic collaborative work can be achieve.\\nDexcalibur runs a web server.  So, if several people are on the same network of this web server and if host firewall is well configured, you can be several to work on the same Dexcalibur instance.\\nActual limitations are :\\n\\nNo authentication : everybody into the network can send request to Dexcalibur instance and doing RCE the host through search engine.\\nNo identifying : modifying are not tracked, so, if someone rename a symbol, you could not know who renamed it. Similar case : you are not able to know who created a specific hook.\\nSingle device instrumentation : if several devices are connected to Dexcalibur\\'s host, and even if you can choose the device to instrument, instrumentation and hook messages are linked to the last device selected. So, you cannot generate instrumention for several devices simultaneously.\\n\\nE. Github Contributors\\nA special thanks to contributors :\\n\\nubamrein\\njhscheer\\neybisi\\nmonperrus\\n\\nF. Troubleshoots\\nF.1 Dexcalibur continues to start into \"install mode\"\\nBefore to go deeper :\\n\\nEnsure you are connected to Internet : Apktool and target platform are downloaded during install\\nDid you have tried to reinstall it by doing dexcalibur --reinstall command ? If no, try it.\\n\\nFirst, check if global settings have been saved into <user_directory>/.dexcalibur/\\n$ ls -la ~/.dexcalibur      \\n\\ntotal 8\\ndrwxr-xr-x   3 test_user  staff    96 29 avr 11:41 .\\ndrwxr-xr-x+ 87 test_user  staff  2784 29 avr 11:47 ..\\n-rw-r--r--   1 test_user  staff   204 29 avr 11:41 config.json\\n\\n\\n$ cat ~/.dexcalibur/config.json \\n\\n{\\n    \"workspace\":\"/Users/test_user/dexcaliburWS3\",\\n    \"registry\":\"https://github.com/FrenchYeti/dexcalibur-registry/raw/master/\",\\n    \"registryAPI\":\"https://api.github.com/repos/FrenchYeti/dexcalibur-registry/contents/\"\\n}\\n\\nNext, check if structure of Dexcalibur workspace is as following (content of /api folder may differs).\\n$ ls -la ~/dexcaliburWS/.dxc/*\\n/Users/test_user/dexcaliburWS/.dxc/api:\\ntotal 0\\ndrwxr-xr-x  3 test_user  staff   96 29 avr 11:41 .\\ndrwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..\\ndrwxr-xr-x  8 test_user  staff  256 29 avr 11:41 sdk_androidapi_29_google\\n\\n/Users/test_user/dexcaliburWS/.dxc/bin:\\ntotal 34824\\ndrwxr-xr-x   4 test_user  staff       128 29 avr 11:41 .\\ndrwxr-xr-x   7 test_user  staff       224 29 avr 11:41 ..\\n-rwxr-xr-x   1 test_user  staff  17661172 29 avr 11:41 apktool.jar\\ndrwxr-xr-x  18 test_user  staff       576 29 avr 11:41 platform-tools\\n\\n/Users/test_user/dexcaliburWS/.dxc/cfg:\\ntotal 8\\ndrwxr-xr-x  3 test_user  staff   96 29 avr 11:41 .\\ndrwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..\\n-rw-r--r--  1 test_user  staff  314 29 avr 11:41 config.json\\n\\n/Users/test_user/dexcaliburWS/.dxc/dev:\\ntotal 0\\ndrwxr-xr-x  2 test_user  staff   64 29 avr 11:41 .\\ndrwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..\\n\\n/Users/test_user/dexcaliburWS/.dxc/tmp:\\ntotal 0\\ndrwxr-xr-x  2 test_user  staff   64 29 avr 11:41 .\\ndrwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..\\n\\nG. FAQ\\nMy device not appears when into device list\\nIf you use a physical device connected over USB, ensure developper mode and USB debugging are enabled.\\nIf you use a virtual device, go to /splash.html,  select Device Manager,  click Connect over TCP ... and follow instructions. If you don\\'t know IP address of your device, let Dexcalibur detect it by checking box automatic configuration.\\nUSB debugging is enabled, but my device not appears when into device list\\n\\nConnect/disconnect USB and ensure your computer is allowed.\\nSelect file transfert\\n\\nWhy enroll a new device ?\\nYou need to enroll the target device before to be able to use it.\\nDuring enrollment Dexcalibur gather device metadata and push a compatible version of Frida server.\\nSuch metadata are used to select right frida-server and frida-gadget targets.\\nMy device is listed into Device Manager, but it cannot be enrolled\\nIf a red exclamation mark ! appears on a line into device list, then your desktop is not allowed by device. You probably need to confirm\\nIf your device is listed into DeviceManager and the column online is checked, then click enroll\\nG.1 My device is listed into Device Manager\\nIf your device is listed into DeviceManager and the column online is checked, then click enroll\\nHow to use an emulator instead of a physical device ?\\nDexcalibur version < v0.7 was not able to detect automatically emulated device and use it due to an incomplete ADB output parsing.\\nSince version >= v0.7, once your virtual device is running, go to /splash.html or click on DEXCALIBUR into navigation bar.\\nClick on Device Manager button into left menu, and click the Refresh button at top of array.\\nYou should have a row starting by the ADB ID of your virtual device.\\nHow to use a device over TCP ?\\nFirst, as any target device, you should enroll it.\\nClick Connect over TCP ... to add a new device over TCP or to connect an enrolled device over TCP.\\nIf the device has never been enrolled, so enrollment will be perform through TCP.\\nIn some case, connection over TCP is slower than over USB. So enrollement can take additional time.\\nIf the device has been enrolled over USB, so the new prefered transport type for this device becomes TCP.\\nHow to contribute to the dexcalibur ?\\nCreate a pull request on this repository or create an issue.\\nHow to contribute to the documentation?\\nCreate a pull request on dexcalibur-doc repository.\\nDocumentation is available at here (doc website) and here (wiki)\\nH. Sponsors\\n\\n\\n\\n\\n\\n\\n\\n\\nThey offered a license for All Products <3\\n\\n\\n\\nI. Resources\\nThere is actually few documentation and training resources about Dexcalibur. If you successfully used Dexcalibur to win CTF challenge or to find vulnerability, i highly encourage you to share your experience.\\n\\nSlides of Pass the SALT 2019 (PDF)\\nYoutube : demonstration\\nCLI User Guide\\nUser Guide\\nTroubleshoots\\nScreenshots\\n\\nJ. They wrote something about Dexcalibur\\n\\nAwesome Frida\\nAwesome OpenSource Security\\nn0secure.org - PassTheSalt2019 J2\\nrootshell.be - PassTheSalt2019 Wrap Up\\nPentesterLand - the 5 hacking newsletter 61\\nTechnology Knowledge Database\\nXuanwu Lab Security\\nMobile Gitbook\\n274 - AppsSec Ezine\\nysh329 / Android Reverse Engineering\\n\\n'},\n",
       " {'language': 'JavaScript 94.8',\n",
       "  'readme': 'Blockchain Now\\nAgenda: End-to-end pipeline for the bitcoin analytics. Reach UI with reactive search and charts.\\n\\nCourse project for Insight Data Engineering program\\nHow to install\\nTL;DR\\nGit clone the repository\\ngit clone git@github.com:igorbarinov/blockchainnow.git\\n\\nFrom meteorui/ start meteor server\\nmeteor\\n\\nOpen http://localhost:3000 in you favorite Google Chrome browser\\nWhen you will ready to publish to your hosting:\\nmeteor publish $(echo \"example\")\\n\\nchange example to any desired (and free) hostname in *.meteor.com domain\\ne.g. http://blockchainnow.meteor.com\\nTechnology Stack\\n\\nBitcoin Core Bitcoin Core\\nInsight API A bitcoin blockchain API for web wallets\\nApache Kafka A high-throughput distributed messaging system\\nkafka-node Node.js client with Zookeeper integration for Apache Kafka\\nLogstash Collect, Parse, Transform Logs\\nElastic Search Search & Analyze Data in Real Time\\nMeteor The JavaScript App Platform\\nMeteor Easy Search Plugin for Meteor\\n\\nLinks\\n\\nBlockchain Now \"Blockchain Now\" website\\nSlides\\nKafka Manager\\nElastic Sample Query\\n\\nImportant:\\nPlease star Awesome Data Engineering repository\\n'},\n",
       " {'language': 'JavaScript 88.4',\n",
       "  'readme': 'onexi.github.io\\nEngineering Computation & Data Science -\\n'},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'Data-Engineering-Google-Drive-Data-Pipeline\\nGoogle Drive Data Pipeline Automatically extracting, transforming and loading data from your Google Drive into your preferred data warehouse on a regular interval (up to a minute).\\nThis repo contains the main operators and the DAG to execute the Pipeline.\\n'},\n",
       " {'language': 'JavaScript 49.7',\n",
       "  'readme': 'BuggyGuiAngularPort\\nTelemetry system for the UPRM Moonbuggy Engineering team!\\nHere we collect a lot of rover data in a Database and visualize it to show the judges\\nThis project was generated with Angular CLI version 6.0.1.\\nDevelopment server\\nRun ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.\\nTo test the back-end portion of the server run ng build first. Then run npm start. (You should have Nodemon installed on your system npm install -g nodemon). Navigate to http://localhost:3000/ and you can test the requests using this address. App should reload if you change any server files.\\nIf there are changes in the angular files, you run ng build again to reload the server.\\nCode scaffolding\\nRun ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module.\\nBuild\\nRun ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.\\nRunning unit tests\\nRun ng test to execute the unit tests via Karma.\\nRunning end-to-end tests\\nRun ng e2e to execute the end-to-end tests via Protractor.\\nFurther help\\nTo get more help on the Angular CLI use ng help or go check out the Angular CLI README.\\n'},\n",
       " {'language': 'JavaScript 96.4',\n",
       "  'readme': 'Pupil\\nPupil is a tool for visualizing data from various software-engineering\\ntools. It works as a command-line tool run from a git-repository. When\\nrun, pupil will gather data from the repository and the GitLab\\nREST-API and persist it to a configured ArangoDB instance.\\nPupil then hosts interactive visualizations about the gathered data\\nvia a web-interface.\\nNaming\\n\"Pupil\" is the name used in this repository and also the name used in\\nthe source code to refer to itself. In the accompanying master\\'s\\nthesis, \"pupil\" is referred to as \"zivsed\". On the INSO projects page,\\nit is described as \"binocular\". Don\\'t get confused, its all the same\\nthing - naming is hard ¯\\\\_(ツ)_/¯.\\nDependencies\\n\\nnode.js >= 8\\nArangoDB (tested with 3.1.28)\\n\\nInstallation\\nPupil is not yet published on the npm registry. To install it, you\\nshould clone this repository and then link it:\\n$ git clone git@gitlab.com:romand/pupil.git\\n$ cd pupil\\npupil$ npm link    # <- this will make the `pupil` executable available in your $PATH\\nConfiguration\\nAs pupil needs to access an ArangoDB instance, you have to configure\\nthe database connection before you can use pupil. This can be done in\\nthe global pupil-configuration file ~/.pupilrc. Additionally, the\\nconfiguration file also stores authentication data for the consumed\\nREST-APIs. The configuration file is read by the rc\\nmodule. Check its documentation to\\nsee the supported formats. For the purpose of this README, we\\'ll use\\njson.\\nConfiguration options\\n\\ngitlab: Object holding gitlab configuration options\\n\\nurl: The URL to the GitLab-Instance you want to connect to. Use the\\nbase-url, not the API-URL (see the example)!\\ntoken: The personal access token generated by your GitLab user to\\nuse for authentication (see\\nhttps://docs.gitlab.com/ee/user/profile/personal_access_tokens.html)\\n\\n\\ngithub: Object holding github configuration options\\n\\nauth: Can hold any options that the [github npm-module] can take, check its documentation.\\n\\n\\narango: Object holding arangodb-configuration\\nhost: Hostname\\nport: Port\\nuser: username\\npassword: password\\n\\nA sample configuration file looks like this:\\n{\\n  \"gitlab\": {\\n    \"url\": \"https://gitlab.com/\",\\n    \"token\": \"YOUR_GITLAB_API_TOKEN\"\\n  },\\n  \"github\": {\\n    \"auth\": {\\n      \"type\": \"basic\",\\n      \"username\": \"YOUR_GITLAB_USER\",\\n      \"password\": \"YOUR_GITLAB_PASSWORD\"\\n    }\\n  },\\n  \"arango\": {\\n    \"host\": \"localhost\",\\n    \"port\": 8529,\\n    \"user\": \"YOUR_ARANGODB_USER\",\\n    \"password\": \"YOUR_ARANGODB_PASSWORD\"\\n  }\\n}\\nYou may override configuration options for specific projects by\\nplacing another .pupilrc file in the project\\'s root directory.\\nUsage\\nTo run pupil, simply execute pupil from the repository you want to\\nrun pupil on (you can try it on the pupil-repo itself!). Pupil will\\ntry to guess reasonable defaults for configuration based on your\\n.git/config. A browser window should pop up automatically with\\npupil\\'s web-interface showing the indexing progress and the\\nvisualizations.\\nContributing\\nFor an explanation of pupil\\'s architecture, please see the Contribution\\nguidelines for this project\\n'},\n",
       " {'language': 'JavaScript 51.4',\n",
       "  'readme': 'HIT-DataManage\\n提交1.0\\n提交名为“first commit”\\n内容\\n\\n添加了登录、注册方面的后台处理\\n\\n提交1.1\\n提交名为“login complete rough”\\n内容\\n\\n加入了登录身份验证的过滤器，能够过滤action和页面，未登录的用户无法访问目录下的资源且会重定向到登录界面;\\n与前台的登录和注册页面合并\\n\\n提交1.2\\n提交名为“filter improved”\\n内容\\n\\n完善了过滤器，当用户输入不存在的action时，跳转到默认action\\n\\n提交1.3\\n提交名为“homepage”\\n内容\\n\\n实现了登录后显示的主页\\n\\n提交1.4\\n提交名为“addPDO&Event”\\n内容\\n\\n实现了添加PDO的功能，添加事件(Event)还在继续完善\\n第二天应该能实现添加事件功能\\n\\n提交1.5\\n提交名为“completeAdd”\\n内容\\n\\n实现了添加事件功能\\n\\n提交1.6\\n提交名为“completeDelete”\\n内容\\n\\n实现了删除PDA和Event操作\\n\\n提交1.7\\n提交名为“excelimport”\\n内容\\n\\n实现了excel文件的导入操作\\n\\n提交1.8\\n提交名为“first_merge_master”\\n内容\\n\\n第一次合并总分支\\n\\n提交1.9\\n提交名为“fix some problems”\\n内容\\n\\n实现excel导入，事件删除，pdo删除\\n\\n提交2.0\\n提交名为“complete logout”\\n内容\\n\\n实现登出功能\\n\\n提交2.1\\n提交名为“complete some functions”\\n内容\\n\\n和前端合并\\n\\n提交2.2\\n提交名为“charset problems”\\n内容\\n\\n主要是尝试解决excel导入时候乱码以及数据库乱码问题\\n\\n提交2.3\\n提交名为“fix data format problem on both xls and xlsx”\\n内容\\n\\n终于解决了xls和xlsx文件中所有日期格式处理的问题\\n\\n提交2.4\\n提交名为“complete events counts”\\n内容\\n\\n实现对event个数的计数问题\\n\\n提交2.5\\n提交名为“merge with some pages”\\n内容\\n\\n和前端merge，实现搜索维度的PDO添加\\n\\n提交2.6\\n提交名为“just left search”\\n内容\\n\\n基本功能除搜索都实现了\\n\\n提交2.7\\n提交名为“complete all functions”\\n内容\\n\\n后台完成所有基本功能\\n\\n提交2.8\\n提交名为“improve the filter”\\n内容\\n\\n完善了过滤器\\n\\n提交2.9\\n提交名为“ready to middle-discuss”\\n内容\\n\\n和前端无缝连接，完成所有基本功能，准备中期答辩\\n\\n提交3.0\\n提交名为“fix a bg bug”\\n内容\\n\\n查询时关联数据出错，后台及时修改了\\n\\n提交3.1\\n提交名为“add a sum-up model”\\n内容\\n\\n在主页添加了一些统计信息\\n\\n提交3.2\\n提交名为“fix some bugs”\\n内容\\n\\n解决了一系列bug:\\n\\n注册页面：密码多次匹配，输入不能为空\\n添加事件页面：若有时间属性，改为日历\\n\\n\\n\\n'},\n",
       " {'language': 'JavaScript 97.4',\n",
       "  'readme': \"\\n\\n\\n\\n\\n\\n\\nAn exciting game of programming and Artificial Intelligence\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn WarriorJS, you are a warrior climbing a tall tower to reach The JavaScript\\nSword at the top level. Legend has it that the sword bearer becomes enlightened\\nin the JavaScript language, but be warned: the journey will not be easy. On each\\nfloor, you need to write JavaScript to instruct the warrior to battle enemies,\\nrescue captives, and reach the stairs alive...\\nNo matter if you are new to programming or a JavaScript guru, WarriorJS will\\nput your skills to the test. Will you dare?\\nPlay\\nGo to warriorjs.com and play from the comfort\\nof your browser! Sharpen your skills and compete against other players around\\nthe globe. Good luck in your journey, warrior!\\nDocumentation\\nAlthough there is some in-game documentation, at some point you may want to\\nvisit the official docs.\\nJump straight to some of the most-visited pages:\\n\\nGameplay\\nTowers\\nPlayer API\\n\\nCLI\\nWanna play offline? No problem, just follow these steps:\\n\\nInstall WarriorJS CLI with npm:\\n\\nnpm install --global @warriorjs/cli\\n\\nLaunch the game:\\n\\nwarriorjs\\n\\n\\nCreate your warrior.\\n\\n\\nYou'll be pointed to a README file with instructions for the first level.\\n\\n\\nCheck out the Install docs for\\nmore details.\\nPreview\\n\\n\\nWarriorJS CLI launched from the\\nIntegrated Terminal\\nin VS Code. To the left, Player.js, and to\\nthe right, a\\nMarkdown Preview\\nof README.md.\\n\\nContributing\\nWe welcome contributions to WarriorJS! These are the many ways you can help:\\n\\nSubmit patches and features\\nMake towers (new levels for the\\ngame)\\nImprove the documentation and website\\nReport bugs\\nFollow us on Twitter\\nParticipate in the Spectrum community\\nAnd donate financially!\\n\\nPlease read our contribution guide to get started. Also note\\nthat this project is released with a\\nContributor Code of Conduct, please make sure to review\\nand follow it.\\nContributors\\nThanks goes to each one of our contributors! 🙏\\nBecome a contributor.\\n\\nBackers\\nSupport us with a monthly donation and help us continue our activities!\\nBecome a backer.\\n\\nSponsors\\nBecome a sponsor and get your logo here and on the\\nofficial docs!\\nBecome a sponsor.\\n\\nAcknowledgments\\nThis project was born as a port of\\nruby-warrior. Credits for the original\\nidea go to Ryan Bates.\\nSpecial thanks to Guillermo Cura for designing a\\nwonderful logo.\\nLicense\\nWarriorJS is licensed under a MIT License.\\n\"},\n",
       " {'language': 'JavaScript 100.0',\n",
       "  'readme': 'TooAngel Artificial intelligence for screeps\\n\\n\\n\\n\\n\\n\\nhttps://screeps.com/\\nSee rendered version:\\nhttp://tooangel.github.io/screeps/\\nThis repository is World Driven. Pull Requests\\nare automatically merged and deployed to the\\nScreeps TooAngel account.\\nQuests\\nHead over to Quests\\nFor in game room visitors:\\nHappy to see you visiting one of our rooms. Visit FAQ to find answers\\nInfo\\nThis is the AI I\\'m using for screeps. I managed to reach Top 10\\nfrom November 2015 - March 2016. Main Goal is to automate everything, no\\nmanual interaction needed.\\nThe AI is deployable on a private screeps server, follow the information on\\nSteam or\\nnpm install screeps-bot-tooangel and bots.spawn(\\'screeps-bot-tooangel\\', ROOMNAME)\\nNote\\nThis is not a good example for code quality or structure, most LOCs written\\nwhile fighting or other occasions which needed quick fixes or in the ingame\\neditor (getting better :-)). But I think there are a couple of funny ideas.\\nEvery contribution is welcome.\\nFeatures\\n\\nAutomatic Base building\\nExternal room harvesting\\nBasic mineral handling\\nPower harvesting\\nNew rooms claiming on GCL level up\\nAutomatic attack\\nRebuild of fallen rooms\\nLayout visualization\\nManual commands\\nGraphs\\nTesting\\n\\nTweaking\\nAdd a src/friends.js with player names to ignore them from all attack\\nconsiderations.\\nE.g.:\\nmodule.exports = [\\'TooAngel\\'];\\nAdd a src/config_local.js to overwrite configuration values. Copy\\nconfig_local.js.example to src/config_local.js as an example. src/config.js\\nhas the default values.\\nDebugging\\nWithin the config_local.js certain config.debug flags can be enabled.\\nTo add debug messages Room.debugLog(TYPE, MESSAGE) and\\nCreep.creepLog(MESSAGE) are suggested. Especially the creepLog allows\\ngranular output of the creep behavior based on the room and the creep role.\\nUpload\\ninstall dependencies\\nnpm install\\n\\nadd your account credentials\\nto screeps.com\\nTo deploy to the live server provide the credentials.\\nvia env\\nexport email=EMAIL\\nexport password=PASSWORD\\n\\nvia git ignored file\\necho \"module.exports = { email: \\'your-email@here.tld\\', password: \\'your-secret\\' };\" > account.screeps.com.js\\n\\nor edit and rename account.screeps.com.js.sample to account.screeps.com.js\\nAnd deploy to the server:\\ngrunt\\n\\nto private server\\nCreate a .localSync.js file with content:\\nmodule.exports = [{\\n  cwd: \\'src\\',\\n  src: [\\n    \\'*.js\\'\\n  ],\\n  dest: \\'$HOME/.config/Screeps/scripts/SERVER/default\\',\\n}];\\n\\ngrunt local\\n\\nDevelop\\ngrunt dev\\n\\nRelease\\nReleasing to npm is done automatically by increasing the version and merging to master.\\nnpm version 10.0.1\\ngit push --follow-tags\\n\\nEvery deploy to master is automatically deployed to the live tooangel account.\\nTesting\\nnode utils/test.js will start a private server and add some bots as test\\ncases. Within in the tmp-test-server directory the server can be easily\\nstarted via screeps start.\\nDesign\\nMore details of the AI design\\n'},\n",
       " {'language': 'JavaScript 63.8',\n",
       "  'readme': '\\n\\n\\n\\ni.am.aiAI Expert Roadmap\\nRoadmap to becoming an Artificial Intelligence Expert in 2020\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBelow you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an ai expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.\\nIf you are interested to become an AI EXPERT at AMAI in Germany, or you want to hire an AI Expert, please say hi@am.ai.\\nNote\\n👉 An interactive version with links to follow about each bullet of the list can be found at i.am.ai/roadmap 👈\\nTo receive updates star ⭐ and watch 👀 the GitHub Repo to get notified, when we add new content to stay on the top of the most recent research.\\nDisclaimer\\nThe purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better suited for some cases than the other and remember hip and trendy never means best suited for the job.\\nIntroduction\\n\\n\\n\\n\\n\\nData Science Roadmap\\n\\n\\n\\n\\n\\nMachine Learning Roadmap\\n\\n\\n\\n\\n\\nDeep Learning Roadmap\\n\\n\\n\\n\\n\\nData Engineer Roadmap\\n\\n\\n\\n\\n\\nBig Data Engineer Roadmap\\n\\n\\n\\n\\n\\n🚦 Wrap Up\\nIf you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, we will continue to improve this, so you might want to watch/star this repository to revisit.\\n🙌 Contribution\\n\\nHave a look at the contribution docs for how to update any of the roadmaps\\n\\n\\nOpen pull request with improvements\\nDiscuss ideas in issues\\nSpread the word\\nReach out with any feedback\\n\\nSupported By\\n\\n\\n'},\n",
       " {'language': 'JavaScript 88.9',\n",
       "  'readme': '\\n\\n\\n\\ni.am.aiAI Expert Roadmap\\nRoadmap to becoming an Artificial Intelligence Expert in 2020\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBelow you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an ai expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.\\nIf you are interested to become an AI EXPERT at AMAI in Germany, or you want to hire an AI Expert, please say hi@am.ai.\\nNote\\n👉 An interactive version with links to follow about each bullet of the list can be found at i.am.ai/roadmap 👈\\nTo receive updates star ⭐ and watch 👀 the GitHub Repo to get notified, when we add new content to stay on the top of the most recent research.\\nDisclaimer\\nThe purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better suited for some cases than the other and remember hip and trendy never means best suited for the job.\\nIntroduction\\n\\n\\n\\n\\n\\nData Science Roadmap\\n\\n\\n\\n\\n\\nMachine Learning Roadmap\\n\\n\\n\\n\\n\\nDeep Learning Roadmap\\n\\n\\n\\n\\n\\nData Engineer Roadmap\\n\\n\\n\\n\\n\\nBig Data Engineer Roadmap\\n\\n\\n\\n\\n\\n🚦 Wrap Up\\nIf you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, we will continue to improve this, so you might want to watch/star this repository to revisit.\\n🙌 Contribution\\n\\nHave a look at the contribution docs for how to update any of the roadmaps\\n\\n\\nOpen pull request with improvements\\nDiscuss ideas in issues\\nSpread the word\\nReach out with any feedback\\n\\nSupported By\\n\\n\\n'},\n",
       " {'language': 'JavaScript 46.0',\n",
       "  'readme': \"Four legendary heroes were fighting for the land of Vindinium\\nMaking their way in the dangerous woods\\nSlashing goblins and stealing gold mines\\nAnd looking for a tavern where to drink their gold\\nWarning\\nThe vindinium dot org website has been discontinued, and the domain now belongs to Internet parasites.\\nInstallation\\nFeel free to install a local instance for your private tournaments.\\nYou need sbt, a MongoDB instance running, and a Unix machine (only Linux has been tested, tho).\\ngit clone git://github.com/ornicar/vindinium\\ncd vindinium\\ncd client\\n./build.sh\\ncd ..\\nsbt compile\\nsbt run\\nVindinium is now running on http://localhost:9000.\\nOptional reverse proxy\\nHere's an exemple of nginx configuration:\\nserver {\\n listen 80;\\n server_name my-domain.org;\\n\\n  location / {\\n    proxy_http_version 1.1;\\n    proxy_read_timeout 24h;\\n    proxy_set_header Host $host;\\n    proxy_pass  http://127.0.0.1:9000/;\\n  }\\n}\\n\\nDeveloping on the Client Side stack\\nwhile the Server runs with a sbt run, you can go in another terminal in the client/ folder and:\\n\\nInstall once the dependencies with npm install (This requires nodejs to be installed)\\nBe sure to have grunt installed with npm install -g grunt-cli\\nUse grunt to compile client sources and watch for client source changes.\\n\\nCredits\\nKudos to:\\n\\nvjousse for the UI and testing\\nveloce for the JavaScript and testing\\ngre for the shiny new JS playground\\n\\n\"},\n",
       " {'language': 'JavaScript 60.8',\n",
       "  'readme': 'Halite\\n\\n\\n\\nHalite is a AI programming competition. Contestants write bots to play an original multi-player turn-based strategy game played on a rectangular grid. For more information about the game, visit our website.\\nContributing\\nSee the Contributing Guide.\\nQuestions\\nSee the Forums and our Discord chat.\\nAuthors\\nHalite I was conceived of and developed by Ben Spector and Michael Truell in 2016. Two Sigma, having had a history of playful programming challenges for its mathematical and software-oriented teams (e.g., see the Robotic Air Hockey Competition) retained Ben and Michael as 2016 summer interns to develop Halite, run an internal Halite Challenge, and ultimately open Halite up to human and non-human coding enthusiasts worldwide. Halite I was a great success, developing a flourishing community of bot builders from around the globe, representing 35+ universities and 20+ organizations.\\nAs a result of the community’s enthusiasm, the Two Sigma team decided to create Halite II, an iteration of the original game with new rules but a similar structure and philosophy. With Ben and Michael as creative advisors, Halite II was developed by David Li, Jaques Clapauch, Harikrishna Menon, Julia Kastner as an evolution of Halite I.\\nThe team considered simply reviving Halite I, but given the progress the community made and the number of open source bots that had been published, the team decided to create Halite II with new game mechanics and a fun background story that fleshes out the Halite universe. Halite involved moving pieces around a board with only up-down-left-right options. In 2017’s Halite II, bots battle for control of a virtual continuous universe, where ships mine planets to grow larger fleets and defeat their opponents.\\n'},\n",
       " {'language': 'JavaScript 80.3',\n",
       "  'readme': 'aima-javascript\\nVisualization of concepts from Russell And Norvig\\'s \"Artificial Intelligence — A Modern Approach\", and Javascript code for algorithms. Unlike aima-python, aima-java, and other sibling projects, this project is primarily about the visualizations and secondarily about the code.\\n\\nView the visualizations\\nHow to contribute\\nChat on Gitter\\n\\nSome Javascript visualizations that I admire, and would like to see similar kinds of things here:\\n\\nRed Blob Games: A* Tutorial\\nQiao Search Demo\\nNicky Case: Simulating the World\\nRafael Matsunaga: Genetic Car Thingy\\nLee Yiyuan: 2048 Bot\\n\\n'},\n",
       " {'language': 'JavaScript 44.1',\n",
       "  'readme': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArtificial Intelligence as a Service\\n\\n\\nOrigami is an AI-as-a-service that allows researchers to easily convert their deep learning models into an online service that is widely accessible to everyone without the need to setup the infrastructure, resolve the dependencies, and build a web service around the deep learning model. By lowering the barrier to entry to latest AI algorithms, we provide developers, researchers and students the ability to access any model using a simple REST API call.\\n\\n\\nThe aim of this project is to create a framework that can help people create a web based demo out of their machine learning code and share it. Others can test the model without going into the implementation details. Usually testing models by other people involves a lot of preparation and setup. This project aims to cut that down.\\n\\n\\nThis app is presently under active development and we welcome contributions. Please check out our issues thread to find things to work on, or ping us on Gitter.\\nInstallation Instructions\\nWindows Installation\\nVirtualBox\\nOne of the easier ways to get started with Origami on Windows is by using a virtual machine of Ubunutu 16.04 LTS on Oracle\\'s VirtualBox. With Ubuntu installed, Origami can be installed by following the instructions in the next sections. We can install VirtualBox in just two easy steps.\\nStep One - Downloading Virtual Box\\nYou can install Virtual Box on Oracle\\'s VirtualBox website.\\n\\nNext, under \"Virtual binaries,\" click on Windows hosts under \"VirtualBox X.X.XX platform packages\" to download the executable file for the latest version of VirtualBox. Wait for this to install and open the file when the download has completed.\\nStep Two - Starting installation\\nThe .exe file will have the following format: VirtualBox-VersionNumber-BuildNumber-Win.exe.\\n\\nOnce the setup wizard is open, follow the instructions. Everything can be kept as default, but feel free to change anything to your preference.\\nIf you encounter a Windows User Account Control Warning pop-up, click Yes to accept and continue.\\nWhen you reach the Network Interface dialouge box, be sure to proceed. VirtualBox will install network interfaces that will interact with the installed your virtual machine(s) and Windows. You will be temporarily disconnected from the Internet, but this connection will be re-established.\\n\\nWhen you launch VirtualBox, you should see a screen similar to the one below. Congratualtions, you have successfully installed VirtualBox!\\n\\nSetting up an Ubuntu 16.04 LTS virtual machine with VirtualBox\\nOrigami works well on an Ubuntu 16.04 LTS virtual machine, which is what we will use when creating our virtual machine.\\nDisable Hyper-V\\nHyper-V is a tool that provides hardware virtualization, or allows virtual machines to run on virtual hardware. While this sounds useful, it can hamper your ability to use a 64-bit version of Ubuntu for your virtual machine. To avoid issues further along the line, we will disable this feature. We need to use a 64-bit virtual machine, as this is required for Docker, which will be used to install Origami.\\nNote: This will disable other applications that may require Hyper-V, such as Docker for Windows. You can always switch Hyper-V back on, but you will only be able to use VirtualBox or the other application(s) at a time.\\n\\nPress Windows Key + X and select Apps and Features\\n\\n\\n\\nUnder \"Related settings,\" select Programs and Features\\n\\n\\n\\nNext, click Turn windows features on or off on the left pane\\n\\n\\n\\nFind Hyper-V and unmark it\\n\\n\\n\\nFinally, click OK to save changes and reboot your computer\\n\\nInstalling Ubuntu inside Windows with VirtualBox\\nAlthough, below, we install Ubuntu 16.04 for Origami, this method can be used to install all other distributions of Ubuntu. Please be aware that you have at least a minimum of 512 MB of RAM on your computer, but keep in mind 1 or more GB is recommended.\\nStep One - Downloading the Ubuntu disk image (.iso file)\\nNavigate to this page to view the Ubuntu 16.04.5 LTS release page. Select the \"64-bit PC (AMD64) desktop image\" and save this for usage laters (install the 32-bit desktop image below the 64-bit option if you plan on using a 32-bit virtual machine).\\n\\nStep Two - Creating the New Virtual Machine\\nAfter installing the disk image, we will create the virtual machine on VirtualBox. Launch VirtualBox and select New to proceed. Type in \"Ubuntu\" into the \"Name:\" field of the New Virtual Machine Wizard pop-up. Conveniently, this should adjust the \"Type:\" and \"Version:\" fields automatically as needed.\\n\\nStep Three - Setting Base Memory (RAM)\\nVirtualBox will give a recommendation of how much memory (RAM) to allocate for your virtual machine. If you do not have much RAM, especially 1 GB or less, stick with VirtualBox\\'s recommendation. If you have ample RAM, try to stick to a quarter of your total RAM. If you do not know how much RAM you have, or as a matter of fact do not know what RAM is, stick with the recommendation.\\n\\nStep Four - Hard Disk\\nSince this is probably your first time using VirtualBox, create a new hard disk and then click \"Next.\"\\n\\nStep Five - Disk Type\\nLeave file type as \"VDI (VirtualBox Disk Image)\" and click \"Next.\"\\n\\nStep Six - Storage Details\\nA dynamically expanding virtual hard drive may be best, as it will only take up the space that you actually use on your virtual machine. However, there has been issues where the virtual hard drive fills up instead of actually expanding. Thus, it is recommended to pick \"Fixed size.\"\\n\\nStep Seven - Disk File Location and Hard Drive Size\\nAlthough Origami itself does not take up relatively much space, when installing Docker and other software, hard drive space can run low. Be sure to add as much hard drive space as you can, as it is a bit tedious to expand hard drive space after the virtual machine has been fully set up.\\n\\nStep Eight - Create the Virtual Hard Drive\\nSimply click \"Create\" from the dialouge box from the step prior and wait for the virtual hard drive to be created. As this is usually a large file, it may take a bit of time.\\n\\nStep Nine - Adding the Downloaded Ubuntu Disk Image\\nBefore we boot the virtual machine, we need to add the downloaded Ubuntu disk image (.iso file) onto the virtual machine. While your virtual machine is selected in the left pane, click Settings and then Storage. Next, under \"IDE Controller,\" select Empty and click on the little disk icon. In the menu, click Choose Virtual Optical Disk File... next to the folder icon.\\n\\nNavigate to the Ubuntu disk image file downloaded earlier and click \"Open.\"\\nNote: Both disk image versions for Ubuntu desktop are downloaded in the image below. As we are using a 64-bit virtual machine, we are opening the 64-bit .iso file.\\n\\nAfterward, \"Empty\" should now be replaced by the filename of our disk image file, and we can now click OK.\\n\\nStep Ten - Downloading Ubuntu onto your virtual machine\\nDouble-click your virtual machine to start it up. You may get various pop-ups providing warnings and instructions in regard to operating a virtual machine with VirtualBox. Be sure to read these, and you can mark not to see these again if you would like. Once Ubuntu is booted up, click Install Ubuntu and follow the instructions as if you were installing Ubuntu on an actual hard drive.\\n\\n\\n\\nInstalling Docker\\nWe use Docker to install Origami. As Origami runs well on Ubuntu, we recommend you follow the official Docker documentation here. Use the \"repository method\" for the installation of Docker CE on this site. CE stands for \"Community Edition,\" as is designed for developers and ordinary users. Make sure to install the latest version of Docker (skip step #3 on \"Installing Docker CE\"), and if you followed the tutorial above and created an Ubuntu virtual machine, follow the x86_64 architecture command when setting up the repository.\\nIf you are using MacOS, follow the instructions on Docker\\'s site here.\\nSetting the environment variables\\nRefer to the below during the installation process as needed.\\n\\norigami.env stores all the environment variables necessary to run Origami.\\n\\n\\nHOST should be set to the hostname of the server.\\nPORT should be set to the port you want the server to listen on.\\nDB_NAME will be used to set the name for your postgres database.\\nDB_PASS will be used to set the password for the database user. This is also the admin password.\\nDB_USER is the username for a user who can modify the database. This is also the admin username.\\nDB_USER_EMAIL stores the email for the admin.\\nDB_HOST should be set to postgres in production and localhost in development.\\nREDIS_HOST should be set to redis and localhost in development.\\n\\nTo create the file, cp origami.env.sample origami.env and edit the file with the above fields.\\n\\nOrigami/outCalls/config.js stores config variables needed by the UI.\\n\\n\\nCLIENT_IP should be set to the same value as HOST in origami.env\\nCLIENT_PORT should be set to the same value as PORT in origami.env\\nFor DROPBOX_API_KEY , check step 3 of configuring Origami\\n\\nProduction setup instructions\\nUse Docker to setup Origami on production\\nRunning the server\\nYou can run the server with the help of docker and docker-compose.\\nRun  $ docker-compose up\\nDevelopment setup instructions\\nThis application requires Pip, Node.js v5+, Yarn, and Python 2.7/3.4+ to install\\nInstalling Pip\\nIf you do not already have pip installed, run the following command:\\n$ sudo apt-get update\\n$ sudo apt get python-pip\\n\\nMacOS: $ sudo easy_install pip\\n\\nInstalling Node.js\\nTo install a stable and up-to-date version of Node.js, we will use Node\\'s PPA (Personal Package Archive). Keep in mind this is optimal for Linux Mint and Ubuntu operating systems. Please run the following commands to install the latest version of Node.js:\\n$ sudo apt-get update\\n$ sudo apt install curl\\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo bash -\\n$ sudo apt install nodejs\\n$ node -v\\n\\nVerify that your Node.js version is v5 or greater.\\n\\nInstalling Yarn\\nYarn helps install dependencies and other packages with ease. Here we will use npm (Node Package Manager) to install Yarn. As npm is installed with Node.js, be sure Node is already installed. Notice in the command that we include the -g flag for installation globally, so Yarn can thus be used in all of your projects.\\n$ sudo apt-get update\\n$ sudo npm install yarn -g\\n\\n\\nInstalling Python\\nFinally, we can install Python. Follow these commands to get the most up-to-date version of Python. If you would like a specific version of Python, be sure to include your preference after python (e.g. python 3.6 for Python 3.6). Ubuntu comes with Python installed, which is typically Python 2.7. Below, we install python3, and to use this, we would replace all python commands with python3. Below are the commands:\\n$ sudo apt-get update\\n$ sudo apt-get install python3\\n$ python3 --version\\n\\n\\nCreate a Virtual Environment\\n\\n$ pip install virtualenv\\n$ virtualenv venv venv = Name of virtualenv\\n$ source venv/bin/activate\\n\\nNote: Step 2 will create a folder named venv in your working directory\\nGetting the code and dependencies\\n\\nClone the repository via git\\n\\n$ git clone --recursive https://github.com/Cloud-CV/Origami.git && cd Origami/\\n\\nRenaming origami.env.sample as origami.env and setting environment variables in origami.env\\n\\n$ cp origami.env.sample origami.env\\n$ nano origami.env\\n\\nHere, set the environment variables according to the above instructions on environment variables. Once they have been edited, Ctrl O, Enter, and Ctrl X to save and exit. The following is an example of what this may look like (be sure to include localhost as the necessary values below if you are going to run Origami on your local machine).\\nset -a\\nHOST=localhost\\nPORT=8000\\nDB_NAME=origami546\\nDB_PASS=origami546\\nDB_USER=origami546\\nDB_USER_EMAIL=example@gmail.com\\nDB_HOST=localhost\\nREDIS_HOST=localhost\\nset +a\\n\\nAfterward, run the following to set more variables as entailed in the above section for environment variables for Origami/outCalls/config.js\\n$ nano Origami/outCalls/config.js\\n\\n\\n\\nAdd all of the Python dependencies.\\n$ pip install -r requirements.txt\\n\\n\\nSet up the Postgresql database\\n\\n\\nInstall postgresql if you have not already. The -contrib package will add more utilities and added functionality.\\nsudo apt-get update\\nsudo apt-get install postgresql postgresql-contrib\\n\\nNext we will create a database containing the details we will use for Origami. Following the previous example, creating the database may look like the following:\\n$ sudo service postgresql start\\n$ sudo -u postgres psql\\npostgres=# CREATE DATABASE origami546;\\npostgres=# CREATE USER origami546 WITH PASSWORD \\'origami546\\';\\npostgres=# ALTER ROLE origami546 SET client_encoding TO \\'utf8\\';\\npostgres=# ALTER ROLE origami546 SET default_transaction_isolation TO \\'read committed\\';\\npostgres=# ALTER ROLE origami546 SET timezone TO \\'UTC\\';\\npostgres=# ALTER USER origami546 CREATEDB;\\npostgres=# \\\\q\\n\\n\\n\\nAdd all of the Javascript dependencies\\n$ yarn (preferably)\\nor\\n$ npm install\\n\\n\\nSetup the Redis server\\n\\n\\n$ docker run -d -p 6379:6379 --name origami-redis redis:alpine\\n\\n\\nActivate the environment\\n$ source origami.env\\n\\n\\nSetting up the database\\nCreate all the tables\\n$ python manage.py makemigrations\\n$ python manage.py migrate\\n\\nCreate admin account\\n$ python manage.py initadmin\\nStart the server\\nTo ensure everything works out, follow these steps carefully. Make sure all three terminals are running at the same time.\\n\\nStart the server by\\n\\n$ python manage.py runserver --noworker\\n\\nStart the worker\\n\\nOpen a second terminal and run the following:\\n$ source venv/bin/activate\\n$ cd Origami/\\n$ source origami.env\\n$ python manage.py runworker\\n\\n\\nRunning the server with Yarn\\n\\nOpen a third terminal and run the following:\\n$ source venv/bin/activate\\n$ cd Origami/\\n$ source origami.env\\n$ yarn run dev\\n\\n\\nGo to localhost:8000\\nVisit Read the docs for further instructions on getting started. If you have never created an OAuth App on GitHub, see the below instructions.\\n\\nSetup Authentication for Virtual Environment\\n\\n\\nGo to Github Developer Applications and create a new application here.\\n\\n\\nFor local deployments, use the following information:\\n\\nApplication name: Origami\\nHomepage URL: http://localhost:8000\\nApplication description: Origami\\nAuthorization callback URL: http://localhost:8000/auth/github/login/callback/\\n\\n\\n\\nGithub will provide you with a client ID and secret Key, save these.\\n\\n\\nStart the application.\\n\\n\\n$ python manage.py runserver\\n\\n\\n\\nOpen http://localhost:8000/admin\\n\\n\\nLogin with the credentials from your admin account. This should be your username and password you used for the Postgresql if everything was kept consistent.\\n\\n\\nFrom the Django admin home page, go to Sites under the Sites category and make sure \"localhost:8000\" is the only site listed under \"DOMAIN NAME\".\\n\\n\\nContributing to Origami\\n\\n\\nMake sure you run tests on your changes before you push the code using:\\n\\n$ python manage.py test\\n$ yarn run test\\n\\n\\n\\nFix lint issues with the code using:\\n\\n$ yarn run lint:fix\\n\\n\\n\\nLicense\\nThis software is licensed under GNU AGPLv3. Please see the included License file. All external libraries, if modified, will be mentioned below explicitly.\\n'},\n",
       " {'language': 'JavaScript 54.8',\n",
       "  'readme': 'UnityAI\\nReusable Artificial Intelligence Experiments\\n\\n\\nCurrent Features\\n\\nPathfinding editor for waypoints and pathing visualization\\nA* Route Planning\\nRandom waypoint navigation\\nWaypoint to waypoint following using Pathfinder\\nNavigation mesh processor in the Tools Menu (creates a NavmeshNode network with triangles and vertices from a selected mesh)\\nPathing for navigation meshes\\nFunnel algorithm for navigation meshes\\nSteering behavior for path following\\nA basic FPS with path following spiders, ammo, and health spawned by an AI director\\n\\nScript Locations\\nThis project includes 2 Unity projects:\\n\\nUnityPathing (a sandbox project for pathfinding experiments)\\nBasicGame (the FPS demo with pathfinding and an AI director)\\n\\nPlanning scripts are located in BasicGame/Assets/Pathfinding Scripts and gameplay/director scripts are located in BasicGame/Assets/Scripts\\n##License: MIT\\nCopyright (c) 2013 Julian Ceipek, Alyssa Bawgus, Eric Tappan, Alex Adkins\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'},\n",
       " {'language': 'JavaScript 77.7',\n",
       "  'readme': '\\n  TimeMap v0\\n\\n\\nTimeMap is a tool for exploration, monitoring and classification of incidents in time and space.See a live instance here.\\n\\n\\n\\nOverview\\nTimeMap is a standalone frontend application that allows to explore and monitor events in time and space. TimeMap uses OpenStreetMap satellite imagery as a backdrop by default, but can also be configured to use mapbox. It uses Leaflet and d3 to visually map information.\\nThe recommended way to run a backend for timemap is using datasheet-server. This allows you to work with a spreadsheet or Google Sheet as a dynamic database for for timemap.\\nTimeMap has the following high-level features capabilites:\\n\\nVisualize incidents of particular events on a map.\\nVisualize and filter these incidents over time, on an adjustable timeline that allows to zoom in and out.\\nVisualize types of incidents by tag and by category, which can be displayed using different styles.\\n\\nA fully-functioning live version can be found as a result of the Forensic Architecture investigation of the Battle of Ilovaisk.\\nGet up and running\\nThese easiest way to get up and running with timemap and datasheet-server is to\\nfollow the tutorial here.\\nInstructions\\n\\nPull this repository.\\n\\ngit clone https://github.com/forensic-architecture/timemap\\n\\nInstall dependencies via yarn (recommended, it\\'s just faster) or npm.\\n\\nyarn          # npm install\\n\\nRun it via yarn.\\n\\nyarn dev      # npm run dev\\nTo run with a file that is not \\'config.js\\' in the root directory, set the CONFIG environment variable:\\nCONFIG=\"myotherconfig.js\" yarn dev\\n\\nIMPORTANT: Although the application will run just like that, in order for TimeMap to be able to display interesting information, you\\'ll have to make sure to have the capacity to serve data, as well as adjusting some configuration parameters. See next section.\\nRunning without datasheet-server\\nTechnically, timemap is backend agnostic, but it requires a series of endpoints to provide data for it to visualize. The data is expected in JSON format. Some data elements are required and their format has some required fields. Other additional endpoints are optional, and if enabled, they simply add features to your taste.\\nThe combination of all these data types is called the domain of the application in the context of TimeMap.\\nContribute\\nCode of Conduct\\nPlease read before contributing. We endeavour to cultivate a community around timemap and other OSS at Forensic Architecture that is inclusive and respectful. Please join us in this!\\nContributing Guide\\nLearn more about our development process,  i.e. how to propose bugfixes and improvements.\\nCommunity\\nIf you have any questions or just want to chat, please join our team fa_open_source on Keybase for community discussion. Keybase is a great platform for encrypted chat and file sharing that we use as a public forum.\\nLicense\\ntimemap is distributed under the DoNoHarm license.\\n'},\n",
       " {'language': 'JavaScript 95.2',\n",
       "  'readme': 'This repository is a collection of recent experiments I\\'ve been working on in three.js.\\nThree.js is a JavaScript library built on top of the WebGL graphics language.  WebGL is a low level, verbose language used to create graphics in the browser that can be both very performant and very hard to use.  Three.js greatly reduces the amount of boilerplate code you have to write to build rich 3D graphics, and wraps common operations into intuitive constructor functions.  If you\\'re interested in learning three.js, I recently completed two new tutorials on getting started with the three.js library.  You can find them at loftus.xyz\\nYou will find the different simulations in this repo in the examples folder.  To develop and test out the simulations locally, first clone the repo down to your local machine.\\ngit clone https://github.com/MattLoftus/threejs-space-simulations.git\\n\\nTo avoid cross-origin errors when using textures(every example in this repo), you will need to host the files on a local server.  I recommend using python simple server or npm live server.  First navigate to the root of the directory, then run the following command.\\npython -m SimpleHTTPServer\\n\\nThis will host the folder on port 8000, so you can head over to the browser and type \"localhost:8000\" into the address bar, and you will see a listing for the directory. If you happen to have a version of python on your machine >= 3.0, you may need to run the following command instead.\\npython2.7 -m SimpleHTTPServer\\n\\n'},\n",
       " {'language': 'JavaScript 67.1',\n",
       "  'readme': 'The Kabal Invasion\\nThe Kabal Invasion is a web-based 4X space game. It is coded in PHP/HTML/JS/SQL.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is it?\\nA web based space exploration (4x) game based on the old BBS door games that went\\nby many names (Tradewars, Galactic Warzone, Ultimate Universe, and\\nmany other games like this) but shares no code with them.  It is\\nwritten 100% in PHP/HTML/JS/SQL.\\n\\nWhy should I run this?\\nWeb-based games that recreate the door game BBS experience can be fun!\\nSince it is Free and open source software, anyone can examine, learn, and contribute.\\n\\nIs this game ready to install and play?\\nAt the moment, we\\'ve identified a number of release-blocking issues including broken\\npassword management, user sign-up, and issues with non-functional database calls. Serious\\neffort is underway to fix all of these issues, and we are working towards a release. In the meantime,\\ncurious developers are encouraged to download, install, and play as the admin user to find issues\\nand report them. When we get to a point where the game is stable for players,\\nwe will make an announcement, change this note, and release!\\n\\nLicense: Affero GPL v 3\\nCredits:\\nThe Kabal Invasion forked from Blacknova Traders, please visit their sourceforge page for more information about their project. We proudly stand on the shoulders of giants, with BNT originally having hundreds of developers, players, and admins. We honor and appreciate their 15+ year contribution that makes our project possible.\\nRequirements:\\nServer (generally, the most recent/current version of each is our recommendation, but these should suffice):\\n\\nA Linux server. Our primary development platform is Fedora, but most Linux distributions should work, and potentially even OpenBSD.\\nA webserver capable of TLS such as apache v2.4+ (we have not determined a required minimum).\\nphp v7.4.5+ (needed for type-hinting property types).\\nmariadb v5.5+ or v10.0+ (needed for utf8mb4 schemas).\\npdo PHP extension.\\n\\nWeb:\\n\\nChrome v50+ or Firefox v40+ (recommended).\\nSafari v9.1.2+.\\nIE v11.\\n\\nNotes:\\n\\nTKI will likely run on lighttpd and nginix, however htaccess will not work out of the box - potentially causing security risks. It has not been tested on either.\\nIIS and/or Windows is NOT supported, please do not ask! (But we welcome code to make it work on IIS)\\nDevelopment \"Snapshots\" are intended only for developers that are actively involved in the development process, and require additional effort to work (composer, etc).\\nWe make use of Smarty templates, HTML Purifier, Swiftmailer, and Adodb (although we are working to replace adodb with PDO).\\n\\nInstallation:\\nPlease see the /docs/install.md file.\\nUpgrades:\\nAs is typical with our releases, we highly recommend a fresh install. Upgrades are not supported at this time.\\nCode quality:\\nThe project began in the early PHP4 era, and as a result, is less than ideal. Substantial progress has been made towards modernization, and we are continuing that process. As a general guideline, we follow PSR-1,2,4, and the upcoming 12, with the major exceptions that we use BSD/Allman brace/brackets and do not yet follow line length limits. Feedback and PR\\'s are welcome and appreciated.\\nCritical needs:\\nThe two areas we need the most focus in would be the documentation, and testing. Both can be done with little or no knowledge of PHP, and would help us dramatically.\\nSecurity issue reporting:\\nPlease report all security issues to thekabal@gmail.com.\\nContributing:\\nFeel free to contribute to the project! We use Gitlab for our issues tracking (provide feedback!), milestone planning, code storage, and releases.\\nI hope you enjoy the game!\\nThe Kabal\\n'},\n",
       " {'language': 'JavaScript 86.9',\n",
       "  'readme': 'VAE Latent Space Explorer\\nThis application is a toy visualization that allows you to generate new images of 28x28 numerical digits using a variational autoencoder.\\nYou can view the visualization here\\nImplementation details\\nThe variational autoencoder was implemented using Keras and the relevant code is located in the scripts directory in a Jupyter notebook.\\nOnce the model is trained, the architecture and weights are saved in a format that can be ingested by tensorflow-js. Tensorflow-js handles\\nimplementing the model architecture and loading the weight in the browser. By leveraging WebGL, the model can efficient generate new image samples when given an appropriate latent space vector in the client browser.\\nThe application uses React.js for interface updates along with html Canvas to draw the image matrices. D3.js was used to generate the scatterplot and handle hover events.\\nCreated by Taylor Denouden (April 2018)\\n'},\n",
       " {'language': 'JavaScript 99.0',\n",
       "  'readme': \"SEH - Space Exploration History\\nThe Solar System\\nBehold: the entire history of Solar System exploration in one graphic.\\nShowing all missions beyond Earth orbit send to explore Solar System objects since 1958.\\nSome more documentary listings/garaphics:\\nGround Segment Map overwiew\\nMissions, Destinations, Launch sites Lists\\nRockets in one graphic in order of appearance\\nSpace Telescopes\\nExtensive description here\\nSpectral Range of all operational and future space telescopes and some ground based examples for comparison\\nSpectral Range vs. Angular Resolution for operational scopes\\nThe apps require a HTML5-capable browser, so all fairly new versions of Chrome, Firefox, IE, Opera and what-have-you should work. If your browser only shows a static image, it is too old.\\nSources\\nThese sites (and books) helped me greatly to gather all the data represented here:\\nNational Space Science Data Center (NSSDC)\\nNASA History Astronautics and Aeronautics Chronology Series\\nSolar System Exploration Mission Profiles\\nJPL Mission and Spacecraft Library\\nEncyclopedia Astronautica, Mark Wade\\nJonathan's Space Report, Johnathan McDowell\\nSpaceflight Realtime Simulations and Information, Daniel Muller\\nEarth Observation Portal, ESA\\nGunter's Space Page, Gunter Krebs\\nJohnston's Archive, Robert Johnston\\nThe Planetary Society, Emily Lakdawalla et.al.\\nVenera: The Soviet Exploration of Venus, Don P. Mitchell\\nRussian Space Web, Anatoly Zak\\nSpaceflight 101, Patrick Blau\\nZarya Soviet, Russian and International Space Flight, Robert Christy\\nVisual Satellite Observing FAQ\\nWeebau Space Encyclopedia, Pierre Bauduin\\nand of course all the mission websites linked in the app above.\\nImages\\nThe Planetary Society, planet images\\nPlanetary Maps, Steve Albers\\nNASA Visible Earth\\nMoon shaded relief\\nMars MOLA map\\nDr. Paul Schenk (Neptune Rings)\\nHistoric Spacecraft, Richard Kruse (Rocket images)\\nSolar System Data\\nPlanetary Fact Sheets (NSSDC)\\nSolar System Dynamics (JPL)\\nThe Astronomical Almanac Online (USN)\\nGazetteer of Planetary Nomenclature (USGS)\\nMinor Planet Center (IAU)\\nPlanetary Data System (PDS) - Rings node -\\nAtmospheres node\\nBooks\\nDeep Space Chronicle, Asif Siddiqi, NASA History Monograph SP 2002-4524\\nSoviet Robots in the Solar System, Wesley T. Huntress, Jr. & Mikhail Ya. Marov, Springer 2011\\n--\\nSpacecraft positions algorithm with great help from Project Pluto\\nReleased under BSD License\\n\"},\n",
       " {'language': 'JavaScript 84.8',\n",
       "  'readme': 'SpaceApps2020 WINNER - Space Nearby\\n\\nChallenge\\nAn interactive webapp using Google Maps API to reveal locations involved in the space exploration industry nearby!\\nNote: Google API keys have been temporarily disabled.\\nDevelopers:\\nJared Bentvelsen\\n\\n\\nBassel Rezkalla\\n\\n\\nMatthew McCracken\\n\\n\\nChristopher Andrade\\n\\n\\nYuvraj Randhawa\\n\\n\\n'},\n",
       " {'language': 'JavaScript 98.4',\n",
       "  'readme': 'Space exploration web app\\nhttp://compact.github.io/space/\\nThis app allows you to navigate through space in your browser. It uses three.js to render astronomical bodies.\\nWorkflow\\nDevelop\\nnpm install\\nbower install\\nnode app.js\\n\\nGo to http://localhost:3001.\\nBuild\\nnpm install -g grunt\\ngrunt build\\n\\nThe built files are located in /dist/.\\nGenerate docs\\nnpm install -g jsdoc\\njsdoc -p -d app/docs app/scripts/ app/data/\\n\\n'},\n",
       " {'language': 'JavaScript 99.6',\n",
       "  'readme': 'Space exploration web app\\nhttp://compact.github.io/space/\\nThis app allows you to navigate through space in your browser. It uses three.js to render astronomical bodies.\\nWorkflow\\nDevelop\\nnpm install\\nbower install\\nnode app.js\\n\\nGo to http://localhost:3001.\\nBuild\\nnpm install -g grunt\\ngrunt build\\n\\nThe built files are located in /dist/.\\nGenerate docs\\nnpm install -g jsdoc\\njsdoc -p -d app/docs app/scripts/ app/data/\\n\\n'},\n",
       " {'language': 'JavaScript 68.8',\n",
       "  'readme': 'Wandering Lines\\nAn exploration of lines wandering across a space. Inspired from the work of Anders Hoff, @inconvergent.\\nRunning locally\\nMake sure npm is installed, download this repo, then from the folder in the command line run npm install to install the dependencies, then run npm run dev:1 to start a local reload server. Then to finish and build the final files run npm run build:1. Replace the 1 in dev:1 and build:1 to run and build each version in the series.\\n'},\n",
       " {'language': 'JavaScript 99.9',\n",
       "  'readme': \"Rogue Starfarer\\nRogue Starfarer is interstellar exploration roguelike created for the 2018 7DRL competition.\\nHow to Build\\nJust open up index.html in your browser.\\nHow to Play\\nThe green 'X' indicates the position that your ship will move to in the next turn, based on your momentum. You can fire your rockets to plot your course by moving the green 'X' with the arrow keys or numpad. The '0' indicates your course if you make no adjustments. Press space to end the turn.\\nBeware that the amount you can adjust your ship's course in any one turn is limited by the energy reserves and propulsion of your ship! The Maneuver: -3/Δ on the sidebar indicates that adjusting your trajectory will cost you 3 energy. The more weapons and armor you add to you ship, the more it will cost to maneuver, and the more you upgrade your engines, the less it will cost.\\nThe green arrow indicates the the direction your ship is currently facing. Your ship's facing changes automatically based on your trajectory.\\nWeapons are mounted on particular hull zones and each weapon can only fire out of that hull zone. Each weapon has its own firing arc determined by its range, hull zone, and your ship's facing. Press 'w' to cycle through weapons, press 'tab' to cycle through eligible targets within the weapon's arc, and press 'f' to fire. You can also target and fire on ships using the mouse.\\nExploration\\nThe universe is filled with strange events and mysterious sites to explore. You can land on any planet by flying over it and coming to a stop. If you collide with a planet while traveling faster than speed 1, you will suffer damage. You can board a destroyed or abandoned ship by flying over it. Note that destroyed ships will continue on their original trajectories, so you may find that a tractor beam is helpful to bring them to a stop.\\nYou will also find perplexing anomalies in space and merchant stations that can repair and refit your ship. You can interact with these by flying over them.\\nShip Systems\\nReactor and Energy Storage\\nYour ship has a state-of-the-art fusion reactor that continously produces energy which is stored in your ship's capacitor banks. Maneuvering and firing weapons consumes energy. If your current energy is at least 50% of maximum, your shields and warp core will slowly recharge. In normal circumstances your energy reserves will never exceed your maximum. However, if any special item or effect raises your energy reserves above twice your maximum, your reactor will overload and begin to melt down, dealing 1 point of hull damage per turn.\\nCrew\\nYour ship has a minimum and maximum crew. For every crewmember you have above the minimum, there is a chance every turn that you will repair 1 point of hull damage. For every crewmember you have below the minimum, there is a chance that your reactor will produce no energy that turn.\\nWarp Core\\nYour ship has a warp core which allows it to jump between star systems. To do so you must have the hyperspace coordinates for the new system and your warp core must be fully charged (20/20). Your warp core will recharge 1 point per turn while your ship's energy reserves are above 50%. Jump to hyperspace by pressing the 'j' key.\\nShields\\nYour shields reduce damage from most types of attacks. Your shields will naturally recharge when you are above 50% of your maximum energy.\\nWeapon Types\\nThere are four main types of weapons: lasers, ion cannons, tractor beams, and neutron beams. Lasers deal damage to shields first, and to hull second. Ion cannons deal damage to shields first and to energy reserves second. Note that ion cannons can reduce a ship's energy reserves to negative levels. Tractor beams reduce a ship's speed, but only work if the target's shields are already down. Neutron beams deal damage directly to a ship's crew, but like tractor beams they are completely blocked by any amount of shields.\\nEach weapon can only fire once per turn, and only if sufficient energy is available. Mounting a weapon increases your ship's mass (increasing your maneuver cost) and increases your minimum crew requirement. Beyond this, there are no restrictions to the number of weapons you can affix to a hull zone.\\nRumors of persist of ancient precusor relics that are far more powerful than the standard types of weapons listed above.\\nBoarding Tubes\\nBoarding tubes can bridge the vacuum of space and slice through even the thickest armor. If you and an enemy ship are adjacent, and either ship is at 0 speed, a boarding action will be initiated. Your crewmemebers will fight directly, though an entire shipboard battle can rarely be resolved in a single turn.\\n\"},\n",
       " {'language': 'JavaScript 52.8',\n",
       "  'readme': \"Opentrons Platform\\n\\n\\n\\n\\nOverview\\nOpentrons API\\nOpentrons App\\nContributing\\n\\nOverview\\nOpentrons makes robots for biologists.\\nOur mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other's work. Our robots automate experiments that would otherwise be done by hand, allowing our users to spend more time pursuing answers to the 21st century’s most important questions, and less time pipetting.\\nThis repository contains the source code for the Opentrons API and OT App. We'd love for you to to explore, hack, and build upon them!\\nOpentrons API\\nThe Opentrons API is a simple framework designed to make writing automated biology lab protocols easy.\\nWe've designed it in a way we hope is accessible to anyone with basic computer and wetlab skills. As a bench scientist, you should be able to code your automated protocols in a way that reads like a lab notebook.\\npipette.aspirate(location=trough['A1'], volume=30)\\npipette.dispense(location=well_plate['A1'], volume=30)\\nThis example tells the Opentrons OT-2 to pipette 30 µL of liquid from a trough to well plate. Learn more here:\\n\\nDocumentation\\nSource code\\n\\nOpentrons App\\nEasily upload a protocol, calibrate positions, and run your experiment from your computer.\\n\\nDownload Here\\nDocumentation\\nSource code\\n\\n\\nOpentrons Protocol Designer\\nEasily create a protocol to run on your robot with this graphical tool.\\n\\nAccess Here\\nDocumentation\\nSource code\\n\\nContributing\\nWe love contributors! Here is the best way to work with us:\\n\\n\\nFiling a bug report. We will fix these as quickly as we can, and appreciate your help uncovering bugs in our code.\\n\\n\\nSubmit a pull request with any new features you've added to a branch of the API or App. We will reach out to talk with you about integration testing and launching it into our product!\\n\\n\\nFor more information and development setup instructions, please read the contributing guide.\\nEnjoy!\\n\"},\n",
       " {'language': 'JavaScript 89.3',\n",
       "  'readme': 'GENtle2\\nA re-think for the web of the original GENtle.\\nGENtle2 has been almost entirely rewritten over the past year, and remains\\nvery much in development. Core features will be extracted into their own modules\\nin the coming months.\\n\\n\\nGetting started\\n\\nClone the repository locally and cd into it\\nRun the following to install the app and its dependencies and compile it.\\n\\nnpm install --production\\n\\nRun the following command to start the application\\n\\nnpm start\\n\\nOpen you browser and navigate to http://localhost:3000\\n\\nContributing\\nFor more details about how the application works, see CONTRIBUTING.md\\n'},\n",
       " {'language': 'JavaScript 68.9',\n",
       "  'readme': '\\n\\nInstalling JBrowse\\nTo install jbrowse, visit http://jbrowse.org/blog and download the latest JBrowse zip file. See instructions at http://jbrowse.org/docs/installation.html for a tutorial on setting up a sample instance.\\nInstall JBrowse from GitHub (for developers)\\nTo install from GitHub, you can simply clone the repo and run the setup.sh script\\ngit clone https://github.com/GMOD/jbrowse\\ncd jbrowse\\n./setup.sh\\n\\nDevelop JBrowse or JBrowse plugins\\nTo obtain a jbrowse development environment, e.g. for jbrowse source code editing or plugin development (or just running jbrowse from the github repo)\\ngit clone https://github.com/GMOD/jbrowse\\ncd jbrowse\\n./setup.sh # not strictly necessary if you don\\'t need to sample data\\n\\nIf you are going to edit the jbrowse source code, then also run\\nyarn watch\\n\\nAnd keep yarn watch running in the background as you create changes to your code.\\nTo start a temporary dev server, can also run\\nyarn start\\n\\nAnd keep this running in the background, this will launch a webserver running jbrowse on port 8082.\\nAlternatively, you can put this jbrowse folder in your webserver (e.g. /var/www/html/) directory. The key is, if you are modifying jbrowse or plugin source code, to run yarn watch in the background, so that webpack incorporates your changes in either the main codebase (src/JBrowse folder) or any plugins (plugins/YourPlugin).\\nNote for users in China\\nIn order to make downloads faster you can set a mirror for the npm registry\\nnpm config set registry http://r.cnpmjs.org\\nnpm config set puppeteer_download_host=http://cnpmjs.org/mirrors\\nexport ELECTRON_MIRROR=\"http://cnpmjs.org/mirrors/electron/\"\\n\\nNotes on setting up a JBrowse server\\n\\n\\nIf you don\\'t have a webserver such as apache or nginx, you can run npm run start and open http://localhost:8082/index.html?data=sample_data/json/volvox to see the code running from a small express.js server.\\n\\n\\nYou can alternatively just move the jbrowse folder into a nginx or apache root directory e.g. /var/www/html and then navigate to http://localhost/jbrowse\\n\\n\\nNote: you should avoid using sudo tasks like ./setup.sh and instead use chown/chmod on folders to your own user as necessary.\\nAlso note: After editing a file, you must re-run the webpack build with npm run build or you can keep webpack running in \"watch\" mode by running  npm run watch.\\nAlso also note: by default git clone will clone the master branch which contains the latest stable release. The latest development branch is called dev. Run git checkout dev after clone to retrieve this\\nInstalling as an npm module\\nTo install jbrowse from NPM directly, you can run.\\nnpm install @gmod/jbrowse\\n\\nTo setup a simple instance, you can use\\nnode_modules/.bin/jb_setup.js\\nnode_modules/.bin/jb_run.js\\n\\nThen visit http://localhost:3000/?data=sample_data/json/volvox\\nContributing\\nLooking for places to contribute to the codebase?\\nCheck out the \"help wanted\" label.\\nRunning the developer test suites\\nThe Travis-CI suite runs Perl, JavaScript, and Selenium automated tests. To run locally, you can use\\nprove -Isrc/perl5 -lr tests\\nnode tests/js_tests/run-puppeteer.js http://localhost/jbrowse/tests/js_tests/index.html\\npip install selenium nose\\nMOZ_HEADLESS=1 SELENIUM_BROWSER=firefox JBROWSE_URL=\\'http://localhost/jbrowse/index.html\\' nosetests\\n\\nSupported browsers for SELENIUM_BROWSER are \\'firefox\\', \\'chrome\\', \\'phantom\\', and \\'travis_saucelabs\\'.  The Sauce Labs + Travis\\none will only work in a properly configured Travis CI build environment.\\nManual testing\\n\\nJBrowse has a free open source account on Browserstack for manual testing.  Contact @rbuels for access.\\nGenerating Packaged Builds\\nYou can also optionally run build steps to create the minimized codebase. Extra perl dependencies Text::Markdown and DateTime are required to run the build step.\\nmake -f build/Makefile\\n\\nTo build the Electron app (JBrowse desktop app), run the following\\nnpm install -g electron-packager\\nmake -f build/Makefile release-electron-all\\n\\nTo run the Electron app in debug mode run the following\\nnpm install -g electron\\nelectron browser/main.js\\n\\nMaking a JBrowse release\\nNOTE: Beginning in 1.12.4,\\n\\n\\nRun build/release.sh $newReleaseVersion $nextReleaseVersion-alpha.0 notes.txt, where notes.txt is any additional information to add to a blogpost. Then check its work, and then run the git push command it suggests to you. This makes a tag in the repository for the release, named, e.g. 1.6.3-release.  This should cause Travis CI\\nto create a release on GitHub under https://github.com/GMOD/jbrowse/releases\\n\\n\\nTest that the page loads in IE11 on BrowserStack\\n\\n\\nAdd release notes to the new GitHub release that Travis created. Can just paste these from release-notes.md, which is in Markdown format.\\n\\n\\nWrite a twitter post for usejbrowse and JBrowseGossip with the announcement link to the blogpost\\n\\n\\nWrite an email announcing the release, sending to gmod-ajax. If it is a major release, add gmod-announce and make a GMOD news item.\\n\\n\\nAs you can tell, this process could really use some more streamlining and automation.\\n'},\n",
       " {'language': 'JavaScript 89.6',\n",
       "  'readme': '\\n\\n\\n\\n\\n\\n\\nEscher\\nEscher is a web-based tool to build, view, share, and embed metabolic maps. The\\neasiest way to use Escher is to browse or build maps on the\\nEscher website.\\nVisit the documentation to get started with\\nEscher and explore the API.\\nCheck out the\\ndeveloper docs,\\nthe Gitter chat room, and the\\nDevelopment Roadmap for information\\non Escher development. Feel free to submit bugs and feature requests as Issues,\\nor, better yet, Pull Requests.\\nFollow @zakandrewking for Escher updates.\\nYou can help support Escher by citing our publication when you use Escher or\\nEscherConverter:\\nZachary A. King, Andreas Dräger, Ali Ebrahim, Nikolaus Sonnenschein, Nathan\\nE. Lewis, and Bernhard O. Palsson (2015) Escher: A web application for\\nbuilding, sharing, and embedding data-rich visualizations of biological\\npathways, PLOS Computational Biology 11(8):\\ne1004321. doi:10.1371/journal.pcbi.1004321\\nEscher was developed at SBRG. Funding was\\nprovided by The National Science Foundation Graduate Research Fellowship\\nunder Grant no. DGE-1144086, The European Commission as part of a Marie Curie\\nInternational Outgoing Fellowship within the EU 7th Framework Program for\\nResearch and Technological Development (EU project AMBiCon, 332020),\\nand The Novo Nordisk Foundation\\nthrough The Center for Biosustainability\\nat the Technical University of Denmark (NNF10CC1016517)\\nBuilding and testing Escher\\nJavaScript\\nFirst, install dependencies with npm (or you can use\\nyarn):\\nnpm install\\n\\nEscher uses webpack to manage the build process. To run typical build steps, just run:\\nnpm run build\\n\\nYou can run a development server with:\\nnpm run start\\n# or for live updates when the source code changes:\\nnpm run watch\\n\\nTo test the JavaScript files, run:\\nnpm run test\\n\\nPython\\nEscher has a Python package for generating Escher visualizations from within a\\nPython data anlaysis session. To learn more about using the features of the\\nPython package, check out the documentation:\\nhttps://escher.readthedocs.io/en/latest/escher-python.html\\nYou can install it with pip:\\npip install escher\\n\\nJupyter extensions\\nWhen you pip install escher, the Jupyter notebook extension should be\\ninstalled automatically. If that doesn\\'t work, try:\\n# The notebook extenstion should install automatically. You can check by running:\\njupyter nbextension list\\n# Make sure you have version >=5 of the `notebook` package\\npip install \"notebook>=5\"\\n# To manually install the extension\\njupyter nbextension install --py escher\\njupyter nbextension enable --py escher\\n# depending on you environment, you might need the `--sysprefix` flag with those commands\\nTo install the Jupyter lab extension, simply install Escher with pip install escher then\\ninstall the extension:\\njupyter labextension install @jupyter-widgets/jupyterlab-manager\\njupyter labextension install escher\\nPython/Jupyter Development\\nFor development of the Python package, first build the JavaScript package and\\ncopy it over to the py directory with these commands in the Escher root:\\nnpm install\\nnpm run build\\nnpm run copy\\n\\nThen in the py directory, install the Python package:\\ncd py\\npip install -e . # installs escher in develop mode and dependencies\\n\\nFor Python testing, run this in the py directory:\\ncd py\\npytest\\n\\nTo develop the Jupyter notebook and Jupyter Lab extensions, you will need\\ninstall them with symlinks.\\nFirst, install the Python package for development as described above.\\nFor the Jupyter notebooks, run:\\ncd py\\njupyter nbextension install --py --symlink escher\\njupyter nbextension enable --py escher\\n\\nIf you are using virtualenv or conda, you can add the --sys-prefix flag to\\nthose commands to keep your environment isolated and reproducible.\\nWhen you make changes, you will need to yarn build && yarn copy and refresh\\nnotebook browser tab.\\nFor Jupyter Lab, run (in the root directory):\\nyarn watch # keep this running as a separate process\\njupyter labextension install @jupyter-widgets/jupyterlab-manager\\njupyter labextension link\\njupyter lab --watch\\n\\nIf you don\\'t see changes when you edit the code, try refreshing or restarting\\njupyter lab --watch.\\nDocs\\nBuild and run the docs::\\ncd docs\\n./build_docs\\ncd _build/html\\npython -m SimpleHTTPServer # python 2\\npython -m http.server # python 3\\n\\n'},\n",
       " {'language': 'JavaScript 53.2',\n",
       "  'readme': \"\\n\\n\\nlayout\\ntitle\\npermalink\\n\\n\\n\\n\\ndocs\\nOverview of DIYbiosphere\\n/docs/introduction/overview/\\n\\n\\n\\n\\n\\n\\n\\n🎉 Welcome to the DIYbiophere repository 🎉\\nWe really appreciate your interest in our project and we would ❤️ your contributions!\\nAbout\\nThe DIYbiosphere is a open-source project to connect Do-It-Yourself Biology (DIYbio) initiatives from all over the world. The goal is to have a shared and common platform that can connect people and ideas in all its possibilities and encourage the DIYbio community to work on a project together.\\nHow it works\\nThe platform functions similar to a wiki but uses GitHub Pages instead; hosting the raw files at https://github.com/DIYbiosphere/sphere and rendering webpages at http://sphere.diybio.org\\nEach DIYbio initiative has its own entry which are organized into eight collections: projects, startups, labs, incubators, groups, networks, events, and others. An entry is added by creating its own folder in its respective collection, and adding a text file in markdown syntax with a YAML front matter, which is then rendered into its webpage. For example, the file _projects/MyDIYbioProject/MyDIYbioProject.md could look like this:\\n---\\n# This is the front matter in YAML; between two lines of three consecutive dashes (---)\\ntitle: My DIYbio Project\\nstart-date: 2000\\ntype-org: non-profit\\nwebsite: http://my-diybio-project.io\\ntags:\\n  - open hardware\\n  - citizen science\\n---\\n# This is the text in Markdown syntax; after the front matter\\n\\nMy DIYbio project is about **open hardware** and **citizen science**.\\n\\nThe front matter includes several key: value pairs that render into different elements in the webpage. The Avocado Lab is an example entry for pedagogical purposes. You can check out the raw file raw file rendered into this webpage. See the rendered image below, and by its side the different elements of the page labeled.\\n{:.ui.fluid.image}\\nContribute\\nTo contribute, you need a GitHub account (sign up).\\nYou also need to abide to our Code of Conduct (COC) and consent to our Contributor Terms (CT) determined by our Terms of Use (aka Copyrights)).\\n\\nTL;DR (Too Long; Didn't Read)\\n\\nCOC: Be kind and respectful. Gross, rude or inappropriate behavior will not be tolerated. Confront wrongdoers directly or report them to the board of directors.\\nCT: You freely share your contributions to the repository under the MIT license. If your contributions are displayed in the website, you freely waive authorship rights of these contributions (public domain; CC0), otherwise you will specify their copyright. You will also note contributions on behalf of third parties and specify their copyright.\\n\\n\\nIn increasing order of engagement you can contribute to DIYbiosphere by:\\n\\nSHARING THE LOVE\\n\\nShare this project with your friends and followers! They might be interested in using the project to find DIYbio initiatives or adding their own. You don't need a GitHub account for this!\\n⭐ Star the project on GitHub! Starring helps attract potential contributors, especially expert and master developers!\\n\\n\\nWRITING ENTRIES\\n\\nAdd a new entry:  whether it's your initiative or someone else's\\nEdit an existing entry: misspellings, outdated information, or just inaccurate, help us keep the entries error-free and up-to-date!\\n\\n\\nPARTICIPATING IN THE ISSUES\\n\\nComment, answer, and vote: search our issues and see if you can help out by moving our issues along:\\nSubmit a new issue: report a bug, ask a question, share your idea and wait for feedback and support from the community.\\nFork, commit, pull request your contributions! Tackle a good first issue to get you started\\n\\n\\nGETTING INVOLVED\\n\\nJoin the development community. The project is managed by members of the DIYbiosphere community. Request membership by submitting an issue enjoy more access privileges to the project!\\nJoin the conversation. You can freely join the Gitter chatroom at gitter.im/diybiosphere/sphere, or in Slack at diybiosphere.slack.com\\n\\n\\n\\nCopyright\\nIn short: the work in DIYbiosphere is freely available to use, modify and distribute. More specifically:\\n\\nFiles in the Repository are available under the MIT License\\nContent in the Website is shared under the public domain by CC0 License\\n\\nCredit our work as “© DIYbiosphere contributors” or “© DIYbiosphere” with a link to the Repository at: https://github.com/DIYbiosphere/sphere, or the Website at: http://sphere.diybio.org\\nYou can review our Terms of Use for a human-readable version of the copyrights, and our Contributor Terms to understand in legal terms the rights granted and/or waived from your Contributions. For further detail you should read in full both MIT and CC0 licenses.\\n\"},\n",
       " {'language': 'JavaScript 91.9',\n",
       "  'readme': \"\\n\\n\\nSynBioHub is a Web-based repository for synthetic biology, enabling users to browse, upload, and share synthetic biology designs.\\nTo learn more about the SynBioHub, including installation instructions and documentation, visit the SynBioHub wiki.\\nTo access a sample instance of SynBioHub containing enriched Bacillus subtilis data, features from the Escherichia coli genome, and the complete iGEM Registry of Standard Biological Parts, visit synbiohub.org. To access a bleeding-edge version of SynBioHub, visit dev.synbiohub.org.\\nInstallation\\nThe recommended way to install SynBioHub is via the Docker image.  See Installation for more information.\\nManual Installation\\nSynBioHub has both JavaScript (node.js) and Java components.\\nPrequisites:\\nLinux (only tested with Ubuntu 18.04.01) or macOS\\n\\nIf you're using macOS, first install homebrew\\n\\nA JDK\\n\\n\\n\\nOS\\nCommand\\n\\n\\n\\n\\nUbuntu\\napt install default-jdk\\n\\n\\nMac\\nbrew install openjdk\\n\\n\\n\\nApache Maven\\n\\n\\n\\nOS\\nCommand\\n\\n\\n\\n\\nUbuntu\\napt install maven\\n\\n\\nMac\\nbrew install maven\\n\\n\\n\\nnode.js >= 11.0.0\\n\\n\\n\\nOS\\nCommand/Link\\n\\n\\n\\n\\nUbuntu\\nvisit https://nodejs.org/en/\\n\\n\\nMac\\nbrew install node\\n\\n\\n\\nOpenLink Virtuoso 7.x.x\\n\\n\\n\\nOS\\nCommand/Link\\n\\n\\n\\n\\nUbuntu\\nvisit https://github.com/openlink/virtuoso-opensource\\n\\n\\nMac\\nbrew install virtuoso\\n\\n\\n\\nrapper\\n\\n\\n\\nOS\\nCommand\\n\\n\\n\\n\\nUbuntu\\napt install raptor2-utils\\n\\n\\nMac\\nbrew install raptor\\n\\n\\n\\njq\\n\\n\\n\\nOS\\nCommand\\n\\n\\n\\n\\nUbuntu\\napt install jq\\n\\n\\nMac\\nbrew install jq\\n\\n\\n\\nUbuntu 18.04.01\\n\\nInstall Virtuoso 7 from source at\\nhttps://github.com/openlink/virtuoso-opensource\\n\\n\\nSwitch to the branch stable/7 before installing.\\nFollow the README on installing virtuoso from source. This involves installing all the dependencies and running build commands.\\nCurrently, Virtuoso does not support versions of OpenSSL 1.1.0 and above, or versions of OpenSSL below 1.0.0. When installing the dependency, build from a binary between those versions from https://www.openssl.org/source/.\\n\\n\\nSet up the Node.js repository\\n\\nDownload the Node setup script curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -\\nUpdate your package repositories apt update\\n\\n\\nInstall the necessary packages apt install default-jdk maven raptor2-utils nodejs jq build-essential python\\nStart virtuoso process virtuoso-t +configfile /usr/local/virtuoso-opensource/var/lib/virtuoso/db/virtuoso.ini -f\\n\\nMacOS\\n\\nInstall the necessary packages brew install openjdk maven node virtuoso raptor jq python\\nStart virtuoso process\\n\\ncd /usr/local/Cellar/virtuoso/7.2.5.1_1/var/lib/virtuoso/db\\n\\nThe command above is based on where the virtuoso.ini file is located. Your installation might be located\\nsomewhere different than /usr/local/Cellar/virtuoso/7.2.5.1_1/var/lib/virtuoso/db, or the version might be\\ndifferent (7.2.5.1_1 might be 7.3.6.1_1 or any other version number).\\nIf you're having trouble finding the location of the virtuoso.ini file, run sudo find / -name virtuoso.ini.\\nPress the control and c keys simultaneously to quit the search.\\n\\n\\nvirtuoso-t -f\\n\\n\\n\\nBoth Systems\\n\\nClone the SynBioHub repository git clone https://github.com/SynBioHub/synbiohub\\nChange to the SynBioHub directory cd synbiohub\\nBuild the Java components with Maven cd java && mvn package\\nReturn to the root directory and install the Node dependencies with yarn cd ../ && yarn install\\nMake sure that yarn is being used, not 'cmdtest'.\\nInstall nodemon and forever with npm install nodemon -g && npm install forever -g\\nAdd SPARQL update rights to the dba user in virtuoso.\\n\\n\\nVisit localhost:8890, click conductor on the left hand side, and login with user name dba and password dba.\\nVisit system admin -> user accounts in the menu at the top.\\nFind the accound labled dba and edit.Add SPARQL_UPDATE to roles using the menu at the bottom.\\nIf no dba account exists, add one, then add update rights.\\n\\n\\nStart the SynBioHub process npm start or npm run-script dev\\n\\nPublishing\\nThe repository is set up to prohibit commits directly to the master branch.\\nCommits must be made in another branch, and then a GitHub PR used to merge them into master.\\nGitHub PRs must be approved by at least one other developer before they can be merged into master.\\nAdditionally, they must pass Travis checks, which build a Docker image and run the SBOLTestSuite and SynBioHub integration tests against it.\\nEach time a PR is merged into master, the Travis checks are re-run on the master branch, and if they succeed the resulting image is pushed by Travis to DockerHub under the tag snapshot-standalone.\\nPublishing a release\\nReleases are published automatically using GitHub Actions.\\nThere is an action which fires on release publication.\\nIt publishes an image to Docker Hub under the $VERSION-standalone tag, and updates the synbiohub-docker master branch to point to this version.\\nMore information available here.\\n\"},\n",
       " {'language': 'JavaScript 98.6',\n",
       "  'readme': \"\\n\\n\\n\\nClustergrammer is a web-based tool for visualizing high-dimensional data (e.g. a matrix) as an interactive and shareable hierarchically clustered heatmap. Clustergrammer's front end (Clustergrammer-JS) is built using D3.js and its back-end (Clustergrammer-PY) is built using Python. Clustergrammer produces highly interactive visualizations that enable intuitive exploration of high-dimensional data and has several biology-specific features (e.g. enrichment analysis, see Biology-Specific Features) to facilitate the exploration of gene-level biological data. Click the screenshot below to view an interactive tutorial:\\n\\nClustergrammer's interacive features include:\\n\\nZooming and Panning\\nRow and Column Reordering\\nInteractive Dendrogram\\nInteractive Dimensionality Reduction\\nInteractive Categories\\nCropping\\nRow Searching\\nBiology-Specific Features\\n\\nClustergrammer can be used in three main ways (this repo contains the source code for Clustergrammer-JS):\\n\\nClustergrammer Web App (http://amp.pharm.mssm.edu/clustergrammer/)\\nClustergrammer Jupyter Widget\\nClustergrammer-JS and Clustergrammer-PY libraries\\n\\nFor information about building a webpage or app using Clustergrammer see: Web-Development with Clustergrammer\\nWhat's New\\nClustergrammer2\\n \\n\\nClustergrammer is being re-built using the WebGL library regl. The new in-development front-end is Clustergrammer-GL and the new in-development Jupyter widget is Clustergrammer2. The above notebook shows how Clustergrammer2 can be used to load a small dataset and visualize a large random DataFrame. By running the notebook on MyBinder using Jupyter Lab it can also be used to visualize a user uploaded dataset. Please see the video tutorial above for more information.\\nFor additional examples and tutorials please see:\\n\\nCase Studies and Tutorials\\nClustergrammer2-Notebooks GitHub repository\\n\\nJupyterCon 2018 Presentation\\n\\nClustergrammer was recently presented at JupyterCon 2018 (see slides).\\nUsing Clustergrammer\\nPleae see Clustergramer's documentation for detailed information or select a specific topic below:\\n\\nGetting Started\\nInteracting with the Visualization\\nWeb-Development with Clustergrammer (example pages)\\nClustergrammer Web App and Clustergrammer Jupyter Widget\\nMatrix Formats and Input/Output\\nCore libraries: Clustergrammer-JS and Clustergrammer-PY\\nApp Integration Examples\\nCase Studies and Examples\\nBiology-Specific Features\\nDeveloping Clustergrammer\\n\\nCiting Clustergrammer\\nPlease consider supporting Clustergrammer by citing our publication:\\nFernandez, N. F. et al. Clustergrammer, a web-based heatmap visualization and analysis tool for high-dimensional biological data. Sci. Data 4:170151 doi: 10.1038/sdata.2017.151 (2017).\\nLicensing\\nClustergrammer was developed by the Ma'ayan lab at the Icahn School of Medicine at Mount Sinai for the BD2K-LINCS DCIC and the KMC-IDG. Clustergrammer's license and third-party licenses are in the LICENSES directory and more information can be found at Clustergrammer License.\\nPlease contact us for support, licensing questions, comments, and suggestions.\\n\"},\n",
       " {'language': 'JavaScript 97.6',\n",
       "  'readme': \"Boolean Network Simulator\\nThis software provides a browseable user interface,\\nwhich allows the user to load Boolean networks in various formats,\\ndisplay them and interactively simulate them.\\nIt's coded in JavaScript, so it should work platform-independent.\\nA recent version of a HTML5-capable internet browser is required for proper rendering,\\nChromium recommended (http://www.chromium.org/getting-involved/download-chromium).\\nSoftware license: GNU Affero GPL v3\\nDownload using\\ngit clone https://github.com/matthiasbock/BooleSim.git\\n\\nAfter downloading, open or drag'n'drop index.html into your web browser.\\nClick here for help: https://github.com/matthiasbock/BooleSim/wiki\\nBuilt upon previous work located on Google Code at http://biographer.googlecode.com/\\nFor questions, wishes or bug reports please visit https://github.com/matthiasbock/BooleSim/issues\\nHave fun!\\n\"},\n",
       " {'language': 'JavaScript 99.7',\n",
       "  'readme': 'The Noctua Stack\\nThe Noctua Stack is a curation platform developped by the Gene Ontology Consortium. The stack is composed of:\\n\\nMinerva: the backend data server to retrieve, store, update and delete annotations.\\nBarista: an authentication layer controling and formating all communications from/to Minerva.\\nNoctua: the website to browse the annotations in production and development and provide an editorial platform to produce Gene Ontology Causal Activity Models (or GO-CAMs) using either the simple UI Noctua Form or the more advanced Graph Editor.\\n\\nThe biological knowledge are stored in RDF/OWL using the blazegraph triplestore implementation.\\nIn effect, any piece of knowledge stored in RDF/OWL is a triple { subject, predicate, object } defining a relationship (or association) between a subject and an object. Those triples are also commonly stored in Turtle files.\\nInstallation\\nPre-requisite\\nYou must have npm installed. On ubuntu/debian, simply type:\\nsudo apt-get install nodejs\\n\\nOn OSX, it is also possible to install npm either from nodejs.org or using brew:\\nbrew install node\\n\\nSteps for a local Installation\\n# The full Noctua stack is a multi-repositorie project; optionally create a main directory for the stack to contain all the repositories.\\n# These instruction assume that \"gulp\" is in your path; if local-only, use: `./node_modules/.bin/gulp`.\\n\\n# Creating a local directory for our work.\\nmkdir noctua-stack && cd noctua-stack\\n\\n# Repo containing metadata (users, groups, etc.).\\ngit clone https://github.com/geneontology/go-site.git\\n# The data repo to start the store and save to.\\ngit clone https://github.com/geneontology/noctua-models.git\\n# Repo for the backend server.\\ngit clone https://github.com/geneontology/minerva.git\\n# Repo for the Noctua client and middleware (Barista).\\ngit clone https://github.com/geneontology/noctua.git\\n\\n# Build the Minerva server (and CLI).\\ncd minerva && sh ./build-cli.sh && cd ..\\n\\n# Create default authentication users with your favorite editor.\\nmkdir barista\\nvim barista/local.yaml\\n-\\n uri: \\'http://orcid.org/XXXX-XXXX-XXXX-XXXX\\'\\n username: my_username\\n password: my_password\\n\\n# Install Noctua Form (old \"simple-annoton-editor\")\\ngit clone https://github.com/geneontology/noctua-form.git\\ngit clone https://github.com/geneontology/noctua-landing-page.git\\n\\n# Install Noctua as an all-local installation.\\ncd noctua\\nnpm install\\ncp config/startup.yaml.stack-dev ./startup.yaml\\n\\n# Edit configuration file (barista, user, group, noctua models location, minerva memory to at least 16GB, link to NoctuaForm / SAE)\\nvim startup.yaml\\n\\n# Build the stack and Blazegraph Journal (triplestore)\\n./node_modules/.bin/gulp build\\n# If running first time.\\n./node_modules/.bin/gulp batch-minerva-destroy-journal\\n./node_modules/.bin/gulp batch-minerva-destroy-ontology-journal\\n./node_modules/.bin/gulp batch-minerva-create-journal\\n\\n# Then launch the stack, waiting for each to successfully start up:\\n./node_modules/.bin/gulp run-minerva &> minerva.log &\\n./node_modules/.bin/gulp run-barista &> barista.log &\\n./node_modules/.bin/gulp run-noctua &> noctua.log &\\n\\nAdditional notes\\nGulp Tasks\\n\\ndoc - build the docs, available in doc/\\ntest - need more here\\nbuild - assemble the apps for running\\nwatch - development file monitor\\nclean - clean out /doc and /deploy\\n\\nIn addition, the last 3 lines of the installation steps launch all the 3 layers of the Noctua Stack:\\ngulp run-barista &> barista.log &\\ngulp run-minerva &> minerva.log &\\ngulp run-noctua &> noctua.log &\\n\\nAnd Gulp can be used to both destroy and create blazegraph journals (triplestore):\\ngulp batch-minerva-destroy-journal\\ngulp batch-minerva-destroy-ontology-journal\\ngulp batch-minerva-create-journal\\n\\nUsers & groups\\nBarista, the authentication layer needs two files to run: users.yaml and groups.yaml.\\nThese files defined who is authorized to log in to the Noctua Stack to perform biological curations.\\n\\nTo know more about curation with the Noctua Stack, visit our wiki.\\nTo request an account to curate with the Noctua Stack, contact us\\n\\nLibraries and CLI to communicate with the Noctua Stack\\nbbop-manager-minerva\\nThis is the high-level API with OWL formatted requests (e.g. add individual, add fact or evidence using class expressions).\\nhttps://github.com/berkeleybop/bbop-manager-minerva\\nminerva-requests\\nThis is the request object used to format specific queries to Minerva. It is composed of a basic request object as well as a request_set designed to chain multiple request objects and speed up complex tasks.\\nhttps://github.com/berkeleybop/minerva-requests\\nSome useful details about the API are described here\\nCLI (REPL)\\nThe Noctua REPL is a recommended step for anyone trying to learn the syntax and how to build requests to Minerva in the Noctua Stack.\\nAs any REPL, it allows for the rapid testing of multiple commands and to check the responses from barista.\\nThis project can be considered as a basic prototype for any other client wanting to interact with the stack.\\nhttps://github.com/geneontology/noctua-repl\\nKnown issues\\nThe bulk of major issues and feature requests are handled by the\\ntracker (https://github.com/geneontology/noctua/issues). If something is\\nnot mentioned here or in the tracker, please contact Seth Carbon or Chris Mungall.\\n\\nSometimes, when moving instance or relations near a boundary, the\\nrelations will fall out of sync; either move nearby instances or\\nrefresh the model\\nSometimes, when editing an instance, the relations (edges) will\\nfall out of sync; either move nearby instances or refresh the\\nmodel\\nThe endpoint scheme is reversed between creation and instantiation\\nTODO, etc.\\n\\n'},\n",
       " {'language': 'JavaScript 99.7',\n",
       "  'readme': \"\\nBSD-licensed implementation of the Synthetic Biology Open Language (SBOL) in JavaScript.\\nRequires a JavaScript environment with ES6 class support (e.g. recent versions of node, Chrome, ...)\\nFeatures:\\n\\nRead generic RDF, XML\\nSerialize SBOL XML, JSON\\nBuild SBOL documents programatically\\n\\nInstallation\\nnpm install sboljs\\n\\nUsage\\nvar SBOLDocument = require('sboljs')\\n\\nSBOLDocument.loadRDFFile('foo.xml', function(err, doc) {\\n\\n    doc.componentDefinitions.forEach(function(componentDefinition) {\\n\\n        console.log(componentDefinition.name)\\n\\n    })\\n\\n})\\n\\nDocumentation\\n\"},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'PYDFS-LINEUP-OPTIMIZER \\npydfs-lineup-optimizer is a tool for creating optimal lineups for daily fantasy sport.\\nInstallation\\nTo install pydfs-lineup-optimizer, simply run:\\n$ pip install pydfs-lineup-optimizer\\n\\nSupport\\nNow it supports following dfs sites:\\n\\n\\n\\nLeague\\nDraftKings\\nFanDuel\\nFantasyDraft\\nYahoo\\nFanBall\\nDraftKings Captain Mode\\nFanDuel Single Game\\nDraftKings Tiers\\n\\n\\n\\n\\nNFL\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n-\\n\\n\\nNBA\\n+\\n+\\n+\\n+\\n-\\n+\\n+\\n+\\n\\n\\nNHL\\n+\\n+\\n+\\n+\\n-\\n-\\n-\\n+\\n\\n\\nMLB\\n+\\n+\\n+\\n+\\n-\\n+\\n-\\n+\\n\\n\\nWNBA\\n+\\n+\\n-\\n-\\n-\\n+\\n-\\n-\\n\\n\\nGolf\\n+\\n+\\n+\\n+\\n-\\n-\\n-\\n-\\n\\n\\nSoccer\\n+\\n-\\n-\\n+\\n-\\n+\\n-\\n-\\n\\n\\nCFL\\n+\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n\\nLOL\\n-\\n+\\n-\\n-\\n-\\n+\\n+\\n-\\n\\n\\nMMA\\n+\\n+\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n\\nNASCAR\\n+\\n+\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n\\nTennis\\n+\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n\\nCSGO\\n+\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n\\n\\nDocumentation\\nDocumentation is available at https://pydfs-lineup-optimizer.readthedocs.io/en/latest\\nExample\\nHere is an example for evaluating optimal lineup for Yahoo fantasy NBA. It loads players list from \"yahoo-NBA.csv\" and select 10 best lineups.\\nfrom pydfs_lineup_optimizer import Site, Sport, get_optimizer\\n\\n\\noptimizer = get_optimizer(Site.YAHOO, Sport.BASKETBALL)\\noptimizer.load_players_from_csv(\"yahoo-NBA.csv\")\\nfor lineup in optimizer.optimize(10):\\n    print(lineup)\\n'},\n",
       " {'language': 'Python 96.3',\n",
       "  'readme': \"Sportsreference: A free sports API written for python\\n\\n\\n\\n\\n\\n\\nContents\\n\\nInstallation\\nExamples\\nGet instances of all NHL teams for the 2018 season\\nPrint every NBA team's name and abbreviation\\nGet a specific NFL team's season information\\nPrint the date of every game for a NCAA Men's Basketball team\\nPrint the number of interceptions by the away team in a NCAA Football game\\nGet a Pandas DataFrame of all stats for a MLB game\\nFind the number of goals a football team has scored\\n\\n\\nDocumentation\\nTesting\\n\\n\\nSportsreference is a free python API that pulls the stats from\\nwww.sports-reference.com and allows them to be easily be used in python-based\\napplications, especially ones involving data analytics and machine learning.\\nSportsreference exposes a plethora of sports information from major sports\\nleagues in North America, such as the MLB, NBA, College Football and Basketball,\\nNFL, and NHL. Sportsreference also now supports Professional Football (or\\nSoccer) for thousands of teams from leagues around the world. Every sport has\\nits own set of valid API queries ranging from the list of teams in a league, to\\nthe date and time of a game, to the total number of wins a team has secured\\nduring the season, and many, many more metrics that paint a more detailed\\npicture of how a team has performed during a game or throughout a season.\\n\\nInstallation\\nThe easiest way to install sportsreference is by downloading the latest\\nreleased binary from PyPI using PIP. For instructions on installing PIP, visit\\nPyPA.io for detailed steps on\\ninstalling the package manager for your local environment.\\nNext, run:\\npip install sportsreference\\n\\nto download and install the latest official release of sportsreference on\\nyour machine. You now have the latest stable version of sportsreference\\ninstalled and can begin using it following the examples below!\\nIf the bleeding-edge version of sportsreference is desired, clone this\\nrepository using git and install all of the package requirements with PIP:\\ngit clone https://github.com/roclark/sportsreference\\ncd sportsreference\\npip install -r requirements.txt\\n\\nOnce complete, create a Python wheel for your default version of Python by\\nrunning the following command:\\npython setup.py sdist bdist_wheel\\n\\nThis will create a .whl file in the dist directory which can be installed\\nwith the following command:\\npip install dist/*.whl\\n\\n\\nExamples\\nThe following are a few examples showcasing how easy it can be to collect\\nan abundance of metrics and information from all of the tracked leagues. The\\nexamples below are only a miniscule subset of the total number of statistics\\nthat can be pulled using sportsreference. Visit the documentation on\\nRead The Docs for a\\ncomplete list of all information exposed by the API.\\n\\nGet instances of all NHL teams for the 2018 season\\nfrom sportsreference.nhl.teams import Teams\\n\\nteams = Teams(2018)\\n\\nPrint every NBA team's name and abbreviation\\nfrom sportsreference.nba.teams import Teams\\n\\nteams = Teams()\\nfor team in teams:\\n    print(team.name, team.abbreviation)\\n\\nGet a specific NFL team's season information\\nfrom sportsreference.nfl.teams import Teams\\n\\nteams = Teams()\\nlions = teams('DET')\\n\\nPrint the date of every game for a NCAA Men's Basketball team\\nfrom sportsreference.ncaab.schedule import Schedule\\n\\npurdue_schedule = Schedule('purdue')\\nfor game in purdue_schedule:\\n    print(game.date)\\n\\nPrint the number of interceptions by the away team in a NCAA Football game\\nfrom sportsreference.ncaaf.boxscore import Boxscore\\n\\nchampionship_game = Boxscore('2018-01-08-georgia')\\nprint(championship_game.away_interceptions)\\n\\nGet a Pandas DataFrame of all stats for a MLB game\\nfrom sportsreference.mlb.boxscore import Boxscore\\n\\ngame = Boxscore('BOS201806070')\\ndf = game.dataframe\\n\\nFind the number of goals a football team has scored\\nfrom sportsreference.fb.team import Team\\n\\ntottenham = Team('Tottenham Hotspur')\\nprint(tottenham.goals_scored)\\n\\nDocumentation\\nTwo blog posts detailing the creation and basic usage of sportsreference can\\nbe found on The Medium at the following links:\\n\\nPart 1: Creating a public sports API\\nPart 2: Pull any sports metric in 10 lines of Python\\n\\nThe second post in particular is a great guide for getting started with\\nsportsreference and is highly recommended for anyone who is new to the\\npackage.\\nComplete documentation is hosted on\\nreadthedocs.org. Refer to\\nthe documentation for a full list of all metrics and information exposed by\\nsportsreference. The documentation is auto-generated using Sphinx based on the\\ndocstrings in the sportsreference package.\\n\\nTesting\\nSportsreference contains a testing suite which aims to test all major portions\\nof code for proper functionality. To run the test suite against your\\nenvironment, ensure all of the requirements are installed by running:\\npip install -r requirements.txt\\n\\nNext, start the tests by running py.test while optionally including coverage\\nflags which identify the amount of production code covered by the testing\\nframework:\\npy.test --cov=sportsreference --cov-report term-missing tests/\\n\\nIf the tests were successful, it will return a green line will show a message at\\nthe end of the output similar to the following:\\n======================= 380 passed in 245.56 seconds =======================\\n\\nIf a test failed, it will show the number of failed and what went wrong within\\nthe test output. If that's the case, ensure you have the latest version of code\\nand are in a supported environment. Otherwise, create an issue on GitHub to\\nattempt to get the issue resolved.\\n\"},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"SportsBook\\nA sports data scraping and analysis tool\\nThis project is in its very early days and currently only supports football (soccer).\\nFearure list:\\n\\n\\nImport leagues and fixtures from one of two available online sources.\\n\\n\\nSee a visual comparison of the teams playing in any game.\\n\\n\\nRun a benchmark comparison on a selected fixture.\\n\\nThis allows you to see the results of games where both teams have played the same opponent.\\nIt shows results where both teams faced the same opponent at home, then away and then where\\nthe home team have faced the opponent at home and the away team have faced the opponent away.\\nOnly games where three of the above comparisons are shown. If the home team, at home, haven't\\nplayed the same team as the awyay team have played away on three occasions, the game is ommitted\\nfrom predictions.\\n\\n\\n\\nRun manual comparison of two teams from any loaded leagues generating (somewhat inaccurate) predictions.\\n\\n\\nRun predictions on all loaded fixtures (more accurate as the teams are guaranteed to be from the same leagues).\\n\\nSelect from using all available league data to compare home and away goal scoring and conceding form or only using data from games\\nwhere the home team has played the same team at home as the away team as played away, making the data a fairer representation of the\\nteam's capabilities.\\n\\n\\n\\nFilter predictions to show games where specific requirements are met (eg. prediction of home side winning by 2 goals).\\n\\n\\nFilter filtered predictions further with other filters.\\n\\n\\nFilter predictions using special filters (either produced by guest contributors or specially designed filters for specific bet types).\\n\\n\\nDisplay filtered predictions and all predictions on screen.\\n\\n\\nChange the range of dates or games the predictions cover.\\n\\n\\nDisplay results from throughout the whole current season of each league.\\n\\n\\nChange the range of dates the results cover.\\n\\n\\nProduce a spreadsheet of all predictions or filtered predictions with a wealth of current stats for each team in each prediction.\\n\\n\\nExport currently loaded league data to a JSON file.\\n\\n\\nImport the league data from a JSON file.\\n\\n\\nThe project is growing fairly quickly. I'd love to hear what your thoughts are and even keep you up to date with new features if you like. Join the Slack group here if you're interested:\\nhttps://join.slack.com/t/sportsbookgroup/shared_invite/enQtNDc4MjYwNzMwNzg4LTAzMDk0MDM3OWFiMGJhZWU2MzAyMzQyNGI4OTlhNjgxMWRlNTZjOTAzMTM3ODdhMDIxNDU3YjI2MzM4OTlmZjg\\nIt'd be great to hear from you so please pop in and say hi!\\n\"},\n",
       " {'language': 'Python 89.5',\n",
       "  'readme': 'Sports Tracker Liberator\\nUnder a catchy name lies an implementation which uses/implements Endomondo Mobile Api.\\nStatus\\nCurrently only Endomondo Api is somewhat implemented. Retrieving stuff somewhat works, and submitting new workouts works as in most basic form. All social media crap is ignored.\\nUsage\\nAuthentication\\nEndomondo implements basic token mechanism, except that earlier versions didn\\'t do that correctly and was only protected by can\\'t be arsed with it -securitysystem. Later App versions tried to fix this, while maintaining backward compatibility by implementing a second, secureToken param. This is mainly used for social media crap, so we\\'ll conveniently ignore it.\\nTo authenticate, one needs existing email and password:\\nfrom endomondo import MobileApi\\nendomondo = MobileApi(email=\\'email@example.com\\', password=\\'p4ssw0rd\\')\\n\\nauth_token = endomondo.get_auth_token()\\n\\nThis will return an auth_token, which can be stored on keychain or similar. In future, it should be used to skip whole login juggalloo.\\nendomondo = MobileApi(auth_token=auth_token)\\n\\nRetrieving workouts:\\nTo retrieve latest workouts:\\nendomondo.get_workouts()\\n\\nEndomondo Mobile Api provides some oddities, and one of them is that maxResults actually work! And as usually in REST APIs, before date can be defined:\\nworkouts = endomondo.get_workouts(maxResults=2)\\nendomondo.get_workouts(before=workouts[-1].start_time)\\n\\nStructure for workouts page is similar to other providers, except paging links are missing.\\nRetrieving single workout\\nMobileApi.get_workout() accepts either existing workout, or workout ID.\\nworkout = endomondo.get_workout(workoutId=\\'234246\\')\\n[...]\\nreload = endomondo.get_workout(workout)\\n\\nFor workouts, or workout history, you can pass fields param to define, which attributes you are interested at. Again, a bit suprisingly, this works.\\nsocial_workout = endomondo.get_workout(fields=[\\'lcp_count\\'])\\n\\nThease attributes aren\\'t documented anywhere, but most of known can be found ín MobileApi.get_workout(), and they follow somewhat logical naming conventions.\\nCreating a workout and a track.\\nSome, if not most of program logic lies in mobile app, and server just stores data. This means that one needs to calculate most of their data by themselves.\\nCurrently only most basic workout creation is supported. Later if/when workout update is added, more features becomes available. This is somewhat related to implementation of Endomondo Mobile Api, as only sport type, calories, hydration, duration and distance in form of track point is created. After initial creation, rest of data is updated to it.\\nTo create workout:\\nfrom endomondo import MobileApi, Workout, TrackPoint\\nfrom datetime import datetime\\n\\nendomondo = MobileApi(auth_token=\\'1234\\')\\n\\nworkout = Workout()\\n\\nworkout.start_time = datetime.utcnow()\\n\\n# See ``workout.sports``\\nworkout.sport = 0\\n\\n# Units are in user\\' local units, and only Metric is supported.\\n# `distance` is in km.\\n# Note that at creation time, this is not required. It\\'s only used for automatic track point generation.\\nworkout.distance = 1.5\\n\\n# Duration in seconds\\nworkout.duration = 600\\n\\nendomondo.post_workout(workout=workout, properties={\\'audioMessage\\': \\'false\\'})\\n\\nif workout.id:\\n\\tprint \"Saved!\"\\n\\nAltought track points aren\\'t technically required by Endomondo backend, which is likely just for a sake of backwards compatibility, and in practice you always want to have at least one TrackPoint in your Workout. MobileApi.post_workout() can automate this, and will create one if none exists.\\nOther\\nFor other, tested, functionality see main.py\\nDisclaimer, legalese and everything else.\\nThis is not affiliated or endorset by Endomondo, or any other party. If you are copying this for a commercial project, be aware that it might be so that clean room implementation rules aren\\'t fully complied with.\\n'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"NBA Player Movements\\nThis is a script for visualization of NBA games from raw SportVU logs.\\nIf you admire both Spurs' and Warriors' ball movement, Brad Stevens' playbook, or just miss KD in OKC you'll find this entertaining.\\nExamples\\n\\n\\n\\n\\nUsage\\n\\nClone this repo:\\n\\n$ git clone https://github.com/linouk23/NBA-Player-Movements\\n\\n\\nChoose any NBA game from data/2016.NBA.Raw.SportVU.Game.Logs directory.\\n\\n\\nGenerate an animation for the play by running the following script:\\n\\n\\n$ python3 main.py --path=Celtics@Lakers.json --event=140\\nrequired arguments:\\n  --path PATH    a path to json file to read the events from\\n\\noptional arguments:\\n  --event EVENT  an index of the event to create the animation to\\n                 (the indexing start with zero, if you index goes beyond out\\n                 the total number of events (plays), it will show you the last\\n                 one of the game)\\n  -h, --help     show the help message and exit\\n\\n\"},\n",
       " {'language': 'Python 99.3',\n",
       "  'readme': 'SportScanner\\n\\nThis project is no longer actively maintained. Please submit PRs and I will review but support is patchy at best! Thanks for watching!\\nScanner and Metadata Agent for Plex that uses www.thesportsdb.com\\n#Installation\\nPlex main folder location:\\n* \\'%LOCALAPPDATA%\\\\Plex Media Server\\\\\\'                                        # Windows Vista/7/8\\n* \\'%USERPROFILE%\\\\Local Settings\\\\Application Data\\\\Plex Media Server\\\\\\'         # Windows XP, 2003, Home Server\\n* \\'$HOME/Library/Application Support/Plex Media Server/\\'                     # Mac OS\\n* \\'$PLEX_HOME/Library/Application Support/Plex Media Server/\\',               # Linux\\n* \\'/var/lib/plexmediaserver/Library/Application Support/Plex Media Server/\\', # Debian,Fedora,CentOS,Ubuntu\\n* \\'/usr/local/plexdata/Plex Media Server/\\',                                  # FreeBSD\\n* \\'/usr/pbi/plexmediaserver-amd64/plexdata/Plex Media Server/\\',              # FreeNAS\\n* \\'${JAIL_ROOT}/var/db/plexdata/Plex Media Server/\\',                         # FreeNAS\\n* \\'/c/.plex/Library/Application Support/Plex Media Server/\\',                 # ReadyNAS\\n* \\'/share/MD0_DATA/.qpkg/PlexMediaServer/Library/Plex Media Server/\\',        # QNAP\\n* \\'/volume1/Plex/Library/Application Support/Plex Media Server/\\',            # Synology, Asustor\\n* \\'/raid0/data/module/Plex/sys/Plex Media Server/\\',                          # Thecus\\n* \\'/raid0/data/PLEX_CONFIG/Plex Media Server/\\'                               # Thecus Plex community\\n\\n\\nDownload the latest release from https://github.com/mmmmmtasty/SportScanner/releases\\nExtract files\\nCopy the extracted directory \"Scanners\" into your Plex main folder location - check the list above for more clues\\nCopy the extracted directory \"SportScanner.bundle\" into the Plug-ins directory in your main folder location - check the list above for more clues\\nYou may need to restart Plex\\nCreate a new library and under Advanced options you should be able to select \"SportScanner\" as both your scanner and metadata agent.\\n\\n#Media Format\\nThe SportScanner scanner requires one of two folder structures to work correctly, the first of which matches Plex\\'s standard folder structure.\\n##RECOMMENDED METHOD\\nFollow the Plex standards for folder structure - TV Show\\\\Season<files>. For SportScanner, TV Shows = League Name. For example for 2015/2016 NHL you would do something like the following:\\n\\n~LibraryRoot/NHL/Season 1516/NHL.2015.09.25.New-York-Islanders.vs.Philadelphia-Flyers.720p.HDTV.60fps.x264-Reborn4HD_h.mp4\\n\\nIn this scenario you still need all the information in the file name, I aim to remove that requirement down the line. The only information that comes only from the folder structure is the season.\\n##Alternative naming standard\\nYou can also choose to ignore the season directory and have the scanner work it out with a folder structure like so:\\n\\n~LibraryRoot/Ice Hockey/NHL/NHL.2015.09.25.New-York-Islanders.vs.Philadelphia-Flyers.720p.HDTV.60fps.x264-Reborn4HD_h.mp4\\n\\nTHERE IS A DOWN SIDE TO THIS! For this to work you must include a file in each league directory called \"SportScanner.txt\" that contains information about how the seasons work for this sport. The first line in the file will always be \"XXXX\" or \"XXYY\". \"XXXX\" means that the seasons happens within one calendar year and will therefore be named \"2015\" of \"1999\" for example. \"XXYY\" means that a season occurs across two seasons and will take the format \"1516\" or \"9899\" for example. When you define the season as \"XXYY\" you MUST then on the next line write the integer values of a month and a day in the form \"month,day\". This should be a a month and a day somewhere in the off-season for that sport. This tells the scanner when one season has finished and the next one is beginning to ensure that it puts files in the correct season based off the date the event happened. As an example, if you are trying to add NHL you would create a file at the following path:\\n\\n~LibraryRoot/Ice Hockey/NHL/SportScanner.txt\\n\\nIn this instance the contents of this file would be as follows, saying that seasons should be in \"XXYY\" format and a date in the middle of the off-season is 1st July:\\nXXYY\\n7,1\\nNOT RECOMMENDED (but works for now)\\nSportScanner does not actually pay attention to the name of the League directory when it comes to matching events - all info has to be in the filename. This means that you can still group all sports together and as long as they share a season format you can create a SportScanner.txt file as outlined above and everything will work.\\nThis is rubbish, it kind of accidentally works, I don\\'t recommend it as I will cut it out as part of improvement works in future.\\n#Known Issues\\n\\nNo posters for seasons\\nCan only handle individual files, not multipart or those in folders\\nAll information must be in the filename regardless of the directory structure.\\n\\n'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Sports Betting with RL\\nOverview\\nThis is the code for this video on Youtube by Siraj Raval on Sports Betting using Reinforcement Learning. This is apart of the Move 37 course at the School of AI.\\nDependencies\\nNone.\\nUsage\\nType python value_iteration.py into terminal and it will run.\\nHistory\\nThis is an adapted version of the \"Gambler\\'s Problem\" that I\\'ve applied to sports betting. Details below\\n-The Gambler Problem as discussed in Example 4.3 in Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.\\n-The problem from the book is described below:\\nGambler’s Problem: A gambler has the opportunity to make bets\\non the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as\\nmany dollars as he has staked on that flip; if it is tails, he loses his stake. The game\\nends when the gambler wins by reaching his goal of $100, or loses by running out of\\nmoney. On each flip, the gambler must decide what portion of his capital to stake,\\nin integer numbers of dollars. This problem can be formulated as an undiscounted,\\nepisodic, finite MDP. The state is the gambler’s capital, s ∈ {1, 2, . . . , 99} and the\\nactions are stakes, a ∈ {0, 1, . . . , min(s, 100−s)}. The reward is zero on all transitions\\nexcept those on which the gambler reaches his goal, when it is +1. The state-value\\nfunction then gives the probability of winning from each state. A policy is a mapping\\nfrom levels of capital to stakes. The optimal policy maximizes the probability of\\nreaching the goal. Let ph denote the probability of the coin coming up heads. If ph\\nis known, then the entire problem is known and it can be solved, for instance, by\\nvalue iteration\\n'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"YHandler\\nYahoo Fantasy Sports OAuth And Request Handler\\nThis is the Python Script I use to access the Yahoo Fantasy Sports API via OAuth for my desktop app. It's still far from polished, and not the most generalized, but has been updated to work with newer version of the requests library. It should be okay with future versions of Requests as the OAuth support has been written specifically for Yahoo's OAuth 1.0a process, which allows refresh of the access token. It looks like they now also support OAuth 2.0, but still remain backward compatible with OAuth1.0a.\\n\\nInstallation\\nYou can install using pip:\\npip install YHandler\\n\\n\\nHow To Use\\nCopy the auth.json.sample file and rename to auth.json and then place your consumer key, and consumer secret in the auth.json file.\\nIn [1]: from YHandler import YHandler, YQuery\\n\\nIn [2]: handler = YHandler()\\n\\nIn [3]: query = YQuery(handler, 'nfl')\\n\\nIn [4]: query.get_games_info()\\nOut[4]:\\n[{'code': 'nfl',\\n  'key': '359',\\n  'name': 'Football',\\n  'season': '2016',\\n  'type': '2016'}]\\n\\nIn [5]: query.get_games_info(True)\\nOut[5]:\\n[{'code': 'nfl',\\n  'key': '359',\\n  'name': 'Football',\\n  'season': '2016',\\n  'type': '2016'}]\\n\\nIn [10]: query.get_user_leagues()\\nOut[10]:\\n[{'id': '577090',\\n  'is_finished': True,\\n  'name': 'IniTeCh',\\n  'season': '2015',\\n  'week': '16'},\\n {'id': '126737',\\n  'is_finished': False,\\n  'name': 'Yahoo Public 126737',\\n  'season': '2016',\\n  'week': '1'}]\\n\\nIn [17]: query.find_player(126737, 'antonio brown')\\nOut[17]: [{'id': '24171', 'name': 'Antonio Brown', 'team': 'Pittsburgh Steelers'}]\\n\\nIn [18]: query.get_player_week_stats(24171, '8')\\nOut[18]:\\n{'0': {'detail': 'Games Played', 'name': 'GP', 'value': '0'},\\n '1': {'detail': 'Passing Attempts', 'name': 'Pass Att', 'value': '0'},\\n '10': {'detail': 'Rushing Touchdowns', 'name': 'Rush TD', 'value': '0'},\\n '11': {'detail': 'Receptions', 'name': 'Rec', 'value': '0'},\\n '12': {'detail': 'Reception Yards', 'name': 'Rec Yds', 'value': '0'},\\n '13': {'detail': 'Reception Touchdowns', 'name': 'Rec TD', 'value': '0'},\\n '14': {'detail': 'Return Yards', 'name': 'Ret Yds', 'value': '0'},\\n '15': {'detail': 'Return Touchdowns', 'name': 'Ret TD', 'value': '0'},\\n '16': {'detail': '2-Point Conversions', 'name': '2-PT', 'value': '0'},\\n '17': {'detail': 'Fumbles', 'name': 'Fum', 'value': '0'},\\n '18': {'detail': 'Fumbles Lost', 'name': 'Fum Lost', 'value': '0'},\\n '2': {'detail': 'Completions', 'name': 'Comp', 'value': '0'},\\n '3': {'detail': 'Incomplete Passes', 'name': 'Inc', 'value': '0'},\\n '4': {'detail': 'Passing Yards', 'name': 'Pass Yds', 'value': '0'},\\n '5': {'detail': 'Passing Touchdowns', 'name': 'Pass TD', 'value': '0'},\\n '57': {'detail': 'Offensive Fumble Return TD',\\n  'name': 'Fum Ret TD',\\n  'value': '0'},\\n '58': {'detail': 'Pick Sixes Thrown', 'name': 'Pick Six', 'value': '0'},\\n '59': {'detail': '40+ Yard Completions', 'name': '40 Yd Comp', 'value': '0'},\\n '6': {'detail': 'Interceptions', 'name': 'Int', 'value': '0'},\\n '60': {'detail': '40+ Yard Passing Touchdowns',\\n  'name': '40 Yd Pass TD',\\n  'value': '0'},\\n '61': {'detail': '40+ Yard Run', 'name': '40 Yd Rush', 'value': '0'},\\n '62': {'detail': '40+ Yard Rushing Touchdowns',\\n  'name': '40 Yd Rush TD',\\n  'value': '0'},\\n '63': {'detail': '40+ Yard Receptions', 'name': '40 Yd Rec', 'value': '0'},\\n '64': {'detail': '40+ Yard Reception Touchdowns',\\n  'name': '40 Yd Rec TD',\\n  'value': '0'},\\n '7': {'detail': 'Sacks', 'name': 'Sack', 'value': '0'},\\n '78': {'detail': 'Targets', 'name': 'Targets', 'value': '0'},\\n '79': {'detail': 'Passing 1st Downs', 'name': 'Pass 1st Downs', 'value': '0'},\\n '8': {'detail': 'Rushing Attempts', 'name': 'Rush Att', 'value': '0'},\\n '80': {'detail': 'Receiving 1st Downs',\\n  'name': 'Rec 1st Downs',\\n  'value': '0'},\\n '81': {'detail': 'Rushing 1st Downs', 'name': 'Rush 1st Downs', 'value': '0'},\\n '9': {'detail': 'Rushing Yards', 'name': 'Rush Yds', 'value': '0'}}\\n\\n\"},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"跑步是不可能跑步的\\n高校体育app自动跑步\\n仅供交流学习使用，not for evil use :)\\n使用了百度地图api自动寻路（虽然可能很绕 已经挺像真人跑的喽 :) ）\\n\\n\\n使用方法\\n有 本地直接运行 和 运行微信机器人 两种方法， 微信机器人 只建议有服务器的同学使用\\n本地直接运行\\nwindows\\n\\n下载 /dist/run.exe， 运行，输入账号密码即可完成一次锻炼（为了避免封号， 完全模拟了跑步流程，耗时较长，未完成前不要关闭）\\n\\nlinux/macos\\ngit clone https://github.com/FengLi666/sports.git\\ncd sports\\npip3 install -r requirement.txt\\nexport PYTHONPATH='.'\\npython3 ./mysports/run.py\\n\\n\\n输入账号密码\\n默认情况下跑步数据在一段时间后才会提交给app服务器(即你要保持这个进程一直运行）\\n如果想立即提交跑步数据\\n可以使用如下命令\\npython3 ./mysports/run.py --debug True\\n\\n运行微信机器人\\ngit clone https://github.com/FengLi666/sports.git\\ncd sports\\npip3 install -r requirement.txt\\nexport PYTHONPATH='.'\\npython3 ./wechat_bot/wechat_bot.py\\n\\n具体见代码\\n\\n感谢 @RyuBAI\\n\"},\n",
       " {'language': 'Python 99.0',\n",
       "  'readme': \"  \\n   \\n \\n \\n\\n\\nsports-betting\\nsports-betting is a tool that makes it easy to create machine learning based\\nmodels for sports betting and evaluate their performance. It is compatible with\\nscikit-learn.\\n\\nDocumentation\\nInstallation documentation, API documentation, and examples can be found on the\\ndocumentation.\\n\\nDependencies\\nsports-betting is tested to work under Python 3.6+. The dependencies are the\\nfollowing:\\n\\nnumpy(>=1.1)\\nscikit-learn(>=0.21)\\n\\nAdditionally, to run the examples, you need matplotlib(>=2.0.0) and\\npandas(>=0.22).\\n\\nInstallation\\nsports-betting is currently available on the PyPi's repository and you can\\ninstall it via pip:\\npip install -U sports-betting\\n\\nThe package is released also in Anaconda Cloud platform:\\nconda install -c algowit sports-betting\\n\\nIf you prefer, you can clone it and run the setup.py file. Use the following\\ncommands to get a copy from GitHub and install all dependencies:\\ngit clone https://github.com/AlgoWit/sports-betting.git\\ncd sports-betting\\npip install .\\n\\nOr install using pip and GitHub:\\npip install -U git+https://github.com/AlgoWit/sports-betting.git\\n\\n\\nTesting\\nAfter installation, you can use pytest to run the test suite:\\nmake test\\n\\n\"},\n",
       " {'language': 'Python 61.9',\n",
       "  'readme': \"Data Engineering Projects\\n\\nProject 1: Data Modeling with Postgres\\nIn this project, we apply Data Modeling with Postgres and build an ETL pipeline using Python. A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Currently, they are collecting data in json format and the analytics team is particularly interested in understanding what songs users are listening to.\\nLink: Data_Modeling_with_Postgres\\nProject 2: Data Modeling with Cassandra\\nIn this project, we apply Data Modeling with Cassandra and build an ETL pipeline using Python. We will build a Data Model around our queries that we want to get answers for.\\nFor our use case we want below answers:\\n\\nGet details of a song that was herad on the music app history during a particular session.\\nGet songs played by a user during particular session on music app.\\nGet all users from the music app history who listened to a particular song.\\n\\nLink : Data_Modeling_with_Apache_Cassandra\\nProject 3: Data Warehouse\\nIn this project, we apply the Data Warehouse architectures we learnt and build a Data Warehouse on AWS cloud. We build an ETL pipeline to extract and transform data stored in json format in s3 buckets and move the data to Warehouse hosted on Amazon Redshift.\\nUse Redshift IaC script - Redshift_IaC_README\\nLink  - Data_Warehouse\\nProject 4: Data Lake\\nIn this project, we will build a Data Lake on AWS cloud using Spark and AWS EMR cluster. The data lake will serve as a Single Source of Truth for the Analytics Platform. We will write spark jobs to perform ELT operations that picks data from landing zone on S3 and transform and stores data on the S3 processed zone.\\nLink: Data_Lake\\nProject 5: Data Pipelines with Airflow\\nIn this project, we will orchestrate our Data Pipeline workflow using an open-source Apache project called Apache Airflow. We will schedule our ETL jobs in Airflow, create project related custom plugins and operators and automate the pipeline execution.\\nLink:  Airflow_Data_Pipelines\\nProject 6: Api Data to Postgres\\nIn this project, we build an etl pipeline to fetch data from yelp API and insert it into the Postgres Database. This project is a very basic example of fetching real time data from an open source API.\\nLink: API to Postgres\\nCAPSTONE PROJECT\\nUdacity provides their own crafted Capstone project with dataset that include data on immigration to the United States, and supplementary datasets that include data on airport codes, U.S. city demographics, and temperature data.\\nI worked on my own open-ended project. \\nHere is the link - goodreads_etl_pipeline\\n\"},\n",
       " {'language': 'Python 99.9',\n",
       "  'readme': \"\\n\\nPyQtGraph\\nA pure-Python graphics library for PyQt/PySide/PyQt5/PySide2\\nCopyright 2020 Luke Campagnola, University of North Carolina at Chapel Hill\\nhttp://www.pyqtgraph.org\\nPyQtGraph is intended for use in mathematics / scientific / engineering applications.\\nDespite being written entirely in python, the library is fast due to its\\nheavy leverage of numpy for number crunching, Qt's GraphicsView framework for\\n2D display, and OpenGL for 3D display.\\nRequirements\\n\\nPython 2.7, or 3.x\\nRequired\\n\\nPyQt 4.8+, PySide, PyQt5, or PySide2\\nnumpy\\n\\n\\nOptional\\n\\nscipy for image processing\\npyopengl for 3D graphics\\nhdf5 for large hdf5 binary format support\\n\\n\\n\\nQt Bindings Test Matrix\\nThe following table represents the python environments we test in our CI system.  Our CI system uses Ubuntu 18.04, Windows Server 2019, and macOS 10.15 base images.\\n\\n\\n\\nQt-Bindings\\nPython 2.7\\nPython 3.6\\nPython 3.7\\nPython 3.8\\n\\n\\n\\n\\nPyQt-4\\n✅\\n❌\\n❌\\n❌\\n\\n\\nPySide1\\n✅\\n❌\\n❌\\n❌\\n\\n\\nPyQt5-5.9\\n❌\\n✅\\n❌\\n❌\\n\\n\\nPySide2-5.13\\n❌\\n❌\\n✅\\n❌\\n\\n\\nPyQt5-Latest\\n❌\\n❌\\n❌\\n✅\\n\\n\\nPySide2-Latest\\n❌\\n❌\\n❌\\n✅\\n\\n\\n\\n\\npyqtgraph has had some incompatibilities with PySide2 versions 5.6-5.11, and we recommend you avoid those versions if possible\\non macOS with Python 2.7 and Qt4 bindings (PyQt4 or PySide) the openGL related visualizations do not work reliably\\n\\nSupport\\n\\nReport issues on the GitHub issue tracker\\nPost questions to the mailing list / forum or StackOverflow\\n\\nInstallation Methods\\n\\nFrom PyPI:\\n\\nLast released version: pip install pyqtgraph\\nLatest development version: pip install git+https://github.com/pyqtgraph/pyqtgraph@master\\n\\n\\nFrom conda\\n\\nLast released version: conda install -c conda-forge pyqtgraph\\n\\n\\nTo install system-wide from source distribution: python setup.py install\\nMany linux package repositories have release versions.\\nTo use with a specific project, simply copy the pyqtgraph subdirectory\\nanywhere that is importable from your project.\\n\\nDocumentation\\nThe official documentation lives at https://pyqtgraph.readthedocs.io\\nThe easiest way to learn pyqtgraph is to browse through the examples; run python -m pyqtgraph.examples to launch the examples application.\\n\"},\n",
       " {'language': 'Python 89.0',\n",
       "  'readme': \"Data Engineering 101: Building a Data Pipeline\\nThis repository contains the files and data from the workshop as well as resources around Data Engineering. For the workshop (and after) we will use a Discord chatroom to keep the conversation going: https://discord.gg/86cYcgU.\\nAnd/or please do not hesitate to reach out to me directly via email at inquiries@jonathan.industries or over twitter @memoryphoneme\\nThe presentation can be found on Slideshare here or in this repository (presentation.pdf). Video can be found here.\\n\\n\\nThroughout this workshop, you will learn how to make a scalable and sustainable data pipeline in Python with Luigi\\n\\nLearning Objectives\\n\\nRun a simple 1 stage Luigi flow reading/writing to local files\\nWrite a Luigi flow containing stages with multiple dependencies\\n\\nVisualize the progress of the flow using the centralized scheduler\\nParameterize the flow from the command line\\nOutput parameter specific output files\\n\\n\\nManage serialization to/from a Postgres database\\nIntegrate a Hadoop Map/Reduce task into an existing flow\\nParallelize non-dependent stages of a multi-stage Luigi flow\\nSchedule a local Luigi job to run once every day\\nRun any arbitrary shell command in a repeatable way\\n\\nPrerequisites\\nPrior experience with Python and the scientific Python stack is beneficial.  The workshop will focus on using the Luigi framework, but will have code from the following lobraries as well:\\n\\nnumpy\\nscikit-learn\\nFlask\\n\\nRun the Code\\nLocal\\n\\nInstall libraries and dependencies: pip install -r requirements.txt\\nStart the UI server: luigid --background --logdir logs\\nNavigate with a web browser to http://localhost:[port] where [port] is the port the luigid server has started on (luigid defaults to port 8082)\\nstart the API Server: python app.py\\nEvaluate Model: python ml-pipeline.py EvaluateModel --input-dir text --lam 0.8\\nRun evaluation server (at localhost:9191): topmodel/topmodel_server.py\\nRun the final pipeline: python ml-pipeline.py BuildModels --input-dir text --num-topics 10 --lam 0.8\\n\\n--\\nFor parallelism, set --workers (note this is Task parallelism):\\npython ml-pipeline.py BuildModels --input-dir text --num-topics 10 --lam 0.8 --workers 4\\nHadoop\\n\\nStart Hadoop cluster: bin/start-dfs.sh; sbin/start-yarn.sh\\nSetup Directory Structure: hadoop fs -mkdir /tmp/text\\nGet files on cluster: hadoop fs -put ./data/text /tmp/text\\nRetrieve results: hadoop fs -getmerge /tmp/text-count/2012-06-01 ./counts.txt\\nView results: head ./counts.txt\\n\\nFlask\\n\\ndocker run -it -v /LOCAL/PATH/TO/REPO/data-engineering-101:/root/workshop clearspandex/pydata-seattle bash\\npip2 install flask\\nipython2 app.py\\n\\nLibraries Used\\n\\nluigi\\nscikit-learn\\nnltk\\nipdb\\n\\nWhats in here?\\ntext/                   20newsgroups text files\\ntopmodel/               Stripe's topmodel evaluation library\\nexample_luigi.py        example scaffold of a luigi pipeline\\nhadoop_word_count.py    example luigi pipeline using Hadoop\\nml-pipeline.py          luigi pipeline covered in workshop\\napp.py                  Flask server to deploy a scikit-learn model\\nLICENSE                 Details of rights of use and distribution\\npresentation.pdf        lecture slides from presentation\\nreadme.md               this file!\\n\\nThe Data\\nThe data (in the text/ folder) is from the 20 newsgroups dataset, a standard benchmarking dataset for machine learning and NLP.  Each file in text corresponds to a single 'document' (or post) from one of two selected newsgroups (comp.sys.ibm.pc.hardware or alt.atheism).  The first line provides which group the document is from and everything thereafter is the body of the post.\\ncomp.sys.ibm.pc.hardware\\nI'm looking for a better method to back up files.  Currently using a MaynStream\\n250Q that uses DC 6250 tapes.  I will need to have a capacity of 600 Mb to 1Gb\\nfor future backups.  Only DOS files.\\n\\nI would be VERY appreciative of information about backup devices or\\nmanufacturers of these products.  Flopticals, DAT, tape, anything.  \\nIf possible, please include price, backup speed, manufacturer (phone #?), \\nand opinions about the quality/reliability.\\n\\nPlease E-Mail, I'll send summaries to those interested.\\n\\nThanx in advance,\\n\\nResources/References\\n\\nQuestioning the Lambda Architecture\\nLuigi: NYC Data Science Meetup\\nThe Log: What every software engineer should know about real-time data's unifying abstraction\\nI (heart) Log\\nWhy Loggly Loves Apache Kafka\\nBuffer's New Data Architecture\\nPutting Apache Kafka to Use\\nMetric Driven Development\\nThe Unified Logging Infrastructure for Data Analytics at Twitter\\nStream Processing and Mining just got more interesting\\nHow to Beat the CAP Theorem\\nBeating the CAP Theorem Checklist\\n\\nLicense\\nCopyright 2015 Jonathan Dinu.\\nAll files and content licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License\\n\"},\n",
       " {'language': 'Python 98.1',\n",
       "  'readme': '\\n\\n\\n\\n\\nTheme\\nStatus\\n\\n\\n\\n\\nPython Version\\n\\n\\n\\nLatest PyPI Release\\n\\n\\n\\nLatest Conda Release\\n\\n\\n\\nmaster Branch Build\\n\\n\\n\\ndevelop Branch Build\\n\\n\\n\\nDocumentation Build\\n\\n\\n\\nLicense\\n\\n\\n\\nCode Style\\n\\n\\n\\nQuestions\\n\\n\\n\\n\\nWhat is Kedro?\\n\\n\"The centre of your data pipeline.\"\\n\\nKedro is an open-source Python framework that applies software engineering best-practice to data and machine-learning pipelines.  You can use it, for example, to optimise the process of taking a machine learning model into a production environment. You can use Kedro to organise a single user project running on a local environment, or collaborate within a team on an enterprise-level project.\\nWe provide a standard approach so that you can:\\n\\nWorry less about how to write production-ready code,\\nSpend more time building data pipelines that are robust, scalable, deployable, reproducible and versioned,\\nStandardise the way that your team collaborates across your project.\\n\\nHow do I install Kedro?\\nkedro is a Python package built for Python 3.6, 3.7 and 3.8.\\nTo install Kedro from the Python Package Index (PyPI) simply run:\\npip install kedro\\n\\nYou can also install kedro using conda, a package and environment manager program bundled with Anaconda. With conda already installed, simply run:\\nconda install -c conda-forge kedro\\n\\nOur Get Started guide contains full installation instructions, and includes how to set up Python virtual environments.\\nWe also recommend the frequently asked questions and the API reference documentation for additional information.\\nWhat are the main features of Kedro?\\n\\nA pipeline visualisation generated using Kedro-Viz\\n\\n\\n\\nFeature\\nWhat is this?\\n\\n\\n\\n\\nProject Template\\nA standard, modifiable and easy-to-use project template based on Cookiecutter Data Science.\\n\\n\\nData Catalog\\nA series of lightweight data connectors used for saving and loading data across many different file formats and file systems including local and network file systems, cloud object stores, and HDFS. The Data Catalog also includes data and model versioning for file-based systems. Used with a Python or YAML API.\\n\\n\\nPipeline Abstraction\\nAutomatic resolution of dependencies between pure Python functions and data pipeline visualisation using Kedro-Viz.\\n\\n\\nThe Journal\\nAn ability to reproduce pipeline runs with saved pipeline run results.\\n\\n\\nCoding Standards\\nTest-driven development using pytest, produce well-documented code using Sphinx, create linted code with support for flake8, isort and black and make use of the standard Python logging library.\\n\\n\\nFlexible Deployment\\nDeployment strategies that include the use of Docker with Kedro-Docker, conversion of Kedro pipelines into Airflow DAGs with Kedro-Airflow, leveraging a REST API endpoint with Kedro-Server (coming soon) and serving Kedro pipelines as a Python package. Kedro can be deployed locally, on-premise and cloud (AWS, Azure and Google Cloud Platform) servers, or clusters (EMR, EC2, Azure HDinsight and Databricks).\\n\\n\\n\\nHow do I use Kedro?\\nThe Kedro documentation includes three examples to help get you started:\\n\\nA typical \"Hello World\" example, for an entry-level description of the main Kedro concepts\\nThe more detailed \"spaceflights\" tutorial to give you hands-on experience as you learn about Kedro\\n\\nAdditional documentation includes:\\n\\nAn overview of Kedro architecture\\nHow to use the CLI offered by kedro_cli.py (kedro new, kedro run, ...)\\n\\n\\nNote: The CLI is a convenient tool for being able to run kedro commands but you can also invoke the Kedro CLI as a Python module with python -m kedro\\n\\nEvery Kedro function or class has extensive help, which you can call from a Python session as follows if the item is in local scope:\\nfrom kedro.io import MemoryDataSet\\nhelp(MemoryDataSet)\\n\\nWhy does Kedro exist?\\nKedro is built upon our collective best-practice (and mistakes) trying to deliver real-world ML applications that have vast amounts of raw unvetted data. We developed Kedro to achieve the following:\\n\\nCollaboration on an analytics codebase when different team members have varied exposure to software engineering best-practice\\nA focus on maintainable data and ML pipelines as the standard, instead of a singular activity of deploying models in production\\nA way to inspire the creation of reusable analytics code so that we never start from scratch when working on a new project\\nEfficient use of time because we\\'re able to quickly move from experimentation into production\\n\\nThe humans behind Kedro\\nKedro was originally designed by Aris Valtazanos and Nikolaos Tsaousis to solve challenges they faced in their project work.\\nTheir work was later turned into an internal product by Peteris Erins, Ivan Danov, Nikolaos Kaltsas, Meisam Emamjome and Nikolaos Tsaousis.\\nCurrently the core Kedro team consists of Yetunde Dada, Ivan Danov, Richard Westenra, Dmitrii Deriabin, Lorena Balan, Kiyohito Kunii, Zain Patel, Lim Hoang, Andrii Ivaniuk, Jo Stichbury, Laís Carvalho, Merel Theisen, Gabriel Comym, and Liam Brummitt\\nFormer core team members with significant contributions include: Gordon Wrigley, Nasef Khan and Anton Kirilenko.\\nAnd last but not least, all the open-source contributors whose work went into all Kedro releases.\\nCan I contribute?\\nYes! Want to help build Kedro? Check out our guide to contributing to Kedro.\\nWhere can I learn more?\\nThere is a growing community around Kedro. Have a look at the Kedro FAQs to find projects using Kedro and links to articles, podcasts and talks.\\nWho is using Kedro?\\n\\nAI Singapore\\nCaterpillar\\nElementAI\\nJungle Scout\\nMercadoLibre Argentina\\nMosaic Data Science\\nNaranjaX\\nOpen Data Science LatAm\\nRetrieva\\nRoche\\nUrbanLogiq\\nXP\\nDendra Systems\\n\\nWhat licence do you use?\\nKedro is licensed under the Apache 2.0 License.\\nWe\\'re hiring!\\nDo you want to be part of the team that builds Kedro and other great products at QuantumBlack? If so, you\\'re in luck! QuantumBlack is currently hiring Software Engineers who love using data to drive their decisions. Take a look at our open positions and see if you\\'re a fit.\\n'},\n",
       " {'language': 'Python 96.0',\n",
       "  'readme': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGeomancer is a geospatial feature engineering library. It leverages geospatial\\ndata such as OpenStreetMap (OSM) alongside a\\ndata warehouse like BigQuery. You can use this to create, share, and iterate\\ngeospatial features for your downstream tasks (analysis, modelling,\\nvisualization, etc.).\\n\\n\\n\\nFeatures\\nGeomancer can perform geospatial feature engineering for all types of vector data\\n(i.e. points, lines, polygons).\\n\\nFeature primitives for geospatial feature engineering\\nAbility to switch out data warehouses (BigQuery, SQLite, PostgreSQL (In Progress))\\nCompile and share your features using our SpellBook\\n\\nSetup and Installation\\nInstalling the library\\nGeomancer can be installed using pip.\\n$ pip install geomancer\\n\\nThis will install all dependencies for every data-warehouse we support. If\\nyou wish to do this only for a specific warehouse, then you can add an\\nidentifier:\\n$ pip install geomancer[bq] # For BigQuery\\n$ pip install geomancer[sqlite] # For SQLite\\n$ pip install geomancer[psql] # For PostgreSQL\\n\\nAlternatively, you can also clone the repository then run install.\\n$ git clone https://github.com/thinkingmachines/geomancer.git\\n$ cd geomancer\\n$ python setup.py install\\n\\nSetting up your data warehouse\\nGeomancer is powered by a geospatial data warehouse: we highly-recommend using\\nBigQuery as your data warehouse and\\nGeofabrik\\'s OSM catalog as your\\nsource of Points and Lines of interest.\\n\\nYou can see the set-up instructions in this link\\nBasic Usage\\nAll of the feature engineering functions in Geomancer are called \"spells\". For\\nexample, you want to get the distance to the nearest supermarket for each\\npoint.\\nfrom geomancer.spells import DistanceToNearest\\n\\n# Load your dataset in a pandas dataframe\\n# df = load_dataset()\\n\\ndist_spell = DistanceToNearest(\\n    \"supermarket\",\\n    source_table=\"ph_osm.gis_osm_pois_free_1\",\\n    feature_name=\"dist_supermarket\",\\n    dburl=\"bigquery://project-name\",\\n).cast(df)\\nYou can specify the type of filter  using the format {column}:{filter}.  By\\ndefault, the column value is fclass. For example, if you wish to look for\\nroads on a bridge, then pass bridge:T:\\nfrom geomancer.spells import DistanceToNearest\\n\\n# Load the dataset in a pandas dataframe\\n# df = load_dataset()\\n\\ndist_spell = DistanceToNearest(\\n    \"bridge:T\",\\n    source_table=\"ph_osm.gis_osm_roads_free_1\",\\n    feature_name=\"dist_road_bridges\",\\n    dburl=\"bigquery://project-name\",\\n).cast(df)\\nCompose multiple spells into a \"spell book\" which you can export as a JSON file.\\nfrom geomancer.spells import DistanceToNearest\\nfrom geomancer.spellbook import SpellBook\\n\\nspellbook = SpellBook([\\n    DistanceToNearest(\\n        \"supermarket\",\\n        source_table=\"ph_osm.gis_osm_pois_free_1\",\\n        feature_name=\"dist_supermarket\",\\n        dburl=\"bigquery://project-name\",\\n    ),\\n    DistanceToNearest(\\n        \"embassy\",\\n        source_table=\"ph_osm.gis_osm_pois_free_1\",\\n        feature_name=\"dist_embassy\",\\n        dburl=\"bigquery://project-name\",\\n    ),\\n])\\nspellbook.to_json(\"dist_supermarket_and_embassy.json\")\\nYou can share the generated file so other people can re-use your feature extractions\\nwith their own datasets.\\nfrom geomancer.spellbook import SpellBook\\n\\n# Load the dataset in a pandas dataframe\\n# df = load_dataset()\\n\\nspellbook = SpellBook.read_json(\"dist_supermarket_and_embassy.json\")\\ndist_supermarket_and_embassy = spellbook.cast(df)\\nContributing\\nThis project is open for contributors! Contibutions can come in the form of\\nfeature requests, bug fixes, documentation, tutorials and the like! We highly\\nrecommend to file an Issue first before submitting a Pull\\nRequest.\\nSimply fork this repository and make a Pull Request! We\\'d definitely appreciate:\\n\\nImplementation of new features\\nBug Reports\\nDocumentation\\nTesting\\n\\nAlso, we have a\\nCONTRIBUTING\\nand a CODE_OF_CONDUCT,\\nso please check that one out!\\nLicense\\nMIT License © 2019, Thinking Machines Data Science\\n'},\n",
       " {'language': 'Python 75.8',\n",
       "  'readme': \"Datasets\\nWe provide the datasets used in our empirical studies and evaluations. The description is provided according to the type of datasets and the associated work.\\nIf you use our dataset, please cite our relevant paper in your publication. The bib  is also provided.\\n\\nThere are three groups of datasets according to the dataset's characteristics.\\n\\n\\nagile sprints\\nThese are the datasets on the iterative development (e.g. sprint). Our work on these datasets focuses on predicting delivery capability in iterative development. We published the work in IEEE TSE\\n\\n\\ndelay issues\\nThese are the datasets on the delayed issues. We used these datasets in our work on predicting delayed issues which we published in MSR2015, ASE2015, and in the journal of Empirical Software Engineering.\\n\\n\\nstory points\\nThese are the datasets on the story point estimation. We provide the datasets and the models from our work on a deep learning model for estimating story points which we published in IEEE TSE.\\n\\n\\nYou can also find preprints in the folders.\\nVisit our homepage for more informaiton SEA@UOW\\n\\nbib\\n\\nIEEE TSE2018: A deep learning model for estimating story points\\n\\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, T. T. M. Pham, A. Ghose, and T. Menzies, “A deep learning model for estimating story points,” IEEE Trans. Softw. Eng., vol. PP, no. 99, p. 1, 2018.\\n@article{Choetkiertikul2018,\\nauthor = {Choetkiertikul, M and Dam, H K and Tran, T and Pham, T T M and Ghose, A and Menzies, T},\\ndoi = {10.1109/TSE.2018.2792473},\\nissn = {0098-5589 VO  - PP},\\njournal = {IEEE Transactions on Software Engineering},\\nkeywords = {Estimation,Machine learning,Planning,Predictive models,Software,Springs,deep learning,effort estimation,software analytics,story point estimation},\\nnumber = {99},\\npages = {1},\\ntitle = {{A deep learning model for estimating story points}},\\nvolume = {PP},\\nyear = {2018}\\n}\\n\\n\\nIEEE TSE2017: Predicting Delivery Capability in Iterative Software Development\\n\\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, A. Ghose, and J. Grundy, “Predicting Delivery Capability in Iterative Software Development,” IEEE Trans. Softw. Eng., vol. 14, no. 8, pp. 1–1, 2017.\\n@article{Choetkiertikul2017,\\ntitle = {{Predicting Delivery Capability in Iterative Software Development}},\\nauthor = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya and Grundy, John},\\ndoi = {10.1109/TSE.2017.2693989},\\nissn = {0098-5589},\\njournal = {IEEE Transactions on Software Engineering},\\nnumber = {8},\\npages = {1--1},\\nvolume = {14},\\nyear = {2017}\\n}\\n\\n\\nEMSE2017: Predicting the delay of issues with due dates in software projects\\n\\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, “Predicting the delay of issues with due dates in software projects,” Empir. Softw. Eng., vol. 22, no. 3, pp. 1223–1263, 2017.\\n@article{Choetkiertikul2017,\\ntitle = {{Predicting the delay of issues with due dates in software projects}},\\nauthor = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},\\ndoi = {10.1007/s10664-016-9496-7},\\nissn = {15737616},\\njournal = {Empirical Software Engineering},\\nnumber = {3},\\npages = {1223--1263},\\npublisher = {Empirical Software Engineering},\\nvolume = {22},\\nyear = {2017}\\n}\\n\\n\\nMSR2015: Characterization and prediction of issue-related risks in software projects\\n\\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, “Characterization and prediction of issue-related risks in software projects,” in Proceedings of the 12th Working Conference on Mining Software Repositories (MSR), 2015, pp. 280–291.\\n@inproceedings{Morakot2015,\\ntitle = {{Characterization and Prediction of Issue-Related Risks in Software Projects}},\\nauthor = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},\\nbooktitle = {Proceedings of the 12th IEEE/ACM Working Conference on Mining Software Repositories (MSR)},\\ndoi = {10.1109/MSR.2015.33},\\nisbn = {978-0-7695-5594-2},\\nissn = {21601860},\\npages = {280--291},\\npublisher = {IEEE},\\nurl = {http://ieeexplore.ieee.org/document/7180087/},\\nyear = {2015}\\n}\\n\\n\\nASE2015: Predicting delays in software projects using networked classification\\n\\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, “Predicting delays in software projects using networked classification,” in Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2015, pp. 353–364.\\n@inproceedings{Choetkiertikul2015,\\ntitle = {{Predicting delays in software projects using networked classification}},\\nauthor = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},\\nbooktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},\\ndoi = {10.1109/ASE.2015.55},\\nisbn = {9781509000241},\\npages = {353--364},\\nyear = {2015}\\n}\\n\\n\\n\"},\n",
       " {'language': 'Python 99.1',\n",
       "  'readme': 'socrata-py\\nPython SDK for the Socrata Data Management API. Use this library to call into publishing and ETL functionality offered when writing to Socrata datasets.\\nwith open(\\'cool_dataset.csv\\', \\'rb\\') as file:\\n    (revision, output) = Socrata(auth).create(\\n        name = \"cool dataset\",\\n        description = \"a description\"\\n    ).csv(file)\\n    \\n    revision.apply(output_schema = output)\\n\\nInstallation\\nExample\\nUsing\\n\\nBoilerplate\\nSimple usage\\n\\nCreate a new Dataset from a csv, tsv, xls or xlsx file\\nCreate a new Dataset from Pandas\\nUpdating a dataset\\nGenerating a config and using it to update\\n\\n\\n\\n\\nAdvanced usage\\n\\nCreate a revision\\nCreate an upload\\nUpload a file\\nTransforming your data\\nWait for the transformation to finish\\nErrors in a transformation\\nValidating rows\\nDo the upsert!\\nMetadata only revisions\\n\\n\\n\\n\\nDevelopment\\n\\nTesting\\nGenerating docs\\nReleasing\\n\\n\\nLibrary Docs\\n\\nSocrata\\n\\ncreate\\nnew\\nusing_config\\n\\n\\nAuthorization\\n\\nlive_dangerously\\n\\n\\nRevisions\\n\\ncreate_delete_revision\\ncreate_replace_revision\\ncreate_update_revision\\ncreate_using_config\\nlist\\nlookup\\n\\n\\nRevision\\n\\napply\\ncreate_upload\\ndiscard\\nlist_operations\\nopen_in_browser\\nplan\\nset_output_schema\\nsource_as_blob\\nsource_from_agent\\nsource_from_dataset\\nsource_from_url\\nui_url\\nupdate\\n\\n\\nSources\\n\\ncreate_upload\\nlookup\\n\\n\\nSource\\n\\nadd_to_revision\\nblob\\nchange_parse_option\\ncsv\\ndf\\ngeojson\\nkml\\nlist_operations\\nload\\nopen_in_browser\\nshapefile\\ntsv\\nui_url\\nwait_for_finish\\nxls\\nxlsx\\n\\n\\nConfigs\\n\\ncreate\\nlist\\nlookup\\n\\n\\nConfig\\n\\nchange_parse_option\\ncreate_revision\\ndelete\\nlist_operations\\nupdate\\n\\n\\nInputSchema\\n\\nget_latest_output_schema\\nlatest_output\\nlist_operations\\ntransform\\n\\n\\nOutputSchema\\n\\nadd_column\\nbuild_config\\nchange_column_metadata\\nchange_column_transform\\ndrop_column\\nlist_operations\\nrows\\nrun\\nschema_errors\\nschema_errors_csv\\nset_row_id\\nvalidate_row_id\\nwait_for_finish\\n\\n\\nJob\\n\\nis_complete\\nlist_operations\\nwait_for_finish\\n\\n\\n\\n\\n\\nInstallation\\nThis only supports python3.\\nInstallation is available through pip. Using a virtualenv is advised. Install\\nthe package by running\\npip3 install socrata-py\\n\\nThe only hard dependency is requests which will be installed via pip. Pandas is not required, but creating a dataset from a Pandas dataframe is supported. See below.\\nExample\\nTry the command line example with\\npython -m examples.create \\'Police Reports\\' ~/Desktop/catalog.data.gov/Seattle_Real_Time_Fire_911_Calls.csv \\'pete-test.test-socrata.com\\' --username $SOCRATA_USERNAME --password $SOCRATA_PASSWORD\\nUsing\\nBoilerplate\\n# Import some stuff\\nfrom socrata.authorization import Authorization\\nfrom socrata import Socrata\\nimport os\\n\\n# Boilerplate...\\n# Make an auth object\\nauth = Authorization(\\n  \"pete-test.test-socrata.com\",\\n  os.environ[\\'SOCRATA_USERNAME\\'],\\n  os.environ[\\'SOCRATA_PASSWORD\\']\\n)\\nSimple usage\\nCreate a new Dataset from a csv, tsv, xls or xlsx file\\nTo create a dataset, you can do this:\\nwith open(\\'cool_dataset.csv\\', \\'rb\\') as file:\\n    # Upload + Transform step\\n\\n    # revision is the *change* to the view in the catalog, which has not yet been applied.\\n    # output is the OutputSchema, which is a change to data which can be applied via the revision\\n    (revision, output) = Socrata(auth).create(\\n        name = \"cool dataset\",\\n        description = \"a description\"\\n    ).csv(file)\\n\\n    # Transformation step\\n    # We want to add some metadata to our column, drop another column, and add a new column which will\\n    # be filled with values from another column and then transformed\\n    output = output\\\\\\n        .change_column_metadata(\\'a_column\\', \\'display_name\\').to(\\'A Column!\\')\\\\\\n        .change_column_metadata(\\'a_column\\', \\'description\\').to(\\'Here is a description of my A Column\\')\\\\\\n        .drop_column(\\'b_column\\')\\\\\\n        .add_column(\\'a_column_squared\\', \\'A Column, but times itself\\', \\'to_number(`a_column`) * to_number(`a_column`)\\', \\'this is a column squared\\')\\\\\\n        .run()\\n\\n\\n    # Validation of the results step\\n    output = output.wait_for_finish()\\n    # The data has been validated now, and we can access errors that happened during validation. For example, if one of the cells in `a_column` couldn\\'t be converted to a number in the call to `to_number`, that error would be reflected in this error_count\\n    assert output.attributes[\\'error_count\\'] == 0\\n\\n    # If you want, you can get a csv stream of all the errors\\n    errors = output.schema_errors_csv()\\n    for line in errors.iter_lines():\\n        print(line)\\n\\n    # Update step\\n\\n    # Apply the revision - this will make it public and available to make\\n    # visualizations from\\n    job = revision.apply(output_schema = output)\\n\\n    # This opens a browser window to your revision, and you will see the progress\\n    # of the job\\n    revision.open_in_browser()\\n\\n    # Application is async - this will block until all the data\\n    # is in place and readable\\n    job.wait_for_finish()\\nSimilar to the csv method are the xls, xlsx, and tsv methods, which upload\\nthose files.\\nThere is a blob method as well, which uploads blobby data to the source. This means the data will not be parsed, and will be displayed under \"Files and Documents\" in the catalog once the revision is applied.\\nCreate a new Dataset from Pandas\\nDatasets can also be created from Pandas DataFrames\\nimport pandas as pd\\ndf = pd.read_csv(\\'socrata-py/test/fixtures/simple.csv\\')\\n# Do various Pandas-y changes and modifications, then...\\n(revision, output) = Socrata(auth).create(\\n    name = \"Pandas Dataset\",\\n    description = \"Dataset made from a Pandas Dataframe\"\\n).df(df)\\n\\n# Same code as above to apply the revision.\\nUpdating a dataset\\nA Socrata update is actually an upsert. Rows are updated or created based on the row identifier. If the row-identifer doesn\\'t exist, all updates are just appends to the dataset.\\nA replace truncates the whole dataset and then inserts the new data.\\nGenerating a config and using it to update\\n# This is how we create our view initially\\nwith open(\\'cool_dataset.csv\\', \\'rb\\') as file:\\n    (revision, output) = Socrata(auth).create(\\n        name = \"cool dataset\",\\n        description = \"a description\"\\n    ).csv(file)\\n\\n    revision.apply(output_schema = output)\\n\\n# This will build a configuration using the same settings (file parsing and\\n# data transformation rules) that we used to get our output. The action\\n# that we will take will be \"update\", though it could also be \"replace\"\\nconfig = output.build_config(\"cool-dataset-config\", \"update\")\\n\\n# Now we need to save our configuration name and view id somewhere so we\\n# can update the view using our config\\nconfiguration_name = \"cool-dataset-config\"\\nview_id = revision.view_id()\\n\\n# Now later, if we want to use that config to update our view, we just need the view and the configuration_name\\nsocrata = Socrata(auth)\\nview = socrata.views.lookup(view_id) # View will be the view we are updating with the new data\\n\\nwith open(\\'updated-cool-dataset.csv\\', \\'rb\\') as my_file:\\n    (revision, job) = socrata.using_config(\\n        configuration_name,\\n        view\\n    ).csv(my_file)\\n    print(job) # Our update job is now running\\nAdvanced usage\\nCreate a revision\\n# This is our socrata object, using the auth variable from above\\nsocrata = Socrata(auth)\\n\\n# This will make our initial revision, on a view that doesn\\'t yet exist\\nrevision = socrata.new({\\'name\\': \\'cool dataset\\'})\\n\\n# revision is a Revision object, we can print it\\nprint(revision)\\nRevision({\\'created_by\\': {\\'display_name\\': \\'rozap\\',\\n                \\'email\\': \\'chris.duranti@socrata.com\\',\\n                \\'user_id\\': \\'tugg-ikce\\'},\\n \\'fourfour\\': \\'ij46-xpxe\\',\\n \\'id\\': 346,\\n \\'inserted_at\\': \\'2017-02-27T23:05:08.522796\\',\\n \\'metadata\\': None,\\n \\'update_seq\\': 285,\\n \\'upsert_jobs\\': []})\\n\\n# We can also access the attributes of the revision\\nprint(revision.attributes[\\'metadata\\'][\\'name\\'])\\n\\'cool dataset\\'\\nCreate an upload\\n# Using that revision, we can create an upload\\nupload = revision.create_upload(\\'foo.csv\\')\\n\\n# And print it\\nprint(upload)\\nSource({\\'content_type\\': None,\\n \\'created_by\\': {\\'display_name\\': \\'rozap\\',\\n                \\'email\\': \\'chris.duranti@socrata.com\\',\\n                \\'user_id\\': \\'tugg-ikce\\'},\\n \\'source_type\\': {\\n    \\'filename\\': \\'foo.csv\\',\\n    \\'type\\': \\'upload\\'\\n },\\n \\'finished_at\\': None,\\n \\'id\\': 290,\\n \\'inserted_at\\': \\'2017-02-27T23:07:18.309676\\',\\n \\'schemas\\': []})\\nUpload a file\\n# And using that upload we just created, we can put bytes into it\\nwith open(\\'test/fixtures/simple.csv\\', \\'rb\\') as f:\\n    source = upload.csv(f)\\nTransforming your data\\nTransforming data consists of going from input data (data exactly as it appeared in the source)\\nto output data (data as you want it to appear).\\nTransformation from input data to output data often has problems. You might, for example, have a column\\nfull of numbers, but one row in that column is actually the value hehe! which cannot be transformed into\\na number. Rather than failing at each datum which is dirty or wrong, transforming your data allows you to\\nreconcile these issues.\\nWe might have a dataset called temps.csv that looks like\\ndate, celsius\\n8-24-2017, 22\\n8-25-2017, 20\\n8-26-2017, 23\\n8-27-2017, hehe!\\n8-28-2017,\\n8-29-2017, 21\\n\\nSuppose we uploaded it in our previous step, like this:\\nwith open(\\'temps.csv\\', \\'rb\\') as f:\\n    source = upload.csv(f)\\n    input_schema = source.get_latest_input_schema()\\nOur input_schema is the input data exactly as it appeared in the CSV, with all values of type string.\\nOur output_schema is the output data as it was guessed by Socrata. Guessing may not always be correct, which is why we have import configs to \"lock in\" a schema for automation. We can get the output_schema\\nlike so:\\noutput_schema = input_schema.get_latest_output_schema()\\nWe can now make changes to the schema, like so\\nnew_output_schema = output\\n    # Change the field_name of date to the_date\\n    .change_column_metadata(\\'date\\', \\'field_name\\').to(\\'the_date\\')\\\\\\n    # Change the description of the celsius column\\n    .change_column_metadata(\\'celsius\\', \\'description\\').to(\\'the temperature in celsius\\')\\\\\\n    # Change the display name of the celsius column\\n    .change_column_metadata(\\'celsius\\', \\'display_name\\').to(\\'Degrees (Celsius)\\')\\\\\\n    # Change the transform of the_date column to to_fixed_timestamp(`date`)\\n    .change_column_transform(\\'the_date\\').to(\\'to_fixed_timestamp(`date`)\\')\\\\\\n    # Make the celsius column all numbers\\n    .change_column_transform(\\'celsius\\').to(\\'to_number(`celsius`)\\')\\\\\\n    # Add a new column, which is computed from the `celsius` column\\n    .add_column(\\'fahrenheit\\', \\'Degrees (Fahrenheit)\\', \\'(to_number(`celsius`) * (9 / 5)) + 32\\', \\'the temperature in celsius\\')\\\\\\n    .run()\\nchange_column_metadata(column_name, column_attribute) takes the field name used to\\nidentify the column and the column attribute to change (field_name, display_name, description, position)\\nadd_column(field_name, display_name, transform_expression, description) will create a new column\\nWe can also call drop_column(celsius) which will drop the column.\\n.run() will then make a request and return the new output_schema, or an error if something is invalid.\\nTransforms can be complex SoQL expressions. Available functions are listed here. You can do lots of stuff with them;\\nFor example, you could change all null values into errors (which won\\'t be imported) by doing\\nsomething like\\nnew_output_schema = output\\n    .change_column_transform(\\'celsius\\').to(\\'coalesce(to_number(`celsius`), error(\"Celsius was null!\"))\\')\\n    .run()\\nOr you could add a new column that says if the day was hot or not\\nnew_output_schema = output\\n    .add_column(\\'is_hot\\', \\'Was the day hot?\\', \\'to_number(`celsius`) >= 23\\')\\n    .run()\\nOr you could geocode a column, given the following CSV\\naddress,city,zip,state\\n10028 Ravenna Ave NE, Seattle, 98128, WA\\n1600 Pennsylvania Avenue, Washington DC, 20500, DC\\n6511 32nd Ave NW, Seattle, 98155, WA\\n\\nWe could transform our first output_schema into a single column dataset, where that\\nsingle column is a Point of the address\\noutput = output\\\\\\n    .add_column(\\'location\\', \\'Incident Location\\', \\'geocode(`address`, `city`, `state`, `zip`)\\')\\\\\\n    .drop_column(\\'address\\')\\\\\\n    .drop_column(\\'city\\')\\\\\\n    .drop_column(\\'state\\')\\\\\\n    .drop_column(\\'zip\\')\\\\\\n    .run()\\nComposing these SoQL functions into expressions will allow you to validate, shape, clean and extend your data to make it more useful to the consumer.\\nWait for the transformation to finish\\nTransformations are async, so if you want to wait for it to finish, you can do so\\n\\nErrors in a transformation\\nTransformations may have had errors, like in the previous example, we can\\'t convert hehe! to a number. We can see the count of them like this:\\nprint(output_schema.attributes[\\'error_count\\'])\\nWe can view the detailed errors like this:\\nerrors = output_schema.schema_errors()\\nWe can get a CSV of the errors like this:\\ncsv_stream = output_schema.schema_errors_csv()\\nValidating rows\\nWe can look at the rows of our schema as well\\nrows = output_schema.rows(offset = 0, limit = 20)\\n\\nself.assertEqual(rows, [\\n    {\\'b\\': {\\'ok\\': \\' bfoo\\'}},\\n    {\\'b\\': {\\'ok\\': \\' bfoo\\'}},\\n    {\\'b\\': {\\'ok\\': \\' bfoo\\'}},\\n    {\\'b\\': {\\'ok\\': \\' bfoo\\'}}\\n])\\nDo the upsert!\\n# Now we have transformed our data into the shape we want, let\\'s do an upsert\\njob = revision.apply(output_schema = output_schema)\\n\\n# This will complete the upsert behind the scenes. If we want to\\n# re-fetch the current state of the upsert job, we can do so\\njob = job.show()\\n\\n# To get the progress\\nprint(job.attributes[\\'log\\'])\\n[\\n    {\\'details\\': {\\'Errors\\': 0, \\'Rows Created\\': 0, \\'Rows Updated\\': 0, \\'By RowIdentifier\\': 0, \\'By SID\\': 0, \\'Rows Deleted\\': 0}, \\'time\\': \\'2017-02-28T20:20:59\\', \\'stage\\': \\'upsert_complete\\'},\\n    {\\'details\\': {\\'created\\': 1}, \\'time\\': \\'2017-02-28T20:20:59\\', \\'stage\\': \\'columns_created\\'},\\n    {\\'details\\': {\\'created\\': 1}, \\'time\\': \\'2017-02-28T20:20:59\\', \\'stage\\': \\'columns_created\\'},\\n    {\\'details\\': None, \\'time\\': \\'2017-02-28T20:20:59\\', \\'stage\\': \\'started\\'}\\n]\\n\\n\\n# So maybe we just want to wait here, printing the progress, until the job is done\\njob.wait_for_finish(progress = lambda job: print(job.attributes[\\'log\\']))\\n\\n# So now if we go look at our original four-four, our data will be there\\nMetadata only revisions\\nWhen there is an existing Socrata view that you\\'d like to update the metadata of, you can do so by creating a Source which is the Socrata view.\\nview = socrata.views.lookup(\\'abba-cafe\\')\\n\\nrevision = view.revisions.create_replace_revision()\\nsource = revision.source_from_dataset()\\noutput_schema = source.get_latest_input_schema().get_latest_output_schema()\\nnew_output_schema = output_schema\\\\\\n    .change_column_metadata(\\'a\\', \\'description\\').to(\\'meh\\')\\\\\\n    .change_column_metadata(\\'b\\', \\'display_name\\').to(\\'bbbb\\')\\\\\\n    .change_column_metadata(\\'c\\', \\'field_name\\').to(\\'ccc\\')\\\\\\n    .run()\\n\\n\\nrevision.apply(output_schema = new_output_schema)\\nDevelopment\\nTesting\\nInstall test deps by running pip install -r requirements.txt. This will install pdoc and pandas which are required to run the tests.\\nConfiguration is set in test/auth.py for tests. It reads the domain, username, and password from environment variables. If you want to run the tests, set those environment variables to something that will work.\\nIf I wanted to run the tests against my local instance, I would run:\\nSOCRATA_DOMAIN=localhost SOCRATA_USERNAME=$SOCRATA_LOCAL_USER SOCRATA_PASSWORD=$SOCRATA_LOCAL_PASS bin/test\\nGenerating docs\\nmake the docs by running\\nmake docs\\nReleasing\\nrelease to pypi by bumping the version to something reasonable and running\\npython setup.py sdist upload -r pypi\\n\\nNote you\\'ll need your .pypirc file in your home directory. For help, read this\\nLibrary Docs\\nSocrata\\nArgSpec\\n    Args: auth\\n\\nTop level publishing object.\\nAll functions making HTTP calls return a result tuple, where the first element in the\\ntuple is whether or not the call succeeded, and the second element is the returned\\nobject if it was a success, or a dictionary containing the error response if the call\\nfailed. 2xx responses are considered successes. 4xx and 5xx responses are considered failures.\\nIn the event of a socket hangup, an exception is raised.\\ncreate\\nShortcut to create a dataset. Returns a Create object,\\nwhich contains functions which will create a view, upload\\nyour file, and validate data quality in one step.\\nTo actually place the validated data into a view, you can call .apply()\\non the revision\\n(revision, output_schema) Socrata(auth).create(\\n    name = \"cool dataset\",\\n    description = \"a description\"\\n).csv(file)\\n\\njob = revision.apply(output_schema = output_schema)\\n\\nArgs:\\n   **kwargs: Arbitrary revision metadata values\\n\\nReturns:\\n    result (Revision, OutputSchema): Returns the revision that was created and the\\n        OutputSchema created from your uploaded file\\n\\nExamples:\\nSocrata(auth).create(\\n    name = \"cool dataset\",\\n    description = \"a description\"\\n).csv(open(\\'my-file.csv\\'))\\nnew\\nArgSpec\\n    Args: metadata\\n\\nCreate an empty revision, on a view that doesn\\'t exist yet. The\\nview will be created for you, and the initial revision will be returned.\\nArgs:\\n    metadata (dict): Metadata to apply to the revision\\n\\nReturns:\\n    Revision\\n\\nExamples:\\n    rev = Socrata(auth).new({\\n        \\'name\\': \\'hi\\',\\n        \\'description\\': \\'foo!\\',\\n        \\'metadata\\': {\\n            \\'view\\': \\'metadata\\',\\n            \\'anything\\': \\'is allowed here\\'\\n\\n        }\\n    })\\nusing_config\\nArgSpec\\n    Args: config_name, view\\n\\nUpdate a dataset, using the configuration that you previously\\ncreated, and saved the name of. Takes the config_name parameter\\nwhich uniquely identifies the config, and the View object, which can\\nbe obtained from socrata.views.lookup(\\'view-id42\\')\\nArgs:\\n    config_name (str): The config name\\n    view (View): The view to update\\n\\nReturns:\\n    result (ConfiguredJob): Returns the ConfiguredJob\\n\\nNote:\\nTypical usage would be in a context manager block (as demonstrated in the example\\nbelow). In this case, the ConfiguredJob is created and immediately launched by way of\\nthe call to the ConfiguredJob.csv method.\\nExamples:\\n    with open(\\'my-file.csv\\', \\'rb\\') as my_file:\\n        (rev, job) = p.using_config(name, view).csv(my_file)\\n\\nAuthorization\\nArgSpec\\n    Args: domain, username, password, request_id_prefix\\n    Defaults: domain=\\n\\nManages basic authorization for accessing the socrata API.\\nThis is passed into the Socrata object once, which is the entry\\npoint for all operations.\\nauth = Authorization(\\n    \"data.seattle.gov\",\\n    os.environ[\\'SOCRATA_USERNAME\\'],\\n    os.environ[\\'SOCRATA_PASSWORD\\']\\n)\\npublishing = Socrata(auth)\\n\\nlive_dangerously\\nDisable SSL checking. Note that this should only be used while developing\\nagainst a local Socrata instance.\\nRevisions\\nArgSpec\\n    Args: fourfour, auth\\n\\ncreate_delete_revision\\nArgSpec\\n    Args: metadata, permission\\n    Defaults: metadata={}, permission=public\\n\\nCreate a revision on the view, which when applied, will delete rows of data.\\nThis is an upsert; a row id must be set.\\nArgs:\\n    metadata (dict): The metadata to change; these changes will be applied when the revision is applied\\n    permission (string): \\'public\\' or \\'private\\'\\n\\nReturns:\\n    Revision The new revision, or an error\\n\\nExamples:\\n    view.revisions.create_delete_revision(metadata = {\\n        \\'name\\': \\'new dataset name\\',\\n        \\'description\\': \\'description\\'\\n    })\\ncreate_replace_revision\\nArgSpec\\n    Args: metadata, permission\\n    Defaults: metadata={}, permission=public\\n\\nCreate a revision on the view, which when applied, will replace the data.\\nArgs:\\n    metadata (dict): The metadata to change; these changes will be applied when the revision\\n        is applied\\n    permission (string): \\'public\\' or \\'private\\'\\n\\nReturns:\\n    Revision The new revision, or an error\\n\\nExamples:\\n    >>> view.revisions.create_replace_revision(metadata = {\\'name\\': \\'new dataset name\\', \\'description\\': \\'updated description\\'})\\n\\ncreate_update_revision\\nArgSpec\\n    Args: metadata, permission\\n    Defaults: metadata={}, permission=public\\n\\nCreate a revision on the view, which when applied, will update the data\\nrather than replacing it.\\nThis is an upsert; if there is a rowId defined and you have duplicate ID values,\\nthose rows will be updated. Otherwise they will be appended.\\nArgs:\\n    metadata (dict): The metadata to change; these changes will be applied when the revision is applied\\n    permission (string): \\'public\\' or \\'private\\'\\n\\nReturns:\\n    Revision The new revision, or an error\\n\\nExamples:\\n    view.revisions.create_update_revision(metadata = {\\n        \\'name\\': \\'new dataset name\\',\\n        \\'description\\': \\'updated description\\'\\n    })\\ncreate_using_config\\nArgSpec\\n    Args: config\\n\\nCreate a revision for the given dataset.\\nlist\\nList all the revisions on the view\\nReturns:\\n    list[Revision]\\n\\nlookup\\nArgSpec\\n    Args: revision_seq\\n\\nLookup a revision within the view based on the sequence number\\nArgs:\\n    revision_seq (int): The sequence number of the revision to lookup\\n\\nReturns:\\n    Revision The Revision resulting from this API call, or an error\\n\\nRevision\\nArgSpec\\n    Args: auth, response, parent\\n\\nA revision is a change to a dataset\\napply\\nArgSpec\\n    Args: output_schema\\n\\nApply the Revision to the view that it was opened on\\nArgs:\\n    output_schema (OutputSchema): Optional output schema. If your revision includes\\n        data changes, this should be included. If it is a metadata only revision,\\n        then you will not have an output schema, and you do not need to pass anything\\n        here\\n\\nReturns:\\n    Job\\n\\nExamples:\\njob = revision.apply(output_schema = my_output_schema)\\n\\ncreate_upload\\nArgSpec\\n    Args: filename, parse_options\\n    Defaults: filename={}\\n\\nCreate an upload within this revision\\nArgs:\\n    filename (str): The name of the file to upload\\n\\nReturns:\\n    Source: Returns the new Source The Source created by this API call, or an error\\n\\ndiscard\\nDiscard this open revision.\\nReturns:\\n    Revision The closed Revision or an error\\n\\nlist_operations\\nGet a list of the operations that you can perform on this\\nobject. These map directly onto what\\'s returned from the API\\nin the links section of each resource\\nopen_in_browser\\nOpen this revision in your browser, this will open a window\\nplan\\nReturn the list of operations this revision will make when it is applied\\nReturns:\\n    dict\\n\\nset_output_schema\\nArgSpec\\n    Args: output_schema_id\\n\\nSet the output schema id on the revision. This is what will get applied when\\nthe revision is applied if no ouput schema is explicitly supplied\\nArgs:\\n    output_schema_id (int): The output schema id\\n\\nReturns:\\n    Revision The updated Revision as a result of this API call, or an error\\n\\nExamples:\\n    revision = revision.set_output_schema(42)\\nsource_as_blob\\nArgSpec\\n    Args: filename, parse_options\\n    Defaults: filename={}\\n\\nCreate a source from a file that should remain unparsed\\nsource_from_agent\\nArgSpec\\n    Args: agent_uid, namespace, path, parse_options, parameters\\n    Defaults: agent_uid={}, namespace={}\\n\\nCreate a source from a connection agent in this revision\\nsource_from_dataset\\nArgSpec\\n    Args: parse_options\\n    Defaults: parse_options={}\\n\\nCreate a dataset source within this revision\\nsource_from_url\\nArgSpec\\n    Args: url, parse_options\\n    Defaults: url={}\\n\\nCreate a URL source\\nArgs:\\n    url (str): The URL to create the dataset from\\n\\nReturns:\\n    Source: Returns the new Source The Source created by this API call, or an error\\n\\nui_url\\nThis is the URL to the landing page in the UI for this revision\\nReturns:\\n    url (str): URL you can paste into a browser to view the revision UI\\n\\nupdate\\nArgSpec\\n    Args: body\\n\\nSet the metadata to be applied to the view\\nwhen this revision is applied\\nArgs:\\n    body (dict): The changes to make to this revision\\n\\nReturns:\\n    Revision The updated Revision as a result of this API call, or an error\\n\\nExamples:\\n    revision = revision.update({\\n        \\'metadata\\': {\\n            \\'name\\': \\'new name\\',\\n            \\'description\\': \\'new description\\'\\n        }\\n    })\\nSources\\nArgSpec\\n    Args: auth\\n\\ncreate_upload\\nArgSpec\\n    Args: filename\\n\\nCreate a new source. Takes a body param, which must contain a filename\\nof the file.\\nArgs:\\n    filename (str): The name of the file you are uploading\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    upload = revision.create_upload(\\'foo.csv\\')\\nlookup\\nArgSpec\\n    Args: source_id\\n\\nLookup a source\\nArgs:\\n    source_id (int): The id\\n\\nReturns:\\n    Source: Returns the new Source The Source resulting from this API call, or an error\\n\\nSource\\nArgSpec\\n    Args: auth, response, parent\\n\\nadd_to_revision\\nArgSpec\\n    Args: revision\\n\\nAssociate this Source with the given revision.\\nblob\\nArgSpec\\n    Args: file_handle\\n\\nUploads a Blob dataset. A blob is a file that will not be parsed as a data file,\\nie: an image, video, etc.\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    with open(\\'my-blob.jpg\\', \\'rb\\') as f:\\n        upload = upload.blob(f)\\nchange_parse_option\\nArgSpec\\n    Args: name\\n\\nChange a parse option on the source.\\nIf there are not yet bytes uploaded, these parse options will be used\\nin order to parse the file.\\nIf there are already bytes uploaded, this will trigger a re-parsing of\\nthe file, and consequently a new InputSchema will be created. You can call\\nsource.latest_input() to get the newest one.\\nParse options are:\\nheader_count (int): the number of rows considered a header\\ncolumn_header (int): the one based index of row to use to generate the header\\nencoding (string): defaults to guessing the encoding, but it can be explicitly set\\ncolumn_separator (string): For CSVs, this defaults to \",\", and for TSVs \"       \", but you can use a custom separator\\nquote_char (string): Character used to quote values that should be escaped. Defaults to \"\"\"\\nArgs:\\n    name (string): One of the options above, ie: \"column_separator\" or \"header_count\"\\n\\nReturns:\\n    change (ParseOptionChange): implements a `.to(value)` function which you call to set the value\\n\\nFor our example, assume we have this dataset\\nThis is my cool dataset\\nA, B, C\\n1, 2, 3\\n4, 5, 6\\n\\nWe want to say that the first 2 rows are headers, and the second of those 2\\nrows should be used to make the column header. We would do that like so:\\nExamples:\\n    source = source            .change_parse_option(\\'header_count\\').to(2)            .change_parse_option(\\'column_header\\').to(2)            .run()\\ncsv\\nArgSpec\\n    Args: file_handle\\n\\nUpload a CSV, returns the new input schema.\\nArgs:\\n    file_handle: The file handle, as returned by the python function `open()`\\n\\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    with open(\\'my-file.csv\\', \\'rb\\') as f:\\n        upload = upload.csv(f)\\ndf\\nArgSpec\\n    Args: dataframe\\n\\nUpload a pandas DataFrame, returns the new source.\\nArgs:\\n    file_handle: The file handle, as returned by the python function `open()`\\n\\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    import pandas\\n    df = pandas.read_csv(\\'test/fixtures/simple.csv\\')\\n    upload = upload.df(df)\\ngeojson\\nArgSpec\\n    Args: file_handle\\n\\nUpload a geojson file, returns the new input schema.\\nArgs:\\n    file_handle: The file handle, as returned by the python function `open()`\\n\\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    with open(\\'my-geojson-file.geojson\\', \\'rb\\') as f:\\n        upload = upload.geojson(f)\\nkml\\nArgSpec\\n    Args: file_handle\\n\\nUpload a KML file, returns the new input schema.\\nArgs:\\n    file_handle: The file handle, as returned by the python function `open()`\\n\\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    with open(\\'my-kml-file.kml\\', \\'rb\\') as f:\\n        upload = upload.kml(f)\\nlist_operations\\nGet a list of the operations that you can perform on this\\nobject. These map directly onto what\\'s returned from the API\\nin the links section of each resource\\nload\\nForces the source to load, if it\\'s a view source.\\nReturns:\\n    Source: Returns the new Source\\n\\nopen_in_browser\\nOpen this source in your browser, this will open a window\\nshapefile\\nArgSpec\\n    Args: file_handle\\n\\nUpload a Shapefile, returns the new input schema.\\nArgs:\\n    file_handle: The file handle, as returned by the python function `open()`\\n\\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    with open(\\'my-shapefile-archive.zip\\', \\'rb\\') as f:\\n        upload = upload.shapefile(f)\\ntsv\\nArgSpec\\n    Args: file_handle\\n\\nUpload a TSV, returns the new input schema.\\nArgs:\\n    file_handle: The file handle, as returned by the python function `open()`\\n\\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    with open(\\'my-file.tsv\\', \\'rb\\') as f:\\n        upload = upload.tsv(f)\\nui_url\\nThis is the URL to the landing page in the UI for the sources\\nReturns:\\n    url (str): URL you can paste into a browser to view the source UI\\n\\nwait_for_finish\\nArgSpec\\n    Args: progress, timeout, sleeptime\\n    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1\\n\\nWait for this dataset to finish transforming and validating. Accepts a progress function\\nand a timeout.\\nxls\\nArgSpec\\n    Args: file_handle\\n\\nUpload an XLS, returns the new input schema\\nArgs:\\n    file_handle: The file handle, as returned by the python function `open()`\\n\\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    with open(\\'my-file.xls\\', \\'rb\\') as f:\\n        upload = upload.xls(f)\\nxlsx\\nArgSpec\\n    Args: file_handle\\n\\nUpload an XLSX, returns the new input schema.\\nArgs:\\n    file_handle: The file handle, as returned by the python function `open()`\\n\\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\\n\\nReturns:\\n    Source: Returns the new Source\\n\\nExamples:\\n    with open(\\'my-file.xlsx\\', \\'rb\\') as f:\\n        upload = upload.xlsx(f)\\nConfigs\\nArgSpec\\n    Args: auth\\n\\ncreate\\nArgSpec\\n    Args: name, data_action, parse_options, columns\\n\\nCreate a new ImportConfig. See http://docs.socratapublishing.apiary.io/\\nImportConfig section for what is supported in data_action, parse_options,\\nand columns.\\nlist\\nList all the ImportConfigs on this domain\\nlookup\\nArgSpec\\n    Args: name\\n\\nObtain a single ImportConfig by name\\nConfig\\nArgSpec\\n    Args: auth, response, parent\\n\\nchange_parse_option\\nArgSpec\\n    Args: name\\n\\nChange a parse option on the source.\\nIf there are not yet bytes uploaded, these parse options will be used\\nin order to parse the file.\\nIf there are already bytes uploaded, this will trigger a re-parsing of\\nthe file, and consequently a new InputSchema will be created. You can call\\nsource.latest_input() to get the newest one.\\nParse options are:\\nheader_count (int): the number of rows considered a header\\ncolumn_header (int): the one based index of row to use to generate the header\\nencoding (string): defaults to guessing the encoding, but it can be explicitly set\\ncolumn_separator (string): For CSVs, this defaults to \",\", and for TSVs \"       \", but you can use a custom separator\\nquote_char (string): Character used to quote values that should be escaped. Defaults to \"\"\"\\nArgs:\\n    name (string): One of the options above, ie: \"column_separator\" or \"header_count\"\\n\\nReturns:\\n    change (ParseOptionChange): implements a `.to(value)` function which you call to set the value\\n\\nFor our example, assume we have this dataset\\nThis is my cool dataset\\nA, B, C\\n1, 2, 3\\n4, 5, 6\\n\\nWe want to say that the first 2 rows are headers, and the second of those 2\\nrows should be used to make the column header. We would do that like so:\\nExamples:\\n    source = source            .change_parse_option(\\'header_count\\').to(2)            .change_parse_option(\\'column_header\\').to(2)            .run()\\ncreate_revision\\nArgSpec\\n    Args: fourfour\\n\\nCreate a new Revision in the context of this ImportConfig.\\nSources that happen in this Revision will take on the values\\nin this Config.\\ndelete\\nDelete this ImportConfig. Note that this cannot be undone.\\nlist_operations\\nGet a list of the operations that you can perform on this\\nobject. These map directly onto what\\'s returned from the API\\nin the links section of each resource\\nupdate\\nArgSpec\\n    Args: body\\n\\nMutate this ImportConfig in place. Subsequent revisions opened against this\\nImportConfig will take on its new value.\\nInputSchema\\nArgSpec\\n    Args: auth, response, parent\\n\\nThis represents a schema exactly as it appeared in the source\\nget_latest_output_schema\\nNote that this does not make an API request\\nReturns:\\noutput_schema (OutputSchema): Returns the latest output schema\\nlatest_output\\nGet the latest (most recently created) OutputSchema\\nwhich descends from this InputSchema\\nReturns:\\nOutputSchema\\nlist_operations\\nGet a list of the operations that you can perform on this\\nobject. These map directly onto what\\'s returned from the API\\nin the links section of each resource\\ntransform\\nArgSpec\\n    Args: body\\n\\nTransform this InputSchema into an Output. Returns the\\nnew OutputSchema. Note that this call is async - the data\\nmay still be transforming even though the OutputSchema is\\nreturned. See OutputSchema.wait_for_finish to block until\\nthe\\nOutputSchema\\nThis is data as transformed from an InputSchema\\nadd_column\\nArgSpec\\n    Args: field_name, display_name, transform_expr, description\\n\\nAdd a column\\nArgs:\\n    field_name (str): The column\\'s field_name, must be unique\\n    display_name (str): The columns display name\\n    transform_expr (str): SoQL expression to evaluate and fill the column with data from\\n    description (str): Optional column description\\n\\nReturns:\\n    output_schema (OutputSchema): Returns self for easy chaining\\n\\nExamples:\\nnew_output_schema = output\\n    # Add a new column, which is computed from the `celsius` column\\n    .add_column(\\'fahrenheit\\', \\'Degrees (Fahrenheit)\\', \\'(to_number(`celsius`) * (9 / 5)) + 32\\', \\'the temperature in celsius\\')\\n    # Add a new column, which is computed from the `celsius` column\\n    .add_column(\\'kelvin\\', \\'Degrees (Kelvin)\\', \\'(to_number(`celsius`) + 273.15\\')\\n    .run()\\nbuild_config\\nArgSpec\\n    Args: name, data_action\\n\\nCreate a new ImportConfig from this OutputSchema. See the API\\ndocs for what an ImportConfig is and why they\\'re useful\\nchange_column_metadata\\nArgSpec\\n    Args: field_name, attribute\\n\\nChange the column metadata. This returns a ColumnChange,\\nwhich implements a .to function, which takes the new value to change to\\nArgs:\\n    field_name (str): The column to change\\n    attribute (str): The attribute of the column to change\\n\\nReturns:\\n    change (TransformChange): The transform change, which implements the `.to` function\\n\\nExamples:\\n    new_output_schema = output\\n        # Change the field_name of date to the_date\\n        .change_column_metadata(\\'date\\', \\'field_name\\').to(\\'the_date\\')\\n        # Change the description of the celsius column\\n        .change_column_metadata(\\'celsius\\', \\'description\\').to(\\'the temperature in celsius\\')\\n        # Change the display name of the celsius column\\n        .change_column_metadata(\\'celsius\\', \\'display_name\\').to(\\'Degrees (Celsius)\\')\\n        .run()\\nchange_column_transform\\nArgSpec\\n    Args: field_name\\n\\nChange the column transform. This returns a TransformChange,\\nwhich implements a .to function, which takes a transform expression.\\nArgs:\\n    field_name (str): The column to change\\n\\nReturns:\\n    change (TransformChange): The transform change, which implements the `.to` function\\n\\nExamples:\\n    new_output_schema = output\\n        .change_column_transform(\\'the_date\\').to(\\'to_fixed_timestamp(`date`)\\')\\n        # Make the celsius column all numbers\\n        .change_column_transform(\\'celsius\\').to(\\'to_number(`celsius`)\\')\\n        # Add a new column, which is computed from the `celsius` column\\n        .add_column(\\'fahrenheit\\', \\'Degrees (Fahrenheit)\\', \\'(to_number(`celsius`) * (9 / 5)) + 32\\', \\'the temperature in celsius\\')\\n        .run()\\ndrop_column\\nArgSpec\\n    Args: field_name\\n\\nDrop the column\\nArgs:\\n    field_name (str): The column to drop\\n\\nReturns:\\n    output_schema (OutputSchema): Returns self for easy chaining\\n\\nExamples:\\n    new_output_schema = output\\n        .drop_column(\\'foo\\')\\n        .run()\\nlist_operations\\nGet a list of the operations that you can perform on this\\nobject. These map directly onto what\\'s returned from the API\\nin the links section of each resource\\nrows\\nArgSpec\\n    Args: offset, limit\\n    Defaults: offset=0, limit=500\\n\\nGet the rows for this OutputSchema. Acceps offset and limit params\\nfor paging through the data.\\nrun\\nRun all adds, drops, and column changes.\\nReturns:\\n    OutputSchema\\n\\nExamples:\\n    new_output_schema = output\\n        # Change the field_name of date to the_date\\n        .change_column_metadata(\\'date\\', \\'field_name\\').to(\\'the_date\\')\\n        # Change the description of the celsius column\\n        .change_column_metadata(\\'celsius\\', \\'description\\').to(\\'the temperature in celsius\\')\\n        # Change the display name of the celsius column\\n        .change_column_metadata(\\'celsius\\', \\'display_name\\').to(\\'Degrees (Celsius)\\')\\n        # Change the transform of the_date column to to_fixed_timestamp(`date`)\\n        .change_column_transform(\\'the_date\\').to(\\'to_fixed_timestamp(`date`)\\')\\n        # Make the celsius column all numbers\\n        .change_column_transform(\\'celsius\\').to(\\'to_number(`celsius`)\\')\\n        # Add a new column, which is computed from the `celsius` column\\n        .add_column(\\'fahrenheit\\', \\'Degrees (Fahrenheit)\\', \\'(to_number(`celsius`) * (9 / 5)) + 32\\', \\'the temperature in celsius\\')\\n        .run()\\nschema_errors\\nArgSpec\\n    Args: offset, limit\\n    Defaults: offset=0, limit=500\\n\\nGet the errors that resulted in transforming into this output schema.\\nAccepts offset and limit params\\nschema_errors_csv\\nGet the errors that results in transforming into this output schema\\nas a CSV stream.\\nNote that this returns a Reponse, where Reponse\\nis a python requests Reponse object\\nset_row_id\\nArgSpec\\n    Args: field_name\\n\\nSet the row id. Note you must call validate_row_id before doing this.\\nArgs:\\n    field_name (str): The column to set as the row id\\n\\nReturns:\\n    OutputSchema\\n\\nExamples:\\nnew_output_schema = output.set_row_id(\\'the_id_column\\')\\nvalidate_row_id\\nArgSpec\\n    Args: field_name\\n\\nSet the row id. Note you must call validate_row_id before doing this.\\nArgs:\\n    field_name (str): The column to validate as the row id\\n\\nReturns:\\n    boolean\\n\\nwait_for_finish\\nArgSpec\\n    Args: progress, timeout, sleeptime\\n    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1\\n\\nWait for this dataset to finish transforming and validating. Accepts a progress function\\nand a timeout.\\nJob\\nArgSpec\\n    Args: auth, response, parent\\n\\nis_complete\\nHas this job finished or failed\\nlist_operations\\nGet a list of the operations that you can perform on this\\nobject. These map directly onto what\\'s returned from the API\\nin the links section of each resource\\nwait_for_finish\\nArgSpec\\n    Args: progress, timeout, sleeptime\\n    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1\\n\\nWait for this dataset to finish transforming and validating. Accepts a progress function\\nand a timeout.\\n'},\n",
       " {'language': 'Python 87.9',\n",
       "  'readme': 'Data Engineering Project\\n  \\nData Engineering Project is an implementation of the data pipeline which consumes the latest news from RSS Feeds and makes them available for users via handy API.\\nThe pipeline infrastructure is built using popular, open-source projects.\\nAccess the latest news and headlines in one place. 💪\\nTable of Contents\\n\\nArchitecture diagram\\nHow it works\\n\\nData scraping\\nData flow\\nData access\\n\\n\\nPrerequisites\\nRunning project\\nTesting\\nAPI service\\nReferences\\nContributions\\nLicense\\nContact\\n\\nArchitecture diagram\\n\\nHow it works\\nData Scraping\\nAirflow DAG is responsible for the execution of Python scraping modules.\\nIt runs periodically every X minutes producing micro-batches.\\n\\n\\nFirst task updates proxypool. Using proxies in combination with rotating user agents can help get scrapers past most of the anti-scraping measures and prevent being detected as a scraper.\\n\\n\\nSecond task extracts news from RSS feeds provided in the configuration file, validates the quality and sends data into Kafka topic A. The extraction process is using validated proxies from proxypool.\\n\\n\\nData flow\\n\\nKafka Connect Mongo Sink consumes data from Kafka topic A and stores news in MongoDB using upsert functionality based on _id field.\\nDebezium MongoDB Source tracks a MongoDB replica set for document changes in databases and collections, recording those changes as events in Kafka topic B.\\nKafka Connect Elasticsearch Sink consumes data from Kafka topic B and upserts news in Elasticsearch. Data replicated between topics A and B ensures MongoDB and ElasticSearch synchronization. Command Query Responsibility Segregation (CQRS) pattern allows the use of separate models for updating and reading information.\\nKafka Connect S3-Minio Sink consumes records from Kafka topic B and stores them in MinIO (high-performance object storage) to ensure data persistency.\\n\\nData access\\n\\nData gathered by previous steps can be easily accessed in API service  using public endpoints.\\n\\nPrerequisites\\nSoftware required to run the project. Install:\\n\\nDocker\\nPython 3.8+ (pip)\\ndocker-compose\\n\\nRunning project\\nScript manage.sh - wrapper for docker-compose works as a managing tool.\\n\\nBuild project infrastructure\\n\\n./manage.sh up\\n\\nStop project infrastructure\\n\\n./manage.sh stop\\n\\nDelete project infrastructure\\n\\n./manage.sh down\\nTesting\\nScript run_tests.sh executes unit tests against Airflow scraping modules and Django Rest Framework applications.\\n./run_tests.sh\\nAPI service\\nRead detailed documentation on how to interact with data collected by pipeline using search endpoints.\\nExample searches:\\n\\nsee all news\\n\\nhttp://0.0.0.0:5000/api/v1/news/ \\n\\n\\nadd search_fields title and description, see all of the news containing the Robert Lewandowski phrase\\n\\nhttp://0.0.0.0:5000/api/v1/news/?search=Robert%20Lewandowski \\n\\n\\nfind news containing the Lewandowski phrase in their titles\\n\\nhttp://0.0.0.0:5000/api/v1/news/?search=title|Lewandowski \\n\\n\\nsee all of the polish news containing the Lewandowski phrase\\n\\nhttp://0.0.0.0:5000/api/v1/news/?search=lewandowski&language=pl\\n\\nReferences\\nInspired by following codes, articles and videos:\\n\\nHow we launched a data product in 60 days with AWS\\nToruń JUG #55 - \"Kafka Connect - szwajcarski scyzoryk w rękach inżyniera?\" - Mariusz Strzelecki\\nKafka Elasticsearch Sink Connector and the Power of Single Message Transformations\\nDocker Tips and Tricks with Kafka Connect, ksqlDB, and Kafka\\n\\nContributions\\nContributions are what makes the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.\\n\\nFork the Project\\nCreate your Feature Branch (git checkout -b feature/AmazingFeature)\\nCommit your Changes (git commit -m \\'Add some AmazingFeature\\')\\nPush to the Branch (git push origin feature/AmazingFeature)\\nOpen a Pull Request\\n\\nLicense\\nDistributed under the MIT License. See LICENSE for more information.\\nContact\\nPlease feel free to contact me if you have any questions.\\nDamian Kliś @DamianKlis\\n'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Data Enginner's Essential Commands\\n\\nLinux: Link\\nPython: Link\\nPySpark: Link\\nAWS: Link\\n\\nEKS: Link\\nEMR: Link\\nS3: Link\\n\\n\\nTerraform: Link\\nGit: Link\\nHelm: Link\\n\\nJupyterhub: Link\\n\\n\\n\\n\\nWant to contribute?\\n\\n\\nThe commands should not be copy-pasted from any source in bulk.\\nOnly add those commands that you use more frequently but may be unknown to other developers.\\n\\nExample: pwd, ls e.t.c., are not allowed\\n\\nFollow the structure and don't forget to embed any reference links either in heading or command description.\\n\\nPut it inside a directory if applicable\\nGive a proper heading\\nUse markdown script for block code or inline code to embed commands\\n\\n\\nIf the command heading is not sufficient to explain the uses, give 1 liner explanation with an example.\\nI would be happy to accept your pull request even if you add one good command than adding ten not so good commands.\\n\\n\\n\\n\"},\n",
       " {'language': 'Python 59.0',\n",
       "  'readme': \"Data Engineering Projects\\n\\nProject 1: Data Modeling with Postgres\\nIn this project, we apply Data Modeling with Postgres and build an ETL pipeline using Python. A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Currently, they are collecting data in json format and the analytics team is particularly interested in understanding what songs users are listening to.\\nLink: Data_Modeling_with_Postgres\\nProject 2: Data Modeling with Cassandra\\nIn this project, we apply Data Modeling with Cassandra and build an ETL pipeline using Python. We will build a Data Model around our queries that we want to get answers for.\\nFor our use case we want below answers:\\n\\nGet details of a song that was herad on the music app history during a particular session.\\nGet songs played by a user during particular session on music app.\\nGet all users from the music app history who listened to a particular song.\\n\\nLink : Data_Modeling_with_Apache_Cassandra\\nProject 3: Data Warehouse\\nIn this project, we apply the Data Warehouse architectures we learnt and build a Data Warehouse on AWS cloud. We build an ETL pipeline to extract and transform data stored in json format in s3 buckets and move the data to Warehouse hosted on Amazon Redshift.\\nUse Redshift IaC script - Redshift_IaC_README\\nLink  - Data_Warehouse\\nProject 4: Data Lake\\nIn this project, we will build a Data Lake on AWS cloud using Spark and AWS EMR cluster. The data lake will serve as a Single Source of Truth for the Analytics Platform. We will write spark jobs to perform ELT operations that picks data from landing zone on S3 and transform and stores data on the S3 processed zone.\\nLink: Data_Lake\\nProject 5: Data Pipelines with Airflow\\nIn this project, we will orchestrate our Data Pipeline workflow using an open-source Apache project called Apache Airflow. We will schedule our ETL jobs in Airflow, create project related custom plugins and operators and automate the pipeline execution.\\nLink:  Airflow_Data_Pipelines\\nCAPSTONE PROJECT\\nUdacity provides their own crafted Capstone project with dataset that include data on immigration to the United States, and supplementary datasets that include data on airport codes, U.S. city demographics, and temperature data.\\nI worked on my own open-ended project. Here is the link - goodreads_etl_pipeline\\n\"},\n",
       " {'language': 'Python 93.2',\n",
       "  'readme': 'NEW LIST 2020 - 2021: Machine-Learning / Deep-Learning / AI -Tutorials\\nHi - Thanks for dropping by!\\n\\nI will be updating this tutorials site on a daily basis adding all relevant topcis, including latest researches papers from internet such as arxiv.org, BIORXIV - Specifically Neuroscience to name a few. \\n\\nMore importantly the applications of ML/DL/AI into industry areas such as Transportation, Medicine/Healthcare etc. will be something I\\'ll watch with keen interest and would love to share the same with you.\\n\\nFinally, it is YOUR help I will seek to make it more useful and less boring, so please do suggest/comment/contribute!\\n\\n\\n\\nIndex\\n\\n\\ndeep-learning\\n\\nUBER | Pyro\\nNetflix | VectorFlow\\nPyTorch\\ntensorflow\\ntheano\\nkeras\\ncaffe\\nTorch/Lua\\nMXNET\\n\\n\\n\\nscikit-learn\\n\\n\\nstatistical-inference-scipy\\n\\n\\npandas\\n\\n\\nmatplotlib\\n\\n\\nnumpy\\n\\n\\npython-data\\n\\n\\nkaggle-and-business-analyses\\n\\n\\nspark\\n\\n\\nmapreduce-python\\n\\n\\namazon web services\\n\\n\\ncommand lines\\n\\n\\nmisc\\n\\n\\nnotebook-installation\\n\\n\\nCurated list of Deep Learning / AI blogs\\n\\n\\ncredits\\n\\n\\ncontributing\\n\\n\\ncontact-info\\n\\n\\nlicense\\n\\n\\ndeep-learning\\nIPython Notebook(s) and other programming tools such as Torch/Lua/D lang in demonstrating deep learning functionality.\\nuber-pyro-probabalistic-tutorials\\n\\n\\n\\nAdditional PyRo tutorials:\\n\\npyro-examples/full examples\\npyro-examples/Variational Autoencoders\\npyro-examples/Bayesian Regression\\npyro-examples/Deep Markov Model\\npyro-examples/AIR(Attend Infer Repeat)\\npyro-examples/Semi-Supervised VE\\npyro-examples/GMM\\npyro-examples/Gaussian Process\\npyro-examples/Bayesian Optimization\\nFull Pyro Code\\n\\nnetflix-vectorflow-tutorials\\n\\n\\n\\n\\nMNIST Example, running with Dlang\\n\\npytorch-tutorials\\n\\n\\n\\n\\n\\n\\nLevel\\nDescription\\n\\n\\n\\n\\nBeginners/Zakizhou\\nLearning the basics of PyTorch from Facebook.\\n\\n\\nIntermedia/Quanvuong\\nLearning the intermediate stuff about PyTorch of from Facebook.\\n\\n\\nAdvanced/Chsasank\\nLearning the advanced stuff about PyTorch of from Facebook.\\n\\n\\nLearning PyTorch by Examples - Numpy, Tensors and Autograd\\nAt its core, PyTorch provides two main features an n-dimensional Tensor, similar to numpy but can run on GPUs AND automatic differentiation for building and training neural networks.\\n\\n\\nPyTorch - Getting to know autograd.Variable, Gradient, Neural Network\\nHere we start with ultimate basics of Tensors, wrap a Tensor with Variable module, play with nn.Module and implement forward and backward function.\\n\\n\\n\\ntensor-flow-tutorials\\n\\n\\n\\n\\nAdditional TensorFlow tutorials:\\n\\npkmital/tensorflow_tutorials\\nnlintz/TensorFlow-Tutorials\\nalrojo/tensorflow-tutorial\\nBinRoot/TensorFlow-Book\\n\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\ntsf-basics\\nLearn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google.\\n\\n\\ntsf-linear\\nImplement linear regression in TensorFlow.\\n\\n\\ntsf-logistic\\nImplement logistic regression in TensorFlow.\\n\\n\\ntsf-nn\\nImplement nearest neighboars in TensorFlow.\\n\\n\\ntsf-alex\\nImplement AlexNet in TensorFlow.\\n\\n\\ntsf-cnn\\nImplement convolutional neural networks in TensorFlow.\\n\\n\\ntsf-mlp\\nImplement multilayer perceptrons in TensorFlow.\\n\\n\\ntsf-rnn\\nImplement recurrent neural networks in TensorFlow.\\n\\n\\ntsf-gpu\\nLearn about basic multi-GPU computation in TensorFlow.\\n\\n\\ntsf-gviz\\nLearn about graph visualization in TensorFlow.\\n\\n\\ntsf-lviz\\nLearn about loss visualization in TensorFlow.\\n\\n\\n\\ntensor-flow-exercises\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\ntsf-not-mnist\\nLearn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow.\\n\\n\\ntsf-fully-connected\\nProgressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow.\\n\\n\\ntsf-regularization\\nExplore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow.\\n\\n\\ntsf-convolutions\\nCreate convolutional neural networks in TensorFlow.\\n\\n\\ntsf-word2vec\\nTrain a skip-gram model over Text8 data in TensorFlow.\\n\\n\\ntsf-lstm\\nTrain a LSTM character model over Text8 data in TensorFlow.\\n\\n\\n\\n\\n\\n\\n\\ntheano-tutorials\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\ntheano-intro\\nIntro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.\\n\\n\\ntheano-scan\\nLearn scans, a mechanism to perform loops in a Theano graph.\\n\\n\\ntheano-logistic\\nImplement logistic regression in Theano.\\n\\n\\ntheano-rnn\\nImplement recurrent neural networks in Theano.\\n\\n\\ntheano-mlp\\nImplement multilayer perceptrons in Theano.\\n\\n\\n\\n\\n\\n\\n\\nkeras-tutorials\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nkeras\\nKeras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano.\\n\\n\\nsetup\\nLearn about the tutorial goals and how to set up your Keras environment.\\n\\n\\nintro-deep-learning-ann\\nGet an intro to deep learning with Keras and Artificial Neural Networks (ANN).\\n\\n\\nPerceptrons and Adaline\\nImplement Peceptron and adaptive linear neurons.\\n\\n\\nMLP and MNIST Data\\nClassifying handwritten digits,implement MLP, train and debug ANN\\n\\n\\ntheano\\nLearn about Theano by working with weights matrices and gradients.\\n\\n\\nkeras-otto\\nLearn about Keras by looking at the Kaggle Otto challenge.\\n\\n\\nann-mnist\\nReview a simple implementation of ANN for MNIST using Keras.\\n\\n\\nconv-nets\\nLearn about Convolutional Neural Networks (CNNs) with Keras.\\n\\n\\nconv-net-1\\nRecognize handwritten digits from MNIST using Keras - Part 1.\\n\\n\\nconv-net-2\\nRecognize handwritten digits from MNIST using Keras - Part 2.\\n\\n\\nkeras-models\\nUse pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras.\\n\\n\\nauto-encoders\\nLearn about Autoencoders with Keras.\\n\\n\\nrnn-lstm\\nLearn about Recurrent Neural Networks (RNNs) with Keras.\\n\\n\\nlstm-sentence-gen\\nLearn about RNNs using Long Short Term Memory (LSTM) networks with Keras.\\n\\n\\nnlp-deep-learning\\nLearn about NLP using ANN (Artificial Neural Networks.\\n\\n\\nhyperparamter-tuning\\nHyperparamters tuning using keras-wrapper.scikit-learn\\n\\n\\n\\ndeep-learning-misc\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\ndeep-dream\\nCaffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images.\\n\\n\\n\\n\\n\\n\\n\\nscikit-learn\\nIPython Notebook(s) demonstrating scikit-learn functionality.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nintro\\nIntro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.\\n\\n\\nknn\\nImplement k-nearest neighbors in scikit-learn.\\n\\n\\nlinear-reg\\nImplement linear regression in scikit-learn.\\n\\n\\nsvm\\nImplement support vector machine classifiers with and without kernels in scikit-learn.\\n\\n\\nrandom-forest\\nImplement random forest classifiers and regressors in scikit-learn.\\n\\n\\nk-means\\nImplement k-means clustering in scikit-learn.\\n\\n\\npca\\nImplement principal component analysis in scikit-learn.\\n\\n\\ngmm\\nImplement Gaussian mixture models in scikit-learn.\\n\\n\\nvalidation\\nImplement validation and model selection in scikit-learn.\\n\\n\\n\\n\\n\\n\\n\\nstatistical-inference-scipy\\nIPython Notebook(s) demonstrating statistical inference with SciPy functionality.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nscipy\\nSciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.\\n\\n\\neffect-size\\nExplore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States.\\n\\n\\nsampling\\nExplore random sampling by analyzing the average weight of men and women in the United States using BRFSS data.\\n\\n\\nhypothesis\\nExplore hypothesis testing by analyzing the difference of first-born babies compared with others.\\n\\n\\n\\n\\n\\n\\n\\npandas\\nIPython Notebook(s) demonstrating pandas functionality.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\npandas\\nSoftware library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series.\\n\\n\\ngithub-data-wrangling\\nLearn how to load, clean, merge, and feature engineer by analyzing GitHub data from the Viz repo.\\n\\n\\nIntroduction-to-Pandas\\nIntroduction to Pandas.\\n\\n\\nIntroducing-Pandas-Objects\\nLearn about Pandas objects.\\n\\n\\nData Indexing and Selection\\nLearn about data indexing and selection in Pandas.\\n\\n\\nOperations-in-Pandas\\nLearn about operating on data in Pandas.\\n\\n\\nMissing-Values\\nLearn about handling missing data in Pandas.\\n\\n\\nHierarchical-Indexing\\nLearn about hierarchical indexing in Pandas.\\n\\n\\nConcat-And-Append\\nLearn about combining datasets: concat and append in Pandas.\\n\\n\\nMerge-and-Join\\nLearn about combining datasets: merge and join in Pandas.\\n\\n\\nAggregation-and-Grouping\\nLearn about aggregation and grouping in Pandas.\\n\\n\\nPivot-Tables\\nLearn about pivot tables in Pandas.\\n\\n\\nWorking-With-Strings\\nLearn about vectorized string operations in Pandas.\\n\\n\\nWorking-with-Time-Series\\nLearn about working with time series in pandas.\\n\\n\\nPerformance-Eval-and-Query\\nLearn about high-performance Pandas: eval() and query() in Pandas.\\n\\n\\n\\n\\n\\n\\n\\nmatplotlib\\nIPython Notebook(s) demonstrating matplotlib functionality.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nmatplotlib\\nPython 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\\n\\n\\nmatplotlib-applied\\nApply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots.\\n\\n\\nIntroduction-To-Matplotlib\\nIntroduction to Matplotlib.\\n\\n\\nSimple-Line-Plots\\nLearn about simple line plots in Matplotlib.\\n\\n\\nSimple-Scatter-Plots\\nLearn about simple scatter plots in Matplotlib.\\n\\n\\nErrorbars.ipynb\\nLearn about visualizing errors in Matplotlib.\\n\\n\\nDensity-and-Contour-Plots\\nLearn about density and contour plots in Matplotlib.\\n\\n\\nHistograms-and-Binnings\\nLearn about histograms, binnings, and density in Matplotlib.\\n\\n\\nCustomizing-Legends\\nLearn about customizing plot legends in Matplotlib.\\n\\n\\nCustomizing-Colorbars\\nLearn about customizing colorbars in Matplotlib.\\n\\n\\nMultiple-Subplots\\nLearn about multiple subplots in Matplotlib.\\n\\n\\nText-and-Annotation\\nLearn about text and annotation in Matplotlib.\\n\\n\\nCustomizing-Ticks\\nLearn about customizing ticks in Matplotlib.\\n\\n\\nSettings-and-Stylesheets\\nLearn about customizing Matplotlib: configurations and stylesheets.\\n\\n\\nThree-Dimensional-Plotting\\nLearn about three-dimensional plotting in Matplotlib.\\n\\n\\nGeographic-Data-With-Basemap\\nLearn about geographic data with basemap in Matplotlib.\\n\\n\\nVisualization-With-Seaborn\\nLearn about visualization with Seaborn.\\n\\n\\n\\n\\n\\n\\n\\nnumpy\\nIPython Notebook(s) demonstrating NumPy functionality.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nnumpy\\nAdds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.\\n\\n\\nIntroduction-to-NumPy\\nIntroduction to NumPy.\\n\\n\\nUnderstanding-Data-Types\\nLearn about data types in Python.\\n\\n\\nThe-Basics-Of-NumPy-Arrays\\nLearn about the basics of NumPy arrays.\\n\\n\\nComputation-on-arrays-ufuncs\\nLearn about computations on NumPy arrays: universal functions.\\n\\n\\nComputation-on-arrays-aggregates\\nLearn about aggregations: min, max, and everything in between in NumPy.\\n\\n\\nComputation-on-arrays-broadcasting\\nLearn about computation on arrays: broadcasting in NumPy.\\n\\n\\nBoolean-Arrays-and-Masks\\nLearn about comparisons, masks, and boolean logic in NumPy.\\n\\n\\nFancy-Indexing\\nLearn about fancy indexing in NumPy.\\n\\n\\nSorting\\nLearn about sorting arrays in NumPy.\\n\\n\\nStructured-Data-NumPy\\nLearn about structured data: NumPy\\'s structured arrays.\\n\\n\\n\\n\\n\\n\\n\\npython-data\\nIPython Notebook(s) demonstrating Python functionality geared towards data analysis.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\ndata structures\\nLearn Python basics with tuples, lists, dicts, sets.\\n\\n\\ndata structure utilities\\nLearn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions.\\n\\n\\nfunctions\\nLearn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools.\\n\\n\\ndatetime\\nLearn how to work with Python dates and times: datetime, strftime, strptime, timedelta.\\n\\n\\nlogging\\nLearn about Python logging with RotatingFileHandler and TimedRotatingFileHandler.\\n\\n\\npdb\\nLearn how to debug in Python with the interactive source code debugger.\\n\\n\\nunit tests\\nLearn how to test in Python with Nose unit tests.\\n\\n\\n\\n\\n\\n\\n\\nkaggle-and-business-analyses\\nIPython Notebook(s) used in kaggle competitions and business analyses.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\ntitanic\\nPredict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning.\\n\\n\\nchurn-analysis\\nPredict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.\\n\\n\\n\\n\\n\\n\\n\\nspark\\nIPython Notebook(s) demonstrating spark and HDFS functionality.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nspark\\nIn-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms.\\n\\n\\nhdfs\\nReliably stores very large files across machines in a large cluster.\\n\\n\\n\\n\\n\\n\\n\\nmapreduce-python\\nIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nmapreduce-python\\nRuns MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and mrjob config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  Disco is another python-based alternative.\\n\\n\\n\\n\\n\\n\\n\\naws\\nIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.\\nAlso check out:\\n\\nSAWS: A Supercharged AWS command line interface (CLI).\\nAwesome AWS: A curated list of libraries, open source repos, guides, blogs, and other resources.\\n\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nboto\\nOfficial AWS SDK for Python.\\n\\n\\ns3cmd\\nInteracts with S3 through the command line.\\n\\n\\ns3distcp\\nCombines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster.\\n\\n\\ns3-parallel-put\\nUploads multiple files to S3 in parallel.\\n\\n\\nredshift\\nActs as a fast data warehouse built on top of technology from massive parallel processing (MPP).\\n\\n\\nkinesis\\nStreams data in real time with the ability to process thousands of data streams per second.\\n\\n\\nlambda\\nRuns code in response to events, automatically managing compute resources.\\n\\n\\n\\n\\n\\n\\n\\ncommands\\nIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nlinux\\nUnix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.\\n\\n\\nanaconda\\nDistribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment.\\n\\n\\nipython notebook\\nWeb-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document.\\n\\n\\ngit\\nDistributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows.\\n\\n\\nruby\\nUsed to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages.\\n\\n\\njekyll\\nSimple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server.\\n\\n\\npelican\\nPython-based alternative to Jekyll.\\n\\n\\ndjango\\nHigh-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include Pyramid, Flask, Tornado, and Bottle.\\n\\n\\n\\nmisc\\nIPython Notebook(s) demonstrating miscellaneous functionality.\\n\\n\\n\\nNotebook\\nDescription\\n\\n\\n\\n\\nregex\\nRegular expression cheat sheet useful in data wrangling.\\n\\n\\nalgorithmia\\nAlgorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.\\n\\n\\n\\nnotebook-installation\\nanaconda\\nAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.\\nFollow instructions to install Anaconda or the more lightweight miniconda.\\ndev-setup\\nFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the dev-setup repo.\\nrunning-notebooks\\nNote: If you intend to learn the hard way (preferred method)then I\\'d strongly advice to write as much code as you can yourself and not just run pre-written code. If you still want to test it, then do the following:\\nTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found here.\\n$ git clone https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials.git\\n$ cd Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\\n$ jupyter notebook\\n\\nNotebooks tested with Python 3.7+\\ncurated-list-of-deeplearning-blogs\\n\\nA Blog From a Human-engineer-being http://www.erogol.com/ (RSS)\\nAakash Japi http://aakashjapi.com/ (RSS)\\nAdit Deshpande https://adeshpande3.github.io/ (RSS)\\nAdvanced Analytics & R http://advanceddataanalytics.net/ (RSS)\\nAdventures in Data Land http://blog.smola.org (RSS)\\nAgile Data Science http://blog.sense.io/ (RSS)\\nAhmed El Deeb https://medium.com/@D33B (RSS)\\nAirbnb Data blog http://nerds.airbnb.com/data/ (RSS)\\nAlex Castrounis | InnoArchiTech http://www.innoarchitech.com/ (RSS)\\nAlex Perrier http://alexperrier.github.io/ (RSS)\\nAlgobeans | Data Analytics Tutorials & Experiments for the Layman https://algobeans.com (RSS)\\nAmazon AWS AI Blog https://aws.amazon.com/blogs/ai/ (RSS)\\nAnalytics Vidhya http://www.analyticsvidhya.com/blog/ (RSS)\\nAnalytics and Visualization in Big Data @ Sicara https://blog.sicara.com (RSS)\\nAndreas Müller http://peekaboo-vision.blogspot.com/ (RSS)\\nAndrej Karpathy blog http://karpathy.github.io/ (RSS)\\nAndrew Brooks http://brooksandrew.github.io/simpleblog/ (RSS)\\nAndrey Kurenkov http://www.andreykurenkov.com/writing/ (RSS)\\nAnton Lebedevich\\'s Blog http://mabrek.github.io/ (RSS)\\nArthur Juliani https://medium.com/@awjuliani (RSS)\\nAudun M. Øygard http://www.auduno.com/ (RSS)\\nAvi Singh https://avisingh599.github.io/ (RSS)\\nBeautiful Data http://beautifuldata.net/ (RSS)\\nBeckerfuffle http://mdbecker.github.io/ (RSS)\\nBecoming A Data Scientist http://www.becomingadatascientist.com/ (RSS)\\nBen Bolte\\'s Blog http://benjaminbolte.com/ml/ (RSS)\\nBen Frederickson http://www.benfrederickson.com/blog/ (RSS)\\nBerkeley AI Research http://bair.berkeley.edu/blog/ (RSS)\\nBig-Ish Data http://bigishdata.com/ (RSS)\\nBlog on neural networks http://yerevann.github.io/ (RSS)\\nBlogistic RegressionAbout Projects http://d10genes.github.io/blog/ (RSS)\\nblogR | R tips and tricks from a scientist https://drsimonj.svbtle.com/ (RSS)\\nBrain of mat kelcey http://matpalm.com/blog/ (RSS)\\nBrilliantly wrong thoughts on science and programming https://arogozhnikov.github.io/ (RSS)\\nBugra Akyildiz http://bugra.github.io/ (RSS)\\nBuilding Babylon https://building-babylon.net/ (RSS)\\nCarl Shan http://carlshan.com/ (RSS)\\nChris Stucchio https://www.chrisstucchio.com/blog/index.html (RSS)\\nChristophe Bourguignat https://medium.com/@chris_bour (RSS)\\nChristopher Nguyen https://medium.com/@ctn (RSS)\\nCloudera Data Science Posts http://blog.cloudera.com/blog/category/data-science/ (RSS)\\ncolah\\'s blog http://colah.github.io/archive.html (RSS)\\nCortana Intelligence and Machine Learning Blog https://blogs.technet.microsoft.com/machinelearning/ (RSS)\\nDaniel Forsyth http://www.danielforsyth.me/ (RSS)\\nDaniel Homola http://danielhomola.com/category/blog/ (RSS)\\nDaniel Nee http://danielnee.com (RSS)\\nData Based Inventions http://datalab.lu/ (RSS)\\nData Blogger https://www.data-blogger.com/ (RSS)\\nData Labs http://blog.insightdatalabs.com/ (RSS)\\nData Meets Media http://datameetsmedia.com/ (RSS)\\nData Miners Blog http://blog.data-miners.com/ (RSS)\\nData Mining Research http://www.dataminingblog.com/ (RSS)\\nData Mining: Text Mining, Visualization and Social Media http://datamining.typepad.com/data_mining/ (RSS)\\nData Piques http://blog.ethanrosenthal.com/ (RSS)\\nData School http://www.dataschool.io/ (RSS)\\nData Science 101 http://101.datascience.community/ (RSS)\\nData Science @ Facebook https://research.facebook.com/blog/datascience/ (RSS)\\nData Science Insights http://www.datasciencebowl.com/data-science-insights/ (RSS)\\nData Science Tutorials https://codementor.io/data-science/tutorial (RSS)\\nData Science Vademecum http://datasciencevademecum.wordpress.com/ (RSS)\\nDataaspirant http://dataaspirant.com/ (RSS)\\nDataclysm http://blog.okcupid.com/ (RSS)\\nDataGenetics http://datagenetics.com/blog.html (RSS)\\nDataiku https://www.dataiku.com/blog/ (RSS)\\nDataKind http://www.datakind.org/blog (RSS)\\nDataLook http://blog.datalook.io/ (RSS)\\nDatanice https://datanice.wordpress.com/ (RSS)\\nDataquest Blog https://www.dataquest.io/blog/ (RSS)\\nDataRobot http://www.datarobot.com/blog/ (RSS)\\nDatascope http://datascopeanalytics.com/blog (RSS)\\nDatasFrame http://tomaugspurger.github.io/ (RSS)\\nDavid Mimno http://www.mimno.org/ (RSS)\\nDayne Batten http://daynebatten.com (RSS)\\nDeep Learning http://deeplearning.net/blog/ (RSS)\\nDeepdish http://deepdish.io/ (RSS)\\nDelip Rao http://deliprao.com/ (RSS)\\nDENNY\\'S BLOG http://blog.dennybritz.com/ (RSS)\\nDimensionless https://dimensionless.in/blog/ (RSS)\\nDistill http://distill.pub/ (RSS)\\nDistrict Data Labs http://districtdatalabs.silvrback.com/ (RSS)\\nDiving into data https://blog.datadive.net/ (RSS)\\nDomino Data Lab\\'s blog http://blog.dominodatalab.com/ (RSS)\\nDr. Randal S. Olson http://www.randalolson.com/blog/ (RSS)\\nDrew Conway https://medium.com/@drewconway (RSS)\\nDustin Tran http://dustintran.com/blog/ (RSS)\\nEder Santana https://edersantana.github.io/blog.html (RSS)\\nEdwin Chen http://blog.echen.me (RSS)\\nEFavDB http://efavdb.com/ (RSS)\\nEmilio Ferrara, Ph.D.  http://www.emilio.ferrara.name/ (RSS)\\nEntrepreneurial Geekiness http://ianozsvald.com/ (RSS)\\nEric Jonas http://ericjonas.com/archives.html (RSS)\\nEric Siegel http://www.predictiveanalyticsworld.com/blog (RSS)\\nErik Bern http://erikbern.com (RSS)\\nERIN SHELLMAN http://www.erinshellman.com/ (RSS)\\nEugenio Culurciello http://culurciello.github.io/ (RSS)\\nFabian Pedregosa http://fa.bianp.net/ (RSS)\\nFast Forward Labs http://blog.fastforwardlabs.com/ (RSS)\\nFastML http://fastml.com/ (RSS)\\nFlorian Hartl http://florianhartl.com/ (RSS)\\nFlowingData http://flowingdata.com/ (RSS)\\nFull Stack ML http://fullstackml.com/ (RSS)\\nGAB41 http://www.lab41.org/gab41/ (RSS)\\nGarbled Notes http://www.chioka.in/ (RSS)\\nGreg Reda http://www.gregreda.com/blog/ (RSS)\\nHyon S Chu https://medium.com/@adailyventure (RSS)\\ni am trask http://iamtrask.github.io/ (RSS)\\nI Quant NY http://iquantny.tumblr.com/ (RSS)\\ninFERENCe http://www.inference.vc/ (RSS)\\nInsight Data Science https://blog.insightdatascience.com/ (RSS)\\nINSPIRATION INFORMATION http://myinspirationinformation.com/ (RSS)\\nIra Korshunova http://irakorshunova.github.io/ (RSS)\\nI’m a bandit https://blogs.princeton.edu/imabandit/ (RSS)\\nJason Toy http://www.jtoy.net/ (RSS)\\nJeremy D. Jackson, PhD http://www.jeremydjacksonphd.com/ (RSS)\\nJesse Steinweg-Woods https://jessesw.com/ (RSS)\\nJoe Cauteruccio http://www.joecjr.com/ (RSS)\\nJohn Myles White http://www.johnmyleswhite.com/ (RSS)\\nJohn\\'s Soapbox http://joschu.github.io/ (RSS)\\nJonas Degrave http://317070.github.io/ (RSS)\\nJoy Of Data http://www.joyofdata.de/blog/ (RSS)\\nJulia Evans http://jvns.ca/ (RSS)\\nKDnuggets http://www.kdnuggets.com/ (RSS)\\nKeeping Up With The Latest Techniques http://colinpriest.com/ (RSS)\\nKenny Bastani http://www.kennybastani.com/ (RSS)\\nKevin Davenport http://kldavenport.com/ (RSS)\\nkevin frans http://kvfrans.com/ (RSS)\\nkorbonits | Math ∩ Data http://korbonits.github.io/ (RSS)\\nLarge Scale Machine Learning  http://bickson.blogspot.com/ (RSS)\\nLATERAL BLOG https://blog.lateral.io/ (RSS)\\nLazy Programmer http://lazyprogrammer.me/ (RSS)\\nLearn Analytics Here https://learnanalyticshere.wordpress.com/ (RSS)\\nLearnDataSci http://www.learndatasci.com/ (RSS)\\nLearning With Data http://learningwithdata.com/ (RSS)\\nLife, Language, Learning http://daoudclarke.github.io/ (RSS)\\nLocke Data https://itsalocke.com/blog/ (RSS)\\nLouis Dorard http://www.louisdorard.com/blog/ (RSS)\\nM.E.Driscoll http://medriscoll.com/ (RSS)\\nMachinalis http://www.machinalis.com/blog (RSS)\\nMachine Learning (Theory) http://hunch.net/ (RSS)\\nMachine Learning and Data Science http://alexhwoods.com/blog/ (RSS)\\nMachine Learning https://charlesmartin14.wordpress.com/ (RSS)\\nMachine Learning Mastery http://machinelearningmastery.com/blog/ (RSS)\\nMachine Learning Blogs https://machinelearningblogs.com/ (RSS)\\nMachine Learning, etc http://yaroslavvb.blogspot.com (RSS)\\nMachine Learning, Maths and Physics https://mlopezm.wordpress.com/ (RSS)\\nMachine Learning Flashcards https://machinelearningflashcards.com/ $10, but a nicely illustrated set of 300 flash cards\\nMachined Learnings http://www.machinedlearnings.com/ (RSS)\\nMAPPING BABEL https://jack-clark.net/ (RSS)\\nMAPR Blog https://www.mapr.com/blog (RSS)\\nMAREK REI http://www.marekrei.com/blog/ (RSS)\\nMARGINALLY INTERESTING http://blog.mikiobraun.de/ (RSS)\\nMath ∩ Programming http://jeremykun.com/ (RSS)\\nMatthew Rocklin http://matthewrocklin.com/blog/ (RSS)\\nMelody Wolk http://melodywolk.com/projects/ (RSS)\\nMic Farris http://www.micfarris.com/ (RSS)\\nMike Tyka http://mtyka.github.io/ (RSS)\\nminimaxir | Max Woolf\\'s Blog http://minimaxir.com/ (RSS)\\nMirror Image https://mirror2image.wordpress.com/ (RSS)\\nMitch Crowe http://www.dataphoric.com/ (RSS)\\nMLWave http://mlwave.com/ (RSS)\\nMLWhiz http://mlwhiz.com/ (RSS)\\nModels are illuminating and wrong https://peadarcoyle.wordpress.com/ (RSS)\\nMoody Rd http://blog.mrtz.org/ (RSS)\\nMoonshots http://jxieeducation.com/ (RSS)\\nMourad Mourafiq http://mourafiq.com/ (RSS)\\nMy thoughts on Data science, predictive analytics, Python http://shahramabyari.com/ (RSS)\\nNatural language processing blog http://nlpers.blogspot.fr/ (RSS)\\nNeil Lawrence http://inverseprobability.com/blog.html (RSS)\\nNLP and Deep Learning enthusiast http://camron.xyz/ (RSS)\\nno free hunch http://blog.kaggle.com/ (RSS)\\nNuit Blanche http://nuit-blanche.blogspot.com/ (RSS)\\nNumber 2147483647 https://no2147483647.wordpress.com/ (RSS)\\nOn Machine Intelligence https://aimatters.wordpress.com/ (RSS)\\nOpiate for the masses Data is our religion. http://opiateforthemass.es/ (RSS)\\np-value.info http://www.p-value.info/ (RSS)\\nPete Warden\\'s blog http://petewarden.com/ (RSS)\\nPlotly Blog http://blog.plot.ly/ (RSS)\\nProbably Overthinking It http://allendowney.blogspot.ca/ (RSS)\\nProoffreader.com http://www.prooffreader.com (RSS)\\nProoffreaderPlus http://prooffreaderplus.blogspot.ca/ (RSS)\\nPublishable Stuff http://www.sumsar.net/ (RSS)\\nPyImageSearch http://www.pyimagesearch.com/ (RSS)\\nPythonic Perambulations https://jakevdp.github.io/ (RSS)\\nquintuitive http://quintuitive.com/ (RSS)\\nR and Data Mining https://rdatamining.wordpress.com/ (RSS)\\nR-bloggers http://www.r-bloggers.com/ (RSS)\\nR2RT http://r2rt.com/ (RSS)\\nRamiro Gómez http://ramiro.org/notebooks/ (RSS)\\nRandom notes on Computer Science, Mathematics and Software Engineering http://barmaley-exe.github.io/ (RSS)\\nRandy Zwitch http://randyzwitch.com/ (RSS)\\nRaRe Technologies http://rare-technologies.com/blog/ (RSS)\\nRayli.Net http://rayli.net/blog/ (RSS)\\nRevolutions http://blog.revolutionanalytics.com/ (RSS)\\nRinu Boney http://rinuboney.github.io/ (RSS)\\nRNDuja Blog http://rnduja.github.io/ (RSS)\\nRobert Chang https://medium.com/@rchang (RSS)\\nRocket-Powered Data Science http://rocketdatascience.org (RSS)\\nSachin Joglekar\\'s blog https://codesachin.wordpress.com/ (RSS)\\nsamim https://medium.com/@samim (RSS)\\nSean J. Taylor http://seanjtaylor.com/ (RSS)\\nSebastian Raschka http://sebastianraschka.com/blog/index.html (RSS)\\nSebastian Ruder http://sebastianruder.com/ (RSS)\\nSebastian\\'s slow blog http://www.nowozin.net/sebastian/blog/ (RSS)\\nSFL Scientific Blog https://sflscientific.com/blog/ (RSS)\\nShakir\\'s Machine Learning Blog http://blog.shakirm.com/ (RSS)\\nSimply Statistics http://simplystatistics.org (RSS)\\nSpringboard Blog http://springboard.com/blog\\nStartup.ML Blog http://startup.ml/blog (RSS)\\nStatistical Modeling, Causal Inference, and Social Science http://andrewgelman.com/ (RSS)\\nStigler Diet http://stiglerdiet.com/ (RSS)\\nStitch Fix Tech Blog http://multithreaded.stitchfix.com/blog/ (RSS)\\nStochastic R&D Notes http://arseny.info/ (RSS)\\nStorytelling with Statistics on Quora http://datastories.quora.com/ (RSS)\\nStreamHacker http://streamhacker.com/ (RSS)\\nSubconscious Musings http://blogs.sas.com/content/subconsciousmusings/ (RSS)\\nSwan Intelligence http://swanintelligence.com/ (RSS)\\nTechnoCalifornia http://technocalifornia.blogspot.se/ (RSS)\\nTEXT ANALYSIS BLOG | AYLIEN http://blog.aylien.com/ (RSS)\\nThe Angry Statistician http://angrystatistician.blogspot.com/ (RSS)\\nThe Clever Machine https://theclevermachine.wordpress.com/ (RSS)\\nThe Data Camp Blog https://www.datacamp.com/community/blog (RSS)\\nThe Data Incubator http://blog.thedataincubator.com/ (RSS)\\nThe Data Science Lab https://datasciencelab.wordpress.com/ (RSS)\\nTHE ETZ-FILES http://alexanderetz.com/ (RSS)\\nThe Science of Data http://www.martingoodson.com (RSS)\\nThe Shape of Data https://shapeofdata.wordpress.com (RSS)\\nThe unofficial Google data science Blog http://www.unofficialgoogledatascience.com/ (RSS)\\nTim Dettmers http://timdettmers.com/ (RSS)\\nTombone\\'s Computer Vision Blog http://www.computervisionblog.com/ (RSS)\\nTommy Blanchard http://tommyblanchard.com/category/projects (RSS)\\nTrevor Stephens http://trevorstephens.com/ (RSS)\\nTrey Causey http://treycausey.com/ (RSS)\\nUW Data Science Blog http://datasciencedegree.wisconsin.edu/blog/ (RSS)\\nWellecks http://wellecks.wordpress.com/ (RSS)\\nWes McKinney http://wesmckinney.com/archives.html (RSS)\\nWhile My MCMC Gently Samples http://twiecki.github.io/ (RSS)\\nWildML http://www.wildml.com/ (RSS)\\nWill do stuff for stuff http://rinzewind.org/blog-en (RSS)\\nWill wolf http://willwolf.io/ (RSS)\\nWILL\\'S NOISE http://www.willmcginnis.com/ (RSS)\\nWilliam Lyon http://www.lyonwj.com/ (RSS)\\nWin-Vector Blog http://www.win-vector.com/blog/ (RSS)\\nYanir Seroussi http://yanirseroussi.com/ (RSS)\\nZac Stewart http://zacstewart.com/ (RSS)\\nŷhat http://blog.yhat.com/ (RSS)\\nℚuantitative √ourney http://outlace.com/ (RSS)\\n大トロ http://blog.otoro.net/ (RSS)\\n\\ncredits\\n\\nPython for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython by Wes McKinney\\nPyCon 2015 Scikit-learn Tutorial by Jake VanderPlas\\nPython Data Science Handbook by Jake VanderPlas\\nParallel Machine Learning with scikit-learn and IPython by Olivier Grisel\\nStatistical Interference Using Computational Methods in Python by Allen Downey\\nTensorFlow Examples by Aymeric Damien\\nTensorFlow Tutorials by Parag K Mital\\nTensorFlow Tutorials by Nathan Lintz\\nTensorFlow Tutorials by Alexander R Johansen\\nTensorFlow Book by Nishant Shukla\\nSummer School 2015 by mila-udem\\nKeras tutorials by Valerio Maggio\\nKaggle\\nYhat Blog\\n\\ncontributing\\nContributions are welcome!  For bug reports or requests please submit an issue.\\ncontact-info\\nFeel free to contact me to discuss any issues, questions, or comments.\\n\\nEmail: tarry.singh@gmail.com\\nTwitter: @tarrysingh\\nGitHub: tarrysingh\\nLinkedIn: Tarry Singh\\nWebsite: tarrysingh.com\\nMedium: tarry@Medium\\nQuora : Answers from Tarry on Quora\\n\\nlicense\\nThis repository contains a variety of content; some developed by Tarry Singh and some from third-parties and a lot will be maintained by me. The third-party content is distributed under the license provided by those parties.\\nThe content was originally developed by Donne Martin is distributed under the following license. I will be maintaining and revamping it by adding PyTorch, Torch/Lua, MXNET and much more:\\nI am providing code and resources in this repository to you under an open source license.\\nCopyright 2017 Tarry Singh\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n'},\n",
       " {'language': 'Python 59.8',\n",
       "  'readme': 'Artificial Intelligence Nanodegree Program Resources\\nClassroom Exercises\\n1. Constraint Satisfaction Problems\\nIn this exercise you will explore Constraint Satisfaction Problems in a Jupyter notebook and use a CSP solver to solve a variety of problems.\\nRead more here\\n2. Classical Search for PacMan (only in classroom)\\nPlease DO NOT publish your work on this exercise.\\nIn this exercise you will teach Pac-Man to search his world to complete the following tasks:\\n\\nfind a single obstacle\\nfind multiple obstacles\\nfind the fastest way to eat all the food in the map\\n\\n3. Local Search Optimization\\nIn this exercise, you\\'ll implement several local search algorithms and test them on the Traveling Salesman Problem (TSP) between a few dozen US state capitals.\\nProjects\\n1. Sudoku Solver\\nIn this project, you will extend the Sudoku-solving agent developed in the classroom lectures to solve diagonal Sudoku puzzles and implement a new constraint strategy called \"naked twins\". A diagonal Sudoku puzzle is identical to traditional Sudoku puzzles with the added constraint that the boxes on the two main diagonals of the board must also contain the digits 1-9 in each cell (just like the rows, columns, and 3x3 blocks).\\nRead more here\\n2. Classical Planning\\nThis project is split between implementation and analysis. First you will combine symbolic logic and classical search to implement an agent that performs progression search to solve planning problems. Then you will experiment with different search algorithms and heuristics, and use the results to answer questions about designing planning systems.\\nRead more here\\n3. Game Playing\\nIn this project you will choose an experiment with adversarial game-playing techniques like minimax, Monte Carlo tree search, opening books, and more. Your goal will be to build and evaluate the performance of your agent in a finite deterministic two player game of perfect information called Isolation.\\nRead more here\\n4. Part of Speech Tagger\\nIn this notebook, you\\'ll use the Pomegranate library to build a hidden Markov model for part of speech tagging with a universal tagset. Hidden Markov models have been able to achieve >96% tag accuracy with larger tagsets on realistic text corpora. Hidden Markov models have also been used for speech recognition and speech generation, machine translation, gene recognition for bioinformatics, and human gesture recognition for computer vision, and more.\\nRead more here\\n'},\n",
       " {'language': 'Python 92.7',\n",
       "  'readme': '   \\n \\n\\n\\nMycroft\\nMycroft is a hackable open source voice assistant.\\nTable of Contents\\n\\nTable of Contents\\nGetting Started\\nRunning Mycroft\\nUsing Mycroft\\n\\nHome Device and Account Manager\\nSkills\\n\\n\\nBehind the scenes\\n\\nPairing Information\\nConfiguration\\nUsing Mycroft Without Home\\nAPI Key Services\\nUsing Mycroft behind a proxy\\n\\nUsing Mycroft behind a proxy without authentication\\nUsing Mycroft behind an authenticated proxy\\n\\n\\n\\n\\nGetting Involved\\nLinks\\n\\nGetting Started\\nFirst, get the code on your system!  The simplest method is via git (git installation instructions):\\n\\ncd ~/\\ngit clone https://github.com/MycroftAI/mycroft-core.git\\ncd mycroft-core\\nbash dev_setup.sh\\n\\nThis script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.\\nNOTE: The default branch for this repository is \\'dev\\', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the \\'master\\' branch.\\nRunning Mycroft\\nMycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:\\n\\ncd ~/mycroft-core\\n./start-mycroft.sh debug\\n\\nThe \"debug\" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.\\nThe background services can be stopped as a group with:\\n\\n./stop-mycroft.sh\\n\\nUsing Mycroft\\nHome Device and Account Manager\\nMycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai\\nBy default, mycroft-core  is configured to use Home. By saying \"Hey Mycroft, pair my device\" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can enter into the pairing page within the Mycroft Home site.\\nOnce paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.\\nSkills\\nMycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!\\nBehind the scenes\\nPairing Information\\nPairing information generated by registering with Home is stored in:\\n~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!\\nConfiguration\\nMycroft configuration consists of 4 possible locations:\\n\\nmycroft-core/mycroft/configuration/mycroft.conf(Defaults)\\nMycroft Home (Remote)\\n/etc/mycroft/mycroft.conf(Machine)\\n$HOME/.mycroft/mycroft.conf(User)\\n\\nWhen the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.\\nUsing Mycroft Without Home\\nIf you do not wish to use the Mycroft Home service, before starting Mycroft for the first time, create $HOME/.mycroft/mycroft.conf with the following contents:\\n{\\n  \"skills\": {\\n    \"blacklisted_skills\": [\\n      \"mycroft-configuration.mycroftai\",\\n      \"mycroft-pairing.mycroftai\"\\n    ]\\n  }\\n}\\n\\nMycroft will then be unable to perform speech-to-text conversion, so you\\'ll need to set that up as well, using one of the STT engines Mycroft supports.\\nYou may insert your own API keys into the configuration files listed above in Configuration.  For example, to insert the API key for the Weather skill, create a new JSON key in the configuration file like so:\\n{\\n  // other configuration settings...\\n  //\\n  \"WeatherSkill\": {\\n    \"api_key\": \"<insert your API key here>\"\\n  }\\n}\\n\\nAPI Key Services\\nThese are the keys currently used in Mycroft Core:\\n\\nSTT API, Google STT, Google Cloud Speech\\nWeather Skill API, OpenWeatherMap\\nWolfram-Alpha Skill\\n\\nUsing Mycroft behind a proxy\\nMany schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.\\nIf you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.\\nNOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.\\nUsing Mycroft behind a proxy without authentication\\nIf you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).\\n$ export http_proxy=http://proxy_hostname.com:proxy_port\\n$ export https_port=http://proxy_hostname.com:proxy_port\\n$ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"\\nUsing Mycroft behind an authenticated proxy\\nIf  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).\\n$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port\\n$ export https_port=http://user:password@proxy_hostname.com:proxy_port\\n$ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"\\nGetting Involved\\nThis is an open source project and we would love your help. We have prepared a contributing guide to help you get started.\\nIf this is your first PR or you\\'re not sure where to get started,\\nsay hi in Mycroft Chat and a team member would be happy to mentor you.\\nJoin the Mycroft Forum for questions and answers.\\nLinks\\n\\nCreating a Skill\\nDocumentation\\nSkill Writer API Docs\\nRelease Notes\\nMycroft Chat\\nMycroft Forum\\nMycroft Blog\\n\\n'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Artificial Intelligence with Python\\nThis is the code repository for Artificial Intelligence with Python, published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.\\nAbout the Book\\nDuring the course of this book, you will find out how to make informed decisions about what algorithms to use in a given context. Starting from the basics of Artificial Intelligence, you will learn how to develop various building blocks using different data mining techniques. You will see how to implement different algorithms to get the best possible results, and will understand how to apply them to real-world scenarios. If you want to add an intelligence layer to any application that’s based on images, text, stock market, or some other form of data, this exciting book on Artificial Intelligence will definitely be your guide!\\n##Instructions and Navigation\\nAll of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter02.\\nThe code will look like the following:\\nA block of code is set as follows:\\n\\n[default]\\nexten => s,1,Dial(Zap/1|30)\\nexten => s,2,Voicemail(u100)\\nexten => s,102,Voicemail(b100)\\nexten => i,1,Voicemail(s0)\\n\\nThis book is focused on artificial intelligence in Python as opposed to the Python itself. We have used Python 3 to build various applications. We focus on how to utilize various Python libraries in the best possible way to build real world applications. In that spirit, we have tried to keep all of the code as friendly and readable as possible. We feel that this will enable our readers to easily understand the code and readily use it in different scenarios.\\nRelated Products\\n\\n\\nDeep Learning with Python [Video]\\n\\n\\nLearning IPython for Interactive Computing and Data Visualization\\n\\n\\nPython High Performance Programming\\n\\n\\nSuggestions and Feedback\\nClick here if you have any feedback or suggestions.\\n'},\n",
       " {'language': 'Python 51.4',\n",
       "  'readme': 'Artificial Intelligence with Python\\nThis is the code repository for Artificial Intelligence with Python, published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.\\nAbout the Book\\nDuring the course of this book, you will find out how to make informed decisions about what algorithms to use in a given context. Starting from the basics of Artificial Intelligence, you will learn how to develop various building blocks using different data mining techniques. You will see how to implement different algorithms to get the best possible results, and will understand how to apply them to real-world scenarios. If you want to add an intelligence layer to any application that’s based on images, text, stock market, or some other form of data, this exciting book on Artificial Intelligence will definitely be your guide!\\n##Instructions and Navigation\\nAll of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter02.\\nThe code will look like the following:\\nA block of code is set as follows:\\n\\n[default]\\nexten => s,1,Dial(Zap/1|30)\\nexten => s,2,Voicemail(u100)\\nexten => s,102,Voicemail(b100)\\nexten => i,1,Voicemail(s0)\\n\\nThis book is focused on artificial intelligence in Python as opposed to the Python itself. We have used Python 3 to build various applications. We focus on how to utilize various Python libraries in the best possible way to build real world applications. In that spirit, we have tried to keep all of the code as friendly and readable as possible. We feel that this will enable our readers to easily understand the code and readily use it in different scenarios.\\nRelated Products\\n\\n\\nDeep Learning with Python [Video]\\n\\n\\nLearning IPython for Interactive Computing and Data Visualization\\n\\n\\nPython High Performance Programming\\n\\n\\nSuggestions and Feedback\\nClick here if you have any feedback or suggestions.\\n'},\n",
       " {'language': 'Python 91.1',\n",
       "  'readme': '\\nSimple AI\\nProject home: http://github.com/simpleai-team/simpleai\\nThis lib implements many of the artificial intelligence algorithms described on the book \"Artificial Intelligence, a Modern Approach\", from Stuart Russel and Peter Norvig. We strongly recommend you to read the book, or at least the introductory chapters and the ones related to the components you want to use, because we won\\'t explain the algorithms here.\\nThis implementation takes some of the ideas from the Norvig\\'s implementation (the aima-python lib), but it\\'s made with a more \"pythonic\" approach, and more emphasis on creating a stable, modern, and maintainable version. We are testing the majority of the lib, it\\'s available via pip install, has a standard repo and lib architecture, well documented, respects the python pep8 guidelines, provides only working code (no placeholders for future things), etc. Even the internal code is written with readability in mind, not only the external API.\\nAt this moment, the implementation includes:\\n\\n\\nSearch\\n\\nTraditional search algorithms (not informed and informed)\\nLocal Search algorithms\\nConstraint Satisfaction Problems algorithms\\nInteractive execution viewers for search algorithms (web-based and terminal-based)\\n\\n\\n\\n\\n\\nMachine Learning\\n\\nStatistical Classification\\n\\n\\n\\n\\n\\n\\nInstallation\\nJust get it:\\npip install simpleai\\n\\nAnd if you want to use the interactive search viewers, also install:\\npip install pydot flask\\n\\nYou will need to have pip installed on your system. On linux install the\\npython-pip package, on windows follow this.\\nAlso, if you are on linux and not working with a virtualenv, remember to use\\nsudo for both commands (sudo pip install ...).\\n\\nExamples\\nSimple AI allows you to define problems and look for the solution with\\ndifferent strategies. Another samples are in the samples directory, but\\nhere is an easy one.\\nThis problem tries to create the string \"HELLO WORLD\" using the A* algorithm:\\nfrom simpleai.search import SearchProblem, astar\\n\\nGOAL = \\'HELLO WORLD\\'\\n\\n\\nclass HelloProblem(SearchProblem):\\n    def actions(self, state):\\n        if len(state) < len(GOAL):\\n            return list(\\' ABCDEFGHIJKLMNOPQRSTUVWXYZ\\')\\n        else:\\n            return []\\n\\n    def result(self, state, action):\\n        return state + action\\n\\n    def is_goal(self, state):\\n        return state == GOAL\\n\\n    def heuristic(self, state):\\n        # how far are we from the goal?\\n        wrong = sum([1 if state[i] != GOAL[i] else 0\\n                    for i in range(len(state))])\\n        missing = len(GOAL) - len(state)\\n        return wrong + missing\\n\\nproblem = HelloProblem(initial_state=\\'\\')\\nresult = astar(problem)\\n\\nprint(result.state)\\nprint(result.path())\\n\\nMore detailed documentation\\nYou can read the docs online here. Or for offline access, you can clone the project code repository and read them from the docs folder.\\n\\nHelp and discussion\\nJoin us at the Simple AI google group.\\n\\nAuthors\\n\\nMany people you can find on the contributors section.\\nSpecial acknowledgements to Machinalis for the time provided to work on this project. Machinalis also works on some other very interesting projects, like Quepy and more.\\n\\n'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"Snake\\n  \\nThe project focuses on the artificial intelligence of the Snake game. The snake's goal is to eat the food continuously and fill the map with its bodies as soon as possible. Originally, the project was written in C++. It has now been rewritten in Python for a user-friendly GUI and the simplicity in algorithm implementations.\\nAlgorithms >\\nExperiments\\nWe use two metrics to evaluate the performance of an AI:\\n\\nAverage Length: Average length the snake has grown to (max: 64).\\nAverage Steps: Average steps the snake has moved.\\n\\nTest results (averaged over 1000 episodes):\\n\\n\\n\\nSolver\\nDemo (optimal)\\nAverage Length\\nAverage Steps\\n\\n\\n\\n\\nHamilton\\n\\n63.93\\n717.83\\n\\n\\nGreedy\\n\\n60.15\\n904.56\\n\\n\\nDQN(experimental)\\n\\n24.44\\n131.69\\n\\n\\n\\nInstallation\\nRequirements: Python 3.5+ (64-bit) with Tkinter installed.\\n$ pip3 install -r requirements.txt\\n\\n# Use -h for more details\\n$ python3 run.py [-h]\\n\\nRun unit tests:\\n$ python3 -m pytest -v\\n\\nLicense\\nSee the LICENSE file for license rights and limitations.\\n\"},\n",
       " {'language': 'Python 76.6',\n",
       "  'readme': \"\\n\\n\\n\\n\\n\\nNon-tech crash course into Operação Serenata de Amor\\nTech crash course into Operação Serenata de Amor\\nContributing with code and tech skills\\nSupporting\\nAcknowledgments\\n\\nNon-tech crash course into Operação Serenata de Amor\\nWhat\\nSerenata de Amor is an open project using artificial intelligence for social control of public administration.\\nWho\\nWe are a group of people who believes in power to the people motto. We are also part of the Data Science for Civic Innovation Programme from Open Knowledge Brasil.\\nAmong founders and long-term members, we can list a group of eight people – plus numerous contributors from the open source and open knowledge communities:  Tatiana Balachova, Felipe Cabral, Eduardo Cuducos,  Irio Musskopf, Bruno Pazzim, Ana Schwendler, Jessica Temporal, Yasodara Córdova and Pedro Vilanova.\\nHow\\nSimilar to organizations like Google, Facebook, and Netflix, we use technology to track government spendings and make open data accessible for everyone. We started looking into data from the Chamber of Deputies (Brazilian lower house) but we expanded to the Federal Senate (Brazilian upper house) and to municipalities.\\nWhen\\nIrio had the main ideas for the project in early 2016. For a few months, he experimented and gathered people around the project. September, 2016 marks the launching of our first crowd funding. Since then, we have been creating open source technological products and tools, as well as high quality content on civic tech on our Facebook and Medium.\\nWhere\\nWe have no non-virtual headquarters, but we work remotely everyday. Most of our ideas are crafted to work in any country that offers open data, but our main implementations focus in Brazil.\\nWhy\\nEmpowering citizens with data is important: people talk about smart cities, surveillance and privacy. We prefer to focus on smart citizens, accountability and open knowledge.\\nTech crash course into Operação Serenata de Amor\\nWhat\\nSerenata de Amor develops open source tools to make it easy for people to use open data. The focus is to gather relevant insights and share them in an accessible interface. Through this interface, we invite citizens to dialogue with politicians, state and government about public spendings.\\nWho\\nSerenata's main role is played by Rosie: she is an artificial intelligence who analyzes Brazilian congresspeople expenses while they are in office. Rosie can find suspicious spendings and engage citizens in the discussion about these findings. She's on Twitter.\\nTo allow people to visualize and make sense of data Rosie generates, we have created Jarbas. On this website, users can browse congresspeople expenses and get details about each of the suspicions. It is the starting point to validate a suspicion.\\nHow\\nWe have two main repositories on GitHub. This is the main repo and hosts Rosie and Jarbas. In addition, we have the toolbox - a pip installable package. Yet there are experimental notebooks maintained by the community and our static webpage.\\nWhen\\nDespite all these players acting together, the core part of the job is ran manually from time to time. The only part that is always online is Jarbas – freely serving a wide range of information about public expenditure 24/7.\\nRoughly once a month, we manually run Rosie and update Jarbas. A few times per year, we upload versioned datasets accessible via the toolbox – but we encourage you to use the toolbox to generate fresh datasets whenever you need.\\nWhere\\nJarbas is running in Digital Ocean droplets, and deployed using the Docker Cloud architecture.\\nWhy\\nThe answer to most technical why questions is because that is what we had in the past and enabled us to deliver fast. We acknowledge that this is not the best stack ever, but it has brought us here.\\nContributing with code and tech skills\\nMake sure you have read the Tech crash course on this page. Next, check out our contributing guide.\\nSupporting\\n\\nJoin our recurring crowd funding campaign on Apoia.se\\nDonate via Bitcoin to 1Gbvfjmjvur7qwbwNFdPSNDgx66KSdVB5b\\nFollow, share and interact with us on Facebook\\nFollow, retweet and join Rosie on Twitter to interact with your favourite congresspeople\\n\\nAcknowledgments\\n \\n\"},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'easyAI\\nEasyAI (full documentation here) is a pure-Python artificial intelligence framework for two-players abstract games such as Tic Tac Toe, Connect 4, Reversi, etc.\\nIt makes it easy to define the mechanisms of a game, and play against the computer or solve the game.\\nUnder the hood, the AI is a Negamax algorithm with alpha-beta pruning and transposition tables as described on Wikipedia.\\n\\nInstallation\\nIf you have pip installed, type this in a terminal\\nsudo pip install easyAI\\n\\nOtherwise, dowload the source code (for instance on Github), unzip everything into one folder and in this folder, in a terminal, type\\nsudo python setup.py install\\n\\nAdditionnally you will need to install Numpy to be able to run some of the examples.\\n\\nA quick example\\nLet us define the rules of a game and start a match against the AI:\\nfrom easyAI import TwoPlayersGame, Human_Player, AI_Player, Negamax\\n\\nclass GameOfBones( TwoPlayersGame ):\\n    \"\"\" In turn, the players remove one, two or three bones from a\\n    pile of bones. The player who removes the last bone loses. \"\"\"\\n\\n    def __init__(self, players):\\n        self.players = players\\n        self.pile = 20 # start with 20 bones in the pile\\n        self.nplayer = 1 # player 1 starts\\n\\n    def possible_moves(self): return [\\'1\\',\\'2\\',\\'3\\']\\n    def make_move(self,move): self.pile -= int(move) # remove bones.\\n    def win(self): return self.pile<=0 # opponent took the last bone ?\\n    def is_over(self): return self.win() # Game stops when someone wins.\\n    def show(self): print (\"%d bones left in the pile\" % self.pile)\\n    def scoring(self): return 100 if game.win() else 0 # For the AI\\n\\n# Start a match (and store the history of moves when it ends)\\nai = Negamax(13) # The AI will think 13 moves in advance\\ngame = GameOfBones( [ Human_Player(), AI_Player(ai) ] )\\nhistory = game.play()\\nResult:\\n20 bones left in the pile\\n\\nPlayer 1 what do you play ? 3\\n\\nMove #1: player 1 plays 3 :\\n17 bones left in the pile\\n\\nMove #2: player 2 plays 1 :\\n16 bones left in the pile\\n\\nPlayer 1 what do you play ?\\n\\n\\nSolving the game\\nLet us now solve the game:\\nfrom easyAI import id_solve\\nr,d,m = id_solve(GameOfBones, ai_depths=range(2,20), win_score=100)\\nWe obtain r=1, meaning that if both players play perfectly, the first player to play can always win (-1 would have meant always lose), d=10, which means that the wins will be in ten moves (i.e. 5 moves per player) or less, and m=\\'3\\', which indicates that the first player\\'s first move should be \\'3\\'.\\nThese computations can be sped up using a transposition table which will store the situations encountered and the best moves for each:\\ntt = TT()\\nGameOfBones.ttentry = lambda game : game.pile # key for the table\\nr,d,m = id_solve(GameOfBones, range(2,20), win_score=100, tt=tt)\\nAfter these lines are run the variable tt contains a transposition table storing the possible situations (here, the possible sizes of the pile) and the optimal moves to perform. With tt you can play perfectly without thinking:\\n.. code:: python\\n\\n\\ngame = GameOfBones( [  AI_Player( tt ), Human_Player() ] )\\ngame.play() # you will always lose this game :)\\n\\nContribute !\\nEasyAI is an open source software originally written by Zulko and released under the MIT licence. It could do with some improvements, so if your are a Python/AI guru maybe you can contribute through Github . Some ideas: AI algos for incomplete information games, better game solving strategies, (efficient) use of databases to store moves,  AI algorithms using parallelisation.\\nFor troubleshooting and bug reports, the best for now is to ask on Github.\\n\\nMaintainers\\n\\nZulko (owner)\\nJohnAD\\n\\n'},\n",
       " {'language': 'Python 100.0', 'readme': '说明\\n学习人工智能过程中的一些模式识别作业\\n'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\\nCode accompanying CVPR'20 paper of the same title. Paper link: https://arxiv.org/pdf/2003.03808.pdf\\nNOTE\\nWe have noticed a lot of concern that PULSE will be used to identify individuals whose faces have been blurred out. We want to emphasize that this is impossible - PULSE makes imaginary faces of people who do not exist, which should not be confused for real people. It will not help identify or reconstruct the original image.\\nWe also want to address concerns of bias in PULSE. We have now included a new section in the paper and an accompanying model card directly addressing this bias.\\n\\n\\n\\n\\nTable of Contents\\n\\nPULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\\nTable of Contents\\n\\nWhat does it do?\\nUsage\\n\\nPrereqs\\nData\\nApplying PULSE\\n\\n\\n\\n\\n\\nWhat does it do?\\nGiven a low-resolution input image, PULSE searches the outputs of a generative model (here, StyleGAN) for high-resolution images that are perceptually realistic and downscale correctly.\\n\\nUsage\\nThe main file of interest for applying PULSE is run.py. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.\\nPrereqs\\nYou will need to install cmake first (required for dlib, which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux and Windows. For the full set of required Python packages, create a Conda environment from the provided YAML, e.g.\\nconda create -f pulse.yml \\n\\nor (Anaconda on Windows):\\nconda env create -n pulse -f pulse.yml\\nconda activate pulse\\n\\nIn some environments (e.g. on Windows), you may have to edit the pulse.yml to remove the version specific hash on each dependency and remove any dependency that still throws an error after running conda env create... (such as readline)\\ndependencies\\n  - blas=1.0=mkl\\n  ...\\n\\nto\\ndependencies\\n  - blas=1.0\\n ...\\n\\nFinally, you will need an internet connection the first time you run the code as it will automatically download the relevant pretrained model from Google Drive (if it has already been downloaded, it will use the local copy). In the event that the public Google Drive is out of capacity, add the files to your own Google Drive instead; get the share URL and replace the ID in the https://drive.google.com/uc?=ID links in align_face.py and PULSE.py with the new file ids from the share URL given by your own Drive file.\\nData\\nBy default, input data for run.py should be placed in ./input/ (though this can be modified). However, this assumes faces have already been aligned and downscaled. If you have data that is not already in this form, place it in realpics and run align_face.py which will automatically do this for you. (Again, all directories can be changed by command line arguments if more convenient.) You will at this stage pic a downscaling factor.\\nNote that if your data begins at a low resolution already, downscaling it further will retain very little information. In this case, you may wish to bicubically upsample (usually, to 1024x1024) and allow align_face.py to downscale for you.\\nApplying PULSE\\nOnce your data is appropriately formatted, all you need to do is\\npython run.py\\n\\nEnjoy!\\n\"},\n",
       " {'language': 'Python 72.1',\n",
       "  'readme': \"This repository was originally at https://github.com/aiforearth/SpaceNetExploration. For other Microsoft AI for Earth repositories, search for the topic #aiforearth on GitHub or visit them here.\\nBuilding Footprint Extraction\\nOverview\\nThis repository contains a walkthrough demonstrating how to perform semantic segmentation using convolutional neural networks (CNNs) on satellite images to extract the footprints of buildings. We show how to carry out the procedure on an Azure Deep Learning Virtual Machine (DLVM), which are GPU-enabled and have all major frameworks pre-installed so you can start model training straight-away. We use a subset of the data and labels from the SpaceNet Challenge, an online repository of freely available satellite imagery released to encourage the application of machine learning to geospatial data.\\nThe blog post that first announced this sample project is here on the Azure Blog.\\nData\\nSpaceNet Building Footprint Extraction Dataset\\nThe code in this repository was developed for training a semantic segmentation model (currently two variants of the U-Net are implemented) on the Vegas set of the SpaceNet building footprint extraction data. This makes the sample code clearer, but it can be easily extended to take in training data from the four other locations.\\nThe organizers release a portion of this data as training data and the rest are held out for the purpose of the competitions they hold. For the experiments discussed here, we split the official training set 70:15:15 into our own training, validation and test sets. These are 39 GB in size as raw images in TIFF format with labels.\\nGenerate Input from Raw Data\\nInstruction for downloading the SpaceNet data can be found on their website. The authors provide a set of utilities to convert the raw images to a format that semantic segmentation models can take as input. The utilities are in this repo. Most of the functionalities you will need are in the python folder. Please read their instructions on the repo's README to understand all the tools and parameters available. After using python/createDataSpaceNet.py from the utilities repo to process the raw data, the input image and its label look like the following:\\n\\nEnvironment Setup\\nProvision an Azure Deep Learning Virtual Machine\\nYou could train your models on a Deep Learning Virtual Machine (DLVM) on Azure to get started quickly, where all the major deep learning frameworks, including PyTorch used in this repo, are installed and ready to use. These VMs are configured specifically for use with GPUs. Instructions for provisioning can be found here. The code here has been used on a Ubuntu Linux DLVM, but you should be able to use it on a Windows DLVM with minor modifications to the commands such as those setting environment variable values. The commands on this page are for running in a Linux shell.\\nAdditional Packages to Install\\nThere are two additional packages for the polygonization of the result of the CNN model so that our results can be compared to the original labels, which are expressed in a polygon data type. You can install these using pip:\\npip install rasterio\\npip install shapely\\n\\nData Storage Options\\nFor quick experimentations you could download your data to the OS disk, but this makes data transfer and sharing costly when you scale out.\\nThere are several options for storing the data while you perform computation on them in Azure. Here's a piece of documentation to guide you through choosing among these, and here are the pricing information.\\nIf you are not planning on training models distributedly across several machines, you could attach a data disk to your VM. See instructions on attaching a data disk to a Linux VM. You can later re-attach this data disk to a more powerful VM, but it can only be attached to one machine at a time.\\nFor both Azure Blob Storage and File Share, you can browse the files stored from any computer using the Storage Explorer desktop app. Both blob storage containers and file shares can be mounted on your VM so you can use them as if they were local disks. See instructions for mounting blob storage and file shares. Note however that such file systems have different performance for writing and deleting files than local file systems. Please refer to Azure Storage performance targets for more information.\\nModel Training\\nWe tackle the problem of outlining building footprints in satellite images by applying a semantic segmentation model to first classify each pixel as background, building, or boundary of buildings. The U-Net is used for this task. There are two variants of the U-Net implemented in the models directory, differing by the sizes of filters used. The baseline U-Net is a similar version as used by the winner of the SpaceNet Building Footprint competition XD_XD. We referenced several open source implementations, noted in the relevant files.\\nCode for training the model is in the pipeline directory. The training script is train.py and all the paths to input/output, parameters and other arguments are specified in train_config.py, which you can modify and experiment with. The default configuration has total_epochs set to 15 to run training for 15 epochs, which takes about an hour in total on a VM with a P100 GPU (SKU NC6s_v2 on Azure). For the sample image above, the result of the segmentation model is as follows at epoch 3, 5, 7 and 10:\\n\\nGenerate Polygons of the Building Footprints\\nStandard graphics techniques are used to convert contiguous blobs of building pixels identified by the segmentation model, using libraries Rasterio and Shapely. The script pipeline/polygonize.py performs this procedure, and you can change various parameters in polygonize_config.py in the same directory. The most important parameter influencing the performance of the model is min_polygon_area, which is the area in squared pixels below which blobs of building pixels are discarded, reducing the noise in our results. Increasing this threshold decreases the number of false positive footprint proposals.\\nEvaluation\\nThe evaluation metric used by the SpaceNet Challenge is the F1 score, where a footprint proposal is counted as a true positive if its intersection over union (IoU) with the ground truth polygon is above 0.5.\\nYou can of course employ your own metric to suit your application, but if you would like to use the SpaceNet utilities to compute the F1 score based on polygons of building footprints, you need to first combine the annotations for each image in geojson format into a csv with python/createCSVFromGEOJSON.py from the utilities repo. In the root directory of utilities, run\\npython python/createCSVFromGEOJSON.py -imgDir /tutorial_data/val/RGB-PanSharpen -geoDir /tutorial_data/val/geojson/buildings -o ground_truth.csv --CreateProposalFile\\n\\nThen you can use python/evaluateScene.py to compute the F1 score, giving the ground truth csv produced from the last command and the csv output proposals.csv produced by pipeline/polygonize.py in this repo:\\npython python/evaluateScene.py ground_truth.csv proposal.csv\\n\\nRelated Materials\\nBing team's announcement that they released a large quantity of building footprints in the US in support of the Open Street Map community, and article briefly describing their method of extracting them.\\nVery helpful blog post and code on road extraction from satellite images by Jeff Wen on a different dataset. We also took inspiration in structuring the training pipeline from this repo.\\nSpaceNet road extraction challenge.\\nTutorial on pixel-level land cover classification using semantic segmentation in CNTK on Azure.\\n\"},\n",
       " {'language': 'Python 69.0',\n",
       "  'readme': \"\\n\\n\\n\\nCosmonium is a 3D astronomy and space exploration program. With Cosmonium you can navigate in our solar system and discover all the planets and their moons. You can also visit the neighboring stars and discover the true size of our galaxy and the Universe.\\nCosmonium supports (or will support) the creation of fictional planets, stellar systems nebulaes, ... using procedural generation.\\nCosmonium also already supports some Celestia addons (though CMOD and CelX are not yet supported).\\nRequirements\\nCosmonium runs on Windows (Vista or above), Linux (CentOS 5, Ubuntu 14 or above) or macOS (mac0S 10.9 or above)\\nwith a graphic card supporting OpenGL 2.1 or better (OpenGL 4.5 is recommended) and at least 512MB of disk\\n(up to 4GB if the HD and UHD textures are installed).\\nInstallation\\nDownload the installer or package for your platform from the download page and see the [[Installation]] page\\nThe package contains only low resolution textures, see here to install extra HD and UHD textures.\\nScreenshots\\nSee in the Wiki some screenshots of the application with views of\\nSaturn,\\nJupiter,\\nMars,\\nthe Moon,\\nprocedural planets, ...\\n\\nLaunch\\nSimply starts cosmonium from your application menu or from the cosmonium folder. See also the installation page for more options.\\nUser interface\\nCosmonium user interface is still heavily based on Celestia, most of the command and keyboard shortcuts work the same.\\nGo to First steps to have an explanation of the basic command or see the Control page for an exhaustive list.\\nFull documentation\\nCosmonium is still in its infancy, but it is already usable to explore all the planets and the moons of our solar system, all the neighbor or visible stars and much more.\\nIt also support custom content and addons, either as Cosmonium or Celestia addons.\\nThe full documentation is available in the Wiki\\nBugs\\nIf you encounter any problem to install or run Cosmonium, please don't hesitate to fill a bug report in the issue tracker here on Github.\\nLicense\\nCosmonium is (C) 2018-2020 Laurent Deru.\\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.\\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details, which you should have received along with this program. If not, request a copy from: Free Software Foundation, Inc. 59 Temple Place - Suite 330 Boston, MA 02111-1307 USA.\\nCosmonium uses several third-party libraries which are subject to their own licenses,  see Third-Party.md for the complete list.\\nCosmonium data (textures, models, orbital elements,..) come from many sources. Their respective copyright holder, license and reference are available in the info panel of the displayed object and in the related yaml file.\\nPowered by\\n\\n\\n\"},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'SpaceNet Buildings Exploration\\nTransform SpaceNet geojson buidling labels data into raster masks.\\nDownload data via:\\naws s3api get-object --bucket spacenet-dataset \\\\\\n--key AOI_1_Rio/processedData/processedBuildingLabels.tar.gz \\\\\\n--request-payer requester processedBuildingLabels.tar.gz\\n\\nDownload spacenet utilities from:\\nhttps://github.com/SpaceNetChallenge/utilities/tree/master/python/spaceNet\\nFor further details, see:\\nhttps://medium.com/the-downlinq/getting-started-with-spacenet-data-827fd2ec9f53\\nExample outputs are included in the example_outputs directory\\n\\n'},\n",
       " {'language': 'Python 99.4',\n",
       "  'readme': 'pypet\\n\\n\\n\\n\\n\\n\\nThe new python parameter exploration toolkit:\\npypet manages exploration of the parameter space\\nof any numerical simulation in python,\\nthereby storing your data into HDF5 files for you.\\nMoreover, pypet offers a new data container which\\nlets you access all your parameters and results\\nfrom a single source. Data I/O of your simulations and\\nanalyses becomes a piece of cake!\\nRequirements\\nPython 3.6, 3.7 or 3.8 and\\n\\n\\ntables >=  3.5.0\\n\\n\\npandas >= 1.0.0\\n\\n\\nnumpy >= 1.16.0\\n\\n\\nscipy >= 1.3.0\\n\\n\\nHDF5 >= 1.10.0\\n\\n\\nThere are also some optional packages that you can but do not have to install.\\nIf you want to combine pypet with SCOOP you need\\n\\nscoop >= 0.7.1\\n\\nFor git integration you additionally need\\n\\nGitPython >= 3.1.3\\n\\nTo utilize the cap feature for multiprocessing you need\\n\\npsutil >= 5.7.0\\n\\nTo utilize the continuing of crashed trajectories you need\\n\\ndill >= 0.3.1\\n\\nAutomatic Sumatra records are supported for\\n\\nSumatra >= 0.7.1\\n\\nPython 2.7\\nThis release no longer supports Python 2.7.\\nIf you are still using Python 2.7, you need to\\nuse the pypet legacy version 0.3.0 (https://pypi.python.org/pypi/pypet/0.3.0).\\nWhat is pypet all about?\\nWhenever you do numerical simulations in science, you come across two major challenges.\\nFirst, you need some way to save your data. Secondly, you extensively explore the parameter space.\\nIn order to accomplish both you write some hacky I/O functionality to get it done the quick and\\ndirty way. This means storing stuff into text files, as MATLAB m-files,\\nor whatever comes in handy.\\nAfter a while and many simulations later, you want to look back at some of your very\\nfirst results. But because of unforeseen circumstances, you changed a lot of your code.\\nAs a consequence, you can no longer use your old data, but you need to write a hacky\\nconverter to format your previous results to your new needs.\\nThe more complexity you add to your simulations, the worse it gets, and you spend way\\ntoo much time formatting your data than doing science.\\nIndeed, this was a situation I was confronted with pretty soon at the beginning of my PhD.\\nSo this project was born. I wanted to tackle the I/O problems more generally and produce code\\nthat was not specific to my current simulations, but I could also use for future scientific\\nprojects right out of the box.\\nThe python parameter exploration toolkit (pypet) provides a framework to define parameters\\nthat you need to run your simulations. You can actively explore these by following a\\ntrajectory through the space spanned by the parameters.\\nAnd finally, you can get your results together and store everything appropriately to disk.\\nThe storage format of choice is HDF5 (http://www.hdfgroup.org/HDF5/) via PyTables\\n(http://www.pytables.org/).\\nPackage Organization\\nThis project encompasses these core modules:\\n\\n\\nThe pypet.environment module for handling the running of simulations\\n\\n\\nThe pypet.trajectory module for managing the parameters and results,\\nand providing a way to explore your parameter space. Somewhat related is also the\\npypet.naturalnaming module, that provides functionality to access and put data into\\nthe trajectory.\\n\\n\\nThe pypet.parameters module including containers for parameters and results\\n\\n\\nThe pypet.storageservice for saving your data to disk\\n\\n\\nInstall\\nIf you don\\'t have all prerequisites (numpy, scipy, tables, pandas) install them first.\\nThese are standard python packages, so chances are high that they are already installed.\\nBy the way, in case you use the python package manager pip\\nyou can list all installed packages with pip freeze.\\nNext, simply install pypet via pip install pypet\\nOr\\nThe package release can also be found on https://pypi.python.org/pypi/pypet. Download, unpack\\nand python setup.py install it.\\nOr\\nIn case you use Windows, you have to download the tar file from https://pypi.python.org/pypi/pypet\\nand unzip it. Next, open a windows terminal\\nand navigate to your unpacked pypet files to the folder containing the setup.py file.\\nAs above run from the terminal python setup.py install.\\nDocumentation and Support\\nDocumentation can be found on http://pypet.readthedocs.org/.\\nThere is a Google Groups mailing list for support: https://groups.google.com/forum/?hl=de#!forum/pypet\\nIf you have any further questions feel free to contact me at robert.meyer (at) ni.tu-berlin.de.\\nMain Features\\n\\n\\nNovel tree container Trajectory, for handling and managing of\\nparameters and results of numerical simulations\\n\\n\\nGroup your parameters and results into meaningful categories\\n\\n\\nAccess data via natural naming, e.g. traj.parameters.traffic.ncars\\n\\n\\nAutomatic storage of simulation data into HDF5 files via PyTables\\n\\n\\nSupport for many different data formats\\n\\n\\npython native data types: bool, int, long, float, str, complex\\n\\n\\nlist, tuple, dict\\n\\n\\nNumpy arrays and matrices\\n\\n\\nScipy sparse matrices\\n\\n\\npandas DataFrames (http://pandas.pydata.org/)\\n\\n\\nBRIAN2 quantities and monitors (http://briansimulator.org/)\\n\\n\\n\\n\\nEasily extendable to other data formats!\\n\\n\\nExploration of the parameter space of your simulations\\n\\n\\nMerging of trajectories residing in the same space\\n\\n\\nSupport for multiprocessing, pypet can run your simulations in parallel\\n\\n\\nAnalyse your data on-the-fly during multiprocessing\\n\\n\\nAdaptively explore tha parameter space combining pypet with optimization\\ntools like the evolutionary algorithms framework DEAP (http://deap.readthedocs.org/en/)\\n\\n\\nDynamic Loading, load only the parts of your data you currently need\\n\\n\\nResume a crashed or halted simulation\\n\\n\\nAnnotate your parameters, results and groups\\n\\n\\nGit Integration, let pypet make automatic commits of your codebase\\n\\n\\nSumatra Integration, let pypet add your simulations to the electronic lab notebook tool\\nSumatra (http://neuralensemble.org/sumatra/)\\n\\n\\npypet can be used on computing clusters or multiple servers at once if it is combined with\\nSCOOP (http://scoop.readthedocs.org/)\\n\\n\\nQuick Working Example\\nThe best way to show how stuff works is by giving examples. I will start right away with a\\nvery simple code snippet.\\nWell, what we have in mind is some sort of numerical simulation. For now we will keep it simple,\\nlet\\'s say we need to simulate the multiplication of 2 values, i.e. z=x*y.\\nWe have two objectives, a) we want to store results of this simulation z and\\nb) we want to explore the parameter space and try different values of x and y.\\nLet\\'s take a look at the snippet at once:\\nfrom pypet import Environment, cartesian_product\\n\\ndef multiply(traj):\\n    \"\"\"Example of a sophisticated simulation that involves multiplying two values.\\n\\n    :param traj:\\n\\n        Trajectory containing the parameters in a particular combination,\\n        it also serves as a container for results.\\n\\n    \"\"\"\\n    z=traj.x * traj.y\\n    traj.f_add_result(\\'z\\',z, comment=\\'I am the product of two values!\\')\\n\\n# Create an environment that handles running our simulation\\nenv = Environment(trajectory=\\'Multiplication\\',filename=\\'./HDF/example_01.hdf5\\',\\n                    file_title=\\'Example_01\\',\\n                    comment = \\'I am the first example!\\')\\n\\n# Get the trajectory from the environment\\ntraj = env.trajectory\\n\\n# Add both parameters\\ntraj.f_add_parameter(\\'x\\', 1.0, comment=\\'Im the first dimension!\\')\\ntraj.f_add_parameter(\\'y\\', 1.0, comment=\\'Im the second dimension!\\')\\n\\n# Explore the parameters with a cartesian product\\ntraj.f_explore(cartesian_product({\\'x\\':[1.0,2.0,3.0,4.0], \\'y\\':[6.0,7.0,8.0]}))\\n\\n# Run the simulation with all parameter combinations\\nenv.run(multiply)\\nAnd now let\\'s go through it one by one. At first we have a job to do, that is multiplying two\\nvalues:\\ndef multiply(traj):\\n    \"\"\"Example of a sophisticated simulation that involves multiplying two values.\\n\\n    :param traj:\\n\\n        Trajectory containing the parameters in a particular combination,\\n        it also serves as a container for results.\\n\\n    \"\"\"\\n    z=traj.x * traj.y\\n    traj.f_add_result(\\'z\\',z, comment=\\'I am the product of two values!\\')\\nThis is our simulation function multiply. The function uses a so called trajectory\\ncontainer which manages our parameters. We can access the parameters simply by natural naming,\\nas seen above via traj.x and traj.y. The value of z is simply added as a result\\nto the traj object.\\nAfter the definition of the job that we want to simulate, we create an environment which\\nwill run the simulation.\\n# Create an environment that handles running our simulation\\nenv = Environment(trajectory=\\'Multiplication\\',filename=\\'./HDF/example_01.hdf5\\',\\n                    file_title=\\'Example_01\\',\\n                    comment = \\'I am the first example!\\')\\nThe environment uses some parameters here, that is the name of the new trajectory, a filename to\\nstore the trajectory into, the title of the file, and a comment that is added to the trajectory.\\nThere are more options available like the number of processors for multiprocessing or\\nhow verbose the final HDF5 file is supposed to be.\\nCheck out the documentation (http://pypet.readthedocs.org/) if you want to know more.\\nThe environment will automatically generate a trajectory for us which we can access via:\\n# Get the trajectory from the environment\\ntraj = env.trajectory\\nNow we need to populate our trajectory with our parameters. They are added with the default values\\nof x=y=1.0.\\n# Add both parameters\\ntraj.f_add_parameter(\\'x\\', 1.0, comment=\\'Im the first dimension!\\')\\ntraj.f_add_parameter(\\'y\\', 1.0, comment=\\'Im the second dimension!\\')\\nWell, calculating 1.0 * 1.0 is quite boring, we want to figure out more products, that is\\nthe results of the cartesian product set {1.0,2.0,3.0,4.0} x {6.0,7.0,8.0}.\\nTherefore, we use f_explore in combination with the builder function\\ncartesian_product.\\n# Explore the parameters with a cartesian product\\ntraj.f_explore(cartesian_product({\\'x\\':[1.0,2.0,3.0,4.0], \\'y\\':[6.0,7.0,8.0]}))\\nFinally, we need to tell the environment to run our job multiply with all parameter\\ncombinations.\\n# Run the simulation with all parameter combinations\\nenv.run(multiply)\\nAnd that\\'s it. The environment will evoke the function multiply now 12 times with\\nall parameter combinations. Every time it will pass a traj container with another one of these\\n12 combinations of different x and y values to calculate the value of z.\\nMoreover, the environment and the storage service will have taken care about the storage\\nof our trajectory  - including the results we have computed - into an HDF5 file.\\nSo have fun using this tool!\\nCheers,\\nRobert\\nMiscellaneous\\nAcknowledgements\\n\\n\\nThanks to Robert Pröpper and Philipp Meier for answering all my Python questions\\nYou might want to check out their SpykeViewer (https://github.com/rproepp/spykeviewer)\\ntool for visualization of MEA recordings and NEO (http://pythonhosted.org/neo) data\\n\\n\\nThanks to Owen Mackwood for his SNEP toolbox which provided the initial ideas\\nfor this project\\n\\n\\nThanks to Mehmet Nevvaf Timur for his work on the SCOOP integration and the \\'NETQUEUE\\' feature\\n\\n\\nThanks to Henri Bunting for his work on the BRIAN2 subpackage\\n\\n\\nThanks to the BCCN Berlin (http://www.bccn-berlin.de),\\nthe Research Training Group GRK 1589/1, and the\\nNeural Information Processing Group ( http://www.ni.tu-berlin.de) for support\\n\\n\\nTests\\nTests can be found in pypet/tests.\\nNote that they involve heavy file I/O and you need privileges\\nto write files to a temporary folder.\\nThe tests suite will make use of the tempfile.gettempdir() function to\\ncreate such a temporary folder.\\nEach test module can be run individually, for instance $ python trajectory_test.py.\\nYou can run all tests with $ python all_tests.py which can also be found under\\npypet/tests.\\nYou can pass additional arguments as $ python all_tests.py -k --folder=myfolder/\\nwith -k to keep the HDF5 and log files created by the tests\\n(if you want to inspect them, otherwise they will be deleted after the completed tests),\\nand --folder= to specify a folder where to store the HDF5 files instead of the temporary one.\\nIf the folder cannot be created, the program defaults to tempfile.gettempdir().\\nRunning all tests can take up to 20 minutes. The test suite encompasses more than 1000 tests\\nand has a code coverage of about 90%!\\nMoreover, pypet is constantly tested with Python 3.7 and 3.8 for Linux using\\nTravis-CI. Testing for Windows platforms is performed via Appveyor.\\nThe source code is available at https://github.com/SmokinCaterpillar/pypet/.\\nLicense\\nBSD, please read LICENSE file.\\nLegal Notice\\npypet was created by Robert Meyer at the Neural Information Processing Group (TU Berlin),\\nsupported by the Research Training Group GRK 1589/1.\\nContact\\nrobert.meyer (at) alcemy.tech\\nalcemy GmbH\\nChoriner Str. 83\\n10119 Berlin, Germany\\n'},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"ParamNoise\\nA comparison of parameter space noise methods for exploration in deep reinforcement learning\\nNOTE: This project is not maintained.  Reach out if you'd like to help reboot it.\\nLinks to papers\\nParameter Space Noise for Exploration : https://openreview.net/forum?id=ByBAl2eAZ&noteId=ByBAl2eAZ\\nNoisy Networks For Exploration : https://openreview.net/forum?id=rywHCPkAW&noteId=rywHCPkAW\\nResources\\n\\nOpenAI Baselines for useful Atari wrappers and replay buffer\\nbearpaw's pytorch-classification repo for utilities, logging, training framework\\nikostrikov's PPO implementation for other utilities and PPO guidance\\npytorch-rl for DQN help\\nPyTorch DQN tutorial for PyTorch tricks\\nOriginal DQN paper since both papers use the original hyperparameters, for the most part\\n\\nTODOs\\n\\nImplement PPO and MuJoCo env handling\\nRevisit logging; make sure everything is there to reproduce results in papers\\nImplement plotting (matplotlib is in Logger object; maybe try out visdom)\\nMore tests (figure out different combinations of arguments to ensure everything's interacting well)\\nBegin experiments (start with Mujoco; it's cheaper)\\n\\nAtari Games to Test\\n\\nAlien: Adaptive helps a lot, learned shows no improvement\\nEnduro: Both methods improve\\nSeaquest: Adaptive helps, learned performs worse than baseline\\nSpace Invaders: Adaptive helps, but learned helps more\\nWizardOfWor: Adaptive worse than baseline, but learned helps a lot\\n\\nMuJoCo enviroments to test\\n\\nHopper\\nWalker2d\\nHalfCheetah\\nSparse versions of these? (from rllab)\\n\\n\"},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': \"PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\\nCode accompanying CVPR'20 paper of the same title. Paper link: https://arxiv.org/abs/2003.03808\\n\\n\\n\\nTable of Contents\\n\\nPULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\\nTable of Contents\\n\\nWhat does it do?\\nHow do I use it?\\nUsage\\n\\nPrereqs\\nData\\nApplying PULSE\\n\\n\\n\\n\\n\\nWhat does it do?\\nGiven a low-resolution input image, PULSE searches the outputs of a generative model (here, StyleGAN) for high-resolution images that are perceptually realistic and downscale correctly.\\n\\nHow do I use it?\\nThe easiest way to apply PULSE to your own images is with our interactive demo, found at https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true.\\nIf you want to try using this codebase, continue on.\\nUsage\\nThe main file of interest for applying PULSE is run.py. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.\\nPrereqs\\nYou will need to install cmake first (required for dlib, which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux. For the full set of required Python packages, create a Conda environment from the provided YAML, e.g.\\nconda create -f pulse.yml\\n\\nFinally, you will need an internet connection the first time you run the code as it will automatically download the relevant pretrained model from Google Drive (if it has already been downloaded, it will use the local copy).\\nData\\nBy default, input data for run.py should be placed in ./input/ (though this can be modified). However, this assumes faces have already been aligned and downscaled. If you have data that is not already in this form, place it in realpics and run align_face.py which will automatically do this for you. (Again, all directories can be changed by command line arguments if more convenient.) You will at this stage pic a downscaling factor.\\nNote that if your data begins at a low resolution already, downscaling it further will retain very little information. In this case, you may wish to bicubically upsample (usually, to 1024x1024) and allow align_face.py to downscale for you.\\nThe dataset we evaluated on was CelebA-HQ, but in our experience PULSE works with any picture of a realistic face.\\nApplying PULSE\\nOnce your data is appropriately formatted, all you need to do is\\npython run.py\\n\\nEnjoy!\\nContact both Sachit Menon and Alex Damian (sachit.menon@duke.edu and alexandru.damian@duke.edu) for questions regarding this work.\\n\"},\n",
       " {'language': 'Python 98.8',\n",
       "  'readme': \"BOOM\\nAn easy-to-use multi-process Configuration Space Exploration pipeline framework.\\nFeatures\\n\\nEasy to use: you only need to write your configuration file and modules, we will handle everything else!\\nFlexible: we offer common modules for QA pipelines, and it is very easy to develop your own modules.\\nParameter tuning: automatically run on all possible parameter combinations and saves the results.\\nHigh efficiency: we use multiple processes to run the whole pipeline.\\nCompatibility: only compatible with Python 2.\\n\\nInstallation\\nFirst, install RabbitMQ and pip.\\nThen, clone the repository, run\\nmake install\\n\\nIt will install dependencies using pip, and install this framework to your PATH.\\nRun\\nWe offer a command-line executable program boom.\\nWhen executing, it will load conf.yaml from the current directory.\\nYou can also specify the configuration file to use by adding option -conf PATH_TO_CONF_FILE.\\nFor more options, run boom --help.\\nDocker image\\nTo build the docker image, run make docker.\\nTutorials\\nPlease check out the two tutorials in examples folder.\\nQuick tutorails\\nConfiguation Space Exploration\\nA QA pipeline may be consisted with several modules, each module may have some parameters.\\nEach combination of parameters corresponds to a path in the parameter space.\\nBOOM exhaustively run the pipeline on every possible parameter combinations, saves all intermediate results and final results.\\nThe following figure shows a pipeline which has several modules.\\nThe execution path is a tree which every level corresponds to a module, and each node stands for a different parameter value.\\nThe leaf nodes are metric values.\\nRed arrows belongs to the best parameter combination.\\n\\nComponents\\nThere are two main components to a BOOM pipeline: the modules and the configuration file.\\nEach pipeline can have an arbitrary number of modules (n >= 1) but there is only one configuration file that defines the pipeline.\\nBOOM works by instantiating each module and passing data along from one module to the next, allowing each to process and transform the data along the way.\\nModules\\nThe building block of a BOOM pipeline is the Module class. Each module in the pipeline takes in the data in the exact format returned by the previous module and return the data for the next module in its process() method. At a minimum, each user-defined module must subclass Module and implement the __init__() and process() methods.\\nConfiguration Files\\nThe configuration file defines the structure and composition of the pipeline and allows the user to define a parameter space for the pipeline to be executed over. The configuration is written is a YAML file and contains two core components: pipeline, where pipeline metadata is declared, and modules, where the pipeline composition is defined.\\nFollowing is the pipeline section of the toy example's configuration file:\\npipeline:\\n    name: toy_pipeline\\n    rabbitmq_host: 127.0.0.1\\n    clean_up: false\\n    use_mongodb: false\\n    mongodb_host: 127.0.0.1\\n\\nUnder the pipeline key, there are 5 key-value pairs that need to be declared:\\nname\\nrabbitmq_host\\nclean_up\\nuse_mongodb\\nmongodb_host\\n\\nname allows the user to declare a name for the pipeline. rabbitmq_host and mongodb_host are simply the host addresses for RabbitMQ and MongoDB, respectively. clean_up is a boolean value that will delete intermediate output files if declared true. use_mongodb is a boolean value that will write data to MongoDB instead of files if declared true.\\nFollowing is the modules section of the toy example's configuration file:\\npipeline: # Pipeline section, defines pipeline's properties\\n    mode: docker # Running mode, local or docker, default local\\n    name: toy_pipeline # Name of the pipeline\\n    rabbitmq_host: 127.0.0.1 # RabbitMQ's host uri\\n    clean_up: false # Whether the pipeline cleans up after finished running, true or false\\n    use_mongodb: true # Whether to use MongoDB, true or false, default false\\n    mongodb_host: 127.0.0.1 # MongoDB's host\\n\\nmodules:\\n    -   name: module_1 # Name of the module\\n        type: Sample # Type of the module\\n        input_file: data.json # Input file's uri\\n        output_module: module_2 # The following module's name\\n        instances: 1 # Number of instances of this module\\n        params:\\n            -   name: p1\\n                type: collection # Type of the param, int, float or collection\\n                values: # Possible vaules for collection param\\n                    - val1\\n                    - val2\\n                    - val3\\n\\n            -   name: p2\\n                type: int\\n                start: 0\\n                end: 20\\n                step_size: 20\\n\\n    -   name: module_2\\n        type: Sample\\n        output_module: module_3\\n        instances: 1\\n        params:\\n            -   name: p\\n                type: float\\n                start: 0.0\\n                end: 80.0\\n                step_size: 40.0\\n        \\n    -   name: module_3\\n        type: Sample\\n        output_module: module_4\\n        instances: 1\\n\\n    -   name: module_4\\n        type: CSVWriter\\n        output_file: results.csv \\n        instances: 1\\n\\nThe modules section of the configuration file should contain a list of modules. Each module consists of a set of key-value pairs which  must include name, type, input_file (first module only), output_module (or output_file for the last module), instances, and (optionally) params. params is a list of parameters, defined by a name, type (float, int, or collection). If the parameter is a float or int, the param should also contain start, end, and step_size. If the parameter is of type collection, then it should contain a values list.\\nAPI documentation\\nYou can find the API documentation here.\\nWarning\\nThis framework is still under heavy development,\\nplease be careful.\\n\"},\n",
       " {'language': 'Python 97.6',\n",
       "  'readme': \"atiamML\\nChemla - Latent representations for real-time synthesis space exploration\\n\\n\\ncode/ contains project's code along with simple scripts that demonstrate the use of the developped methodologies - see code/README.md\\nreport/ contains the report in PDF format along with the LaTeX source and eventual figures - see report/README.md\\ntoy/ contains toy datasets, along with the procedural scripts to generate it - see toy/README.md\\n\\n\"},\n",
       " {'language': 'Python 100.0',\n",
       "  'readme': 'Safe-Explorer\\nIntroduction\\nThis repository contains Pytorch implementation of paper \"Safe Exploration in Continuous Action Spaces\" [Dalal et al.] along with \"Continuous Control With Deep Reinforcement\\nLearning\" [Lillicrap et al.]. Dalal et al. present a closed form analytically optimal solution to ensure safety in continuous action space. The proposed \"safety layer\",\\nmakes the smallest possible perturbation to the original action such that safety constraints are satisfied.\\n\\nDalal et al. also propose two new domains BallND and Spaceship which are governed by first and second order dynamics respectively. In Spaceship domain agent receives a reward only on task completion, while BallND has continuous reward based distance from the target. Implementation of both of these tasks extend OpenAI gym\\'s environment interface (gym.Env).\\nSetup\\nThe code requires Python 3.6+ and is tested with torch 1.1.0. To install dependencies run,\\npip install -r requirements.txt\\nTraining\\nTo obtain list of parameters and their default values run,\\npython -m safe_explorer.main --help\\nTrain the model by simply running,\\nBallND\\npython -m safe_explorer.main --main_trainer_task ballnd\\nSpaceship\\npython -m safe_explorer.main --main_trainer_task spaceship\\nMonitor training with Tensorboard,\\ntensorboard --logdir=runs\\nResults\\nTo be updated.\\nAcknowledgement\\nSome modifications in DDPG implementation are based OpenAI Spinning Up implement.\\nReferences\\n\\n\\nLillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971 (2015).\\n\\n\\nDalal, Gal, et al. \"Safe exploration in continuous action spaces.\" arXiv preprint arXiv:1801.08757 (2018).\\n\\n\\n'},\n",
       " {'language': 'Java 99.8',\n",
       "  'readme': 'About\\nCyberBiology - life simulator on computer\\nBuild\\nLinux\\nBuild artifact:\\nmake\\nYou can run after build:\\njava ./build/world.jar\\n\\n'},\n",
       " {'language': 'Java 98.1',\n",
       "  'readme': 'Caleydo - Visualization for Molecular Biology\\n\\nCaleydo is a visualization framework for molecular biology data. It is targeted at analyzing multiple heterogeneous but related tabular datasets (e.g.,  mRNA expression, copy number status and clinical variables), stratifications or clusters in these datasets and their relationships to biological pathways.\\nFor user documentation please refer to the Caleydo Help. For general information and downloads based on binaries please use the Caleydo Website. This guide assumes that you want to install Caleydo from source.\\nInstallation\\nCaleydo uses Java, OpenGL and the Eclipse Rich Client Platform (RCP). Things you need to install before being able to run Caleydo:\\n\\nEclipse Kepler for RCP and RAP Developers, which you can get from the eclipse download page. Other Eclipse versions won\\'t work.\\nInstall EGit in Eclipse using software updates.\\nJava SDK >= 1.7\\n\\nTo install Caleydo use EGit within Eclipse and clone the repository. Each directory in the caleydo-dev folder corresponds to an Eclipse project. Here is a good tutorial on how to import Eclipse projects from git.\\nIf you want to use ssh (instead of https) for communicating with github out of eclipse follow these instructions.\\nYou will have to generate a new RSA key and save it to you ~/.ssh folder. Remeber to set a passphrase for you key. This will result in a file ida_rsa and ida_rsa.pub turning up in your ssh folder.\\nSave your public rsa key with your eclipse account folder.\\nWhen cloning the repository follow the above tutorial. Don\\'t change the username \"git\" to your username!\\nTeam\\nCaleydo is an academic project currently developed by members of\\n\\nInstitute for Computer Graphics and Vision at Graz University of Technology, Austria\\nInstitute of Computer Graphics at Johannes Kepler University Linz, Austria\\nPfister Lab at the School of Engineering and Applied Sciences, Harvard University, Cambridge, USA\\nPark Lab at Harvard Medical School, Boston, USA\\n\\nAcknowledgements\\nCaleydo makes use of a range of open source tools, bioinformatics resources and pre-packages several datasets, which we gratefully acknowledge here.\\nSoftware Libraries\\n\\nBubble Sets - A Java implementation of the visualization technique.\\nCDK - The Chemistry Development Kit.\\nJGrahT - A graph library.\\nJogl - Java bindings for OpenGL.\\nPathVisio - loading and parsing WikiPathways.\\nWordHoard - statistical utilities.\\n\\nBioinformatics Resources\\n\\nDavid Bioinformatics Resources - Gene ID mapping.\\nKEGG - Pathways.\\nWikiPathways - Pathways.\\n\\nDatasets\\n\\nCCLE - BROAD Institute Cancer Cell Line Encyclopedia.\\nTCGA - The Cancer Genome Atlas.\\n\\nResources\\n\\nGitHub - Hosting our source code.\\nWebStorm - Free developer licenses for our web projects from JetBrains.\\n\\n'},\n",
       " {'language': 'Java 78.4',\n",
       "  'readme': '20n/act: An open source platform for bioengineering\\n20n/act is the data aggregation and prediction system for bioengineering. For a target molecule, 20n/act predicts DNA insertions into cells (usually a microbe such as E. coli or S. cerevisiae) that modify the cell. These modified cells make the target molecule by fermentation from sugar. We call these \"target molecules/chemicals\" the bioreachables. The system predicted/invented the first bio-route to Acetaminophen/Tylenol/APAP. Read more on our blog post. The technical details of the APAP work can be found in patents applications on coli and yeast fermentation.\\nGetting started\\nLive preview\\nSee predicted DNA for 11 sample molecules at Bioreachables Preview (Login:Pass = public:preview). Due to limitations we can only make a preview version available. If you\\'d like the full version please contact us.\\nBuilding the project\\nCheckout the repo. Follow instructions to run to create the database and prediction corpus. If you\\'d rather get a pre-packaged DB without creating it yourself please contact us. The codebase is public to further the state-of-the-art in automating biological engineering/synthetic biology. Some modules are specific to microbes, but most of the predictive stack deals with host-agnostic enzymatic biochemistry.\\nComponents of 20n/act\\nPredictor stack\\nAnswers \"what DNA do I insert if I want to make my chemical?\"\\n\\n\\n\\n\\nModule\\nFunction\\nCode\\n\\n\\n\\n\\n1\\nInstaller\\nIntegrates heterogeneous raw data\\nCode:com.act.reachables.initdb Run:Instructions\\n\\n\\n2\\nReaction operator (RO) inference\\nMines rules of enzymatic catalysis\\nCode:biointerpretation module\\n\\n\\n2\\nStructure Activity Relationship (SAR) inference\\nMines substrate specificities\\nCode:biointerpretation module\\n\\n\\n3\\nBiointerpretation\\nMechanistic validation of enzymatic transforms (using ROs)\\nCode:com.act.biointerpretation.BiointerpretationDriver  Run:Instructions\\n\\n\\n4\\nReachables computation\\nExhaustively enumerates all biosynthesizable chemicals\\nCode:com.act.reachables.reachablesCode:com.act.reachables.postprocess_reachablesRun:Instructions\\n\\n\\n5\\nCascades computation\\nExhaustively enumerates all enzymatic routes from metabolic natives to bioreachable target\\nCode:com.act.reachables.cascadesRun:Instructions\\n\\n\\n6\\nDNA designer\\nComputes protein & DNA design (coli specific) for each non-natural enzymatic path\\nCode:org.twentyn.proteintodna.ProteinToDNADriverRun:Instructions\\n\\n\\n7\\nApplication miner\\nMines chemical applications using web searches [Bing]\\nCode:act.installer.bing.BingSearcherRun:Instructions\\n\\n\\n8\\nEnzymatic biochemistry NLP\\nText -> Chemical tokens -> Biologically feasible reactions using ROs\\nCode:act.shared.TextToRxnsFrontend:TextToRxnsUI\\n\\n\\n9\\nPatent search\\nChemical -> Patents\\nCode:act.installer.reachablesexplorer.PatentFinderRun:Instructions\\n\\n\\n10\\nBioreachables wiki\\nAggregates reachables, cascades, use cases, protein and DNA designs into a user friendly wiki interface\\nDocumentation\\n\\n\\n\\n  \\nAnalytics\\nAnswers \"Is my bio-engineered cell doing what I want it to?\"\\n\\n\\n\\n\\nModule\\nFunction\\nCode\\n\\n\\n\\n\\n1\\nLCMS: Untargeted metabolomics\\nDeep-learnt signal processing to identify all chemical [side]effects of DNA engineering on cell\\nCode:DeepLearningLcmsPeakCode:com.act.lcms.UntargetedMetabolomics\\n\\n\\n2\\nLCMS: Comparative visualization\\nVisualizing traces side-by-side from untargeted evaluation of over and underexpressed peaks\\nDoc:LCMSDataVisualisation\\n\\n\\n\\n  \\nUnit economics of bioproduction\\nAnswers \"Can I use bio-production to make this chemical at scale?\"\\n\\n\\n\\n\\nModule\\nFunction\\nCode\\n\\n\\n\\n\\n1\\nCost model: Manufacturing unit economics for large scale production\\nIt backcalculates cell efficiency (yield, titers, productivity) objectives based on given COGS ($ per ton) of target chemical. From cell efficiency objectives it guesstimates the R&D investment (money and time) and ROI expectations\\nCode:act.installer.bing.CostModelCode (viz server):costModelUISource model:XLS\\n\\n\\n\\nLicense and Contributing\\nCode licensed under the GNU General Public License v3.0.\\nIf an alternative license is desired, please contact 20n.\\nOriginal Authors\\n\\nSaurabh Srivastava\\nJ. Christopher Anderson\\nMark T. Daly\\nMichael Lampe\\nThomas Legrand\\nVijay Ramakrishnan\\nGil Goldshlager\\nNishant Kakar\\n\\n'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': \"This is the official University of Minnesota Populus Git Repository.\\nAbout\\nPopulus is a package of educational software allowing students to manipulate ecological and evolutionary models, producing graphical representations of their dynamics.  It also contains an integrated help system discussing each of the models.\\nPlease go to https://www.cbs.umn.edu/populus/ for more information about Populus.\\nCopyright\\nDon Alstad \\nDepartment of Ecology, Evolution & Behavior \\nUniversity of Minnesota \\n1987 Upper Buford Circle \\nSt. Paul, MN 55108-6097\\nHow to run\\nInstallers will be available on the main Populus page, https://www.cbs.umn.edu/populus/.\\nTo build and run from source, use the gradle wrapper in the top directory:\\n$ ./gradlew build\\n$ ./gradlew run\\nIf you are using JDK 14 or later, you can also create installer/packager:\\n$ ./gradlew jpackage\\nThe images and installers will be in the build directory.\\nNote that for Windows, you'll need to run gradlew.bat instead, and need to install WiX to package.\\nFeedback\\nIf you find bugs, irregularities, places for improvement, or have other comments, please email populus@umn.edu.\\nLanguage Support\\nSpanish translations of some of the more basic models are provided. We would be interested in corresponding with people who would be able to help with other translations.  If interested, please email populus@umn.edu.\\nProgramming Credits\\nJava versions: Amos Anderson, Lars Roe, Sharareh Noorbaloochi \\nDOS versions: Chris Bratteli\\nLicense\\n\\n\"},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': \"\\nlibSBOLj provides the core Java interfaces and their implementation for\\nthe Synthetic Biology Open Language (SBOL). The library provides an API to\\nwork with SBOL objects, the functionality to read and write SBOL documents as XML/RDF files, and a validator to check the\\ncorrectness of SBOL models.\\nUsing the libSBOLj library\\nIn a Maven project:\\nIn a Maven project that utilizes the libSBOLj library, add a dependency in the Maven project's pom.xml file.\\n<dependency>\\n\\t<groupId>org.sbolstandard</groupId>\\n\\t<artifactId>libSBOLj</artifactId>\\n\\t<version>2.4.0</version>\\n</dependency>\\n\\nIn a non-Maven project:\\nDownload libSBOLj-<version>-withDependencies.jar from the latest github release.\\nRunning in the command line:\\nlibSBOLj comes with a command-line interface (CLI) that can be used to validate SBOL files. You can execute\\nlibSBOLj-<version>-withDependencies.jar to validate and convert files as follows.\\njava -jar libSBOLj-<version>-withDependencies.jar <inputFile> -l <language>\\n\\nIf validation/conversion is successful, the program will print the contents of the input file in the specified language (SBOL1, SBOL2, GenBank, and FASTA). You can also output the result to a file.\\njava -jar libSBOLj-<version>-withDependencies.jar <inputFile> -l <language> -o <outputFile>\\n\\nOne can also provide a URI using the -s flag for a TopLevel object, and only that object and all its dependencies will be output.\\nIf validation fails with an error, there will be a message printed about the validation error.  The -f flag can be used to indicate that validation should continue after the first error, while the -d flag will provide a detailed error trace on a validation error.\\nIn addition to checking all required validation rules, it will also check if the URIs are compliant and whether the SBOL document is complete (i.e., all referenced objects are contained within the file).  These validation checks can be turned off with the -n and -i flags, respectively.  It is also possible to turn-on best practices checking using the -b flag.\\nWhen the input file is being converted into SBOL 2.0, the conversion should be provided a default URI prefix.  It can also be provided a default version, if desired.  Finally, the -t flag will insert the type of top level objects into the URI during conversion, if desired.\\njava -jar libSBOLj-<version>-withDependencies.jar <inFile> -o <outFile> -p <URIprefix> -v <version>\\n\\nFinally, it can be used to compare the equality of the contents of two SBOL files using the command below:\\njava -jar libSBOLj-<version>-withDependencies.jar <firstSBOLFile> -e <secondSBOLFile>\\n\\nUsing the latest libSBOLj SNAPSHOT\\nGetting the libSBOLj source\\n\\nCreate a GitHub account.\\nSetup Git on your machine.\\nClone the libSBOLj GitHub repository to your machine.\\nRetrieve the SBOLTestSuite Submodule using the instructions below.\\n\\nRetrieving SBOLTestSuite Submodule\\ngit submodule update --init --recursive\\n\\nCompiling and Packaging libSBOLj\\n\\n\\nSetup Apache Maven. A tutorial on using Apache Maven is provided here.\\n\\n\\nIn the command line, change to the libSBOLj directory (e.g. cd /path/to/libSBOLj) and execute the following command\\n\\n\\nmvn package\\n\\nThis will compile the libSBOLj source files, package the compiled source into a libSBOLj JAR file (libSBOLj-<version>-SNAPSHOT-withDependencies.jar), and place the JAR file into the core2/target sub-directory.\\n\"},\n",
       " {'language': 'Java 98.1',\n",
       "  'readme': \"JVARKIT\\nJava utilities for Bioinformatics\\n\\n\\nDocumentation\\nDocumentation is available at: http://lindenb.github.io/jvarkit/\\nCompilation\\nFebruary 2019. I'm moving to java OpenJdk. See the [[NEWS]] file.\\nEach tool is compiled independently of each other.\\nSee the documentation for each tool at http://lindenb.github.io/jvarkit/. All the pages should include a paragraph titled 'Download and Compile'\\nYou shouldn't try to compile all the tools because some of them are not tested, deprecated, or just too specific to my lab.\\nAuthor\\nPierre Lindenbaum PhD\\nhttp://plindenbaum.blogspot.com\\n@yokofakun\\n\"},\n",
       " {'language': 'Java 99.0',\n",
       "  'readme': 'This repo contains code samples, data and problem solutions for the\\nbook \"Computational Biology with Java\".\\nSee petergarst.com for more information.\\n'},\n",
       " {'language': 'Java 100.0',\n",
       "  'readme': \"\\nBioLær er en native Android quiz-applikation med fokus på molekylærbiologi. I denne første udgave af app'en er det laboratoriumanalysen ELISA, der kan quizzes i. Dog er systemet konstrueret således, at det let senere kan udvides til andre fagområder. Applikationen er udviklet i forbindelse med førsteårsprojektet på datamatikeruddannelsen. I projektets Wiki-sektion kan du læse meget mere om tilblivelsen, den tilhørende rapport, rettigheder mv.\\nEnglish description\\n\\nThis project includes a native Android Application about laboratory analysis ELISA.\\nThe idea was to make a biology application to help students understand the term ELISA.\\nDeveloped in Java and XML with Android Studio in the context of a school project.\\n\\nAuthors\\n\\nSebastian Ougter Olsen\\nMathias Elholm Blomgaard\\nThomas Christensen\\nDaniel Lyck\\nMichael Trans\\n\\n\\n\"},\n",
       " {'language': 'Java 100.0', 'readme': 'BiologyAnalyze\\n蛋白质网络、基因测序相关数据分析\\n'},\n",
       " {'language': 'Java 97.5',\n",
       "  'readme': '\\n\\n\\n\\niBioSim is a computer-aided design (CAD) tool aimed for the modeling, analysis, and design of genetic circuits.\\nWhile iBioSim primarily targets models of genetic circuits, models representing metabolic networks, cell-signaling pathways,\\nand other biological and chemical systems can also be analyzed.\\niBioSim also includes modeling and visualization support for multi-cellular and spatial models as well.\\nIt is capable of importing and exporting models specified using the Systems Biology Markup Language (SBML).\\nIt can import all levels and versions of SBML and is able to export Level 3 Version 1.\\nIt supports all core SBML modeling constructs except some types of fast reactions, and also has support for the\\nhierarchical model composition, layout, flux balance constraints, and arrays packages.\\nIt has also been tested successfully on the stochastic benchmark suite and the curated models in the BioModels database.\\niBioSim also supports the Synthetic Biology Open Language (SBOL), an emerging standard for information exchange in synthetic\\nbiology.\\nWebsite: iBioSim\\nVideo Demo: Tools Workflow\\nContact: Chris Myers (@cjmyers) myers@ece.utah.edu\\nContributor(s): Nathan Barker, Pedro Fontanarrosa, Scott Glass, Kevin Jones, Hiroyuki Kuwahara, Curtis Madsen, Nam Nguyen, Tramy Nguyen, Tyler Patterson, Nicholas Roehner, Jason Stevens, Leandro Watanabe, Michael Zhang, Zhen Zhang, and Zach Zundel.\\nActive Developer(s): Pedro Fontanarrosa, Chris Myers, Tramy Nguyen, Leandro Watanabe.\\nRunning iBioSim\\n\\nDownload the iBioSim tool from the release page here:\\nAfter downloading the tool, run the corresponding start-up script:\\n\\nWindows: iBioSim.bat\\nMac OS X: iBioSim.mac64\\nLinux: iBioSim.linux64\\n\\n\\n\\n[Optional] Installing iBioSim for Development\\nPre-installation Requirements\\n\\nCreate a GitHub account.\\nSetup Git on your machine.\\nInstall Maven plugin on your machine.\\nInstall Eclipse IDE  for Java.\\nInstall libSBML for validation and flattening.\\nClone the iBioSim GitHub repository to your machine\\n\\nImporting iBioSim to Eclipse\\n\\nClone the iBioSim (https://github.com/MyersResearchGroup/iBioSim.git) project (e.g. git clone https://github.com/MyersResearchGroup/iBioSim.git) to a location of your preference.\\nOpen up your Eclipse workspace that you want to import your iBioSim project to.\\nSelect Import from the File Menu.\\nWhen given the option to select which project import, select Existing Maven Projects under Maven\\n\\nSet Maven Projects:\\n\\nRoot Directory: full path to your iBioSim project (i.e. path/to/iBioSim)\\nOnce root directory is set, all the pom.xml should be displayed under Projects. Select all pom.xml files.\\nAll installation should be complete so click Finish\\n\\n\\n\\n\\n\\nSetting up iBioSim Configurations in Eclipse\\n\\nOpen up iBioSim Run Configurations window and create a new Java Application in your Eclipse workspace\\n\\n\\nGive the java application a name (i.e. iBioSim_GUI)\\nSet the Main tab to the following information:\\n\\nProject: iBioSim-gui\\nMain class: edu.utah.ece.async.ibiosim.gui.Gui\\n\\n\\nSet the Environment tab to the following information:\\n\\nCreate variables with the corresponding value:\\n\\nBIOSIM: full path to your iBioSim project (i.e. path/to/iBioSim)\\nPATH: append your copy of iBioSim bin directory to whatever existing PATH already supplied to the value of this variable (i.e. $PATH:path/to/iBioSim/bin).\\n\\n\\n\\n\\nSet Arguments tab to the following information:\\n\\nProgram arguments: -Xms2048 -Xms2048 -XX:+UseSerialGC -Djava.library.path=/path/to/lib/\\nNote: for the java library path, /path/to/lib/ is the location where libSBML is installed. The libSBML is installed by default in /usr/local/lib in Linux and Mac OS X machines and libSBML-5.17.0-win64 in Windows 64-bit machines.\\n\\n\\nIf you are running on Mac OS X, also set the following:\\n\\nVM arguments: -Dapple.laf.useScreenMenuBar=true -Xdock:name=\"iBioSim\" -Xdock:icon=$BIOSIM/src/resources/icons/iBioSim.jpg\\n\\n\\nAll run configurations are complete. Make sure to apply all your changes.\\n\\nBuilding iBioSim\\n\\nGo to the directory where the iBioSim is checked out and perform mvn clean install (NOTE: if you do not want to generate javadocs, use the flag -Dmaven.javadoc.skip=true).\\n\\n[Optional] Building reb2sac and GeneNet dependencies\\n\\niBioSim incorporates tools that are not Java-based, and therefore, have to be installed separately.\\nThe easiest way to install reb2sac and GeneNet is to simply download the pre-compiled binaries for your operating system below:\\n\\nreb2sac\\nGeneNet\\n\\n\\nAnother way to install them is to compile these tools on your machine following the instructions below:\\n\\nreb2sac\\nGeneNet\\n\\n\\nAfter compiling or downloading reb2sac and GeneNet, copy the compiled binaries into the bin directory in the local copy of your iBioSim.\\n\\n'},\n",
       " {'language': 'C++ 93.2',\n",
       "  'readme': '\\n\\n\\n\\niBioSim is a computer-aided design (CAD) tool aimed for the modeling, analysis, and design of genetic circuits.\\nWhile iBioSim primarily targets models of genetic circuits, models representing metabolic networks, cell-signaling pathways,\\nand other biological and chemical systems can also be analyzed.\\niBioSim also includes modeling and visualization support for multi-cellular and spatial models as well.\\nIt is capable of importing and exporting models specified using the Systems Biology Markup Language (SBML).\\nIt can import all levels and versions of SBML and is able to export Level 3 Version 1.\\nIt supports all core SBML modeling constructs except some types of fast reactions, and also has support for the\\nhierarchical model composition, layout, flux balance constraints, and arrays packages.\\nIt has also been tested successfully on the stochastic benchmark suite and the curated models in the BioModels database.\\niBioSim also supports the Synthetic Biology Open Language (SBOL), an emerging standard for information exchange in synthetic\\nbiology.\\nWebsite: iBioSim\\nVideo Demo: Tools Workflow\\nContact: Chris Myers (@cjmyers) myers@ece.utah.edu\\nContributor(s): Nathan Barker, Pedro Fontanarrosa, Scott Glass, Kevin Jones, Hiroyuki Kuwahara, Curtis Madsen, Nam Nguyen, Tramy Nguyen, Tyler Patterson, Nicholas Roehner, Jason Stevens, Leandro Watanabe, Michael Zhang, Zhen Zhang, and Zach Zundel.\\nActive Developer(s): Pedro Fontanarrosa, Chris Myers, Tramy Nguyen, Leandro Watanabe.\\nRunning iBioSim\\n\\nDownload the iBioSim tool from the release page here:\\nAfter downloading the tool, run the corresponding start-up script:\\n\\nWindows: iBioSim.bat\\nMac OS X: iBioSim.mac64\\nLinux: iBioSim.linux64\\n\\n\\n\\n[Optional] Installing iBioSim for Development\\nPre-installation Requirements\\n\\nCreate a GitHub account.\\nSetup Git on your machine.\\nInstall Maven plugin on your machine.\\nInstall Eclipse IDE  for Java.\\nInstall libSBML for validation and flattening.\\nClone the iBioSim GitHub repository to your machine\\n\\nImporting iBioSim to Eclipse\\n\\nClone the iBioSim (https://github.com/MyersResearchGroup/iBioSim.git) project (e.g. git clone https://github.com/MyersResearchGroup/iBioSim.git) to a location of your preference.\\nOpen up your Eclipse workspace that you want to import your iBioSim project to.\\nSelect Import from the File Menu.\\nWhen given the option to select which project import, select Existing Maven Projects under Maven\\n\\nSet Maven Projects:\\n\\nRoot Directory: full path to your iBioSim project (i.e. path/to/iBioSim)\\nOnce root directory is set, all the pom.xml should be displayed under Projects. Select all pom.xml files.\\nAll installation should be complete so click Finish\\n\\n\\n\\n\\n\\nSetting up iBioSim Configurations in Eclipse\\n\\nOpen up iBioSim Run Configurations window and create a new Java Application in your Eclipse workspace\\n\\n\\nGive the java application a name (i.e. iBioSim_GUI)\\nSet the Main tab to the following information:\\n\\nProject: iBioSim-gui\\nMain class: edu.utah.ece.async.ibiosim.gui.Gui\\n\\n\\nSet the Environment tab to the following information:\\n\\nCreate variables with the corresponding value:\\n\\nBIOSIM: full path to your iBioSim project (i.e. path/to/iBioSim)\\nPATH: append your copy of iBioSim bin directory to whatever existing PATH already supplied to the value of this variable (i.e. $PATH:path/to/iBioSim/bin).\\n\\n\\n\\n\\nSet Arguments tab to the following information:\\n\\nProgram arguments: -Xms2048 -Xms2048 -XX:+UseSerialGC -Djava.library.path=/path/to/lib/\\nNote: for the java library path, /path/to/lib/ is the location where libSBML is installed. The libSBML is installed by default in /usr/local/lib in Linux and Mac OS X machines and libSBML-5.17.0-win64 in Windows 64-bit machines.\\n\\n\\nIf you are running on Mac OS X, also set the following:\\n\\nVM arguments: -Dapple.laf.useScreenMenuBar=true -Xdock:name=\"iBioSim\" -Xdock:icon=$BIOSIM/src/resources/icons/iBioSim.jpg\\n\\n\\nAll run configurations are complete. Make sure to apply all your changes.\\n\\nBuilding iBioSim\\n\\nGo to the directory where the iBioSim is checked out and perform mvn clean install (NOTE: if you do not want to generate javadocs, use the flag -Dmaven.javadoc.skip=true).\\n\\n[Optional] Building reb2sac and GeneNet dependencies\\n\\niBioSim incorporates tools that are not Java-based, and therefore, have to be installed separately.\\nThe easiest way to install reb2sac and GeneNet is to simply download the pre-compiled binaries for your operating system below:\\n\\nreb2sac\\nGeneNet\\n\\n\\nAnother way to install them is to compile these tools on your machine following the instructions below:\\n\\nreb2sac\\nGeneNet\\n\\n\\nAfter compiling or downloading reb2sac and GeneNet, copy the compiled binaries into the bin directory in the local copy of your iBioSim.\\n\\n'},\n",
       " {'language': 'C++ 96.2',\n",
       "  'readme': 'Introducing Scorum\\nScorum platform has three core functions:\\n\\nBlogging platform where authors and readers will be rewarded for creating and engaging with content\\nStatistical centers where fans can browse and authors can use Microsoft’s Power BI tool to integrate data-rich visuals into their content\\nCommission-free betting exchange where fans can place bets against each other using Scorum Coins (SCR)\\nScorum’s blockchain protocol is built on the Graphene Framework and utilizes a delegated proof of stake consensus.\\n\\nPublic Announcement & Discussion\\nThe Scorum team has been hard at work developing the blogging platform and the statistics center.\\nFind out more as we take the project public through the following channels:\\n\\nGet the latest updates and chat with us on Telegram, Facebook, and Twitter\\nRead more about our vision on Steemit\\nJoin our affiliate program to get Scorum Coins for free or apply for whitelist to get coins with a discount\\n\\nNo Support & No Warranty\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\\nIN THE SOFTWARE.\\nBlockchain consensus rules\\nRather than attempt to describe the rules of the blockchain, it is up to\\neach individual to inspect the code to understand the consensus rules.\\nQuickstart\\nJust want to get up and running quickly?  Try deploying a prebuilt\\ndockerized container.\\nDockerized Node\\nSee this guide for detailed instructions including commands for Ubuntu  16.04 (LTS).\\nSeed Nodes\\nA list of some seed nodes to get you started can be found in\\nseed-nodes. This list is embedded into default config.ini.\\nBuilding\\nSee doc/building.md for detailed build instructions, including\\ncompile-time options, and specific commands for Ubuntu 16.04 (LTS).\\nSystem Requirements\\nFor a full node, you need 10GB of space available. Scorumd uses a memory mapped file which currently holds 2GB of data and by default is set to use up to 10GB. It\\'s highly recommended to run scorumd on a fast disk such as an SSD or by placing the shared memory files in a ramdisk and using the shared-file-dir config (or command line) option to specify where. Any CPU with decent single core performance should be sufficient.\\nMain net chain_id\\ngenesis.json hash sum: db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1\\nTest net chain_id\\ngenesis.testnet.json hash sum: d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2\\n'},\n",
       " {'language': 'C++ 100.0',\n",
       "  'readme': 'Introducing Scorum\\nScorum platform has three core functions:\\n\\nBlogging platform where authors and readers will be rewarded for creating and engaging with content\\nStatistical centers where fans can browse and authors can use Microsoft’s Power BI tool to integrate data-rich visuals into their content\\nCommission-free betting exchange where fans can place bets against each other using Scorum Coins (SCR)\\nScorum’s blockchain protocol is built on the Graphene Framework and utilizes a delegated proof of stake consensus.\\n\\nPublic Announcement & Discussion\\nThe Scorum team has been hard at work developing the blogging platform and the statistics center.\\nFind out more as we take the project public through the following channels:\\n\\nGet the latest updates and chat with us on Telegram, Facebook, and Twitter\\nRead more about our vision on Steemit\\nJoin our affiliate program to get Scorum Coins for free or apply for whitelist to get coins with a discount\\n\\nNo Support & No Warranty\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\\nIN THE SOFTWARE.\\nBlockchain consensus rules\\nRather than attempt to describe the rules of the blockchain, it is up to\\neach individual to inspect the code to understand the consensus rules.\\nQuickstart\\nJust want to get up and running quickly?  Try deploying a prebuilt\\ndockerized container.\\nDockerized Node\\nSee this guide for detailed instructions including commands for Ubuntu  16.04 (LTS).\\nSeed Nodes\\nA list of some seed nodes to get you started can be found in\\nseed-nodes. This list is embedded into default config.ini.\\nBuilding\\nSee doc/building.md for detailed build instructions, including\\ncompile-time options, and specific commands for Ubuntu 16.04 (LTS).\\nSystem Requirements\\nFor a full node, you need 10GB of space available. Scorumd uses a memory mapped file which currently holds 2GB of data and by default is set to use up to 10GB. It\\'s highly recommended to run scorumd on a fast disk such as an SSD or by placing the shared memory files in a ramdisk and using the shared-file-dir config (or command line) option to specify where. Any CPU with decent single core performance should be sufficient.\\nMain net chain_id\\ngenesis.json hash sum: db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1\\nTest net chain_id\\ngenesis.testnet.json hash sum: d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2\\n'},\n",
       " {'language': 'C++ 78.1',\n",
       "  'readme': '\\n\\nПерейти на русский язык\\nThis project is devoted to development of the electronic timing system for orienteering with inexpensive base stations and cheap tags.\\nIt is also possible to use one on rogaining events, adventure races, trail running, wherever time keeping is required.\\nHere are hardware and firmware parts of the timing system.\\nLinks to data processing software are placed below.\\nDownload latest release\\nManual\\nThis project is open and free. Whoever is not afraid of difficulties can try doing it oneself. Just follow the instructions from the Manual.\\nThe low cost of the components can be worth your efforts (about USD $10 for one base station and $0.2 per RFID tag).\\nThis development is a hobby.\\nNo guarantees are given, various kinds of problems are possible during reproduction.\\nSupport is also not guaranteed. So, act at your own risk.\\nVersion\\nThe version consists of three numbers. The first number indicates the version of the hardware.\\nIf any changes are made to the circuit or to the PCB, this number is incremented by 1.\\nThe second and third numbers indicates the version of the firmware.\\nIf any new function is added to the firmware, the second number is incremented by 1.\\nIf the firmware just fixes bugs, the third number in the version is incremented by 1.\\nWhen a new version of the firmware is released with new functions, the third number is reset to 0.\\nThe base station and the master station have their own versions. The release version is the largest of these two numbers.\\nThe current release version is 3.7.0\\nThe current base station version is 3.7.0\\nThe current master station version is 1.7.0\\nChangelog\\nBuild the firmware of the base station with #define HW_VERS 1 to install the firmware vX.6.X or greater on the PCB v1 or v2.\\nReporting Issues and Asking for Help\\nIssues and suggested improvements can be posted on Issues page.\\nPlease make sure you provide all relevant information about your problem or idea.\\nWe also have our Telegram chat where you can ask any questions about the system.\\nContributing\\nYou can contribute by writing code.\\nWe welcome software for working with the system on a PC via USB and on Android via Bluetooth or NFC.\\nThe data transfer protocol and commands are described in the Manual.\\nWith pleasure we will add a link to your developments working with Sportiduino.\\nPull requests, creation of forks, developing any new ideas are welcome.\\nYou can also help by improving documentation and its translation.\\nParts of the system\\nCards\\nThe system uses cards NTAG213/215/216.\\nAs stickers on Chinese web marketplaces they cost about $0.1, 0.2 and 0.4, respectively.\\nAs key fobs the cost is doubled.\\nMemory of these cards can keep 32, 120 and 216 marks, respectively.\\nAlso it is possible to use Mifare Classic 1K and 4K cards.\\nCards 1K are also cheap and come bundled with the RC522 module.\\nThe memory of these chips is enough for 42 marks. They work a little slower than NTAG.\\nThe system automatically detects the type of used cards.\\nRead more\\nBase stations\\nThe main components of the station are the ATmega328P microcontroller and the MFRC522 module,\\nwhich operates at a frequency of 13.56 MHz, real-time clock DS3231SN.\\nAll powered by 3 AA batteries through the MCP1700T-33 stabilizer.\\nThe capacity of the kit of three alkaline AA batteries should be enough for a year of active use.\\nTested at ambient temperatures from -20 to +50 degrees Celcius.\\nTotally, the initial components for one base station and the consumables cost about $10 (in 2019).\\nRead more\\nMaster station\\nWith the master station you can read and write tags and configure base stations.\\nIt is simpler than the base station.\\nIt consists of Arduino Nano, RFID module, LED and buzzer.\\nIt connects with a PC via USB.\\nRead more\\nThere is also a wireless station with the Bluetooth module.\\nData processing\\nSportiduinoPQ\\nCards and stations are configured by SportiduinoPQ program.\\nThe program is written on Python, based on PyQt5 and SportiduinoPython module.\\nSportOrg\\nReading cards is implemented in the SportOrg program.\\n\\nThis system and its variants have been used in Russia at a number of events\\nup to approx. 1400 participants and approx. 70 check points.\\n\\nAvailable from:  https://github.com/sportiduino/sportiduino\\nLicense:         GNU GPLv3\\n'},\n",
       " {'language': 'C++ 71.7',\n",
       "  'readme': 'Sports Recognition in Videos\\nThis repository builds a classifier to predict the sport being played using Dense Trajectory Features. It works\\non the UCF Sports dataset and builds a multi-class classifier based on SVM using chi-squared kernel. The paper behind the project is:\\nHeng Wang, Alexander Kläser, Cordelia Schmid, Liu Cheng-Lin. Action Recognition by Dense Trajectories. CVPR 2011 - IEEE Conference on Computer Vision & Pattern Recognition, Jun 2011\\nPrerequisites\\n\\nThe UCF Sports Dataset, a\\nstripped down version (with no image dumps) can be found\\nhere\\nDense Trajectory Features,\\nthis code has been included in dense_trajectory_release_v1.2 folder\\n\\nProject Setup\\nRun setup.sh to install all dependencies. It also builds a DenseTrack executable\\nwhich gives out the features of all videos\\nCode Flow\\n\\nThe DenseTrack executable computes a large feature vector comprising of\\nHOG + HOF + MBH descriptors concatenated with each other\\nThe data is split into train and test with a ratio test_size\\nA codebook of size k using k-means clustering is generated in attempts\\nA bag-of-visual-words representation is created for each video using the\\nhistogram built using the above clustering\\nAll the bag-of-visual-words are fed into the SVM using chi-squared kernel\\nand classified using a One-Vs-Rest Classifier\\n\\nExecution\\nTo run the code, run Driver.py which generates the One-Vs-Rest Classifier and\\ndumps in a model.p file and along with the codebook centers for bag-of-visual-words\\nin centers.p.\\nEvaluation\\nThe code has been evaluated on accuracy of predictions after the test and train\\nsplit ratio of 0.3. Following classes have been used from the UCF Sports Dataset:\\n\\nDiving-Side (7 videos)\\nKicking-Front (10 videos)\\nRiding-Horse (12 videos)\\nRun-Side (13 videos)\\nSkateBoarding-Front (12 videos)\\nSwing-Bench (20 videos)\\nSwing-SideAngle (13 videos)\\nWalk-Front (22 videos)\\n\\nAfter preliminary evaluation, we achieved a result of around 29%.\\nContact\\nThe repository has been made available at\\nhttps://github.com/ChinmayJindal/sports-recognition\\n'},\n",
       " {'language': 'C++ 91.9',\n",
       "  'readme': 'two_point_calib\\nThis is an implementation of \"A Two-point Method for PTZ camera Calibration in Sports\" (WACV2018)\\nDependences:\\n\\nOpenCV 3.1 or later.\\nEigen 3.2.6 or later.\\nflann 1.8.4 or later.\\nmatio: https://github.com/tbeu/matio\\n\\nThe code is tested on Xcode 6.4 on a Mac 10.10.5 system. But the code has minimum dependence on compile and system, so it should work well on linux and windows as well.\\nFile structure:\\nmatlab: synthetic example of the two-point calibration method.\\nIn the src/pan_tilt_forest folder\\nbt_dtr: a general implementation of regression forest with back tracking\\ncvx_gl and cvx_pgl: geometry and project geometry files\\ndt_util: decision tree utility\\nutil: pan-tilt-zoom camera pose estimation given ray-3D correspondences\\nTodo: cmake file and training parameters\\n'},\n",
       " {'language': 'C++ 72.6',\n",
       "  'readme': 'Pekka Pirilän tulospalveluohjelma (sports time keeping program)\\nBriefly in Finnish / Lyhyesti suomeksi\\nTässä ovat Pekka Pirilän (1945-2015) tulospalveluohjelmien lähdekoodit. Mukana\\novat kaikki muunnelmat: teksti- ja Windows-versio sekä henkilökohtaisesta että\\nviestiohjelmasta. Pekan perhe julkaisi lähdekoodin avoimen lähdekoodin\\nGPLv3-lisenssin alaiseksi. Lisenssi tarkoittaa karkeasti sitä, että kuka vain\\nvoi muuttaa ja käyttää ohjelmaa vapaasti kaikissa tilanteissa. Ohjelman\\nkäyttöön liittyviä palveluita ja jopa kopioita ohjelmasta saa myydä, mutta\\nmuutetun version levittäjä sitoutuu julkaisemaan versiostaan myös lähdekoodin\\nja ostaja saa jälleen tehdä kopiollaan mitä haluaa.\\nValmiiksi käännetyt ja paketoidut versiot sekä ohjeet löytyvät edelleen\\nosoitteesta http://www.pirila.fi/ohj/index.html.\\nPekan perheen toiveena on, että ohjelmasta olisi hyötyä urheiluyhteisölle vielä\\nvuosien ajan.\\nDescription\\nThis is the source code of a suite of sports time keeping programs Pekka Pirilä\\n(1945-2015) started developing in around 1986. The program originally\\nspecialized in orienteering, but was later amended to support additional\\nsports. The user interface is in Finnish and there are Finnish language\\nvariables and comments throughout the source code. The source code is released\\nunder GPLv3. More information in Finnish at http://www.pirila.fi/ohj/index.html.\\nConsole programs\\nRequired tools\\nThe console program\\'s project files are for Visual Studio. They were\\nsuccessfully compiled with Visual Studio Express 2013 for Windows Desktop,\\nbut Visual Studio 2010 and anything newer can probably be made to work. The\\nlanguage is probably compliant with C++03.\\nCompile\\n\\nOpen TPsource\\\\V52\\\\VS\\\\Libs\\\\tputilv2.sln\\nCompile by pressing F7\\n\\nTwo new folders will be created next to TPsource: vc10 and TPexe\\n\\n\\nOpen TPsource\\\\V52\\\\VS\\\\Hk\\\\HkMaali520.sln\\nCompile by pressing F7\\n\\nA stand-alone executable TPexe\\\\Hk\\\\V521\\\\HkMaali.exe is built and ready\\nto use\\n\\n\\nTo build the relay version, repeat with TPsource\\\\V52\\\\VS\\\\V\\\\JukMaali520.sln\\n\\nWindows programs\\nRequired tools\\nThe Windows programs are made with\\nEmbarcadero C++ Builder.\\nVersion 10.1 Berlin was successfully used to compile and run the program.\\nSecureBridge 7.1 for RAD Studio 10.1 Berlin\\nis an add-on that is required to compile and run the program. NOTE: At least\\nin Windows 10 you must edit one of SecureBridge\\'s header files to be able to\\ncompile this program. Open\\n\"Program Files (x86)\\\\Devart\\\\SecureBridge for RAD Studio 10\\\\Include\\\\Win32\\\\ScSSHSocket.hpp\"\\nand change Winapi::Winsock::PSockAddrIn to Winapi::Winsock2::PSockAddrIn.\\nCompile\\n\\nOpen TPsource\\\\V52\\\\RADStudio10\\\\DBboxm-XE.cbproj\\nRight-click on DBboxm-XE.lib in Project Manager and select Make\\nOpen TPsource\\\\V52\\\\RADStudio10\\\\Tputil-XE.cbproj\\nRight-click on Tputil-XE.lib in Project Manager and select Make\\nOpen TPsource\\\\V52\\\\RADStudio10\\\\HkKisaWin.cbproj and Run to start the\\nprogram for individual competitions\\nOpen TPsource\\\\V52\\\\RADStudio10\\\\ViestiWin.cbproj and Run to start the\\nrelay program\\n\\nHeap errors during compilation\\nIf you get heap errors with linking, you can try these things\\n\\nhttp://stackoverflow.com/questions/28929516/c-builder-xe7-lme288-error\\n\\nRun command prompt as Administrator.\\nType (without quotes) \"bcdedit /set IncreaseUserVa 3072\"\\nReboot computer.\\n\\n\\nEmpty %TEMP%, reboot, try again, repeat\\nRun C++ Builder as admin\\n\\n'},\n",
       " {'language': 'C++ 88.4',\n",
       "  'readme': \"DFSCoin development tree\\nDFSCoin is a PoS-based cryptocurrency.\\nDevelopment process\\nDevelopers work in their own trees, then submit pull requests when\\nthey think their feature or bug fix is ready.\\nThe patch will be accepted if there is broad consensus that it is a\\ngood thing.  Developers should expect to rework and resubmit patches\\nif they don't match the project's coding conventions (see coding.txt)\\nor are controversial.\\nThe master branch is regularly built and tested, but is not guaranteed\\nto be completely stable. Tags are regularly created to indicate new\\nstable release versions of DFSCoin.\\nFeature branches are created when there are major new features being\\nworked on by several people.\\nFrom time to time a pull request will become outdated. If this occurs, and\\nthe pull is no longer automatically mergeable; a comment on the pull will\\nbe used to issue a warning of closure. The pull will be closed 15 days\\nafter the warning if action is not taken by the author. Pull requests closed\\nin this manner will have their corresponding issue labeled 'stagnant'.\\nIssues with no commits will be given a similar warning, and closed after\\n15 days from their last activity. Issues closed in this manner will be\\nlabeled 'stale'.\\n\"},\n",
       " {'language': 'C++ 74.6',\n",
       "  'readme': 'SportsCash Core integration/staging repository\\nDash forked Bitcoin - PIVX Forked Dash - SPORTSCASH Forked PIVX\\nProject INFO\\n\\nBitcointalkhttps://bitcointalk.org/index.php?topic=4356707.msg38922386#msg38922386\\nTelegramhttps://t.me/joinchat/GzldMxHltxGz_ZqHSFlGPg\\nDiscordhttps://discord.gg/GfuxwGB\\nTwitterhttps://twitter.com/SportsCashCoin\\nReddithttps://www.reddit.com/r/SportsCashCoin/\\nWebsitehttp://sportscash.co\\n\\nCoin Specs\\n\\nAlgoQuark\\nBlock Time60 Seconds\\nDifficulty RetargetingEvery Block\\nMax Coin Supply (PoS)24,000,000 SCC\\nPremine1,000,000 SCC\\nMasternode Collateral10,000 SCC\\nPort Collateral33001\\nRPCPort Collateral33002\\n\\nPoS Rewards Breakdown\\n\\nBlock HeightRewardMasternodesStakers\\n<= 2,0001 SCC0.9 SCC0.1 SCC\\n2001-1,144,8577 SCC6.3 SCC0.7 SCC\\n1,144,858-2,744,8575 SCC4.5 SCC0.5 SCC\\n>2744858-54115233 SCC2.7 SCC0.3 SCC\\n5411523- 0 SCC0 SCC0 SCC\\n\\n'},\n",
       " {'language': 'C++ 100.0',\n",
       "  'readme': 'sports\\nAnalytics, tools and puzzles applicable to a variety of sports.\\n'},\n",
       " {'language': 'C++ 68.3',\n",
       "  'readme': \"Coursera-Data Structures and Algorithms Specialization\\nThis specialization is a mix of theory and practice: you will learn algorithmic techniques for solving various computational problems and will implement about 100 algorithmic coding problems in a programming language of your choice. No other online course in Algorithms even comes close to offering you a wealth of programming challenges that you may face at your next job interview. To prepare you, we invested over 3000 hours into designing our challenges as an alternative to multiple choice questions that you usually find in MOOCs. Sorry, we do not believe in multiple choice questions when it comes to learning algorithms...or anything else in computer science! For each algorithm you develop and implement, we designed multiple tests to check its correctness and running time — you will have to debug your programs without even knowing what these tests are! It may sound difficult, but we believe it is the only way to truly understand how the algorithms work and to master the art of programming. The specialization contains two real-world projects: Big Networks and Genome Assembly. You will analyze both road networks and social networks and will learn how to compute the shortest route between New York and San Francisco (1000 times faster than the standard shortest path algorithms!) Afterwards, you will learn how to assemble genomes from millions of short fragments of DNA and how assembly algorithms fuel recent developments in personalized medicine.\\nData Structures and Algorithms Specialization\\nSkills Gained:\\nAlgorithms Data Structure Debugging Graph Theory Software Testing Binary Search Tree Computer Programming\\nCourse 1 - Algorithmic Toolbox\\nThe course covers basic algorithmic techniques and ideas for computational problems arising frequently in practical applications: sorting and searching, divide and conquer, greedy algorithms, dynamic programming. We will learn a lot of theory: how to sort data and how it helps for searching; how to break a large problem into pieces and solve them recursively; when it makes sense to proceed greedily; how dynamic programming is used in genomic studies. You will practice solving computational problems, designing new algorithms, and implementing solutions efficiently (so that they run in less than a second).\\nCourse 2 - Data Structures\\nA good algorithm usually comes together with a set of good data structures that allow the algorithm to manipulate the data efficiently. In this course, we consider the common data structures that are used in various computational problems. You will learn how these data structures are implemented in different programming languages and will practice implementing them in our programming assignments. This will help you to understand what is going on inside a particular built-in implementation of a data structure and what to expect from it. You will also learn typical use cases for these data structures.\\nA few examples of questions that we are going to cover in this class are the following:\\n\\nWhat is a good strategy of resizing a dynamic array?\\nHow priority queues are implemented in C++, Java, and Python?\\nHow to implement a hash table so that the amortized running time of all operations is O(1) on average?\\nWhat are good strategies to keep a binary tree balanced?\\n\\nCourse 3 - Algorithms on Graphs\\nIf you have ever used a navigation service to find optimal route and estimate time to destination, you've used algorithms on graphs. Graphs arise in various real-world situations as there are road networks, computer networks and, most recently, social networks! If you're looking for the fastest time to get to work, cheapest way to connect set of computers into a network or efficient algorithm to automatically find communities and opinion leaders in Facebook, you're going to work with graphs and algorithms on graphs.\\nIn this course, you will first learn what a graph is and what are some of the most important properties. Then you'll learn several ways to traverse graphs and how you can do useful things while traversing the graph in some order. We will then talk about shortest paths algorithms — from the basic ones to those which open door for 1000000 times faster algorithms used in Google Maps and other navigational services. You will use these algorithms if you choose to work on our Fast Shortest Routes industrial capstone project. We will finish with minimum spanning trees which are used to plan road, telephone and computer networks and also find applications in clustering and approximate algorithms.\\nCourse 4 - Algorithms on Strings\\nWorld and internet is full of textual information. We search for information using textual queries, we read websites, books, e-mails. All those are strings from the point of view of computer science. To make sense of all that information and make search efficient, search engines use many string algorithms. Moreover, the emerging field of personalized medicine uses many search algorithms to find disease-causing mutations in the human genome.\\nCourse 5 - Advanced Algorithms and Complexity\\nYou've learned the basic algorithms now and are ready to step into the area of more complex problems and algorithms to solve them. Advanced algorithms build upon basic ones and use new ideas. We will start with networks flows which are used in more typical applications such as optimal matchings, finding disjoint paths and flight scheduling as well as more surprising ones like image segmentation in computer vision. We then proceed to linear programming with applications in optimizing budget allocation, portfolio optimization, finding the cheapest diet satisfying all requirements and many others. Next we discuss inherently hard problems for which no exact good solutions are known (and not likely to be found) and how to solve them in practice. We finish with a soft introduction to streaming algorithms that are heavily used in Big Data processing. Such algorithms are usually designed to be able to process huge datasets without being able even to store a dataset.\\nCourse 6 - Genome Assembly Programming Challenge\\nIn Spring 2011, thousands of people in Germany were hospitalized with a deadly disease that started as food poisoning with bloody diarrhea and often led to kidney failure. It was the beginning of the deadliest outbreak in recent history, caused by a mysterious bacterial strain that we will refer to as E. coli X. Soon, German officials linked the outbreak to a restaurant in Lübeck, where nearly 20% of the patrons had developed bloody diarrhea in a single week. At this point, biologists knew that they were facing a previously unknown pathogen and that traditional methods would not suffice – computational biologists would be needed to assemble and analyze the genome of the newly emerged pathogen.\\nTo investigate the evolutionary origin and pathogenic potential of the outbreak strain, researchers started a crowdsourced research program. They released bacterial DNA sequencing data from one of a patient, which elicited a burst of analyses carried out by computational biologists on four continents. They even used GitHub for the project: https://github.com/ehec-outbreak-crowdsourced/BGI-data-analysis/wiki\\nThe 2011 German outbreak represented an early example of epidemiologists collaborating with computational biologists to stop an outbreak. In this Genome Assembly Programming Challenge, you will follow in the footsteps of the bioinformaticians investigating the outbreak by developing a program to assemble the genome of the E. coli X from millions of overlapping substrings of the E.coli X genome.\\n\"},\n",
       " {'language': 'C++ 98.3',\n",
       "  'readme': 'NewQuant\\nNewQuant is a C++ library for data analysis and financial engineering computation. It is in building now, not a completed version.\\n01.ExceptionClass is finished，it is a self-defined exception class.\\n02.MathematicsExpression is finished，it helps users to build numerical functors in a convenient way.\\n03.MatrixComputation，the most important module of NewQuant，is finished mostly，it includes kinds of matrices and kinds of linear-equations solvers. Users can use it to do basic matrix computation，to solve linear-equation，to do matrix decomposition(such as LU decomposition)，to solve least square problem.\\n04.MonteCarlo，the majority part is finished，it includes kinds of SDEsolvers，users can use it to simulate kinds of SDEs，for example GBM. This module is very useful for financial engineering.\\n05.Regression is in building now，it is the base of econometrics.\\n06.SpecialFunction is in building now，it is the base of StatisticsComputation module.\\n07.StatisticsComputation，is finished partly，it includes kinds of computation about pdf，cdf and quantile now，it is also the base of econometrics.\\nNewQuant is released under BSD license.\\n'},\n",
       " {'language': 'C++ 92.4',\n",
       "  'readme': \"FEAT\\n\\n\\nFEAT is a feature engineering automation tool that learns new representations of raw data\\nto improve classifier and regressor performance. The underlying methods use Pareto\\noptimization and evolutionary computation to search the space of possible transformations.\\nFEAT wraps around a user-chosen ML method and provides a set of representations that give the best\\nperformance for that method. Each individual in FEAT's population is its own data representation.\\nFEAT uses the Shogun C++ ML toolbox to fit models.\\nCheck out the documentation for installation and examples.\\nCite\\nLa Cava, W., Singh, T. R., Taggart, J., Suri, S., & Moore, J. H.. Learning concise representations for regression by evolving networks of trees. ICLR 2019. arxiv:1807.0091\\nBibtex:\\n@inproceedings{la_cava_learning_2019,\\n    series = {{ICLR}},\\n    title = {Learning concise representations for regression by evolving networks of trees},\\n    url = {https://arxiv.org/abs/1807.00981},\\n    language = {en},\\n    booktitle = {International {Conference} on {Learning} {Representations}},\\n    author = {La Cava, William and Singh, Tilak Raj and Taggart, James and Suri, Srinivas and Moore, Jason H.},\\n    year = {2019},\\n}\\n\\nContact\\nMaintained by William La Cava (lacava at upenn.edu)\\nAcknowledgments\\nThis work is supported by grant K99-LM012926 from the National Library of Medicine.\\nFEAT is being developed to study human disease in the Epistasis Lab\\nat UPenn.\\nLicense\\nGNU GPLv3, see LICENSE\\n\"},\n",
       " {'language': 'C++ 66.5',\n",
       "  'readme': \"FEAT\\n\\n\\nFEAT is a feature engineering automation tool that learns new representations of raw data\\nto improve classifier and regressor performance. The underlying methods use Pareto\\noptimization and evolutionary computation to search the space of possible transformations.\\nFEAT wraps around a user-chosen ML method and provides a set of representations that give the best\\nperformance for that method. Each individual in FEAT's population is its own data representation.\\nFEAT uses the Shogun C++ ML toolbox to fit models.\\nCheck out the documentation for installation and examples.\\nCite\\nLa Cava, W., Singh, T. R., Taggart, J., Suri, S., & Moore, J. H.. Learning concise representations for regression by evolving networks of trees. ICLR 2019. arxiv:1807.0091\\nBibtex:\\n@inproceedings{la_cava_learning_2019,\\n    series = {{ICLR}},\\n    title = {Learning concise representations for regression by evolving networks of trees},\\n    url = {https://arxiv.org/abs/1807.00981},\\n    language = {en},\\n    booktitle = {International {Conference} on {Learning} {Representations}},\\n    author = {La Cava, William and Singh, Tilak Raj and Taggart, James and Suri, Srinivas and Moore, Jason H.},\\n    year = {2019},\\n}\\n\\nContact\\nMaintained by William La Cava (lacava at upenn.edu)\\nAcknowledgments\\nThis work is supported by grant K99-LM012926 from the National Library of Medicine.\\nFEAT is being developed to study human disease in the Epistasis Lab\\nat UPenn.\\nLicense\\nGNU GPLv3, see LICENSE\\n\"},\n",
       " {'language': 'C++ 89.3',\n",
       "  'readme': 'data-structures-and-algorithms\\nThis project contains learning materials for the Data Structures and Algorithms course for students from Software Engineering program at Sofia University, Faculty of Mathematics and informatics\\n'},\n",
       " {'language': 'C++ 98.7',\n",
       "  'readme': 'TH10_DataReversing\\nUsing binary reverse engineering techniques to extract the memory data of the Touhou 10th game, Mountain of Faith.\\nThese are some AI projects about MoF:\\ntwinject by Netdex\\nTH10AI by DREAMWORLDVOID\\ntouhou10-dqn by actumn\\n'},\n",
       " {'language': 'C++ 76.6',\n",
       "  'readme': 'game-data-reverse-engineering\\nReverse engineering resources for data files from various video games I like\\n'},\n",
       " {'language': 'C++ 60.4',\n",
       "  'readme': 'Engineering\\nMy academic adventures.\\nThis repository contains all my projects during my graduation.\\n'},\n",
       " {'language': 'C++ 88.0',\n",
       "  'readme': \"libSPRITE\\n#Installation\\nTo install, type make install as root.\\nThis will copy headers files to /usr/local/include/SPRITE/ and static library\\nto /usr/local/lib/SPRITE by default.\\nTo uninstall, type make uninstall as root.\\nLua Paths\\nlibSPRITE assumes the Lua headers are install in /usr/local/include. Some\\ndistributions place it elsewhere. You can change where the Makefile looks for\\nthe Lua include files by specifying the LUA_INCLUDE variable in the arguments\\nto make.\\nex.: make LUA_INCLUDE=/usr/include/lua5.2\\nYou can also change the Lua library path by setting the LUA_LIB variable.\\nTesting\\n'make test' will build the unit tests for this package. You must have cppunit\\ninstalled to compile and run the unit tests. After compiling, run './run_test'\\nto execute the unit tests. You will have to run as root to execute all test cases.\\nNOTE: You must start from a clean systems ('make clean') before running 'make test'. Otherwise, some tests that depend on compile time assertions will fail.\\nMakefile Overrides\\nBy default, libSPRITE sends output to stdout and stderr for info, warnings, and errors. To supress these messages, you can specifiy -DNO_PRINT_INFO -DNO_PRINT_WARNING -DNO_PRINT_ERROR. The best way to do that is by appending to these options to the USER_CFLAGS. For example:\\nmake USER_CFLAGS='-DNO_PRINT_INFO -DNO_PRINT_WARNING'\\n\\nThere are also overides for CPPFLAGS (USER_CPPFLAGS) and LDFLAGS (USER_LDFLAGS).\\nUsing CMake\\nTo build with cmake, create a directory called build, cd to the build directory and type cmake ../.\\nTo specify the build prefix, using the CMAKE_INSTALL_PREFIX macro. Example\\ncmake -DCMAKE_INSTALL_PREFIX=/usr/local/\\n\\nBy default, the build type is Release. To build unit tests us the CMAKE_BUILD_TYPE macro. Example:\\ncmake -DCMAKE_BUILD_TYPE=Test\\n\\nAfter specifying cmake with this macro, make will create a run_test executable in each folder. Execute the run_test executable for the folder you wish to test.\\nRun cmake with the -DCMAKE_BUILD_TYPE=Release option to switch back to the normal build.\\nDocumentation\\nDocumentation can be found on the Wiki\\nTutorial\\nA growing Tutorial for developing applications using libSPRITE can be found here\\n\"},\n",
       " {'language': 'C++ 100.0',\n",
       "  'readme': 'Welcome to GitHub Pages\\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\\nMarkdown\\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\\nSyntax highlighted code block\\n\\n# Header 1\\n## Header 2\\n### Header 3\\n\\n- Bulleted\\n- List\\n\\n1. Numbered\\n2. List\\n\\n**Bold** and _Italic_ and `Code` text\\n\\n[Link](url) and ![Image](src)\\nFor more details see GitHub Flavored Markdown.\\nJekyll Themes\\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\\nSupport or Contact\\nHaving trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.\\n'},\n",
       " {'language': 'C++ 53.5',\n",
       "  'readme': 'Artificial intelligence library\\nMy C++ deep learning framework (with GPU support) & other machine learning algorithms implementations\\nDeepLearning Operations\\n\\nConvolution\\nDropout\\nSoftmax\\nRecurrent\\nLinear\\nSigmoid, Tanh, Relu, Selu activations\\nLayer normalization\\nAddition\\nConcatenation\\nMaxpooling\\nAveragepooling\\nResidualBlock\\nAutoencoder\\nL1 e L2 regularizations\\nGradient clipping\\n\\nNeural network optimizers\\n\\nStochastic gradient descent with minibatch and momentum\\nDirect Feedback Alignment\\n\\nDeep Reinforcement Learning agents\\n\\nDeep Qlearning Agents\\n\\nDeep Reinforcement Learning environments\\n\\nKbandits\\nTicTacToe\\n\\nOptimization algorithms\\n\\nGenetic algorithms (with multicore features for high performance)\\nParticle Swarm Optimization\\n\\nOther machine learning algorithms\\n\\nlinear regression\\nlogistic regression\\ngenetic programming\\n\\nData Mining\\n\\nK-means clustering\\n\\nVisualization\\n\\nBitmap class for loading, saving and processing multiple image formats\\nVisualizaiton namespace for fast data visualization\\n\\nLicense\\nThe MIT License (MIT)\\nCopyright (c) 2015 Carlo Meroni\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'},\n",
       " {'language': 'C++ 92.2',\n",
       "  'readme': 'About This Repository\\nThis repository should now be considered a historical curiosity only.\\nThe original version of this code was developed between 2002-2004 and included free with the book \"Artificial Intelligence for Games\". Over the intervening years, the code has become a less useful reference. The third edition of the textbook is considerably larger, and this code did not keep up with errata or improvements in the algorithms. I have not use the code in this repository for my consulting work in over a decade.\\nThe third edition of the textbook does not mention this code, but includes considerably expanded and corrected listings in the text in a language neutral format. I would recommend that version.\\n\\nHistorical Information\\nThe Artificial Intelligence for Games system.\\nCopyright (c) Ian Millington 2003-2009. All Rights Reserved.\\nThis software is distributed under licence. Use of this software\\nimplies agreement with all terms and conditions of the accompanying\\nsoftware licence.\\nThis code also contains portions of the AI Core engine.\\nCopyright (c) Icosagon Limited 2003-2007. All Rights Reserved.\\nPlease see accompanying LICENSE file.\\nInstallation\\nThe code can be extracted to any directory.\\nPlatform Compatibility\\nThe software has been designed for platform indepedence as much as\\npossible. The only file that may need altering for your platform is\\n./src/timing.cpp which currently wraps the windows multimedia timer.\\nBuilding\\nBuilding with Scons\\nThe code can be built using SCONS, available from\\nhttp://www.scons.org/. Simply cd (change directory) into the build\\ndirectory and type scons.\\n\\ncd build\\nscons\\n\\nTo remove intermediate files after building (but leaving the library\\nand demos):\\n\\nscons -c ../src\\n\\nBuilding with Microsoft Visual Studio 8 Professional\\nSolution and project files are included for use with Microsoft visual\\nstudio 8 professional. They may also work with Express edition, but\\nI\\'ve not tested that.\\nBuilding with CMake on Linux\\nThe code can be built on Linux (and possibly on other platforms)\\nusing CMake:\\ncd build\\ncmake .\\nmake\\nDocumentation\\nTo build the documentation (see below) you must have doxygen\\ninstalled (it is available from http://www.stack.nl/~dimitri/doxygen/\\nSimply cd into the ./doc/build/doxygen directory, then type:\\n\\ndoxygen aicore.config\\n\\nto build the documentation.\\nLayout\\nThe build process creates a statically linked library in ./lib which\\ncan be used with the include headers in ./include. The demo programs\\nare built and placed in the ./bin directory.\\nSource code is contained in the ./src directory, and documentation\\nis in the ./doc directory, in particular the reference documentation\\nis in the ./doc/ref directory.\\nDocumentation\\nThe source code is heavily documented, and the contents correspond to\\nthe discussion in the \"Artificial Intelligence for Games\" book.\\nIt is possible to create \\'doxygen\\' documentation with the tags in the\\nsource code files, and a configuration for building the documentation\\nis provided in the ./doc/build/doxygen directory. The doxygen\\nconfiguration supplied provides only html output, since other output\\nformats depend on how your machine is configured.\\nThis is not currently targeted from the scons configuration, because\\nscons suport for doxygen depends on where doxygen is installed on your\\nmachine.\\nDemos\\nTo run the demos you will require OpenGL and GLUT installed on your\\nmachine, and the relevant DLLs or shared objects on the path.\\n'},\n",
       " {'language': 'C++ 51.7',\n",
       "  'readme': 'OpenNERO\\n   \\nOpenNERO is an open source software platform designed for\\nresearch and education in Artificial Intelligence. The project is based on the\\nNeuro-Evolving Robotic Operatives (NERO) game developed by graduate\\nand undergraduate students at the  Neural Networks Research Group and\\nDepartment of Computer Science at the\\nUniversity of Texas at Austin.\\nIn particular, OpenNERO has been used to implement several demos and exercises for Russell\\nand Norvig\\'s textbook Artificial Intelligence: A Modern Approach.  These\\ndemos and exercises illustrate AI methods such as brute-force search, heuristic search, scripting,\\nreinforcement learning, and evolutionary computation, and AI problems such as maze running,\\nvacuuming, and robotic battle. The methods and problems are implemented in several different\\nenvironments (or \"mods\"), as described below.\\nMore environments, problems, and methods, as well as demos and exercises illustrating them, will\\nbe added in the future. The current ones are intended to serve as a starting point on which new\\nones can be built, by us, but also by the community at large. If you have questions or would like to contribute, check out the OpenNERO Google Group.\\nGet Started\\n\\nGet OpenNERO\\n\\nOpenNERO Dependencies\\nDownload a Binary\\n\\nLinux\\nWindows\\nMac OS X\\n\\n\\nBuild from Source\\n\\nLinux\\nWindows\\nMac OS X\\n\\n\\n\\n\\nDemonstrations\\n\\nSearch\\n\\nThe Maze Environment\\nFirst Person Search\\nUninformed Search\\nHeuristic Search\\n\\n\\nPlanning\\n\\nTower of Hanoi Environment\\nSymbolic Planning\\n\\n\\nNatural Language Processing\\n\\nTower of Hanoi Environment\\nNatural Language Processing\\n\\n\\nReinforcement Learning\\n\\nThe Maze Environment\\nQ-learning\\n\\n\\nEvolutionary Computation\\n\\nThe Roomba Environment\\nNeuroevolution\\n\\n\\nMulti-Agent Systems\\n\\nNERO Environment\\nNERO Machine Learning Game\\n\\n\\nVision\\n\\nThe Vision Environment\\nVision: Edge Detection\\n\\n\\n\\n\\nExercises\\n\\nOpenNero Setup\\nAdding Stuff\\nCreate Roomba Agent\\nMaze Generator\\nMaze Solver\\nAI Exercises\\nHeuristic Search\\nPlanning\\nNLPExercise\\nQLearningExercise\\nMaze Learner\\nAdvanced Maze\\nNero Tournament\\nSample Tournament Results\\nObject Recognition\\n\\n\\nSystem Documentation\\nHeadless Mode\\nSystem Overview\\n\\nContributors\\nMany people have contributed to OpenNERO, including Igor V. Karpov, John B. Sheblak, Adam Dziuk, Minh Phan, Dan Lessin, Wes Tansey, Reza Mahjourian, Risto Miikkulainen, members of the Neural Networks Research Group at UT Austin, students and alumni of the Computational Intelligence and Game Design stream of the Freshman Research Initiative at UT Austin.\\n\\nNOTE: as with any active project, OpenNERO is a work in progress and many updates are frequently being made. If you have trouble with OpenNERO, check the discussion group and then consider submitting an issue. And of course, if you would like to contribute, let us know!\\n'},\n",
       " {'language': 'C++ 88.8',\n",
       "  'readme': 'AIKIDO - AI for KIDO   \\n\\n⚠️ Warning: AIKIDO is under heavy development. These instructions are\\nprimarily for reference by the developers.\\n\\nAIKIDO is a C++ library, complete with Python bindings, for solving robotic motion\\nplanning and decision making problems. This library is tightly integrated with\\nDART for kinematic/dynamics calculations and OMPL for motion planning. AIKIDO\\noptionally integrates with ROS, through the suite of aikido_ros packages, for\\nexecution on real robots.\\nInstallation\\nOn Ubuntu Trusty using apt-get\\nAIKIDO depends on ROS. You should install ROS by adding the ROS repository to your sources.list as follows. We encourage users to install indigo.\\n$ sudo sh -c \\'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list\\'\\n$ sudo apt-key adv --keyserver \\'hkp://keyserver.ubuntu.com:80\\' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654\\n$ sudo apt-get update\\n$ sudo apt-get install ros-indigo-actionlib ros-indigo-geometry-msgs ros-indigo-interactive-markers ros-indigo-roscpp ros-indigo-std-msgs ros-indigo-tf ros-indigo-trajectory-msgs ros-indigo-visualization-msgs\\nOnce ROS is installed, you can install AIKIDO from the Personal Robotics Lab PPA:\\n$ sudo add-apt-repository ppa:libccd-debs/ppa\\n$ sudo add-apt-repository ppa:fcl-debs/ppa\\n$ sudo add-apt-repository ppa:dartsim/ppa\\n$ sudo add-apt-repository ppa:personalrobotics/ppa\\n$ sudo apt-get update\\n$ sudo apt-get install libaikido-all-dev\\nOn macOS using Homebrew\\n# Install the Homebrew package manager\\n$ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\\n# Add Homebrew tap for Personal Robotics Lab software\\n$ brew tap personalrobotics/tap\\n# Install AIKIDO\\n$ brew install aikido\\n\\nNote: While ROS seems to be available on macOS, we haven\\'t tested it with AIKIDO. For now, brew install aikido installs AIKIDO without the ROS-dependent components.\\n\\nBuilding from Source\\nDependencies\\nAIKIDO depends on CMake, Boost, DART (version 6.3 or above), OMPL, yaml-cpp, tinyxml2, pr-control-msgs, libmicrohttpd, and the\\nPython development headers (python-dev on Debian systems). DART and AIKIDO both\\nmake heavy use of C++14 and require a modern compiler.\\nOn Ubuntu Trusty using CMake\\nYou should install the ROS packages as described above to build all the ROS-dependent AIKIDO components (e.g., aikido-control-ros).\\nInstall the other dependencies:\\n$ sudo add-apt-repository ppa:libccd-debs/ppa\\n$ sudo add-apt-repository ppa:fcl-debs/ppa\\n$ sudo add-apt-repository ppa:dartsim/ppa\\n$ sudo add-apt-repository ppa:personalrobotics/ppa\\n$ sudo apt-get update\\n$ sudo apt-get install cmake build-essential libboost-filesystem-dev libdart6-optimizer-nlopt-dev libdart6-utils-dev libdart6-utils-urdf-dev libmicrohttpd-dev libompl-dev libtinyxml2-dev libyaml-cpp-dev pr-control-msgs\\nOnce the dependencies are installed, you can build and install AIKIDO using CMake:\\n$ mkdir build\\n$ cd build\\n$ cmake ..\\n$ make  # you may want to build AIKIDO using multi-core by executing `make -j4`\\n$ sudo make install\\nAIKIDO includes several optional components that depend on ROS. While we\\nsuggest building AIKIDO in a Catkin workspace (see below) to enable the ROS\\ncomponents, it is also possible to build those components in a standalone\\nbuild. To do so, source the setup.bash file in your Catkin workspace before\\nrunning the above commands, e.g.:\\n$ . /path/to/my/workspace/setup.bash\\nOn Ubuntu Trusty using Catkin\\nIt is also possible to build AIKIDO as a third-party package inside a\\nCatkin workspace. To do so, clone AIKIDO into your Catkin\\nworkspace and use the catkin build command like normal.\\nIf you are using the older catkin_make command, then you must build your workspace\\nwith catkin_make_isolated. This may dramatically increase your build time, so we\\nstrongly recommend that you use catkin build, which is provided by the\\ncatkin_tools package, if possible.\\nOn macOS using CMake\\nPlease install Homebrew as described above, then you can easily install the dependencies as follows:\\n$ cd <aikido_directory>\\n$ brew bundle\\nOnce the dependencies are installed, you can build and install AIKIDO using CMake:\\n$ cd <aikido_directory>\\n$ mkdir build\\n$ cd build\\n$ cmake ..\\n$ make  # you may want to build AIKIDO using multi-core by executing `make -j4`\\n$ sudo make install\\nCode Style\\nPlease follow the AIKIDO style guidelines when making a contribution.\\nLicense\\nAIKIDO is licensed under a BSD license. See LICENSE for more\\ninformation.\\nAuthors\\nAIKIDO is developed by the\\nPersonal Robotics Lab in the\\nPaul G. Allen School of Computer Science and Engineering at\\nthe University of Washington.\\nThe library was started by\\nMichael Koval (@mkoval)\\nand Pras Velagapudi (@psigen).\\nIt has received major contributions from\\nShushman Choudhury (@Shushman),\\nBrian Hou (@brianhou),\\nAaron Johnson (@aaronjoh),\\nJennifer King (@jeking),\\nGilwoo Lee (@gilwoolee),\\nJeongseok Lee (@jslee02),\\nand Clint Liddick (@ClintLiddick).\\nWe also would like to thank\\nMichael Grey (@mxgrey)\\nand Jeongseok Lee (@jslee02)\\nfor making changes to DART to better support AIKIDO.\\n'},\n",
       " {'language': 'C++ 61.7',\n",
       "  'readme': '⚡电磁AI智能车\\n\\n\\n\\n\\n\\n\\n采集数据发送至上位机、运行训练好的模型\\n✨FEATURE\\n\\n🚀纯电磁四轮\\n🚑基于状态机的出轨、堵转保护\\n🎨1.3吋OLED与6个按键的中文GUI\\n💥一键切换AI模式与PID模式！\\n🗃️可在运行时切换AI模型\\n🔧在Keil中通过GUI配置全车参数\\n⚡PID采用CMSIS-DSP库实现\\n⚡兼顾效率与灵活性的滑动平均滤波器\\n🔒运行时错误检查：CAR_ERROR_CHECK\\n⤴️极简调度器\\n🕒基于平衡二叉树的软件定时器\\n🔋低电量自动关机\\n♻️std::set_new_handler\\n\\n👀PREVIEW\\n控制台\\n\\n复位后打印的内容\\n\\n\\n开始\\n\\n图中柱子是电池电量，数字是“CPU占用率”\\n\\n\\n主页\\n\\n右上角数字代表“CPU占用率”的位数\\n\\n\\n模型选择\\n\\n\\n预存了3个model.nncu.c，可在运行时切换。\\n\\n控制面板\\n\\n\\n电机设置\\n\\n\\n方向控制\\n\\n数字电位器\\n\\n电磁传感器\\n\\n\\n\\n单击切换显示方式\\n\\n显示波形\\n\\n\\n配置参数\\n\\nLIBRARY\\n\\n致敬开源！\\n\\n\\nnncu\\nSimpleGUI\\n逐飞科技RT1064开源库\\nStateMachineCompact\\nMCUXpressoSDK\\nDSP\\n\\n📌DEPENDENCE\\n\\n开发环境：Keil 5.31\\n编译器：ArmClang V6.14\\n使用MCUXpresso Config Tool初始化时钟和外设\\n请使用Keil打开car_config.h\\n\\n十五届必胜 🎉 🎉 🎉\\n'},\n",
       " {'language': 'C++ 64.1',\n",
       "  'readme': 'Welcome to the modern version of Polyworld, an Artificial Life\\nsystem designed as an approach to Artificial Intelligence.\\nDocumentation is hosted on our wiki: https://github.com/polyworld/polyworld/wiki\\nFor installation instructions, please refer to the wiki page for your OS:\\n\\nLinux Installation\\nMac Installation\\n\\nNote that the Github repository at https://github.com/polyworld/polyworld is the official\\nhome of the Polyworld project, which was formerly hosted at\\nhttp://sourceforge.net/projects/polyworld.\\n'},\n",
       " {'language': 'C++ 80.9',\n",
       "  'readme': \"Quackle   \\n\\n\\nCrossword game artificial intelligence and analysis tool.\\nSee LICENSE in this directory.\\nBuilding Quackle:\\nQuackle is built and tested with the latest releases of Qt 5.12 and 5.13.\\nSee README.MacOS and README.Windows for platform-specific instructions.  Generally:\\nClone the repo or download the tarball and untar.  Use qmake to build quackle.pro and quackleio/quackleio.pro:\\nqmake quackle.pro && make\\ncd quackleio && qmake && make && cd ..\\n\\nFinally, build the main binary.\\ncd quacker && qmake && make\\n\\nThe binary will build as 'Quackle'.  It might be found in the quacker directory or in the release subdirectory.\\nFile organization:\\n\\nquackle/ - libquackle sources.  libquackle is the engine, and can be linked to any convenient interface.  It does not use Qt.\\nquackle/quackleio/ - I/O library for Quackle.  Implements stuff for accessing dictionaries, serializing GCG files, etc.  Also, command-line option handling.  This does have some modest dependencies on Qt.\\nquackle/quacker/ - code for full Quackle UI.  Written in Qt, and requires libquackleio and libquackle.\\nquackle/makeminidawg/ - standalone console program for building Quackle dictionaries.\\nquackle/makegaddag/ - standalone console program for building gaddag files.\\nquackle/data/ - lexicons, strategy files, and alphabet resources for Quackle.\\nIn this directory is libquackle. Run qmake and then run make in this directory. Then cd to quackle/quackleio/, run qmake, and then run make.\\n\\nolaughlin@gmail.com\\njasonkatzbrown@gmail.edu\\njfultz@wolfram.com\\nmatt.liberty@gmail.com\\n\"},\n",
       " {'language': 'C++ 87.6',\n",
       "  'readme': \"Physis Shard\\nPhysis Shard is a framework for developing learning systems.\\nIt defines basically two simple interfaces that allows easy addition of agents and problems.\\nIn other words, to add agents or problems they only need to implement these interfaces.\\nContents\\nAgents:\\n\\nSpectrum-diverse Unified Neuron Evolution Architecture (SUNA)\\nRandom agent (named Mysterious_Agent)\\nDummy agent (the user define its output every iteration)\\n\\nEnvironments (i.e., Problems):\\n\\nMountain Car\\nDouble Cart Pole (with and without velocities)\\nFunction Approximation\\nMultiplexer\\nSingle Cart Pole\\n\\nInstall\\nThis library depends on the zweifel library.\\nYou can install it from the git address:\\nhttps://github.com/zweifel/zweifel\\nAfter installing the zweifel library, its full path need to be defined in the Physis Shard's Makefile.\\nIn other words, change the following variable in the Makefile to point to zweifel library's correct installing location:\\nPATH_TO_ZWEIFEL_LIBRARY=/home/user/zweifel\\n\\nRun make afterwards:\\nmake\\n\\nIt should produce two executables: rl and rl_live.\\nTo run tests run:\\n./rl\\n\\nAt the end of the test, the best solution's dna will be saved with the name dna_best_individual\\nTo test this final solution run:\\n./rl_live dna_best_individual\\n\\nChanging Environments\\nEnvironment can be changed in main.cpp.\\nFor example commenting out where the Reinforcement_Environment is defined and\\nuncommenting the line with:\\nReinforcement_Environment* env= new Double_Cart_Pole(random);\\n\\nIf the environment should be terminated when the maximum steps is reached\\nuncomment the following in parameters.h:\\n#define TERMINATE_IF_MAX_STEPS_REACHED\\t\\t\\n\\nDo not forget to comment it out when surpassing the maximum number of steps is\\nnot a termination condition! For example, montain car does not need it while\\ndouble cart pole does.\\nChanging Parameters\\nMany parameters of the environment as well as of the agent can be changed by modifying some definitions\\nin parameters.h\\nRunning Experiments\\nTo run a trial until its maximum number of trials defined in main.cpp, run:\\n./rl\\n\\nTo test the best individual, run:\\n./rl_live dna_best_individual\\n\\nA series of trials can be run by using the script mean_curve.sh\\nAdding Agents or Problems\\nAn agent needs to implement the Reinforcement_Agent.h while a problem needs to implement the Reinforcement_Environment.h.\\nThere are simple examples of agents and problems inside respectively the agents/ and environments/ directories.\\nMost of the examples were built with the general reinforcement learning in mind, however they can be applied to supervised learning as well as unsupervised learning (e.g., consider the reward from the system as an error).\\nLicense\\nApache License Version 2.0\\n\"},\n",
       " {'language': 'C++ 84.1',\n",
       "  'readme': \"Russian AI Cup (not only Russian!)\\nArtificial intelligence programming contest. Official website: http://russianaicup.ru\\nRussian AI Cup — intitiative of the company Mail.Ru Group within the IT-oriented competitions.In this championship participants compete in skills of creating an artificial intelligence on example of game stratefies. Organizers are Mail.Ru Group and Codeforces. Best participants will receive special prizes.\\nRussian AI Cup — largest annual artificial intelligence programming contest in Russia, and third open competition for talented IT-specialists that is part of Mail.Ru Group strategy of forming and developing competitive Russian IT-industry on a global scale.\\nRAIC 2018: CodeBall\\nWe are pleased to welcome all the participants of the Russian AI Cup: CodeBall 2018 open beta!\\nThe beta test will run until 21:00 UTC on December 23. Please note that at this time we can make a significant changes in the rules, scoring system and any other aspects of the championship. This week we will try to fix possible mistakes, optimize performance and make any other necessary improvements. The ratings will be reset after this week of beta.\\nSome links on the website are not working. We will fix it within a few days.\\nGood luck and have fun!\\nUPDATE 19.12.18\\nHere you can read about updates.\\nUseful links\\n\\nAbout CodeBall. News, Notifications;\\nQuick start instructions. Other sources will be published soon on this repository and here;\\nPlay CodeBall in the browser.\\n\\nCommunity for English speakers (Discord)\\nWe are HERE. Join us!\\nCommunity for Russian speakers (Telegram)\\nHERE. Join us!\\nOfficial Contacts\\nAlso, we've email cups@corp.mail.ru. If you have any private questions, you can ask us. We always check our inbox and reply to all.\\n\"},\n",
       " {'language': 'C++ 97.1',\n",
       "  'readme': \"Russian AI Cup (not only Russian!)\\nArtificial intelligence programming contest. Official website: http://russianaicup.ru\\nRussian AI Cup — intitiative of the company Mail.Ru Group within the IT-oriented competitions.In this championship participants compete in skills of creating an artificial intelligence on example of game stratefies. Organizers are Mail.Ru Group and Codeforces. Best participants will receive special prizes.\\nRussian AI Cup — largest annual artificial intelligence programming contest in Russia, and third open competition for talented IT-specialists that is part of Mail.Ru Group strategy of forming and developing competitive Russian IT-industry on a global scale.\\nRAIC 2018: CodeBall\\nWe are pleased to welcome all the participants of the Russian AI Cup: CodeBall 2018 open beta!\\nThe beta test will run until 21:00 UTC on December 23. Please note that at this time we can make a significant changes in the rules, scoring system and any other aspects of the championship. This week we will try to fix possible mistakes, optimize performance and make any other necessary improvements. The ratings will be reset after this week of beta.\\nSome links on the website are not working. We will fix it within a few days.\\nGood luck and have fun!\\nUPDATE 19.12.18\\nHere you can read about updates.\\nUseful links\\n\\nAbout CodeBall. News, Notifications;\\nQuick start instructions. Other sources will be published soon on this repository and here;\\nPlay CodeBall in the browser.\\n\\nCommunity for English speakers (Discord)\\nWe are HERE. Join us!\\nCommunity for Russian speakers (Telegram)\\nHERE. Join us!\\nOfficial Contacts\\nAlso, we've email cups@corp.mail.ru. If you have any private questions, you can ask us. We always check our inbox and reply to all.\\n\"},\n",
       " {'language': 'C++ 98.9',\n",
       "  'readme': \"Endless Sky\\nExplore other star systems. Earn money by trading, carrying passengers, or completing missions. Use your earnings to buy a better ship or to upgrade the weapons and engines on your current one. Blow up pirates. Take sides in a civil war. Or leave human space behind and hope to find some friendly aliens whose culture is more civilized than your own...\\n\\nEndless Sky is a sandbox-style space exploration game similar to Elite, Escape Velocity, or Star Control. You start out as the captain of a tiny space ship and can choose what to do from there. The game includes a major plot line and many minor missions, but you can choose whether you want to play through the plot or strike out on your own as a merchant or bounty hunter or explorer.\\nSee the player's manual for more information, or the home page for screenshots and the occasional blog post.\\nInstalling the game\\nOfficial releases of Endless Sky are available on Steam and as direct downloads from GitHub. A PPA is available for Ubuntu and for Debian. Other package managers may also include the game, though the specific version provided may not be up-to-date.\\nSystem Requirements\\nEndless Sky has very minimal system requirements, meaning most systems should be able to run the game. The most restrictive requirement is likely that your device must support at least OpenGL 3.\\n\\n\\n\\n\\nMinimum\\nRecommended\\n\\n\\n\\n\\nRAM\\n350 MB\\n750 MB\\n\\n\\nGraphics\\nOpenGL 3.0\\nOpenGL 3.3\\n\\n\\nStorage Free\\n120 MB\\n300 MB\\n\\n\\n\\nBuilding from source\\nWhile most development is done on Linux using the SCons build tool to compile the project, IDE-specific files are provided for XCode and Code::Blocks to simplify the installation on Mac OS and Windows. It is possible to use other IDEs or build systems to compile the game, but support is not provided.\\nFor full installation instructions, consult the Build Instructions wiki page.\\nContributing\\nAs a free and open source game, Endless Sky is the product of many peoples' work. Contributions of artwork, storylines, and other writing are most in-demand, while there is a loosely defined development roadmap. Those who wish to contribute are encouraged to review the wiki, and to post in the discussion forum.\\nLicensing\\nEndless Sky is a free, open source game. The source code is available under the GPL v3 license, and all the artwork is either public domain or released under a variety of Creative Commons licenses. (To determine the copyright status of any of the artwork, consult the copyright file.)\\n\"},\n",
       " {'language': 'C++ 54.2',\n",
       "  'readme': \"\\n\\n\\n\\nPioneer Space Simulator\\n\\nPioneer is a space adventure game set in the Milky Way galaxy at the turn of\\nthe 31st century.\\nThe game is open-ended, and you are free to explore the millions of star\\nsystems in the game. You can land on planets, slingshot past gas giants, and\\nburn yourself to a crisp flying between binary star systems. You can try your\\nhand at piracy, make your fortune trading between systems, or do missions for\\nthe various factions fighting for power, freedom or self-determination.\\nFor more information, see:\\nhttp://pioneerspacesim.net/\\nCommunity\\nCome by #pioneer at irc.freenode.net and say hi to the team:\\nhttp://pioneerspacesim.net/irc\\nBugs? Please log an issue:\\nhttp://pioneerspacesim.net/issues\\nFollow Pioneer on Twitter:\\nhttps://twitter.com/pioneerspacesim/\\nPioneer wiki\\nhttp://pioneerwiki.com/wiki/Pioneer_Wiki\\nJoin the player's forum:\\nhttp://spacesimcentral.com/community/pioneer/\\nJoin the development forum:\\nhttp://pioneerspacesim.net/forum\\nManual\\nManual can be found at:\\nhttp://pioneerwiki.com/wiki/Manual\\nBasic flight:\\nhttps://pioneerwiki.com/wiki/Basic_flight\\nKeyboard and mouse control is found at:\\nhttp://pioneerwiki.com/wiki/Keyboard_and_mouse_controls\\nFAQ\\nFor frequently asked questions, please see\\nhttp://pioneerwiki.com/wiki/FAQ\\nBUG Reporting\\nPlease see the section of the FAQ pertaining to bugs, crashs and reporting other issues: Bug Reporting FAQs.\\nPlease do your best to fill out the issue template as completely as possible, especially when you're reporting a crash bug or a graphical issue. Having system information including graphics drivers and the method you used to install Pioneer helps immensely to diagnose and fix these kinds of issues.\\nContributing\\nIf you are hungry to contribute, more information can be found here:\\nhttp://pioneerwiki.com/wiki/How_you_can_contribute\\nIf you have a contribution you want to share, and want to learn how to make a\\npull request, see:\\nhttp://pioneerwiki.com/wiki/Using_git_and_GitHub\\nLocalization\\nLocalization for Pioneer is handled trough Transifex, and pulled to the source from there automatically. Because of this please don't make pull requests for translations. You can find the localization project here.\\nYou need to register at transifex to be able to access the translations.\\nIf you want a new language introduced, please request it on the Freenode IRC channel of Pioneer, or here by making an issue for it.\\nGetting Pioneer\\nLatest build is available at\\nhttps://pioneerspacesim.net/page/download/\\nFor compiling from source, please see COMPILING.txt\\nChangelog\\nPlease see Changelog.txt\\n\"},\n",
       " {'language': 'C++ 100.0',\n",
       "  'readme': 'Woozoolike\\nA simple space exploration roguelike for 7DRL 2017.\\nScreenshots\\n\\n\\nBuilds (Windows)\\n\\n7DRL Version(English) - Download\\nLatest Version(English) - Download\\nLatest Version(Korean) - Download\\n\\nContact\\n\\nDiscord: https://discord.gg/RhH3vyn\\n\\n'},\n",
       " {'language': 'C++ 67.8',\n",
       "  'readme': \"OSP (OpenGL + C++ Experiments)\\nThis repo hopes to become a space exploration game along the lines of Squad's Kerbal Space Program; as of today it's a place to experiment implementing different features in C++/OpenGL.\\nFeatures\\n(Those marked with a ✔️ are already implemented)\\n(Those marked with a ♻️ are ongoing or started)\\n(The list may be expanded at any time)\\n✔️ Keplerian Orbit Simulator for on-rails solar system\\n\\n✔️ Simulating elliptic orbits\\n♻️ Simulating parabolic and hyperbolic orbits (Not really neccesary for now)\\n\\nVessel Building and Controlling\\n\\nVessel assembly from a list of parts\\n\\n♻️ Procedural parts\\n\\n♻️ Procedural Engines\\n\\n✔️ Nozzle simulator\\nLiquid fueled engines\\nSolid fueled engines\\n\\n\\nProcedural structures (tanks, fuselage...)\\n\\n\\nPre-made parts\\n\\n\\nVessel controlling\\n\\n♻️ Navball\\n\\n✔️ Navball aligns with vessel\\n♻️ Navball aligns to reference frames\\n\\n\\nS.A.S\\n\\n\\n\\n♻️ Planetary Surfaces\\n\\nEither very big view distance or joining together multiple scaled cameras\\nRendering of planets from far away\\n\\nSimple billboard shader\\n\\n\\n♻️ Rendering of planets from the surface and near space\\n\\n♻️ Near space rendering\\n\\n♻️ Rocky bodies\\n\\n♻️ Cubesphere rendering\\n\\nVery complex cubespheres for asteroids and weird-shaped bodies\\nNot spheric planets (see above, could be related)\\nVery big scale shadow rendering (Per vertex?)\\n\\n\\nLOD\\n\\nSeamless LOD transitions\\nRemoving seams between quads\\n\\n\\nAtmospheres\\n\\nAtmospheric shader\\nSeamless transition from space to ground\\nClouds / Cloud shadows\\n\\n\\n\\n\\nOther bodies\\n\\nGas body rendering\\n\\nClouds and animations?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn-Vessel physics\\n\\nRigidbody physics simulator\\nExtraction of acceleration and rotation from the simulation\\nApplying forces to the vessel from outside (Gravitational gradient?)\\nSeamless transition from in-vessel physics to the orbit simulator\\n\\nMaybe they don't need to be separated, but perfomance could suffer\\n\\n\\nInteraction with the terrain system\\n\\n♻️ Newtonian Orbit Simulator for vessels\\n\\n✔️ Orbit propagation\\n✔️ Orbit predictor (threaded)\\n♻️ Maneouver planning\\nImproving perfomance so many vessels can be simulated at high warp speeds\\n\\nModding\\nThe engine is data-driven, so pretty much no hardcoded stuff.\\nCustom behaviour may be implemented using a scripting language (lua, squirrel...), or even a bigger language like C# (Mono)\\nScreenshots (may be outdated)\\n\\n\\n\\n\"},\n",
       " {'language': 'C++ 82.7',\n",
       "  'readme': 'Light-HLS: Fast, Accurate and Convenient\\nLet\\'s try to make HLS developemnt easier for everyone~ ^_^.\\nLight-HLS is a light weight high-level synthesis (HLS) framework for academic exploration and evaluation, which can be called to perform various design space exploration (DSE) for FPGA-based HLS design. It covers the abilities of previous works, overcomes the existing limitations and brings more practical features. Light-HLS is modularized and portable so designers can use the components of Light-HLS to conduct various DSE procedures.  Light-HLS gets rid of RTL code generation so it will not suffer from the time-consuming synthesis of commercial HLS tools like VivadoHLS, which involves many detailed operations in both its frond-end and back-end, but can accurately estimate timing, resource and some other results of commercial tools for applications.\\nIf Light-HLS helps for your works, please cite our paper in ICCAD 2019 ^_^:\\nT. Liang, J. Zhao, L. Feng, S. Sinha and W. Zhang, \"Hi-ClockFlow: Multi-Clock Dataflow Automation and Throughput Optimization in High-Level Synthesis,\" 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), Westminster, CO, USA, 2019, pp. 1-6. doi: 10.1109/ICCAD45719.2019.8942136\\n\\nA well-organzied Wiki can be find here. Since we are still developing this project and there could be some bugs and issues, if you have any problems, PLEASE feel free to let us know: ( tliang@connect.ust.hk ), for which we will sincerely appreciate ^_^. We strongly recommand to you send us an email so we can add you into our maillist for the latest information of Light-HLS, because Light-HLS is a young tool and continuously updated to add features and fix bugs. This is a young project and if you want to join us, we are happy to make it a better one togather! \\\\^_^/ Here are some known issues raised by our users, which we are handling one by one.\\n\\nLight-HLS Frond-End\\nThe goal of Light-HLS frond-end is to generate IR code close enough to those generated via commercial tools, like VivadoHLS, for DSE purpose. In the front-end of Light-HLS, initial IR codes generated via Clang will be processed by HLS optimization passes consisted of three different levels: (a) At instruction level, Light-HLS modifies, removes or reorders the instructions, e.g. reducing bitwidth,  removing redundant instruction  and reordering computation. (b) At loop/function level, functions will be instantiated and loops may be extracted into sub-functions. (c) As for memory access level, redundant load/store instructions will be removed based on dependency analysis.\\nLight-HLS Back-End\\nThe back-end of Light-HLS is developed to schedule and bind for the optimized IR codes, so it can predict the resultant performance and resource cost accurately based on the given settings. The IR instructions can be automatically characterized by Light-HLS and a corresponding library, which records the timing and resource of different types of operations, will be generated. For scheduling, based on the generated library, Light-HLS maps most operations to corresponding cycles based on as-soon-as-possible (ASAP) strategy. For some pipelined loops, the constraints of the port number of the BRAMs and loop-carried dependencies are considered. Moreover, some operations might be scheduled as late as possible (ALAP) to lower the II. \\tAs for resource binding, Light-HLS accumulates the resource cost by each operation and the chaining of operations is considered. The reusing of hardware resource is an important feature in HLS and Light-HLS reuses resources based on more detailed rules, e.g. the source and type of input.\\nLight-HLS Application Scenariors\\nLet\\'s first see what we can do with Light-HLS in the research about HLS.\\n\\n\\nHLS designs can be set with various configurations, leading to different results of performance and resource. To find the optmial solution, designers can determine the configuration and call Light-HLS to predict the result in tens milliseconds, which will be close to the result in VivadoHLS. An example is Hi-ClockFlow, a tool which searches for the configuration of clock settings and HLS directives for the multi-clock dataflow.\\n\\n\\nHLS designs are sensitive to the source codes, some of which are friendly to FPGA while the others are not. If researchers want to analyze and optimize the design at source code level, Light-HLS have accomplished the back-tracing from back-end, to front-end, to source code, so researchers can find out which part of source code have some interesting behaviors causing problems. In the example Hi-ClockFlow, Light-HLS helps to partition the source code and map the performance and resource to the different parts of the source code.\\n\\n\\nIn the front-end of HLS, source code will be processed by a series of LLVM Passes for analysis and optimization. However, for most of researchers, even if they come up with an idea for the front-end processing, they can hardly estimate the exact outcome of the solution if it can be applied to the commercial HLS tools. Currently, Light-HLS can generate IR code similar to the one generated by VivadoHLS and provide accurate scheduling and resource binding in back-end. Therefore, researchers might implement a Pass, plug it into the front-end of Light-HLS (Yes, just plug ^_^), and evaluate it with the back-end of Light-HLS to see the effect.\\n\\n\\nIn the back-end of HLS, the IR instructions/blocks/loops/functions are scheduled and binded to specific hardware resource on FPGA. Based on the IR code similar to the one generated by commercial tools, how to properly schedule the source code and bind the resource can be tested and analyzed with Light-HLS. Currently, Light-HLS can provide the estimated performance and resource cost close to those from VivadoHLS 2018.2. (We will catch up the version of 2019.2 recently.) Light-HLS can generate the library of the timing and resource cost of all the IR instructions, e.g. add, fmul, MAC, fptoui and etc, for a specified devive, like Zedboard. Researchers can change the original scheme of Light-HLS\\'s scheduling and binding to see the effect.\\n\\n\\nCategory:\\nInstallation of Light-HLS\\nUsage of Light-HLS\\nImplementation of Light-HLS and Further development\\nNotes for You to Create Your Own HLS Tools with LLVM\\nGood Good Study Day Day Up (^o^)/~\\n'},\n",
       " {'language': 'C++ 89.0',\n",
       "  'readme': 'Molpher-lib: Introduction\\nThis C++/Python library is a chemical space exploration software. It is based on the Molpher program which introduced a chemical space exploration method called molecular morphing. The original Molpher method uses stochastic optimization to traverse chemical space between two existing molecules. The main promise of this algorithm is that a virtual library enriched in compounds with improved biological activity could be generated in this way.\\nThe purpose of Molpher-lib is to bring molecular morphing closer to the cheminformatics community, but also offer new features that go beyond the capabilities of the original Molpher program. Molpher-lib makes it possible to roam the chemical universe freely and with little constraints on the inputs. For example, we could just use a carbon atom as a starting point and have Molpher-lib autonomously evolve it into a complete molecular structure. To ensure that the generated molecules have required properties, Molpher-lib also helps with implementation of custom rules and constraints. If you want to know more about Molpher-lib and its usage, make sure to check out some examples on the website. We also have some Jupyter notebooks with examples\\nthat you can explore.\\nIf you would like to participate in the development or just check out the current features of the library, there is extensive documentation which can help you. A big part of the documentation is dedicated to a detailed tutorial that should introduce the philosophy of Molpher-lib in more detail and give you a good idea of what it is currently capable of.\\nThe library is actively developed and many new features are planned to be added. The long-term goal is to make Molpher-lib a universal and easy-to-use de novo drug design framework. Ideas, comments and feature requests are more than welcome and can be submitted to the issue tracker. You can also subscribe to the RSS feed of the dev branch for development updates. If you want to know what is new in the current version, you can look at the changelog.\\nInstallation\\nSupported platforms:\\n\\nLinux 64-bit\\n\\nAt the moment, the library binaries are only compiled for 64-bit Linux systems. However, development for other platforms is also planned. If you manage to compile the library on a different platform, consider making a pull request or comment on the issue tracker. Any help is much appreciated.\\nInstallation with Anaconda\\nMolpher-lib is distributed as a conda package. At the moment, this is the preferred way to install and use the library. All you need to do is get the full Anaconda distribution or its lightweight variant, Miniconda. It is essentially a Python distribution, package manager and virtual environment in one and makes setting up a development environment for any project very easy. After installing Anaconda/Miniconda you can run the following in the Linux terminal:\\nconda install -c rdkit -c lich molpher-lib\\nThis will automatically download the latest version of the library and install everything to the currently active environment (for more information on environments and the conda command see Conda Test Drive). The library depends on the popular cheminformatics toolkit RDKit so do not forget to add the rdkit channel.\\nIf you are interested in the development snapshots of the library\\n(most up to date code, but can contain bugs)\\n, you can use the dev channel instead:\\nconda install -c rdkit -c lich/label/dev molpher-lib\\nAfter that the library should import in your environment and you should be able to successfully run the integrated unit tests:\\nfrom molpher.tests import run\\n\\nrun()\\nYou can also check out the Jupyter notebooks with examples from the documentation.\\nCompiling from Source\\nCompiling and installing from source is a little bit more elaborate. This process is described in detail in the documentation, but in the simplest case the following should work:\\n# get dependencies\\nsudo apt-get install git build-essential python3-dev python3-numpy cmake python3-setuptools\\n\\n# clone the repo\\ngit clone https://github.com/lich-uct/molpher-lib.git\\ngit checkout dev # or the branch/tag/commit you want\\nREPOSITORY_ROOT=`pwd`/molpher-lib\\n\\n# this might take a while, but you if you are lucky, \\n# cmake might be able to find dependencies \\n# if you already have them somewhere on your system\\n# so you can skip this step if you have TBB, Boost and RDKit\\n# installed at standard locations\\ncd ${REPOSITORY_ROOT}/deps\\n./build_deps.sh --all\\n\\n# finally, build the library itself\\ncd ${REPOSITORY_ROOT}\\nmkdir cmake-build\\ncd cmake-build\\ncmake .. -DCMAKE_BUILD_TYPE=Debug -DPYTHON_EXECUTABLE=python3\\nmake molpher_install_python\\nAfter setting the appropriate variables:\\nexport CMAKE_INSTALL_PREFIX=\"${REPOSITORY_ROOT}/dist\"\\nexport DEPS_DIR=${CMAKE_INSTALL_PREFIX}/../deps\\nexport PYTHONPATH=${DEPS_DIR}/rdkit/:${CMAKE_INSTALL_PREFIX}/lib/python3.5/site-packages\\nexport LD_LIBRARY_PATH=${DEPS_DIR}/tbb/lib/intel64/gcc4.7:${DEPS_DIR}/rdkit/lib/:${DEPS_DIR}/boost/stage/lib:${CMAKE_INSTALL_PREFIX}/lib\\nyou should be good to go:\\npython3\\nfrom molpher.tests import run\\n\\nrun()\\nThis will run the integrated unit tests. They should all pass without problems.\\nIf you want to explore some example code from the documentations, there are\\na few Jupyter notebooks located under doc/notebooks. You can create\\nthe needed conda environment\\nand launch your Jupyter server as follows:\\ncd ${REPOSITORY_ROOT}\\nconda env create -f \"environment.yml\"\\n. source_2_activate\\npython setup.py build_ext --inplace\\ncd doc/notebooks/\\njupyter-notebook\\nNote that you will need to have the library already compiled and installed in the standard\\n${REPOSITORY_ROOT}/dist directory.\\nThis installation process has been tested on common Debian-based systems so experience on other Linux flavors may differ. If you run into problems, report them to the issue tracker and hopefully someone will be able to help.\\n'},\n",
       " {'language': 'C++ 68.7',\n",
       "  'readme': '#SYNCHROTRACE (Now deprecated. Please use https://github.com/dpac-vlsi/SynchroTrace-gem5 )\\nThere are two tools which together form the prototype SynchroTrace simulation flow built into Gem5.\\n1) Sigil - Multi-threaded Trace Capture Tool\\n2) Replay - Event-Trace Replay Framework\\nThis code base includes (2) Replay.\\nCurrently, the Sigil version required to generate traces is provided separately from the same github user \"dpac-vlsi\".\\nThe logical steps to using this simulation environment for design space exploration or CMP simulation is as follows:\\n1)\\na) Generate Multi-threaded Event Traces for the program binary you are testing (See Sigil documentation for further information):\\n-Use the Sigil wrapping script (runsigil_and_gz_newbranch.py) with necessary options on your binary\\nOR\\nb) Use previously generated or sample traces\\n2) Compile SynchroTrace (For a list of dependencies, please look in the Additional Notes section below)\\n3) Run SynchroTrace with necessary options on the generated traces\\n#####Installing SynchroTrace and Running the 8 Thread FFT Example\\n\\n\\nCheck Necessary Dependencies:\\ngcc-4.4.7\\ngmp-5.1.1\\nmpc-1.0\\nmpfr-3.1.2\\nswig-2.0.1\\npython-2.7.6\\nscons-2.3.0\\nzlib-dev or zlib1g-dev\\nm4\\n\\nPlease note:\\na) Runtime problems have been encountered when using later versions of swig and gcc.\\n\\nb) Please ensure /usr/bin/gcc-4.4 is symbolically linked to your defaultgcc (/usr/bin/gcc)\\nc) gmp, mpc, and mpfr are packaged with gcc when using package installer such as apt.\\nd) swig-2.0.1 can be found at http://sourceforge.net/projects/swig/files/swig/swig-2.0.1/\\ne) Currently we do not provide any means to automatically install the missing packages.\\n\\n\\nBuild SynchroTrace\\n\\n\\na) If not done so, clone the SynchroTrace repo from GitHub:\\n$ git clone https://github.com/dpac-vlsi/SynchroTrace\\nb) Go to the base SynchroTrace directory\\nc) Run the following command (Note that the number of jobs refers to the number of cores available for compilation):\\n     scons build/X86_MESI_CMP_directory/gem5.opt --jobs=6\\nAt this point, the gem5 executable should be built with integrated Trace Replay in the location specified in the command above.\\ngem5 is usually run with a configuration script that hooks up the various architecture models packaged with the gem5 framework.\\nWe have written a SynchroTrace configuration script to which arguments can be passed to configure the desired system.\\nThis script can be found at <SYNCHROTRACE_FOLDER>/configs/synchrotrace/synchrotrace.py\\nThe run_synchrotrace_fft.pl run script can be used to run the FFT example simply as follows:\\n$ ./run_synchrotrace_fft.pl\\nThis script can be modified and emulated to run your own configuration.\\nA different trace location by changing the $eventDir variable.\\nThis design being simulated is set by changing the arguments provided in the $synchrotracecmd variable.\\nFor a list of valid arguments, run the following from the main SynchroTrace folder:\\n$ ./build/X86_MESI_CMP_directory/gem5.opt ./configs/synchrotrace/synchrotrace.py --help\\nd) Once the run is completed, simulated metrics can be found in m5out/stats.txt and m5out/ruby.stats\\n####################################################################################################################################\\n#####Additional Notes:\\n\\n\\nSample Sigil Traces are located in $BASESYNCHROTRACEDIR/sample_sigil_traces\\n\\n\\nSynchroTrace configurations are located in $BASESYNCHROTRACEDIR/configs/synchrotrace/synchrotrace.py and $BASESYNCHROTRACEDIR/configs/ruby/Ruby.py\\n\\n\\nThe run_synchrotrace_fft.pl run script has a section for debug flags. The following is a list of the available debug flags used by SynchroTrace with brief descriptions.\\n-DebugFlag(\\'mutexLogger\\') - Prints order of threads obtaining mutex lock\\n-DebugFlag(\\'printEvent\\') - Prints EventID# for specific thread before/after event started/completed. This debug flag makes the simulation time very slow.\\n-DebugFlag(\\'printEventFull\\') - Prints EventIDs for Threads, Threads on what Cores every 50k cycles\\n-DebugFlag(\\'cacheMiss\\') - Prints out cache misses as they happen and address\\n-DebugFlag(\\'memoryInBarrier\\') - prints memory reads, writes, read bytes, write bytes every barrier\\n-DebugFlag(\\'flitsInBarrier\\') - prints flits generated every barrier\\n-DebugFlag(\\'l1MissesInBarrier\\') - prints l1 misses per thread every barrier\\n-DebugFlag(\\'latencyInBarrier\\') - prints 3 lines. # packets in barrier, Accumulated queueing delay in barrier, Accumulated network latency in Barrier.\\n-DebugFlag(\\'powerStatsInBarrier\\') - prints the total router power specifically for that barrier, i.e. not a rolling average.\\n-DebugFlag(\\'roi\\') - Prints out the cycle when we reach the parallel region in Debate. Prints out when the threads all join up.\\n-DebugFlag(\\'netMessages\\') - Prints the network packet messages out at 10k cycle buckets.\\n-DebugFlag(\\'amTrace\\') - Original default debug flag.\\n\\n\\nAn example of this command with a debug flag is as follows:\\n\\n\\n./build/X86_MESI_CMP_directory/gem5.opt --debug-flags=printEventFull ./configs/synchrotrace/synchrotrace.py --garnet-network=fixed --topology=Mesh --mesh-rows=8 --eventDir=$eventDir --outputDir=$outputDir --num-cpus=8 --num_threads=8 --num-dirs=8 --num-l2caches=8 --l1d_size=8kB --l1d_assoc=16 --l1i_size=8kB --l1i_assoc=2 --l2_size=128kB --l2_assoc=4 --cpi_iops=1 --cpi_flops=2 --bandwidth_factor=4 --l1_latency=3 --masterFreq=1 2> fft.err\";\\nwhere the $eventDir points to the directory of the traces and $outputDir points to the desired output directory path.\\n'},\n",
       " {'language': 'C++ 99.2',\n",
       "  'readme': 'Finite Galaxy\\nFinite Galaxy is a free and open source space exploration game; the repository containing all files is located at https://github.com/finite-galaxy/finite-galaxy/\\nIt is derived from Endless Sky, a game created by Michael Zahniser, which is located at https://github.com/endless-sky/endless-sky/\\nBoth games can be installed alongside and played independently of each other. Although derived from the same source code and basically using the same content, Finite Galaxy and Endless Sky have diverged and are no longer compatible. If you transpose a save game from one to the other, you are likely to encounter hundreds of errors.\\nTable of contents\\n\\nInstallation\\n\\nGNU/Linux\\nApple/Mac OS X\\nMicrosoft Windows\\n\\n\\nIntroduction\\nChanges\\n\\nMajor changes\\nMinor changes\\nNot yet implemented ideas\\n\\n\\nContributing\\n\\nHelp wanted\\nPosting issues\\nPosting pull requests\\n\\n\\nReveal entire map\\n\\nInstallation\\nGNU Linux\\nOpen your terminal and enter:\\n\\nto install dependencies:\\n\\non ArchLinux: pacman -S --needed git gcc scons sdl2 libpng libjpeg-turbo mesa glew openal libmad pango ttf-linux-libertine\\non Debian/Ubuntu: sudo apt-get install git g++ scons libsdl2-dev libpng-dev libjpeg-dev libgl1-mesa-dev libglew-dev libopenal-dev libmad0-dev libpango fonts-linuxlibertine\\non Fedora/RHEL/CentOS: sudo dnf install git gcc-c++ scons SDL2-devel libpng-devel libjpeg-turbo-devel mesa-libGL-devel glew-devel openal-soft-devel libmad-devel pango linux-libertine-fonts (replace dnf with yum on some versions).\\n\\n\\ngit clone https://github.com/finite-galaxy/finite-galaxy.git to get a local copy of the repository.\\ncd finite-galaxy/ to open the directory.\\ngit pull to update the game.\\nscons to compile the game.\\n./finite-galaxy to run the game.\\n\\nFor more help, consult the man page (the finite-galaxy.6 file).\\n(return to top)\\nApple Mac OS X\\nIf you have trouble compiling or encounter errors, please post here.\\nTo build Finite Galaxy, you will first need to download Xcode from the App Store.\\nNext, install Homebrew (from http://brew.sh).\\nOnce Homebrew is installed, use it to install the libraries you will need:\\nbrew install libpng\\nbrew install libjpeg-turbo\\nbrew install libmad\\nbrew install sdl2\\nbrew install pango\\n\\nIf the versions of those libraries are different from the ones that the Xcode project is set up for, you will need to modify the file paths in the “Frameworks” section in Xcode.\\nIt is possible that you will also need to modify the “Header Search Paths” and “Library Search Paths” in “Build Settings” to point to wherever Homebrew installed those libraries.\\nLibrary paths\\nTo create a Mac OS X binary that will work on systems other than your own, you may also need to use install_name_tool to modify the libraries so that their location is relative to the @rpath.\\nsudo install_name_tool -id \"@rpath/libpng16.16.dylib\" /usr/local/lib/libpng16.16.dylib\\nsudo install_name_tool -id \"@rpath/libmad.0.2.1.dylib\" /usr/local/lib/libmad.0.2.1.dylib\\nsudo install_name_tool -id \"@rpath/libturbojpeg.0.dylib\" /usr/local/opt/libjpeg-turbo/lib/libturbojpeg.0.dylib\\nsudo install_name_tool -id \"@rpath/libSDL2-2.0.0.dylib\" /usr/local/lib/libSDL2-2.0.0.dylib\\nsudo install_name_tool -id \"@rpath/pango-1.44.7.dylib\" /usr/local/lib/pango-1.44.7.dylib\\n\\n(return to top)\\nMicrosoft Windows\\nIf you have trouble compiling or encounter errors, please post here.\\n\\nAcquire the files with git clone https://github.com/finite-galaxy/finite-galaxy.git or click “Download ZIP” and extract it.\\nIf you don\\'t have it already, open finite-galaxy/fonts/LinLibertine_DRah.ttf and install the font (or copy it to the appropiate location).\\nFor building the game, see https://github.com/endless-sky/endless-sky/wiki/BuildInstructions#Windows\\n\\n(return to top)\\nIntroduction\\nWhy did I start this project? Why not contribute to Endless Sky instead?\\n\\nAlthough I like Endless Sky as a whole, it also contains things I don\\'t like. Conversation scenes, news and portraits, tribute, and plundering of installed outfits are just a few examples.\\nThe original creator, Michael Zahniser, seemed to disappear and pace of development appeared to slow down: there were only nine commits in September 2018 and zero in October. (I started Finite Galaxy on October 18.)\\nNumerous pull requests over there have been open for over a year, reviews are haphazard, there are many lengthy discussions on unimportant things, while useful proposals were often ignored.\\nThe direction and vision is not always clear.\\nSupport for plug-ins is rather limited.\\n\\nIn short, I consider it a better use of time to work on a project where I can incorporate most of my ideas, where I can remove things I dislike, and where I can contribute whenever I like, without having to wait weeks or months for a review or wasting my time on something that won\\'t be included.\\nFinite Galaxy is very much a work in progress. Nevertheless, it can be compiled and played without errors. Feel free to try it out yourself!\\n(return to top)\\nChanges\\nMajor changes\\n\\nHyperjump fuel is based on your ship\\'s effective mass (including cargo and carried ships)\\nHyperjump fuel is no longer free, its price depends on the planet (when landing) or the government (when hailing ships in space)\\nShips continuously consume energy, based upon the number of bunks, to represent life-support\\nShip categories are based on total mass\\n\\nship mass = hull mass + outfit space + cargo space\\n\\n\\nIntroduced core space, reserved for energy generators, shields and hull systems, and hyperdrives\\n\\noutfit space = core space + engine space + weapon space\\n\\n\\nInstalled outfits can no longer be plundered by default; outfits in cargo still can\\nMinimum depreciation value raised to 50%, time lowered to one year\\nWeapon projectile damage is a random number between damage and damage + random damage\\nGuns fire in parallel by default, i.e. no harmonized angle convergence.\\nShip info display shows more stats\\nRedistributed most human ships and many outfits to have more regional specialization\\nRemoved tribute from planets (relevant code is still present for plug-ins)\\nRemoved news and portraits (relevant code is still present for plug-ins)\\nDistances from planets to the sytem\\'s centre are trebled; as a result space feels larger, thrusters are more desirable, and players won\\'t always land immediately in the middle of a space fight\\nNon-missile weapons have their weapon range increased by about a third\\nExploding ships are significantly more dangerous\\nAdd support for Unicode and different writing directions\\n\\n(return to top)\\nMinor changes\\nSee changelog.txt, ship_overview.txt, and https://github.com/finite-galaxy/finite-galaxy/commits/master\\n(return to top)\\nNot yet implemented ideas\\n\\nAdd quotation mark preference\\nAdd “Licences” tab in player info panel\\nAdd “Tribute” tab in player info panel\\nAdd “Manufacturer” to ships\\nAdd functionality to deposit credits at the bank for a fixed time (e.g. one year), receiving either the sum plus interest when it expires, or the sum minus a penalty when you claim it beforehand\\nAdd planet landing fees support\\nAllow friendly fire\\nAllow sorting available jobs (by e.g. cargo size, distance, name, payment, etc.)\\nAllow sorting outfits by cost, mass, name\\nAllow sorting ships by cost, mass, name, outfit space, shields, etc.\\nDisplay flagship speed by default and display target\\'s speed with tactical scanner\\nDe-hardcode jump radius\\nIncrease jump radius if you have multiple jump drives installed, perhaps 100*(jump drive)^0.5\\nLimit the commodities for sale on specific planets\\nMake ship explosion ‘weapon’ automatically proportional to mass (base, empty, or total mass)\\nSeparate fleet overview column in outfitter and shipyard from ship info display\\nSeparate slots for guns and missile launchers\\nShips entering a system from hyperspace should be positioned near the system\\'s centre, instead of near the first inhabited planet\\nhttps://github.com/endless-sky/endless-sky/wiki/DevelopmentRoadmap\\n\\n(return to top)\\nContributing\\nContributions are welcome; anyone can contribute; feel free to open issues or make pull requests.\\nHelp wanted\\nCode:\\n\\nDe-hardcode Drone/Fighter classes (currently a boolean is used), to allow for multiple, customizable fighter/bay types (e.g. small, medium, large).\\nDe-hardcode hardpoint slots (currently a boolean is used), to allow for multiple, customizable hardpoint types (e.g. gun slots, missile bays, turret mounts).\\nImplementing the ideas listed above.\\nOther new mechanics that make the game more enjoyable.\\nUpdate code to C++17.\\n\\nArt:\\nBecause my Blender skills are non-existent, I could use help from people who are capable and willing to:\\n\\nMake new outfit sprites and turret hardpoints (e.g. six-gun blaster turret).\\nModify existing ship sprites and thumbnails.\\nProduce ship thumbnails for ships that only have sprites.\\nCreate new ships.\\nSee open issues.\\n\\nMiscellaneous:\\nUnfortunately I\\'m unable to test things on platforms other than my own (Fedora Linux). BSD, MacOS X, and Windows users could help by trying compiling and running the game, and if necessary, correct the appropiate files accordingly.\\n(return to top)\\nPosting issues\\nThe issues page on GitHub is for tracking bugs and for art, content, and feature requests. When posting a new issue, please:\\n\\nBe polite and always assume good faith.\\nCheck to make sure it\\'s not a duplicate of an existing issue.\\nCreate a separate “issue” for each bug, problem, question, or request.\\n\\nIf requesting a new feature, first ask yourself: will this make the game more fun or interesting? Remember that this is a game, not a simulator. Changes will not be made purely for the sake of realism, especially if they introduce needless complexity or aggravation.\\nIf you believe your issue has been resolved, you can close the issue yourself.\\n(return to top)\\nPosting pull requests\\nIf you are posting a pull request, please:\\n\\nDo not combine multiple unrelated changes into a single pull.\\nCheck the diff and make sure the pull request does not contain unintended changes.\\nIf proposing a major pull request, start by posting an issue and discussing the best way to implement it. Often the first strategy that occurs to you will not be the cleanest or most effective way to implement a new feature.\\ncode/:\\n\\nfollow the coding standard.\\nC++14\\ndo not use tabs; use two spaces instead\\nmake numbers with many digits easier to read for humans by inserting \\'\\n\\n(decimal numbers) at intervals of three digits if there are more than four in a row\\n(hexadecimal numbers) at intervals of four digits if there are more than six in a row\\n\\n\\nuse Oxford English\\n\\n\\ndata/:\\n\\ndo not use tabs; use two spaces instead\\nuse Oxford spelling (the variant of English used by many scientific journals and international organizations such as the United Nations), instead of American, British, Canadian, or other national varieties.\\nno diacritics in English:\\n\\ná, à, â → a; same for other vowels\\nå → aa\\næ, ä → ae\\nœ, ø, ö → oe\\nü → ue\\nİ/ı → I/i\\nç → c\\nč, ć, ċ → ch\\nš, ś, ş → sh\\nñ → nh\\nß → ss\\n\\n\\ntext strings (conversations, descriptions, mission dialogues, tooltips, etc.):\\n\\navoid abbreviations (e.g., i.e., etc.); contractions (isn\\'t) are fine\\nuse an Oxford comma when giving more than two items (e.g. one, two, and three; not one, two and three)\\nuse U+2013 – en-dash for number ranges (e.g. 10–12) and for parenthetical expressions – like this – instead of parentheses, em-dashes, or hyphens\\nuse U+2026 … horizontal ellipsis instead of three full stops\\nuse U+202F \\u202f narrow non-breaking space as a thousands separator for numbers with five or more digits (e.g. 12\\u202f345)\\nuse U+2212 − minus sign for negative numbers, subtractions, and deductions\\n\\n\\nuse the \" quote for direct speech and \\' apostrophe within direct speech; the source code replaces these with proper “primary” and ‘secondary’ opening and closing quotation marks; surround such strings with ` backticks\\n\\n\\nrepeatedly check and double check any new or changed strings to avoid unnecessary typos; e.g. mind the difference between it\\'s (cf. he\\'s, she\\'s) and its (cf. his, her).\\n\\n\\nimages/:\\n\\nfile names are lower case and use underscores instead of spaces\\nadd both normal and @2x versions\\n\\nfor ships, also create thumbnails\\nfor turrets, also create hardpoints\\n\\n\\ninsert yourself in the copyright.txt file\\ninclude all assets (Blender, GIMP, other files) in the opening post\\n\\n\\nsounds/:\\n\\nfile names are lower case and use underscores instead of spaces\\ninsert yourself in the copyright.txt file\\n\\n\\n\\n(return to top)\\nReveal entire map\\nPart of the fun of the game is travelling around and exploring. However, if you don\\'t have time for that and simply want to reveal everything in the entire galaxy, then open your save game, find # What you know: and insert the following lines directly afterwards:\\nvisited \"1 Axis\"\\nvisited \"10 Pole\"\\nvisited \"11 Autumn Above\"\\nvisited \"11 Spring Below\"\\nvisited \"12 Autumn Above\"\\nvisited \"14 Pole\"\\nvisited \"14 Summer Above\"\\nvisited \"14 Winter Below\"\\nvisited \"16 Autumn Rising\"\\nvisited \"3 Axis\"\\nvisited \"3 Pole\"\\nvisited \"3 Spring Rising\"\\nvisited \"4 Axis\"\\nvisited \"4 Spring Rising\"\\nvisited \"4 Summer Rising\"\\nvisited \"4 Winter Rising\"\\nvisited \"5 Axis\"\\nvisited \"5 Spring Below\"\\nvisited \"5 Summer Above\"\\nvisited \"5 Winter Above\"\\nvisited \"7 Autumn Rising\"\\nvisited \"8 Winter Below\"\\nvisited \"9 Spring Above\"\\nvisited \"Ablodab\"\\nvisited \"Ablub\"\\nvisited \"Acamar\"\\nvisited \"Achernar\"\\nvisited \"Acrux\"\\nvisited \"Adhara\"\\nvisited \"Aescolanus\"\\nvisited \"Al Dhanab\"\\nvisited \"Albaldah\"\\nvisited \"Albireo\"\\nvisited \"Alcyone\"\\nvisited \"Aldebaran\"\\nvisited \"Alderamin\"\\nvisited \"Aldhibain\"\\nvisited \"Algedi\"\\nvisited \"Algenib\"\\nvisited \"Algenubi\"\\nvisited \"Algieba\"\\nvisited \"Algol\"\\nvisited \"Algorel\"\\nvisited \"Alheka\"\\nvisited \"Alhena\"\\nvisited \"Alioth\"\\nvisited \"Alkaid\"\\nvisited \"Almaaz\"\\nvisited \"Almach\"\\nvisited \"Alnair\"\\nvisited \"Alnasl\"\\nvisited \"Alnilam\"\\nvisited \"Alnitak\"\\nvisited \"Alniyat\"\\nvisited \"Alpha Arae\"\\nvisited \"Alpha Centauri\"\\nvisited \"Alpha Hydri\"\\nvisited \"Alphard\"\\nvisited \"Alphecca\"\\nvisited \"Alpheratz\"\\nvisited \"Altair\"\\nvisited \"Aludra\"\\nvisited \"Ancient Hope\"\\nvisited \"Ankaa\"\\nvisited \"Answer\"\\nvisited \"Antares\"\\nvisited \"Antevorta\"\\nvisited \"Ap\\'arak\"\\nvisited \"Arcturus\"\\nvisited \"Arculus\"\\nvisited \"Arneb\"\\nvisited \"Ascella\"\\nvisited \"Asikafarnut\"\\nvisited \"Aspidiske\"\\nvisited \"Atria\"\\nvisited \"Avior\"\\nvisited \"Aya\\'k\\'k\"\\nvisited \"Beginning\"\\nvisited \"Bellatrix\"\\nvisited \"Belonging\"\\nvisited \"Belug\"\\nvisited \"Belugt\"\\nvisited \"Beta Lupi\"\\nvisited \"Betelgeuse\"\\nvisited \"Bloptab\"\\nvisited \"Blubipad\"\\nvisited \"Blugtad\"\\nvisited \"Boral\"\\nvisited \"Bore Fah\"\\nvisited \"Bote Asu\"\\nvisited \"Bright Void\"\\nvisited \"Broken Bowl\"\\nvisited \"Caeculus\"\\nvisited \"Canopus\"\\nvisited \"Capella\"\\nvisited \"Caph\"\\nvisited \"Cardax\"\\nvisited \"Cardea\"\\nvisited \"Castor\"\\nvisited \"Cebalrai\"\\nvisited \"Celeborim\"\\nvisited \"Chalawan\"\\nvisited \"Charm\"\\nvisited \"Chikatip\"\\nvisited \"Chimitarp\"\\nvisited \"Chirr\\'ay\\'akai\"\\nvisited \"Chornifath\"\\nvisited \"Chy\\'chra\"\\nvisited \"Cinxia\"\\nvisited \"Coluber\"\\nvisited \"Companion\"\\nvisited \"Convector\"\\nvisited \"Cor Caroli\"\\nvisited \"Da Ent\"\\nvisited \"Da Lest\"\\nvisited \"Dabih\"\\nvisited \"Danoa\"\\nvisited \"Dark Hills\"\\nvisited \"Debrugt\"\\nvisited \"Delta Capricorni\"\\nvisited \"Delta Sagittarii\"\\nvisited \"Delta Velorum\"\\nvisited \"Deneb\"\\nvisited \"Denebola\"\\nvisited \"Diphda\"\\nvisited \"Dokdobaru\"\\nvisited \"Dschubba\"\\nvisited \"Dubhe\"\\nvisited \"Due Yoot\"\\nvisited \"Durax\"\\nvisited \"Eber\"\\nvisited \"Eblumab\"\\nvisited \"Edusa\"\\nvisited \"Ehma Ti\"\\nvisited \"Ek\\'kek\\'ru\"\\nvisited \"Ekuarik\"\\nvisited \"Elnath\"\\nvisited \"Eltanin\"\\nvisited \"Eneremprukt\"\\nvisited \"Enif\"\\nvisited \"Es\\'sprak\\'ai\"\\nvisited \"Eshkoshtar\"\\nvisited \"Eteron\"\\nvisited \"Fah Root\"\\nvisited \"Fah Soom\"\\nvisited \"Fala\"\\nvisited \"Fallen Leaf\"\\nvisited \"Far Horizon\"\\nvisited \"Farbutero\"\\nvisited \"Farinus\"\\nvisited \"Faronektu\"\\nvisited \"Fasitopfar\"\\nvisited \"Fell Omen\"\\nvisited \"Feroteri\"\\nvisited \"Ferukistek\"\\nvisited \"Fingol\"\\nvisited \"Flugbu\"\\nvisited \"Fomalhaut\"\\nvisited \"Fornarep\"\\nvisited \"Four Pillars\"\\nvisited \"Furmeliki\"\\nvisited \"Gacrux\"\\nvisited \"Gamma Cassiopeiae\"\\nvisited \"Gamma Corvi\"\\nvisited \"Gienah\"\\nvisited \"Girtab\"\\nvisited \"Glubatub\"\\nvisited \"Gomeisa\"\\nvisited \"Good Omen\"\\nvisited \"Gorvi\"\\nvisited \"Graffias\"\\nvisited \"Gupta\"\\nvisited \"Hadar\"\\nvisited \"Hamal\"\\nvisited \"Han\"\\nvisited \"Hassaleh\"\\nvisited \"Hatysa\"\\nvisited \"Heia Due\"\\nvisited \"Hesselpost\"\\nvisited \"Hevru Hai\"\\nvisited \"Hi Yahr\"\\nvisited \"Hintar\"\\nvisited \"Holeb\"\\nvisited \"Homeward\"\\nvisited \"Host\"\\nvisited \"Hunter\"\\nvisited \"Ik\\'kara\\'ka\"\\nvisited \"Ildaria\"\\nvisited \"Imo Dep\"\\nvisited \"Insitor\"\\nvisited \"Io Lowe\"\\nvisited \"Io Mann\"\\nvisited \"Ipsing\"\\nvisited \"Iyech\\'yek\"\\nvisited \"Izar\"\\nvisited \"Ka\\'ch\\'chrai\"\\nvisited \"Ka\\'pru\"\\nvisited \"Kaliptari\"\\nvisited \"Kappa Centauri\"\\nvisited \"Kashikt\"\\nvisited \"Kasikfar\"\\nvisited \"Kaus Australis\"\\nvisited \"Kaus Borealis\"\\nvisited \"Ki War Ek\"\\nvisited \"Kiro\\'ku\"\\nvisited \"Kiru\\'kichi\"\\nvisited \"Kochab\"\\nvisited \"Kor Ak\\'Mari\"\\nvisited \"Kor En\\'lakfar\"\\nvisited \"Kor Fel\\'tar\"\\nvisited \"Kor Men\"\\nvisited \"Kor Nor\\'peli\"\\nvisited \"Kor Tar\\'bei\"\\nvisited \"Kor Zena\\'i\"\\nvisited \"Kornephoros\"\\nvisited \"Korsmanath\"\\nvisited \"Kraz\"\\nvisited \"Kugel\"\\nvisited \"Kursa\"\\nvisited \"Last Word\"\\nvisited \"Lesath\"\\nvisited \"Levana\"\\nvisited \"Limen\"\\nvisited \"Lolami\"\\nvisited \"Lom Tahr\"\\nvisited \"Lone Cloud\"\\nvisited \"Lucina\"\\nvisited \"Lurata\"\\nvisited \"Makferuti\"\\nvisited \"Markab\"\\nvisited \"Markeb\"\\nvisited \"Matar\"\\nvisited \"Mebla\"\\nvisited \"Mebsuta\"\\nvisited \"Meftarkata\"\\nvisited \"Mei Yohn\"\\nvisited \"Mekislepti\"\\nvisited \"Membulem\"\\nvisited \"Men\"\\nvisited \"Menkalinan\"\\nvisited \"Menkar\"\\nvisited \"Menkent\"\\nvisited \"Merak\"\\nvisited \"Mesuket\"\\nvisited \"Miaplacidus\"\\nvisited \"Miblulub\"\\nvisited \"Mimosa\"\\nvisited \"Minkar\"\\nvisited \"Mintaka\"\\nvisited \"Mirach\"\\nvisited \"Mirfak\"\\nvisited \"Mirzam\"\\nvisited \"Mizar\"\\nvisited \"Moktar\"\\nvisited \"Mora\"\\nvisited \"Muhlifain\"\\nvisited \"Muphrid\"\\nvisited \"Naos\"\\nvisited \"Naper\"\\nvisited \"Nashira\"\\nvisited \"Nenia\"\\nvisited \"Nihal\"\\nvisited \"Nocte\"\\nvisited \"Nunki\"\\nvisited \"Oblate\"\\nvisited \"Orbona\"\\nvisited \"Orvala\"\\nvisited \"Ossipago\"\\nvisited \"Over the Rainbow\"\\nvisited \"Pantica\"\\nvisited \"Parca\"\\nvisited \"Peacock\"\\nvisited \"Pelubta\"\\nvisited \"Peragenor\"\\nvisited \"Peresedersi\"\\nvisited \"Perfica\"\\nvisited \"Persian\"\\nvisited \"Persitar\"\\nvisited \"Phact\"\\nvisited \"Phecda\"\\nvisited \"Pherkad\"\\nvisited \"Phurad\"\\nvisited \"Pik\\'ro\\'iyak\"\\nvisited \"Plort\"\\nvisited \"Polaris\"\\nvisited \"Pollux\"\\nvisited \"Porrima\"\\nvisited \"Prakacha\\'a\"\\nvisited \"Procyon\"\\nvisited \"Pug Iyik\"\\nvisited \"Quaru\"\\nvisited \"Rajak\"\\nvisited \"Rasalhague\"\\nvisited \"Rastaban\"\\nvisited \"Rati Cal\"\\nvisited \"Regor\"\\nvisited \"Regulus\"\\nvisited \"Remembrance\"\\nvisited \"Rigel\"\\nvisited \"Ruchbah\"\\nvisited \"Rutilicus\"\\nvisited \"Ruwarku\"\\nvisited \"Sabik\"\\nvisited \"Sabriset\"\\nvisited \"Sadachbia\"\\nvisited \"Sadalmelik\"\\nvisited \"Sadalsuud\"\\nvisited \"Sadr\"\\nvisited \"Sagittarius A*\"\\nvisited \"Saiph\"\\nvisited \"Salipastart\"\\nvisited \"Salm\"\\nvisited \"Sargas\"\\nvisited \"Sarin\"\\nvisited \"Sayaiban\"\\nvisited \"Scheat\"\\nvisited \"Schedar\"\\nvisited \"Segesta\"\\nvisited \"Seginus\"\\nvisited \"Seketra\"\\nvisited \"Sepetrosk\"\\nvisited \"Sepriaptu\"\\nvisited \"Sevrelect\"\\nvisited \"Shaula\"\\nvisited \"Sheratan\"\\nvisited \"Si\\'yak\\'ku\"\\nvisited \"Sich\\'ka\\'ara\"\\nvisited \"Silikatakfar\"\\nvisited \"Silver Bell\"\\nvisited \"Silver String\"\\nvisited \"Similisti\"\\nvisited \"Sirius\"\\nvisited \"Situla\"\\nvisited \"Skeruto\"\\nvisited \"Sko\\'karak\"\\nvisited \"Sobarati\"\\nvisited \"Sol\"\\nvisited \"Sol Arachi\"\\nvisited \"Sol Kimek\"\\nvisited \"Sol Saryds\"\\nvisited \"Solifar\"\\nvisited \"Sospi\"\\nvisited \"Speloog\"\\nvisited \"Spica\"\\nvisited \"Steep Roof\"\\nvisited \"Stercutus\"\\nvisited \"Suhail\"\\nvisited \"Sumar\"\\nvisited \"Sumprast\"\\nvisited \"Tais\"\\nvisited \"Talita\"\\nvisited \"Tania Australis\"\\nvisited \"Tarazed\"\\nvisited \"Tarf\"\\nvisited \"Tebuteb\"\\nvisited \"Tejat\"\\nvisited \"Terminus\"\\nvisited \"Terra Incognita\"\\nvisited \"Torbab\"\\nvisited \"Tortor\"\\nvisited \"Turais\"\\nvisited \"Ula Mon\"\\nvisited \"Ultima Thule\"\\nvisited \"Umbral\"\\nvisited \"Unagi\"\\nvisited \"Unukalhai\"\\nvisited \"Uwa Fahn\"\\nvisited \"Vega\"\\nvisited \"Vindemiatrix\"\\nvisited \"Volax\"\\nvisited \"Wah Ki\"\\nvisited \"Wah Oh\"\\nvisited \"Wah Yoot\"\\nvisited \"Waypoint\"\\nvisited \"Wazn\"\\nvisited \"Wei\"\\nvisited \"Wezen\"\\nvisited \"World\\'s End\"\\nvisited \"Ya Hai\"\\nvisited \"Yed Prior\"\\nvisited \"Zaurak\"\\nvisited \"Zeta Aquilae\"\\nvisited \"Zeta Centauri\"\\nvisited \"Zosma\"\\nvisited \"Zuba Zub\"\\nvisited \"Zubenelgenubi\"\\nvisited \"Zubenelhakrabi\"\\nvisited \"Zubeneschamali\"\\n\"visited planet\" \"Ember Reaches\"\\n\"visited planet\" \"Ember Threshold\"\\n\"visited planet\" \"Ember Wormhole\"\\n\"visited planet\" \"Hai Wormhole\"\\n\"visited planet\" \"Pirate Wormhole\"\\n\"visited planet\" \"Pug Wormhole\"\\n\"visited planet\" \"Quarg Wormhole\"\\n\"visited planet\" \"Remnant Wormhole\"\\n\"visited planet\" \"Rim Wormhole\"\\n\"visited planet\" \"The Eye\"\\n\\n(return to top)\\n'},\n",
       " {'language': 'C++ 94.9',\n",
       "  'readme': \"DeSyDe\\nDeSyDe is a design space exploration tool developed at KTH (ForSyDe research group).\\nReleases:\\n\\n\\nlatest:\\n\\nRelease for our DSD'18 publication + user tutorial\\n\\n\\n\\nprevious:\\n\\nRelease for our TODAES article\\nRelease for our RAPIDO'17 publication\\n\\n\\n\\n(Almost) hassle-free installation\\nYou need to install DeSyDe via the automated build scripts. We have\\ntried assuring an (almost) fully-automated installation process,\\nespecially for Linux machines. The idea is quite simple: the script\\ndownloads almost everything necessary so that nothing on your system\\nis touched and then proceeds to compile everything. This sandboxing\\ncomes with the cost of added compilation time, but since this should\\nbe a one-time process, the larger time frame is a good trade-off for\\nflexibility.\\nThe only dependency that is not cloned directly from its repo and\\ncompiled alongside DeSyDe is Qt, as DeSyDe currently does not make\\nGecode's Gist optional. Please ensure that you have the basic\\ndevelopment files for Qt installed and reachable in your machine. In\\nfuture releases this necessity will be removed.\\nIf you are on any debian-based distro with reasonably updated\\npackages, you should be good to go by issuing the following install\\ncommand (do not forget to prepend sudo if necessary):\\napt install automake libtool qt5-default\\n\\nAs of 2019-06-05, it seems from user feedback that on ubuntu and other\\nderived distros not all dependencies are pulled with these commands, so\\nit may be necessary to install qtcreator to be able to compile DeSyDe\\n(do not forget to prepend sudo if necessary):\\napt install qtcreator\\n\\nThen, a make followed by make install should do the trick. Tested\\non Linux Mint 18.3 and Debian 10.\\nUsage\\nPlease follow the tutorial for more details on how\\nto use the tool and how to interpret its output.\\nRunning the Experiments\\nThe experiments provided in the examples folder represent those that are still functional and were\\nused as proof of concepts into previous papers this project was involved. For a step-by-step tutorial\\non how to setup your own experiment, check out the tutorial provided in this repo.\\nIncluded examples\\n\\nDSD18: experiments from our DSD'18 dealing with TDN NoCs exploration that optimize power while respecting real time constraints.\\nScalAnalysis: folder containing scripts that generates experiments for different sized NoCs platforms based on a template extracted from DSD18.\\ntutorial: the files used for the user tutorial.\\n\\nPublications\\nKathrin Rosvall, Tage Mohammadat, George Ungureanu, Johnny Öberg, and Ingo Sander. “Exploring Power and Throughput for Dataflow Applications on Predictable NoC Multiprocessors,” 719–26, 2018.\\nKathrin Rosvall, Nima Khalilzad, George Ungureanu, and Ingo Sander. Throughput propagation in constraint-based design space exploration for mixed-criticality systems. In Proceedings of the 2017 Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools (RAPIDO '17), Stockholm, Sweden. ACM, January 2017.\\nNima Khalilzad, Kathrin Rosvall, and Ingo Sander. A modular design space exploration framework for multiprocessor real-time systems. In Forum on specification & Design Languages (FDL '16), Bremen, Germany. IEEE, September 2016.\\nKathrin Rosvall and Ingo Sander. A constraint-based design space exploration framework for real-time applications on MPSoCs. In Design Automation and Test in Europe (DATE '14), Dresden, Germany, Mar. 2014.\\n\"},\n",
       " {'language': 'C++ 97.5',\n",
       "  'readme': \"Tux in Space\\nTux in Space: space exploration game\\nThis program is a simulation game. The program simulates the moving, under\\nthe phisic's laws, of planets, spaceships, suns, and everything else in\\ndeep space.\\nHighlighted features:\\n\\nA wide gerarchic collection of various object types with different\\nbehaviours\\nA phisic engine that simulates the gravity force and different types of\\nimpacts between objects\\nSpace Monsters with basic AI\\n\\ntest/use the program:\\nThere is an executable file, compiled in a 64 bit Linux; Maybe can run\\nin other computers as well, but you can easily compile the program with\\nthe makefile (just run 'make' in the master directory).\\nThe program is written for linux only, but with very little work or maybe no work at all could also\\nrun on other operative systems. (See information file)\\nOfficially supported compiler is gcc >= 6.3.\\nFor developers is suggested to use the latest version in the\\nmaster branch, for normal users the latest release.\\nStatus\\nThe program is far far away from being complete.\\nLicense\\nTux in Space - space exploration game\\nCopyright (C) 2016-2017 emanuele.sorce@hotmail.com\\nThis program is free software; you can redistribute it and/or modify\\nit under the terms of the GNU General Public License as published by\\nthe Free Software Foundation, version 3 or compatible.\\nThis program is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty or\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\nGNU General Public License for more details.\\nYou should have received a copy of the GNU General Public License\\nalong with this program; if not, write to the Free Software\\nFoundation, Inc.\\n\"},\n",
       " {'language': 'C++ 90.3',\n",
       "  'readme': 'turbo-ros-pkg\\nROS software repository, Robotics & Biology Laboratory, TU Berlin\\n'},\n",
       " {'language': 'C++ 66.9',\n",
       "  'readme': 'DIY spectrophotometer\\nHow to use it ?\\nPlease check the userguide to obtain all the useful information about the user interface.\\nGoal of the project\\nThis project was done at the Hackuarium association, a DIY biology hacking-space and open laboratory located in Ecublens (Switzerland). Feel free to contact us if you would like additional information or would like to buy a fully built and tested spectrophotometer.\\nRecently, the open-source and open-hardware community has been involved in the creation of open scientific tools. In this context, a few spectrophotometer projects emerged. However, none of them was sufficiently evolved to be casually used in a laboratory. It is for this reason that we started this project.\\nOur goal was to create a simple spectrophotometer that measures the\\nabsorbance of a sample for 3 different light colors (red, green, blue), that is\\nself-contained (battery powered) and displays the result on a LCD screen. In addition, the tool had to be reliable, precise and cheap.\\nThe initial idea was that you could teach spectrophotometry without\\nhaving to buy an expensive instrument, since you can find experiments\\nthat do not require a specific light wavelength. This includes\\noptical density for bacteria culture, determination of a pigment\\'s concentration in\\na solution, determination of the kinetic of a reaction and many others.\\nThe device is rather cheap, for you can buy all the components on AliExpress\\n(but one) and either 3D print or laser cut the case (MDF or acrylic glass). When constructing 10, the price\\nper spectrophotometer is around $30. However if you want to have a final\\nproduct with an aluminium case and PCBs already assembled, the cost would rather be $90 each if you order 20 of them.\\n\\nCan we do science with this ?\\nBefore trying to explain how it is done more in detail maybe the first\\nquestion that we should answer is : can we do science with this tool?\\nWhen thinking about the design of the spectro, we found an light sensor that converts the light energy to frequency and that is linear on a range of nearly 10^6. This means that the luminosity range that we can measure precisely is really big. Only the precision of the component is therefore really promising and shows that we could do something that gives good results.\\nAnd indeed, the final product gives really accurate results in fields like:\\n\\nChemistry\\nBiology\\n\\nAnd other, which you can see on our lab notebook.\\nSome of our results\\nPatent blue V\\nTo begin we did a very simple experiment that allows to measure the absorbance of solutions containing different known concentrations of patent blue V (E131), a blue pigment that is used in various blue candies.\\n\\nThe result is rather good knowing that the solutions were simply prepared by adding various volumes (0 to 2 mL) of a concentrated pigmented solution with a 1mL seringe to 100mL of water. Not the most accurate method...\\nEscherichia coli\\nAnother feature of the simple-spectrophotometer is that it works on a rechargeable battery (autonomy of 48 hours). Consequently, you can measure a kinetic by placing the device directly inside an incubator, which is not feasible with a regular commercial spectrophotometer.\\nOptical density (OD) at 600nm is often used in biology to determine the biomass increase versus time. In this experiment we have placed Escherichia coli with cell growing media directly in the spectrophotometer cuvette and placed the spectrophotometer itself on a shaker in the incubator.\\nWe can then program it to measure the absorbance every X seconds. In our case we measured the absorbance every 30 minutes (up to 40 measures). The following curve was obtained.\\n\\nThis experiment clearly shows the log phase and the stationary phase. But the growth was faster than expected. An acquisition every 15 minutes would have been a better choice.\\nUsing the simple spectrophotometer to teach science\\nThe use of the spectrophotometer is rather simple. Just put a blank, press the button, wait 10s and put the sample. The result is displayed on the screen. In addition, it could be open so that students could see how it is inside and could really understand how the tool works (avoiding the \"black box\" effect). Also, since it is inexpensive, having ten of these for a classroom would be possible, which would allow students to have one instrument per pair.\\nUsing the spectrophotometer\\nFor all this reason, we also consider our instrument as a pedagogical tool. This allowed us to use it to teach some basic scientific concepts to groups of children and teenagers.\\nSchool in Mondomo (Colombia)\\nHow to make it ?\\nThe electronic design is compatible with the Arduino platform. We use an\\nATMEGA32U4 microcontroller which is directly connected to the peripherals we need for this spectrophotometer.\\nWe prefer to design the full PCB rather than to make a shield for an existing Arduino board. Indeed, the extra work to add the microcontroller is rather limited, so it does not make sense from our point of view to create a shield. Also, the resulting board is smaller and more reliable.\\nAll the components we use but one are mainstream and can be found on eBay or AliExpress. The only special component is the light to frequency converter TSL235R\\nthat we buy on Mouser website.\\n\\nFor more details about the construction process, follow one of the links underneath.\\n\\nThe PCB (using eagle)\\nThe case (using FreeCAD and OpenSCAD)\\nThe software (using Arduino)\\n\\nExperiments\\n\\nBacteria growth\\nPigment concentration\\nMeasure of the fluorescence\\nKinetic of a reaction\\n\\nTests\\nUnderneath, you will find some of the tests we have run to verify the quality of our spectrophotometer.\\n\\nBattery\\nReproducibility of results\\n\\nWorkshop\\nWe regularly organize one day workshops \"Build your own spectrophotometer\" during which the participants learn all the process we went through to obtain the final product and build their own spectrophotometer. Please contact us if you are interested by this kind of workshop.\\nHere are the slides of the presentation we make during the workshop.\\n\\nSlides\\n\\nCloning the project\\nThis project uses SUBMODULES.\\nIn order to clone it use:\\ngit clone --recurse-submodules https://github.com/hackuarium/simple-spectro\\nTo update the submodules:\\ngit submodule update --recursive --remote\\n'},\n",
       " {'language': 'C++ 76.1',\n",
       "  'readme': 'LibAPR - The Adaptive Particle Representation Library\\nLibrary for producing and processing on the Adaptive Particle Representation (APR) (For article see: https://www.nature.com/articles/s41467-018-07390-9).\\n\\nLabeled Zebrafish nuclei: Gopi Shah, Huisken Lab (MPI-CBG, Dresden and Morgridge Institute for Research, Madison); see also Schmid et al., Nature Communications 2017\\n\\n\\nDependencies\\n\\nHDF5 1.8.20 or higher\\nOpenMP > 3.0 (optional, but suggested)\\nCMake 3.6 or higher\\nLibTIFF 4.0 or higher\\n\\nNB: This update to 2.0 introduces changes to IO and iteration that are not compatable with old versions.\\nBuilding\\nThe repository requires sub-modules, so the repository needs to be cloned recursively:\\ngit clone --recursive https://github.com/cheesema/LibAPR\\n\\nIf you need to update your clone at any point later, run\\ngit pull\\ngit submodule update\\n\\nBuilding on Linux\\nOn Ubuntu, install the cmake, build-essential, libhdf5-dev and libtiff5-dev packages (on other distributions, refer to the documentation there, the package names will be similar). OpenMP support is provided by the GCC compiler installed as part of the build-essential package.\\nIn the directory of the cloned repository, run\\nmkdir build\\ncd build\\ncmake ..\\nmake\\n\\nThis will create the libapr.so library in the build directory, as well as all of the examples.\\nDocker build\\nWe provide a working Dockerfile that install the library within the image on a separate repo.\\nBuilding on OSX\\nOn OSX, install the cmake, hdf5 and libtiff homebrew packages and have the Xcode command line tools installed.\\nIf you want to compile with OpenMP support, also install the llvm package (this can also be done using homebrew), as the clang version shipped by Apple currently does not support OpenMP.\\nIn the directory of the cloned repository, run\\nmkdir build\\ncd build\\ncmake ..\\nmake\\n\\nThis will create the libapr.dylib library in the build directory, as well as all of the examples.\\nIn case you want to use the homebrew-installed clang (OpenMP support), modify the call to cmake above to\\nCC=\"/usr/local/opt/llvm/bin/clang\" CXX=\"/usr/local/opt/llvm/bin/clang++\" LDFLAGS=\"-L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib\" CPPFLAGS=\"-I/usr/local/opt/llvm/include\" cmake ..\\n\\nBuilding on Windows\\nThe simplest way to utilise the library from Windows 10 is through using the Windows Subsystem for Linux; see: https://docs.microsoft.com/en-us/windows/wsl/install-win10 then follow linux instructions.\\nCompilation only works with mingw64/clang or the Intel C++ Compiler, with Intel C++ being the recommended way\\nThe below instructions for VS can be attempted; however they have not been reproduced.\\nYou need to have Visual Studio 2017 installed, with the community edition being sufficient. LibAPR does not compile correctly with the default Visual Studio compiler, so you also need to have the Intel C++ Compiler, 18.0 or higher installed. cmake is also a requirement.\\nFurthermore, you need to have HDF5 installed (binary distribution download at The HDF Group and LibTIFF (source download from SimpleSystems. LibTIFF needs to be compiled via cmake. LibTIFF\\'s install target will then install the library into C:\\\\Program Files\\\\tiff.\\nIn the directory of the cloned repository, run:\\nmkdir build\\ncd build\\ncmake -G \"Visual Studio 15 2017 Win64\" -DTIFF_INCLUDE_DIR=\"C:/Program Files/tiff/include\" -DTIFF_LIBRARY=\"C:/Program Files/tiff/lib/tiff.lib \" -DHDF5_ROOT=\"C:/Program Files/HDF_Group/HDF5/1.8.17\"  -T \"Intel C++ Compiler 18.0\" ..\\ncmake --build . --config Debug\\n\\nThis will set the appropriate hints for Visual Studio to find both LibTIFF and HDF5. This will create the apr.dll library in the build/Debug directory, as well as all of the examples. If you need a Release build, run cmake --build . --config Release from the build directory.\\nExamples and Documentation\\nThese examples can be turned on by adding -DAPR_BUILD_EXAMPLES=ON to the cmake command.\\nThere are nine basic examples, that show how to generate and compute with the APR:\\n\\n\\n\\nExample\\nHow to ...\\n\\n\\n\\n\\nExample_get_apr\\ncreate an APR from a TIFF and store as hdf5.\\n\\n\\nExample_apr_iterate\\niterate through a given APR.\\n\\n\\nExample_neighbour_access\\naccess particle and face neighbours.\\n\\n\\nExample_compress_apr\\nadditionally compress the intensities stored in an APR.\\n\\n\\nExample_random_access\\nperform random access operations on particles.\\n\\n\\nExample_ray_cast\\nperform a maximum intensity projection ray cast directly on the APR data structures read from an APR.\\n\\n\\nExample_reconstruct_image\\nreconstruct a pixel image from an APR.\\n\\n\\n\\nAll examples except Example_get_apr require an already produced APR, such as those created by Example_get_apr.\\nFor tutorial on how to use the examples, and explanation of data-structures see the library guide.\\nLibAPR Tests\\nThe testing framework can be turned on by adding -DAPR_TESTS=ON to the cmake command. All tests can then be run by executing on the command line your build folder.\\nctest\\n\\nPlease let us know by creating an issue, if any of these tests are failing on your machine.\\nPython support\\nNote: These have been updated and externalised, and will be released shortly.\\nJava wrappers\\nBasic Java wrappers can be found at LibAPR-java-wrapper\\nComing soon\\n\\nmore examples for APR-based filtering and segmentation\\ndeployment of the Java wrappers to Maven Central so they can be used in your project directly\\nsupport for loading the APR in Fiji, including scenery-based 3D rendering\\nimproved java wrapper support\\nCUDA GPU-accelerated APR generation and processing\\nBlock based decomposition for extremely large images.\\nTime series support.\\n\\nContact us\\nIf anything is not working as you think it should, or would like it to, please get in touch with us!! Further, if you have a project, or algorithm, you would like to try using the APR for also please get in contact we would be glad to help!\\n\\nCiting this work\\nIf you use this library in an academic context, please cite the following paper:\\n\\nCheeseman, Günther, Gonciarz, Susik, Sbalzarini: Adaptive Particle Representation of Fluorescence Microscopy Images (Nature Communications, 2018) https://doi.org/10.1038/s41467-018-07390-9\\n\\n'},\n",
       " {'language': 'C++ 85.4',\n",
       "  'readme': 'All the C program code are translated to C++ STL. Smart pointers are used to avoid danger pointers and memory leak.\\nIt is rewritten to support MVC design pattern required by MFC under Visual Studio 2017. It would be a good code reference for C++ programmers. The \"model\" folder in source code should be portable to other C++ compilers. As mentioned above, to understand the code to display the Windows UI required the fundamental knowledge of Microsoft Foundation Class (MFC) Library. C++ Beginners might find it difficult to understand the code of some essential UI controls. Many thanks to the MFC experts provided the source code of such advanced controls.\\nWelcome for C++ experts for further improvement or contribution of coding enhancement.\\n'},\n",
       " {'language': 'C++ 67.9',\n",
       "  'readme': \"Cyclops LED Driver\\nPrecision, wide-bandwidth current source with optional optical feedback mode\\nfor driving high-power LEDs and laser diodes. Good for sneaking optogenetic\\nstimuli between fast things (e.g. galvo flyback on a 2P system). Good for\\nreally controlling the amount of light you deliver during 1P imaging or\\noptogenetic stimulation.\\nIf you have questions or comments, please come talk on the open-ephys slack\\nin the #cyclops channel.\\nFeatures\\n\\nUltra-precise\\nHigh power\\nUp to 1.5A per LED\\nWide bandwidth\\n\\n~2.5 MHz -3 dB bandwidth\\nMaximum 100 ns 1.0A rise and fall times\\n\\n\\nCurrent and optical feedback modes\\nBuilt-in waveform generation\\nOver-current protection\\nModular\\n\\nArduino compatible: internal waveform generation\\nAlso, accepts external analog, gate, or trigger inputs\\n\\n\\n\\nStimulus generation options\\n\\nExternal stimulus sequencer\\nExternal digital trigger\\n\\nTTL logic level\\n\\n\\nExternal analog waveform generator\\n\\n0-5V analog signals\\n\\n\\nInternal 12-bit DAC\\n\\nSynchronized across up to 4 drivers\\nArduino library\\nProgrammable triggering logic\\nRespond to USB input\\n\\n\\n\\n\\nBuying one\\nYou can purchase a fully assembled cyclops driver from the open-ephys\\nstore. All profits go towards continued\\noperation of open-ephys.\\nDocumentation\\nDocumentation and usage information are here: MANUAL.pdf. If you\\nhave questions concerning usage, performance, etc., please direct them toward\\nthe Open Ephys forum.\\nHardware Licensing\\nCopyright Jonathan P. Newman 2020.\\nThis work is licensed under CC BY-NC-SA 4.0. To view a copy of this license,\\nvisit https://creativecommons.org/licenses/by-nc-sa/4.0\\nNote: This license applies to hardware designs and documentation which reside\\nin the 'device', 'experimental', 'resources' folders of this repository along\\nwith information in 'MANUAL.md' and 'MANUAL.pdf'\\nSoftware Licensing\\nCopyright (c) Jonathan P. Newman 2017. All right reserved.\\nThe code associated with the Cyclops project is free software: you can\\nredistribute it and/or modify it under the terms of the GNU General Public\\nLicense as published by the Free Software Foundation, either version 3 of the\\nLicense, or (at your option) any later version.\\nThe code associated with the Cyclops project is distributed in the hope that it\\nwill be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General\\nPublic License for more details.\\nYou should have received a copy of the GNU General Public License along with\\nthis code.  If not, see http://www.gnu.org/licenses/.\\nNote: This license applies to software/fireware source code which resides in in\\nthe 'lib' folder of this repository\\n\"},\n",
       " {'language': 'C++ 71.9',\n",
       "  'readme': 'PhysiBoSS\\nMultiscale simulation of multi-cellular system\\nOverview:\\n\\nPresentation\\nUsage\\nDocumentation\\nReferences\\nRemarks\\n\\nPresentation\\nPhysiBoSS (PhysiCell-MaBoSS) is C++ software for multiscale simulation of heterogeneous multi-cellular system. It integrates together cell\\'s internal signalling pathway model (boolean formalism), physical representation of cell (agent-based) and extra-cellular matrix diffusing or fixed entities.\\nIt is adapted from PhysiCell sources, with the boolean network computation inside each cell from MaBoSS software.\\n\\nUsage\\nCompiling PhysiBoSS\\nPhysiBoSS should run and be easily installed on Linux and MacOS system.\\nIt requires moderatly recent version of C++ (at least c++11) and OpenMP support. Compilation of MaBoSS library requires flex and bison library, usually already present (and can be easily installed on e.g. Linux ubuntu with sudo apt-get install bison flex). We also provide a Docker image of PhysiBoSS that can be used if it cannot be installed in your machine. It can also be used without any installation via a Web interface for specific simulations on nanohub.\\nTo install it on Linux system, from a Terminal:\\nClone the repository on your local machine, and go inside the main directory. Type make install, which will install and compile MaBoSS then PhysiBoSS. The executables will be created in the \\'bin\\' directory if all goes well.\\nIt can be compiled in \\'Debug\\', \\'Release\\' or \\'Proliling\\' modes, to set in the \\'Makefile\\' file. Default is \\'Release\\' mode (fastest).\\nYou might also have to change your c++ compiler in the Makefile according to your operating system.\\nCommands list:\\ngit clone https://github.com/gletort/PhysiBoSS.git\\ncd PhysiBoSS\\nmake install\\nIf errors happened during the compilation, please refer to the installation page.\\nRunning one simulation\\nTo run a simulation, you need (at least) a XML parameter file indicating the conditions of the simulation, and the networks file (you can find some on MaBoSS website and on our logical modelling pipeline repository).\\nOther options are possible, cf the code-documentation or this repository wiki for more informations.\\nExample of a parameter file (with only few parameters shown):\\n  <?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n \\n  <simulation>\\n \\t\\t<time_step> 0.2 </time_step>\\n \\t\\t<mechanics_time_step> 0.1 </mechanics_time_step>\\n \\t\\t....\\n  </simulation>\\n \\n  <cell_properties>\\n \\t\\t<mode_motility> 1 </mode_motility>\\n \\t\\t<polarity_coefficient> 0.5 </polarity_coefficient>\\n \\t\\t...\\n  </cell_properties>\\n \\n  <network>\\n \\t\\t<network_update_step> 10 </network_update_step>\\n \\t\\t...\\n  </network>\\n \\n  <initial_configuration>\\n \\t\\t<load_cells_from_file> init.txt </load_cells_from_file>\\n \\t\\t...\\n  </initial_configuration>\\nImage and analyse a simulation\\nTo visualize graphically the result of a simulation, with use the software Paraview (or you can also generate a .svg snapshot of the simulation). Analysis of the result files were done with python scripts proposed in this directory. For documentation on how to use Paraview to set-up the rendering of PhysiBoSS outputs, see here, with the explication on how to draw spheres from a set of points (x, y, z, radius).\\nNanohub\\nPhysiBoSS can be directly used via a Web interface on nanohub. This allows to run it without any installation and running directly on the server and can be used without any coding skills. The parameters of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server. It just required a nanohub account.\\nAvailable simulations tools of PhysiBoSS can be found on https://nanohub.org/resources/tools, under the keywords PhysiBoSS or PhysiBoSSa.\\nA model of tumors cell spheroid growing and invading into the surrounding extra-cellular matrix (ECM) is currently available PhysiBoSSa_ECM. Various parameters as the density of the extracellular matrix, the cell motility, ECM degradation from the cells, TGF-beta production... can be tuned by the user.\\nDocumentation\\nCode-oriented documentation can be generated with Doxygen:\\nmake doc\\nin the main directory.\\nIt can be configured in the Doxyfile file present in this directory.\\nIt will generate the html documentation in the doc/html directory.\\nYou can visualize it in a browser, e.g.:\\nfirefox doc/html/index.html &\\nYou can also refer to (future) publications with PhysiBoSS for scientific applications of this software and description of the models.\\nStep-by-step examples with the necessary files to run them are also proposed in the \\'examples\\' directory and on the Wiki of this repository.\\nReferences\\n For PhysiBoSS: \\n\\nPhysiBoSS publication: Letort G, Montagud A, Stoll G, Heiland R, Barillot E, Macklin P, Zinovyev A, Calzone L .  PhysiBoSS: a multi-scale agent-based modelling framework integrating physical dimension and cell signalling.  Bioinformatics, bty766, doi:10.1093/bioinformatics/bty766\\n\\n\\nFor PhysiCell: \\nPaul Macklin\\'s lab website  \\nPhysiCell publication: A. Ghaffarizadeh, S.H. Friedman, S.M. Mumenthaler, and P. Macklin, PhysiCell: an Open Source Physics-Based Cell Simulator for 3-D Multicellular Systems, bioRxiv 088773, 2016. DOI: 10.1101/088773. \\nBioFVM website  \\nBioFVM publication: A. Ghaffarizadeh, S.H. Friedman, and P. Macklin. BioFVM: an efficient, parallelized diffusive transport solver for 3-D biological simulations. Bioinformatics, 2015. \\n\\n\\nFor MaBoSS:\\nMaBoSS website  \\nMaBoSS publication: Stoll G, Viara E, Barillot E, Calzone L. Continuous time Boolean modeling for biological signaling: application of Gillespie algorithm. BMC Syst Biol. 2012 Aug 29;6:116. doi: 10.1186/1752-0509-6-116. \\n\\n\\nRemarks\\nPlease, refer to the Wiki of this repository for a much more extended documentation, with step by step examples instructions.\\nPhysiCell is developed in Paul Macklin\\'s lab.\\nMaBoSS and PhysiBoSS are developed in the Computational Systems Biology of Cancer group at Institut Curie (Paris, France).\\nWe invite you to use PhysiBoSS for you research and give feedbacks to us. Any help in developing it further is more than welcome.\\nDo not hesitate to contact us for any comments or difficulties in using PhysiBoSS: physiboss@gmail.com.\\nWishing you to enjoy using PhysiBoSS,\\nPhysiBoSS\\'s team.\\n'},\n",
       " {'language': 'C++ 77.9',\n",
       "  'readme': 'PhysiBoSS\\nMultiscale simulation of multi-cellular system\\nOverview:\\n\\nPresentation\\nUsage\\nDocumentation\\nReferences\\nRemarks\\n\\nPresentation\\nPhysiBoSS (PhysiCell-MaBoSS) is C++ software for multiscale simulation of heterogeneous multi-cellular system. It integrates together cell\\'s internal signalling pathway model (boolean formalism), physical representation of cell (agent-based) and extra-cellular matrix diffusing or fixed entities.\\nIt is adapted from PhysiCell sources, with the boolean network computation inside each cell from MaBoSS software.\\n\\nUsage\\nCompiling PhysiBoSS\\nPhysiBoSS should run and be easily installed on Linux and MacOS system.\\nIt requires moderatly recent version of C++ (at least c++11) and OpenMP support. Compilation of MaBoSS library requires flex and bison library, usually already present (and can be easily installed on e.g. Linux ubuntu with sudo apt-get install bison flex). We also provide a Docker image of PhysiBoSS that can be used if it cannot be installed in your machine. It can also be used without any installation via a Web interface for specific simulations on nanohub.\\nTo install it on Linux system, from a Terminal:\\nClone the repository on your local machine, and go inside the main directory. Type make install, which will install and compile MaBoSS then PhysiBoSS. The executables will be created in the \\'bin\\' directory if all goes well.\\nIt can be compiled in \\'Debug\\', \\'Release\\' or \\'Proliling\\' modes, to set in the \\'Makefile\\' file. Default is \\'Release\\' mode (fastest).\\nYou might also have to change your c++ compiler in the Makefile according to your operating system.\\nCommands list:\\ngit clone https://github.com/gletort/PhysiBoSS.git\\ncd PhysiBoSS\\nmake install\\nIf errors happened during the compilation, please refer to the installation page.\\nRunning one simulation\\nTo run a simulation, you need (at least) a XML parameter file indicating the conditions of the simulation, and the networks file (you can find some on MaBoSS website and on our logical modelling pipeline repository).\\nOther options are possible, cf the code-documentation or this repository wiki for more informations.\\nExample of a parameter file (with only few parameters shown):\\n  <?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n \\n  <simulation>\\n \\t\\t<time_step> 0.2 </time_step>\\n \\t\\t<mechanics_time_step> 0.1 </mechanics_time_step>\\n \\t\\t....\\n  </simulation>\\n \\n  <cell_properties>\\n \\t\\t<mode_motility> 1 </mode_motility>\\n \\t\\t<polarity_coefficient> 0.5 </polarity_coefficient>\\n \\t\\t...\\n  </cell_properties>\\n \\n  <network>\\n \\t\\t<network_update_step> 10 </network_update_step>\\n \\t\\t...\\n  </network>\\n \\n  <initial_configuration>\\n \\t\\t<load_cells_from_file> init.txt </load_cells_from_file>\\n \\t\\t...\\n  </initial_configuration>\\nImage and analyse a simulation\\nTo visualize graphically the result of a simulation, with use the software Paraview (or you can also generate a .svg snapshot of the simulation). Analysis of the result files were done with python scripts proposed in this directory. For documentation on how to use Paraview to set-up the rendering of PhysiBoSS outputs, see here, with the explication on how to draw spheres from a set of points (x, y, z, radius).\\nNanohub\\nPhysiBoSS can be directly used via a Web interface on nanohub. This allows to run it without any installation and running directly on the server and can be used without any coding skills. The parameters of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server. It just required a nanohub account.\\nAvailable simulations tools of PhysiBoSS can be found on https://nanohub.org/resources/tools, under the keywords PhysiBoSS or PhysiBoSSa.\\nA model of tumors cell spheroid growing and invading into the surrounding extra-cellular matrix (ECM) is currently available PhysiBoSSa_ECM. Various parameters as the density of the extracellular matrix, the cell motility, ECM degradation from the cells, TGF-beta production... can be tuned by the user.\\nDocumentation\\nCode-oriented documentation can be generated with Doxygen:\\nmake doc\\nin the main directory.\\nIt can be configured in the Doxyfile file present in this directory.\\nIt will generate the html documentation in the doc/html directory.\\nYou can visualize it in a browser, e.g.:\\nfirefox doc/html/index.html &\\nYou can also refer to (future) publications with PhysiBoSS for scientific applications of this software and description of the models.\\nStep-by-step examples with the necessary files to run them are also proposed in the \\'examples\\' directory and on the Wiki of this repository.\\nReferences\\n For PhysiBoSS: \\n\\nPhysiBoSS publication: Letort G, Montagud A, Stoll G, Heiland R, Barillot E, Macklin P, Zinovyev A, Calzone L .  PhysiBoSS: a multi-scale agent-based modelling framework integrating physical dimension and cell signalling.  Bioinformatics, bty766, doi:10.1093/bioinformatics/bty766\\n\\n\\nFor PhysiCell: \\nPaul Macklin\\'s lab website  \\nPhysiCell publication: A. Ghaffarizadeh, S.H. Friedman, S.M. Mumenthaler, and P. Macklin, PhysiCell: an Open Source Physics-Based Cell Simulator for 3-D Multicellular Systems, bioRxiv 088773, 2016. DOI: 10.1101/088773. \\nBioFVM website  \\nBioFVM publication: A. Ghaffarizadeh, S.H. Friedman, and P. Macklin. BioFVM: an efficient, parallelized diffusive transport solver for 3-D biological simulations. Bioinformatics, 2015. \\n\\n\\nFor MaBoSS:\\nMaBoSS website  \\nMaBoSS publication: Stoll G, Viara E, Barillot E, Calzone L. Continuous time Boolean modeling for biological signaling: application of Gillespie algorithm. BMC Syst Biol. 2012 Aug 29;6:116. doi: 10.1186/1752-0509-6-116. \\n\\n\\nRemarks\\nPlease, refer to the Wiki of this repository for a much more extended documentation, with step by step examples instructions.\\nPhysiCell is developed in Paul Macklin\\'s lab.\\nMaBoSS and PhysiBoSS are developed in the Computational Systems Biology of Cancer group at Institut Curie (Paris, France).\\nWe invite you to use PhysiBoSS for you research and give feedbacks to us. Any help in developing it further is more than welcome.\\nDo not hesitate to contact us for any comments or difficulties in using PhysiBoSS: physiboss@gmail.com.\\nWishing you to enjoy using PhysiBoSS,\\nPhysiBoSS\\'s team.\\n'},\n",
       " {'language': 'C++ 79.4',\n",
       "  'readme': '\\n\\n\\nCore Principles\\nThese are the core principles of object-oriented approach to the current state of artificial neural networks that is inspired by synaptic plasticity between biological neurons:\\n\\nUnlike the current ANN implementations, neurons must be objects not tensors between matrices.\\nJust the current ANN implementations, neurons should be GPU accelerated (ideally) to provide the necessary parallelism.\\nWhile the current ANN implementations can only create special cases, a Plexus Network must be architecture-free (i.e. adaptive) to create a generalized solution of all machine learning problems.\\nInstead of dealing with decision of choosing an ANN layer combination(such as Convolution, Pooling or Recurrent layers), the network must have a layerless design.\\nThere must be fundamentally two types of neurons: sensory neuron, interneuron.\\nInput of the network must be made of sensory neurons. Any interneuron can be picked as a motor neuron (an element of the output). There are literally no difference between an interneuron and a motor neuron except the intervene of the network for igniting the wick of learning process through the motor neurons. Any non-motor interneuron can be assumed as a cognitive neuron which collectively forms the cognition of network.\\nThere can be arbitrary amount of I/O groups in a single network.\\nInstead of batch size, iteration, and epoch concepts, training examples must be fed on time basis with a manner like; learn first sample for X seconds, OK done? then learn second sample for Y seconds. By this approach, you can assign importance factors to your samples with maximum flexibility.\\nNetwork must be retrainable.\\nNetwork must be modular. In other words: You must be able to train a small network and then plug that network into a bigger network (we are talking about some kind of self-fusing here).\\nNeurons must exhibit the characteristics of cellular automata just like Conway\\'s Game of Life.\\nNumber of neurons in the network can be increased or decreased (scalability).\\nThere must be no need for a network-wide oscillation. Yet the execution of neurons should follow a path very similar to flow of electric current nevertheless.\\nNetwork should use randomness and/or uncertainty principle flawlessly. Consciousness is an emergent property from cellular level to macro scale, the network. But it\\'s also an emergent property for the neuron from quantum level uncertainty to cellular mechanisms. In such a way that randomness is the cause of the illusion of consciousness.\\nMost importantly, the network must and can not iterate through the whole dataset. Besides that, it\\'s also generally impossible to iterate the whole dataset on real life situations if the system is continuous like in robotics. Because of that; the network must be designed to handle such a continuous data stream that literally endless and must be designed to handle that data stream chunk by chunk. Therefore, when you are feeding the network, use a diverse feed but not a grouped feed (like 123123123123123123 but not like 111111222222333333).\\n\\nActivation function\\nThe activation function that used by Plexus is Sigmoid:\\n\\n\\n\\nand the derivative of the Sigmoid function:\\n\\n\\n\\nImplementation of this algorithm in Python programming language is publicly accessible through this link: https://github.com/mertyildiran/Plexus/blob/master/plexus/plexus.py\\nYou can directly skip to Application part if you are not willing to understand the mathematical and algorithmic background.\\nBasics\\nPlexus Network has only two classes; Network and Neuron. In a Plexus Network, there are many instances of Neuron class but there is only one instance of Network class.\\nWhen you crate a new Plexus Network you give these five parameters to the Network class: size of the network, input dimension, output dimension, connectivity rate, precision. The network accordingly builds itself.\\n\\n\\n\\n\\nsize is literally equal to total number of neurons in the network. All neurons are referenced in an instance variable called Network.neurons\\ninput dimension specifies the number of sensory neurons. Sensory neurons are randomly selected from neurons.\\noutput dimension specifies the number of motor neurons. Motor neurons are randomly selected from non-sensory neurons.\\nnumber of neurons multiplied by connectivity rate gives the average number of subscriptions made by a single neuron.\\n\\n\\n\\n\\n\\nprecision simply defines the precision of the all calculations will be made by neurons (how many digits after the decimal point).\\n\\nAfter the network has been successfully created. It will ignite itself automatically. Ignition in simple terms, no matter if you have plugged in some data or not, it will fire the neurons with using some mechanism very similar to flow of electric current (will be explained later on this paper).\\nAnatomy of a Single Neuron\\nA single neuron in a Plexus Network, takes the network as the only parameter and stores these seven very important information (in it\\'s instance variables): subscriptions, publications, potential, desired_potential, loss and type\\n\\n\\n\\nThere are eventually there types of neurons:\\n\\nNeuron.type = 1 means it\\'s a sensory neuron.\\nNeuron.type = 2 means it\\'s a motor neuron.\\nNeuron.type = 0 means it\\'s neither a sensory nor a motor neuron. It means it\\'s an cognitive interneuron (or just cognitive neuron).\\n\\nFunctionality of a neuron is relative to its type.\\nsubscriptions is neuron\\'s indirect data feed. Each non-sensory neuron subscribes to some other neurons of any type. For sensory neurons, subscriptions are completely meaningless and empty by default because it gets its data feed from outside world by assignments of the network. Subscriptions are literally the Plexus Network equivalent of Dendrites in biological neurons. subscriptions is a dictionary that holds Neuron(reference) as key and Weight as value.\\npublications holds literally the mirror data of subscriptions in the target neurons. In other words; any subscription creates also a publication reference in the target neuron. Similarly, publications is the Plexus Network equivalent of Axons in biological neurons.\\npotential p is the overall total potential value of all subscriptions multiplied by the corresponding weights. Only in sensory neurons, it is directly assigned by the network. Value of potential may only be updated by the neuron\\'s itself and its being calculated by this simple formula each time when the neuron is fired:\\n\\n\\n\\n\\n\\n\\ndesired_potential p\\' is the ideal value of the neuron\\'s potential that is desired to eventually reach. For sensory neurons, it is meaningless. For motor neurons, it is assigned by the network. If it\\'s None then the neuron does not learn anything and just calculates potential when it\\'s fired.\\nloss l is calculated not just at the output but in every neuron except sensory ones and it is equal to absolute difference (distance) between desired potential and current potential.\\n\\n\\n\\nAll numerical values inside a neuron are floating point numbers and all the calculations obey to the precision that given at start.\\nSensory and Motor Neurons\\nInput Layer in classical neural networks renamed as Sensory Neurons in Plexus networks, and Target/Output Layer renamed as Motor Neurons. This naming convention is necessary cause the built of the relevance of artificial neural networks with biological neural networks and Neuroscience.\\nThe difference of sensory neurons from the cognitive neurons (that neither sensory nor motor ones) is, they do not actually fire. They just stand still for the data load. They do not have any subscriptions to the other neurons (literally no subscriptions). But they can be subscribed by the other neurons, including motor ones. They do not learn, they do not consume any CPU resources. They just stored in the memory. You can assign an image, a frame of a video, or a chunk of an audio to a group of sensory neurons.\\nThe difference of motor neurons form the other neurons is, they are only responsible to the network. They act as the fuse of the learning and calculation of the loss. The network dictates a desired potential on each motor neuron. The motor neuron calculates its potential, compares it with desired potential, calculates the loss then tries to update its weights randomly many times and if it fails, it blames its subscriptions. So just like the network, motor neurons are also able to dictate a desired potential on the other non-motor neurons. This is why any neuron holds an additional potential variable called desired_potential.\\nPartially Subscribe\\nOn the second phase of the network initiation, any non-sensory neurons are forced to subscribe to some non-motor neurons which are selected by random sampling. Length of this sample is also selected by random sampling (rounds to nearest integer) is done from a normal distribution. Such a normal distribution that, the average number of subscriptions is the mean, and square root of the mean is the standard deviation. (e.g. if neurons on average has 100 subscriptions then the mean is 100 and the standard deviation is 10)\\n\\n\\n\\nAlgorithm\\nEven so the Python implementation of Plexus Network is easy to understand, it will be helpful for readers to explain the algorithm in pseudocode;\\nInitiation\\nprocedure initiate the network is\\n    connectivity ← size * connectivity_rate;\\n    connectivity_sqrt ← sqrt(connectivity);\\n    connectivity_sqrt_sqrt ← sqrt(connectivity_sqrt);\\n    for item in size, do\\n        create neuron;\\n    end\\n    pick sensory neurons randomly;\\n    pick motor neurons randomly;\\n    determine non-sensory neurons;\\n    determine non-motor neurons;\\n    initiate subscriptions;\\n    initiate instance variables;\\n    ignite the network;\\nInitiation is nothing more than a make the assignments for once phase until the ignition. The final step (ignition) never stops but can be paused (if user wants).\\nInitiate Subscriptions\\nprocedure initiate subscriptions is\\n    for neuron in neurons, do\\n        if neuron is not a sensory neuron, then\\n            call neuron.partially_subscribe();\\n        end\\n    end\\n    return True;\\nPartially Subscribe\\nprocedure partially subscribe is\\n    sample ← randomly sample approximately \"connectivity\" units of a neuron from within all non-motor neurons;\\n    for neuron in sample, do\\n        if neuron is not self, then\\n            establish a subscription;    // weight is randomly assigned\\n            establish a publication;\\n        end\\n    end\\n    return True;\\nThe time complexity of the procedure initiate subscriptions is O(n2), so this may take a while if the size of the network and connectivity is big.\\nIgnite\\nprocedure ignite subscriptions is\\n    create an empty ban_list;\\n    while network is not frozen, do\\n        if next_queue is empty, then\\n            get the output of network and print it;\\n            increase the wave_counter;\\n            if first_queue is empty, then\\n                for neuron in sensory neurons, do\\n                    for target_neuron in neuron.publications, do\\n                        append target_neuron to first_queue;\\n                    end\\n                end\\n                copy first_queue to next_queue;\\n            end\\n        end\\n        copy next_queue to current_queue;\\n        empty next_queue;\\n        for neuron in ban_list, do\\n            if neuron.ban_counter > connectivity_sqrt_sqrt, then\\n                remove the neuron from current_queue;\\n            end\\n        end\\n        while current_queue is not empty, do\\n            neuron ← select a random neuron from current_queue;\\n            remove the neuron from current_queue;\\n            if neuron.ban_counter <= connectivity_sqrt_sqrt, then\\n                call neuron.fire();\\n                append the neuron to ban_list;\\n                increase neuron.ban_counter;\\n                for target_neuron in neuron.publications, do\\n                    append target_neuron to next_queue;\\n                end\\n            end\\n        end\\n    end\\nProcedure ignite regulates the firing order of neurons and creates an effect very similar to flow of electric current, network wide. It continuously runs until the network frozen, nothing else can stop it. It fires the neurons step by step through adding them to a queue.\\nIt generates its first queue from the publications of sensory neurons. Time complexity of if next_queue is empty, then block is O(n2) but it can be ignored (unless there are too many sensory neurons) because it runs once per wave.\\nIt eliminates banned neurons with for neuron in ban_list, do block. Function of ban_counter is giving neurons connectivity_sqrt_sqrt amount of chance after they added to ban_list. Then it fires the neurons inside current_queue, one by one, choosing them randomly.\\nAfter a neuron fired, it adds the fired neuron to ban_list and lastly copies the publications of that neuron to next_queue so execution(firing process) can follow the path through the connections.\\nEach execution from first sensory neuron to last motor neuron symbolizes one wave. Every time a wave finished, procedure falls into if next_queue is empty, then block so wave starts over from the sensory neurons.\\nban_counter and connectivity_sqrt_sqrt comparison creates execution loops inside cognitive neurons and these loops act like memory units which is a pretty important concept. Because loops create the relation between currently fed data and previously learned data. Without these loops the network fails on both classification and regression problems.\\nBecause neuron.fire() has a time complexity of O(n2), each turn inside while network is not frozen, do block, has a time complexity of O(n4). But don\\'t worry because it will approximate to O(n3) because of the probabilistic nature of fire function and the network will fire more than a million of neurons per minute. By the way, while network is not frozen, do block is ignored because it\\'s an endless loop under normal conditions.\\nFire\\nprocedure fire is\\n    if self is not a sensory neuron, then\\n        potential ← calculate potential;\\n        increase fire counter;\\n        if desired_potential is not None, then\\n\\n            loss ← calculate loss;\\n            if loss = 0, then\\n                desired_potential ← None;\\n                return True;\\n            end\\n            if blame_lock is not empty, then\\n                if (wave_counter - blame_lock) < connectivity, then\\n                    return True;\\n                else\\n                    blame_lock ← None;\\n            end\\n\\n            try connectivity times:\\n                generate new weights randomly;\\n                calculate new potential and new loss according to these weights;\\n                if loss_new < loss_current, then return True;\\n            end\\n\\n            try sqrt(connectivity) times:\\n                generate hypothetical potentials for neurons in subscriptions randomly;\\n                calculate new potential and new loss according to these hypothetical potentials;\\n                if loss_new < loss_current, then\\n                    apply these hypothetical potentials as \"desired_potential\"s;\\n                    return True;\\n                end\\n            end\\n\\n            if (still) not improved, then\\n                either create some new subscriptions;\\n                or break some of the subscriptions;\\n                return True;\\n            end\\n\\n        end\\n    end\\nProcedure fire handles all feedforwarding, backpropagation and learning process by itself. fire function is an instance method of Neuron class. This procedure is by far the most important one in the Plexus Network. It\\'s basically the core function and CPU spends most of its time to execute fire functions again and again.\\nIf desired_potential is not assigned to a value, then it just calculates the potential and finishes.\\nIf desired_potential is assigned to a value, then first it calculates the loss. If loss is equal to zero, then the current state of the neuron is perfectly well and there is nothing to learn.\\nIf blame_lock is not empty, then it will pass this function connectivity times with this control statement: if blame_lock is not empty, then.\\n\\n\\n\\nIt tries to improve the current state of the neuron by updating its weights randomly, connectivity times. If it\\'s improved, then break.\\nIt tries to improve the current state of the neuron by dictating randomly generated hypothetical potentials over the subscriptions, square root of connectivity times. If it\\'s improved, then break.\\nIf it still is not improved, then it either creates some new subscriptions or breaks some of the subscriptions it currently has and hopes it will lead the neuron to new improvements in the future.\\nOn the first wave, the fire function is only meaningful for motor neurons but after the first wave desired_potential dictation will spread throughout the cognitive neurons.\\nLoad\\nprocedure load (input, output) is\\n    if output is None, then\\n        for neuron in motor neurons, do\\n            neuron.desired_potential ← None;\\n        end\\n    end\\n    if (number of sensory neurons is not equal to input length), then\\n        raise an error but do not interrupt the network;\\n    else\\n        for neuron in sensory neurons, do\\n            neuron.potential ← load from the input;\\n        end\\n    end\\n    if (number of motor neurons is not equal to output length), then\\n        raise an error but do not interrupt the network;\\n    else\\n        for neuron in motor neurons, do\\n            neuron.desired_potential ← load from the output;\\n        end\\n    end\\nProcedure load is the only method that you can feed your data to the network. You should call that function and load your data in real time. Also you should do it periodically and continuously, like every 3 seconds. If you leave second parameter empty then this procedure will automatically assume that you are testing the network, so it will replace desired_potential values of motor neurons with None. Otherwise, it means you are training the network so it will load the input data to sensory neurons and it will load the output data to desired_potential values of motor neurons.\\nApplication\\nInstallation of the Python Package\\npip install plexus\\nIf you want to install Plexus on development mode:\\ngit clone https://github.com/mertyildiran/Plexus.git\\ncd Plexus/\\npip install -e .\\nor alternatively:\\nmake dev\\nand test the installation with:\\nmake cpp\\nExamples\\nBinary Classification Example\\n(you can alternatively run this example with python3 examples/classification_binary.py command using a pre-written script version of below commands)\\nSuppose you need to train the network to figure out that the elements of given arrays are bigger than 0.5 or not (like [0.9, 0.6, 1.0, 0.8] or [0.1, 0.3, 0.0, 0.4]) and suppose it\\'s a 4-element array. So let\\'s create a network according to your needs:\\nimport cplexus as plexus\\n\\nSIZE = 14\\nINPUT_SIZE = 4\\nOUTPUT_SIZE = 2\\nCONNECTIVITY = 1\\nPRECISION = 2\\nRANDOMLY_FIRE = False\\nDYNAMIC_OUTPUT = True\\nVISUALIZATION = False\\nnet = plexus.Network(\\n    SIZE,\\n    INPUT_SIZE,\\n    OUTPUT_SIZE,\\n    CONNECTIVITY,\\n    PRECISION,\\n    RANDOMLY_FIRE,\\n    DYNAMIC_OUTPUT,\\n    VISUALIZATION\\n)\\nIf you want to visualize the network using PyQtGraph enable VISUALIZATION = False. Because our network is automatically initiated and ignited, now all we have to do is training the network. So let\\'s train our network with 80 samples:\\nPRECISION = 2\\nTRAINING_SAMPLE_SIZE = 20\\nfor i in range(1, TRAINING_SAMPLE_SIZE):\\n    if (i % 2) == 0:\\n        output = [1.0, 0.0]\\n        generated_list = generate_list_bigger()\\n        notify_the_load(generated_list, output, TRAINING_DURATION)\\n        net.load(generated_list, output)\\n    else:\\n        output = [0.0, 1.0]\\n        generated_list = generate_list_smaller()\\n        notify_the_load(generated_list, output, TRAINING_DURATION)\\n        net.load(generated_list, output)\\n    time.sleep(TRAINING_DURATION)\\nYou should load your data one by one from each kind, respectively. Because it will prevent over fitting to one specific kind. You must wait a short time like TRAINING_DURATION = 0.01 seconds (which is a reasonable duration in such a case), after each load.\\noutput[0] will converge to detect bigger than 0.5 inputs.\\noutput[1] will converge to detect smaller than 0.5 inputs.\\nBefore the testing you should define a criteria called DOMINANCE_THRESHOLD so you can catch the decision making. Now let\\'s test the network:\\nerror = 0\\nerror_divisor = 0\\nfor i in repeat(None, TESTING_SAMPLE_SIZE):\\n    binary_random = random.randint(0, 1)\\n    if binary_random == 0:\\n        generated_list = generate_list_bigger()\\n        expected = [1.0, 0.0]\\n    else:\\n        generated_list = generate_list_smaller()\\n        expected = [0.0, 1.0]\\n\\n    net.load(generated_list)\\n    time.sleep(TRAINING_DURATION)\\n\\n    output = net.output\\n    error += abs(expected[0] - output[0])\\n    error += abs(expected[1] - output[1])\\n    error_divisor += 2\\nWith the while loop given above, you will be able to check the output by giving enough time to propagate your input throught the network. By giving net.load() only one parameter here, you automatically disable the training.\\nNow freeze your network and calculate the overall error:\\nnet.freeze()\\nerror = error / error_divisor\\nwhich outputs:\\nOverall error: 0.010249996604397894\\n\\nClassifying Prime Numbers Example\\nThis example is quite simple to the previous example but this time we are teaching the network to understand if the given number is prime or not. Which is a relatively complex problem.\\nRun python3 examples/classification_prime.py 1 -l cpp to see the result. You will observe that the network is able to learn the solution for such a complex problem in the matter of seconds.\\nSequence Basic Example\\nIn this example, instead of classification, we will train the network to detect a pattern in given sequence. The magic here is; without even changing anything related to network, just by changing logic we feed the data into the network, the network automatically turns into a Recurrent Neural Network.\\nRun python3 examples/sequence_basic.py to see the output. This is the output you should see:\\n___ PLEXUS NETWORK BASIC SEQUENCE RECOGNITION EXAMPLE ___\\n\\nCreate a Plexus network with 14 neurons, 4 of them sensory, 1 of them motor, 1 connectivity rate, 2 digit precision\\n\\nPrecision of the network will be 0.01\\nEach individual non-sensory neuron will subscribe to 14 different neurons\\n14 neurons created\\n4 neuron picked as sensory neuron\\n1 neuron picked as motor neuron\\nNetwork has been ignited\\n\\n*** LEARNING ***\\n\\nGenerate The Dataset (20 Items Long) To Recognize a Sequence & Learn for 0.1 Seconds Each\\nLoad Input: [1.0, 0.0, 0.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 1.0, 0.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 0.0, 1.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 0.0, 0.0, 1.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [1.0, 0.0, 0.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 1.0, 0.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 0.0, 1.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 0.0, 0.0, 1.0]\\tOutput: [1.0]\\tand wait 0.1 seconds\\nLoad Input: [1.0, 0.0, 0.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 1.0, 0.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 0.0, 1.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 0.0, 0.0, 1.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [1.0, 0.0, 0.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 1.0, 0.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 0.0, 1.0, 0.0]\\tOutput: [0.0]\\tand wait 0.1 seconds\\nLoad Input: [0.0, 0.0, 0.0, 1.0]\\tOutput: [1.0]\\tand wait 0.1 seconds\\n...\\n\\nby looking at this output, you should be able to see the pattern. Now on testing stage you can see how successful the network is on detecting the pattern:\\n*** TESTING ***\\n\\nTest the network with random data (20 times)\\nLoad Input: ([1.0, 0.0, 0.0, 0.0], [0.0])\\tRESULT: 0.019999999552965164\\nLoad Input: ([0.0, 1.0, 0.0, 0.0], [0.0])\\tRESULT: 0.0\\nLoad Input: ([0.0, 0.0, 1.0, 0.0], [0.0])\\tRESULT: 0.019999999552965164\\nLoad Input: ([0.0, 0.0, 0.0, 1.0], [0.0])\\tRESULT: 0.019999999552965164\\nLoad Input: ([1.0, 0.0, 0.0, 0.0], [0.0])\\tRESULT: 0.0\\nLoad Input: ([0.0, 1.0, 0.0, 0.0], [0.0])\\tRESULT: 0.0\\nLoad Input: ([0.0, 0.0, 1.0, 0.0], [0.0])\\tRESULT: 0.05999999865889549\\nLoad Input: ([0.0, 0.0, 0.0, 1.0], [1.0])\\tRESULT: 0.6600000262260437\\nLoad Input: ([1.0, 0.0, 0.0, 0.0], [0.0])\\tRESULT: 0.019999999552965164\\nLoad Input: ([0.0, 1.0, 0.0, 0.0], [0.0])\\tRESULT: 0.0\\nLoad Input: ([0.0, 0.0, 1.0, 0.0], [0.0])\\tRESULT: 0.05999999865889549\\nLoad Input: ([0.0, 0.0, 0.0, 1.0], [0.0])\\tRESULT: 0.6600000262260437\\nLoad Input: ([1.0, 0.0, 0.0, 0.0], [0.0])\\tRESULT: 0.019999999552965164\\nLoad Input: ([0.0, 1.0, 0.0, 0.0], [0.0])\\tRESULT: 0.0\\nLoad Input: ([0.0, 0.0, 1.0, 0.0], [0.0])\\tRESULT: 0.0\\nLoad Input: ([0.0, 0.0, 0.0, 1.0], [1.0])\\tRESULT: 0.9800000190734863\\n...\\n\\nand the overall error:\\nNetwork is now frozen\\n\\n1786760 waves are executed throughout the network\\n\\nIn total: 66110093 times a random non-sensory neuron is fired\\n\\n\\nOverall error: 0.040124996623490006\\n\\nCatDog Example\\n(you can alternatively run this example with python3 examples/catdog.py command using a pre-written script version of below commands)\\nSuppose you need to train the network to figure out that the given image (32x32 RGB) is an image of a cat or a dog and map them to blue and red respectively. So let\\'s create a network according to your needs:\\nSIZE = 32 * 32 * 3 + 3 + 256\\nINPUT_SIZE = 32 * 32 * 3\\nOUTPUT_SIZE = 3\\nCONNECTIVITY = 0.005\\nPRECISION = 3\\nTRAINING_DURATION = 3\\nRANDOMLY_FIRE = False\\nDYNAMIC_OUTPUT = False\\nVISUALIZATION = False\\nnet = plexus.Network(\\n    SIZE,\\n    INPUT_SIZE,\\n    OUTPUT_SIZE,\\n    CONNECTIVITY,\\n    PRECISION,\\n    RANDOMLY_FIRE,\\n    DYNAMIC_OUTPUT,\\n    VISUALIZATION\\n)\\nWe will plug in 32x32 RGB to the network so we need 3072 sensory neurons. 3 motor neurons for see how RGB our result is and 256 cognitive neurons to train. We need 3 digits precision because we need to store 255 different values between 0.0 and 1.0 range.\\nExplaining the answer of How to load CIFAR-10 dataset and use it is out of the scope of this paper but you can easily understand it by reading the code: examples/catdog.py Once you get the numpy array of CIFAR-10 (or any other image data) just normalize it and load:\\nTRAINING_SAMPLE_SIZE = 20\\nfor i in range(1, TRAINING_SAMPLE_SIZE):\\n    if (i % 2) == 0:\\n        cat = random.sample(cats, 1)[0]\\n        cat_normalized = np.true_divide(cat, 255).flatten()\\n        blue_normalized = np.true_divide(blue, 255).flatten()\\n        cv2.imshow(\"Input\", cat)\\n        net.load(cat_normalized, blue_normalized)\\n    else:\\n        dog = random.sample(dogs, 1)[0]\\n        dog_normalized = np.true_divide(dog, 255).flatten()\\n        red_normalized = np.true_divide(red, 255).flatten()\\n        cv2.imshow(\"Input\", dog)\\n        net.load(dog_normalized, red_normalized)\\n    show_output(net)\\nYou will get an Overall error as the result very similar to examples above although this time the input length was 768 times bigger. This is because Plexus Network amalgamates the problems from all levels of difficulty on a single medium. It makes easy problems relatively hard, and hard problems relatively easy.\\nWhen you run this example, you will get a slightly better result when compared to flipping a coin. You will most likely get an Overall error between 0.35 - 0.45 which is the proof that the network is able to learn something.\\nBy the way, don\\'t forget that; Plexus Network does not iterate over the dataset and furthermore it runs in real-time. Also you have trained the network just for 4-5 minutes. Now let\\'s see what happens if we train our network for a long period of time:\\nNote\\nImplementation of GPU acceleration and saving the trained network to disk are in work-in-progress (WIP) state. Therefore some parts of the implementation are subject to change.\\n'},\n",
       " {'language': 'C++ 96.1',\n",
       "  'readme': \"CATH Tools  \\nProtein structure comparison tools such as SSAP, as used by the Orengo Group in curating CATH.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExecutable DOWNLOADS   (for Linux/Mac; chmod them to be executable)\\nDocs   \\xa0 \\nCode   \\xa0 \\nBuilds   \\xa0 \\nExtras repo   \\xa0 \\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n cath-cluster   Complete-linkage cluster arbitrary data.\\n\\n\\n\\n cath-map-clusters   Map names from previous clusters to new clusters based on (the overlaps between) their members (which may be specified as regions within a parent sequence). Renumber any clusters with no equivalents.\\n\\n\\n\\n cath-resolve-hits   Collapse a list of domain matches to your query sequence(s) down to the non-overlapping subset (ie domain architecture) that maximises the sum of the hits' scores.\\n\\n\\n\\n cath-ssap   Structurally align a pair of proteins.\\n\\n\\n\\n cath-superpose   Superpose two or more protein structures using an existing alignment.\\n\\n\\n\\nExtra Tools\\n\\nbuild-test          Perform the cath-tools tests (which should all pass, albeit with a few warnings)\\ncath-assign-domains Use an SVM model on SSAP+PRC data to form a plan for assigning the domains to CATH superfamilies/folds\\ncath-refine-align   Iteratively refine an existing alignment by attempting to optimise SSAP score\\ncath-score-align    Score an existing alignment using structural data\\n\\nAuthors\\nThe SSAP algorithm (cath-ssap) was devised by Christine A Orengo and William R Taylor.\\nPlease cite: Protein Structure Alignment, Taylor and Orengo, Journal of Molecular Biology 208, 1-22, PMID: 2769748. (PubMed, Elsevier)\\nSince then, many people have contributed to this code, most notably:\\n\\nTony E Lewis             (2011–…)\\nOliver C Redfern                                          (~2003–2011)\\nJames E Bray, Ian Sillitoe (~2000–2003)\\nAndrew C R Martin    (considerable edits around 2001)\\n\\nAcknowledgements\\ncath-ssap typically uses DSSP, either by reading DSSP files or via its own implementation of the DSSP algorithms.\\ncath-cluster uses Fionn Murtagh's reciprocal-nearest-neighbour algorithm (see Multidimensional clustering algorithms, volume 4 of Compstat Lectures.\\nPhysica-Verlag, Würzburg/ Wien, 1985. ISBN 3-7051-0008-4) as described and refined in Daniel Müllner's Modern hierarchical, agglomerative clustering algorithms (2011, arXiv:1109.2378).\\nFeedback\\nPlease tell us about your cath-tools bugs/suggestions here.\\nIf you find this software useful, please spread the word and star the GitHub repo.\\n\"},\n",
       " {'language': 'C++ 98.2',\n",
       "  'readme': 'skew-biology\\nSoftware for my DIY spectrometers, sensors, pcr, and incubators.\\nSpectrometer Software\\nSoftware for reading a spectrum from a DIY spectrometer using OpenCV,\\ncalibrating it using non-linear regression to the function with the gnu scientific library.\\nExample DIY Lego Spectrometer\\n\\n\\n1k diffraction grating\\nSony IMX179 8MP CCD\\nVelcro, aluminum foil, and a lot of black legos\\nCCD at m=0\\n\\nUsage:\\nspec [roi x y w h] [cal C0 C1 C2]\\nThe ROI and constants for calibration can be specified on the command line. The calculated intensity and wavelength can be saved to csv alongside the calibration file for analysis in other programs.\\nCalibration Details\\nThis software currently uses a second-order polynomial for calibration off a minimum of three points. There may be value in moving to a cubic function.\\nnm = C0 + C1p + C2p^2\\nCalibration References\\n\\nCalibrating the Wavelength of Your Spectrometer\\nOceanOptics Cubic Calibration\\nPublicLab Linear Calibration\\n\\nSelecting a Region of Interest\\nOn startup the region of interest can be selected with a mouse. The pixels in this region of interest are summed to calculate intensity.\\nCommands:\\n\\n\\n\\nCommand\\nNote\\n\\n\\n\\n\\ntop\\nlists the top wavelengths, intensities, and pixel indicies\\n\\n\\ncal\\nenters calibration mode\\n\\n\\nsave\\nsaves to csv along with the calibration\\n\\n\\n\\nBuilding:\\ncmake .\\nmake\\n\\nExample usage:\\nCalibrating a CFL using the mercury peaks terbium peaks. Appears to accurately predict europium peaks at 612nm.\\nCFL Spectrum Reference\\n\\nIncubator\\nPID controled incubator w/ IR\\nRGB\\nsimple rgb sensor control for recording the rgb values of a sample\\nbeing incubated by a connected computer\\n'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('read_me.json', 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
