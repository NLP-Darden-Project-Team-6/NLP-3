{"language":{"0":"JavaScript 98.7","1":"JavaScript 97.4","2":"JavaScript 60.0","3":"JavaScript 63.6","4":"JavaScript 39.3","5":"JavaScript 91.4","6":"JavaScript 100.0","7":"JavaScript 72.0","8":"JavaScript 100.0","9":"JavaScript 97.0","10":"JavaScript 98.0","11":"JavaScript 61.9","12":"JavaScript 74.5","13":"JavaScript 95.7","14":"JavaScript 94.8","15":"JavaScript 88.4","16":"JavaScript 100.0","17":"JavaScript 49.7","18":"JavaScript 96.4","19":"JavaScript 51.4","20":"JavaScript 97.4","21":"JavaScript 63.8","22":"JavaScript 100.0","23":"JavaScript 88.9","24":"JavaScript 46.0","25":"JavaScript 60.8","26":"JavaScript 80.3","27":"JavaScript 44.1","28":"JavaScript 54.8","29":"JavaScript 77.5","30":"JavaScript 95.2","31":"JavaScript 67.1","32":"JavaScript 86.9","33":"JavaScript 99.0","34":"JavaScript 84.8","35":"JavaScript 98.4","36":"JavaScript 99.6","37":"JavaScript 68.8","38":"JavaScript 99.9","39":"JavaScript 52.8","40":"JavaScript 89.3","41":"JavaScript 68.9","42":"JavaScript 89.6","43":"JavaScript 53.2","44":"JavaScript 91.9","45":"JavaScript 98.6","46":"JavaScript 97.6","47":"JavaScript 99.7","48":"JavaScript 99.7","49":"Python 100.0","50":"Python 96.3","51":"Python 100.0","52":"Python 89.5","53":"Python 100.0","54":"Python 99.3","55":"Python 100.0","56":"Python 100.0","57":"Python 100.0","58":"Python 99.0","59":"Python 61.9","60":"Python 99.9","61":"Python 89.0","62":"Python 98.1","63":"Python 96.0","64":"Python 87.9","65":"Python 75.8","66":"Python 99.1","67":"Python 100.0","68":"Python 59.0","69":"Python 93.2","70":"Python 59.8","71":"Python 92.7","72":"Python 100.0","73":"Python 51.4","74":"Python 91.1","75":"Python 100.0","76":"Python 76.6","77":"Python 100.0","78":"Python 100.0","79":"Python 100.0","80":"Python 72.1","81":"Python 69.3","82":"Python 100.0","83":"Python 99.4","84":"Python 100.0","85":"Python 100.0","86":"Python 98.8","87":"Python 97.6","88":"Python 100.0","89":"Python 97.5","90":"Python 99.7","91":"Python 100.0","92":"Python 46.6","93":"Python 52.3","94":"Python 76.1","95":"Python 98.6","96":"Python 100.0","97":"Python 99.7","98":"Python 93.0","99":"Java 100.0","100":"Java 63.9","101":"Java 89.4","102":"Java 79.8","103":"Java 69.8","104":"Java 100.0","105":"Java 100.0","106":"Java 76.3","107":"Java 100.0","108":"Java 100.0","109":"Java 100.0","110":"Java 94.1","111":"Java 96.0","112":"Java 38.9","113":"Java 42.1","114":"Java 54.4","115":"Java 71.5","116":"Java 80.0","117":"Java 79.9","118":"Java 100.0","119":"Java 33.3","120":"Java 87.7","121":"Java 86.3","122":"Java 59.6","123":"Java 98.2","124":"Java 94.4","125":"Java 100.0","126":"Java 100.0","127":"Java 86.0","128":"Java 100.0","129":"Java 91.4","130":"Java 100.0","131":"Java 99.3","132":"Java 100.0","133":"Java 100.0","134":"Java 98.7","135":"Java 100.0","136":"Java 100.0","137":"Java 100.0","138":"Java 100.0","139":"Java 99.8","140":"Java 98.1","141":"Java 78.4","142":"Java 100.0","143":"Java 100.0","144":"Java 98.1","145":"Java 99.0","146":"Java 100.0","147":"Java 100.0","148":"Java 97.5","149":"C++ 93.2","150":"C++ 96.2","151":"C++ 100.0","152":"C++ 78.1","153":"C++ 71.7","154":"C++ 91.9","155":"C++ 72.6","156":"C++ 88.4","157":"C++ 74.6","158":"C++ 100.0","159":"C++ 68.3","160":"C++ 98.3","161":"C++ 92.4","162":"C++ 66.5","163":"C++ 89.3","164":"C++ 98.7","165":"C++ 76.6","166":"C++ 60.4","167":"C++ 88.0","168":"C++ 100.0","169":"C++ 53.5","170":"C++ 92.2","171":"C++ 51.7","172":"C++ 88.8","173":"C++ 61.7","174":"C++ 64.1","175":"C++ 80.9","176":"C++ 87.6","177":"C++ 84.1","178":"C++ 97.1","179":"C++ 99.0","180":"C++ 54.2","181":"C++ 100.0","182":"C++ 67.8","183":"C++ 82.7","184":"C++ 89.0","185":"C++ 68.7","186":"C++ 99.2","187":"C++ 94.9","188":"C++ 97.5","189":"C++ 90.3","190":"C++ 66.9","191":"C++ 76.1","192":"C++ 85.4","193":"C++ 67.9","194":"C++ 71.9","195":"C++ 77.9","196":"C++ 79.4","197":"C++ 96.1","198":"C++ 98.2","199":"JavaScript 89.8","200":"JavaScript 84.8","201":"JavaScript 98.2","202":"JavaScript 55.9","203":"JavaScript 100.0","204":"JavaScript 100.0","205":"JavaScript 99.5","206":"JavaScript 100.0","207":"JavaScript 100.0","208":"JavaScript 72.3","209":"Python 95.9","210":"Python 100.0","211":"Python 100.0","212":"Python 100.0","213":"Python 100.0","214":"Python 96.2","215":"Python 94.9","216":"Python 100.0","217":"Python 97.9","218":"Python 94.0","219":"Java 100.0","220":"Java 98.9","221":"Java 100.0","222":"Java 99.2","223":"Java 100.0","224":"Java 99.3","225":"Java 100.0","226":"Java 100.0","227":"Java 100.0","228":"Java 76.2","229":"C++ 96.4","230":"C++ 98.7","231":"C++ 87.3","232":"C++ 90.0","233":"C++ 57.2","234":"C++ 81.7","235":"C++ 77.0","236":"C++ 71.8","237":"C++ 52.4","238":"C++ 47.7","239":"JavaScript 91.5","240":"JavaScript 100.0","241":"JavaScript 100.0","242":"JavaScript 93.9","243":"JavaScript 92.2","244":"JavaScript 73.3","245":"JavaScript 92.8","246":"JavaScript 100.0","247":"JavaScript 100.0","248":"JavaScript 100.0","249":"Python 99.3","250":"Python 81.3","251":"Python 100.0","252":"Python 100.0","253":"Python 100.0","254":"Python 100.0","255":"Python 100.0","256":"Python 100.0","257":"Python 100.0","258":"Python 51.5","259":"Java 99.6","260":"Java 100.0","261":"Java 99.3","262":"Java 100.0","263":"Java 100.0","264":"Java 100.0","265":"Java 81.7","266":"Java 100.0","267":"Java 100.0","268":"Java 99.8","269":"C++ 47.7","270":"C++ 81.4","271":"C++ 33.5","272":"C++ 57.5","273":"C++ 80.6","274":"C++ 75.8","275":"C++ 95.1","276":"C++ 96.6","277":"C++ 70.9","278":"C++ 92.7","279":"JavaScript 93.8","280":"JavaScript 100.0","281":"JavaScript 91.9","282":"JavaScript 95.6","283":"JavaScript 100.0","284":"JavaScript 100.0","285":"JavaScript 92.8","286":"JavaScript 100.0","287":"JavaScript 100.0","288":"JavaScript 97.4","289":"Python 100.0","290":"Python 56.4","291":"Python 88.4","292":"Python 96.7","293":"Python 100.0","294":"Python 100.0","295":"Python 98.3","296":"Python 100.0","297":"Python 100.0","298":"Python 99.5","299":"Java 95.3","300":"Java 100.0","301":"Java 98.0","302":"Java 100.0","303":"Java 61.1","304":"Java 62.1","305":"Java 100.0","306":"Java 100.0","307":"Java 93.4","308":"Java 100.0","309":"C++ 91.2","310":"C++ 66.8","311":"C++ 96.3","312":"C++ 88.1","313":"C++ 96.4","314":"C++ 71.8","315":"C++ 95.3","316":"C++ 100.0","317":"JavaScript 100.0","318":"JavaScript 96.6","319":"JavaScript 100.0","320":"JavaScript 95.8","321":"JavaScript 100.0","322":"JavaScript 100.0","323":"JavaScript 100.0","324":"JavaScript 84.0","325":"JavaScript 72.1","326":"JavaScript 89.5","327":"Python 51.0","328":"Python 98.7","329":"Python 100.0","330":"Python 97.8","331":"Python 99.8","332":"Python 70.4","333":"Python 70.8","334":"Python 97.6","335":"Python 91.9","336":"Java 100.0","337":"Java 100.0","338":"Java 88.2","339":"Java 100.0","340":"Java 45.1","341":"Java 100.0","342":"Java 79.9","343":"Java 59.3","344":"Java 100.0","345":"Java 100.0","346":"C++ 72.4","347":"C++ 88.3","348":"C++ 63.5","349":"C++ 92.7","350":"C++ 50.2","351":"C++ 63.0","352":"C++ 85.9","353":"C++ 92.7","354":"C++ 97.7","355":"C++ 58.9","356":"JavaScript 75.0","357":"JavaScript 100.0","358":"JavaScript 100.0","359":"JavaScript 100.0","360":"JavaScript 49.3","361":"JavaScript 80.8","362":"JavaScript 99.0","363":"JavaScript 82.9","364":"JavaScript 72.5","365":"JavaScript 100.0","366":"Python 73.3","367":"Python 85.7","368":"Python 100.0","369":"Python 78.0","370":"Python 100.0","371":"Python 81.3","372":"Python 100.0","373":"Python 100.0","374":"Python 100.0","375":"Java 100.0","376":"Java 100.0","377":"Java 100.0","378":"Java 100.0","379":"Java 77.8","380":"Java 85.1","381":"C++ 93.7","382":"C++ 52.9","383":"C++ 94.2","384":"C++ 73.6","385":"C++ 98.4","386":"C++ 98.5","387":"C++ 50.5","388":"C++ 55.3","389":"C++ 81.5","390":"C++ 56.1","391":"JavaScript 89.3","392":"JavaScript 99.4","393":"JavaScript 96.1","394":"JavaScript 89.7","395":"JavaScript 100.0","396":"JavaScript 100.0","397":"JavaScript 86.2","398":"JavaScript 89.6","399":"JavaScript 71.8","400":"JavaScript 100.0","401":"Python 100.0","402":"Python 100.0","403":"Python 100.0","404":"Python 99.7","405":"Python 100.0","406":"Java 69.8","407":"Java 100.0","408":"Java 100.0","409":"Java 96.9","410":"Java 100.0","411":"Java 97.8","412":"Java 100.0","413":"Java 100.0","414":"Java 100.0","415":"Java 100.0","416":"C++ 63.5","417":"C++ 69.1","418":"C++ 55.6","419":"C++ 93.4","420":"C++ 74.3","421":"JavaScript 56.2","422":"JavaScript 100.0","423":"JavaScript 96.6","424":"JavaScript 86.3","425":"JavaScript 100.0","426":"Python 100.0","427":"Python 98.4","428":"Python 100.0","429":"Python 100.0","430":"Python 65.1","431":"Python 94.1","432":"Python 100.0","433":"Python 100.0","434":"Python 100.0","435":"Python 100.0","436":"Java 60.2","437":"Java 100.0","438":"Java 100.0","439":"Java 91.1","440":"Java 100.0","441":"Java 100.0","442":"Java 100.0","443":"Java 100.0","444":"Java 100.0","445":"C++ 63.4","446":"C++ 89.0","447":"C++ 70.1","448":"C++ 66.5","449":"C++ 73.7","450":"C++ 87.1","451":"C++ 72.7","452":"C++ 96.8","453":"C++ 60.9","454":"C++ 86.1","455":"JavaScript 96.0","456":"JavaScript 68.8","457":"JavaScript 99.1","458":"Python 100.0","459":"Python 66.8","460":"Python 99.3","461":"Python 100.0","462":"Java 100.0","463":"Java 99.8","464":"Java 100.0","465":"Java 100.0","466":"Java 98.5","467":"C++ 98.0","468":"C++ 97.8","469":"C++ 100.0","470":"C++ 52.0","471":"JavaScript 55.7","472":"JavaScript 99.6","473":"JavaScript 100.0","474":"JavaScript 72.3","475":"JavaScript 71.8","476":"JavaScript 100.0","477":"JavaScript 100.0","478":"JavaScript 65.7","479":"JavaScript 98.4","480":"JavaScript 100.0","481":"Python 93.1","482":"Python 100.0","483":"Python 100.0","484":"Java 100.0","485":"Java 68.7","486":"Java 100.0","487":"Java 41.5","488":"C++ 62.8","489":"C++ 95.7","490":"C++ 52.1","491":"C++ 71.4","492":"C++ 82.0","493":"C++ 80.8","494":"C++ 70.3","495":"C++ 67.9","496":"C++ 77.7","497":"JavaScript 64.7","498":"JavaScript 100.0","499":"JavaScript 100.0","500":"JavaScript 100.0","501":"JavaScript 100.0","502":"JavaScript 100.0","503":"JavaScript 98.0","504":"JavaScript 80.7","505":"JavaScript 100.0","506":"JavaScript 99.4","507":"Python 92.8","508":"Python 100.0","509":"Python 100.0","510":"Python 100.0","511":"Python 100.0","512":"Python 100.0","513":"Python 82.0","514":"Python 99.0","515":"Python 90.7","516":"Python 87.7","517":"Java 100.0","518":"Java 100.0","519":"Java 100.0","520":"Java 100.0","521":"Java 99.2","522":"Java 70.9","523":"Java 81.2","524":"Java 97.6","525":"Java 100.0","526":"Java 100.0","527":"C++ 64.4","528":"C++ 63.5","529":"C++ 80.4","530":"C++ 56.2","531":"JavaScript 88.1","532":"JavaScript 76.1","533":"JavaScript 100.0","534":"JavaScript 96.1","535":"JavaScript 49.3","536":"JavaScript 99.7","537":"Python 100.0","538":"Python 84.4","539":"Java 66.4","540":"Java 100.0","541":"Java 99.5","542":"Java 100.0","543":"C++ 52.2","544":"C++ 85.5","545":"C++ 42.5","546":"C++ 98.0","547":"C++ 60.2","548":"C++ 89.7","549":"C++ 96.8","550":"C++ 69.5","551":"C++ 84.6","552":"C++ 64.2","553":"JavaScript 61.4","554":"JavaScript 92.3","555":"JavaScript 100.0","556":"JavaScript 53.3","557":"JavaScript 99.6","558":"JavaScript 100.0","559":"JavaScript 53.6","560":"JavaScript 77.0","561":"JavaScript 98.4","562":"JavaScript 97.1","563":"Python 100.0","564":"Python 81.9","565":"Python 100.0","566":"Python 98.1","567":"Python 100.0","568":"Python 100.0","569":"Python 100.0","570":"Python 85.9","571":"Python 51.2","572":"Java 100.0","573":"Java 100.0","574":"Java 100.0","575":"Java 55.3","576":"Java 100.0","577":"Java 100.0","578":"Java 100.0","579":"C++ 56.6","580":"C++ 47.7","581":"C++ 94.5","582":"C++ 100.0","583":"C++ 67.3","584":"C++ 91.1","585":"C++ 53.4","586":"C++ 86.2","587":"C++ 58.0","588":"C++ 81.9"},"readme":{"0":"FCC League-For-Good\nThis is a free, open-source web application designed to help sports leagues track their player and team stats, and simplify the other day-to-day tasks involved with team management. It was designed to accommodate many popular sports.\nJoin Us On Slack!\nYou can now join us on slack. Get Invite Here\nGetting Started\nPrerequisites\n\nNodeJS\nMongoDB\n\nIn order for the authorization component of this app to work, it needs to be registered with Google. Here is a helpful walkthrough of that process: https:\/\/developers.google.com\/identity\/sign-in\/web\/devconsole-project.\nYou will want to register Type as Web application, set Authorized JavaScript origins to\nhttp:\/\/localhost:4000 (if you're running the application locally) and set the Authorized\nredirect URI to http:\/\/localhost:4000\/auth\/google\/callback (this can be set through the Google API\nConsole Dashboard under Credentials if not offered as an option during setup).\nYou will also need to enable the \"Google+ API\" on the Google API Console Dashboard - if you forget,\nGoogle will display an error message (with a link to the API) the first time you try to log in.\nSteps\n\nFork and clone the repo\nRun npm install\nIn the root directory, create a .env file and place the following:\n\nMONGO_URI = Your database uri - typically mongodb:\/\/localhost:27017\/your_project_name if your MongoDB is local\nGOOGLE_CLIENT_ID = Client id assigned by Google\nGOOGLE_CLIENT_SECRET = Client secret assigned by Google\nSESSION_SECRET = Any random string of characters\nGOOGLE_CALLBACK_URL = http:\/\/localhost:4000\/auth\/google\/callback (Use for the callback url in the Google API console)\n\n\nOpen a new terminal session and run mongod if mongodb is on the local machine\nRun npm run dev\nNavigate to localhost:4000 in your browser\n\nAvailable Scripts\nIn the project directory, the following commands are available:\nnpm install\nInstalls all the dependencies\nnpm run dev\nBuilds the app for development. It is watched by webpack for any changes in the front end.\nHow to Contribute\nWe warmly welcome contributions from anyone. Check out our how to contribute section to find out how you can do so.\n","1":"nba\nNode.js client for nba.com API endpoints\nnpm install nba\nNOTES:\nBROWSER USAGE\nThis package can't be used from the browser because of CORS restrictions imposed by nba.com. Currently the hostnames are hardcoded so this package can't be used with a proxy host but if you want support for this use case please open an issue!\nBLACKLISTED IP ADDRESSES:\nIt appears as though the NBA has blacklisted certain blocks of IP addresses, specifically those of cloud hosting providers including AWS. As such, you may hit a situation where an application using this package works fine on your local machine, but doesn't work at all when deployed to a cloud server. Annoyingly, requests from these IPs seem to just hang. More information here and here -- the second issue has a curl command somewhere which will quickly tell you if NBA is accepting requests from your IP. (Incidentally, this is also the same reason the TravisCI build is always \"broken\" but tests all pass locally). There is a simple pass-through server in scripts\/proxy that can be used to get around this restriction; you can put the proxy server somewhere that can reach NBA.com (e.g. not on AWS or Heroku or similar) and host your actual application on a cloud provider.\nNBA API\nThe stats.nba.com uses a large number of undocumented JSON endpoints to provide the statistics tables and charts displayed on that website. This library provides a JavaScript client for interacting with many of those API endpoints.\nGetting Started\nNBA.findPlayer(str) will return an object with a player's name, their ID, and their team information. This method is built into the package.\nAll methods in the NBA.stats namespace require an object to be passed in as a parameter. The keys to the object are in the docs for the stats namespace here\nconst NBA = require(\"nba\");\nconst curry = NBA.findPlayer('Stephen Curry');\nconsole.log(curry);\n\/* logs the following:\n{\n  firstName: 'Stephen',\n  lastName: 'Curry',\n  playerId: 201939,\n  teamId: 1610612744,\n  fullName: 'Stephen Curry',\n  downcaseName: 'stephen curry'\n}\n*\/\nNBA.stats.playerInfo({ PlayerID: curry.playerId }).then(console.log);\nFor more example API calls, see \/test\/integration\/stats.js\nStability Warning\nThis is a client for an unstable and undocumented API. While I try to follow semver for changes to the JavaScript API this library exposes, the underlying HTTP API can (and has) changed without warning. In particular, the NBA has repeatedly deprecated endpoints, or added certain required headers without which requests will fail. Further, this library comes bundled with a (relatively) up-to-date list of current NBA players which is subject to change at any time -- the specific contents of it should not be considered part of this library's API contract.\nUsability\nTo put it nicely, the NBA's API endpoints are a little clunky to work with. This library tries to strike a balance between being usable but not making assumptions about how the data will be used. Specifically, the NBA sends data in a concise \"table\" form where the column headers come first then each result is an array of values that need to be matched with the proper header. This library does a simple transformation to zip the header and values arrays into a header-keyed object. Beyond that, it tries to not do too much. This is important to note because sometimes the various \"result sets\" that come back on a single endpoint seem sort of arbitrary. The underlying HTTP API doesn't seem to follow standard REST practices; rather it seems the endpoints are tied directly to the data needed by specific tables and charts displayed on stats.nba.com. This is what I mean by \"clunky\" to work with -- it can be tricky to assemble the data you need for a specific analysis from the various endpoints available.\nDocumentation\nstill lots to do here...\nThere are four primary parts of this library\n\nTop-level methods\nstats namespace \u2014 docs\nsportVu namespace\nsynergy namespace\n\nTransport Layer\nIn some cases you will want to use a different transport layer to handle HTTP requests. Perhaps you have an HTTP client library you like better than what I used here. Better yet, you want to get stats for the WNBA or the G League. The following code snippet shows how to use the withTransport method to create a new client with your own transport function.\n\/\/ here we are getting stats for the WNBA!\n\nconst nba = require(\"nba\");\nconst getJSON = require(\"nba\/src\/get-json\");\n\n\/\/ for the G League, try \"stats.gleague.nba.com\"\nconst newHost = \"stats.wnba.com\";\n\nconst transport = (url, params, options) => {\n  \/\/ simply swap the host and then defer the rest to the built in getJSON function\n  const fixedURL = url.replace(\"stats.nba.com\", \"stats.wnba.com\");\n  return getJSON(fixedURL, params, options);\n};\n\n\/\/ create a new stats client here with our WNBA transport\nconst wnbaStats = nba.stats.withTransport(transport);\n\n(async () => {\n  const result = await wnbaStats.playerInfo({ PlayerID: \"1628886\" });\n  console.log(result);\n})();\n\"I don't use Node.js\"\nPlease take a look at nba-client-template. The relevant part of the repo is a single JSON document from which many programming languages can dynamically generate an API client. The repo contains (sloppy) examples in Ruby and Python. Compiled languages can use code generation techniques to the same effect -- there's a (again, sloppy) example in Go. If you'd like me to publish it to a specific registry so you can install it with your language's package manager, please open an issue. Please note, however, that package only includes  the endpoints exposed by this library under the stats namespace -- sportvu and synergy endpoints aren't yet included in it. I also plan to add a command-line interface to this library so that it can be easily driven as a child process by another program.\n\n","2":"SportsLeague: Laravel 5.4 based system for various sports leagues to manage their teams\/players\/games.\nIt is a demo project for demonstrating what can be generated with QuickAdminPanel tool.\nSportsLeague is all generated with QuickAdmin except for front-end code.\nClickable live-demo\ndemo-sportsleague.quickadminpanel.com\n\nEmail: admin@admin.com\nPass: password\n\n\n\nHow to use\n\nClone the repository with git clone\nCopy .env.example file to .env and edit database credentials there\nRun composer install\nRun php artisan key:generate\nRun php artisan migrate --seed (it has some seeded data for your testing)\nThat's it: launch the main URL or go to \/login and login with default credentials admin@admin.com - password\n\nLicense\nBasically, feel free to use and re-use any way you want.\n\nMore from our LaravelDaily Team\n\nCheck out our adminpanel generator QuickAdminPanel\nRead our Blog with Laravel Tutorials\nFREE E-book: 50 Laravel Quick Tips (and counting)\nSubscribe to our YouTube channel Laravel Business\nEnroll in our Laravel Online Courses\n\n","3":"Team Colors\n\nTeam Colors is a reference of HEX, RGB, CMYK, and Pantone color values of major league sports teams.\nHow-To\nInstall: yarn install\nDevelopment: yarn start\nBuild: yarn build\nHow It Works\nSite is built on the react framework. index.html is the shell container for react app. If javascript is not supported, a link is shown to the raw JSON data which has all color information.\nColor data is housed in a single .json file src\/teams.json. Any changes to team colors can be done there. Note on colors: Color definitions for each team are in arrays and grouped by color mode. Color values should match index position in the array across color modes, for example:\ncolors:\n  rgb: TEAMS-RGB-BLUE, TEAMS-RGB-RED\n  hex: TEAMS-HEX-BLUE, TEAMS-HEX-RED\n\nSource artwork for each team is grouped by league in sketch. Production versions of these logo should be in .svg format in public\/img.\nEdit Team Color or Name\nFind teams .json file in src\/teams.json, and edit the info you need.\nAdd a Team\n\nDetermine the team\u2019s league\nFollowing the established pattern, add the team\u2019s name and colors the .json file\nAdd a vector logo for the team in its corresponding .sketch league file in sketch with the team\u2019s name (as referenced in its .json file) in lowercase with hyphens, i.e. \"utah-jazz\"\nExport the team\u2019s .svg logo to public\/img\/\nPreferably, optimize the svg (with a tool like SVGO)\nRun yarn build, commit, push\n\nOfficial Color References\nNBA\nAll NBA colors are official (source user & pass: nbamedia).\nThe NBA only provides RGB, CMYK, and Pantone colors for each team, so the HEX color is a programmatic conversion of the RGB color.\nNFL\nAll NFL colors are official (see sources below).\nThe NFL provides official RGB, HEX, CMYK, and Pantone colors (so none of the colors you see on Team Colors are conversions).\nThe NFL has logo slicks which detail team color values. These are provided on a per-conference basis. Note: each of these source links are over 100MB in size, so they take a while to download.\n\nAFC North\nAFC South\nAFC East\nAFC West\nNFC North\nNFC South\nNFC East\nNFC West\n\nMLB\nMLB colors have been extracted from the official \u201cRGB Digital Art\u201d spot color logo slicks provided at MLB Press Box (user account required). They were not explicitly stated values, but they are color values pulled from individual team logos in an official MLB document.\nThe extracted colors are in HEX form and their RGB counterparts are generated programmatically.\n\nAmerican League logo slick\nNational League logo slick\n\nNHL\nNHL colors are official. As per Michael Sharer of the NHL.\nMLS\nMLS colors are currently approximations, with the exceptions listed below. I am working on getting official colors of the remaining teams.\n\nPhiladelphia Union\n\nEPL\nThese leagues\u2019 teams and colors are currently approximations. I am working on getting official colors. If you know how\/where to find them, please open an issue here in Github.\nTo-Dos\n\n Switch to flex for layout\n Improve filtering with fuzzy string search\n Improve error states for when data doesn't render\n Consider alternatives to no-js users rather than just \"here's the raw data\" (something that doesn't required a build if a single color in the JSON file is changed)\n Possibly add team id manually to JSON file ??\n\n","4":"vue-sports\n\nA Vue.js project\n\n\u4eff\u51e4\u51f0\u65b0\u95fb\u4f53\u80b2\u677f\u5757+\u8d5b\u4e8b\u6570\u636e\n\u4f53\u80b2\u65b0\u95fb\u677f\u5757\u5df2\u7ecf\u5b8c\u6210(\u6682\u65f6\u672a\u7528\u5230vuex) \uff0c\u6b63\u5728\u5f04\u4f53\u80b2\u8d5b\u4e8b\u6570\u636e\u63a5\u53e3(\u8ba1\u5212\u52a0\u5165vuex)\n\n\n\nBuild Setup\n# install dependencies\nnpm install\n\n# serve with hot reload at localhost:8080\nnpm run dev\n\n# build for production with minification\nnpm run build\n\n# build for production and view the bundle analyzer report\nnpm run build --report\n\u76ee\u6807\u529f\u80fd\n\n \u4f53\u80b2\u65b0\u95fb -- \u5b8c\u6210\n \u6bd4\u8d5b\u6570\u636e -- \u672a\u5b8c\u6210\n\n","5":"\nCourtside: pick up sports app.\nMake game plans and let all your friends know about it.\n\nInstallation\n\n\nTo install requirements:\n$ pip install -r requirements.txt\n\n\nYou will need to create a keys file in keys module for the twitter and fb app keys, ones there are old and not used.\n\n\n\n\nContributions\nAll software contributions are welcome and encouraged.\n\nTwitter\n\nMahdi Yusuf @myusuf3\nOmar Shammas @omarshammas\nSerena Ngai @serenangai\n\n","6":"\u5fae\u4fe1\u5c0f\u7a0b\u5e8f Sports News(\u4f53\u80b2\u65b0\u95fb) \u6301\u7eed\u66f4\u65b0\n\u5c0f\u7a0b\u5e8f\u9884\u89c8\n\n\n\n\u4f7f\u7528\u6b65\u9aa4\n\n\n\u5c06\u4ed3\u5e93\u514b\u9686\u5230\u672c\u5730\uff1a\n$ git clone https:\/\/github.com\/havenxie\/weapp-sportsnews.git weapp-sportsnews --depth 1\n\n$ cd weapp-sportsnews\n\n\n\u6253\u5f00\u5fae\u4fe1Web\u5f00\u53d1\u8005\u5de5\u5177\n\n\n\n\u6211\u7528\u7684\u662f0.11.122100\u7248\u672c\n\u4e0d\u9700\u8981\u6240\u8c13\u7684\u7834\u89e3\uff0c\u7f51\u4e0a\u6240\u8c13\u7684\u7834\u89e3\u53ea\u662f\u9488\u5bf9\u4e4b\u524d\u76840.9.092100\u7248\u672c\uff0c\u65b0\u7684\u5b98\u65b9\u7248\u672c\u4e0d\u9700\u8981\u7834\u89e3\uff01\n\u4e0b\u8f7d\u94fe\u63a5\uff1a\"https:\/\/mp.weixin.qq.com\/debug\/wxadoc\/dev\/devtools\/download.html\"\n\n\n\u9009\u62e9\u6dfb\u52a0\u9879\u76ee\uff0c\u586b\u5199\u6216\u9009\u62e9\u76f8\u5e94\u4fe1\u606f\n\n\nAppID\uff1a\u70b9\u51fb\u53f3\u4e0b\u89d2\u65e0AppID\uff08\u6211\u4e5f\u6ca1\u6709\u8d44\u683c\u62ff\u5230\uff09\n\u9879\u76ee\u540d\u79f0\uff1a\u968f\u4fbf\u586b\u5199\uff0c\u56e0\u4e3a\u4e0d\u6d89\u53ca\u5230\u90e8\u7f72\uff0c\u6240\u4ee5\u65e0\u6240\u8c13\n\u9879\u76ee\u76ee\u5f55\uff1a\u9009\u62e9\u521a\u521a\u514b\u9686\u7684\u6587\u4ef6\u5939\n\u70b9\u51fb\u6dfb\u52a0\u9879\u76ee\n\n\n\n\u4f60\u53ef\u4ee5\u9009\u62e9\u5728\u5fae\u4fe1Web\u5f00\u53d1\u8005\u5de5\u5177\u4e2d\u7f16\u7801\uff08\u4e5f\u53ef\u9009\u62e9\u4f60\u559c\u6b22\u7684\u7f16\u8f91\u5668\uff0c\u6211\u7528\u7684\u662fsublime,\u73b0\u5728\u53d1\u73b0vs code\u6bd4sublime\u597d\u7528\u591a\u4e86\uff09\n\n\n\u901a\u8fc7\u5de6\u4e0b\u89d2\u91cd\u542f\u6309\u94ae\uff0c\u5237\u65b0\u7f16\u7801\u8fc7\u540e\u7684\u9884\u89c8\n\n\n\u4ee3\u7801\u4e2d\u7528\u5230\u4e86\u5927\u91cfES6\u7684\u8bed\u6cd5\uff0c\u53ef\u80fd\u9700\u8981node\u73af\u5883\uff0c\u8bf7\u81ea\u884c\u5b89\u88c5\n\n\n\u5269\u4e0b\u7684\u53ef\u4ee5\u81ea\u7531\u53d1\u6325\u4e86\n\n\n\u8bbe\u7f6e\u9875\u6ca1\u6709\u505a\uff0c\u56e0\u4e3a\u4e0d\u77e5\u9053\u8981\u505a\u4e9b\u4ec0\u4e48\uff0c\u5047\u5982\u4f60\u6709\u5174\u8da3\u7684\u8bdd\u53ef\u4ee5\u81ea\u5df1\u53d1\u6325\n\n\n\u4e0b\u8f7d\u540e\u5c06images\u4e2d\u7684GIF5.jpg\u5220\u9664\u3002\n\n\n\u6709\u4ec0\u4e48\u4e0d\u660e\u767d\u6216\u8005\u60f3\u627e\u6211\u4ea4\u6d41\u7684\u670b\u53cb\u53ef\u4ee5\u548c\u6211\u8054\u7cfb\n\n\n\n\u4e2a\u4eba\u5fae\u4fe1\n\u4e2a\u4eba\u516c\u4f17\u53f7\n\n\n\n\n\n\n\n\n\n\u81f3\u6b64\u544a\u4e00\u6bb5\u843d\uff0c\u5565\u65f6\u5019\u518d\u6709\u5174\u8da3\u518d\u6765\u7ee7\u7eed\u6dfb\u52a0\u529f\u80fd\u5427\u3002\n\u6709\u5174\u8da3\u7684\u5c0f\u4f19\u4f34\u53ef\u4ee5\u4e00\u8d77\u6765\u63d0\u4ea4\u4ee3\u7801\u3002\u3002\u3002\n\u8bb8\u53ef\nMIT \u00a9 havenxie\n","7":"TableChamp\nTablesports leaderboard app\nTrack each ping pong, pool, foosball, air hockey, or shuffleboard game that's played. Find out who really is number one (in your office, or out of your group of friends).\nWhat is it?\nWith TableChamp, you can add players, track every game that is played, and always know who's #1.\n\nYou can view stats on each player, including their 20 most recent games:\n\nYou can manage all of the settings of the app in one convenient sidebar:\n\nYou can even select from one of 14 languages:\n\nHow does it work?\nTableChamp is written entirely in JS\/HTML\/CSS. There is no back-end code (like python, or PHP). It uses FireBase as a back-end real-time DB to store all of the data, and manage the user authentication.\nInstallation\n1) You'll need a hosting account for the JS\/HTML\/CSS files\nNOTE: you can run a FireBase app locally, but you'll need to follow these instructions to get set up with FireBase CLI.\nJust clone this entire project to your server. Once you've done that, move on to step 2.\n2) You'll need to sign up for a free FireBase account\n\nEven if you have a large team, the free FireBase account should offer plenty of resources.\nOnce you've signed up for a free FireBase account, move on to the next step.\n3) Create a new FireBase app\n\nGo through the process of creating a new FireBase Project. You can name it \"Table Champ\", or anything you'd like.\n\nFind the \"Add to your web app\" option, and click it:\n\nYou now have all of the information you need to connect to connect the app to FireBase:\n\nOnce you have your FireBase API info, move on to the next step\n4) Copy your FireBase info to the \/js\/firebase-key.js file\nOpen up \/js\/firebase-key.js:\n\nPaste in the FireBase apiKey, authDomain, and databaseURL from step 3 above:\n\nOnce you've done this, save your changes, and move on to the next step.\n5) Add your first FireBase user\nFireBase handles storing all of your data, as well as authentication. We'll need to set up a user in the FireBase admin, so that you can log into your app. I'll walk you through how to add a single user, but you can add as many login users as you'd like.\nNOTE: Users are separate from players. Users are set up in the FireBase admin, and have an email & password attached to them so that you can log in. Players are managed from the settings section once you've logged into your app.\n\nAll you need to enter to set up a user is an email, and a password.\nOnce you've added your first user, continue to the next step.\n6) Create a database instance\nFrom your FireBase console, click into the Database section:\n\nCreate a new \"Real-time database\" (not a Firestore DB - note: they try and get you to create a Firestore DB by default).\nOnce you've created your real-time DB, you'll need to change the security rules. Click the \"Rules\" tab and and replace what's there with the following:\n{\n  \"rules\": {\n    \".read\": true,\n    \".write\": true\n  }\n}\n\nHere's what it should look like:\n\n7) Login, and add your players\nNow you can log into your app for the first time. Go to the index.html file (wherever it's being hosted from step 1 above). You should see:\n\nOnce you've logged in, you should see:\n\nEnter your organizations name, and the game you'll be tracking:\n\nThen click on the Players tab:\n\nClick \"Add Players\" and enter the names of your players (one name per line):\n\nYou're all set\nYou should be ready to start tracking games:\n\n","8":"Yahoo! Fantasy API Node Module\nThis is a node module created to wrap the Yahoo! Fantasy Sports API (link). At the moment, not all subresources are available, nor are any of the 'collection' elements. I do hope to add them, and they have been added to the code, but as of now this project is very much in an open beta phase.\nThe API is designed to act as a helper for those interacting with the Y! Fantasy API. The goal is for ease of use for the user, both in terms of querying endpoints and parsing responses. I've noticed that in working with the API, the data is not always the easiest to understand, so hopefully what I have created here will help people out.\nInstallation\nYou can install the module via npm by running:\n$ npm install yahoo-fantasy\n\nLicence\nThis module is available under the MIT Licence\nDocumentation\nMore complete documentation can be found using the application sandbox. This sandbox is always a work in progress, if I've learned anything it's that nothing is ever complete.\nThe API can be used by simply importing the module and querying data, since version 4.0 the authentication flow has been built into the library to make things easier for users.\n\/\/ import the library\nconst YahooFantasy = require('yahoo-fantasy');\n\n\/\/ you can get an application key\/secret by creating a new application on Yahoo!\nconst yf = new YahooFantasy(\n  Y!APPLICATION_KEY, \/\/ Yahoo! Application Key\n  Y!APPLICATION_SECRET, \/\/ Yahoo! Application Secret\n  tokenCallbackFunction, \/\/ callback function when user token is refreshed (optional)\n  redirectUri \/\/ redirect endpoint when user authenticates (optional)\n);\n\n\/\/ you can authenticate a user by setting a route to call the auth function\n\/\/ note: from v4.0 on, public queries are now supported; that is, you can query\n\/\/ public resources without authenticating a user (ie\/ game meta, player meta,\n\/\/ and information from public leagues)\nyf.auth(\n  response \/\/ response object to redirect the user to the Yahoo! login screen\n)\n\n\/\/ you also need to set up the callback route (defined as the redirect uri above)\n\/\/ note: this will automatically set the user and refresh token if the request is\n\/\/ successful, but you can also call them manually, described below\nyf.authCallback(\n  request, \/\/ the request will contain the auth code from Yahoo!\n  callback \/\/ callback function that will be called after the token has been retrieved\n)\n\n\/\/ if you're not authenticating via the library you'll need to set the Yahoo!\n\/\/ token for the user\nyf.setUserToken(\n  Y!CLIENT_TOKEN\n);\n\n\/\/ you can do the same for the refresh token...\n\/\/ if you set this and the token expires (lasts an hour) then the token will automatically\n\/\/ refresh and call the above \"tokenCallbackFunction\" that you've specified to persist the\n\/\/ token elsewhere\nyf.setRefreshToken(\n  Y!CLIENT_REFRESH_TOKEN\n);\n\n\/\/ query a resource\/subresource\nyf.{resource}.{subresource} (\n  {possible argument(s)},\n  function cb(err, data) {\n    \/\/ handle error\n    \/\/ callback function\n    \/\/ do your thing\n  }\n);\n\nStarting with v3.1.0 you can also use a promise chain to query resources and subresources\nyf.{resource}.{subresource} (\n  {possible argument(s)}\n)\n.then(data => \/\/ do your thing)\n.catch(err => \/\/ handle error)\n\nThis also opens the door to use async\/await in version of node that support it\ntry {\n  let data = await yf.{resource}.{subresource} (\n    {possible argument(s)}\n  )\n\n  \/\/ do your thing\n} catch(err) {\n  \/\/ handle error\n}\n\nBugs & Issues\nThis project is very much still a work in progress, please report any issues via the GitHub issues page.\nChangelog\n4.1.1\n\nSmall change to the way the resource and collection files are being imported as it was causing issues on some hosts...\n\n4.1.0\n\nMaybe would have made sense as a 5.0.0 as there may be breaking changes, but I haven't been able to find any yet...\nthe authCallback() function will now return an object with the user's access_token and refresh_token\nthe auth() function will accept a \"state\" string, allowing for state persistence through the authentication process\nre-enabled the transactions.fetch() collection call\ncleaned up the \"wavier_days\" and \"stat_categories\" objects on league resources\nadded deprecation warnings to the game.leagues and game.players functions as they're not very useful in that context\n\n4.0.0\n\nAdded auth(), authCallback, setRefreshToken() functions to the library\nAutomatically handle refreshing of the token and call a user defined function when the token has expired\nAdded support for public queries\nGeneral cleanup\n\n3.2.0\n\nAdded \"players\" subresource to \"league\" in order to obtain weekly \/ season stats for a player based on league settings\nFixed a bug where the starting status wasn't properly being returned due to a shift in how the data was being returned\nRemoved use of \"request\" library for size and performance reasons\nGeneral code optimizations and improvements\n\n3.1.2\n\nUpdated outdated dependencies\n\n3.1.1\n\nResolve error when no team logo is present (Issue #42)\n\n3.1.0\n\nIntroduced promise based flow for all endpoints as an alternative to callbacks. Thanks Marois!\n\n3.0.4\n\nFixed a bug in the players.league collection call where it was trying to use split on an array... (Issue #46).\nFixed similar bugs in other places...\n\n3.0.3\n\nAdded the ability to specify a date or week when querying the team.stats resource.\nUnit test fixes (Issue #42). Thanks Marios!\nUpdated \"vulnerable\" dependencies.\n\n3.0.2\n\nFixed an issue with the user.game_leagues resource, where the data was not at all user friendly (renamed leagues to games at the top level of the return object)\n\n3.0.1\n\nFixed some typos in some import statements which caused issues on some servers\n\n3.0.0\n\nMajor refactor to use ES6?... 2015? ...2018? Whatever the hell they're calling it now...\nUsing ES Modules (mjs) files where possible\nRemoved transactions collections (they'll be back!)\n\n2.0.4\n\nAdded a fix to give a cleaner value for the new \"batting order\" attribute in the player oject.\n\n2.0.3\n\nFixed a bug where the league players collection was not properly parsing the ownership subresource\n\n2.0.2\n\nFixed a bug where \"mapTeamPoints\" helper function was not defining \"self\". Thanks platky!\n\n2.0.1\n\nRemoved the code that added a \"reason\" to errors coming from Yahoo! as it was breaking other errors. Retry notifications should now be handled within the application using the module.\n\n2.0.0\n\nMoved to Yahoo!'s OAuth2.0 authentication mechanism.\n\n1.0.2\n\nFixed game resource roster postions callback bug.\n\n1.0.1\n\nFixed a typo that was breaking team mapping.\n\n1.0.0\n\nBreaking changes\nFixed NFL scoreboard\/matchups bug (Issue #19)\nIn fixing this bug I realized that my \"team\" set up was really only useful for MLB fantasy, so I rewrote team mapping to work better across all sports and give additional details that weren't previously reported. This will cause errors if you are using the team.manager attribute in your code.\n\n0.5.3\n\nFixed a bug where leagueFetch was throwing an error, thanks danielspector!\n\n0.5.2\n\nFixed a bug where player stats by week url was not being created properly, thanks withsmilo!\n\n0.5.1\n\nFixed a bug where collections that contained subresources would return no data.\n\n0.5.0\n\nAdded \"Transactions\" collection with functionality to add players, drop players, and add\/drop players, thanks again githubsmilo!\n\n0.4.4\n\nFixed a bug in player.draft_analysis, thanks githubsmilo!\n\n0.4.3\n\nAdded weeks param for league.scoreboard\nAdded weeks param for team.matchups\nFixed a bug where individual players weren't mapping properly\nMinor code cleanup\n\n0.4.2\n\nAdded the ability to specify a date or week when querying the roster resource.\nCleaned up the player normalization model\nFixed a bug where the team.roster call was erroring\n\n0.4.1\n\nFixes to how POST data is handled\n\n0.4.0\n\nSignificantly restructured the code to have more consistency and set it up better for future plans, namely POST methods and proper unit testing\nRemoved the \"refresh user token\" and instead return the error to the user who can handle the refresh within their application.\n\n0.3.1\n\nAdditional player attributes added, thanks ryus08!\n\n0.3.0\n\nAdded a method to refresh the user's token if it has expired.\n\n0.2.2\n\nHotfix to fix \"Teams\" collection - use error first convention\n\n0.2.0\n\nMade helper classes more consistent\nAdded collections for games, leagues, players, and teams\nMoved to error first convention because JavaScript\n\n0.1.2\n\nAdded 'Team Matchups' subresource\nAdded 'League Scoreboard' subresource\nMinor code cleanup and improvements\n\n0.1.1\n\nRefactored module to fix a bug where user sessions were not necessarily unique because of require caching.\n\n0.1\n\nInitial release.\n\n","9":"#Angular Sports Ticker Directive\n##What is this?\nAn Angular directive that approximates a responsive (for screen sizes greater than 767px) sports news ticker (similar to a popular sports news network's \"bottomline\").\n##Why?\nThis was a fun-side project I used to teach myself more about Angular and CSS animations. There are definitely some flaws and it's certainly not as polished as it could be, but I'm putting it out there anyway.\n##How do I use it?\n\nInclude the sportsTicker.js and sportsTicker.css in your document\nAdd the \"sportsTicker\" module as a dependency in your Angular app\nAdd the <sportsticker> tag to your page's markup:\n<sportsticker feed=\"feed\" message-delay=\"4000\" scroll-speed-factor=\"6.25\"><\/sportsticker>\nHave an Angular controller provide a JSON \"feed\" to the directive (see feed.json for examples of all item types)\n\nSee the demo app for a complete example.\n##Caveats:\nThe ticker is hidden on mobile devices (i.e., for devices with a max-width of 767px), and performance on tablets is likely shaky at best.  I just didn't think I could provide a nice mobile experience with the limited screen real estate, and this code isn't optimized for the non-desktop experience (e.g., no hardware-acceleration on animations).  In fact, the code isn't really optimized at all.  As I stated, this was just a side-project, and should not be viewed as battle-tested, production-ready code.\nI don't have any immediate plans to address these limitations, so feel free to clone and do with this what you wish, if anything.  Code is MIT licensed, so go crazy.\n##Acknowledgements\n\nmodernizr\nbackstretch\nprefix-free\nnormalize.css\nvery cool faux NFL logos\n\n","10":"Statistics and Data Analysis\n\n\nMini javascript statistics library for nodejs or the browser.\nNo production dependencies.\nCurrent Library Coverage\n\nStandard Deviation\nMean\nMedian (sorts before calculating)\nMedian Absolute Deviation (MAD)\nOutlier Detection & Filtering using Iglewicz and Hoaglin's method (MAD) - Use this if the order of your data does not matter.\nOutlier Detection & Filtering using Median Differencing (Default method) - Use this if the order of your data matters. This looks at the difference between adjacent points best for time series data.\n\nNode.js \/ Browserify \/ ES6 module\n$ npm install stats-analysis\n\nvar stats = require(\".\/stats-analysis\") \/\/ include statistics library\nBrowser\n<script src=\"https:\/\/unpkg.com\/stats-analysis\"><\/script>\nwindow.stats\nUsage\nvar arr = [-2, 1, 2, 3, 3, 4, 15]\n\n\/\/standard deviation\nstats.stdev(arr).toFixed(2) * 1 \/\/ Round to 2dp and convert to number\n> 4.98\n\n\/\/mean\nstats.mean(arr).toFixed(2) * 1\n> 3.57\n\n\/\/median\nstats.median(arr)\n> 2\n\n\/\/median absolute deviation\nstats.MAD(arr)\n> 1\n\n\/\/ Outlier detection. Returns indexes of outliers\nstats.indexOfOutliers(arr)  \/\/ Default theshold of 3\n> [6]\n\nstats.indexOfOutliers(arr, 6) \/\/ Supply higher threshold to allow more outliers.\n\n\/\/ Outlier filtering. Returns array with outliers removed.\nstats.filterOutliers(arr)\n> [-2, 1, 2, 3, 3, 4]\nTo use different outlier methods:\nstats.filterOutliers(arr, stats.outlierMethod.medianDiff)\nstats.filterOutliers(arr, stats.outlierMethod.medianDiff, 6) \/\/ Different threshold\nstats.filterOutliers(arr, stats.outlierMethod.MAD) \/\/ Default\n\nstats.indexOfOutliers(arr, stats.outlierMethod.medianDiff)\nstats.indexOfOutliers(arr, stats.outlierMethod.medianDiff, 6) \/\/ Different threshold\nstats.indexOfOutliers(arr, stats.outlierMethod.MAD) \/\/ Default\nDevelopment\nMocha is used as the testing framework.\nIstanbul and codecov used for code coverage.\nCommands:\n$ npm install   \/\/ Grab mocha\n$ npm run lint  \/\/ Ensure code consistency with standard\n$ npm test      \/\/ Run tests\n$ npm run cov   \/\/ Run code coverage. (Ensure 100%)\nResources\nEngineering statistics handbook:\nhttp:\/\/www.itl.nist.gov\/div898\/handbook\/index.htm\nContribute to the library\n\nFork it!\nCreate your feature branch: git checkout -b my-new-feature\nMake changes and ensure tests and code coverage all pass.\nCommit your changes: git commit -m 'Add some feature'\nPush to the branch: git push origin my-new-feature\nSubmit a pull request :D\n\nLicense\nMIT\n","11":"Online-Courses-Learning\nComputer Science\nData Science\n\nIBM Data Science Professional Certificate - Coursera\n\nWhat is Data Science? - Coursera - Github\nOpen Source tools for Data Science - Coursera - Github\nData Science Methodology - Coursera - Github\nPython for Data Science - Coursera - Github\nDatabases and SQL for Data Science - Coursera - Github\nData Analysis with Python - Coursera - Github\nData Visualization with Python - Coursera - Github\n\n\n\nMachine Learning\n\nMachine Learning with TensorFlow on Google Cloud Platform - Google Cloud\n\nHow Google does Machine Learning\nLaunching into Machine Learning\nIntro to TensorFlow\nFeature Engineering\nArt and Science of Machine Learning\n\n\n\nProgramming Language\n\n\nPython Programming Language\n\nPython for Everody - University of Michigan - Coursera\n\nProgramming for Everybody (Getting Started with Python) - Coursera - Github\nPython Data Structures - Coursera - Github\nUsing Python to Access Web Data - Coursera - Github\nUsing Databases with Python - Coursera - Github\nCapstone: Retrieving, Processing, and Visualizing Data with Python - Coursera - Github\n\n\n\n\n\nGo Programming Language\n\nProgramming with Google Go - University of California, Irvine - Coursera\n\nGetting Started with Go - Coursera - Github\nFunctions, Methods, and Interfaces in Go - Coursera - Github\nConcurrency in Go - Coursera - Github\n\n\n\n\n\nMATLAB Programming Language\n\nIntroduction to Programming with MATLAB - Vanderbilt University - Coursera - Github\n\n\n\nJavaScript Programming Language\n\nIntroduction to Computer Programming - University of London - Coursera - Github\n\n\n\nOperating System\n\nOpen Source Software Development, Linux and Git - The Linux Foundation - Coursera\n\nOpen Source Software Development Methods - Coursera - Github\nLinux for Developers - Coursera - Github\nLinux Tools for Developers - Coursera - Github\nUsing Git for Distributed Development - Coursera - Github\n\n\n\nMechanical Engineering\n\nMechanics of Materials - Georgia Institute of Technology\n\nMechanics of Materials I: Fundamentals of Stress & Strain and Axial Loading - Coursera - Github\nMechanics of Materials II: Thin-Walled Pressure Vessels and Torsion - Coursera - Github\n\n\n\nMathematics\n\nCalculus: Single Variable - University of Pennsylvania\n\nCalculus: Single Variable Part 1 - Functions - Coursera - Github\nCalculus: Single Variable Part 2 - Differentiation - Coursera - Github\nCalculus: Single Variable Part 3 - Integration - Coursera - Github\nCalculus: Single Variable Part 4 - Applications - Coursera - Github\n\n\n\nRobotics\n\nAn Introduction to Programming the Internet of Things (IOT) - University of California - Coursera\n\nIntroduction to the Internet of Things and Embedded Systems - Coursera - Github\nThe Arduino Platform and C Programming - Coursera - Github\nInterfacing with the Arduino - Coursera - Github\nThe Raspberry Pi Platform and Python Programming for the Raspberry Pi - Coursera - Github\nInterfacing with the Raspberry Pi - Coursera - Github\nProgramming for the Internet of Things Project - Coursera - Github\n\n\n\n","12":"OnDataEngineering.net\nWelcome to the content for the http:\/\/OnDataEngineering.net site\nFor details on how to contribute please see http:\/\/OnDataEngineering.net\/site\/contributing\/\nThis content is licensed under a Creative Commons Attribution 4.0 International License. For more details please please see http:\/\/OnDataEngineering.net\/site\/content-license.\nAll code in this repository (primarily to be found in the _includes and _layouts directory) is licensed under the Apache 2.0 licence.\nServing this content locally\nThis is entirely optional, but if you'd like to view the content in a browser and check pages have been correctly created and metadata correctly set then you can do this under either Windows, Linux or Mac as follows:\n\n\nInstall the latest stable version of Ruby v2 from http:\/\/www.ruby-lang.org\/en\/downloads\/.  Under Windows, you'll also need to install the appropriate development kit - see http:\/\/jekyll-windows.juthilo.com\/1-ruby-and-devkit\/.\n\n\nInstall the latest stable version of RubyGems from http:\/\/rubygems.org\/pages\/download\n\n\nInstall the required ruby gems (including Jekyll) by running \"gem install bundler\" and then \"bundle install\"\n\n\nStart the jekyll server via \"bundle exec jekyll serve\"\n\n\nGo to http:\/\/localhost:4000 to view the site\n\n\nNote that this is not the full OnDataEngineering site, but a cut down simplified version for the purposes of creating and checking content.\nThere's some basic checking of metadata included, which will show up as error messages at the top of the relevent page.  These can also be searched for with \"grep -r ERROR _site\"\n","13":"\n ****\n\n\n\n\n\n\n\nDexcalibur\nDexcalibur is an Android reverse engineering platform focus on instrumentation automation. Its particularity is to use dynamic analysis to improve static analysis heuristics. It aims automate boring tasks related to dynamic instrumentation, such as :\n\nDecompile\/disass intercepted bytecode at runtime\nWrite hook code and Manage lot of hook message\nSearch interesting pattern \/ things to hook\nProcess data gathered by hook (dex file, invoked method, class loader, ...)\nand so ...\nBut not only that, because Dexcalibur has own static analysis engine and it is able to execute partial piece of smali.\n\nOfficial documentation is available here (website - work in progress).\nSee the latest news here : http:\/\/docs.dexcalibur.org\/News.html\nShow Dexcalibur demo videos : Demo: Less than 1 minute to hook 61 methods ? Not a problem. (youtube)\nHow to support Dexcalibur ?\nContribute !\nDon't hesitate ! There are several ways to contribute :\n\nMake a pull request related to a fix or a new feature\nCreate an issue to help me to patch\/involves tools\nHelp me to develop UI\nSend me a mail with your feedback\netc ...\n\nA. Installation\nA.1 New install\nEnsure following dependencies are installed :\n\nNodeJS 12.x LTS (upper non-LTS versions can cause issues during installation if there is not prebuilt binaries for frida-node, see Issue #27. Else, you can rebuild frida-node)\nJava >= 8\n\nRun command:\n$  npm install -g dexcalibur\n\nAnd start Dexcalibur with:\n$  dexcalibur\n\nVisit http:\/\/127.0.0.1:8000 and follow instruction.\nYour default port number 8000 is already in use ? Specify a custom port by using --port=<number> like $  dexcalibur --port=9999\ufffd\nFill the form with the location of your workspace and default listening port, and submit it.\nThe workspace will contain a folder for each application you reverse using Dexcalibur.\n\nDexcalibur will create the workspace if the folder not exists. A standalone version of android platform tools, APKtool, and more will be downloaded into this workspace.\n\nOnce install is done, restart Dexcalibur by killing it and doing (again)\n$ dexcalibur\n\nFor more information, please visit intallation guide\nOr use Docker (DEPRECATED) (See docker guide):\nA.2 Update\nFrom version <= 0.6.x\nYou are using a previous version of Dexcalibur ?\nFollow same steps than a new install, and when you should enter workspace path, enter your current workspace location.\nFrom version >= 0.7\nJust by doing:\n$  npm install -g dexcalibur\n\nExisting configuration and workspace will be detected automatically.\nC. Screenshots\nFollowing screenshots illustrate the automatic update of xrefs at runtime.\n\n\nD. Features and limitations\nActually, the biggest limitation is Dexcalibur is not able to generate source code of hook targeting native function (into JNI library). However, you can declare manually a Frida's Interceptor by editing a hook.\nAssuming Dexcalibur does not provide (for the moment) features to analyse native part such as JNI library or JNA, only features and limitations related to Java part have been detailled.\nAnalysis accuracy depends of the completeness of the Android API image used during early steps of the analysis. That means, if you use a DEX file generated from the Android.jar file from Android SDK, some references to internal methods, fields, or classes from Android java API could be missing. Better results are obtained when the analysis start from a \"boot.oat\" file extracted directly from a real device running the expected Android version.\nD.1 Features\nD.1.A Static analyzer\nTODO : write text\nD.1.B Hook manager\nTODO : write text\nD.1.C Dexcalibur's smali VM\nTracked behaviors\nStatic analyzer involved into \"Run smali (VM)\" action is able to discover and accept but track following behaviors :\n\nOut-of-bound destination register (register out of v0 - v255)\nOut-of-bound source register (register out of v0 - v65535)\nDetect invalid instruction throwing implicitely an internal exception\nDetect some piece of valid bytecode non-compliant with Android specification\nCompute length of undefined array\nFill undefined array\nand more ...\n\nActually, handlers\/listeners for such invalid instruction are not supported but events are tracked and rendered.\nDexcalibur IR\nThe VM produces a custom and simplified Intermediate Representation (IR) which is displayed only to help analyst to perform its analysis.\nDepending of the value of the callstack depth and configuration, IR can include or not instruction executed into called function. If the execution enters into a try block and continues to return, but never excute catch, then the catch block will not be rendered. In fact the purpose of Dexcalibur IR is to render only \"what is executed\" or \"what  could be executed depending of some symbol's value\" into VM context.\nDexcalibur IR helps to read a cleaned version of bytcode by removing useless goto and opaque predicate. Dexcalibur IR can be generated by the VM with 2 simplifying levels :\n1st level IR, could be used if you don't trust 2th level IR  :\n\nno CFG simplifying : conditions and incondtionnal jumps are rendered.\nevery move into a register are rendered\n\n2th level :\n\nHide assign if the register is not modified with an unknown value before its use.\nAlways TRUE\/FALSE predicate are removed\nInconditional jump such goto are removed under certain conditions : single predecessor of targeted basic block, etc ...\nResolve & replace Method.inoke() call by called method if possible.\nInstructions into a Try block are not rendered if an exception is thrown before\n...\n\nAndroid API mock\nTODO\nDetails\nSmali VM follows steps :\n\nInit VM : stack memory, heap, classloaders, method area, ...\nThe VM load class declaring the method.\n(Optionnal) If the class has static blocks, clinit() is executed.  It helps to solve concrete value stored into static properties\nLoad method metadata\nExecute method's instructions, if PseudoCodeMaker is enable, Dexcalibur IR is generated.\n\nHow VM handles invoke-* instruction ?\n\nWhen an invoke-* happens, the local symbol table is saved, and the invoked method is loaded.\nIf the class declaring the invoked method  has never been loaded, the class is loaded\nIf the method has never been loaded, the method is loaded (by MethodArea) and its local symbol table initialized by importing symbols of arguments from caller's symbol table.\nInvoked method is push into callstack (StackMemory).\nMethod instruction are executed.\nReturn is push into stack memory\nCaller give flow control\n\nD.1.D Application Topology  analyzers\nManifest analysis (LIMITED)\nBefore the first run, the Android manifest of the application is parsed. Actually, anomalies into the manifest\nsuch insecure configuration are really detected at this level.\nThe only purpose of Android manifest parsing is to populate other kind of analyzers.\nPermission analysis\nEvery permissions extracted from the Manifest are listed and identified and compared to Android specification of the target Android API version.\nDexcalibur provides - only in some case - a description of the permission purpose, the minimal Android API version, ...\nActivities analysis\nProviders analysis\nServices analysis\nReceivers analysis\nD.1.E Runtime monitoring (not implemented)\nNetwork monitoring\nIntent monitoring\nFile access monitoring\nD.1.F Collaborating features\nYou cannot find multi-user menu ? Not a probleme, there is not a menu but minimalistic collaborative work can be achieve.\nDexcalibur runs a web server.  So, if several people are on the same network of this web server and if host firewall is well configured, you can be several to work on the same Dexcalibur instance.\nActual limitations are :\n\nNo authentication : everybody into the network can send request to Dexcalibur instance and doing RCE the host through search engine.\nNo identifying : modifying are not tracked, so, if someone rename a symbol, you could not know who renamed it. Similar case : you are not able to know who created a specific hook.\nSingle device instrumentation : if several devices are connected to Dexcalibur's host, and even if you can choose the device to instrument, instrumentation and hook messages are linked to the last device selected. So, you cannot generate instrumention for several devices simultaneously.\n\nE. Github Contributors\nA special thanks to contributors :\n\nubamrein\njhscheer\neybisi\nmonperrus\n\nF. Troubleshoots\nF.1 Dexcalibur continues to start into \"install mode\"\nBefore to go deeper :\n\nEnsure you are connected to Internet : Apktool and target platform are downloaded during install\nDid you have tried to reinstall it by doing dexcalibur --reinstall command ? If no, try it.\n\nFirst, check if global settings have been saved into <user_directory>\/.dexcalibur\/\n$ ls -la ~\/.dexcalibur      \n\ntotal 8\ndrwxr-xr-x   3 test_user  staff    96 29 avr 11:41 .\ndrwxr-xr-x+ 87 test_user  staff  2784 29 avr 11:47 ..\n-rw-r--r--   1 test_user  staff   204 29 avr 11:41 config.json\n\n\n$ cat ~\/.dexcalibur\/config.json \n\n{\n    \"workspace\":\"\/Users\/test_user\/dexcaliburWS3\",\n    \"registry\":\"https:\/\/github.com\/FrenchYeti\/dexcalibur-registry\/raw\/master\/\",\n    \"registryAPI\":\"https:\/\/api.github.com\/repos\/FrenchYeti\/dexcalibur-registry\/contents\/\"\n}\n\nNext, check if structure of Dexcalibur workspace is as following (content of \/api folder may differs).\n$ ls -la ~\/dexcaliburWS\/.dxc\/*\n\/Users\/test_user\/dexcaliburWS\/.dxc\/api:\ntotal 0\ndrwxr-xr-x  3 test_user  staff   96 29 avr 11:41 .\ndrwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..\ndrwxr-xr-x  8 test_user  staff  256 29 avr 11:41 sdk_androidapi_29_google\n\n\/Users\/test_user\/dexcaliburWS\/.dxc\/bin:\ntotal 34824\ndrwxr-xr-x   4 test_user  staff       128 29 avr 11:41 .\ndrwxr-xr-x   7 test_user  staff       224 29 avr 11:41 ..\n-rwxr-xr-x   1 test_user  staff  17661172 29 avr 11:41 apktool.jar\ndrwxr-xr-x  18 test_user  staff       576 29 avr 11:41 platform-tools\n\n\/Users\/test_user\/dexcaliburWS\/.dxc\/cfg:\ntotal 8\ndrwxr-xr-x  3 test_user  staff   96 29 avr 11:41 .\ndrwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..\n-rw-r--r--  1 test_user  staff  314 29 avr 11:41 config.json\n\n\/Users\/test_user\/dexcaliburWS\/.dxc\/dev:\ntotal 0\ndrwxr-xr-x  2 test_user  staff   64 29 avr 11:41 .\ndrwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..\n\n\/Users\/test_user\/dexcaliburWS\/.dxc\/tmp:\ntotal 0\ndrwxr-xr-x  2 test_user  staff   64 29 avr 11:41 .\ndrwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..\n\nG. FAQ\nMy device not appears when into device list\nIf you use a physical device connected over USB, ensure developper mode and USB debugging are enabled.\nIf you use a virtual device, go to \/splash.html,  select Device Manager,  click Connect over TCP ... and follow instructions. If you don't know IP address of your device, let Dexcalibur detect it by checking box automatic configuration.\nUSB debugging is enabled, but my device not appears when into device list\n\nConnect\/disconnect USB and ensure your computer is allowed.\nSelect file transfert\n\nWhy enroll a new device ?\nYou need to enroll the target device before to be able to use it.\nDuring enrollment Dexcalibur gather device metadata and push a compatible version of Frida server.\nSuch metadata are used to select right frida-server and frida-gadget targets.\nMy device is listed into Device Manager, but it cannot be enrolled\nIf a red exclamation mark ! appears on a line into device list, then your desktop is not allowed by device. You probably need to confirm\nIf your device is listed into DeviceManager and the column online is checked, then click enroll\nG.1 My device is listed into Device Manager\nIf your device is listed into DeviceManager and the column online is checked, then click enroll\nHow to use an emulator instead of a physical device ?\nDexcalibur version < v0.7 was not able to detect automatically emulated device and use it due to an incomplete ADB output parsing.\nSince version >= v0.7, once your virtual device is running, go to \/splash.html or click on DEXCALIBUR into navigation bar.\nClick on Device Manager button into left menu, and click the Refresh button at top of array.\nYou should have a row starting by the ADB ID of your virtual device.\nHow to use a device over TCP ?\nFirst, as any target device, you should enroll it.\nClick Connect over TCP ... to add a new device over TCP or to connect an enrolled device over TCP.\nIf the device has never been enrolled, so enrollment will be perform through TCP.\nIn some case, connection over TCP is slower than over USB. So enrollement can take additional time.\nIf the device has been enrolled over USB, so the new prefered transport type for this device becomes TCP.\nHow to contribute to the dexcalibur ?\nCreate a pull request on this repository or create an issue.\nHow to contribute to the documentation?\nCreate a pull request on dexcalibur-doc repository.\nDocumentation is available at here (doc website) and here (wiki)\nH. Sponsors\n\n\n\n\n\n\n\n\nThey offered a license for All Products <3\n\n\n\nI. Resources\nThere is actually few documentation and training resources about Dexcalibur. If you successfully used Dexcalibur to win CTF challenge or to find vulnerability, i highly encourage you to share your experience.\n\nSlides of Pass the SALT 2019 (PDF)\nYoutube : demonstration\nCLI User Guide\nUser Guide\nTroubleshoots\nScreenshots\n\nJ. They wrote something about Dexcalibur\n\nAwesome Frida\nAwesome OpenSource Security\nn0secure.org - PassTheSalt2019 J2\nrootshell.be - PassTheSalt2019 Wrap Up\nPentesterLand - the 5 hacking newsletter 61\nTechnology Knowledge Database\nXuanwu Lab Security\nMobile Gitbook\n274 - AppsSec Ezine\nysh329 \/ Android Reverse Engineering\n\n","14":"Blockchain Now\nAgenda: End-to-end pipeline for the bitcoin analytics. Reach UI with reactive search and charts.\n\nCourse project for Insight Data Engineering program\nHow to install\nTL;DR\nGit clone the repository\ngit clone git@github.com:igorbarinov\/blockchainnow.git\n\nFrom meteorui\/ start meteor server\nmeteor\n\nOpen http:\/\/localhost:3000 in you favorite Google Chrome browser\nWhen you will ready to publish to your hosting:\nmeteor publish $(echo \"example\")\n\nchange example to any desired (and free) hostname in *.meteor.com domain\ne.g. http:\/\/blockchainnow.meteor.com\nTechnology Stack\n\nBitcoin Core Bitcoin Core\nInsight API A bitcoin blockchain API for web wallets\nApache Kafka A high-throughput distributed messaging system\nkafka-node Node.js client with Zookeeper integration for Apache Kafka\nLogstash Collect, Parse, Transform Logs\nElastic Search Search & Analyze Data in Real Time\nMeteor The JavaScript App Platform\nMeteor Easy Search Plugin for Meteor\n\nLinks\n\nBlockchain Now \"Blockchain Now\" website\nSlides\nKafka Manager\nElastic Sample Query\n\nImportant:\nPlease star Awesome Data Engineering repository\n","15":"onexi.github.io\nEngineering Computation & Data Science -\n","16":"Data-Engineering-Google-Drive-Data-Pipeline\nGoogle Drive Data Pipeline Automatically extracting, transforming and loading data from your Google Drive into your preferred data warehouse on a regular interval (up to a minute).\nThis repo contains the main operators and the DAG to execute the Pipeline.\n","17":"BuggyGuiAngularPort\nTelemetry system for the UPRM Moonbuggy Engineering team!\nHere we collect a lot of rover data in a Database and visualize it to show the judges\nThis project was generated with Angular CLI version 6.0.1.\nDevelopment server\nRun ng serve for a dev server. Navigate to http:\/\/localhost:4200\/. The app will automatically reload if you change any of the source files.\nTo test the back-end portion of the server run ng build first. Then run npm start. (You should have Nodemon installed on your system npm install -g nodemon). Navigate to http:\/\/localhost:3000\/ and you can test the requests using this address. App should reload if you change any server files.\nIf there are changes in the angular files, you run ng build again to reload the server.\nCode scaffolding\nRun ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module.\nBuild\nRun ng build to build the project. The build artifacts will be stored in the dist\/ directory. Use the --prod flag for a production build.\nRunning unit tests\nRun ng test to execute the unit tests via Karma.\nRunning end-to-end tests\nRun ng e2e to execute the end-to-end tests via Protractor.\nFurther help\nTo get more help on the Angular CLI use ng help or go check out the Angular CLI README.\n","18":"Pupil\nPupil is a tool for visualizing data from various software-engineering\ntools. It works as a command-line tool run from a git-repository. When\nrun, pupil will gather data from the repository and the GitLab\nREST-API and persist it to a configured ArangoDB instance.\nPupil then hosts interactive visualizations about the gathered data\nvia a web-interface.\nNaming\n\"Pupil\" is the name used in this repository and also the name used in\nthe source code to refer to itself. In the accompanying master's\nthesis, \"pupil\" is referred to as \"zivsed\". On the INSO projects page,\nit is described as \"binocular\". Don't get confused, its all the same\nthing - naming is hard \u00af\\_(\u30c4)_\/\u00af.\nDependencies\n\nnode.js >= 8\nArangoDB (tested with 3.1.28)\n\nInstallation\nPupil is not yet published on the npm registry. To install it, you\nshould clone this repository and then link it:\n$ git clone git@gitlab.com:romand\/pupil.git\n$ cd pupil\npupil$ npm link    # <- this will make the `pupil` executable available in your $PATH\nConfiguration\nAs pupil needs to access an ArangoDB instance, you have to configure\nthe database connection before you can use pupil. This can be done in\nthe global pupil-configuration file ~\/.pupilrc. Additionally, the\nconfiguration file also stores authentication data for the consumed\nREST-APIs. The configuration file is read by the rc\nmodule. Check its documentation to\nsee the supported formats. For the purpose of this README, we'll use\njson.\nConfiguration options\n\ngitlab: Object holding gitlab configuration options\n\nurl: The URL to the GitLab-Instance you want to connect to. Use the\nbase-url, not the API-URL (see the example)!\ntoken: The personal access token generated by your GitLab user to\nuse for authentication (see\nhttps:\/\/docs.gitlab.com\/ee\/user\/profile\/personal_access_tokens.html)\n\n\ngithub: Object holding github configuration options\n\nauth: Can hold any options that the [github npm-module] can take, check its documentation.\n\n\narango: Object holding arangodb-configuration\nhost: Hostname\nport: Port\nuser: username\npassword: password\n\nA sample configuration file looks like this:\n{\n  \"gitlab\": {\n    \"url\": \"https:\/\/gitlab.com\/\",\n    \"token\": \"YOUR_GITLAB_API_TOKEN\"\n  },\n  \"github\": {\n    \"auth\": {\n      \"type\": \"basic\",\n      \"username\": \"YOUR_GITLAB_USER\",\n      \"password\": \"YOUR_GITLAB_PASSWORD\"\n    }\n  },\n  \"arango\": {\n    \"host\": \"localhost\",\n    \"port\": 8529,\n    \"user\": \"YOUR_ARANGODB_USER\",\n    \"password\": \"YOUR_ARANGODB_PASSWORD\"\n  }\n}\nYou may override configuration options for specific projects by\nplacing another .pupilrc file in the project's root directory.\nUsage\nTo run pupil, simply execute pupil from the repository you want to\nrun pupil on (you can try it on the pupil-repo itself!). Pupil will\ntry to guess reasonable defaults for configuration based on your\n.git\/config. A browser window should pop up automatically with\npupil's web-interface showing the indexing progress and the\nvisualizations.\nContributing\nFor an explanation of pupil's architecture, please see the Contribution\nguidelines for this project\n","19":"HIT-DataManage\n\u63d0\u4ea41.0\n\u63d0\u4ea4\u540d\u4e3a\u201cfirst commit\u201d\n\u5185\u5bb9\n\n\u6dfb\u52a0\u4e86\u767b\u5f55\u3001\u6ce8\u518c\u65b9\u9762\u7684\u540e\u53f0\u5904\u7406\n\n\u63d0\u4ea41.1\n\u63d0\u4ea4\u540d\u4e3a\u201clogin complete rough\u201d\n\u5185\u5bb9\n\n\u52a0\u5165\u4e86\u767b\u5f55\u8eab\u4efd\u9a8c\u8bc1\u7684\u8fc7\u6ee4\u5668\uff0c\u80fd\u591f\u8fc7\u6ee4action\u548c\u9875\u9762\uff0c\u672a\u767b\u5f55\u7684\u7528\u6237\u65e0\u6cd5\u8bbf\u95ee\u76ee\u5f55\u4e0b\u7684\u8d44\u6e90\u4e14\u4f1a\u91cd\u5b9a\u5411\u5230\u767b\u5f55\u754c\u9762;\n\u4e0e\u524d\u53f0\u7684\u767b\u5f55\u548c\u6ce8\u518c\u9875\u9762\u5408\u5e76\n\n\u63d0\u4ea41.2\n\u63d0\u4ea4\u540d\u4e3a\u201cfilter improved\u201d\n\u5185\u5bb9\n\n\u5b8c\u5584\u4e86\u8fc7\u6ee4\u5668\uff0c\u5f53\u7528\u6237\u8f93\u5165\u4e0d\u5b58\u5728\u7684action\u65f6\uff0c\u8df3\u8f6c\u5230\u9ed8\u8ba4action\n\n\u63d0\u4ea41.3\n\u63d0\u4ea4\u540d\u4e3a\u201chomepage\u201d\n\u5185\u5bb9\n\n\u5b9e\u73b0\u4e86\u767b\u5f55\u540e\u663e\u793a\u7684\u4e3b\u9875\n\n\u63d0\u4ea41.4\n\u63d0\u4ea4\u540d\u4e3a\u201caddPDO&Event\u201d\n\u5185\u5bb9\n\n\u5b9e\u73b0\u4e86\u6dfb\u52a0PDO\u7684\u529f\u80fd\uff0c\u6dfb\u52a0\u4e8b\u4ef6(Event)\u8fd8\u5728\u7ee7\u7eed\u5b8c\u5584\n\u7b2c\u4e8c\u5929\u5e94\u8be5\u80fd\u5b9e\u73b0\u6dfb\u52a0\u4e8b\u4ef6\u529f\u80fd\n\n\u63d0\u4ea41.5\n\u63d0\u4ea4\u540d\u4e3a\u201ccompleteAdd\u201d\n\u5185\u5bb9\n\n\u5b9e\u73b0\u4e86\u6dfb\u52a0\u4e8b\u4ef6\u529f\u80fd\n\n\u63d0\u4ea41.6\n\u63d0\u4ea4\u540d\u4e3a\u201ccompleteDelete\u201d\n\u5185\u5bb9\n\n\u5b9e\u73b0\u4e86\u5220\u9664PDA\u548cEvent\u64cd\u4f5c\n\n\u63d0\u4ea41.7\n\u63d0\u4ea4\u540d\u4e3a\u201cexcelimport\u201d\n\u5185\u5bb9\n\n\u5b9e\u73b0\u4e86excel\u6587\u4ef6\u7684\u5bfc\u5165\u64cd\u4f5c\n\n\u63d0\u4ea41.8\n\u63d0\u4ea4\u540d\u4e3a\u201cfirst_merge_master\u201d\n\u5185\u5bb9\n\n\u7b2c\u4e00\u6b21\u5408\u5e76\u603b\u5206\u652f\n\n\u63d0\u4ea41.9\n\u63d0\u4ea4\u540d\u4e3a\u201cfix some problems\u201d\n\u5185\u5bb9\n\n\u5b9e\u73b0excel\u5bfc\u5165\uff0c\u4e8b\u4ef6\u5220\u9664\uff0cpdo\u5220\u9664\n\n\u63d0\u4ea42.0\n\u63d0\u4ea4\u540d\u4e3a\u201ccomplete logout\u201d\n\u5185\u5bb9\n\n\u5b9e\u73b0\u767b\u51fa\u529f\u80fd\n\n\u63d0\u4ea42.1\n\u63d0\u4ea4\u540d\u4e3a\u201ccomplete some functions\u201d\n\u5185\u5bb9\n\n\u548c\u524d\u7aef\u5408\u5e76\n\n\u63d0\u4ea42.2\n\u63d0\u4ea4\u540d\u4e3a\u201ccharset problems\u201d\n\u5185\u5bb9\n\n\u4e3b\u8981\u662f\u5c1d\u8bd5\u89e3\u51b3excel\u5bfc\u5165\u65f6\u5019\u4e71\u7801\u4ee5\u53ca\u6570\u636e\u5e93\u4e71\u7801\u95ee\u9898\n\n\u63d0\u4ea42.3\n\u63d0\u4ea4\u540d\u4e3a\u201cfix data format problem on both xls and xlsx\u201d\n\u5185\u5bb9\n\n\u7ec8\u4e8e\u89e3\u51b3\u4e86xls\u548cxlsx\u6587\u4ef6\u4e2d\u6240\u6709\u65e5\u671f\u683c\u5f0f\u5904\u7406\u7684\u95ee\u9898\n\n\u63d0\u4ea42.4\n\u63d0\u4ea4\u540d\u4e3a\u201ccomplete events counts\u201d\n\u5185\u5bb9\n\n\u5b9e\u73b0\u5bf9event\u4e2a\u6570\u7684\u8ba1\u6570\u95ee\u9898\n\n\u63d0\u4ea42.5\n\u63d0\u4ea4\u540d\u4e3a\u201cmerge with some pages\u201d\n\u5185\u5bb9\n\n\u548c\u524d\u7aefmerge\uff0c\u5b9e\u73b0\u641c\u7d22\u7ef4\u5ea6\u7684PDO\u6dfb\u52a0\n\n\u63d0\u4ea42.6\n\u63d0\u4ea4\u540d\u4e3a\u201cjust left search\u201d\n\u5185\u5bb9\n\n\u57fa\u672c\u529f\u80fd\u9664\u641c\u7d22\u90fd\u5b9e\u73b0\u4e86\n\n\u63d0\u4ea42.7\n\u63d0\u4ea4\u540d\u4e3a\u201ccomplete all functions\u201d\n\u5185\u5bb9\n\n\u540e\u53f0\u5b8c\u6210\u6240\u6709\u57fa\u672c\u529f\u80fd\n\n\u63d0\u4ea42.8\n\u63d0\u4ea4\u540d\u4e3a\u201cimprove the filter\u201d\n\u5185\u5bb9\n\n\u5b8c\u5584\u4e86\u8fc7\u6ee4\u5668\n\n\u63d0\u4ea42.9\n\u63d0\u4ea4\u540d\u4e3a\u201cready to middle-discuss\u201d\n\u5185\u5bb9\n\n\u548c\u524d\u7aef\u65e0\u7f1d\u8fde\u63a5\uff0c\u5b8c\u6210\u6240\u6709\u57fa\u672c\u529f\u80fd\uff0c\u51c6\u5907\u4e2d\u671f\u7b54\u8fa9\n\n\u63d0\u4ea43.0\n\u63d0\u4ea4\u540d\u4e3a\u201cfix a bg bug\u201d\n\u5185\u5bb9\n\n\u67e5\u8be2\u65f6\u5173\u8054\u6570\u636e\u51fa\u9519\uff0c\u540e\u53f0\u53ca\u65f6\u4fee\u6539\u4e86\n\n\u63d0\u4ea43.1\n\u63d0\u4ea4\u540d\u4e3a\u201cadd a sum-up model\u201d\n\u5185\u5bb9\n\n\u5728\u4e3b\u9875\u6dfb\u52a0\u4e86\u4e00\u4e9b\u7edf\u8ba1\u4fe1\u606f\n\n\u63d0\u4ea43.2\n\u63d0\u4ea4\u540d\u4e3a\u201cfix some bugs\u201d\n\u5185\u5bb9\n\n\u89e3\u51b3\u4e86\u4e00\u7cfb\u5217bug:\n\n\u6ce8\u518c\u9875\u9762\uff1a\u5bc6\u7801\u591a\u6b21\u5339\u914d\uff0c\u8f93\u5165\u4e0d\u80fd\u4e3a\u7a7a\n\u6dfb\u52a0\u4e8b\u4ef6\u9875\u9762\uff1a\u82e5\u6709\u65f6\u95f4\u5c5e\u6027\uff0c\u6539\u4e3a\u65e5\u5386\n\n\n\n","20":"\n\n\n\n\n\n\nAn exciting game of programming and Artificial Intelligence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn WarriorJS, you are a warrior climbing a tall tower to reach The JavaScript\nSword at the top level. Legend has it that the sword bearer becomes enlightened\nin the JavaScript language, but be warned: the journey will not be easy. On each\nfloor, you need to write JavaScript to instruct the warrior to battle enemies,\nrescue captives, and reach the stairs alive...\nNo matter if you are new to programming or a JavaScript guru, WarriorJS will\nput your skills to the test. Will you dare?\nPlay\nGo to warriorjs.com and play from the comfort\nof your browser! Sharpen your skills and compete against other players around\nthe globe. Good luck in your journey, warrior!\nDocumentation\nAlthough there is some in-game documentation, at some point you may want to\nvisit the official docs.\nJump straight to some of the most-visited pages:\n\nGameplay\nTowers\nPlayer API\n\nCLI\nWanna play offline? No problem, just follow these steps:\n\nInstall WarriorJS CLI with npm:\n\nnpm install --global @warriorjs\/cli\n\nLaunch the game:\n\nwarriorjs\n\n\nCreate your warrior.\n\n\nYou'll be pointed to a README file with instructions for the first level.\n\n\nCheck out the Install docs for\nmore details.\nPreview\n\n\nWarriorJS CLI launched from the\nIntegrated Terminal\nin VS Code. To the left, Player.js, and to\nthe right, a\nMarkdown Preview\nof README.md.\n\nContributing\nWe welcome contributions to WarriorJS! These are the many ways you can help:\n\nSubmit patches and features\nMake towers (new levels for the\ngame)\nImprove the documentation and website\nReport bugs\nFollow us on Twitter\nParticipate in the Spectrum community\nAnd donate financially!\n\nPlease read our contribution guide to get started. Also note\nthat this project is released with a\nContributor Code of Conduct, please make sure to review\nand follow it.\nContributors\nThanks goes to each one of our contributors! \ud83d\ude4f\nBecome a contributor.\n\nBackers\nSupport us with a monthly donation and help us continue our activities!\nBecome a backer.\n\nSponsors\nBecome a sponsor and get your logo here and on the\nofficial docs!\nBecome a sponsor.\n\nAcknowledgments\nThis project was born as a port of\nruby-warrior. Credits for the original\nidea go to Ryan Bates.\nSpecial thanks to Guillermo Cura for designing a\nwonderful logo.\nLicense\nWarriorJS is licensed under a MIT License.\n","21":"\n\n\n\ni.am.aiAI Expert Roadmap\nRoadmap to becoming an Artificial Intelligence Expert in 2020\n\n\n\n\n\n\n\n\n\nBelow you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an ai expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.\nIf you are interested to become an AI EXPERT at AMAI in Germany, or you want to hire an AI Expert, please say hi@am.ai.\nNote\n\ud83d\udc49 An interactive version with links to follow about each bullet of the list can be found at i.am.ai\/roadmap \ud83d\udc48\nTo receive updates star \u2b50 and watch \ud83d\udc40 the GitHub Repo to get notified, when we add new content to stay on the top of the most recent research.\nDisclaimer\nThe purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better suited for some cases than the other and remember hip and trendy never means best suited for the job.\nIntroduction\n\n\n\n\n\nData Science Roadmap\n\n\n\n\n\nMachine Learning Roadmap\n\n\n\n\n\nDeep Learning Roadmap\n\n\n\n\n\nData Engineer Roadmap\n\n\n\n\n\nBig Data Engineer Roadmap\n\n\n\n\n\n\ud83d\udea6 Wrap Up\nIf you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, we will continue to improve this, so you might want to watch\/star this repository to revisit.\n\ud83d\ude4c Contribution\n\nHave a look at the contribution docs for how to update any of the roadmaps\n\n\nOpen pull request with improvements\nDiscuss ideas in issues\nSpread the word\nReach out with any feedback\n\nSupported By\n\n\n","22":"TooAngel Artificial intelligence for screeps\n\n\n\n\n\n\nhttps:\/\/screeps.com\/\nSee rendered version:\nhttp:\/\/tooangel.github.io\/screeps\/\nThis repository is World Driven. Pull Requests\nare automatically merged and deployed to the\nScreeps TooAngel account.\nQuests\nHead over to Quests\nFor in game room visitors:\nHappy to see you visiting one of our rooms. Visit FAQ to find answers\nInfo\nThis is the AI I'm using for screeps. I managed to reach Top 10\nfrom November 2015 - March 2016. Main Goal is to automate everything, no\nmanual interaction needed.\nThe AI is deployable on a private screeps server, follow the information on\nSteam or\nnpm install screeps-bot-tooangel and bots.spawn('screeps-bot-tooangel', ROOMNAME)\nNote\nThis is not a good example for code quality or structure, most LOCs written\nwhile fighting or other occasions which needed quick fixes or in the ingame\neditor (getting better :-)). But I think there are a couple of funny ideas.\nEvery contribution is welcome.\nFeatures\n\nAutomatic Base building\nExternal room harvesting\nBasic mineral handling\nPower harvesting\nNew rooms claiming on GCL level up\nAutomatic attack\nRebuild of fallen rooms\nLayout visualization\nManual commands\nGraphs\nTesting\n\nTweaking\nAdd a src\/friends.js with player names to ignore them from all attack\nconsiderations.\nE.g.:\nmodule.exports = ['TooAngel'];\nAdd a src\/config_local.js to overwrite configuration values. Copy\nconfig_local.js.example to src\/config_local.js as an example. src\/config.js\nhas the default values.\nDebugging\nWithin the config_local.js certain config.debug flags can be enabled.\nTo add debug messages Room.debugLog(TYPE, MESSAGE) and\nCreep.creepLog(MESSAGE) are suggested. Especially the creepLog allows\ngranular output of the creep behavior based on the room and the creep role.\nUpload\ninstall dependencies\nnpm install\n\nadd your account credentials\nto screeps.com\nTo deploy to the live server provide the credentials.\nvia env\nexport email=EMAIL\nexport password=PASSWORD\n\nvia git ignored file\necho \"module.exports = { email: 'your-email@here.tld', password: 'your-secret' };\" > account.screeps.com.js\n\nor edit and rename account.screeps.com.js.sample to account.screeps.com.js\nAnd deploy to the server:\ngrunt\n\nto private server\nCreate a .localSync.js file with content:\nmodule.exports = [{\n  cwd: 'src',\n  src: [\n    '*.js'\n  ],\n  dest: '$HOME\/.config\/Screeps\/scripts\/SERVER\/default',\n}];\n\ngrunt local\n\nDevelop\ngrunt dev\n\nRelease\nReleasing to npm is done automatically by increasing the version and merging to master.\nnpm version 10.0.1\ngit push --follow-tags\n\nEvery deploy to master is automatically deployed to the live tooangel account.\nTesting\nnode utils\/test.js will start a private server and add some bots as test\ncases. Within in the tmp-test-server directory the server can be easily\nstarted via screeps start.\nDesign\nMore details of the AI design\n","23":"TooAngel Artificial intelligence for screeps\n\n\n\n\n\n\nhttps:\/\/screeps.com\/\nSee rendered version:\nhttp:\/\/tooangel.github.io\/screeps\/\nThis repository is World Driven. Pull Requests\nare automatically merged and deployed to the\nScreeps TooAngel account.\nQuests\nHead over to Quests\nFor in game room visitors:\nHappy to see you visiting one of our rooms. Visit FAQ to find answers\nInfo\nThis is the AI I'm using for screeps. I managed to reach Top 10\nfrom November 2015 - March 2016. Main Goal is to automate everything, no\nmanual interaction needed.\nThe AI is deployable on a private screeps server, follow the information on\nSteam or\nnpm install screeps-bot-tooangel and bots.spawn('screeps-bot-tooangel', ROOMNAME)\nNote\nThis is not a good example for code quality or structure, most LOCs written\nwhile fighting or other occasions which needed quick fixes or in the ingame\neditor (getting better :-)). But I think there are a couple of funny ideas.\nEvery contribution is welcome.\nFeatures\n\nAutomatic Base building\nExternal room harvesting\nBasic mineral handling\nPower harvesting\nNew rooms claiming on GCL level up\nAutomatic attack\nRebuild of fallen rooms\nLayout visualization\nManual commands\nGraphs\nTesting\n\nTweaking\nAdd a src\/friends.js with player names to ignore them from all attack\nconsiderations.\nE.g.:\nmodule.exports = ['TooAngel'];\nAdd a src\/config_local.js to overwrite configuration values. Copy\nconfig_local.js.example to src\/config_local.js as an example. src\/config.js\nhas the default values.\nDebugging\nWithin the config_local.js certain config.debug flags can be enabled.\nTo add debug messages Room.debugLog(TYPE, MESSAGE) and\nCreep.creepLog(MESSAGE) are suggested. Especially the creepLog allows\ngranular output of the creep behavior based on the room and the creep role.\nUpload\ninstall dependencies\nnpm install\n\nadd your account credentials\nto screeps.com\nTo deploy to the live server provide the credentials.\nvia env\nexport email=EMAIL\nexport password=PASSWORD\n\nvia git ignored file\necho \"module.exports = { email: 'your-email@here.tld', password: 'your-secret' };\" > account.screeps.com.js\n\nor edit and rename account.screeps.com.js.sample to account.screeps.com.js\nAnd deploy to the server:\ngrunt\n\nto private server\nCreate a .localSync.js file with content:\nmodule.exports = [{\n  cwd: 'src',\n  src: [\n    '*.js'\n  ],\n  dest: '$HOME\/.config\/Screeps\/scripts\/SERVER\/default',\n}];\n\ngrunt local\n\nDevelop\ngrunt dev\n\nRelease\nReleasing to npm is done automatically by increasing the version and merging to master.\nnpm version 10.0.1\ngit push --follow-tags\n\nEvery deploy to master is automatically deployed to the live tooangel account.\nTesting\nnode utils\/test.js will start a private server and add some bots as test\ncases. Within in the tmp-test-server directory the server can be easily\nstarted via screeps start.\nDesign\nMore details of the AI design\n","24":"Four legendary heroes were fighting for the land of Vindinium\nMaking their way in the dangerous woods\nSlashing goblins and stealing gold mines\nAnd looking for a tavern where to drink their gold\nWarning\nThe vindinium dot org website has been discontinued, and the domain now belongs to Internet parasites.\nInstallation\nFeel free to install a local instance for your private tournaments.\nYou need sbt, a MongoDB instance running, and a Unix machine (only Linux has been tested, tho).\ngit clone git:\/\/github.com\/ornicar\/vindinium\ncd vindinium\ncd client\n.\/build.sh\ncd ..\nsbt compile\nsbt run\nVindinium is now running on http:\/\/localhost:9000.\nOptional reverse proxy\nHere's an exemple of nginx configuration:\nserver {\n listen 80;\n server_name my-domain.org;\n\n  location \/ {\n    proxy_http_version 1.1;\n    proxy_read_timeout 24h;\n    proxy_set_header Host $host;\n    proxy_pass  http:\/\/127.0.0.1:9000\/;\n  }\n}\n\nDeveloping on the Client Side stack\nwhile the Server runs with a sbt run, you can go in another terminal in the client\/ folder and:\n\nInstall once the dependencies with npm install (This requires nodejs to be installed)\nBe sure to have grunt installed with npm install -g grunt-cli\nUse grunt to compile client sources and watch for client source changes.\n\nCredits\nKudos to:\n\nvjousse for the UI and testing\nveloce for the JavaScript and testing\ngre for the shiny new JS playground\n\n","25":"Halite\n\n\n\nHalite is a AI programming competition. Contestants write bots to play an original multi-player turn-based strategy game played on a rectangular grid. For more information about the game, visit our website.\nContributing\nSee the Contributing Guide.\nQuestions\nSee the Forums and our Discord chat.\nAuthors\nHalite I was conceived of and developed by Ben Spector and Michael Truell in 2016. Two Sigma, having had a history of playful programming challenges for its mathematical and software-oriented teams (e.g., see the Robotic Air Hockey Competition) retained Ben and Michael as 2016 summer interns to develop Halite, run an internal Halite Challenge, and ultimately open Halite up to human and non-human coding enthusiasts worldwide. Halite I was a great success, developing a flourishing community of bot builders from around the globe, representing 35+ universities and 20+ organizations.\nAs a result of the community\u2019s enthusiasm, the Two Sigma team decided to create Halite II, an iteration of the original game with new rules but a similar structure and philosophy. With Ben and Michael as creative advisors, Halite II was developed by David Li, Jaques Clapauch, Harikrishna Menon, Julia Kastner as an evolution of Halite I.\nThe team considered simply reviving Halite I, but given the progress the community made and the number of open source bots that had been published, the team decided to create Halite II with new game mechanics and a fun background story that fleshes out the Halite universe. Halite involved moving pieces around a board with only up-down-left-right options. In 2017\u2019s Halite II, bots battle for control of a virtual continuous universe, where ships mine planets to grow larger fleets and defeat their opponents.\n","26":"aima-javascript\nVisualization of concepts from Russell And Norvig's \"Artificial Intelligence \u2014 A Modern Approach\", and Javascript code for algorithms. Unlike aima-python, aima-java, and other sibling projects, this project is primarily about the visualizations and secondarily about the code.\n\nView the visualizations\nHow to contribute\nChat on Gitter\n\nSome Javascript visualizations that I admire, and would like to see similar kinds of things here:\n\nRed Blob Games: A* Tutorial\nQiao Search Demo\nNicky Case: Simulating the World\nRafael Matsunaga: Genetic Car Thingy\nLee Yiyuan: 2048 Bot\n\n","27":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtificial Intelligence as a Service\n\n\nOrigami is an AI-as-a-service that allows researchers to easily convert their deep learning models into an online service that is widely accessible to everyone without the need to setup the infrastructure, resolve the dependencies, and build a web service around the deep learning model. By lowering the barrier to entry to latest AI algorithms, we provide developers, researchers and students the ability to access any model using a simple REST API call.\n\n\nThe aim of this project is to create a framework that can help people create a web based demo out of their machine learning code and share it. Others can test the model without going into the implementation details. Usually testing models by other people involves a lot of preparation and setup. This project aims to cut that down.\n\n\nThis app is presently under active development and we welcome contributions. Please check out our issues thread to find things to work on, or ping us on Gitter.\nInstallation Instructions\nWindows Installation\nVirtualBox\nOne of the easier ways to get started with Origami on Windows is by using a virtual machine of Ubunutu 16.04 LTS on Oracle's VirtualBox. With Ubuntu installed, Origami can be installed by following the instructions in the next sections. We can install VirtualBox in just two easy steps.\nStep One - Downloading Virtual Box\nYou can install Virtual Box on Oracle's VirtualBox website.\n\nNext, under \"Virtual binaries,\" click on Windows hosts under \"VirtualBox X.X.XX platform packages\" to download the executable file for the latest version of VirtualBox. Wait for this to install and open the file when the download has completed.\nStep Two - Starting installation\nThe .exe file will have the following format: VirtualBox-VersionNumber-BuildNumber-Win.exe.\n\nOnce the setup wizard is open, follow the instructions. Everything can be kept as default, but feel free to change anything to your preference.\nIf you encounter a Windows User Account Control Warning pop-up, click Yes to accept and continue.\nWhen you reach the Network Interface dialouge box, be sure to proceed. VirtualBox will install network interfaces that will interact with the installed your virtual machine(s) and Windows. You will be temporarily disconnected from the Internet, but this connection will be re-established.\n\nWhen you launch VirtualBox, you should see a screen similar to the one below. Congratualtions, you have successfully installed VirtualBox!\n\nSetting up an Ubuntu 16.04 LTS virtual machine with VirtualBox\nOrigami works well on an Ubuntu 16.04 LTS virtual machine, which is what we will use when creating our virtual machine.\nDisable Hyper-V\nHyper-V is a tool that provides hardware virtualization, or allows virtual machines to run on virtual hardware. While this sounds useful, it can hamper your ability to use a 64-bit version of Ubuntu for your virtual machine. To avoid issues further along the line, we will disable this feature. We need to use a 64-bit virtual machine, as this is required for Docker, which will be used to install Origami.\nNote: This will disable other applications that may require Hyper-V, such as Docker for Windows. You can always switch Hyper-V back on, but you will only be able to use VirtualBox or the other application(s) at a time.\n\nPress Windows Key + X and select Apps and Features\n\n\n\nUnder \"Related settings,\" select Programs and Features\n\n\n\nNext, click Turn windows features on or off on the left pane\n\n\n\nFind Hyper-V and unmark it\n\n\n\nFinally, click OK to save changes and reboot your computer\n\nInstalling Ubuntu inside Windows with VirtualBox\nAlthough, below, we install Ubuntu 16.04 for Origami, this method can be used to install all other distributions of Ubuntu. Please be aware that you have at least a minimum of 512 MB of RAM on your computer, but keep in mind 1 or more GB is recommended.\nStep One - Downloading the Ubuntu disk image (.iso file)\nNavigate to this page to view the Ubuntu 16.04.5 LTS release page. Select the \"64-bit PC (AMD64) desktop image\" and save this for usage laters (install the 32-bit desktop image below the 64-bit option if you plan on using a 32-bit virtual machine).\n\nStep Two - Creating the New Virtual Machine\nAfter installing the disk image, we will create the virtual machine on VirtualBox. Launch VirtualBox and select New to proceed. Type in \"Ubuntu\" into the \"Name:\" field of the New Virtual Machine Wizard pop-up. Conveniently, this should adjust the \"Type:\" and \"Version:\" fields automatically as needed.\n\nStep Three - Setting Base Memory (RAM)\nVirtualBox will give a recommendation of how much memory (RAM) to allocate for your virtual machine. If you do not have much RAM, especially 1 GB or less, stick with VirtualBox's recommendation. If you have ample RAM, try to stick to a quarter of your total RAM. If you do not know how much RAM you have, or as a matter of fact do not know what RAM is, stick with the recommendation.\n\nStep Four - Hard Disk\nSince this is probably your first time using VirtualBox, create a new hard disk and then click \"Next.\"\n\nStep Five - Disk Type\nLeave file type as \"VDI (VirtualBox Disk Image)\" and click \"Next.\"\n\nStep Six - Storage Details\nA dynamically expanding virtual hard drive may be best, as it will only take up the space that you actually use on your virtual machine. However, there has been issues where the virtual hard drive fills up instead of actually expanding. Thus, it is recommended to pick \"Fixed size.\"\n\nStep Seven - Disk File Location and Hard Drive Size\nAlthough Origami itself does not take up relatively much space, when installing Docker and other software, hard drive space can run low. Be sure to add as much hard drive space as you can, as it is a bit tedious to expand hard drive space after the virtual machine has been fully set up.\n\nStep Eight - Create the Virtual Hard Drive\nSimply click \"Create\" from the dialouge box from the step prior and wait for the virtual hard drive to be created. As this is usually a large file, it may take a bit of time.\n\nStep Nine - Adding the Downloaded Ubuntu Disk Image\nBefore we boot the virtual machine, we need to add the downloaded Ubuntu disk image (.iso file) onto the virtual machine. While your virtual machine is selected in the left pane, click Settings and then Storage. Next, under \"IDE Controller,\" select Empty and click on the little disk icon. In the menu, click Choose Virtual Optical Disk File... next to the folder icon.\n\nNavigate to the Ubuntu disk image file downloaded earlier and click \"Open.\"\nNote: Both disk image versions for Ubuntu desktop are downloaded in the image below. As we are using a 64-bit virtual machine, we are opening the 64-bit .iso file.\n\nAfterward, \"Empty\" should now be replaced by the filename of our disk image file, and we can now click OK.\n\nStep Ten - Downloading Ubuntu onto your virtual machine\nDouble-click your virtual machine to start it up. You may get various pop-ups providing warnings and instructions in regard to operating a virtual machine with VirtualBox. Be sure to read these, and you can mark not to see these again if you would like. Once Ubuntu is booted up, click Install Ubuntu and follow the instructions as if you were installing Ubuntu on an actual hard drive.\n\n\n\nInstalling Docker\nWe use Docker to install Origami. As Origami runs well on Ubuntu, we recommend you follow the official Docker documentation here. Use the \"repository method\" for the installation of Docker CE on this site. CE stands for \"Community Edition,\" as is designed for developers and ordinary users. Make sure to install the latest version of Docker (skip step #3 on \"Installing Docker CE\"), and if you followed the tutorial above and created an Ubuntu virtual machine, follow the x86_64 architecture command when setting up the repository.\nIf you are using MacOS, follow the instructions on Docker's site here.\nSetting the environment variables\nRefer to the below during the installation process as needed.\n\norigami.env stores all the environment variables necessary to run Origami.\n\n\nHOST should be set to the hostname of the server.\nPORT should be set to the port you want the server to listen on.\nDB_NAME will be used to set the name for your postgres database.\nDB_PASS will be used to set the password for the database user. This is also the admin password.\nDB_USER is the username for a user who can modify the database. This is also the admin username.\nDB_USER_EMAIL stores the email for the admin.\nDB_HOST should be set to postgres in production and localhost in development.\nREDIS_HOST should be set to redis and localhost in development.\n\nTo create the file, cp origami.env.sample origami.env and edit the file with the above fields.\n\nOrigami\/outCalls\/config.js stores config variables needed by the UI.\n\n\nCLIENT_IP should be set to the same value as HOST in origami.env\nCLIENT_PORT should be set to the same value as PORT in origami.env\nFor DROPBOX_API_KEY , check step 3 of configuring Origami\n\nProduction setup instructions\nUse Docker to setup Origami on production\nRunning the server\nYou can run the server with the help of docker and docker-compose.\nRun  $ docker-compose up\nDevelopment setup instructions\nThis application requires Pip, Node.js v5+, Yarn, and Python 2.7\/3.4+ to install\nInstalling Pip\nIf you do not already have pip installed, run the following command:\n$ sudo apt-get update\n$ sudo apt get python-pip\n\nMacOS: $ sudo easy_install pip\n\nInstalling Node.js\nTo install a stable and up-to-date version of Node.js, we will use Node's PPA (Personal Package Archive). Keep in mind this is optimal for Linux Mint and Ubuntu operating systems. Please run the following commands to install the latest version of Node.js:\n$ sudo apt-get update\n$ sudo apt install curl\n$ curl -sL https:\/\/deb.nodesource.com\/setup_10.x | sudo bash -\n$ sudo apt install nodejs\n$ node -v\n\nVerify that your Node.js version is v5 or greater.\n\nInstalling Yarn\nYarn helps install dependencies and other packages with ease. Here we will use npm (Node Package Manager) to install Yarn. As npm is installed with Node.js, be sure Node is already installed. Notice in the command that we include the -g flag for installation globally, so Yarn can thus be used in all of your projects.\n$ sudo apt-get update\n$ sudo npm install yarn -g\n\n\nInstalling Python\nFinally, we can install Python. Follow these commands to get the most up-to-date version of Python. If you would like a specific version of Python, be sure to include your preference after python (e.g. python 3.6 for Python 3.6). Ubuntu comes with Python installed, which is typically Python 2.7. Below, we install python3, and to use this, we would replace all python commands with python3. Below are the commands:\n$ sudo apt-get update\n$ sudo apt-get install python3\n$ python3 --version\n\n\nCreate a Virtual Environment\n\n$ pip install virtualenv\n$ virtualenv venv venv = Name of virtualenv\n$ source venv\/bin\/activate\n\nNote: Step 2 will create a folder named venv in your working directory\nGetting the code and dependencies\n\nClone the repository via git\n\n$ git clone --recursive https:\/\/github.com\/Cloud-CV\/Origami.git && cd Origami\/\n\nRenaming origami.env.sample as origami.env and setting environment variables in origami.env\n\n$ cp origami.env.sample origami.env\n$ nano origami.env\n\nHere, set the environment variables according to the above instructions on environment variables. Once they have been edited, Ctrl O, Enter, and Ctrl X to save and exit. The following is an example of what this may look like (be sure to include localhost as the necessary values below if you are going to run Origami on your local machine).\nset -a\nHOST=localhost\nPORT=8000\nDB_NAME=origami546\nDB_PASS=origami546\nDB_USER=origami546\nDB_USER_EMAIL=example@gmail.com\nDB_HOST=localhost\nREDIS_HOST=localhost\nset +a\n\nAfterward, run the following to set more variables as entailed in the above section for environment variables for Origami\/outCalls\/config.js\n$ nano Origami\/outCalls\/config.js\n\n\n\nAdd all of the Python dependencies.\n$ pip install -r requirements.txt\n\n\nSet up the Postgresql database\n\n\nInstall postgresql if you have not already. The -contrib package will add more utilities and added functionality.\nsudo apt-get update\nsudo apt-get install postgresql postgresql-contrib\n\nNext we will create a database containing the details we will use for Origami. Following the previous example, creating the database may look like the following:\n$ sudo service postgresql start\n$ sudo -u postgres psql\npostgres=# CREATE DATABASE origami546;\npostgres=# CREATE USER origami546 WITH PASSWORD 'origami546';\npostgres=# ALTER ROLE origami546 SET client_encoding TO 'utf8';\npostgres=# ALTER ROLE origami546 SET default_transaction_isolation TO 'read committed';\npostgres=# ALTER ROLE origami546 SET timezone TO 'UTC';\npostgres=# ALTER USER origami546 CREATEDB;\npostgres=# \\q\n\n\n\nAdd all of the Javascript dependencies\n$ yarn (preferably)\nor\n$ npm install\n\n\nSetup the Redis server\n\n\n$ docker run -d -p 6379:6379 --name origami-redis redis:alpine\n\n\nActivate the environment\n$ source origami.env\n\n\nSetting up the database\nCreate all the tables\n$ python manage.py makemigrations\n$ python manage.py migrate\n\nCreate admin account\n$ python manage.py initadmin\nStart the server\nTo ensure everything works out, follow these steps carefully. Make sure all three terminals are running at the same time.\n\nStart the server by\n\n$ python manage.py runserver --noworker\n\nStart the worker\n\nOpen a second terminal and run the following:\n$ source venv\/bin\/activate\n$ cd Origami\/\n$ source origami.env\n$ python manage.py runworker\n\n\nRunning the server with Yarn\n\nOpen a third terminal and run the following:\n$ source venv\/bin\/activate\n$ cd Origami\/\n$ source origami.env\n$ yarn run dev\n\n\nGo to localhost:8000\nVisit Read the docs for further instructions on getting started. If you have never created an OAuth App on GitHub, see the below instructions.\n\nSetup Authentication for Virtual Environment\n\n\nGo to Github Developer Applications and create a new application here.\n\n\nFor local deployments, use the following information:\n\nApplication name: Origami\nHomepage URL: http:\/\/localhost:8000\nApplication description: Origami\nAuthorization callback URL: http:\/\/localhost:8000\/auth\/github\/login\/callback\/\n\n\n\nGithub will provide you with a client ID and secret Key, save these.\n\n\nStart the application.\n\n\n$ python manage.py runserver\n\n\n\nOpen http:\/\/localhost:8000\/admin\n\n\nLogin with the credentials from your admin account. This should be your username and password you used for the Postgresql if everything was kept consistent.\n\n\nFrom the Django admin home page, go to Sites under the Sites category and make sure \"localhost:8000\" is the only site listed under \"DOMAIN NAME\".\n\n\nContributing to Origami\n\n\nMake sure you run tests on your changes before you push the code using:\n\n$ python manage.py test\n$ yarn run test\n\n\n\nFix lint issues with the code using:\n\n$ yarn run lint:fix\n\n\n\nLicense\nThis software is licensed under GNU AGPLv3. Please see the included License file. All external libraries, if modified, will be mentioned below explicitly.\n","28":"UnityAI\nReusable Artificial Intelligence Experiments\n\n\nCurrent Features\n\nPathfinding editor for waypoints and pathing visualization\nA* Route Planning\nRandom waypoint navigation\nWaypoint to waypoint following using Pathfinder\nNavigation mesh processor in the Tools Menu (creates a NavmeshNode network with triangles and vertices from a selected mesh)\nPathing for navigation meshes\nFunnel algorithm for navigation meshes\nSteering behavior for path following\nA basic FPS with path following spiders, ammo, and health spawned by an AI director\n\nScript Locations\nThis project includes 2 Unity projects:\n\nUnityPathing (a sandbox project for pathfinding experiments)\nBasicGame (the FPS demo with pathfinding and an AI director)\n\nPlanning scripts are located in BasicGame\/Assets\/Pathfinding Scripts and gameplay\/director scripts are located in BasicGame\/Assets\/Scripts\n##License: MIT\nCopyright (c) 2013 Julian Ceipek, Alyssa Bawgus, Eric Tappan, Alex Adkins\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","29":"\n  TimeMap v0\n\n\nTimeMap is a tool for exploration, monitoring and classification of incidents in time and space.See a live instance here.\n\n\n\nOverview\nTimeMap is a standalone frontend application that allows to explore and monitor events in time and space. TimeMap uses OpenStreetMap satellite imagery as a backdrop by default, but can also be configured to use mapbox. It uses Leaflet and d3 to visually map information.\nThe recommended way to run a backend for timemap is using datasheet-server. This allows you to work with a spreadsheet or Google Sheet as a dynamic database for for timemap.\nTimeMap has the following high-level features capabilites:\n\nVisualize incidents of particular events on a map.\nVisualize and filter these incidents over time, on an adjustable timeline that allows to zoom in and out.\nVisualize types of incidents by tag and by category, which can be displayed using different styles.\n\nA fully-functioning live version can be found as a result of the Forensic Architecture investigation of the Battle of Ilovaisk.\nGet up and running\nThese easiest way to get up and running with timemap and datasheet-server is to\nfollow the tutorial here.\nInstructions\n\nPull this repository.\n\ngit clone https:\/\/github.com\/forensic-architecture\/timemap\n\nInstall dependencies via yarn (recommended, it's just faster) or npm.\n\nyarn          # npm install\n\nRun it via yarn.\n\nyarn dev      # npm run dev\nTo run with a file that is not 'config.js' in the root directory, set the CONFIG environment variable:\nCONFIG=\"myotherconfig.js\" yarn dev\n\nIMPORTANT: Although the application will run just like that, in order for TimeMap to be able to display interesting information, you'll have to make sure to have the capacity to serve data, as well as adjusting some configuration parameters. See next section.\nRunning without datasheet-server\nTechnically, timemap is backend agnostic, but it requires a series of endpoints to provide data for it to visualize. The data is expected in JSON format. Some data elements are required and their format has some required fields. Other additional endpoints are optional, and if enabled, they simply add features to your taste.\nThe combination of all these data types is called the domain of the application in the context of TimeMap.\nContribute\nCode of Conduct\nPlease read before contributing. We endeavour to cultivate a community around timemap and other OSS at Forensic Architecture that is inclusive and respectful. Please join us in this!\nContributing Guide\nLearn more about our development process,  i.e. how to propose bugfixes and improvements.\nCommunity\nIf you have any questions or just want to chat, please join our team fa_open_source on Keybase for community discussion. Keybase is a great platform for encrypted chat and file sharing that we use as a public forum.\nLicense\ntimemap is distributed under the DoNoHarm license.\n","30":"This repository is a collection of recent experiments I've been working on in three.js.\nThree.js is a JavaScript library built on top of the WebGL graphics language.  WebGL is a low level, verbose language used to create graphics in the browser that can be both very performant and very hard to use.  Three.js greatly reduces the amount of boilerplate code you have to write to build rich 3D graphics, and wraps common operations into intuitive constructor functions.  If you're interested in learning three.js, I recently completed two new tutorials on getting started with the three.js library.  You can find them at loftus.xyz\nYou will find the different simulations in this repo in the examples folder.  To develop and test out the simulations locally, first clone the repo down to your local machine.\ngit clone https:\/\/github.com\/MattLoftus\/threejs-space-simulations.git\n\nTo avoid cross-origin errors when using textures(every example in this repo), you will need to host the files on a local server.  I recommend using python simple server or npm live server.  First navigate to the root of the directory, then run the following command.\npython -m SimpleHTTPServer\n\nThis will host the folder on port 8000, so you can head over to the browser and type \"localhost:8000\" into the address bar, and you will see a listing for the directory. If you happen to have a version of python on your machine >= 3.0, you may need to run the following command instead.\npython2.7 -m SimpleHTTPServer\n\n","31":"The Kabal Invasion\nThe Kabal Invasion is a web-based 4X space game. It is coded in PHP\/HTML\/JS\/SQL.\n\n\n\n\n\n\n\n\n\nWhat is it?\nA web based space exploration (4x) game based on the old BBS door games that went\nby many names (Tradewars, Galactic Warzone, Ultimate Universe, and\nmany other games like this) but shares no code with them.  It is\nwritten 100% in PHP\/HTML\/JS\/SQL.\n\nWhy should I run this?\nWeb-based games that recreate the door game BBS experience can be fun!\nSince it is Free and open source software, anyone can examine, learn, and contribute.\n\nIs this game ready to install and play?\nAt the moment, we've identified a number of release-blocking issues including broken\npassword management, user sign-up, and issues with non-functional database calls. Serious\neffort is underway to fix all of these issues, and we are working towards a release. In the meantime,\ncurious developers are encouraged to download, install, and play as the admin user to find issues\nand report them. When we get to a point where the game is stable for players,\nwe will make an announcement, change this note, and release!\n\nLicense: Affero GPL v 3\nCredits:\nThe Kabal Invasion forked from Blacknova Traders, please visit their sourceforge page for more information about their project. We proudly stand on the shoulders of giants, with BNT originally having hundreds of developers, players, and admins. We honor and appreciate their 15+ year contribution that makes our project possible.\nRequirements:\nServer (generally, the most recent\/current version of each is our recommendation, but these should suffice):\n\nA Linux server. Our primary development platform is Fedora, but most Linux distributions should work, and potentially even OpenBSD.\nA webserver capable of TLS such as apache v2.4+ (we have not determined a required minimum).\nphp v7.4.5+ (needed for type-hinting property types).\nmariadb v5.5+ or v10.0+ (needed for utf8mb4 schemas).\npdo PHP extension.\n\nWeb:\n\nChrome v50+ or Firefox v40+ (recommended).\nSafari v9.1.2+.\nIE v11.\n\nNotes:\n\nTKI will likely run on lighttpd and nginix, however htaccess will not work out of the box - potentially causing security risks. It has not been tested on either.\nIIS and\/or Windows is NOT supported, please do not ask! (But we welcome code to make it work on IIS)\nDevelopment \"Snapshots\" are intended only for developers that are actively involved in the development process, and require additional effort to work (composer, etc).\nWe make use of Smarty templates, HTML Purifier, Swiftmailer, and Adodb (although we are working to replace adodb with PDO).\n\nInstallation:\nPlease see the \/docs\/install.md file.\nUpgrades:\nAs is typical with our releases, we highly recommend a fresh install. Upgrades are not supported at this time.\nCode quality:\nThe project began in the early PHP4 era, and as a result, is less than ideal. Substantial progress has been made towards modernization, and we are continuing that process. As a general guideline, we follow PSR-1,2,4, and the upcoming 12, with the major exceptions that we use BSD\/Allman brace\/brackets and do not yet follow line length limits. Feedback and PR's are welcome and appreciated.\nCritical needs:\nThe two areas we need the most focus in would be the documentation, and testing. Both can be done with little or no knowledge of PHP, and would help us dramatically.\nSecurity issue reporting:\nPlease report all security issues to thekabal@gmail.com.\nContributing:\nFeel free to contribute to the project! We use Gitlab for our issues tracking (provide feedback!), milestone planning, code storage, and releases.\nI hope you enjoy the game!\nThe Kabal\n","32":"VAE Latent Space Explorer\nThis application is a toy visualization that allows you to generate new images of 28x28 numerical digits using a variational autoencoder.\nYou can view the visualization here\nImplementation details\nThe variational autoencoder was implemented using Keras and the relevant code is located in the scripts directory in a Jupyter notebook.\nOnce the model is trained, the architecture and weights are saved in a format that can be ingested by tensorflow-js. Tensorflow-js handles\nimplementing the model architecture and loading the weight in the browser. By leveraging WebGL, the model can efficient generate new image samples when given an appropriate latent space vector in the client browser.\nThe application uses React.js for interface updates along with html Canvas to draw the image matrices. D3.js was used to generate the scatterplot and handle hover events.\nCreated by Taylor Denouden (April 2018)\n","33":"SEH - Space Exploration History\nThe Solar System\nBehold: the entire history of Solar System exploration in one graphic.\nShowing all missions beyond Earth orbit send to explore Solar System objects since 1958.\nSome more documentary listings\/garaphics:\nGround Segment Map overwiew\nMissions, Destinations, Launch sites Lists\nRockets in one graphic in order of appearance\nSpace Telescopes\nExtensive description here\nSpectral Range of all operational and future space telescopes and some ground based examples for comparison\nSpectral Range vs. Angular Resolution for operational scopes\nThe apps require a HTML5-capable browser, so all fairly new versions of Chrome, Firefox, IE, Opera and what-have-you should work. If your browser only shows a static image, it is too old.\nSources\nThese sites (and books) helped me greatly to gather all the data represented here:\nNational Space Science Data Center (NSSDC)\nNASA History Astronautics and Aeronautics Chronology Series\nSolar System Exploration Mission Profiles\nJPL Mission and Spacecraft Library\nEncyclopedia Astronautica, Mark Wade\nJonathan's Space Report, Johnathan McDowell\nSpaceflight Realtime Simulations and Information, Daniel Muller\nEarth Observation Portal, ESA\nGunter's Space Page, Gunter Krebs\nJohnston's Archive, Robert Johnston\nThe Planetary Society, Emily Lakdawalla et.al.\nVenera: The Soviet Exploration of Venus, Don P. Mitchell\nRussian Space Web, Anatoly Zak\nSpaceflight 101, Patrick Blau\nZarya Soviet, Russian and International Space Flight, Robert Christy\nVisual Satellite Observing FAQ\nWeebau Space Encyclopedia, Pierre Bauduin\nand of course all the mission websites linked in the app above.\nImages\nThe Planetary Society, planet images\nPlanetary Maps, Steve Albers\nNASA Visible Earth\nMoon shaded relief\nMars MOLA map\nDr. Paul Schenk (Neptune Rings)\nHistoric Spacecraft, Richard Kruse (Rocket images)\nSolar System Data\nPlanetary Fact Sheets (NSSDC)\nSolar System Dynamics (JPL)\nThe Astronomical Almanac Online (USN)\nGazetteer of Planetary Nomenclature (USGS)\nMinor Planet Center (IAU)\nPlanetary Data System (PDS) - Rings node -\nAtmospheres node\nBooks\nDeep Space Chronicle, Asif Siddiqi, NASA History Monograph SP 2002-4524\nSoviet Robots in the Solar System, Wesley T. Huntress, Jr. & Mikhail Ya. Marov, Springer 2011\n--\nSpacecraft positions algorithm with great help from Project Pluto\nReleased under BSD License\n","34":"SpaceApps2020 WINNER - Space Nearby\n\nChallenge\nAn interactive webapp using Google Maps API to reveal locations involved in the space exploration industry nearby!\nNote: Google API keys have been temporarily disabled.\nDevelopers:\nJared Bentvelsen\n\n\nBassel Rezkalla\n\n\nMatthew McCracken\n\n\nChristopher Andrade\n\n\nYuvraj Randhawa\n\n\n","35":"Space exploration web app\nhttp:\/\/compact.github.io\/space\/\nThis app allows you to navigate through space in your browser. It uses three.js to render astronomical bodies.\nWorkflow\nDevelop\nnpm install\nbower install\nnode app.js\n\nGo to http:\/\/localhost:3001.\nBuild\nnpm install -g grunt\ngrunt build\n\nThe built files are located in \/dist\/.\nGenerate docs\nnpm install -g jsdoc\njsdoc -p -d app\/docs app\/scripts\/ app\/data\/\n\n","36":"Space exploration web app\nhttp:\/\/compact.github.io\/space\/\nThis app allows you to navigate through space in your browser. It uses three.js to render astronomical bodies.\nWorkflow\nDevelop\nnpm install\nbower install\nnode app.js\n\nGo to http:\/\/localhost:3001.\nBuild\nnpm install -g grunt\ngrunt build\n\nThe built files are located in \/dist\/.\nGenerate docs\nnpm install -g jsdoc\njsdoc -p -d app\/docs app\/scripts\/ app\/data\/\n\n","37":"Wandering Lines\nAn exploration of lines wandering across a space. Inspired from the work of Anders Hoff, @inconvergent.\nRunning locally\nMake sure npm is installed, download this repo, then from the folder in the command line run npm install to install the dependencies, then run npm run dev:1 to start a local reload server. Then to finish and build the final files run npm run build:1. Replace the 1 in dev:1 and build:1 to run and build each version in the series.\n","38":"Rogue Starfarer\nRogue Starfarer is interstellar exploration roguelike created for the 2018 7DRL competition.\nHow to Build\nJust open up index.html in your browser.\nHow to Play\nThe green 'X' indicates the position that your ship will move to in the next turn, based on your momentum. You can fire your rockets to plot your course by moving the green 'X' with the arrow keys or numpad. The '0' indicates your course if you make no adjustments. Press space to end the turn.\nBeware that the amount you can adjust your ship's course in any one turn is limited by the energy reserves and propulsion of your ship! The Maneuver: -3\/\u0394 on the sidebar indicates that adjusting your trajectory will cost you 3 energy. The more weapons and armor you add to you ship, the more it will cost to maneuver, and the more you upgrade your engines, the less it will cost.\nThe green arrow indicates the the direction your ship is currently facing. Your ship's facing changes automatically based on your trajectory.\nWeapons are mounted on particular hull zones and each weapon can only fire out of that hull zone. Each weapon has its own firing arc determined by its range, hull zone, and your ship's facing. Press 'w' to cycle through weapons, press 'tab' to cycle through eligible targets within the weapon's arc, and press 'f' to fire. You can also target and fire on ships using the mouse.\nExploration\nThe universe is filled with strange events and mysterious sites to explore. You can land on any planet by flying over it and coming to a stop. If you collide with a planet while traveling faster than speed 1, you will suffer damage. You can board a destroyed or abandoned ship by flying over it. Note that destroyed ships will continue on their original trajectories, so you may find that a tractor beam is helpful to bring them to a stop.\nYou will also find perplexing anomalies in space and merchant stations that can repair and refit your ship. You can interact with these by flying over them.\nShip Systems\nReactor and Energy Storage\nYour ship has a state-of-the-art fusion reactor that continously produces energy which is stored in your ship's capacitor banks. Maneuvering and firing weapons consumes energy. If your current energy is at least 50% of maximum, your shields and warp core will slowly recharge. In normal circumstances your energy reserves will never exceed your maximum. However, if any special item or effect raises your energy reserves above twice your maximum, your reactor will overload and begin to melt down, dealing 1 point of hull damage per turn.\nCrew\nYour ship has a minimum and maximum crew. For every crewmember you have above the minimum, there is a chance every turn that you will repair 1 point of hull damage. For every crewmember you have below the minimum, there is a chance that your reactor will produce no energy that turn.\nWarp Core\nYour ship has a warp core which allows it to jump between star systems. To do so you must have the hyperspace coordinates for the new system and your warp core must be fully charged (20\/20). Your warp core will recharge 1 point per turn while your ship's energy reserves are above 50%. Jump to hyperspace by pressing the 'j' key.\nShields\nYour shields reduce damage from most types of attacks. Your shields will naturally recharge when you are above 50% of your maximum energy.\nWeapon Types\nThere are four main types of weapons: lasers, ion cannons, tractor beams, and neutron beams. Lasers deal damage to shields first, and to hull second. Ion cannons deal damage to shields first and to energy reserves second. Note that ion cannons can reduce a ship's energy reserves to negative levels. Tractor beams reduce a ship's speed, but only work if the target's shields are already down. Neutron beams deal damage directly to a ship's crew, but like tractor beams they are completely blocked by any amount of shields.\nEach weapon can only fire once per turn, and only if sufficient energy is available. Mounting a weapon increases your ship's mass (increasing your maneuver cost) and increases your minimum crew requirement. Beyond this, there are no restrictions to the number of weapons you can affix to a hull zone.\nRumors of persist of ancient precusor relics that are far more powerful than the standard types of weapons listed above.\nBoarding Tubes\nBoarding tubes can bridge the vacuum of space and slice through even the thickest armor. If you and an enemy ship are adjacent, and either ship is at 0 speed, a boarding action will be initiated. Your crewmemebers will fight directly, though an entire shipboard battle can rarely be resolved in a single turn.\n","39":"Opentrons Platform\n\n\n\n\nOverview\nOpentrons API\nOpentrons App\nContributing\n\nOverview\nOpentrons makes robots for biologists.\nOur mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other's work. Our robots automate experiments that would otherwise be done by hand, allowing our users to spend more time pursuing answers to the 21st century\u2019s most important questions, and less time pipetting.\nThis repository contains the source code for the Opentrons API and OT App. We'd love for you to to explore, hack, and build upon them!\nOpentrons API\nThe Opentrons API is a simple framework designed to make writing automated biology lab protocols easy.\nWe've designed it in a way we hope is accessible to anyone with basic computer and wetlab skills. As a bench scientist, you should be able to code your automated protocols in a way that reads like a lab notebook.\npipette.aspirate(location=trough['A1'], volume=30)\npipette.dispense(location=well_plate['A1'], volume=30)\nThis example tells the Opentrons OT-2 to pipette 30 \u00b5L of liquid from a trough to well plate. Learn more here:\n\nDocumentation\nSource code\n\nOpentrons App\nEasily upload a protocol, calibrate positions, and run your experiment from your computer.\n\nDownload Here\nDocumentation\nSource code\n\n\nOpentrons Protocol Designer\nEasily create a protocol to run on your robot with this graphical tool.\n\nAccess Here\nDocumentation\nSource code\n\nContributing\nWe love contributors! Here is the best way to work with us:\n\n\nFiling a bug report. We will fix these as quickly as we can, and appreciate your help uncovering bugs in our code.\n\n\nSubmit a pull request with any new features you've added to a branch of the API or App. We will reach out to talk with you about integration testing and launching it into our product!\n\n\nFor more information and development setup instructions, please read the contributing guide.\nEnjoy!\n","40":"GENtle2\nA re-think for the web of the original GENtle.\nGENtle2 has been almost entirely rewritten over the past year, and remains\nvery much in development. Core features will be extracted into their own modules\nin the coming months.\n\n\nGetting started\n\nClone the repository locally and cd into it\nRun the following to install the app and its dependencies and compile it.\n\nnpm install --production\n\nRun the following command to start the application\n\nnpm start\n\nOpen you browser and navigate to http:\/\/localhost:3000\n\nContributing\nFor more details about how the application works, see CONTRIBUTING.md\n","41":"\n\nInstalling JBrowse\nTo install jbrowse, visit http:\/\/jbrowse.org\/blog and download the latest JBrowse zip file. See instructions at http:\/\/jbrowse.org\/docs\/installation.html for a tutorial on setting up a sample instance.\nInstall JBrowse from GitHub (for developers)\nTo install from GitHub, you can simply clone the repo and run the setup.sh script\ngit clone https:\/\/github.com\/GMOD\/jbrowse\ncd jbrowse\n.\/setup.sh\n\nDevelop JBrowse or JBrowse plugins\nTo obtain a jbrowse development environment, e.g. for jbrowse source code editing or plugin development (or just running jbrowse from the github repo)\ngit clone https:\/\/github.com\/GMOD\/jbrowse\ncd jbrowse\n.\/setup.sh # not strictly necessary if you don't need to sample data\n\nIf you are going to edit the jbrowse source code, then also run\nyarn watch\n\nAnd keep yarn watch running in the background as you create changes to your code.\nTo start a temporary dev server, can also run\nyarn start\n\nAnd keep this running in the background, this will launch a webserver running jbrowse on port 8082.\nAlternatively, you can put this jbrowse folder in your webserver (e.g. \/var\/www\/html\/) directory. The key is, if you are modifying jbrowse or plugin source code, to run yarn watch in the background, so that webpack incorporates your changes in either the main codebase (src\/JBrowse folder) or any plugins (plugins\/YourPlugin).\nNote for users in China\nIn order to make downloads faster you can set a mirror for the npm registry\nnpm config set registry http:\/\/r.cnpmjs.org\nnpm config set puppeteer_download_host=http:\/\/cnpmjs.org\/mirrors\nexport ELECTRON_MIRROR=\"http:\/\/cnpmjs.org\/mirrors\/electron\/\"\n\nNotes on setting up a JBrowse server\n\n\nIf you don't have a webserver such as apache or nginx, you can run npm run start and open http:\/\/localhost:8082\/index.html?data=sample_data\/json\/volvox to see the code running from a small express.js server.\n\n\nYou can alternatively just move the jbrowse folder into a nginx or apache root directory e.g. \/var\/www\/html and then navigate to http:\/\/localhost\/jbrowse\n\n\nNote: you should avoid using sudo tasks like .\/setup.sh and instead use chown\/chmod on folders to your own user as necessary.\nAlso note: After editing a file, you must re-run the webpack build with npm run build or you can keep webpack running in \"watch\" mode by running  npm run watch.\nAlso also note: by default git clone will clone the master branch which contains the latest stable release. The latest development branch is called dev. Run git checkout dev after clone to retrieve this\nInstalling as an npm module\nTo install jbrowse from NPM directly, you can run.\nnpm install @gmod\/jbrowse\n\nTo setup a simple instance, you can use\nnode_modules\/.bin\/jb_setup.js\nnode_modules\/.bin\/jb_run.js\n\nThen visit http:\/\/localhost:3000\/?data=sample_data\/json\/volvox\nContributing\nLooking for places to contribute to the codebase?\nCheck out the \"help wanted\" label.\nRunning the developer test suites\nThe Travis-CI suite runs Perl, JavaScript, and Selenium automated tests. To run locally, you can use\nprove -Isrc\/perl5 -lr tests\nnode tests\/js_tests\/run-puppeteer.js http:\/\/localhost\/jbrowse\/tests\/js_tests\/index.html\npip install selenium nose\nMOZ_HEADLESS=1 SELENIUM_BROWSER=firefox JBROWSE_URL='http:\/\/localhost\/jbrowse\/index.html' nosetests\n\nSupported browsers for SELENIUM_BROWSER are 'firefox', 'chrome', 'phantom', and 'travis_saucelabs'.  The Sauce Labs + Travis\none will only work in a properly configured Travis CI build environment.\nManual testing\n\nJBrowse has a free open source account on Browserstack for manual testing.  Contact @rbuels for access.\nGenerating Packaged Builds\nYou can also optionally run build steps to create the minimized codebase. Extra perl dependencies Text::Markdown and DateTime are required to run the build step.\nmake -f build\/Makefile\n\nTo build the Electron app (JBrowse desktop app), run the following\nnpm install -g electron-packager\nmake -f build\/Makefile release-electron-all\n\nTo run the Electron app in debug mode run the following\nnpm install -g electron\nelectron browser\/main.js\n\nMaking a JBrowse release\nNOTE: Beginning in 1.12.4,\n\n\nRun build\/release.sh $newReleaseVersion $nextReleaseVersion-alpha.0 notes.txt, where notes.txt is any additional information to add to a blogpost. Then check its work, and then run the git push command it suggests to you. This makes a tag in the repository for the release, named, e.g. 1.6.3-release.  This should cause Travis CI\nto create a release on GitHub under https:\/\/github.com\/GMOD\/jbrowse\/releases\n\n\nTest that the page loads in IE11 on BrowserStack\n\n\nAdd release notes to the new GitHub release that Travis created. Can just paste these from release-notes.md, which is in Markdown format.\n\n\nWrite a twitter post for usejbrowse and JBrowseGossip with the announcement link to the blogpost\n\n\nWrite an email announcing the release, sending to gmod-ajax. If it is a major release, add gmod-announce and make a GMOD news item.\n\n\nAs you can tell, this process could really use some more streamlining and automation.\n","42":"\n\n\n\n\n\n\nEscher\nEscher is a web-based tool to build, view, share, and embed metabolic maps. The\neasiest way to use Escher is to browse or build maps on the\nEscher website.\nVisit the documentation to get started with\nEscher and explore the API.\nCheck out the\ndeveloper docs,\nthe Gitter chat room, and the\nDevelopment Roadmap for information\non Escher development. Feel free to submit bugs and feature requests as Issues,\nor, better yet, Pull Requests.\nFollow @zakandrewking for Escher updates.\nYou can help support Escher by citing our publication when you use Escher or\nEscherConverter:\nZachary A. King, Andreas Dr\u00e4ger, Ali Ebrahim, Nikolaus Sonnenschein, Nathan\nE. Lewis, and Bernhard O. Palsson (2015) Escher: A web application for\nbuilding, sharing, and embedding data-rich visualizations of biological\npathways, PLOS Computational Biology 11(8):\ne1004321. doi:10.1371\/journal.pcbi.1004321\nEscher was developed at SBRG. Funding was\nprovided by The National Science Foundation Graduate Research Fellowship\nunder Grant no. DGE-1144086, The European Commission as part of a Marie Curie\nInternational Outgoing Fellowship within the EU 7th Framework Program for\nResearch and Technological Development (EU project AMBiCon, 332020),\nand The Novo Nordisk Foundation\nthrough The Center for Biosustainability\nat the Technical University of Denmark (NNF10CC1016517)\nBuilding and testing Escher\nJavaScript\nFirst, install dependencies with npm (or you can use\nyarn):\nnpm install\n\nEscher uses webpack to manage the build process. To run typical build steps, just run:\nnpm run build\n\nYou can run a development server with:\nnpm run start\n# or for live updates when the source code changes:\nnpm run watch\n\nTo test the JavaScript files, run:\nnpm run test\n\nPython\nEscher has a Python package for generating Escher visualizations from within a\nPython data anlaysis session. To learn more about using the features of the\nPython package, check out the documentation:\nhttps:\/\/escher.readthedocs.io\/en\/latest\/escher-python.html\nYou can install it with pip:\npip install escher\n\nJupyter extensions\nWhen you pip install escher, the Jupyter notebook extension should be\ninstalled automatically. If that doesn't work, try:\n# The notebook extenstion should install automatically. You can check by running:\njupyter nbextension list\n# Make sure you have version >=5 of the `notebook` package\npip install \"notebook>=5\"\n# To manually install the extension\njupyter nbextension install --py escher\njupyter nbextension enable --py escher\n# depending on you environment, you might need the `--sysprefix` flag with those commands\nTo install the Jupyter lab extension, simply install Escher with pip install escher then\ninstall the extension:\njupyter labextension install @jupyter-widgets\/jupyterlab-manager\njupyter labextension install escher\nPython\/Jupyter Development\nFor development of the Python package, first build the JavaScript package and\ncopy it over to the py directory with these commands in the Escher root:\nnpm install\nnpm run build\nnpm run copy\n\nThen in the py directory, install the Python package:\ncd py\npip install -e . # installs escher in develop mode and dependencies\n\nFor Python testing, run this in the py directory:\ncd py\npytest\n\nTo develop the Jupyter notebook and Jupyter Lab extensions, you will need\ninstall them with symlinks.\nFirst, install the Python package for development as described above.\nFor the Jupyter notebooks, run:\ncd py\njupyter nbextension install --py --symlink escher\njupyter nbextension enable --py escher\n\nIf you are using virtualenv or conda, you can add the --sys-prefix flag to\nthose commands to keep your environment isolated and reproducible.\nWhen you make changes, you will need to yarn build && yarn copy and refresh\nnotebook browser tab.\nFor Jupyter Lab, run (in the root directory):\nyarn watch # keep this running as a separate process\njupyter labextension install @jupyter-widgets\/jupyterlab-manager\njupyter labextension link\njupyter lab --watch\n\nIf you don't see changes when you edit the code, try refreshing or restarting\njupyter lab --watch.\nDocs\nBuild and run the docs::\ncd docs\n.\/build_docs\ncd _build\/html\npython -m SimpleHTTPServer # python 2\npython -m http.server # python 3\n\n","43":"\n\n\nlayout\ntitle\npermalink\n\n\n\n\ndocs\nOverview of DIYbiosphere\n\/docs\/introduction\/overview\/\n\n\n\n\n\n\n\n\ud83c\udf89 Welcome to the DIYbiophere repository \ud83c\udf89\nWe really appreciate your interest in our project and we would \u2764\ufe0f your contributions!\nAbout\nThe DIYbiosphere is a open-source project to connect Do-It-Yourself Biology (DIYbio) initiatives from all over the world. The goal is to have a shared and common platform that can connect people and ideas in all its possibilities and encourage the DIYbio community to work on a project together.\nHow it works\nThe platform functions similar to a wiki but uses GitHub Pages instead; hosting the raw files at https:\/\/github.com\/DIYbiosphere\/sphere and rendering webpages at http:\/\/sphere.diybio.org\nEach DIYbio initiative has its own entry which are organized into eight collections: projects, startups, labs, incubators, groups, networks, events, and others. An entry is added by creating its own folder in its respective collection, and adding a text file in markdown syntax with a YAML front matter, which is then rendered into its webpage. For example, the file _projects\/MyDIYbioProject\/MyDIYbioProject.md could look like this:\n---\n# This is the front matter in YAML; between two lines of three consecutive dashes (---)\ntitle: My DIYbio Project\nstart-date: 2000\ntype-org: non-profit\nwebsite: http:\/\/my-diybio-project.io\ntags:\n  - open hardware\n  - citizen science\n---\n# This is the text in Markdown syntax; after the front matter\n\nMy DIYbio project is about **open hardware** and **citizen science**.\n\nThe front matter includes several key: value pairs that render into different elements in the webpage. The Avocado Lab is an example entry for pedagogical purposes. You can check out the raw file raw file rendered into this webpage. See the rendered image below, and by its side the different elements of the page labeled.\n{:.ui.fluid.image}\nContribute\nTo contribute, you need a GitHub account (sign up).\nYou also need to abide to our Code of Conduct (COC) and consent to our Contributor Terms (CT) determined by our Terms of Use (aka Copyrights)).\n\nTL;DR (Too Long; Didn't Read)\n\nCOC: Be kind and respectful. Gross, rude or inappropriate behavior will not be tolerated. Confront wrongdoers directly or report them to the board of directors.\nCT: You freely share your contributions to the repository under the MIT license. If your contributions are displayed in the website, you freely waive authorship rights of these contributions (public domain; CC0), otherwise you will specify their copyright. You will also note contributions on behalf of third parties and specify their copyright.\n\n\nIn increasing order of engagement you can contribute to DIYbiosphere by:\n\nSHARING THE LOVE\n\nShare this project with your friends and followers! They might be interested in using the project to find DIYbio initiatives or adding their own. You don't need a GitHub account for this!\n\u2b50 Star the project on GitHub! Starring helps attract potential contributors, especially expert and master developers!\n\n\nWRITING ENTRIES\n\nAdd a new entry:  whether it's your initiative or someone else's\nEdit an existing entry: misspellings, outdated information, or just inaccurate, help us keep the entries error-free and up-to-date!\n\n\nPARTICIPATING IN THE ISSUES\n\nComment, answer, and vote: search our issues and see if you can help out by moving our issues along:\nSubmit a new issue: report a bug, ask a question, share your idea and wait for feedback and support from the community.\nFork, commit, pull request your contributions! Tackle a good first issue to get you started\n\n\nGETTING INVOLVED\n\nJoin the development community. The project is managed by members of the DIYbiosphere community. Request membership by submitting an issue enjoy more access privileges to the project!\nJoin the conversation. You can freely join the Gitter chatroom at gitter.im\/diybiosphere\/sphere, or in Slack at diybiosphere.slack.com\n\n\n\nCopyright\nIn short: the work in DIYbiosphere is freely available to use, modify and distribute. More specifically:\n\nFiles in the Repository are available under the MIT License\nContent in the Website is shared under the public domain by CC0 License\n\nCredit our work as \u201c\u00a9 DIYbiosphere contributors\u201d or \u201c\u00a9 DIYbiosphere\u201d with a link to the Repository at: https:\/\/github.com\/DIYbiosphere\/sphere, or the Website at: http:\/\/sphere.diybio.org\nYou can review our Terms of Use for a human-readable version of the copyrights, and our Contributor Terms to understand in legal terms the rights granted and\/or waived from your Contributions. For further detail you should read in full both MIT and CC0 licenses.\n","44":"\n\n\nSynBioHub is a Web-based repository for synthetic biology, enabling users to browse, upload, and share synthetic biology designs.\nTo learn more about the SynBioHub, including installation instructions and documentation, visit the SynBioHub wiki.\nTo access a sample instance of SynBioHub containing enriched Bacillus subtilis data, features from the Escherichia coli genome, and the complete iGEM Registry of Standard Biological Parts, visit synbiohub.org. To access a bleeding-edge version of SynBioHub, visit dev.synbiohub.org.\nInstallation\nThe recommended way to install SynBioHub is via the Docker image.  See Installation for more information.\nManual Installation\nSynBioHub has both JavaScript (node.js) and Java components.\nPrequisites:\nLinux (only tested with Ubuntu 18.04.01) or macOS\n\nIf you're using macOS, first install homebrew\n\nA JDK\n\n\n\nOS\nCommand\n\n\n\n\nUbuntu\napt install default-jdk\n\n\nMac\nbrew install openjdk\n\n\n\nApache Maven\n\n\n\nOS\nCommand\n\n\n\n\nUbuntu\napt install maven\n\n\nMac\nbrew install maven\n\n\n\nnode.js >= 11.0.0\n\n\n\nOS\nCommand\/Link\n\n\n\n\nUbuntu\nvisit https:\/\/nodejs.org\/en\/\n\n\nMac\nbrew install node\n\n\n\nOpenLink Virtuoso 7.x.x\n\n\n\nOS\nCommand\/Link\n\n\n\n\nUbuntu\nvisit https:\/\/github.com\/openlink\/virtuoso-opensource\n\n\nMac\nbrew install virtuoso\n\n\n\nrapper\n\n\n\nOS\nCommand\n\n\n\n\nUbuntu\napt install raptor2-utils\n\n\nMac\nbrew install raptor\n\n\n\njq\n\n\n\nOS\nCommand\n\n\n\n\nUbuntu\napt install jq\n\n\nMac\nbrew install jq\n\n\n\nUbuntu 18.04.01\n\nInstall Virtuoso 7 from source at\nhttps:\/\/github.com\/openlink\/virtuoso-opensource\n\n\nSwitch to the branch stable\/7 before installing.\nFollow the README on installing virtuoso from source. This involves installing all the dependencies and running build commands.\nCurrently, Virtuoso does not support versions of OpenSSL 1.1.0 and above, or versions of OpenSSL below 1.0.0. When installing the dependency, build from a binary between those versions from https:\/\/www.openssl.org\/source\/.\n\n\nSet up the Node.js repository\n\nDownload the Node setup script curl -sL https:\/\/deb.nodesource.com\/setup_6.x | sudo -E bash -\nUpdate your package repositories apt update\n\n\nInstall the necessary packages apt install default-jdk maven raptor2-utils nodejs jq build-essential python\nStart virtuoso process virtuoso-t +configfile \/usr\/local\/virtuoso-opensource\/var\/lib\/virtuoso\/db\/virtuoso.ini -f\n\nMacOS\n\nInstall the necessary packages brew install openjdk maven node virtuoso raptor jq python\nStart virtuoso process\n\ncd \/usr\/local\/Cellar\/virtuoso\/7.2.5.1_1\/var\/lib\/virtuoso\/db\n\nThe command above is based on where the virtuoso.ini file is located. Your installation might be located\nsomewhere different than \/usr\/local\/Cellar\/virtuoso\/7.2.5.1_1\/var\/lib\/virtuoso\/db, or the version might be\ndifferent (7.2.5.1_1 might be 7.3.6.1_1 or any other version number).\nIf you're having trouble finding the location of the virtuoso.ini file, run sudo find \/ -name virtuoso.ini.\nPress the control and c keys simultaneously to quit the search.\n\n\nvirtuoso-t -f\n\n\n\nBoth Systems\n\nClone the SynBioHub repository git clone https:\/\/github.com\/SynBioHub\/synbiohub\nChange to the SynBioHub directory cd synbiohub\nBuild the Java components with Maven cd java && mvn package\nReturn to the root directory and install the Node dependencies with yarn cd ..\/ && yarn install\nMake sure that yarn is being used, not 'cmdtest'.\nInstall nodemon and forever with npm install nodemon -g && npm install forever -g\nAdd SPARQL update rights to the dba user in virtuoso.\n\n\nVisit localhost:8890, click conductor on the left hand side, and login with user name dba and password dba.\nVisit system admin -> user accounts in the menu at the top.\nFind the accound labled dba and edit.Add SPARQL_UPDATE to roles using the menu at the bottom.\nIf no dba account exists, add one, then add update rights.\n\n\nStart the SynBioHub process npm start or npm run-script dev\n\nPublishing\nThe repository is set up to prohibit commits directly to the master branch.\nCommits must be made in another branch, and then a GitHub PR used to merge them into master.\nGitHub PRs must be approved by at least one other developer before they can be merged into master.\nAdditionally, they must pass Travis checks, which build a Docker image and run the SBOLTestSuite and SynBioHub integration tests against it.\nEach time a PR is merged into master, the Travis checks are re-run on the master branch, and if they succeed the resulting image is pushed by Travis to DockerHub under the tag snapshot-standalone.\nPublishing a release\nReleases are published automatically using GitHub Actions.\nThere is an action which fires on release publication.\nIt publishes an image to Docker Hub under the $VERSION-standalone tag, and updates the synbiohub-docker master branch to point to this version.\nMore information available here.\n","45":"\n\n\n\nClustergrammer is a web-based tool for visualizing high-dimensional data (e.g. a matrix) as an interactive and shareable hierarchically clustered heatmap. Clustergrammer's front end (Clustergrammer-JS) is built using D3.js and its back-end (Clustergrammer-PY) is built using Python. Clustergrammer produces highly interactive visualizations that enable intuitive exploration of high-dimensional data and has several biology-specific features (e.g. enrichment analysis, see Biology-Specific Features) to facilitate the exploration of gene-level biological data. Click the screenshot below to view an interactive tutorial:\n\nClustergrammer's interacive features include:\n\nZooming and Panning\nRow and Column Reordering\nInteractive Dendrogram\nInteractive Dimensionality Reduction\nInteractive Categories\nCropping\nRow Searching\nBiology-Specific Features\n\nClustergrammer can be used in three main ways (this repo contains the source code for Clustergrammer-JS):\n\nClustergrammer Web App (http:\/\/amp.pharm.mssm.edu\/clustergrammer\/)\nClustergrammer Jupyter Widget\nClustergrammer-JS and Clustergrammer-PY libraries\n\nFor information about building a webpage or app using Clustergrammer see: Web-Development with Clustergrammer\nWhat's New\nClustergrammer2\n \n\nClustergrammer is being re-built using the WebGL library regl. The new in-development front-end is Clustergrammer-GL and the new in-development Jupyter widget is Clustergrammer2. The above notebook shows how Clustergrammer2 can be used to load a small dataset and visualize a large random DataFrame. By running the notebook on MyBinder using Jupyter Lab it can also be used to visualize a user uploaded dataset. Please see the video tutorial above for more information.\nFor additional examples and tutorials please see:\n\nCase Studies and Tutorials\nClustergrammer2-Notebooks GitHub repository\n\nJupyterCon 2018 Presentation\n\nClustergrammer was recently presented at JupyterCon 2018 (see slides).\nUsing Clustergrammer\nPleae see Clustergramer's documentation for detailed information or select a specific topic below:\n\nGetting Started\nInteracting with the Visualization\nWeb-Development with Clustergrammer (example pages)\nClustergrammer Web App and Clustergrammer Jupyter Widget\nMatrix Formats and Input\/Output\nCore libraries: Clustergrammer-JS and Clustergrammer-PY\nApp Integration Examples\nCase Studies and Examples\nBiology-Specific Features\nDeveloping Clustergrammer\n\nCiting Clustergrammer\nPlease consider supporting Clustergrammer by citing our publication:\nFernandez, N. F. et al. Clustergrammer, a web-based heatmap visualization and analysis tool for high-dimensional biological data. Sci. Data 4:170151 doi: 10.1038\/sdata.2017.151 (2017).\nLicensing\nClustergrammer was developed by the Ma'ayan lab at the Icahn School of Medicine at Mount Sinai for the BD2K-LINCS DCIC and the KMC-IDG. Clustergrammer's license and third-party licenses are in the LICENSES directory and more information can be found at Clustergrammer License.\nPlease contact us for support, licensing questions, comments, and suggestions.\n","46":"Boolean Network Simulator\nThis software provides a browseable user interface,\nwhich allows the user to load Boolean networks in various formats,\ndisplay them and interactively simulate them.\nIt's coded in JavaScript, so it should work platform-independent.\nA recent version of a HTML5-capable internet browser is required for proper rendering,\nChromium recommended (http:\/\/www.chromium.org\/getting-involved\/download-chromium).\nSoftware license: GNU Affero GPL v3\nDownload using\ngit clone https:\/\/github.com\/matthiasbock\/BooleSim.git\n\nAfter downloading, open or drag'n'drop index.html into your web browser.\nClick here for help: https:\/\/github.com\/matthiasbock\/BooleSim\/wiki\nBuilt upon previous work located on Google Code at http:\/\/biographer.googlecode.com\/\nFor questions, wishes or bug reports please visit https:\/\/github.com\/matthiasbock\/BooleSim\/issues\nHave fun!\n","47":"The Noctua Stack\nThe Noctua Stack is a curation platform developped by the Gene Ontology Consortium. The stack is composed of:\n\nMinerva: the backend data server to retrieve, store, update and delete annotations.\nBarista: an authentication layer controling and formating all communications from\/to Minerva.\nNoctua: the website to browse the annotations in production and development and provide an editorial platform to produce Gene Ontology Causal Activity Models (or GO-CAMs) using either the simple UI Noctua Form or the more advanced Graph Editor.\n\nThe biological knowledge are stored in RDF\/OWL using the blazegraph triplestore implementation.\nIn effect, any piece of knowledge stored in RDF\/OWL is a triple { subject, predicate, object } defining a relationship (or association) between a subject and an object. Those triples are also commonly stored in Turtle files.\nInstallation\nPre-requisite\nYou must have npm installed. On ubuntu\/debian, simply type:\nsudo apt-get install nodejs\n\nOn OSX, it is also possible to install npm either from nodejs.org or using brew:\nbrew install node\n\nSteps for a local Installation\n# The full Noctua stack is a multi-repositorie project; optionally create a main directory for the stack to contain all the repositories.\n# These instruction assume that \"gulp\" is in your path; if local-only, use: `.\/node_modules\/.bin\/gulp`.\n\n# Creating a local directory for our work.\nmkdir noctua-stack && cd noctua-stack\n\n# Repo containing metadata (users, groups, etc.).\ngit clone https:\/\/github.com\/geneontology\/go-site.git\n# The data repo to start the store and save to.\ngit clone https:\/\/github.com\/geneontology\/noctua-models.git\n# Repo for the backend server.\ngit clone https:\/\/github.com\/geneontology\/minerva.git\n# Repo for the Noctua client and middleware (Barista).\ngit clone https:\/\/github.com\/geneontology\/noctua.git\n\n# Build the Minerva server (and CLI).\ncd minerva && sh .\/build-cli.sh && cd ..\n\n# Create default authentication users with your favorite editor.\nmkdir barista\nvim barista\/local.yaml\n-\n uri: 'http:\/\/orcid.org\/XXXX-XXXX-XXXX-XXXX'\n username: my_username\n password: my_password\n\n# Install Noctua Form (old \"simple-annoton-editor\")\ngit clone https:\/\/github.com\/geneontology\/noctua-form.git\ngit clone https:\/\/github.com\/geneontology\/noctua-landing-page.git\n\n# Install Noctua as an all-local installation.\ncd noctua\nnpm install\ncp config\/startup.yaml.stack-dev .\/startup.yaml\n\n# Edit configuration file (barista, user, group, noctua models location, minerva memory to at least 16GB, link to NoctuaForm \/ SAE)\nvim startup.yaml\n\n# Build the stack and Blazegraph Journal (triplestore)\n.\/node_modules\/.bin\/gulp build\n# If running first time.\n.\/node_modules\/.bin\/gulp batch-minerva-destroy-journal\n.\/node_modules\/.bin\/gulp batch-minerva-destroy-ontology-journal\n.\/node_modules\/.bin\/gulp batch-minerva-create-journal\n\n# Then launch the stack, waiting for each to successfully start up:\n.\/node_modules\/.bin\/gulp run-minerva &> minerva.log &\n.\/node_modules\/.bin\/gulp run-barista &> barista.log &\n.\/node_modules\/.bin\/gulp run-noctua &> noctua.log &\n\nAdditional notes\nGulp Tasks\n\ndoc - build the docs, available in doc\/\ntest - need more here\nbuild - assemble the apps for running\nwatch - development file monitor\nclean - clean out \/doc and \/deploy\n\nIn addition, the last 3 lines of the installation steps launch all the 3 layers of the Noctua Stack:\ngulp run-barista &> barista.log &\ngulp run-minerva &> minerva.log &\ngulp run-noctua &> noctua.log &\n\nAnd Gulp can be used to both destroy and create blazegraph journals (triplestore):\ngulp batch-minerva-destroy-journal\ngulp batch-minerva-destroy-ontology-journal\ngulp batch-minerva-create-journal\n\nUsers & groups\nBarista, the authentication layer needs two files to run: users.yaml and groups.yaml.\nThese files defined who is authorized to log in to the Noctua Stack to perform biological curations.\n\nTo know more about curation with the Noctua Stack, visit our wiki.\nTo request an account to curate with the Noctua Stack, contact us\n\nLibraries and CLI to communicate with the Noctua Stack\nbbop-manager-minerva\nThis is the high-level API with OWL formatted requests (e.g. add individual, add fact or evidence using class expressions).\nhttps:\/\/github.com\/berkeleybop\/bbop-manager-minerva\nminerva-requests\nThis is the request object used to format specific queries to Minerva. It is composed of a basic request object as well as a request_set designed to chain multiple request objects and speed up complex tasks.\nhttps:\/\/github.com\/berkeleybop\/minerva-requests\nSome useful details about the API are described here\nCLI (REPL)\nThe Noctua REPL is a recommended step for anyone trying to learn the syntax and how to build requests to Minerva in the Noctua Stack.\nAs any REPL, it allows for the rapid testing of multiple commands and to check the responses from barista.\nThis project can be considered as a basic prototype for any other client wanting to interact with the stack.\nhttps:\/\/github.com\/geneontology\/noctua-repl\nKnown issues\nThe bulk of major issues and feature requests are handled by the\ntracker (https:\/\/github.com\/geneontology\/noctua\/issues). If something is\nnot mentioned here or in the tracker, please contact Seth Carbon or Chris Mungall.\n\nSometimes, when moving instance or relations near a boundary, the\nrelations will fall out of sync; either move nearby instances or\nrefresh the model\nSometimes, when editing an instance, the relations (edges) will\nfall out of sync; either move nearby instances or refresh the\nmodel\nThe endpoint scheme is reversed between creation and instantiation\nTODO, etc.\n\n","48":"\nBSD-licensed implementation of the Synthetic Biology Open Language (SBOL) in JavaScript.\nRequires a JavaScript environment with ES6 class support (e.g. recent versions of node, Chrome, ...)\nFeatures:\n\nRead generic RDF, XML\nSerialize SBOL XML, JSON\nBuild SBOL documents programatically\n\nInstallation\nnpm install sboljs\n\nUsage\nvar SBOLDocument = require('sboljs')\n\nSBOLDocument.loadRDFFile('foo.xml', function(err, doc) {\n\n    doc.componentDefinitions.forEach(function(componentDefinition) {\n\n        console.log(componentDefinition.name)\n\n    })\n\n})\n\nDocumentation\n","49":"PYDFS-LINEUP-OPTIMIZER \npydfs-lineup-optimizer is a tool for creating optimal lineups for daily fantasy sport.\nInstallation\nTo install pydfs-lineup-optimizer, simply run:\n$ pip install pydfs-lineup-optimizer\n\nSupport\nNow it supports following dfs sites:\n\n\n\nLeague\nDraftKings\nFanDuel\nFantasyDraft\nYahoo\nFanBall\nDraftKings Captain Mode\nFanDuel Single Game\nDraftKings Tiers\n\n\n\n\nNFL\n+\n+\n+\n+\n+\n+\n+\n-\n\n\nNBA\n+\n+\n+\n+\n-\n+\n+\n+\n\n\nNHL\n+\n+\n+\n+\n-\n-\n-\n+\n\n\nMLB\n+\n+\n+\n+\n-\n+\n-\n+\n\n\nWNBA\n+\n+\n-\n-\n-\n+\n-\n-\n\n\nGolf\n+\n+\n+\n+\n-\n-\n-\n-\n\n\nSoccer\n+\n-\n-\n+\n-\n+\n-\n-\n\n\nCFL\n+\n-\n-\n-\n-\n-\n-\n-\n\n\nLOL\n-\n+\n-\n-\n-\n+\n+\n-\n\n\nMMA\n+\n+\n-\n-\n-\n-\n-\n-\n\n\nNASCAR\n+\n+\n-\n-\n-\n-\n-\n-\n\n\nTennis\n+\n-\n-\n-\n-\n-\n-\n-\n\n\nCSGO\n+\n-\n-\n-\n-\n-\n-\n-\n\n\n\nDocumentation\nDocumentation is available at https:\/\/pydfs-lineup-optimizer.readthedocs.io\/en\/latest\nExample\nHere is an example for evaluating optimal lineup for Yahoo fantasy NBA. It loads players list from \"yahoo-NBA.csv\" and select 10 best lineups.\nfrom pydfs_lineup_optimizer import Site, Sport, get_optimizer\n\n\noptimizer = get_optimizer(Site.YAHOO, Sport.BASKETBALL)\noptimizer.load_players_from_csv(\"yahoo-NBA.csv\")\nfor lineup in optimizer.optimize(10):\n    print(lineup)\n","50":"Sportsreference: A free sports API written for python\n\n\n\n\n\n\nContents\n\nInstallation\nExamples\nGet instances of all NHL teams for the 2018 season\nPrint every NBA team's name and abbreviation\nGet a specific NFL team's season information\nPrint the date of every game for a NCAA Men's Basketball team\nPrint the number of interceptions by the away team in a NCAA Football game\nGet a Pandas DataFrame of all stats for a MLB game\nFind the number of goals a football team has scored\n\n\nDocumentation\nTesting\n\n\nSportsreference is a free python API that pulls the stats from\nwww.sports-reference.com and allows them to be easily be used in python-based\napplications, especially ones involving data analytics and machine learning.\nSportsreference exposes a plethora of sports information from major sports\nleagues in North America, such as the MLB, NBA, College Football and Basketball,\nNFL, and NHL. Sportsreference also now supports Professional Football (or\nSoccer) for thousands of teams from leagues around the world. Every sport has\nits own set of valid API queries ranging from the list of teams in a league, to\nthe date and time of a game, to the total number of wins a team has secured\nduring the season, and many, many more metrics that paint a more detailed\npicture of how a team has performed during a game or throughout a season.\n\nInstallation\nThe easiest way to install sportsreference is by downloading the latest\nreleased binary from PyPI using PIP. For instructions on installing PIP, visit\nPyPA.io for detailed steps on\ninstalling the package manager for your local environment.\nNext, run:\npip install sportsreference\n\nto download and install the latest official release of sportsreference on\nyour machine. You now have the latest stable version of sportsreference\ninstalled and can begin using it following the examples below!\nIf the bleeding-edge version of sportsreference is desired, clone this\nrepository using git and install all of the package requirements with PIP:\ngit clone https:\/\/github.com\/roclark\/sportsreference\ncd sportsreference\npip install -r requirements.txt\n\nOnce complete, create a Python wheel for your default version of Python by\nrunning the following command:\npython setup.py sdist bdist_wheel\n\nThis will create a .whl file in the dist directory which can be installed\nwith the following command:\npip install dist\/*.whl\n\n\nExamples\nThe following are a few examples showcasing how easy it can be to collect\nan abundance of metrics and information from all of the tracked leagues. The\nexamples below are only a miniscule subset of the total number of statistics\nthat can be pulled using sportsreference. Visit the documentation on\nRead The Docs for a\ncomplete list of all information exposed by the API.\n\nGet instances of all NHL teams for the 2018 season\nfrom sportsreference.nhl.teams import Teams\n\nteams = Teams(2018)\n\nPrint every NBA team's name and abbreviation\nfrom sportsreference.nba.teams import Teams\n\nteams = Teams()\nfor team in teams:\n    print(team.name, team.abbreviation)\n\nGet a specific NFL team's season information\nfrom sportsreference.nfl.teams import Teams\n\nteams = Teams()\nlions = teams('DET')\n\nPrint the date of every game for a NCAA Men's Basketball team\nfrom sportsreference.ncaab.schedule import Schedule\n\npurdue_schedule = Schedule('purdue')\nfor game in purdue_schedule:\n    print(game.date)\n\nPrint the number of interceptions by the away team in a NCAA Football game\nfrom sportsreference.ncaaf.boxscore import Boxscore\n\nchampionship_game = Boxscore('2018-01-08-georgia')\nprint(championship_game.away_interceptions)\n\nGet a Pandas DataFrame of all stats for a MLB game\nfrom sportsreference.mlb.boxscore import Boxscore\n\ngame = Boxscore('BOS201806070')\ndf = game.dataframe\n\nFind the number of goals a football team has scored\nfrom sportsreference.fb.team import Team\n\ntottenham = Team('Tottenham Hotspur')\nprint(tottenham.goals_scored)\n\nDocumentation\nTwo blog posts detailing the creation and basic usage of sportsreference can\nbe found on The Medium at the following links:\n\nPart 1: Creating a public sports API\nPart 2: Pull any sports metric in 10 lines of Python\n\nThe second post in particular is a great guide for getting started with\nsportsreference and is highly recommended for anyone who is new to the\npackage.\nComplete documentation is hosted on\nreadthedocs.org. Refer to\nthe documentation for a full list of all metrics and information exposed by\nsportsreference. The documentation is auto-generated using Sphinx based on the\ndocstrings in the sportsreference package.\n\nTesting\nSportsreference contains a testing suite which aims to test all major portions\nof code for proper functionality. To run the test suite against your\nenvironment, ensure all of the requirements are installed by running:\npip install -r requirements.txt\n\nNext, start the tests by running py.test while optionally including coverage\nflags which identify the amount of production code covered by the testing\nframework:\npy.test --cov=sportsreference --cov-report term-missing tests\/\n\nIf the tests were successful, it will return a green line will show a message at\nthe end of the output similar to the following:\n======================= 380 passed in 245.56 seconds =======================\n\nIf a test failed, it will show the number of failed and what went wrong within\nthe test output. If that's the case, ensure you have the latest version of code\nand are in a supported environment. Otherwise, create an issue on GitHub to\nattempt to get the issue resolved.\n","51":"SportsBook\nA sports data scraping and analysis tool\nThis project is in its very early days and currently only supports football (soccer).\nFearure list:\n\n\nImport leagues and fixtures from one of two available online sources.\n\n\nSee a visual comparison of the teams playing in any game.\n\n\nRun a benchmark comparison on a selected fixture.\n\nThis allows you to see the results of games where both teams have played the same opponent.\nIt shows results where both teams faced the same opponent at home, then away and then where\nthe home team have faced the opponent at home and the away team have faced the opponent away.\nOnly games where three of the above comparisons are shown. If the home team, at home, haven't\nplayed the same team as the awyay team have played away on three occasions, the game is ommitted\nfrom predictions.\n\n\n\nRun manual comparison of two teams from any loaded leagues generating (somewhat inaccurate) predictions.\n\n\nRun predictions on all loaded fixtures (more accurate as the teams are guaranteed to be from the same leagues).\n\nSelect from using all available league data to compare home and away goal scoring and conceding form or only using data from games\nwhere the home team has played the same team at home as the away team as played away, making the data a fairer representation of the\nteam's capabilities.\n\n\n\nFilter predictions to show games where specific requirements are met (eg. prediction of home side winning by 2 goals).\n\n\nFilter filtered predictions further with other filters.\n\n\nFilter predictions using special filters (either produced by guest contributors or specially designed filters for specific bet types).\n\n\nDisplay filtered predictions and all predictions on screen.\n\n\nChange the range of dates or games the predictions cover.\n\n\nDisplay results from throughout the whole current season of each league.\n\n\nChange the range of dates the results cover.\n\n\nProduce a spreadsheet of all predictions or filtered predictions with a wealth of current stats for each team in each prediction.\n\n\nExport currently loaded league data to a JSON file.\n\n\nImport the league data from a JSON file.\n\n\nThe project is growing fairly quickly. I'd love to hear what your thoughts are and even keep you up to date with new features if you like. Join the Slack group here if you're interested:\nhttps:\/\/join.slack.com\/t\/sportsbookgroup\/shared_invite\/enQtNDc4MjYwNzMwNzg4LTAzMDk0MDM3OWFiMGJhZWU2MzAyMzQyNGI4OTlhNjgxMWRlNTZjOTAzMTM3ODdhMDIxNDU3YjI2MzM4OTlmZjg\nIt'd be great to hear from you so please pop in and say hi!\n","52":"Sports Tracker Liberator\nUnder a catchy name lies an implementation which uses\/implements Endomondo Mobile Api.\nStatus\nCurrently only Endomondo Api is somewhat implemented. Retrieving stuff somewhat works, and submitting new workouts works as in most basic form. All social media crap is ignored.\nUsage\nAuthentication\nEndomondo implements basic token mechanism, except that earlier versions didn't do that correctly and was only protected by can't be arsed with it -securitysystem. Later App versions tried to fix this, while maintaining backward compatibility by implementing a second, secureToken param. This is mainly used for social media crap, so we'll conveniently ignore it.\nTo authenticate, one needs existing email and password:\nfrom endomondo import MobileApi\nendomondo = MobileApi(email='email@example.com', password='p4ssw0rd')\n\nauth_token = endomondo.get_auth_token()\n\nThis will return an auth_token, which can be stored on keychain or similar. In future, it should be used to skip whole login juggalloo.\nendomondo = MobileApi(auth_token=auth_token)\n\nRetrieving workouts:\nTo retrieve latest workouts:\nendomondo.get_workouts()\n\nEndomondo Mobile Api provides some oddities, and one of them is that maxResults actually work! And as usually in REST APIs, before date can be defined:\nworkouts = endomondo.get_workouts(maxResults=2)\nendomondo.get_workouts(before=workouts[-1].start_time)\n\nStructure for workouts page is similar to other providers, except paging links are missing.\nRetrieving single workout\nMobileApi.get_workout() accepts either existing workout, or workout ID.\nworkout = endomondo.get_workout(workoutId='234246')\n[...]\nreload = endomondo.get_workout(workout)\n\nFor workouts, or workout history, you can pass fields param to define, which attributes you are interested at. Again, a bit suprisingly, this works.\nsocial_workout = endomondo.get_workout(fields=['lcp_count'])\n\nThease attributes aren't documented anywhere, but most of known can be found \u00edn MobileApi.get_workout(), and they follow somewhat logical naming conventions.\nCreating a workout and a track.\nSome, if not most of program logic lies in mobile app, and server just stores data. This means that one needs to calculate most of their data by themselves.\nCurrently only most basic workout creation is supported. Later if\/when workout update is added, more features becomes available. This is somewhat related to implementation of Endomondo Mobile Api, as only sport type, calories, hydration, duration and distance in form of track point is created. After initial creation, rest of data is updated to it.\nTo create workout:\nfrom endomondo import MobileApi, Workout, TrackPoint\nfrom datetime import datetime\n\nendomondo = MobileApi(auth_token='1234')\n\nworkout = Workout()\n\nworkout.start_time = datetime.utcnow()\n\n# See ``workout.sports``\nworkout.sport = 0\n\n# Units are in user' local units, and only Metric is supported.\n# `distance` is in km.\n# Note that at creation time, this is not required. It's only used for automatic track point generation.\nworkout.distance = 1.5\n\n# Duration in seconds\nworkout.duration = 600\n\nendomondo.post_workout(workout=workout, properties={'audioMessage': 'false'})\n\nif workout.id:\n\tprint \"Saved!\"\n\nAltought track points aren't technically required by Endomondo backend, which is likely just for a sake of backwards compatibility, and in practice you always want to have at least one TrackPoint in your Workout. MobileApi.post_workout() can automate this, and will create one if none exists.\nOther\nFor other, tested, functionality see main.py\nDisclaimer, legalese and everything else.\nThis is not affiliated or endorset by Endomondo, or any other party. If you are copying this for a commercial project, be aware that it might be so that clean room implementation rules aren't fully complied with.\n","53":"NBA Player Movements\nThis is a script for visualization of NBA games from raw SportVU logs.\nIf you admire both Spurs' and Warriors' ball movement, Brad Stevens' playbook, or just miss KD in OKC you'll find this entertaining.\nExamples\n\n\n\n\nUsage\n\nClone this repo:\n\n$ git clone https:\/\/github.com\/linouk23\/NBA-Player-Movements\n\n\nChoose any NBA game from data\/2016.NBA.Raw.SportVU.Game.Logs directory.\n\n\nGenerate an animation for the play by running the following script:\n\n\n$ python3 main.py --path=Celtics@Lakers.json --event=140\nrequired arguments:\n  --path PATH    a path to json file to read the events from\n\noptional arguments:\n  --event EVENT  an index of the event to create the animation to\n                 (the indexing start with zero, if you index goes beyond out\n                 the total number of events (plays), it will show you the last\n                 one of the game)\n  -h, --help     show the help message and exit\n\n","54":"SportScanner\n\nThis project is no longer actively maintained. Please submit PRs and I will review but support is patchy at best! Thanks for watching!\nScanner and Metadata Agent for Plex that uses www.thesportsdb.com\n#Installation\nPlex main folder location:\n* '%LOCALAPPDATA%\\Plex Media Server\\'                                        # Windows Vista\/7\/8\n* '%USERPROFILE%\\Local Settings\\Application Data\\Plex Media Server\\'         # Windows XP, 2003, Home Server\n* '$HOME\/Library\/Application Support\/Plex Media Server\/'                     # Mac OS\n* '$PLEX_HOME\/Library\/Application Support\/Plex Media Server\/',               # Linux\n* '\/var\/lib\/plexmediaserver\/Library\/Application Support\/Plex Media Server\/', # Debian,Fedora,CentOS,Ubuntu\n* '\/usr\/local\/plexdata\/Plex Media Server\/',                                  # FreeBSD\n* '\/usr\/pbi\/plexmediaserver-amd64\/plexdata\/Plex Media Server\/',              # FreeNAS\n* '${JAIL_ROOT}\/var\/db\/plexdata\/Plex Media Server\/',                         # FreeNAS\n* '\/c\/.plex\/Library\/Application Support\/Plex Media Server\/',                 # ReadyNAS\n* '\/share\/MD0_DATA\/.qpkg\/PlexMediaServer\/Library\/Plex Media Server\/',        # QNAP\n* '\/volume1\/Plex\/Library\/Application Support\/Plex Media Server\/',            # Synology, Asustor\n* '\/raid0\/data\/module\/Plex\/sys\/Plex Media Server\/',                          # Thecus\n* '\/raid0\/data\/PLEX_CONFIG\/Plex Media Server\/'                               # Thecus Plex community\n\n\nDownload the latest release from https:\/\/github.com\/mmmmmtasty\/SportScanner\/releases\nExtract files\nCopy the extracted directory \"Scanners\" into your Plex main folder location - check the list above for more clues\nCopy the extracted directory \"SportScanner.bundle\" into the Plug-ins directory in your main folder location - check the list above for more clues\nYou may need to restart Plex\nCreate a new library and under Advanced options you should be able to select \"SportScanner\" as both your scanner and metadata agent.\n\n#Media Format\nThe SportScanner scanner requires one of two folder structures to work correctly, the first of which matches Plex's standard folder structure.\n##RECOMMENDED METHOD\nFollow the Plex standards for folder structure - TV Show\\Season<files>. For SportScanner, TV Shows = League Name. For example for 2015\/2016 NHL you would do something like the following:\n\n~LibraryRoot\/NHL\/Season 1516\/NHL.2015.09.25.New-York-Islanders.vs.Philadelphia-Flyers.720p.HDTV.60fps.x264-Reborn4HD_h.mp4\n\nIn this scenario you still need all the information in the file name, I aim to remove that requirement down the line. The only information that comes only from the folder structure is the season.\n##Alternative naming standard\nYou can also choose to ignore the season directory and have the scanner work it out with a folder structure like so:\n\n~LibraryRoot\/Ice Hockey\/NHL\/NHL.2015.09.25.New-York-Islanders.vs.Philadelphia-Flyers.720p.HDTV.60fps.x264-Reborn4HD_h.mp4\n\nTHERE IS A DOWN SIDE TO THIS! For this to work you must include a file in each league directory called \"SportScanner.txt\" that contains information about how the seasons work for this sport. The first line in the file will always be \"XXXX\" or \"XXYY\". \"XXXX\" means that the seasons happens within one calendar year and will therefore be named \"2015\" of \"1999\" for example. \"XXYY\" means that a season occurs across two seasons and will take the format \"1516\" or \"9899\" for example. When you define the season as \"XXYY\" you MUST then on the next line write the integer values of a month and a day in the form \"month,day\". This should be a a month and a day somewhere in the off-season for that sport. This tells the scanner when one season has finished and the next one is beginning to ensure that it puts files in the correct season based off the date the event happened. As an example, if you are trying to add NHL you would create a file at the following path:\n\n~LibraryRoot\/Ice Hockey\/NHL\/SportScanner.txt\n\nIn this instance the contents of this file would be as follows, saying that seasons should be in \"XXYY\" format and a date in the middle of the off-season is 1st July:\nXXYY\n7,1\nNOT RECOMMENDED (but works for now)\nSportScanner does not actually pay attention to the name of the League directory when it comes to matching events - all info has to be in the filename. This means that you can still group all sports together and as long as they share a season format you can create a SportScanner.txt file as outlined above and everything will work.\nThis is rubbish, it kind of accidentally works, I don't recommend it as I will cut it out as part of improvement works in future.\n#Known Issues\n\nNo posters for seasons\nCan only handle individual files, not multipart or those in folders\nAll information must be in the filename regardless of the directory structure.\n\n","55":"Sports Betting with RL\nOverview\nThis is the code for this video on Youtube by Siraj Raval on Sports Betting using Reinforcement Learning. This is apart of the Move 37 course at the School of AI.\nDependencies\nNone.\nUsage\nType python value_iteration.py into terminal and it will run.\nHistory\nThis is an adapted version of the \"Gambler's Problem\" that I've applied to sports betting. Details below\n-The Gambler Problem as discussed in Example 4.3 in Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.\n-The problem from the book is described below:\nGambler\u2019s Problem: A gambler has the opportunity to make bets\non the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as\nmany dollars as he has staked on that flip; if it is tails, he loses his stake. The game\nends when the gambler wins by reaching his goal of $100, or loses by running out of\nmoney. On each flip, the gambler must decide what portion of his capital to stake,\nin integer numbers of dollars. This problem can be formulated as an undiscounted,\nepisodic, finite MDP. The state is the gambler\u2019s capital, s \u2208 {1, 2, . . . , 99} and the\nactions are stakes, a \u2208 {0, 1, . . . , min(s, 100\u2212s)}. The reward is zero on all transitions\nexcept those on which the gambler reaches his goal, when it is +1. The state-value\nfunction then gives the probability of winning from each state. A policy is a mapping\nfrom levels of capital to stakes. The optimal policy maximizes the probability of\nreaching the goal. Let ph denote the probability of the coin coming up heads. If ph\nis known, then the entire problem is known and it can be solved, for instance, by\nvalue iteration\n","56":"YHandler\nYahoo Fantasy Sports OAuth And Request Handler\nThis is the Python Script I use to access the Yahoo Fantasy Sports API via OAuth for my desktop app. It's still far from polished, and not the most generalized, but has been updated to work with newer version of the requests library. It should be okay with future versions of Requests as the OAuth support has been written specifically for Yahoo's OAuth 1.0a process, which allows refresh of the access token. It looks like they now also support OAuth 2.0, but still remain backward compatible with OAuth1.0a.\n\nInstallation\nYou can install using pip:\npip install YHandler\n\n\nHow To Use\nCopy the auth.json.sample file and rename to auth.json and then place your consumer key, and consumer secret in the auth.json file.\nIn [1]: from YHandler import YHandler, YQuery\n\nIn [2]: handler = YHandler()\n\nIn [3]: query = YQuery(handler, 'nfl')\n\nIn [4]: query.get_games_info()\nOut[4]:\n[{'code': 'nfl',\n  'key': '359',\n  'name': 'Football',\n  'season': '2016',\n  'type': '2016'}]\n\nIn [5]: query.get_games_info(True)\nOut[5]:\n[{'code': 'nfl',\n  'key': '359',\n  'name': 'Football',\n  'season': '2016',\n  'type': '2016'}]\n\nIn [10]: query.get_user_leagues()\nOut[10]:\n[{'id': '577090',\n  'is_finished': True,\n  'name': 'IniTeCh',\n  'season': '2015',\n  'week': '16'},\n {'id': '126737',\n  'is_finished': False,\n  'name': 'Yahoo Public 126737',\n  'season': '2016',\n  'week': '1'}]\n\nIn [17]: query.find_player(126737, 'antonio brown')\nOut[17]: [{'id': '24171', 'name': 'Antonio Brown', 'team': 'Pittsburgh Steelers'}]\n\nIn [18]: query.get_player_week_stats(24171, '8')\nOut[18]:\n{'0': {'detail': 'Games Played', 'name': 'GP', 'value': '0'},\n '1': {'detail': 'Passing Attempts', 'name': 'Pass Att', 'value': '0'},\n '10': {'detail': 'Rushing Touchdowns', 'name': 'Rush TD', 'value': '0'},\n '11': {'detail': 'Receptions', 'name': 'Rec', 'value': '0'},\n '12': {'detail': 'Reception Yards', 'name': 'Rec Yds', 'value': '0'},\n '13': {'detail': 'Reception Touchdowns', 'name': 'Rec TD', 'value': '0'},\n '14': {'detail': 'Return Yards', 'name': 'Ret Yds', 'value': '0'},\n '15': {'detail': 'Return Touchdowns', 'name': 'Ret TD', 'value': '0'},\n '16': {'detail': '2-Point Conversions', 'name': '2-PT', 'value': '0'},\n '17': {'detail': 'Fumbles', 'name': 'Fum', 'value': '0'},\n '18': {'detail': 'Fumbles Lost', 'name': 'Fum Lost', 'value': '0'},\n '2': {'detail': 'Completions', 'name': 'Comp', 'value': '0'},\n '3': {'detail': 'Incomplete Passes', 'name': 'Inc', 'value': '0'},\n '4': {'detail': 'Passing Yards', 'name': 'Pass Yds', 'value': '0'},\n '5': {'detail': 'Passing Touchdowns', 'name': 'Pass TD', 'value': '0'},\n '57': {'detail': 'Offensive Fumble Return TD',\n  'name': 'Fum Ret TD',\n  'value': '0'},\n '58': {'detail': 'Pick Sixes Thrown', 'name': 'Pick Six', 'value': '0'},\n '59': {'detail': '40+ Yard Completions', 'name': '40 Yd Comp', 'value': '0'},\n '6': {'detail': 'Interceptions', 'name': 'Int', 'value': '0'},\n '60': {'detail': '40+ Yard Passing Touchdowns',\n  'name': '40 Yd Pass TD',\n  'value': '0'},\n '61': {'detail': '40+ Yard Run', 'name': '40 Yd Rush', 'value': '0'},\n '62': {'detail': '40+ Yard Rushing Touchdowns',\n  'name': '40 Yd Rush TD',\n  'value': '0'},\n '63': {'detail': '40+ Yard Receptions', 'name': '40 Yd Rec', 'value': '0'},\n '64': {'detail': '40+ Yard Reception Touchdowns',\n  'name': '40 Yd Rec TD',\n  'value': '0'},\n '7': {'detail': 'Sacks', 'name': 'Sack', 'value': '0'},\n '78': {'detail': 'Targets', 'name': 'Targets', 'value': '0'},\n '79': {'detail': 'Passing 1st Downs', 'name': 'Pass 1st Downs', 'value': '0'},\n '8': {'detail': 'Rushing Attempts', 'name': 'Rush Att', 'value': '0'},\n '80': {'detail': 'Receiving 1st Downs',\n  'name': 'Rec 1st Downs',\n  'value': '0'},\n '81': {'detail': 'Rushing 1st Downs', 'name': 'Rush 1st Downs', 'value': '0'},\n '9': {'detail': 'Rushing Yards', 'name': 'Rush Yds', 'value': '0'}}\n\n","57":"\u8dd1\u6b65\u662f\u4e0d\u53ef\u80fd\u8dd1\u6b65\u7684\n\u9ad8\u6821\u4f53\u80b2app\u81ea\u52a8\u8dd1\u6b65\n\u4ec5\u4f9b\u4ea4\u6d41\u5b66\u4e60\u4f7f\u7528\uff0cnot for evil use :)\n\u4f7f\u7528\u4e86\u767e\u5ea6\u5730\u56feapi\u81ea\u52a8\u5bfb\u8def\uff08\u867d\u7136\u53ef\u80fd\u5f88\u7ed5 \u5df2\u7ecf\u633a\u50cf\u771f\u4eba\u8dd1\u7684\u55bd :) \uff09\n\n\n\u4f7f\u7528\u65b9\u6cd5\n\u6709 \u672c\u5730\u76f4\u63a5\u8fd0\u884c \u548c \u8fd0\u884c\u5fae\u4fe1\u673a\u5668\u4eba \u4e24\u79cd\u65b9\u6cd5\uff0c \u5fae\u4fe1\u673a\u5668\u4eba \u53ea\u5efa\u8bae\u6709\u670d\u52a1\u5668\u7684\u540c\u5b66\u4f7f\u7528\n\u672c\u5730\u76f4\u63a5\u8fd0\u884c\nwindows\n\n\u4e0b\u8f7d \/dist\/run.exe\uff0c \u8fd0\u884c\uff0c\u8f93\u5165\u8d26\u53f7\u5bc6\u7801\u5373\u53ef\u5b8c\u6210\u4e00\u6b21\u953b\u70bc\uff08\u4e3a\u4e86\u907f\u514d\u5c01\u53f7\uff0c \u5b8c\u5168\u6a21\u62df\u4e86\u8dd1\u6b65\u6d41\u7a0b\uff0c\u8017\u65f6\u8f83\u957f\uff0c\u672a\u5b8c\u6210\u524d\u4e0d\u8981\u5173\u95ed\uff09\n\nlinux\/macos\ngit clone https:\/\/github.com\/FengLi666\/sports.git\ncd sports\npip3 install -r requirement.txt\nexport PYTHONPATH='.'\npython3 .\/mysports\/run.py\n\n\n\u8f93\u5165\u8d26\u53f7\u5bc6\u7801\n\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8dd1\u6b65\u6570\u636e\u5728\u4e00\u6bb5\u65f6\u95f4\u540e\u624d\u4f1a\u63d0\u4ea4\u7ed9app\u670d\u52a1\u5668(\u5373\u4f60\u8981\u4fdd\u6301\u8fd9\u4e2a\u8fdb\u7a0b\u4e00\u76f4\u8fd0\u884c\uff09\n\u5982\u679c\u60f3\u7acb\u5373\u63d0\u4ea4\u8dd1\u6b65\u6570\u636e\n\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\npython3 .\/mysports\/run.py --debug True\n\n\u8fd0\u884c\u5fae\u4fe1\u673a\u5668\u4eba\ngit clone https:\/\/github.com\/FengLi666\/sports.git\ncd sports\npip3 install -r requirement.txt\nexport PYTHONPATH='.'\npython3 .\/wechat_bot\/wechat_bot.py\n\n\u5177\u4f53\u89c1\u4ee3\u7801\n\n\u611f\u8c22 @RyuBAI\n","58":"  \n   \n \n \n\n\nsports-betting\nsports-betting is a tool that makes it easy to create machine learning based\nmodels for sports betting and evaluate their performance. It is compatible with\nscikit-learn.\n\nDocumentation\nInstallation documentation, API documentation, and examples can be found on the\ndocumentation.\n\nDependencies\nsports-betting is tested to work under Python 3.6+. The dependencies are the\nfollowing:\n\nnumpy(>=1.1)\nscikit-learn(>=0.21)\n\nAdditionally, to run the examples, you need matplotlib(>=2.0.0) and\npandas(>=0.22).\n\nInstallation\nsports-betting is currently available on the PyPi's repository and you can\ninstall it via pip:\npip install -U sports-betting\n\nThe package is released also in Anaconda Cloud platform:\nconda install -c algowit sports-betting\n\nIf you prefer, you can clone it and run the setup.py file. Use the following\ncommands to get a copy from GitHub and install all dependencies:\ngit clone https:\/\/github.com\/AlgoWit\/sports-betting.git\ncd sports-betting\npip install .\n\nOr install using pip and GitHub:\npip install -U git+https:\/\/github.com\/AlgoWit\/sports-betting.git\n\n\nTesting\nAfter installation, you can use pytest to run the test suite:\nmake test\n\n","59":"Data Engineering Projects\n\nProject 1: Data Modeling with Postgres\nIn this project, we apply Data Modeling with Postgres and build an ETL pipeline using Python. A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Currently, they are collecting data in json format and the analytics team is particularly interested in understanding what songs users are listening to.\nLink: Data_Modeling_with_Postgres\nProject 2: Data Modeling with Cassandra\nIn this project, we apply Data Modeling with Cassandra and build an ETL pipeline using Python. We will build a Data Model around our queries that we want to get answers for.\nFor our use case we want below answers:\n\nGet details of a song that was herad on the music app history during a particular session.\nGet songs played by a user during particular session on music app.\nGet all users from the music app history who listened to a particular song.\n\nLink : Data_Modeling_with_Apache_Cassandra\nProject 3: Data Warehouse\nIn this project, we apply the Data Warehouse architectures we learnt and build a Data Warehouse on AWS cloud. We build an ETL pipeline to extract and transform data stored in json format in s3 buckets and move the data to Warehouse hosted on Amazon Redshift.\nUse Redshift IaC script - Redshift_IaC_README\nLink  - Data_Warehouse\nProject 4: Data Lake\nIn this project, we will build a Data Lake on AWS cloud using Spark and AWS EMR cluster. The data lake will serve as a Single Source of Truth for the Analytics Platform. We will write spark jobs to perform ELT operations that picks data from landing zone on S3 and transform and stores data on the S3 processed zone.\nLink: Data_Lake\nProject 5: Data Pipelines with Airflow\nIn this project, we will orchestrate our Data Pipeline workflow using an open-source Apache project called Apache Airflow. We will schedule our ETL jobs in Airflow, create project related custom plugins and operators and automate the pipeline execution.\nLink:  Airflow_Data_Pipelines\nProject 6: Api Data to Postgres\nIn this project, we build an etl pipeline to fetch data from yelp API and insert it into the Postgres Database. This project is a very basic example of fetching real time data from an open source API.\nLink: API to Postgres\nCAPSTONE PROJECT\nUdacity provides their own crafted Capstone project with dataset that include data on immigration to the United States, and supplementary datasets that include data on airport codes, U.S. city demographics, and temperature data.\nI worked on my own open-ended project. \nHere is the link - goodreads_etl_pipeline\n","60":"\n\nPyQtGraph\nA pure-Python graphics library for PyQt\/PySide\/PyQt5\/PySide2\nCopyright 2020 Luke Campagnola, University of North Carolina at Chapel Hill\nhttp:\/\/www.pyqtgraph.org\nPyQtGraph is intended for use in mathematics \/ scientific \/ engineering applications.\nDespite being written entirely in python, the library is fast due to its\nheavy leverage of numpy for number crunching, Qt's GraphicsView framework for\n2D display, and OpenGL for 3D display.\nRequirements\n\nPython 2.7, or 3.x\nRequired\n\nPyQt 4.8+, PySide, PyQt5, or PySide2\nnumpy\n\n\nOptional\n\nscipy for image processing\npyopengl for 3D graphics\nhdf5 for large hdf5 binary format support\n\n\n\nQt Bindings Test Matrix\nThe following table represents the python environments we test in our CI system.  Our CI system uses Ubuntu 18.04, Windows Server 2019, and macOS 10.15 base images.\n\n\n\nQt-Bindings\nPython 2.7\nPython 3.6\nPython 3.7\nPython 3.8\n\n\n\n\nPyQt-4\n\u2705\n\u274c\n\u274c\n\u274c\n\n\nPySide1\n\u2705\n\u274c\n\u274c\n\u274c\n\n\nPyQt5-5.9\n\u274c\n\u2705\n\u274c\n\u274c\n\n\nPySide2-5.13\n\u274c\n\u274c\n\u2705\n\u274c\n\n\nPyQt5-Latest\n\u274c\n\u274c\n\u274c\n\u2705\n\n\nPySide2-Latest\n\u274c\n\u274c\n\u274c\n\u2705\n\n\n\n\npyqtgraph has had some incompatibilities with PySide2 versions 5.6-5.11, and we recommend you avoid those versions if possible\non macOS with Python 2.7 and Qt4 bindings (PyQt4 or PySide) the openGL related visualizations do not work reliably\n\nSupport\n\nReport issues on the GitHub issue tracker\nPost questions to the mailing list \/ forum or StackOverflow\n\nInstallation Methods\n\nFrom PyPI:\n\nLast released version: pip install pyqtgraph\nLatest development version: pip install git+https:\/\/github.com\/pyqtgraph\/pyqtgraph@master\n\n\nFrom conda\n\nLast released version: conda install -c conda-forge pyqtgraph\n\n\nTo install system-wide from source distribution: python setup.py install\nMany linux package repositories have release versions.\nTo use with a specific project, simply copy the pyqtgraph subdirectory\nanywhere that is importable from your project.\n\nDocumentation\nThe official documentation lives at https:\/\/pyqtgraph.readthedocs.io\nThe easiest way to learn pyqtgraph is to browse through the examples; run python -m pyqtgraph.examples to launch the examples application.\n","61":"Data Engineering 101: Building a Data Pipeline\nThis repository contains the files and data from the workshop as well as resources around Data Engineering. For the workshop (and after) we will use a Discord chatroom to keep the conversation going: https:\/\/discord.gg\/86cYcgU.\nAnd\/or please do not hesitate to reach out to me directly via email at inquiries@jonathan.industries or over twitter @memoryphoneme\nThe presentation can be found on Slideshare here or in this repository (presentation.pdf). Video can be found here.\n\n\nThroughout this workshop, you will learn how to make a scalable and sustainable data pipeline in Python with Luigi\n\nLearning Objectives\n\nRun a simple 1 stage Luigi flow reading\/writing to local files\nWrite a Luigi flow containing stages with multiple dependencies\n\nVisualize the progress of the flow using the centralized scheduler\nParameterize the flow from the command line\nOutput parameter specific output files\n\n\nManage serialization to\/from a Postgres database\nIntegrate a Hadoop Map\/Reduce task into an existing flow\nParallelize non-dependent stages of a multi-stage Luigi flow\nSchedule a local Luigi job to run once every day\nRun any arbitrary shell command in a repeatable way\n\nPrerequisites\nPrior experience with Python and the scientific Python stack is beneficial.  The workshop will focus on using the Luigi framework, but will have code from the following lobraries as well:\n\nnumpy\nscikit-learn\nFlask\n\nRun the Code\nLocal\n\nInstall libraries and dependencies: pip install -r requirements.txt\nStart the UI server: luigid --background --logdir logs\nNavigate with a web browser to http:\/\/localhost:[port] where [port] is the port the luigid server has started on (luigid defaults to port 8082)\nstart the API Server: python app.py\nEvaluate Model: python ml-pipeline.py EvaluateModel --input-dir text --lam 0.8\nRun evaluation server (at localhost:9191): topmodel\/topmodel_server.py\nRun the final pipeline: python ml-pipeline.py BuildModels --input-dir text --num-topics 10 --lam 0.8\n\n--\nFor parallelism, set --workers (note this is Task parallelism):\npython ml-pipeline.py BuildModels --input-dir text --num-topics 10 --lam 0.8 --workers 4\nHadoop\n\nStart Hadoop cluster: bin\/start-dfs.sh; sbin\/start-yarn.sh\nSetup Directory Structure: hadoop fs -mkdir \/tmp\/text\nGet files on cluster: hadoop fs -put .\/data\/text \/tmp\/text\nRetrieve results: hadoop fs -getmerge \/tmp\/text-count\/2012-06-01 .\/counts.txt\nView results: head .\/counts.txt\n\nFlask\n\ndocker run -it -v \/LOCAL\/PATH\/TO\/REPO\/data-engineering-101:\/root\/workshop clearspandex\/pydata-seattle bash\npip2 install flask\nipython2 app.py\n\nLibraries Used\n\nluigi\nscikit-learn\nnltk\nipdb\n\nWhats in here?\ntext\/                   20newsgroups text files\ntopmodel\/               Stripe's topmodel evaluation library\nexample_luigi.py        example scaffold of a luigi pipeline\nhadoop_word_count.py    example luigi pipeline using Hadoop\nml-pipeline.py          luigi pipeline covered in workshop\napp.py                  Flask server to deploy a scikit-learn model\nLICENSE                 Details of rights of use and distribution\npresentation.pdf        lecture slides from presentation\nreadme.md               this file!\n\nThe Data\nThe data (in the text\/ folder) is from the 20 newsgroups dataset, a standard benchmarking dataset for machine learning and NLP.  Each file in text corresponds to a single 'document' (or post) from one of two selected newsgroups (comp.sys.ibm.pc.hardware or alt.atheism).  The first line provides which group the document is from and everything thereafter is the body of the post.\ncomp.sys.ibm.pc.hardware\nI'm looking for a better method to back up files.  Currently using a MaynStream\n250Q that uses DC 6250 tapes.  I will need to have a capacity of 600 Mb to 1Gb\nfor future backups.  Only DOS files.\n\nI would be VERY appreciative of information about backup devices or\nmanufacturers of these products.  Flopticals, DAT, tape, anything.  \nIf possible, please include price, backup speed, manufacturer (phone #?), \nand opinions about the quality\/reliability.\n\nPlease E-Mail, I'll send summaries to those interested.\n\nThanx in advance,\n\nResources\/References\n\nQuestioning the Lambda Architecture\nLuigi: NYC Data Science Meetup\nThe Log: What every software engineer should know about real-time data's unifying abstraction\nI (heart) Log\nWhy Loggly Loves Apache Kafka\nBuffer's New Data Architecture\nPutting Apache Kafka to Use\nMetric Driven Development\nThe Unified Logging Infrastructure for Data Analytics at Twitter\nStream Processing and Mining just got more interesting\nHow to Beat the CAP Theorem\nBeating the CAP Theorem Checklist\n\nLicense\nCopyright 2015 Jonathan Dinu.\nAll files and content licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License\n","62":"\n\n\n\n\nTheme\nStatus\n\n\n\n\nPython Version\n\n\n\nLatest PyPI Release\n\n\n\nLatest Conda Release\n\n\n\nmaster Branch Build\n\n\n\ndevelop Branch Build\n\n\n\nDocumentation Build\n\n\n\nLicense\n\n\n\nCode Style\n\n\n\nQuestions\n\n\n\n\nWhat is Kedro?\n\n\"The centre of your data pipeline.\"\n\nKedro is an open-source Python framework that applies software engineering best-practice to data and machine-learning pipelines.  You can use it, for example, to optimise the process of taking a machine learning model into a production environment. You can use Kedro to organise a single user project running on a local environment, or collaborate within a team on an enterprise-level project.\nWe provide a standard approach so that you can:\n\nWorry less about how to write production-ready code,\nSpend more time building data pipelines that are robust, scalable, deployable, reproducible and versioned,\nStandardise the way that your team collaborates across your project.\n\nHow do I install Kedro?\nkedro is a Python package built for Python 3.6, 3.7 and 3.8.\nTo install Kedro from the Python Package Index (PyPI) simply run:\npip install kedro\n\nYou can also install kedro using conda, a package and environment manager program bundled with Anaconda. With conda already installed, simply run:\nconda install -c conda-forge kedro\n\nOur Get Started guide contains full installation instructions, and includes how to set up Python virtual environments.\nWe also recommend the frequently asked questions and the API reference documentation for additional information.\nWhat are the main features of Kedro?\n\nA pipeline visualisation generated using Kedro-Viz\n\n\n\nFeature\nWhat is this?\n\n\n\n\nProject Template\nA standard, modifiable and easy-to-use project template based on Cookiecutter Data Science.\n\n\nData Catalog\nA series of lightweight data connectors used for saving and loading data across many different file formats and file systems including local and network file systems, cloud object stores, and HDFS. The Data Catalog also includes data and model versioning for file-based systems. Used with a Python or YAML API.\n\n\nPipeline Abstraction\nAutomatic resolution of dependencies between pure Python functions and data pipeline visualisation using Kedro-Viz.\n\n\nThe Journal\nAn ability to reproduce pipeline runs with saved pipeline run results.\n\n\nCoding Standards\nTest-driven development using pytest, produce well-documented code using Sphinx, create linted code with support for flake8, isort and black and make use of the standard Python logging library.\n\n\nFlexible Deployment\nDeployment strategies that include the use of Docker with Kedro-Docker, conversion of Kedro pipelines into Airflow DAGs with Kedro-Airflow, leveraging a REST API endpoint with Kedro-Server (coming soon) and serving Kedro pipelines as a Python package. Kedro can be deployed locally, on-premise and cloud (AWS, Azure and Google Cloud Platform) servers, or clusters (EMR, EC2, Azure HDinsight and Databricks).\n\n\n\nHow do I use Kedro?\nThe Kedro documentation includes three examples to help get you started:\n\nA typical \"Hello World\" example, for an entry-level description of the main Kedro concepts\nThe more detailed \"spaceflights\" tutorial to give you hands-on experience as you learn about Kedro\n\nAdditional documentation includes:\n\nAn overview of Kedro architecture\nHow to use the CLI offered by kedro_cli.py (kedro new, kedro run, ...)\n\n\nNote: The CLI is a convenient tool for being able to run kedro commands but you can also invoke the Kedro CLI as a Python module with python -m kedro\n\nEvery Kedro function or class has extensive help, which you can call from a Python session as follows if the item is in local scope:\nfrom kedro.io import MemoryDataSet\nhelp(MemoryDataSet)\n\nWhy does Kedro exist?\nKedro is built upon our collective best-practice (and mistakes) trying to deliver real-world ML applications that have vast amounts of raw unvetted data. We developed Kedro to achieve the following:\n\nCollaboration on an analytics codebase when different team members have varied exposure to software engineering best-practice\nA focus on maintainable data and ML pipelines as the standard, instead of a singular activity of deploying models in production\nA way to inspire the creation of reusable analytics code so that we never start from scratch when working on a new project\nEfficient use of time because we're able to quickly move from experimentation into production\n\nThe humans behind Kedro\nKedro was originally designed by Aris Valtazanos and Nikolaos Tsaousis to solve challenges they faced in their project work.\nTheir work was later turned into an internal product by Peteris Erins, Ivan Danov, Nikolaos Kaltsas, Meisam Emamjome and Nikolaos Tsaousis.\nCurrently the core Kedro team consists of Yetunde Dada, Ivan Danov, Richard Westenra, Dmitrii Deriabin, Lorena Balan, Kiyohito Kunii, Zain Patel, Lim Hoang, Andrii Ivaniuk, Jo Stichbury, La\u00eds Carvalho, Merel Theisen, Gabriel Comym, and Liam Brummitt\nFormer core team members with significant contributions include: Gordon Wrigley, Nasef Khan and Anton Kirilenko.\nAnd last but not least, all the open-source contributors whose work went into all Kedro releases.\nCan I contribute?\nYes! Want to help build Kedro? Check out our guide to contributing to Kedro.\nWhere can I learn more?\nThere is a growing community around Kedro. Have a look at the Kedro FAQs to find projects using Kedro and links to articles, podcasts and talks.\nWho is using Kedro?\n\nAI Singapore\nCaterpillar\nElementAI\nJungle Scout\nMercadoLibre Argentina\nMosaic Data Science\nNaranjaX\nOpen Data Science LatAm\nRetrieva\nRoche\nUrbanLogiq\nXP\nDendra Systems\n\nWhat licence do you use?\nKedro is licensed under the Apache 2.0 License.\nWe're hiring!\nDo you want to be part of the team that builds Kedro and other great products at QuantumBlack? If so, you're in luck! QuantumBlack is currently hiring Software Engineers who love using data to drive their decisions. Take a look at our open positions and see if you're a fit.\n","63":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeomancer is a geospatial feature engineering library. It leverages geospatial\ndata such as OpenStreetMap (OSM) alongside a\ndata warehouse like BigQuery. You can use this to create, share, and iterate\ngeospatial features for your downstream tasks (analysis, modelling,\nvisualization, etc.).\n\n\n\nFeatures\nGeomancer can perform geospatial feature engineering for all types of vector data\n(i.e. points, lines, polygons).\n\nFeature primitives for geospatial feature engineering\nAbility to switch out data warehouses (BigQuery, SQLite, PostgreSQL (In Progress))\nCompile and share your features using our SpellBook\n\nSetup and Installation\nInstalling the library\nGeomancer can be installed using pip.\n$ pip install geomancer\n\nThis will install all dependencies for every data-warehouse we support. If\nyou wish to do this only for a specific warehouse, then you can add an\nidentifier:\n$ pip install geomancer[bq] # For BigQuery\n$ pip install geomancer[sqlite] # For SQLite\n$ pip install geomancer[psql] # For PostgreSQL\n\nAlternatively, you can also clone the repository then run install.\n$ git clone https:\/\/github.com\/thinkingmachines\/geomancer.git\n$ cd geomancer\n$ python setup.py install\n\nSetting up your data warehouse\nGeomancer is powered by a geospatial data warehouse: we highly-recommend using\nBigQuery as your data warehouse and\nGeofabrik's OSM catalog as your\nsource of Points and Lines of interest.\n\nYou can see the set-up instructions in this link\nBasic Usage\nAll of the feature engineering functions in Geomancer are called \"spells\". For\nexample, you want to get the distance to the nearest supermarket for each\npoint.\nfrom geomancer.spells import DistanceToNearest\n\n# Load your dataset in a pandas dataframe\n# df = load_dataset()\n\ndist_spell = DistanceToNearest(\n    \"supermarket\",\n    source_table=\"ph_osm.gis_osm_pois_free_1\",\n    feature_name=\"dist_supermarket\",\n    dburl=\"bigquery:\/\/project-name\",\n).cast(df)\nYou can specify the type of filter  using the format {column}:{filter}.  By\ndefault, the column value is fclass. For example, if you wish to look for\nroads on a bridge, then pass bridge:T:\nfrom geomancer.spells import DistanceToNearest\n\n# Load the dataset in a pandas dataframe\n# df = load_dataset()\n\ndist_spell = DistanceToNearest(\n    \"bridge:T\",\n    source_table=\"ph_osm.gis_osm_roads_free_1\",\n    feature_name=\"dist_road_bridges\",\n    dburl=\"bigquery:\/\/project-name\",\n).cast(df)\nCompose multiple spells into a \"spell book\" which you can export as a JSON file.\nfrom geomancer.spells import DistanceToNearest\nfrom geomancer.spellbook import SpellBook\n\nspellbook = SpellBook([\n    DistanceToNearest(\n        \"supermarket\",\n        source_table=\"ph_osm.gis_osm_pois_free_1\",\n        feature_name=\"dist_supermarket\",\n        dburl=\"bigquery:\/\/project-name\",\n    ),\n    DistanceToNearest(\n        \"embassy\",\n        source_table=\"ph_osm.gis_osm_pois_free_1\",\n        feature_name=\"dist_embassy\",\n        dburl=\"bigquery:\/\/project-name\",\n    ),\n])\nspellbook.to_json(\"dist_supermarket_and_embassy.json\")\nYou can share the generated file so other people can re-use your feature extractions\nwith their own datasets.\nfrom geomancer.spellbook import SpellBook\n\n# Load the dataset in a pandas dataframe\n# df = load_dataset()\n\nspellbook = SpellBook.read_json(\"dist_supermarket_and_embassy.json\")\ndist_supermarket_and_embassy = spellbook.cast(df)\nContributing\nThis project is open for contributors! Contibutions can come in the form of\nfeature requests, bug fixes, documentation, tutorials and the like! We highly\nrecommend to file an Issue first before submitting a Pull\nRequest.\nSimply fork this repository and make a Pull Request! We'd definitely appreciate:\n\nImplementation of new features\nBug Reports\nDocumentation\nTesting\n\nAlso, we have a\nCONTRIBUTING\nand a CODE_OF_CONDUCT,\nso please check that one out!\nLicense\nMIT License \u00a9 2019, Thinking Machines Data Science\n","64":"Data Engineering Project\n  \nData Engineering Project is an implementation of the data pipeline which consumes the latest news from RSS Feeds and makes them available for users via handy API.\nThe pipeline infrastructure is built using popular, open-source projects.\nAccess the latest news and headlines in one place. \ud83d\udcaa\nTable of Contents\n\nArchitecture diagram\nHow it works\n\nData scraping\nData flow\nData access\n\n\nPrerequisites\nRunning project\nTesting\nAPI service\nReferences\nContributions\nLicense\nContact\n\nArchitecture diagram\n\nHow it works\nData Scraping\nAirflow DAG is responsible for the execution of Python scraping modules.\nIt runs periodically every X minutes producing micro-batches.\n\n\nFirst task updates proxypool. Using proxies in combination with rotating user agents can help get scrapers past most of the anti-scraping measures and prevent being detected as a scraper.\n\n\nSecond task extracts news from RSS feeds provided in the configuration file, validates the quality and sends data into Kafka topic A. The extraction process is using validated proxies from proxypool.\n\n\nData flow\n\nKafka Connect Mongo Sink consumes data from Kafka topic A and stores news in MongoDB using upsert functionality based on _id field.\nDebezium MongoDB Source tracks a MongoDB replica set for document changes in databases and collections, recording those changes as events in Kafka topic B.\nKafka Connect Elasticsearch Sink consumes data from Kafka topic B and upserts news in Elasticsearch. Data replicated between topics A and B ensures MongoDB and ElasticSearch synchronization. Command Query Responsibility Segregation (CQRS) pattern allows the use of separate models for updating and reading information.\nKafka Connect S3-Minio Sink consumes records from Kafka topic B and stores them in MinIO (high-performance object storage) to ensure data persistency.\n\nData access\n\nData gathered by previous steps can be easily accessed in API service  using public endpoints.\n\nPrerequisites\nSoftware required to run the project. Install:\n\nDocker\nPython 3.8+ (pip)\ndocker-compose\n\nRunning project\nScript manage.sh - wrapper for docker-compose works as a managing tool.\n\nBuild project infrastructure\n\n.\/manage.sh up\n\nStop project infrastructure\n\n.\/manage.sh stop\n\nDelete project infrastructure\n\n.\/manage.sh down\nTesting\nScript run_tests.sh executes unit tests against Airflow scraping modules and Django Rest Framework applications.\n.\/run_tests.sh\nAPI service\nRead detailed documentation on how to interact with data collected by pipeline using search endpoints.\nExample searches:\n\nsee all news\n\nhttp:\/\/0.0.0.0:5000\/api\/v1\/news\/ \n\n\nadd search_fields title and description, see all of the news containing the Robert Lewandowski phrase\n\nhttp:\/\/0.0.0.0:5000\/api\/v1\/news\/?search=Robert%20Lewandowski \n\n\nfind news containing the Lewandowski phrase in their titles\n\nhttp:\/\/0.0.0.0:5000\/api\/v1\/news\/?search=title|Lewandowski \n\n\nsee all of the polish news containing the Lewandowski phrase\n\nhttp:\/\/0.0.0.0:5000\/api\/v1\/news\/?search=lewandowski&language=pl\n\nReferences\nInspired by following codes, articles and videos:\n\nHow we launched a data product in 60 days with AWS\nToru\u0144 JUG #55 - \"Kafka Connect - szwajcarski scyzoryk w r\u0119kach in\u017cyniera?\" - Mariusz Strzelecki\nKafka Elasticsearch Sink Connector and the Power of Single Message Transformations\nDocker Tips and Tricks with Kafka Connect, ksqlDB, and Kafka\n\nContributions\nContributions are what makes the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.\n\nFork the Project\nCreate your Feature Branch (git checkout -b feature\/AmazingFeature)\nCommit your Changes (git commit -m 'Add some AmazingFeature')\nPush to the Branch (git push origin feature\/AmazingFeature)\nOpen a Pull Request\n\nLicense\nDistributed under the MIT License. See LICENSE for more information.\nContact\nPlease feel free to contact me if you have any questions.\nDamian Kli\u015b @DamianKlis\n","65":"Datasets\nWe provide the datasets used in our empirical studies and evaluations. The description is provided according to the type of datasets and the associated work.\nIf you use our dataset, please cite our relevant paper in your publication. The bib  is also provided.\n\nThere are three groups of datasets according to the dataset's characteristics.\n\n\nagile sprints\nThese are the datasets on the iterative development (e.g. sprint). Our work on these datasets focuses on predicting delivery capability in iterative development. We published the work in IEEE TSE\n\n\ndelay issues\nThese are the datasets on the delayed issues. We used these datasets in our work on predicting delayed issues which we published in MSR2015, ASE2015, and in the journal of Empirical Software Engineering.\n\n\nstory points\nThese are the datasets on the story point estimation. We provide the datasets and the models from our work on a deep learning model for estimating story points which we published in IEEE TSE.\n\n\nYou can also find preprints in the folders.\nVisit our homepage for more informaiton SEA@UOW\n\nbib\n\nIEEE TSE2018: A deep learning model for estimating story points\n\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, T. T. M. Pham, A. Ghose, and T. Menzies, \u201cA deep learning model for estimating story points,\u201d IEEE Trans. Softw. Eng., vol. PP, no. 99, p. 1, 2018.\n@article{Choetkiertikul2018,\nauthor = {Choetkiertikul, M and Dam, H K and Tran, T and Pham, T T M and Ghose, A and Menzies, T},\ndoi = {10.1109\/TSE.2018.2792473},\nissn = {0098-5589 VO  - PP},\njournal = {IEEE Transactions on Software Engineering},\nkeywords = {Estimation,Machine learning,Planning,Predictive models,Software,Springs,deep learning,effort estimation,software analytics,story point estimation},\nnumber = {99},\npages = {1},\ntitle = {{A deep learning model for estimating story points}},\nvolume = {PP},\nyear = {2018}\n}\n\n\nIEEE TSE2017: Predicting Delivery Capability in Iterative Software Development\n\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, A. Ghose, and J. Grundy, \u201cPredicting Delivery Capability in Iterative Software Development,\u201d IEEE Trans. Softw. Eng., vol. 14, no. 8, pp. 1\u20131, 2017.\n@article{Choetkiertikul2017,\ntitle = {{Predicting Delivery Capability in Iterative Software Development}},\nauthor = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya and Grundy, John},\ndoi = {10.1109\/TSE.2017.2693989},\nissn = {0098-5589},\njournal = {IEEE Transactions on Software Engineering},\nnumber = {8},\npages = {1--1},\nvolume = {14},\nyear = {2017}\n}\n\n\nEMSE2017: Predicting the delay of issues with due dates in software projects\n\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, \u201cPredicting the delay of issues with due dates in software projects,\u201d Empir. Softw. Eng., vol. 22, no. 3, pp. 1223\u20131263, 2017.\n@article{Choetkiertikul2017,\ntitle = {{Predicting the delay of issues with due dates in software projects}},\nauthor = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},\ndoi = {10.1007\/s10664-016-9496-7},\nissn = {15737616},\njournal = {Empirical Software Engineering},\nnumber = {3},\npages = {1223--1263},\npublisher = {Empirical Software Engineering},\nvolume = {22},\nyear = {2017}\n}\n\n\nMSR2015: Characterization and prediction of issue-related risks in software projects\n\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, \u201cCharacterization and prediction of issue-related risks in software projects,\u201d in Proceedings of the 12th Working Conference on Mining Software Repositories (MSR), 2015, pp. 280\u2013291.\n@inproceedings{Morakot2015,\ntitle = {{Characterization and Prediction of Issue-Related Risks in Software Projects}},\nauthor = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},\nbooktitle = {Proceedings of the 12th IEEE\/ACM Working Conference on Mining Software Repositories (MSR)},\ndoi = {10.1109\/MSR.2015.33},\nisbn = {978-0-7695-5594-2},\nissn = {21601860},\npages = {280--291},\npublisher = {IEEE},\nurl = {http:\/\/ieeexplore.ieee.org\/document\/7180087\/},\nyear = {2015}\n}\n\n\nASE2015: Predicting delays in software projects using networked classification\n\n[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, \u201cPredicting delays in software projects using networked classification,\u201d in Proceedings of the 30th IEEE\/ACM International Conference on Automated Software Engineering (ASE), 2015, pp. 353\u2013364.\n@inproceedings{Choetkiertikul2015,\ntitle = {{Predicting delays in software projects using networked classification}},\nauthor = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},\nbooktitle = {Proceedings of the 30th IEEE\/ACM International Conference on Automated Software Engineering (ASE)},\ndoi = {10.1109\/ASE.2015.55},\nisbn = {9781509000241},\npages = {353--364},\nyear = {2015}\n}\n\n\n","66":"socrata-py\nPython SDK for the Socrata Data Management API. Use this library to call into publishing and ETL functionality offered when writing to Socrata datasets.\nwith open('cool_dataset.csv', 'rb') as file:\n    (revision, output) = Socrata(auth).create(\n        name = \"cool dataset\",\n        description = \"a description\"\n    ).csv(file)\n    \n    revision.apply(output_schema = output)\n\nInstallation\nExample\nUsing\n\nBoilerplate\nSimple usage\n\nCreate a new Dataset from a csv, tsv, xls or xlsx file\nCreate a new Dataset from Pandas\nUpdating a dataset\nGenerating a config and using it to update\n\n\n\n\nAdvanced usage\n\nCreate a revision\nCreate an upload\nUpload a file\nTransforming your data\nWait for the transformation to finish\nErrors in a transformation\nValidating rows\nDo the upsert!\nMetadata only revisions\n\n\n\n\nDevelopment\n\nTesting\nGenerating docs\nReleasing\n\n\nLibrary Docs\n\nSocrata\n\ncreate\nnew\nusing_config\n\n\nAuthorization\n\nlive_dangerously\n\n\nRevisions\n\ncreate_delete_revision\ncreate_replace_revision\ncreate_update_revision\ncreate_using_config\nlist\nlookup\n\n\nRevision\n\napply\ncreate_upload\ndiscard\nlist_operations\nopen_in_browser\nplan\nset_output_schema\nsource_as_blob\nsource_from_agent\nsource_from_dataset\nsource_from_url\nui_url\nupdate\n\n\nSources\n\ncreate_upload\nlookup\n\n\nSource\n\nadd_to_revision\nblob\nchange_parse_option\ncsv\ndf\ngeojson\nkml\nlist_operations\nload\nopen_in_browser\nshapefile\ntsv\nui_url\nwait_for_finish\nxls\nxlsx\n\n\nConfigs\n\ncreate\nlist\nlookup\n\n\nConfig\n\nchange_parse_option\ncreate_revision\ndelete\nlist_operations\nupdate\n\n\nInputSchema\n\nget_latest_output_schema\nlatest_output\nlist_operations\ntransform\n\n\nOutputSchema\n\nadd_column\nbuild_config\nchange_column_metadata\nchange_column_transform\ndrop_column\nlist_operations\nrows\nrun\nschema_errors\nschema_errors_csv\nset_row_id\nvalidate_row_id\nwait_for_finish\n\n\nJob\n\nis_complete\nlist_operations\nwait_for_finish\n\n\n\n\n\nInstallation\nThis only supports python3.\nInstallation is available through pip. Using a virtualenv is advised. Install\nthe package by running\npip3 install socrata-py\n\nThe only hard dependency is requests which will be installed via pip. Pandas is not required, but creating a dataset from a Pandas dataframe is supported. See below.\nExample\nTry the command line example with\npython -m examples.create 'Police Reports' ~\/Desktop\/catalog.data.gov\/Seattle_Real_Time_Fire_911_Calls.csv 'pete-test.test-socrata.com' --username $SOCRATA_USERNAME --password $SOCRATA_PASSWORD\nUsing\nBoilerplate\n# Import some stuff\nfrom socrata.authorization import Authorization\nfrom socrata import Socrata\nimport os\n\n# Boilerplate...\n# Make an auth object\nauth = Authorization(\n  \"pete-test.test-socrata.com\",\n  os.environ['SOCRATA_USERNAME'],\n  os.environ['SOCRATA_PASSWORD']\n)\nSimple usage\nCreate a new Dataset from a csv, tsv, xls or xlsx file\nTo create a dataset, you can do this:\nwith open('cool_dataset.csv', 'rb') as file:\n    # Upload + Transform step\n\n    # revision is the *change* to the view in the catalog, which has not yet been applied.\n    # output is the OutputSchema, which is a change to data which can be applied via the revision\n    (revision, output) = Socrata(auth).create(\n        name = \"cool dataset\",\n        description = \"a description\"\n    ).csv(file)\n\n    # Transformation step\n    # We want to add some metadata to our column, drop another column, and add a new column which will\n    # be filled with values from another column and then transformed\n    output = output\\\n        .change_column_metadata('a_column', 'display_name').to('A Column!')\\\n        .change_column_metadata('a_column', 'description').to('Here is a description of my A Column')\\\n        .drop_column('b_column')\\\n        .add_column('a_column_squared', 'A Column, but times itself', 'to_number(`a_column`) * to_number(`a_column`)', 'this is a column squared')\\\n        .run()\n\n\n    # Validation of the results step\n    output = output.wait_for_finish()\n    # The data has been validated now, and we can access errors that happened during validation. For example, if one of the cells in `a_column` couldn't be converted to a number in the call to `to_number`, that error would be reflected in this error_count\n    assert output.attributes['error_count'] == 0\n\n    # If you want, you can get a csv stream of all the errors\n    errors = output.schema_errors_csv()\n    for line in errors.iter_lines():\n        print(line)\n\n    # Update step\n\n    # Apply the revision - this will make it public and available to make\n    # visualizations from\n    job = revision.apply(output_schema = output)\n\n    # This opens a browser window to your revision, and you will see the progress\n    # of the job\n    revision.open_in_browser()\n\n    # Application is async - this will block until all the data\n    # is in place and readable\n    job.wait_for_finish()\nSimilar to the csv method are the xls, xlsx, and tsv methods, which upload\nthose files.\nThere is a blob method as well, which uploads blobby data to the source. This means the data will not be parsed, and will be displayed under \"Files and Documents\" in the catalog once the revision is applied.\nCreate a new Dataset from Pandas\nDatasets can also be created from Pandas DataFrames\nimport pandas as pd\ndf = pd.read_csv('socrata-py\/test\/fixtures\/simple.csv')\n# Do various Pandas-y changes and modifications, then...\n(revision, output) = Socrata(auth).create(\n    name = \"Pandas Dataset\",\n    description = \"Dataset made from a Pandas Dataframe\"\n).df(df)\n\n# Same code as above to apply the revision.\nUpdating a dataset\nA Socrata update is actually an upsert. Rows are updated or created based on the row identifier. If the row-identifer doesn't exist, all updates are just appends to the dataset.\nA replace truncates the whole dataset and then inserts the new data.\nGenerating a config and using it to update\n# This is how we create our view initially\nwith open('cool_dataset.csv', 'rb') as file:\n    (revision, output) = Socrata(auth).create(\n        name = \"cool dataset\",\n        description = \"a description\"\n    ).csv(file)\n\n    revision.apply(output_schema = output)\n\n# This will build a configuration using the same settings (file parsing and\n# data transformation rules) that we used to get our output. The action\n# that we will take will be \"update\", though it could also be \"replace\"\nconfig = output.build_config(\"cool-dataset-config\", \"update\")\n\n# Now we need to save our configuration name and view id somewhere so we\n# can update the view using our config\nconfiguration_name = \"cool-dataset-config\"\nview_id = revision.view_id()\n\n# Now later, if we want to use that config to update our view, we just need the view and the configuration_name\nsocrata = Socrata(auth)\nview = socrata.views.lookup(view_id) # View will be the view we are updating with the new data\n\nwith open('updated-cool-dataset.csv', 'rb') as my_file:\n    (revision, job) = socrata.using_config(\n        configuration_name,\n        view\n    ).csv(my_file)\n    print(job) # Our update job is now running\nAdvanced usage\nCreate a revision\n# This is our socrata object, using the auth variable from above\nsocrata = Socrata(auth)\n\n# This will make our initial revision, on a view that doesn't yet exist\nrevision = socrata.new({'name': 'cool dataset'})\n\n# revision is a Revision object, we can print it\nprint(revision)\nRevision({'created_by': {'display_name': 'rozap',\n                'email': 'chris.duranti@socrata.com',\n                'user_id': 'tugg-ikce'},\n 'fourfour': 'ij46-xpxe',\n 'id': 346,\n 'inserted_at': '2017-02-27T23:05:08.522796',\n 'metadata': None,\n 'update_seq': 285,\n 'upsert_jobs': []})\n\n# We can also access the attributes of the revision\nprint(revision.attributes['metadata']['name'])\n'cool dataset'\nCreate an upload\n# Using that revision, we can create an upload\nupload = revision.create_upload('foo.csv')\n\n# And print it\nprint(upload)\nSource({'content_type': None,\n 'created_by': {'display_name': 'rozap',\n                'email': 'chris.duranti@socrata.com',\n                'user_id': 'tugg-ikce'},\n 'source_type': {\n    'filename': 'foo.csv',\n    'type': 'upload'\n },\n 'finished_at': None,\n 'id': 290,\n 'inserted_at': '2017-02-27T23:07:18.309676',\n 'schemas': []})\nUpload a file\n# And using that upload we just created, we can put bytes into it\nwith open('test\/fixtures\/simple.csv', 'rb') as f:\n    source = upload.csv(f)\nTransforming your data\nTransforming data consists of going from input data (data exactly as it appeared in the source)\nto output data (data as you want it to appear).\nTransformation from input data to output data often has problems. You might, for example, have a column\nfull of numbers, but one row in that column is actually the value hehe! which cannot be transformed into\na number. Rather than failing at each datum which is dirty or wrong, transforming your data allows you to\nreconcile these issues.\nWe might have a dataset called temps.csv that looks like\ndate, celsius\n8-24-2017, 22\n8-25-2017, 20\n8-26-2017, 23\n8-27-2017, hehe!\n8-28-2017,\n8-29-2017, 21\n\nSuppose we uploaded it in our previous step, like this:\nwith open('temps.csv', 'rb') as f:\n    source = upload.csv(f)\n    input_schema = source.get_latest_input_schema()\nOur input_schema is the input data exactly as it appeared in the CSV, with all values of type string.\nOur output_schema is the output data as it was guessed by Socrata. Guessing may not always be correct, which is why we have import configs to \"lock in\" a schema for automation. We can get the output_schema\nlike so:\noutput_schema = input_schema.get_latest_output_schema()\nWe can now make changes to the schema, like so\nnew_output_schema = output\n    # Change the field_name of date to the_date\n    .change_column_metadata('date', 'field_name').to('the_date')\\\n    # Change the description of the celsius column\n    .change_column_metadata('celsius', 'description').to('the temperature in celsius')\\\n    # Change the display name of the celsius column\n    .change_column_metadata('celsius', 'display_name').to('Degrees (Celsius)')\\\n    # Change the transform of the_date column to to_fixed_timestamp(`date`)\n    .change_column_transform('the_date').to('to_fixed_timestamp(`date`)')\\\n    # Make the celsius column all numbers\n    .change_column_transform('celsius').to('to_number(`celsius`)')\\\n    # Add a new column, which is computed from the `celsius` column\n    .add_column('fahrenheit', 'Degrees (Fahrenheit)', '(to_number(`celsius`) * (9 \/ 5)) + 32', 'the temperature in celsius')\\\n    .run()\nchange_column_metadata(column_name, column_attribute) takes the field name used to\nidentify the column and the column attribute to change (field_name, display_name, description, position)\nadd_column(field_name, display_name, transform_expression, description) will create a new column\nWe can also call drop_column(celsius) which will drop the column.\n.run() will then make a request and return the new output_schema, or an error if something is invalid.\nTransforms can be complex SoQL expressions. Available functions are listed here. You can do lots of stuff with them;\nFor example, you could change all null values into errors (which won't be imported) by doing\nsomething like\nnew_output_schema = output\n    .change_column_transform('celsius').to('coalesce(to_number(`celsius`), error(\"Celsius was null!\"))')\n    .run()\nOr you could add a new column that says if the day was hot or not\nnew_output_schema = output\n    .add_column('is_hot', 'Was the day hot?', 'to_number(`celsius`) >= 23')\n    .run()\nOr you could geocode a column, given the following CSV\naddress,city,zip,state\n10028 Ravenna Ave NE, Seattle, 98128, WA\n1600 Pennsylvania Avenue, Washington DC, 20500, DC\n6511 32nd Ave NW, Seattle, 98155, WA\n\nWe could transform our first output_schema into a single column dataset, where that\nsingle column is a Point of the address\noutput = output\\\n    .add_column('location', 'Incident Location', 'geocode(`address`, `city`, `state`, `zip`)')\\\n    .drop_column('address')\\\n    .drop_column('city')\\\n    .drop_column('state')\\\n    .drop_column('zip')\\\n    .run()\nComposing these SoQL functions into expressions will allow you to validate, shape, clean and extend your data to make it more useful to the consumer.\nWait for the transformation to finish\nTransformations are async, so if you want to wait for it to finish, you can do so\n\nErrors in a transformation\nTransformations may have had errors, like in the previous example, we can't convert hehe! to a number. We can see the count of them like this:\nprint(output_schema.attributes['error_count'])\nWe can view the detailed errors like this:\nerrors = output_schema.schema_errors()\nWe can get a CSV of the errors like this:\ncsv_stream = output_schema.schema_errors_csv()\nValidating rows\nWe can look at the rows of our schema as well\nrows = output_schema.rows(offset = 0, limit = 20)\n\nself.assertEqual(rows, [\n    {'b': {'ok': ' bfoo'}},\n    {'b': {'ok': ' bfoo'}},\n    {'b': {'ok': ' bfoo'}},\n    {'b': {'ok': ' bfoo'}}\n])\nDo the upsert!\n# Now we have transformed our data into the shape we want, let's do an upsert\njob = revision.apply(output_schema = output_schema)\n\n# This will complete the upsert behind the scenes. If we want to\n# re-fetch the current state of the upsert job, we can do so\njob = job.show()\n\n# To get the progress\nprint(job.attributes['log'])\n[\n    {'details': {'Errors': 0, 'Rows Created': 0, 'Rows Updated': 0, 'By RowIdentifier': 0, 'By SID': 0, 'Rows Deleted': 0}, 'time': '2017-02-28T20:20:59', 'stage': 'upsert_complete'},\n    {'details': {'created': 1}, 'time': '2017-02-28T20:20:59', 'stage': 'columns_created'},\n    {'details': {'created': 1}, 'time': '2017-02-28T20:20:59', 'stage': 'columns_created'},\n    {'details': None, 'time': '2017-02-28T20:20:59', 'stage': 'started'}\n]\n\n\n# So maybe we just want to wait here, printing the progress, until the job is done\njob.wait_for_finish(progress = lambda job: print(job.attributes['log']))\n\n# So now if we go look at our original four-four, our data will be there\nMetadata only revisions\nWhen there is an existing Socrata view that you'd like to update the metadata of, you can do so by creating a Source which is the Socrata view.\nview = socrata.views.lookup('abba-cafe')\n\nrevision = view.revisions.create_replace_revision()\nsource = revision.source_from_dataset()\noutput_schema = source.get_latest_input_schema().get_latest_output_schema()\nnew_output_schema = output_schema\\\n    .change_column_metadata('a', 'description').to('meh')\\\n    .change_column_metadata('b', 'display_name').to('bbbb')\\\n    .change_column_metadata('c', 'field_name').to('ccc')\\\n    .run()\n\n\nrevision.apply(output_schema = new_output_schema)\nDevelopment\nTesting\nInstall test deps by running pip install -r requirements.txt. This will install pdoc and pandas which are required to run the tests.\nConfiguration is set in test\/auth.py for tests. It reads the domain, username, and password from environment variables. If you want to run the tests, set those environment variables to something that will work.\nIf I wanted to run the tests against my local instance, I would run:\nSOCRATA_DOMAIN=localhost SOCRATA_USERNAME=$SOCRATA_LOCAL_USER SOCRATA_PASSWORD=$SOCRATA_LOCAL_PASS bin\/test\nGenerating docs\nmake the docs by running\nmake docs\nReleasing\nrelease to pypi by bumping the version to something reasonable and running\npython setup.py sdist upload -r pypi\n\nNote you'll need your .pypirc file in your home directory. For help, read this\nLibrary Docs\nSocrata\nArgSpec\n    Args: auth\n\nTop level publishing object.\nAll functions making HTTP calls return a result tuple, where the first element in the\ntuple is whether or not the call succeeded, and the second element is the returned\nobject if it was a success, or a dictionary containing the error response if the call\nfailed. 2xx responses are considered successes. 4xx and 5xx responses are considered failures.\nIn the event of a socket hangup, an exception is raised.\ncreate\nShortcut to create a dataset. Returns a Create object,\nwhich contains functions which will create a view, upload\nyour file, and validate data quality in one step.\nTo actually place the validated data into a view, you can call .apply()\non the revision\n(revision, output_schema) Socrata(auth).create(\n    name = \"cool dataset\",\n    description = \"a description\"\n).csv(file)\n\njob = revision.apply(output_schema = output_schema)\n\nArgs:\n   **kwargs: Arbitrary revision metadata values\n\nReturns:\n    result (Revision, OutputSchema): Returns the revision that was created and the\n        OutputSchema created from your uploaded file\n\nExamples:\nSocrata(auth).create(\n    name = \"cool dataset\",\n    description = \"a description\"\n).csv(open('my-file.csv'))\nnew\nArgSpec\n    Args: metadata\n\nCreate an empty revision, on a view that doesn't exist yet. The\nview will be created for you, and the initial revision will be returned.\nArgs:\n    metadata (dict): Metadata to apply to the revision\n\nReturns:\n    Revision\n\nExamples:\n    rev = Socrata(auth).new({\n        'name': 'hi',\n        'description': 'foo!',\n        'metadata': {\n            'view': 'metadata',\n            'anything': 'is allowed here'\n\n        }\n    })\nusing_config\nArgSpec\n    Args: config_name, view\n\nUpdate a dataset, using the configuration that you previously\ncreated, and saved the name of. Takes the config_name parameter\nwhich uniquely identifies the config, and the View object, which can\nbe obtained from socrata.views.lookup('view-id42')\nArgs:\n    config_name (str): The config name\n    view (View): The view to update\n\nReturns:\n    result (ConfiguredJob): Returns the ConfiguredJob\n\nNote:\nTypical usage would be in a context manager block (as demonstrated in the example\nbelow). In this case, the ConfiguredJob is created and immediately launched by way of\nthe call to the ConfiguredJob.csv method.\nExamples:\n    with open('my-file.csv', 'rb') as my_file:\n        (rev, job) = p.using_config(name, view).csv(my_file)\n\nAuthorization\nArgSpec\n    Args: domain, username, password, request_id_prefix\n    Defaults: domain=\n\nManages basic authorization for accessing the socrata API.\nThis is passed into the Socrata object once, which is the entry\npoint for all operations.\nauth = Authorization(\n    \"data.seattle.gov\",\n    os.environ['SOCRATA_USERNAME'],\n    os.environ['SOCRATA_PASSWORD']\n)\npublishing = Socrata(auth)\n\nlive_dangerously\nDisable SSL checking. Note that this should only be used while developing\nagainst a local Socrata instance.\nRevisions\nArgSpec\n    Args: fourfour, auth\n\ncreate_delete_revision\nArgSpec\n    Args: metadata, permission\n    Defaults: metadata={}, permission=public\n\nCreate a revision on the view, which when applied, will delete rows of data.\nThis is an upsert; a row id must be set.\nArgs:\n    metadata (dict): The metadata to change; these changes will be applied when the revision is applied\n    permission (string): 'public' or 'private'\n\nReturns:\n    Revision The new revision, or an error\n\nExamples:\n    view.revisions.create_delete_revision(metadata = {\n        'name': 'new dataset name',\n        'description': 'description'\n    })\ncreate_replace_revision\nArgSpec\n    Args: metadata, permission\n    Defaults: metadata={}, permission=public\n\nCreate a revision on the view, which when applied, will replace the data.\nArgs:\n    metadata (dict): The metadata to change; these changes will be applied when the revision\n        is applied\n    permission (string): 'public' or 'private'\n\nReturns:\n    Revision The new revision, or an error\n\nExamples:\n    >>> view.revisions.create_replace_revision(metadata = {'name': 'new dataset name', 'description': 'updated description'})\n\ncreate_update_revision\nArgSpec\n    Args: metadata, permission\n    Defaults: metadata={}, permission=public\n\nCreate a revision on the view, which when applied, will update the data\nrather than replacing it.\nThis is an upsert; if there is a rowId defined and you have duplicate ID values,\nthose rows will be updated. Otherwise they will be appended.\nArgs:\n    metadata (dict): The metadata to change; these changes will be applied when the revision is applied\n    permission (string): 'public' or 'private'\n\nReturns:\n    Revision The new revision, or an error\n\nExamples:\n    view.revisions.create_update_revision(metadata = {\n        'name': 'new dataset name',\n        'description': 'updated description'\n    })\ncreate_using_config\nArgSpec\n    Args: config\n\nCreate a revision for the given dataset.\nlist\nList all the revisions on the view\nReturns:\n    list[Revision]\n\nlookup\nArgSpec\n    Args: revision_seq\n\nLookup a revision within the view based on the sequence number\nArgs:\n    revision_seq (int): The sequence number of the revision to lookup\n\nReturns:\n    Revision The Revision resulting from this API call, or an error\n\nRevision\nArgSpec\n    Args: auth, response, parent\n\nA revision is a change to a dataset\napply\nArgSpec\n    Args: output_schema\n\nApply the Revision to the view that it was opened on\nArgs:\n    output_schema (OutputSchema): Optional output schema. If your revision includes\n        data changes, this should be included. If it is a metadata only revision,\n        then you will not have an output schema, and you do not need to pass anything\n        here\n\nReturns:\n    Job\n\nExamples:\njob = revision.apply(output_schema = my_output_schema)\n\ncreate_upload\nArgSpec\n    Args: filename, parse_options\n    Defaults: filename={}\n\nCreate an upload within this revision\nArgs:\n    filename (str): The name of the file to upload\n\nReturns:\n    Source: Returns the new Source The Source created by this API call, or an error\n\ndiscard\nDiscard this open revision.\nReturns:\n    Revision The closed Revision or an error\n\nlist_operations\nGet a list of the operations that you can perform on this\nobject. These map directly onto what's returned from the API\nin the links section of each resource\nopen_in_browser\nOpen this revision in your browser, this will open a window\nplan\nReturn the list of operations this revision will make when it is applied\nReturns:\n    dict\n\nset_output_schema\nArgSpec\n    Args: output_schema_id\n\nSet the output schema id on the revision. This is what will get applied when\nthe revision is applied if no ouput schema is explicitly supplied\nArgs:\n    output_schema_id (int): The output schema id\n\nReturns:\n    Revision The updated Revision as a result of this API call, or an error\n\nExamples:\n    revision = revision.set_output_schema(42)\nsource_as_blob\nArgSpec\n    Args: filename, parse_options\n    Defaults: filename={}\n\nCreate a source from a file that should remain unparsed\nsource_from_agent\nArgSpec\n    Args: agent_uid, namespace, path, parse_options, parameters\n    Defaults: agent_uid={}, namespace={}\n\nCreate a source from a connection agent in this revision\nsource_from_dataset\nArgSpec\n    Args: parse_options\n    Defaults: parse_options={}\n\nCreate a dataset source within this revision\nsource_from_url\nArgSpec\n    Args: url, parse_options\n    Defaults: url={}\n\nCreate a URL source\nArgs:\n    url (str): The URL to create the dataset from\n\nReturns:\n    Source: Returns the new Source The Source created by this API call, or an error\n\nui_url\nThis is the URL to the landing page in the UI for this revision\nReturns:\n    url (str): URL you can paste into a browser to view the revision UI\n\nupdate\nArgSpec\n    Args: body\n\nSet the metadata to be applied to the view\nwhen this revision is applied\nArgs:\n    body (dict): The changes to make to this revision\n\nReturns:\n    Revision The updated Revision as a result of this API call, or an error\n\nExamples:\n    revision = revision.update({\n        'metadata': {\n            'name': 'new name',\n            'description': 'new description'\n        }\n    })\nSources\nArgSpec\n    Args: auth\n\ncreate_upload\nArgSpec\n    Args: filename\n\nCreate a new source. Takes a body param, which must contain a filename\nof the file.\nArgs:\n    filename (str): The name of the file you are uploading\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    upload = revision.create_upload('foo.csv')\nlookup\nArgSpec\n    Args: source_id\n\nLookup a source\nArgs:\n    source_id (int): The id\n\nReturns:\n    Source: Returns the new Source The Source resulting from this API call, or an error\n\nSource\nArgSpec\n    Args: auth, response, parent\n\nadd_to_revision\nArgSpec\n    Args: revision\n\nAssociate this Source with the given revision.\nblob\nArgSpec\n    Args: file_handle\n\nUploads a Blob dataset. A blob is a file that will not be parsed as a data file,\nie: an image, video, etc.\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    with open('my-blob.jpg', 'rb') as f:\n        upload = upload.blob(f)\nchange_parse_option\nArgSpec\n    Args: name\n\nChange a parse option on the source.\nIf there are not yet bytes uploaded, these parse options will be used\nin order to parse the file.\nIf there are already bytes uploaded, this will trigger a re-parsing of\nthe file, and consequently a new InputSchema will be created. You can call\nsource.latest_input() to get the newest one.\nParse options are:\nheader_count (int): the number of rows considered a header\ncolumn_header (int): the one based index of row to use to generate the header\nencoding (string): defaults to guessing the encoding, but it can be explicitly set\ncolumn_separator (string): For CSVs, this defaults to \",\", and for TSVs \"       \", but you can use a custom separator\nquote_char (string): Character used to quote values that should be escaped. Defaults to \"\"\"\nArgs:\n    name (string): One of the options above, ie: \"column_separator\" or \"header_count\"\n\nReturns:\n    change (ParseOptionChange): implements a `.to(value)` function which you call to set the value\n\nFor our example, assume we have this dataset\nThis is my cool dataset\nA, B, C\n1, 2, 3\n4, 5, 6\n\nWe want to say that the first 2 rows are headers, and the second of those 2\nrows should be used to make the column header. We would do that like so:\nExamples:\n    source = source            .change_parse_option('header_count').to(2)            .change_parse_option('column_header').to(2)            .run()\ncsv\nArgSpec\n    Args: file_handle\n\nUpload a CSV, returns the new input schema.\nArgs:\n    file_handle: The file handle, as returned by the python function `open()`\n\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    with open('my-file.csv', 'rb') as f:\n        upload = upload.csv(f)\ndf\nArgSpec\n    Args: dataframe\n\nUpload a pandas DataFrame, returns the new source.\nArgs:\n    file_handle: The file handle, as returned by the python function `open()`\n\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    import pandas\n    df = pandas.read_csv('test\/fixtures\/simple.csv')\n    upload = upload.df(df)\ngeojson\nArgSpec\n    Args: file_handle\n\nUpload a geojson file, returns the new input schema.\nArgs:\n    file_handle: The file handle, as returned by the python function `open()`\n\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    with open('my-geojson-file.geojson', 'rb') as f:\n        upload = upload.geojson(f)\nkml\nArgSpec\n    Args: file_handle\n\nUpload a KML file, returns the new input schema.\nArgs:\n    file_handle: The file handle, as returned by the python function `open()`\n\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    with open('my-kml-file.kml', 'rb') as f:\n        upload = upload.kml(f)\nlist_operations\nGet a list of the operations that you can perform on this\nobject. These map directly onto what's returned from the API\nin the links section of each resource\nload\nForces the source to load, if it's a view source.\nReturns:\n    Source: Returns the new Source\n\nopen_in_browser\nOpen this source in your browser, this will open a window\nshapefile\nArgSpec\n    Args: file_handle\n\nUpload a Shapefile, returns the new input schema.\nArgs:\n    file_handle: The file handle, as returned by the python function `open()`\n\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    with open('my-shapefile-archive.zip', 'rb') as f:\n        upload = upload.shapefile(f)\ntsv\nArgSpec\n    Args: file_handle\n\nUpload a TSV, returns the new input schema.\nArgs:\n    file_handle: The file handle, as returned by the python function `open()`\n\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    with open('my-file.tsv', 'rb') as f:\n        upload = upload.tsv(f)\nui_url\nThis is the URL to the landing page in the UI for the sources\nReturns:\n    url (str): URL you can paste into a browser to view the source UI\n\nwait_for_finish\nArgSpec\n    Args: progress, timeout, sleeptime\n    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1\n\nWait for this dataset to finish transforming and validating. Accepts a progress function\nand a timeout.\nxls\nArgSpec\n    Args: file_handle\n\nUpload an XLS, returns the new input schema\nArgs:\n    file_handle: The file handle, as returned by the python function `open()`\n\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    with open('my-file.xls', 'rb') as f:\n        upload = upload.xls(f)\nxlsx\nArgSpec\n    Args: file_handle\n\nUpload an XLSX, returns the new input schema.\nArgs:\n    file_handle: The file handle, as returned by the python function `open()`\n\n    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.\n    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.\n\nReturns:\n    Source: Returns the new Source\n\nExamples:\n    with open('my-file.xlsx', 'rb') as f:\n        upload = upload.xlsx(f)\nConfigs\nArgSpec\n    Args: auth\n\ncreate\nArgSpec\n    Args: name, data_action, parse_options, columns\n\nCreate a new ImportConfig. See http:\/\/docs.socratapublishing.apiary.io\/\nImportConfig section for what is supported in data_action, parse_options,\nand columns.\nlist\nList all the ImportConfigs on this domain\nlookup\nArgSpec\n    Args: name\n\nObtain a single ImportConfig by name\nConfig\nArgSpec\n    Args: auth, response, parent\n\nchange_parse_option\nArgSpec\n    Args: name\n\nChange a parse option on the source.\nIf there are not yet bytes uploaded, these parse options will be used\nin order to parse the file.\nIf there are already bytes uploaded, this will trigger a re-parsing of\nthe file, and consequently a new InputSchema will be created. You can call\nsource.latest_input() to get the newest one.\nParse options are:\nheader_count (int): the number of rows considered a header\ncolumn_header (int): the one based index of row to use to generate the header\nencoding (string): defaults to guessing the encoding, but it can be explicitly set\ncolumn_separator (string): For CSVs, this defaults to \",\", and for TSVs \"       \", but you can use a custom separator\nquote_char (string): Character used to quote values that should be escaped. Defaults to \"\"\"\nArgs:\n    name (string): One of the options above, ie: \"column_separator\" or \"header_count\"\n\nReturns:\n    change (ParseOptionChange): implements a `.to(value)` function which you call to set the value\n\nFor our example, assume we have this dataset\nThis is my cool dataset\nA, B, C\n1, 2, 3\n4, 5, 6\n\nWe want to say that the first 2 rows are headers, and the second of those 2\nrows should be used to make the column header. We would do that like so:\nExamples:\n    source = source            .change_parse_option('header_count').to(2)            .change_parse_option('column_header').to(2)            .run()\ncreate_revision\nArgSpec\n    Args: fourfour\n\nCreate a new Revision in the context of this ImportConfig.\nSources that happen in this Revision will take on the values\nin this Config.\ndelete\nDelete this ImportConfig. Note that this cannot be undone.\nlist_operations\nGet a list of the operations that you can perform on this\nobject. These map directly onto what's returned from the API\nin the links section of each resource\nupdate\nArgSpec\n    Args: body\n\nMutate this ImportConfig in place. Subsequent revisions opened against this\nImportConfig will take on its new value.\nInputSchema\nArgSpec\n    Args: auth, response, parent\n\nThis represents a schema exactly as it appeared in the source\nget_latest_output_schema\nNote that this does not make an API request\nReturns:\noutput_schema (OutputSchema): Returns the latest output schema\nlatest_output\nGet the latest (most recently created) OutputSchema\nwhich descends from this InputSchema\nReturns:\nOutputSchema\nlist_operations\nGet a list of the operations that you can perform on this\nobject. These map directly onto what's returned from the API\nin the links section of each resource\ntransform\nArgSpec\n    Args: body\n\nTransform this InputSchema into an Output. Returns the\nnew OutputSchema. Note that this call is async - the data\nmay still be transforming even though the OutputSchema is\nreturned. See OutputSchema.wait_for_finish to block until\nthe\nOutputSchema\nThis is data as transformed from an InputSchema\nadd_column\nArgSpec\n    Args: field_name, display_name, transform_expr, description\n\nAdd a column\nArgs:\n    field_name (str): The column's field_name, must be unique\n    display_name (str): The columns display name\n    transform_expr (str): SoQL expression to evaluate and fill the column with data from\n    description (str): Optional column description\n\nReturns:\n    output_schema (OutputSchema): Returns self for easy chaining\n\nExamples:\nnew_output_schema = output\n    # Add a new column, which is computed from the `celsius` column\n    .add_column('fahrenheit', 'Degrees (Fahrenheit)', '(to_number(`celsius`) * (9 \/ 5)) + 32', 'the temperature in celsius')\n    # Add a new column, which is computed from the `celsius` column\n    .add_column('kelvin', 'Degrees (Kelvin)', '(to_number(`celsius`) + 273.15')\n    .run()\nbuild_config\nArgSpec\n    Args: name, data_action\n\nCreate a new ImportConfig from this OutputSchema. See the API\ndocs for what an ImportConfig is and why they're useful\nchange_column_metadata\nArgSpec\n    Args: field_name, attribute\n\nChange the column metadata. This returns a ColumnChange,\nwhich implements a .to function, which takes the new value to change to\nArgs:\n    field_name (str): The column to change\n    attribute (str): The attribute of the column to change\n\nReturns:\n    change (TransformChange): The transform change, which implements the `.to` function\n\nExamples:\n    new_output_schema = output\n        # Change the field_name of date to the_date\n        .change_column_metadata('date', 'field_name').to('the_date')\n        # Change the description of the celsius column\n        .change_column_metadata('celsius', 'description').to('the temperature in celsius')\n        # Change the display name of the celsius column\n        .change_column_metadata('celsius', 'display_name').to('Degrees (Celsius)')\n        .run()\nchange_column_transform\nArgSpec\n    Args: field_name\n\nChange the column transform. This returns a TransformChange,\nwhich implements a .to function, which takes a transform expression.\nArgs:\n    field_name (str): The column to change\n\nReturns:\n    change (TransformChange): The transform change, which implements the `.to` function\n\nExamples:\n    new_output_schema = output\n        .change_column_transform('the_date').to('to_fixed_timestamp(`date`)')\n        # Make the celsius column all numbers\n        .change_column_transform('celsius').to('to_number(`celsius`)')\n        # Add a new column, which is computed from the `celsius` column\n        .add_column('fahrenheit', 'Degrees (Fahrenheit)', '(to_number(`celsius`) * (9 \/ 5)) + 32', 'the temperature in celsius')\n        .run()\ndrop_column\nArgSpec\n    Args: field_name\n\nDrop the column\nArgs:\n    field_name (str): The column to drop\n\nReturns:\n    output_schema (OutputSchema): Returns self for easy chaining\n\nExamples:\n    new_output_schema = output\n        .drop_column('foo')\n        .run()\nlist_operations\nGet a list of the operations that you can perform on this\nobject. These map directly onto what's returned from the API\nin the links section of each resource\nrows\nArgSpec\n    Args: offset, limit\n    Defaults: offset=0, limit=500\n\nGet the rows for this OutputSchema. Acceps offset and limit params\nfor paging through the data.\nrun\nRun all adds, drops, and column changes.\nReturns:\n    OutputSchema\n\nExamples:\n    new_output_schema = output\n        # Change the field_name of date to the_date\n        .change_column_metadata('date', 'field_name').to('the_date')\n        # Change the description of the celsius column\n        .change_column_metadata('celsius', 'description').to('the temperature in celsius')\n        # Change the display name of the celsius column\n        .change_column_metadata('celsius', 'display_name').to('Degrees (Celsius)')\n        # Change the transform of the_date column to to_fixed_timestamp(`date`)\n        .change_column_transform('the_date').to('to_fixed_timestamp(`date`)')\n        # Make the celsius column all numbers\n        .change_column_transform('celsius').to('to_number(`celsius`)')\n        # Add a new column, which is computed from the `celsius` column\n        .add_column('fahrenheit', 'Degrees (Fahrenheit)', '(to_number(`celsius`) * (9 \/ 5)) + 32', 'the temperature in celsius')\n        .run()\nschema_errors\nArgSpec\n    Args: offset, limit\n    Defaults: offset=0, limit=500\n\nGet the errors that resulted in transforming into this output schema.\nAccepts offset and limit params\nschema_errors_csv\nGet the errors that results in transforming into this output schema\nas a CSV stream.\nNote that this returns a Reponse, where Reponse\nis a python requests Reponse object\nset_row_id\nArgSpec\n    Args: field_name\n\nSet the row id. Note you must call validate_row_id before doing this.\nArgs:\n    field_name (str): The column to set as the row id\n\nReturns:\n    OutputSchema\n\nExamples:\nnew_output_schema = output.set_row_id('the_id_column')\nvalidate_row_id\nArgSpec\n    Args: field_name\n\nSet the row id. Note you must call validate_row_id before doing this.\nArgs:\n    field_name (str): The column to validate as the row id\n\nReturns:\n    boolean\n\nwait_for_finish\nArgSpec\n    Args: progress, timeout, sleeptime\n    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1\n\nWait for this dataset to finish transforming and validating. Accepts a progress function\nand a timeout.\nJob\nArgSpec\n    Args: auth, response, parent\n\nis_complete\nHas this job finished or failed\nlist_operations\nGet a list of the operations that you can perform on this\nobject. These map directly onto what's returned from the API\nin the links section of each resource\nwait_for_finish\nArgSpec\n    Args: progress, timeout, sleeptime\n    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1\n\nWait for this dataset to finish transforming and validating. Accepts a progress function\nand a timeout.\n","67":"Data Enginner's Essential Commands\n\nLinux: Link\nPython: Link\nPySpark: Link\nAWS: Link\n\nEKS: Link\nEMR: Link\nS3: Link\n\n\nTerraform: Link\nGit: Link\nHelm: Link\n\nJupyterhub: Link\n\n\n\n\nWant to contribute?\n\n\nThe commands should not be copy-pasted from any source in bulk.\nOnly add those commands that you use more frequently but may be unknown to other developers.\n\nExample: pwd, ls e.t.c., are not allowed\n\nFollow the structure and don't forget to embed any reference links either in heading or command description.\n\nPut it inside a directory if applicable\nGive a proper heading\nUse markdown script for block code or inline code to embed commands\n\n\nIf the command heading is not sufficient to explain the uses, give 1 liner explanation with an example.\nI would be happy to accept your pull request even if you add one good command than adding ten not so good commands.\n\n\n\n","68":"Data Engineering Projects\n\nProject 1: Data Modeling with Postgres\nIn this project, we apply Data Modeling with Postgres and build an ETL pipeline using Python. A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Currently, they are collecting data in json format and the analytics team is particularly interested in understanding what songs users are listening to.\nLink: Data_Modeling_with_Postgres\nProject 2: Data Modeling with Cassandra\nIn this project, we apply Data Modeling with Cassandra and build an ETL pipeline using Python. We will build a Data Model around our queries that we want to get answers for.\nFor our use case we want below answers:\n\nGet details of a song that was herad on the music app history during a particular session.\nGet songs played by a user during particular session on music app.\nGet all users from the music app history who listened to a particular song.\n\nLink : Data_Modeling_with_Apache_Cassandra\nProject 3: Data Warehouse\nIn this project, we apply the Data Warehouse architectures we learnt and build a Data Warehouse on AWS cloud. We build an ETL pipeline to extract and transform data stored in json format in s3 buckets and move the data to Warehouse hosted on Amazon Redshift.\nUse Redshift IaC script - Redshift_IaC_README\nLink  - Data_Warehouse\nProject 4: Data Lake\nIn this project, we will build a Data Lake on AWS cloud using Spark and AWS EMR cluster. The data lake will serve as a Single Source of Truth for the Analytics Platform. We will write spark jobs to perform ELT operations that picks data from landing zone on S3 and transform and stores data on the S3 processed zone.\nLink: Data_Lake\nProject 5: Data Pipelines with Airflow\nIn this project, we will orchestrate our Data Pipeline workflow using an open-source Apache project called Apache Airflow. We will schedule our ETL jobs in Airflow, create project related custom plugins and operators and automate the pipeline execution.\nLink:  Airflow_Data_Pipelines\nCAPSTONE PROJECT\nUdacity provides their own crafted Capstone project with dataset that include data on immigration to the United States, and supplementary datasets that include data on airport codes, U.S. city demographics, and temperature data.\nI worked on my own open-ended project. Here is the link - goodreads_etl_pipeline\n","69":"NEW LIST 2020 - 2021: Machine-Learning \/ Deep-Learning \/ AI -Tutorials\nHi - Thanks for dropping by!\n\nI will be updating this tutorials site on a daily basis adding all relevant topcis, including latest researches papers from internet such as arxiv.org, BIORXIV - Specifically Neuroscience to name a few. \n\nMore importantly the applications of ML\/DL\/AI into industry areas such as Transportation, Medicine\/Healthcare etc. will be something I'll watch with keen interest and would love to share the same with you.\n\nFinally, it is YOUR help I will seek to make it more useful and less boring, so please do suggest\/comment\/contribute!\n\n\n\nIndex\n\n\ndeep-learning\n\nUBER | Pyro\nNetflix | VectorFlow\nPyTorch\ntensorflow\ntheano\nkeras\ncaffe\nTorch\/Lua\nMXNET\n\n\n\nscikit-learn\n\n\nstatistical-inference-scipy\n\n\npandas\n\n\nmatplotlib\n\n\nnumpy\n\n\npython-data\n\n\nkaggle-and-business-analyses\n\n\nspark\n\n\nmapreduce-python\n\n\namazon web services\n\n\ncommand lines\n\n\nmisc\n\n\nnotebook-installation\n\n\nCurated list of Deep Learning \/ AI blogs\n\n\ncredits\n\n\ncontributing\n\n\ncontact-info\n\n\nlicense\n\n\ndeep-learning\nIPython Notebook(s) and other programming tools such as Torch\/Lua\/D lang in demonstrating deep learning functionality.\nuber-pyro-probabalistic-tutorials\n\n\n\nAdditional PyRo tutorials:\n\npyro-examples\/full examples\npyro-examples\/Variational Autoencoders\npyro-examples\/Bayesian Regression\npyro-examples\/Deep Markov Model\npyro-examples\/AIR(Attend Infer Repeat)\npyro-examples\/Semi-Supervised VE\npyro-examples\/GMM\npyro-examples\/Gaussian Process\npyro-examples\/Bayesian Optimization\nFull Pyro Code\n\nnetflix-vectorflow-tutorials\n\n\n\n\nMNIST Example, running with Dlang\n\npytorch-tutorials\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\nBeginners\/Zakizhou\nLearning the basics of PyTorch from Facebook.\n\n\nIntermedia\/Quanvuong\nLearning the intermediate stuff about PyTorch of from Facebook.\n\n\nAdvanced\/Chsasank\nLearning the advanced stuff about PyTorch of from Facebook.\n\n\nLearning PyTorch by Examples - Numpy, Tensors and Autograd\nAt its core, PyTorch provides two main features an n-dimensional Tensor, similar to numpy but can run on GPUs AND automatic differentiation for building and training neural networks.\n\n\nPyTorch - Getting to know autograd.Variable, Gradient, Neural Network\nHere we start with ultimate basics of Tensors, wrap a Tensor with Variable module, play with nn.Module and implement forward and backward function.\n\n\n\ntensor-flow-tutorials\n\n\n\n\nAdditional TensorFlow tutorials:\n\npkmital\/tensorflow_tutorials\nnlintz\/TensorFlow-Tutorials\nalrojo\/tensorflow-tutorial\nBinRoot\/TensorFlow-Book\n\n\n\n\nNotebook\nDescription\n\n\n\n\ntsf-basics\nLearn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google.\n\n\ntsf-linear\nImplement linear regression in TensorFlow.\n\n\ntsf-logistic\nImplement logistic regression in TensorFlow.\n\n\ntsf-nn\nImplement nearest neighboars in TensorFlow.\n\n\ntsf-alex\nImplement AlexNet in TensorFlow.\n\n\ntsf-cnn\nImplement convolutional neural networks in TensorFlow.\n\n\ntsf-mlp\nImplement multilayer perceptrons in TensorFlow.\n\n\ntsf-rnn\nImplement recurrent neural networks in TensorFlow.\n\n\ntsf-gpu\nLearn about basic multi-GPU computation in TensorFlow.\n\n\ntsf-gviz\nLearn about graph visualization in TensorFlow.\n\n\ntsf-lviz\nLearn about loss visualization in TensorFlow.\n\n\n\ntensor-flow-exercises\n\n\n\nNotebook\nDescription\n\n\n\n\ntsf-not-mnist\nLearn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow.\n\n\ntsf-fully-connected\nProgressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow.\n\n\ntsf-regularization\nExplore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow.\n\n\ntsf-convolutions\nCreate convolutional neural networks in TensorFlow.\n\n\ntsf-word2vec\nTrain a skip-gram model over Text8 data in TensorFlow.\n\n\ntsf-lstm\nTrain a LSTM character model over Text8 data in TensorFlow.\n\n\n\n\n\n\n\ntheano-tutorials\n\n\n\nNotebook\nDescription\n\n\n\n\ntheano-intro\nIntro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.\n\n\ntheano-scan\nLearn scans, a mechanism to perform loops in a Theano graph.\n\n\ntheano-logistic\nImplement logistic regression in Theano.\n\n\ntheano-rnn\nImplement recurrent neural networks in Theano.\n\n\ntheano-mlp\nImplement multilayer perceptrons in Theano.\n\n\n\n\n\n\n\nkeras-tutorials\n\n\n\nNotebook\nDescription\n\n\n\n\nkeras\nKeras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano.\n\n\nsetup\nLearn about the tutorial goals and how to set up your Keras environment.\n\n\nintro-deep-learning-ann\nGet an intro to deep learning with Keras and Artificial Neural Networks (ANN).\n\n\nPerceptrons and Adaline\nImplement Peceptron and adaptive linear neurons.\n\n\nMLP and MNIST Data\nClassifying handwritten digits,implement MLP, train and debug ANN\n\n\ntheano\nLearn about Theano by working with weights matrices and gradients.\n\n\nkeras-otto\nLearn about Keras by looking at the Kaggle Otto challenge.\n\n\nann-mnist\nReview a simple implementation of ANN for MNIST using Keras.\n\n\nconv-nets\nLearn about Convolutional Neural Networks (CNNs) with Keras.\n\n\nconv-net-1\nRecognize handwritten digits from MNIST using Keras - Part 1.\n\n\nconv-net-2\nRecognize handwritten digits from MNIST using Keras - Part 2.\n\n\nkeras-models\nUse pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras.\n\n\nauto-encoders\nLearn about Autoencoders with Keras.\n\n\nrnn-lstm\nLearn about Recurrent Neural Networks (RNNs) with Keras.\n\n\nlstm-sentence-gen\nLearn about RNNs using Long Short Term Memory (LSTM) networks with Keras.\n\n\nnlp-deep-learning\nLearn about NLP using ANN (Artificial Neural Networks.\n\n\nhyperparamter-tuning\nHyperparamters tuning using keras-wrapper.scikit-learn\n\n\n\ndeep-learning-misc\n\n\n\nNotebook\nDescription\n\n\n\n\ndeep-dream\nCaffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images.\n\n\n\n\n\n\n\nscikit-learn\nIPython Notebook(s) demonstrating scikit-learn functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nintro\nIntro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.\n\n\nknn\nImplement k-nearest neighbors in scikit-learn.\n\n\nlinear-reg\nImplement linear regression in scikit-learn.\n\n\nsvm\nImplement support vector machine classifiers with and without kernels in scikit-learn.\n\n\nrandom-forest\nImplement random forest classifiers and regressors in scikit-learn.\n\n\nk-means\nImplement k-means clustering in scikit-learn.\n\n\npca\nImplement principal component analysis in scikit-learn.\n\n\ngmm\nImplement Gaussian mixture models in scikit-learn.\n\n\nvalidation\nImplement validation and model selection in scikit-learn.\n\n\n\n\n\n\n\nstatistical-inference-scipy\nIPython Notebook(s) demonstrating statistical inference with SciPy functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nscipy\nSciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.\n\n\neffect-size\nExplore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States.\n\n\nsampling\nExplore random sampling by analyzing the average weight of men and women in the United States using BRFSS data.\n\n\nhypothesis\nExplore hypothesis testing by analyzing the difference of first-born babies compared with others.\n\n\n\n\n\n\n\npandas\nIPython Notebook(s) demonstrating pandas functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\npandas\nSoftware library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series.\n\n\ngithub-data-wrangling\nLearn how to load, clean, merge, and feature engineer by analyzing GitHub data from the Viz repo.\n\n\nIntroduction-to-Pandas\nIntroduction to Pandas.\n\n\nIntroducing-Pandas-Objects\nLearn about Pandas objects.\n\n\nData Indexing and Selection\nLearn about data indexing and selection in Pandas.\n\n\nOperations-in-Pandas\nLearn about operating on data in Pandas.\n\n\nMissing-Values\nLearn about handling missing data in Pandas.\n\n\nHierarchical-Indexing\nLearn about hierarchical indexing in Pandas.\n\n\nConcat-And-Append\nLearn about combining datasets: concat and append in Pandas.\n\n\nMerge-and-Join\nLearn about combining datasets: merge and join in Pandas.\n\n\nAggregation-and-Grouping\nLearn about aggregation and grouping in Pandas.\n\n\nPivot-Tables\nLearn about pivot tables in Pandas.\n\n\nWorking-With-Strings\nLearn about vectorized string operations in Pandas.\n\n\nWorking-with-Time-Series\nLearn about working with time series in pandas.\n\n\nPerformance-Eval-and-Query\nLearn about high-performance Pandas: eval() and query() in Pandas.\n\n\n\n\n\n\n\nmatplotlib\nIPython Notebook(s) demonstrating matplotlib functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nmatplotlib\nPython 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n\n\nmatplotlib-applied\nApply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots.\n\n\nIntroduction-To-Matplotlib\nIntroduction to Matplotlib.\n\n\nSimple-Line-Plots\nLearn about simple line plots in Matplotlib.\n\n\nSimple-Scatter-Plots\nLearn about simple scatter plots in Matplotlib.\n\n\nErrorbars.ipynb\nLearn about visualizing errors in Matplotlib.\n\n\nDensity-and-Contour-Plots\nLearn about density and contour plots in Matplotlib.\n\n\nHistograms-and-Binnings\nLearn about histograms, binnings, and density in Matplotlib.\n\n\nCustomizing-Legends\nLearn about customizing plot legends in Matplotlib.\n\n\nCustomizing-Colorbars\nLearn about customizing colorbars in Matplotlib.\n\n\nMultiple-Subplots\nLearn about multiple subplots in Matplotlib.\n\n\nText-and-Annotation\nLearn about text and annotation in Matplotlib.\n\n\nCustomizing-Ticks\nLearn about customizing ticks in Matplotlib.\n\n\nSettings-and-Stylesheets\nLearn about customizing Matplotlib: configurations and stylesheets.\n\n\nThree-Dimensional-Plotting\nLearn about three-dimensional plotting in Matplotlib.\n\n\nGeographic-Data-With-Basemap\nLearn about geographic data with basemap in Matplotlib.\n\n\nVisualization-With-Seaborn\nLearn about visualization with Seaborn.\n\n\n\n\n\n\n\nnumpy\nIPython Notebook(s) demonstrating NumPy functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nnumpy\nAdds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.\n\n\nIntroduction-to-NumPy\nIntroduction to NumPy.\n\n\nUnderstanding-Data-Types\nLearn about data types in Python.\n\n\nThe-Basics-Of-NumPy-Arrays\nLearn about the basics of NumPy arrays.\n\n\nComputation-on-arrays-ufuncs\nLearn about computations on NumPy arrays: universal functions.\n\n\nComputation-on-arrays-aggregates\nLearn about aggregations: min, max, and everything in between in NumPy.\n\n\nComputation-on-arrays-broadcasting\nLearn about computation on arrays: broadcasting in NumPy.\n\n\nBoolean-Arrays-and-Masks\nLearn about comparisons, masks, and boolean logic in NumPy.\n\n\nFancy-Indexing\nLearn about fancy indexing in NumPy.\n\n\nSorting\nLearn about sorting arrays in NumPy.\n\n\nStructured-Data-NumPy\nLearn about structured data: NumPy's structured arrays.\n\n\n\n\n\n\n\npython-data\nIPython Notebook(s) demonstrating Python functionality geared towards data analysis.\n\n\n\nNotebook\nDescription\n\n\n\n\ndata structures\nLearn Python basics with tuples, lists, dicts, sets.\n\n\ndata structure utilities\nLearn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions.\n\n\nfunctions\nLearn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools.\n\n\ndatetime\nLearn how to work with Python dates and times: datetime, strftime, strptime, timedelta.\n\n\nlogging\nLearn about Python logging with RotatingFileHandler and TimedRotatingFileHandler.\n\n\npdb\nLearn how to debug in Python with the interactive source code debugger.\n\n\nunit tests\nLearn how to test in Python with Nose unit tests.\n\n\n\n\n\n\n\nkaggle-and-business-analyses\nIPython Notebook(s) used in kaggle competitions and business analyses.\n\n\n\nNotebook\nDescription\n\n\n\n\ntitanic\nPredict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning.\n\n\nchurn-analysis\nPredict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration\/descrimination.\n\n\n\n\n\n\n\nspark\nIPython Notebook(s) demonstrating spark and HDFS functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nspark\nIn-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms.\n\n\nhdfs\nReliably stores very large files across machines in a large cluster.\n\n\n\n\n\n\n\nmapreduce-python\nIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nmapreduce-python\nRuns MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and mrjob config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  Disco is another python-based alternative.\n\n\n\n\n\n\n\naws\nIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.\nAlso check out:\n\nSAWS: A Supercharged AWS command line interface (CLI).\nAwesome AWS: A curated list of libraries, open source repos, guides, blogs, and other resources.\n\n\n\n\nNotebook\nDescription\n\n\n\n\nboto\nOfficial AWS SDK for Python.\n\n\ns3cmd\nInteracts with S3 through the command line.\n\n\ns3distcp\nCombines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster.\n\n\ns3-parallel-put\nUploads multiple files to S3 in parallel.\n\n\nredshift\nActs as a fast data warehouse built on top of technology from massive parallel processing (MPP).\n\n\nkinesis\nStreams data in real time with the ability to process thousands of data streams per second.\n\n\nlambda\nRuns code in response to events, automatically managing compute resources.\n\n\n\n\n\n\n\ncommands\nIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.\n\n\n\nNotebook\nDescription\n\n\n\n\nlinux\nUnix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.\n\n\nanaconda\nDistribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment.\n\n\nipython notebook\nWeb-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document.\n\n\ngit\nDistributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows.\n\n\nruby\nUsed to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages.\n\n\njekyll\nSimple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server.\n\n\npelican\nPython-based alternative to Jekyll.\n\n\ndjango\nHigh-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports\/analyses and for blogging. Lighter-weight alternatives include Pyramid, Flask, Tornado, and Bottle.\n\n\n\nmisc\nIPython Notebook(s) demonstrating miscellaneous functionality.\n\n\n\nNotebook\nDescription\n\n\n\n\nregex\nRegular expression cheat sheet useful in data wrangling.\n\n\nalgorithmia\nAlgorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.\n\n\n\nnotebook-installation\nanaconda\nAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.\nFollow instructions to install Anaconda or the more lightweight miniconda.\ndev-setup\nFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the dev-setup repo.\nrunning-notebooks\nNote: If you intend to learn the hard way (preferred method)then I'd strongly advice to write as much code as you can yourself and not just run pre-written code. If you still want to test it, then do the following:\nTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found here.\n$ git clone https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials.git\n$ cd Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\n$ jupyter notebook\n\nNotebooks tested with Python 3.7+\ncurated-list-of-deeplearning-blogs\n\nA Blog From a Human-engineer-being http:\/\/www.erogol.com\/ (RSS)\nAakash Japi http:\/\/aakashjapi.com\/ (RSS)\nAdit Deshpande https:\/\/adeshpande3.github.io\/ (RSS)\nAdvanced Analytics & R http:\/\/advanceddataanalytics.net\/ (RSS)\nAdventures in Data Land http:\/\/blog.smola.org (RSS)\nAgile Data Science http:\/\/blog.sense.io\/ (RSS)\nAhmed El Deeb https:\/\/medium.com\/@D33B (RSS)\nAirbnb Data blog http:\/\/nerds.airbnb.com\/data\/ (RSS)\nAlex Castrounis | InnoArchiTech http:\/\/www.innoarchitech.com\/ (RSS)\nAlex Perrier http:\/\/alexperrier.github.io\/ (RSS)\nAlgobeans | Data Analytics Tutorials & Experiments for the Layman https:\/\/algobeans.com (RSS)\nAmazon AWS AI Blog https:\/\/aws.amazon.com\/blogs\/ai\/ (RSS)\nAnalytics Vidhya http:\/\/www.analyticsvidhya.com\/blog\/ (RSS)\nAnalytics and Visualization in Big Data @ Sicara https:\/\/blog.sicara.com (RSS)\nAndreas M\u00fcller http:\/\/peekaboo-vision.blogspot.com\/ (RSS)\nAndrej Karpathy blog http:\/\/karpathy.github.io\/ (RSS)\nAndrew Brooks http:\/\/brooksandrew.github.io\/simpleblog\/ (RSS)\nAndrey Kurenkov http:\/\/www.andreykurenkov.com\/writing\/ (RSS)\nAnton Lebedevich's Blog http:\/\/mabrek.github.io\/ (RSS)\nArthur Juliani https:\/\/medium.com\/@awjuliani (RSS)\nAudun M. \u00d8ygard http:\/\/www.auduno.com\/ (RSS)\nAvi Singh https:\/\/avisingh599.github.io\/ (RSS)\nBeautiful Data http:\/\/beautifuldata.net\/ (RSS)\nBeckerfuffle http:\/\/mdbecker.github.io\/ (RSS)\nBecoming A Data Scientist http:\/\/www.becomingadatascientist.com\/ (RSS)\nBen Bolte's Blog http:\/\/benjaminbolte.com\/ml\/ (RSS)\nBen Frederickson http:\/\/www.benfrederickson.com\/blog\/ (RSS)\nBerkeley AI Research http:\/\/bair.berkeley.edu\/blog\/ (RSS)\nBig-Ish Data http:\/\/bigishdata.com\/ (RSS)\nBlog on neural networks http:\/\/yerevann.github.io\/ (RSS)\nBlogistic RegressionAbout Projects http:\/\/d10genes.github.io\/blog\/ (RSS)\nblogR | R tips and tricks from a scientist https:\/\/drsimonj.svbtle.com\/ (RSS)\nBrain of mat kelcey http:\/\/matpalm.com\/blog\/ (RSS)\nBrilliantly wrong thoughts on science and programming https:\/\/arogozhnikov.github.io\/ (RSS)\nBugra Akyildiz http:\/\/bugra.github.io\/ (RSS)\nBuilding Babylon https:\/\/building-babylon.net\/ (RSS)\nCarl Shan http:\/\/carlshan.com\/ (RSS)\nChris Stucchio https:\/\/www.chrisstucchio.com\/blog\/index.html (RSS)\nChristophe Bourguignat https:\/\/medium.com\/@chris_bour (RSS)\nChristopher Nguyen https:\/\/medium.com\/@ctn (RSS)\nCloudera Data Science Posts http:\/\/blog.cloudera.com\/blog\/category\/data-science\/ (RSS)\ncolah's blog http:\/\/colah.github.io\/archive.html (RSS)\nCortana Intelligence and Machine Learning Blog https:\/\/blogs.technet.microsoft.com\/machinelearning\/ (RSS)\nDaniel Forsyth http:\/\/www.danielforsyth.me\/ (RSS)\nDaniel Homola http:\/\/danielhomola.com\/category\/blog\/ (RSS)\nDaniel Nee http:\/\/danielnee.com (RSS)\nData Based Inventions http:\/\/datalab.lu\/ (RSS)\nData Blogger https:\/\/www.data-blogger.com\/ (RSS)\nData Labs http:\/\/blog.insightdatalabs.com\/ (RSS)\nData Meets Media http:\/\/datameetsmedia.com\/ (RSS)\nData Miners Blog http:\/\/blog.data-miners.com\/ (RSS)\nData Mining Research http:\/\/www.dataminingblog.com\/ (RSS)\nData Mining: Text Mining, Visualization and Social Media http:\/\/datamining.typepad.com\/data_mining\/ (RSS)\nData Piques http:\/\/blog.ethanrosenthal.com\/ (RSS)\nData School http:\/\/www.dataschool.io\/ (RSS)\nData Science 101 http:\/\/101.datascience.community\/ (RSS)\nData Science @ Facebook https:\/\/research.facebook.com\/blog\/datascience\/ (RSS)\nData Science Insights http:\/\/www.datasciencebowl.com\/data-science-insights\/ (RSS)\nData Science Tutorials https:\/\/codementor.io\/data-science\/tutorial (RSS)\nData Science Vademecum http:\/\/datasciencevademecum.wordpress.com\/ (RSS)\nDataaspirant http:\/\/dataaspirant.com\/ (RSS)\nDataclysm http:\/\/blog.okcupid.com\/ (RSS)\nDataGenetics http:\/\/datagenetics.com\/blog.html (RSS)\nDataiku https:\/\/www.dataiku.com\/blog\/ (RSS)\nDataKind http:\/\/www.datakind.org\/blog (RSS)\nDataLook http:\/\/blog.datalook.io\/ (RSS)\nDatanice https:\/\/datanice.wordpress.com\/ (RSS)\nDataquest Blog https:\/\/www.dataquest.io\/blog\/ (RSS)\nDataRobot http:\/\/www.datarobot.com\/blog\/ (RSS)\nDatascope http:\/\/datascopeanalytics.com\/blog (RSS)\nDatasFrame http:\/\/tomaugspurger.github.io\/ (RSS)\nDavid Mimno http:\/\/www.mimno.org\/ (RSS)\nDayne Batten http:\/\/daynebatten.com (RSS)\nDeep Learning http:\/\/deeplearning.net\/blog\/ (RSS)\nDeepdish http:\/\/deepdish.io\/ (RSS)\nDelip Rao http:\/\/deliprao.com\/ (RSS)\nDENNY'S BLOG http:\/\/blog.dennybritz.com\/ (RSS)\nDimensionless https:\/\/dimensionless.in\/blog\/ (RSS)\nDistill http:\/\/distill.pub\/ (RSS)\nDistrict Data Labs http:\/\/districtdatalabs.silvrback.com\/ (RSS)\nDiving into data https:\/\/blog.datadive.net\/ (RSS)\nDomino Data Lab's blog http:\/\/blog.dominodatalab.com\/ (RSS)\nDr. Randal S. Olson http:\/\/www.randalolson.com\/blog\/ (RSS)\nDrew Conway https:\/\/medium.com\/@drewconway (RSS)\nDustin Tran http:\/\/dustintran.com\/blog\/ (RSS)\nEder Santana https:\/\/edersantana.github.io\/blog.html (RSS)\nEdwin Chen http:\/\/blog.echen.me (RSS)\nEFavDB http:\/\/efavdb.com\/ (RSS)\nEmilio Ferrara, Ph.D.  http:\/\/www.emilio.ferrara.name\/ (RSS)\nEntrepreneurial Geekiness http:\/\/ianozsvald.com\/ (RSS)\nEric Jonas http:\/\/ericjonas.com\/archives.html (RSS)\nEric Siegel http:\/\/www.predictiveanalyticsworld.com\/blog (RSS)\nErik Bern http:\/\/erikbern.com (RSS)\nERIN SHELLMAN http:\/\/www.erinshellman.com\/ (RSS)\nEugenio Culurciello http:\/\/culurciello.github.io\/ (RSS)\nFabian Pedregosa http:\/\/fa.bianp.net\/ (RSS)\nFast Forward Labs http:\/\/blog.fastforwardlabs.com\/ (RSS)\nFastML http:\/\/fastml.com\/ (RSS)\nFlorian Hartl http:\/\/florianhartl.com\/ (RSS)\nFlowingData http:\/\/flowingdata.com\/ (RSS)\nFull Stack ML http:\/\/fullstackml.com\/ (RSS)\nGAB41 http:\/\/www.lab41.org\/gab41\/ (RSS)\nGarbled Notes http:\/\/www.chioka.in\/ (RSS)\nGreg Reda http:\/\/www.gregreda.com\/blog\/ (RSS)\nHyon S Chu https:\/\/medium.com\/@adailyventure (RSS)\ni am trask http:\/\/iamtrask.github.io\/ (RSS)\nI Quant NY http:\/\/iquantny.tumblr.com\/ (RSS)\ninFERENCe http:\/\/www.inference.vc\/ (RSS)\nInsight Data Science https:\/\/blog.insightdatascience.com\/ (RSS)\nINSPIRATION INFORMATION http:\/\/myinspirationinformation.com\/ (RSS)\nIra Korshunova http:\/\/irakorshunova.github.io\/ (RSS)\nI\u2019m a bandit https:\/\/blogs.princeton.edu\/imabandit\/ (RSS)\nJason Toy http:\/\/www.jtoy.net\/ (RSS)\nJeremy D. Jackson, PhD http:\/\/www.jeremydjacksonphd.com\/ (RSS)\nJesse Steinweg-Woods https:\/\/jessesw.com\/ (RSS)\nJoe Cauteruccio http:\/\/www.joecjr.com\/ (RSS)\nJohn Myles White http:\/\/www.johnmyleswhite.com\/ (RSS)\nJohn's Soapbox http:\/\/joschu.github.io\/ (RSS)\nJonas Degrave http:\/\/317070.github.io\/ (RSS)\nJoy Of Data http:\/\/www.joyofdata.de\/blog\/ (RSS)\nJulia Evans http:\/\/jvns.ca\/ (RSS)\nKDnuggets http:\/\/www.kdnuggets.com\/ (RSS)\nKeeping Up With The Latest Techniques http:\/\/colinpriest.com\/ (RSS)\nKenny Bastani http:\/\/www.kennybastani.com\/ (RSS)\nKevin Davenport http:\/\/kldavenport.com\/ (RSS)\nkevin frans http:\/\/kvfrans.com\/ (RSS)\nkorbonits | Math \u2229 Data http:\/\/korbonits.github.io\/ (RSS)\nLarge Scale Machine Learning  http:\/\/bickson.blogspot.com\/ (RSS)\nLATERAL BLOG https:\/\/blog.lateral.io\/ (RSS)\nLazy Programmer http:\/\/lazyprogrammer.me\/ (RSS)\nLearn Analytics Here https:\/\/learnanalyticshere.wordpress.com\/ (RSS)\nLearnDataSci http:\/\/www.learndatasci.com\/ (RSS)\nLearning With Data http:\/\/learningwithdata.com\/ (RSS)\nLife, Language, Learning http:\/\/daoudclarke.github.io\/ (RSS)\nLocke Data https:\/\/itsalocke.com\/blog\/ (RSS)\nLouis Dorard http:\/\/www.louisdorard.com\/blog\/ (RSS)\nM.E.Driscoll http:\/\/medriscoll.com\/ (RSS)\nMachinalis http:\/\/www.machinalis.com\/blog (RSS)\nMachine Learning (Theory) http:\/\/hunch.net\/ (RSS)\nMachine Learning and Data Science http:\/\/alexhwoods.com\/blog\/ (RSS)\nMachine Learning https:\/\/charlesmartin14.wordpress.com\/ (RSS)\nMachine Learning Mastery http:\/\/machinelearningmastery.com\/blog\/ (RSS)\nMachine Learning Blogs https:\/\/machinelearningblogs.com\/ (RSS)\nMachine Learning, etc http:\/\/yaroslavvb.blogspot.com (RSS)\nMachine Learning, Maths and Physics https:\/\/mlopezm.wordpress.com\/ (RSS)\nMachine Learning Flashcards https:\/\/machinelearningflashcards.com\/ $10, but a nicely illustrated set of 300 flash cards\nMachined Learnings http:\/\/www.machinedlearnings.com\/ (RSS)\nMAPPING BABEL https:\/\/jack-clark.net\/ (RSS)\nMAPR Blog https:\/\/www.mapr.com\/blog (RSS)\nMAREK REI http:\/\/www.marekrei.com\/blog\/ (RSS)\nMARGINALLY INTERESTING http:\/\/blog.mikiobraun.de\/ (RSS)\nMath \u2229 Programming http:\/\/jeremykun.com\/ (RSS)\nMatthew Rocklin http:\/\/matthewrocklin.com\/blog\/ (RSS)\nMelody Wolk http:\/\/melodywolk.com\/projects\/ (RSS)\nMic Farris http:\/\/www.micfarris.com\/ (RSS)\nMike Tyka http:\/\/mtyka.github.io\/ (RSS)\nminimaxir | Max Woolf's Blog http:\/\/minimaxir.com\/ (RSS)\nMirror Image https:\/\/mirror2image.wordpress.com\/ (RSS)\nMitch Crowe http:\/\/www.dataphoric.com\/ (RSS)\nMLWave http:\/\/mlwave.com\/ (RSS)\nMLWhiz http:\/\/mlwhiz.com\/ (RSS)\nModels are illuminating and wrong https:\/\/peadarcoyle.wordpress.com\/ (RSS)\nMoody Rd http:\/\/blog.mrtz.org\/ (RSS)\nMoonshots http:\/\/jxieeducation.com\/ (RSS)\nMourad Mourafiq http:\/\/mourafiq.com\/ (RSS)\nMy thoughts on Data science, predictive analytics, Python http:\/\/shahramabyari.com\/ (RSS)\nNatural language processing blog http:\/\/nlpers.blogspot.fr\/ (RSS)\nNeil Lawrence http:\/\/inverseprobability.com\/blog.html (RSS)\nNLP and Deep Learning enthusiast http:\/\/camron.xyz\/ (RSS)\nno free hunch http:\/\/blog.kaggle.com\/ (RSS)\nNuit Blanche http:\/\/nuit-blanche.blogspot.com\/ (RSS)\nNumber 2147483647 https:\/\/no2147483647.wordpress.com\/ (RSS)\nOn Machine Intelligence https:\/\/aimatters.wordpress.com\/ (RSS)\nOpiate for the masses Data is our religion. http:\/\/opiateforthemass.es\/ (RSS)\np-value.info http:\/\/www.p-value.info\/ (RSS)\nPete Warden's blog http:\/\/petewarden.com\/ (RSS)\nPlotly Blog http:\/\/blog.plot.ly\/ (RSS)\nProbably Overthinking It http:\/\/allendowney.blogspot.ca\/ (RSS)\nProoffreader.com http:\/\/www.prooffreader.com (RSS)\nProoffreaderPlus http:\/\/prooffreaderplus.blogspot.ca\/ (RSS)\nPublishable Stuff http:\/\/www.sumsar.net\/ (RSS)\nPyImageSearch http:\/\/www.pyimagesearch.com\/ (RSS)\nPythonic Perambulations https:\/\/jakevdp.github.io\/ (RSS)\nquintuitive http:\/\/quintuitive.com\/ (RSS)\nR and Data Mining https:\/\/rdatamining.wordpress.com\/ (RSS)\nR-bloggers http:\/\/www.r-bloggers.com\/ (RSS)\nR2RT http:\/\/r2rt.com\/ (RSS)\nRamiro G\u00f3mez http:\/\/ramiro.org\/notebooks\/ (RSS)\nRandom notes on Computer Science, Mathematics and Software Engineering http:\/\/barmaley-exe.github.io\/ (RSS)\nRandy Zwitch http:\/\/randyzwitch.com\/ (RSS)\nRaRe Technologies http:\/\/rare-technologies.com\/blog\/ (RSS)\nRayli.Net http:\/\/rayli.net\/blog\/ (RSS)\nRevolutions http:\/\/blog.revolutionanalytics.com\/ (RSS)\nRinu Boney http:\/\/rinuboney.github.io\/ (RSS)\nRNDuja Blog http:\/\/rnduja.github.io\/ (RSS)\nRobert Chang https:\/\/medium.com\/@rchang (RSS)\nRocket-Powered Data Science http:\/\/rocketdatascience.org (RSS)\nSachin Joglekar's blog https:\/\/codesachin.wordpress.com\/ (RSS)\nsamim https:\/\/medium.com\/@samim (RSS)\nSean J. Taylor http:\/\/seanjtaylor.com\/ (RSS)\nSebastian Raschka http:\/\/sebastianraschka.com\/blog\/index.html (RSS)\nSebastian Ruder http:\/\/sebastianruder.com\/ (RSS)\nSebastian's slow blog http:\/\/www.nowozin.net\/sebastian\/blog\/ (RSS)\nSFL Scientific Blog https:\/\/sflscientific.com\/blog\/ (RSS)\nShakir's Machine Learning Blog http:\/\/blog.shakirm.com\/ (RSS)\nSimply Statistics http:\/\/simplystatistics.org (RSS)\nSpringboard Blog http:\/\/springboard.com\/blog\nStartup.ML Blog http:\/\/startup.ml\/blog (RSS)\nStatistical Modeling, Causal Inference, and Social Science http:\/\/andrewgelman.com\/ (RSS)\nStigler Diet http:\/\/stiglerdiet.com\/ (RSS)\nStitch Fix Tech Blog http:\/\/multithreaded.stitchfix.com\/blog\/ (RSS)\nStochastic R&D Notes http:\/\/arseny.info\/ (RSS)\nStorytelling with Statistics on Quora http:\/\/datastories.quora.com\/ (RSS)\nStreamHacker http:\/\/streamhacker.com\/ (RSS)\nSubconscious Musings http:\/\/blogs.sas.com\/content\/subconsciousmusings\/ (RSS)\nSwan Intelligence http:\/\/swanintelligence.com\/ (RSS)\nTechnoCalifornia http:\/\/technocalifornia.blogspot.se\/ (RSS)\nTEXT ANALYSIS BLOG | AYLIEN http:\/\/blog.aylien.com\/ (RSS)\nThe Angry Statistician http:\/\/angrystatistician.blogspot.com\/ (RSS)\nThe Clever Machine https:\/\/theclevermachine.wordpress.com\/ (RSS)\nThe Data Camp Blog https:\/\/www.datacamp.com\/community\/blog (RSS)\nThe Data Incubator http:\/\/blog.thedataincubator.com\/ (RSS)\nThe Data Science Lab https:\/\/datasciencelab.wordpress.com\/ (RSS)\nTHE ETZ-FILES http:\/\/alexanderetz.com\/ (RSS)\nThe Science of Data http:\/\/www.martingoodson.com (RSS)\nThe Shape of Data https:\/\/shapeofdata.wordpress.com (RSS)\nThe unofficial Google data science Blog http:\/\/www.unofficialgoogledatascience.com\/ (RSS)\nTim Dettmers http:\/\/timdettmers.com\/ (RSS)\nTombone's Computer Vision Blog http:\/\/www.computervisionblog.com\/ (RSS)\nTommy Blanchard http:\/\/tommyblanchard.com\/category\/projects (RSS)\nTrevor Stephens http:\/\/trevorstephens.com\/ (RSS)\nTrey Causey http:\/\/treycausey.com\/ (RSS)\nUW Data Science Blog http:\/\/datasciencedegree.wisconsin.edu\/blog\/ (RSS)\nWellecks http:\/\/wellecks.wordpress.com\/ (RSS)\nWes McKinney http:\/\/wesmckinney.com\/archives.html (RSS)\nWhile My MCMC Gently Samples http:\/\/twiecki.github.io\/ (RSS)\nWildML http:\/\/www.wildml.com\/ (RSS)\nWill do stuff for stuff http:\/\/rinzewind.org\/blog-en (RSS)\nWill wolf http:\/\/willwolf.io\/ (RSS)\nWILL'S NOISE http:\/\/www.willmcginnis.com\/ (RSS)\nWilliam Lyon http:\/\/www.lyonwj.com\/ (RSS)\nWin-Vector Blog http:\/\/www.win-vector.com\/blog\/ (RSS)\nYanir Seroussi http:\/\/yanirseroussi.com\/ (RSS)\nZac Stewart http:\/\/zacstewart.com\/ (RSS)\n\u0177hat http:\/\/blog.yhat.com\/ (RSS)\n\u211auantitative \u221aourney http:\/\/outlace.com\/ (RSS)\n\u5927\u30c8\u30ed http:\/\/blog.otoro.net\/ (RSS)\n\ncredits\n\nPython for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython by Wes McKinney\nPyCon 2015 Scikit-learn Tutorial by Jake VanderPlas\nPython Data Science Handbook by Jake VanderPlas\nParallel Machine Learning with scikit-learn and IPython by Olivier Grisel\nStatistical Interference Using Computational Methods in Python by Allen Downey\nTensorFlow Examples by Aymeric Damien\nTensorFlow Tutorials by Parag K Mital\nTensorFlow Tutorials by Nathan Lintz\nTensorFlow Tutorials by Alexander R Johansen\nTensorFlow Book by Nishant Shukla\nSummer School 2015 by mila-udem\nKeras tutorials by Valerio Maggio\nKaggle\nYhat Blog\n\ncontributing\nContributions are welcome!  For bug reports or requests please submit an issue.\ncontact-info\nFeel free to contact me to discuss any issues, questions, or comments.\n\nEmail: tarry.singh@gmail.com\nTwitter: @tarrysingh\nGitHub: tarrysingh\nLinkedIn: Tarry Singh\nWebsite: tarrysingh.com\nMedium: tarry@Medium\nQuora : Answers from Tarry on Quora\n\nlicense\nThis repository contains a variety of content; some developed by Tarry Singh and some from third-parties and a lot will be maintained by me. The third-party content is distributed under the license provided by those parties.\nThe content was originally developed by Donne Martin is distributed under the following license. I will be maintaining and revamping it by adding PyTorch, Torch\/Lua, MXNET and much more:\nI am providing code and resources in this repository to you under an open source license.\nCopyright 2017 Tarry Singh\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","70":"Artificial Intelligence Nanodegree Program Resources\nClassroom Exercises\n1. Constraint Satisfaction Problems\nIn this exercise you will explore Constraint Satisfaction Problems in a Jupyter notebook and use a CSP solver to solve a variety of problems.\nRead more here\n2. Classical Search for PacMan (only in classroom)\nPlease DO NOT publish your work on this exercise.\nIn this exercise you will teach Pac-Man to search his world to complete the following tasks:\n\nfind a single obstacle\nfind multiple obstacles\nfind the fastest way to eat all the food in the map\n\n3. Local Search Optimization\nIn this exercise, you'll implement several local search algorithms and test them on the Traveling Salesman Problem (TSP) between a few dozen US state capitals.\nProjects\n1. Sudoku Solver\nIn this project, you will extend the Sudoku-solving agent developed in the classroom lectures to solve diagonal Sudoku puzzles and implement a new constraint strategy called \"naked twins\". A diagonal Sudoku puzzle is identical to traditional Sudoku puzzles with the added constraint that the boxes on the two main diagonals of the board must also contain the digits 1-9 in each cell (just like the rows, columns, and 3x3 blocks).\nRead more here\n2. Classical Planning\nThis project is split between implementation and analysis. First you will combine symbolic logic and classical search to implement an agent that performs progression search to solve planning problems. Then you will experiment with different search algorithms and heuristics, and use the results to answer questions about designing planning systems.\nRead more here\n3. Game Playing\nIn this project you will choose an experiment with adversarial game-playing techniques like minimax, Monte Carlo tree search, opening books, and more. Your goal will be to build and evaluate the performance of your agent in a finite deterministic two player game of perfect information called Isolation.\nRead more here\n4. Part of Speech Tagger\nIn this notebook, you'll use the Pomegranate library to build a hidden Markov model for part of speech tagging with a universal tagset. Hidden Markov models have been able to achieve >96% tag accuracy with larger tagsets on realistic text corpora. Hidden Markov models have also been used for speech recognition and speech generation, machine translation, gene recognition for bioinformatics, and human gesture recognition for computer vision, and more.\nRead more here\n","71":"   \n \n\n\nMycroft\nMycroft is a hackable open source voice assistant.\nTable of Contents\n\nTable of Contents\nGetting Started\nRunning Mycroft\nUsing Mycroft\n\nHome Device and Account Manager\nSkills\n\n\nBehind the scenes\n\nPairing Information\nConfiguration\nUsing Mycroft Without Home\nAPI Key Services\nUsing Mycroft behind a proxy\n\nUsing Mycroft behind a proxy without authentication\nUsing Mycroft behind an authenticated proxy\n\n\n\n\nGetting Involved\nLinks\n\nGetting Started\nFirst, get the code on your system!  The simplest method is via git (git installation instructions):\n\ncd ~\/\ngit clone https:\/\/github.com\/MycroftAI\/mycroft-core.git\ncd mycroft-core\nbash dev_setup.sh\n\nThis script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu\/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.\nNOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.\nRunning Mycroft\nMycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:\n\ncd ~\/mycroft-core\n.\/start-mycroft.sh debug\n\nThe \"debug\" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run .\/start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using .\/start-mycroft.sh cli.\nThe background services can be stopped as a group with:\n\n.\/stop-mycroft.sh\n\nUsing Mycroft\nHome Device and Account Manager\nMycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https:\/\/home.mycroft.ai\nBy default, mycroft-core  is configured to use Home. By saying \"Hey Mycroft, pair my device\" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can enter into the pairing page within the Mycroft Home site.\nOnce paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.\nSkills\nMycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your \/opt\/mycroft\/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!\nBehind the scenes\nPairing Information\nPairing information generated by registering with Home is stored in:\n~\/.mycroft\/identity\/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!\nConfiguration\nMycroft configuration consists of 4 possible locations:\n\nmycroft-core\/mycroft\/configuration\/mycroft.conf(Defaults)\nMycroft Home (Remote)\n\/etc\/mycroft\/mycroft.conf(Machine)\n$HOME\/.mycroft\/mycroft.conf(User)\n\nWhen the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.\nUsing Mycroft Without Home\nIf you do not wish to use the Mycroft Home service, before starting Mycroft for the first time, create $HOME\/.mycroft\/mycroft.conf with the following contents:\n{\n  \"skills\": {\n    \"blacklisted_skills\": [\n      \"mycroft-configuration.mycroftai\",\n      \"mycroft-pairing.mycroftai\"\n    ]\n  }\n}\n\nMycroft will then be unable to perform speech-to-text conversion, so you'll need to set that up as well, using one of the STT engines Mycroft supports.\nYou may insert your own API keys into the configuration files listed above in Configuration.  For example, to insert the API key for the Weather skill, create a new JSON key in the configuration file like so:\n{\n  \/\/ other configuration settings...\n  \/\/\n  \"WeatherSkill\": {\n    \"api_key\": \"<insert your API key here>\"\n  }\n}\n\nAPI Key Services\nThese are the keys currently used in Mycroft Core:\n\nSTT API, Google STT, Google Cloud Speech\nWeather Skill API, OpenWeatherMap\nWolfram-Alpha Skill\n\nUsing Mycroft behind a proxy\nMany schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.\nIf you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.\nNOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.\nUsing Mycroft behind a proxy without authentication\nIf you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).\n$ export http_proxy=http:\/\/proxy_hostname.com:proxy_port\n$ export https_port=http:\/\/proxy_hostname.com:proxy_port\n$ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"\nUsing Mycroft behind an authenticated proxy\nIf  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).\n$ export http_proxy=http:\/\/user:password@proxy_hostname.com:proxy_port\n$ export https_port=http:\/\/user:password@proxy_hostname.com:proxy_port\n$ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"\nGetting Involved\nThis is an open source project and we would love your help. We have prepared a contributing guide to help you get started.\nIf this is your first PR or you're not sure where to get started,\nsay hi in Mycroft Chat and a team member would be happy to mentor you.\nJoin the Mycroft Forum for questions and answers.\nLinks\n\nCreating a Skill\nDocumentation\nSkill Writer API Docs\nRelease Notes\nMycroft Chat\nMycroft Forum\nMycroft Blog\n\n","72":"Artificial Intelligence with Python\nThis is the code repository for Artificial Intelligence with Python, published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.\nAbout the Book\nDuring the course of this book, you will find out how to make informed decisions about what algorithms to use in a given context. Starting from the basics of Artificial Intelligence, you will learn how to develop various building blocks using different data mining techniques. You will see how to implement different algorithms to get the best possible results, and will understand how to apply them to real-world scenarios. If you want to add an intelligence layer to any application that\u2019s based on images, text, stock market, or some other form of data, this exciting book on Artificial Intelligence will definitely be your guide!\n##Instructions and Navigation\nAll of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter02.\nThe code will look like the following:\nA block of code is set as follows:\n\n[default]\nexten => s,1,Dial(Zap\/1|30)\nexten => s,2,Voicemail(u100)\nexten => s,102,Voicemail(b100)\nexten => i,1,Voicemail(s0)\n\nThis book is focused on artificial intelligence in Python as opposed to the Python itself. We have used Python 3 to build various applications. We focus on how to utilize various Python libraries in the best possible way to build real world applications. In that spirit, we have tried to keep all of the code as friendly and readable as possible. We feel that this will enable our readers to easily understand the code and readily use it in different scenarios.\nRelated Products\n\n\nDeep Learning with Python [Video]\n\n\nLearning IPython for Interactive Computing and Data Visualization\n\n\nPython High Performance Programming\n\n\nSuggestions and Feedback\nClick here if you have any feedback or suggestions.\n","73":"Artificial Intelligence with Python\nThis is the code repository for Artificial Intelligence with Python, published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.\nAbout the Book\nDuring the course of this book, you will find out how to make informed decisions about what algorithms to use in a given context. Starting from the basics of Artificial Intelligence, you will learn how to develop various building blocks using different data mining techniques. You will see how to implement different algorithms to get the best possible results, and will understand how to apply them to real-world scenarios. If you want to add an intelligence layer to any application that\u2019s based on images, text, stock market, or some other form of data, this exciting book on Artificial Intelligence will definitely be your guide!\n##Instructions and Navigation\nAll of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter02.\nThe code will look like the following:\nA block of code is set as follows:\n\n[default]\nexten => s,1,Dial(Zap\/1|30)\nexten => s,2,Voicemail(u100)\nexten => s,102,Voicemail(b100)\nexten => i,1,Voicemail(s0)\n\nThis book is focused on artificial intelligence in Python as opposed to the Python itself. We have used Python 3 to build various applications. We focus on how to utilize various Python libraries in the best possible way to build real world applications. In that spirit, we have tried to keep all of the code as friendly and readable as possible. We feel that this will enable our readers to easily understand the code and readily use it in different scenarios.\nRelated Products\n\n\nDeep Learning with Python [Video]\n\n\nLearning IPython for Interactive Computing and Data Visualization\n\n\nPython High Performance Programming\n\n\nSuggestions and Feedback\nClick here if you have any feedback or suggestions.\n","74":"\nSimple AI\nProject home: http:\/\/github.com\/simpleai-team\/simpleai\nThis lib implements many of the artificial intelligence algorithms described on the book \"Artificial Intelligence, a Modern Approach\", from Stuart Russel and Peter Norvig. We strongly recommend you to read the book, or at least the introductory chapters and the ones related to the components you want to use, because we won't explain the algorithms here.\nThis implementation takes some of the ideas from the Norvig's implementation (the aima-python lib), but it's made with a more \"pythonic\" approach, and more emphasis on creating a stable, modern, and maintainable version. We are testing the majority of the lib, it's available via pip install, has a standard repo and lib architecture, well documented, respects the python pep8 guidelines, provides only working code (no placeholders for future things), etc. Even the internal code is written with readability in mind, not only the external API.\nAt this moment, the implementation includes:\n\n\nSearch\n\nTraditional search algorithms (not informed and informed)\nLocal Search algorithms\nConstraint Satisfaction Problems algorithms\nInteractive execution viewers for search algorithms (web-based and terminal-based)\n\n\n\n\n\nMachine Learning\n\nStatistical Classification\n\n\n\n\n\n\nInstallation\nJust get it:\npip install simpleai\n\nAnd if you want to use the interactive search viewers, also install:\npip install pydot flask\n\nYou will need to have pip installed on your system. On linux install the\npython-pip package, on windows follow this.\nAlso, if you are on linux and not working with a virtualenv, remember to use\nsudo for both commands (sudo pip install ...).\n\nExamples\nSimple AI allows you to define problems and look for the solution with\ndifferent strategies. Another samples are in the samples directory, but\nhere is an easy one.\nThis problem tries to create the string \"HELLO WORLD\" using the A* algorithm:\nfrom simpleai.search import SearchProblem, astar\n\nGOAL = 'HELLO WORLD'\n\n\nclass HelloProblem(SearchProblem):\n    def actions(self, state):\n        if len(state) < len(GOAL):\n            return list(' ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n        else:\n            return []\n\n    def result(self, state, action):\n        return state + action\n\n    def is_goal(self, state):\n        return state == GOAL\n\n    def heuristic(self, state):\n        # how far are we from the goal?\n        wrong = sum([1 if state[i] != GOAL[i] else 0\n                    for i in range(len(state))])\n        missing = len(GOAL) - len(state)\n        return wrong + missing\n\nproblem = HelloProblem(initial_state='')\nresult = astar(problem)\n\nprint(result.state)\nprint(result.path())\n\nMore detailed documentation\nYou can read the docs online here. Or for offline access, you can clone the project code repository and read them from the docs folder.\n\nHelp and discussion\nJoin us at the Simple AI google group.\n\nAuthors\n\nMany people you can find on the contributors section.\nSpecial acknowledgements to Machinalis for the time provided to work on this project. Machinalis also works on some other very interesting projects, like Quepy and more.\n\n","75":"Snake\n  \nThe project focuses on the artificial intelligence of the Snake game. The snake's goal is to eat the food continuously and fill the map with its bodies as soon as possible. Originally, the project was written in C++. It has now been rewritten in Python for a user-friendly GUI and the simplicity in algorithm implementations.\nAlgorithms >\nExperiments\nWe use two metrics to evaluate the performance of an AI:\n\nAverage Length: Average length the snake has grown to (max: 64).\nAverage Steps: Average steps the snake has moved.\n\nTest results (averaged over 1000 episodes):\n\n\n\nSolver\nDemo (optimal)\nAverage Length\nAverage Steps\n\n\n\n\nHamilton\n\n63.93\n717.83\n\n\nGreedy\n\n60.15\n904.56\n\n\nDQN(experimental)\n\n24.44\n131.69\n\n\n\nInstallation\nRequirements: Python 3.5+ (64-bit) with Tkinter installed.\n$ pip3 install -r requirements.txt\n\n# Use -h for more details\n$ python3 run.py [-h]\n\nRun unit tests:\n$ python3 -m pytest -v\n\nLicense\nSee the LICENSE file for license rights and limitations.\n","76":"\n\n\n\n\n\nNon-tech crash course into Opera\u00e7\u00e3o Serenata de Amor\nTech crash course into Opera\u00e7\u00e3o Serenata de Amor\nContributing with code and tech skills\nSupporting\nAcknowledgments\n\nNon-tech crash course into Opera\u00e7\u00e3o Serenata de Amor\nWhat\nSerenata de Amor is an open project using artificial intelligence for social control of public administration.\nWho\nWe are a group of people who believes in power to the people motto. We are also part of the Data Science for Civic Innovation Programme from Open Knowledge Brasil.\nAmong founders and long-term members, we can list a group of eight people \u2013 plus numerous contributors from the open source and open knowledge communities:  Tatiana Balachova, Felipe Cabral, Eduardo Cuducos,  Irio Musskopf, Bruno Pazzim, Ana Schwendler, Jessica Temporal, Yasodara C\u00f3rdova and Pedro Vilanova.\nHow\nSimilar to organizations like Google, Facebook, and Netflix, we use technology to track government spendings and make open data accessible for everyone. We started looking into data from the Chamber of Deputies (Brazilian lower house) but we expanded to the Federal Senate (Brazilian upper house) and to municipalities.\nWhen\nIrio had the main ideas for the project in early 2016. For a few months, he experimented and gathered people around the project. September, 2016 marks the launching of our first crowd funding. Since then, we have been creating open source technological products and tools, as well as high quality content on civic tech on our Facebook and Medium.\nWhere\nWe have no non-virtual headquarters, but we work remotely everyday. Most of our ideas are crafted to work in any country that offers open data, but our main implementations focus in Brazil.\nWhy\nEmpowering citizens with data is important: people talk about smart cities, surveillance and privacy. We prefer to focus on smart citizens, accountability and open knowledge.\nTech crash course into Opera\u00e7\u00e3o Serenata de Amor\nWhat\nSerenata de Amor develops open source tools to make it easy for people to use open data. The focus is to gather relevant insights and share them in an accessible interface. Through this interface, we invite citizens to dialogue with politicians, state and government about public spendings.\nWho\nSerenata's main role is played by Rosie: she is an artificial intelligence who analyzes Brazilian congresspeople expenses while they are in office. Rosie can find suspicious spendings and engage citizens in the discussion about these findings. She's on Twitter.\nTo allow people to visualize and make sense of data Rosie generates, we have created Jarbas. On this website, users can browse congresspeople expenses and get details about each of the suspicions. It is the starting point to validate a suspicion.\nHow\nWe have two main repositories on GitHub. This is the main repo and hosts Rosie and Jarbas. In addition, we have the toolbox - a pip installable package. Yet there are experimental notebooks maintained by the community and our static webpage.\nWhen\nDespite all these players acting together, the core part of the job is ran manually from time to time. The only part that is always online is Jarbas \u2013 freely serving a wide range of information about public expenditure 24\/7.\nRoughly once a month, we manually run Rosie and update Jarbas. A few times per year, we upload versioned datasets accessible via the toolbox \u2013 but we encourage you to use the toolbox to generate fresh datasets whenever you need.\nWhere\nJarbas is running in Digital Ocean droplets, and deployed using the Docker Cloud architecture.\nWhy\nThe answer to most technical why questions is because that is what we had in the past and enabled us to deliver fast. We acknowledge that this is not the best stack ever, but it has brought us here.\nContributing with code and tech skills\nMake sure you have read the Tech crash course on this page. Next, check out our contributing guide.\nSupporting\n\nJoin our recurring crowd funding campaign on Apoia.se\nDonate via Bitcoin to 1Gbvfjmjvur7qwbwNFdPSNDgx66KSdVB5b\nFollow, share and interact with us on Facebook\nFollow, retweet and join Rosie on Twitter to interact with your favourite congresspeople\n\nAcknowledgments\n \n","77":"easyAI\nEasyAI (full documentation here) is a pure-Python artificial intelligence framework for two-players abstract games such as Tic Tac Toe, Connect 4, Reversi, etc.\nIt makes it easy to define the mechanisms of a game, and play against the computer or solve the game.\nUnder the hood, the AI is a Negamax algorithm with alpha-beta pruning and transposition tables as described on Wikipedia.\n\nInstallation\nIf you have pip installed, type this in a terminal\nsudo pip install easyAI\n\nOtherwise, dowload the source code (for instance on Github), unzip everything into one folder and in this folder, in a terminal, type\nsudo python setup.py install\n\nAdditionnally you will need to install Numpy to be able to run some of the examples.\n\nA quick example\nLet us define the rules of a game and start a match against the AI:\nfrom easyAI import TwoPlayersGame, Human_Player, AI_Player, Negamax\n\nclass GameOfBones( TwoPlayersGame ):\n    \"\"\" In turn, the players remove one, two or three bones from a\n    pile of bones. The player who removes the last bone loses. \"\"\"\n\n    def __init__(self, players):\n        self.players = players\n        self.pile = 20 # start with 20 bones in the pile\n        self.nplayer = 1 # player 1 starts\n\n    def possible_moves(self): return ['1','2','3']\n    def make_move(self,move): self.pile -= int(move) # remove bones.\n    def win(self): return self.pile<=0 # opponent took the last bone ?\n    def is_over(self): return self.win() # Game stops when someone wins.\n    def show(self): print (\"%d bones left in the pile\" % self.pile)\n    def scoring(self): return 100 if game.win() else 0 # For the AI\n\n# Start a match (and store the history of moves when it ends)\nai = Negamax(13) # The AI will think 13 moves in advance\ngame = GameOfBones( [ Human_Player(), AI_Player(ai) ] )\nhistory = game.play()\nResult:\n20 bones left in the pile\n\nPlayer 1 what do you play ? 3\n\nMove #1: player 1 plays 3 :\n17 bones left in the pile\n\nMove #2: player 2 plays 1 :\n16 bones left in the pile\n\nPlayer 1 what do you play ?\n\n\nSolving the game\nLet us now solve the game:\nfrom easyAI import id_solve\nr,d,m = id_solve(GameOfBones, ai_depths=range(2,20), win_score=100)\nWe obtain r=1, meaning that if both players play perfectly, the first player to play can always win (-1 would have meant always lose), d=10, which means that the wins will be in ten moves (i.e. 5 moves per player) or less, and m='3', which indicates that the first player's first move should be '3'.\nThese computations can be sped up using a transposition table which will store the situations encountered and the best moves for each:\ntt = TT()\nGameOfBones.ttentry = lambda game : game.pile # key for the table\nr,d,m = id_solve(GameOfBones, range(2,20), win_score=100, tt=tt)\nAfter these lines are run the variable tt contains a transposition table storing the possible situations (here, the possible sizes of the pile) and the optimal moves to perform. With tt you can play perfectly without thinking:\n.. code:: python\n\n\ngame = GameOfBones( [  AI_Player( tt ), Human_Player() ] )\ngame.play() # you will always lose this game :)\n\nContribute !\nEasyAI is an open source software originally written by Zulko and released under the MIT licence. It could do with some improvements, so if your are a Python\/AI guru maybe you can contribute through Github . Some ideas: AI algos for incomplete information games, better game solving strategies, (efficient) use of databases to store moves,  AI algorithms using parallelisation.\nFor troubleshooting and bug reports, the best for now is to ask on Github.\n\nMaintainers\n\nZulko (owner)\nJohnAD\n\n","78":"\u8bf4\u660e\n\u5b66\u4e60\u4eba\u5de5\u667a\u80fd\u8fc7\u7a0b\u4e2d\u7684\u4e00\u4e9b\u6a21\u5f0f\u8bc6\u522b\u4f5c\u4e1a\n","79":"PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\nCode accompanying CVPR'20 paper of the same title. Paper link: https:\/\/arxiv.org\/pdf\/2003.03808.pdf\nNOTE\nWe have noticed a lot of concern that PULSE will be used to identify individuals whose faces have been blurred out. We want to emphasize that this is impossible - PULSE makes imaginary faces of people who do not exist, which should not be confused for real people. It will not help identify or reconstruct the original image.\nWe also want to address concerns of bias in PULSE. We have now included a new section in the paper and an accompanying model card directly addressing this bias.\n\n\n\n\nTable of Contents\n\nPULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\nTable of Contents\n\nWhat does it do?\nUsage\n\nPrereqs\nData\nApplying PULSE\n\n\n\n\n\nWhat does it do?\nGiven a low-resolution input image, PULSE searches the outputs of a generative model (here, StyleGAN) for high-resolution images that are perceptually realistic and downscale correctly.\n\nUsage\nThe main file of interest for applying PULSE is run.py. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.\nPrereqs\nYou will need to install cmake first (required for dlib, which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux and Windows. For the full set of required Python packages, create a Conda environment from the provided YAML, e.g.\nconda create -f pulse.yml \n\nor (Anaconda on Windows):\nconda env create -n pulse -f pulse.yml\nconda activate pulse\n\nIn some environments (e.g. on Windows), you may have to edit the pulse.yml to remove the version specific hash on each dependency and remove any dependency that still throws an error after running conda env create... (such as readline)\ndependencies\n  - blas=1.0=mkl\n  ...\n\nto\ndependencies\n  - blas=1.0\n ...\n\nFinally, you will need an internet connection the first time you run the code as it will automatically download the relevant pretrained model from Google Drive (if it has already been downloaded, it will use the local copy). In the event that the public Google Drive is out of capacity, add the files to your own Google Drive instead; get the share URL and replace the ID in the https:\/\/drive.google.com\/uc?=ID links in align_face.py and PULSE.py with the new file ids from the share URL given by your own Drive file.\nData\nBy default, input data for run.py should be placed in .\/input\/ (though this can be modified). However, this assumes faces have already been aligned and downscaled. If you have data that is not already in this form, place it in realpics and run align_face.py which will automatically do this for you. (Again, all directories can be changed by command line arguments if more convenient.) You will at this stage pic a downscaling factor.\nNote that if your data begins at a low resolution already, downscaling it further will retain very little information. In this case, you may wish to bicubically upsample (usually, to 1024x1024) and allow align_face.py to downscale for you.\nApplying PULSE\nOnce your data is appropriately formatted, all you need to do is\npython run.py\n\nEnjoy!\n","80":"This repository was originally at https:\/\/github.com\/aiforearth\/SpaceNetExploration. For other Microsoft AI for Earth repositories, search for the topic #aiforearth on GitHub or visit them here.\nBuilding Footprint Extraction\nOverview\nThis repository contains a walkthrough demonstrating how to perform semantic segmentation using convolutional neural networks (CNNs) on satellite images to extract the footprints of buildings. We show how to carry out the procedure on an Azure Deep Learning Virtual Machine (DLVM), which are GPU-enabled and have all major frameworks pre-installed so you can start model training straight-away. We use a subset of the data and labels from the SpaceNet Challenge, an online repository of freely available satellite imagery released to encourage the application of machine learning to geospatial data.\nThe blog post that first announced this sample project is here on the Azure Blog.\nData\nSpaceNet Building Footprint Extraction Dataset\nThe code in this repository was developed for training a semantic segmentation model (currently two variants of the U-Net are implemented) on the Vegas set of the SpaceNet building footprint extraction data. This makes the sample code clearer, but it can be easily extended to take in training data from the four other locations.\nThe organizers release a portion of this data as training data and the rest are held out for the purpose of the competitions they hold. For the experiments discussed here, we split the official training set 70:15:15 into our own training, validation and test sets. These are 39 GB in size as raw images in TIFF format with labels.\nGenerate Input from Raw Data\nInstruction for downloading the SpaceNet data can be found on their website. The authors provide a set of utilities to convert the raw images to a format that semantic segmentation models can take as input. The utilities are in this repo. Most of the functionalities you will need are in the python folder. Please read their instructions on the repo's README to understand all the tools and parameters available. After using python\/createDataSpaceNet.py from the utilities repo to process the raw data, the input image and its label look like the following:\n\nEnvironment Setup\nProvision an Azure Deep Learning Virtual Machine\nYou could train your models on a Deep Learning Virtual Machine (DLVM) on Azure to get started quickly, where all the major deep learning frameworks, including PyTorch used in this repo, are installed and ready to use. These VMs are configured specifically for use with GPUs. Instructions for provisioning can be found here. The code here has been used on a Ubuntu Linux DLVM, but you should be able to use it on a Windows DLVM with minor modifications to the commands such as those setting environment variable values. The commands on this page are for running in a Linux shell.\nAdditional Packages to Install\nThere are two additional packages for the polygonization of the result of the CNN model so that our results can be compared to the original labels, which are expressed in a polygon data type. You can install these using pip:\npip install rasterio\npip install shapely\n\nData Storage Options\nFor quick experimentations you could download your data to the OS disk, but this makes data transfer and sharing costly when you scale out.\nThere are several options for storing the data while you perform computation on them in Azure. Here's a piece of documentation to guide you through choosing among these, and here are the pricing information.\nIf you are not planning on training models distributedly across several machines, you could attach a data disk to your VM. See instructions on attaching a data disk to a Linux VM. You can later re-attach this data disk to a more powerful VM, but it can only be attached to one machine at a time.\nFor both Azure Blob Storage and File Share, you can browse the files stored from any computer using the Storage Explorer desktop app. Both blob storage containers and file shares can be mounted on your VM so you can use them as if they were local disks. See instructions for mounting blob storage and file shares. Note however that such file systems have different performance for writing and deleting files than local file systems. Please refer to Azure Storage performance targets for more information.\nModel Training\nWe tackle the problem of outlining building footprints in satellite images by applying a semantic segmentation model to first classify each pixel as background, building, or boundary of buildings. The U-Net is used for this task. There are two variants of the U-Net implemented in the models directory, differing by the sizes of filters used. The baseline U-Net is a similar version as used by the winner of the SpaceNet Building Footprint competition XD_XD. We referenced several open source implementations, noted in the relevant files.\nCode for training the model is in the pipeline directory. The training script is train.py and all the paths to input\/output, parameters and other arguments are specified in train_config.py, which you can modify and experiment with. The default configuration has total_epochs set to 15 to run training for 15 epochs, which takes about an hour in total on a VM with a P100 GPU (SKU NC6s_v2 on Azure). For the sample image above, the result of the segmentation model is as follows at epoch 3, 5, 7 and 10:\n\nGenerate Polygons of the Building Footprints\nStandard graphics techniques are used to convert contiguous blobs of building pixels identified by the segmentation model, using libraries Rasterio and Shapely. The script pipeline\/polygonize.py performs this procedure, and you can change various parameters in polygonize_config.py in the same directory. The most important parameter influencing the performance of the model is min_polygon_area, which is the area in squared pixels below which blobs of building pixels are discarded, reducing the noise in our results. Increasing this threshold decreases the number of false positive footprint proposals.\nEvaluation\nThe evaluation metric used by the SpaceNet Challenge is the F1 score, where a footprint proposal is counted as a true positive if its intersection over union (IoU) with the ground truth polygon is above 0.5.\nYou can of course employ your own metric to suit your application, but if you would like to use the SpaceNet utilities to compute the F1 score based on polygons of building footprints, you need to first combine the annotations for each image in geojson format into a csv with python\/createCSVFromGEOJSON.py from the utilities repo. In the root directory of utilities, run\npython python\/createCSVFromGEOJSON.py -imgDir \/tutorial_data\/val\/RGB-PanSharpen -geoDir \/tutorial_data\/val\/geojson\/buildings -o ground_truth.csv --CreateProposalFile\n\nThen you can use python\/evaluateScene.py to compute the F1 score, giving the ground truth csv produced from the last command and the csv output proposals.csv produced by pipeline\/polygonize.py in this repo:\npython python\/evaluateScene.py ground_truth.csv proposal.csv\n\nRelated Materials\nBing team's announcement that they released a large quantity of building footprints in the US in support of the Open Street Map community, and article briefly describing their method of extracting them.\nVery helpful blog post and code on road extraction from satellite images by Jeff Wen on a different dataset. We also took inspiration in structuring the training pipeline from this repo.\nSpaceNet road extraction challenge.\nTutorial on pixel-level land cover classification using semantic segmentation in CNTK on Azure.\n","81":"\n\n\n\nCosmonium is a 3D astronomy and space exploration program. With Cosmonium you can navigate in our solar system and discover all the planets and their moons. You can also visit the neighboring stars and discover the true size of our galaxy and the Universe.\nCosmonium supports (or will support) the creation of fictional planets, stellar systems nebulaes, ... using procedural generation.\nCosmonium also already supports some Celestia addons (though CMOD and CelX are not yet supported).\nRequirements\nCosmonium runs on Windows (Vista or above), Linux (CentOS 5, Ubuntu 14 or above) or macOS (mac0S 10.9 or above)\nwith a graphic card supporting OpenGL 2.1 or better (OpenGL 4.5 is recommended) and at least 512MB of disk\n(up to 4GB if the HD and UHD textures are installed).\nInstallation\nDownload the installer or package for your platform from the download page and see the [[Installation]] page\nThe package contains only low resolution textures, see here to install extra HD and UHD textures.\nScreenshots\nSee in the Wiki some screenshots of the application with views of\nSaturn,\nJupiter,\nMars,\nthe Moon,\nprocedural planets, ...\n\nLaunch\nSimply starts cosmonium from your application menu or from the cosmonium folder. See also the installation page for more options.\nUser interface\nCosmonium user interface is still heavily based on Celestia, most of the command and keyboard shortcuts work the same.\nGo to First steps to have an explanation of the basic command or see the Control page for an exhaustive list.\nFull documentation\nCosmonium is still in its infancy, but it is already usable to explore all the planets and the moons of our solar system, all the neighbor or visible stars and much more.\nIt also support custom content and addons, either as Cosmonium or Celestia addons.\nThe full documentation is available in the Wiki\nBugs\nIf you encounter any problem to install or run Cosmonium, please don't hesitate to fill a bug report in the issue tracker here on Github.\nLicense\nCosmonium is (C) 2018-2020 Laurent Deru.\nThis program is free software; you can redistribute it and\/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details, which you should have received along with this program. If not, request a copy from: Free Software Foundation, Inc. 59 Temple Place - Suite 330 Boston, MA 02111-1307 USA.\nCosmonium uses several third-party libraries which are subject to their own licenses,  see Third-Party.md for the complete list.\nCosmonium data (textures, models, orbital elements,..) come from many sources. Their respective copyright holder, license and reference are available in the info panel of the displayed object and in the related yaml file.\nPowered by\n\n\n","82":"SpaceNet Buildings Exploration\nTransform SpaceNet geojson buidling labels data into raster masks.\nDownload data via:\naws s3api get-object --bucket spacenet-dataset \\\n--key AOI_1_Rio\/processedData\/processedBuildingLabels.tar.gz \\\n--request-payer requester processedBuildingLabels.tar.gz\n\nDownload spacenet utilities from:\nhttps:\/\/github.com\/SpaceNetChallenge\/utilities\/tree\/master\/python\/spaceNet\nFor further details, see:\nhttps:\/\/medium.com\/the-downlinq\/getting-started-with-spacenet-data-827fd2ec9f53\nExample outputs are included in the example_outputs directory\n\n","83":"pypet\n\n\n\n\n\n\nThe new python parameter exploration toolkit:\npypet manages exploration of the parameter space\nof any numerical simulation in python,\nthereby storing your data into HDF5 files for you.\nMoreover, pypet offers a new data container which\nlets you access all your parameters and results\nfrom a single source. Data I\/O of your simulations and\nanalyses becomes a piece of cake!\nRequirements\nPython 3.6, 3.7 or 3.8 and\n\n\ntables >=  3.5.0\n\n\npandas >= 1.0.0\n\n\nnumpy >= 1.16.0\n\n\nscipy >= 1.3.0\n\n\nHDF5 >= 1.10.0\n\n\nThere are also some optional packages that you can but do not have to install.\nIf you want to combine pypet with SCOOP you need\n\nscoop >= 0.7.1\n\nFor git integration you additionally need\n\nGitPython >= 3.1.3\n\nTo utilize the cap feature for multiprocessing you need\n\npsutil >= 5.7.0\n\nTo utilize the continuing of crashed trajectories you need\n\ndill >= 0.3.1\n\nAutomatic Sumatra records are supported for\n\nSumatra >= 0.7.1\n\nPython 2.7\nThis release no longer supports Python 2.7.\nIf you are still using Python 2.7, you need to\nuse the pypet legacy version 0.3.0 (https:\/\/pypi.python.org\/pypi\/pypet\/0.3.0).\nWhat is pypet all about?\nWhenever you do numerical simulations in science, you come across two major challenges.\nFirst, you need some way to save your data. Secondly, you extensively explore the parameter space.\nIn order to accomplish both you write some hacky I\/O functionality to get it done the quick and\ndirty way. This means storing stuff into text files, as MATLAB m-files,\nor whatever comes in handy.\nAfter a while and many simulations later, you want to look back at some of your very\nfirst results. But because of unforeseen circumstances, you changed a lot of your code.\nAs a consequence, you can no longer use your old data, but you need to write a hacky\nconverter to format your previous results to your new needs.\nThe more complexity you add to your simulations, the worse it gets, and you spend way\ntoo much time formatting your data than doing science.\nIndeed, this was a situation I was confronted with pretty soon at the beginning of my PhD.\nSo this project was born. I wanted to tackle the I\/O problems more generally and produce code\nthat was not specific to my current simulations, but I could also use for future scientific\nprojects right out of the box.\nThe python parameter exploration toolkit (pypet) provides a framework to define parameters\nthat you need to run your simulations. You can actively explore these by following a\ntrajectory through the space spanned by the parameters.\nAnd finally, you can get your results together and store everything appropriately to disk.\nThe storage format of choice is HDF5 (http:\/\/www.hdfgroup.org\/HDF5\/) via PyTables\n(http:\/\/www.pytables.org\/).\nPackage Organization\nThis project encompasses these core modules:\n\n\nThe pypet.environment module for handling the running of simulations\n\n\nThe pypet.trajectory module for managing the parameters and results,\nand providing a way to explore your parameter space. Somewhat related is also the\npypet.naturalnaming module, that provides functionality to access and put data into\nthe trajectory.\n\n\nThe pypet.parameters module including containers for parameters and results\n\n\nThe pypet.storageservice for saving your data to disk\n\n\nInstall\nIf you don't have all prerequisites (numpy, scipy, tables, pandas) install them first.\nThese are standard python packages, so chances are high that they are already installed.\nBy the way, in case you use the python package manager pip\nyou can list all installed packages with pip freeze.\nNext, simply install pypet via pip install pypet\nOr\nThe package release can also be found on https:\/\/pypi.python.org\/pypi\/pypet. Download, unpack\nand python setup.py install it.\nOr\nIn case you use Windows, you have to download the tar file from https:\/\/pypi.python.org\/pypi\/pypet\nand unzip it. Next, open a windows terminal\nand navigate to your unpacked pypet files to the folder containing the setup.py file.\nAs above run from the terminal python setup.py install.\nDocumentation and Support\nDocumentation can be found on http:\/\/pypet.readthedocs.org\/.\nThere is a Google Groups mailing list for support: https:\/\/groups.google.com\/forum\/?hl=de#!forum\/pypet\nIf you have any further questions feel free to contact me at robert.meyer (at) ni.tu-berlin.de.\nMain Features\n\n\nNovel tree container Trajectory, for handling and managing of\nparameters and results of numerical simulations\n\n\nGroup your parameters and results into meaningful categories\n\n\nAccess data via natural naming, e.g. traj.parameters.traffic.ncars\n\n\nAutomatic storage of simulation data into HDF5 files via PyTables\n\n\nSupport for many different data formats\n\n\npython native data types: bool, int, long, float, str, complex\n\n\nlist, tuple, dict\n\n\nNumpy arrays and matrices\n\n\nScipy sparse matrices\n\n\npandas DataFrames (http:\/\/pandas.pydata.org\/)\n\n\nBRIAN2 quantities and monitors (http:\/\/briansimulator.org\/)\n\n\n\n\nEasily extendable to other data formats!\n\n\nExploration of the parameter space of your simulations\n\n\nMerging of trajectories residing in the same space\n\n\nSupport for multiprocessing, pypet can run your simulations in parallel\n\n\nAnalyse your data on-the-fly during multiprocessing\n\n\nAdaptively explore tha parameter space combining pypet with optimization\ntools like the evolutionary algorithms framework DEAP (http:\/\/deap.readthedocs.org\/en\/)\n\n\nDynamic Loading, load only the parts of your data you currently need\n\n\nResume a crashed or halted simulation\n\n\nAnnotate your parameters, results and groups\n\n\nGit Integration, let pypet make automatic commits of your codebase\n\n\nSumatra Integration, let pypet add your simulations to the electronic lab notebook tool\nSumatra (http:\/\/neuralensemble.org\/sumatra\/)\n\n\npypet can be used on computing clusters or multiple servers at once if it is combined with\nSCOOP (http:\/\/scoop.readthedocs.org\/)\n\n\nQuick Working Example\nThe best way to show how stuff works is by giving examples. I will start right away with a\nvery simple code snippet.\nWell, what we have in mind is some sort of numerical simulation. For now we will keep it simple,\nlet's say we need to simulate the multiplication of 2 values, i.e. z=x*y.\nWe have two objectives, a) we want to store results of this simulation z and\nb) we want to explore the parameter space and try different values of x and y.\nLet's take a look at the snippet at once:\nfrom pypet import Environment, cartesian_product\n\ndef multiply(traj):\n    \"\"\"Example of a sophisticated simulation that involves multiplying two values.\n\n    :param traj:\n\n        Trajectory containing the parameters in a particular combination,\n        it also serves as a container for results.\n\n    \"\"\"\n    z=traj.x * traj.y\n    traj.f_add_result('z',z, comment='I am the product of two values!')\n\n# Create an environment that handles running our simulation\nenv = Environment(trajectory='Multiplication',filename='.\/HDF\/example_01.hdf5',\n                    file_title='Example_01',\n                    comment = 'I am the first example!')\n\n# Get the trajectory from the environment\ntraj = env.trajectory\n\n# Add both parameters\ntraj.f_add_parameter('x', 1.0, comment='Im the first dimension!')\ntraj.f_add_parameter('y', 1.0, comment='Im the second dimension!')\n\n# Explore the parameters with a cartesian product\ntraj.f_explore(cartesian_product({'x':[1.0,2.0,3.0,4.0], 'y':[6.0,7.0,8.0]}))\n\n# Run the simulation with all parameter combinations\nenv.run(multiply)\nAnd now let's go through it one by one. At first we have a job to do, that is multiplying two\nvalues:\ndef multiply(traj):\n    \"\"\"Example of a sophisticated simulation that involves multiplying two values.\n\n    :param traj:\n\n        Trajectory containing the parameters in a particular combination,\n        it also serves as a container for results.\n\n    \"\"\"\n    z=traj.x * traj.y\n    traj.f_add_result('z',z, comment='I am the product of two values!')\nThis is our simulation function multiply. The function uses a so called trajectory\ncontainer which manages our parameters. We can access the parameters simply by natural naming,\nas seen above via traj.x and traj.y. The value of z is simply added as a result\nto the traj object.\nAfter the definition of the job that we want to simulate, we create an environment which\nwill run the simulation.\n# Create an environment that handles running our simulation\nenv = Environment(trajectory='Multiplication',filename='.\/HDF\/example_01.hdf5',\n                    file_title='Example_01',\n                    comment = 'I am the first example!')\nThe environment uses some parameters here, that is the name of the new trajectory, a filename to\nstore the trajectory into, the title of the file, and a comment that is added to the trajectory.\nThere are more options available like the number of processors for multiprocessing or\nhow verbose the final HDF5 file is supposed to be.\nCheck out the documentation (http:\/\/pypet.readthedocs.org\/) if you want to know more.\nThe environment will automatically generate a trajectory for us which we can access via:\n# Get the trajectory from the environment\ntraj = env.trajectory\nNow we need to populate our trajectory with our parameters. They are added with the default values\nof x=y=1.0.\n# Add both parameters\ntraj.f_add_parameter('x', 1.0, comment='Im the first dimension!')\ntraj.f_add_parameter('y', 1.0, comment='Im the second dimension!')\nWell, calculating 1.0 * 1.0 is quite boring, we want to figure out more products, that is\nthe results of the cartesian product set {1.0,2.0,3.0,4.0} x {6.0,7.0,8.0}.\nTherefore, we use f_explore in combination with the builder function\ncartesian_product.\n# Explore the parameters with a cartesian product\ntraj.f_explore(cartesian_product({'x':[1.0,2.0,3.0,4.0], 'y':[6.0,7.0,8.0]}))\nFinally, we need to tell the environment to run our job multiply with all parameter\ncombinations.\n# Run the simulation with all parameter combinations\nenv.run(multiply)\nAnd that's it. The environment will evoke the function multiply now 12 times with\nall parameter combinations. Every time it will pass a traj container with another one of these\n12 combinations of different x and y values to calculate the value of z.\nMoreover, the environment and the storage service will have taken care about the storage\nof our trajectory  - including the results we have computed - into an HDF5 file.\nSo have fun using this tool!\nCheers,\nRobert\nMiscellaneous\nAcknowledgements\n\n\nThanks to Robert Pr\u00f6pper and Philipp Meier for answering all my Python questions\nYou might want to check out their SpykeViewer (https:\/\/github.com\/rproepp\/spykeviewer)\ntool for visualization of MEA recordings and NEO (http:\/\/pythonhosted.org\/neo) data\n\n\nThanks to Owen Mackwood for his SNEP toolbox which provided the initial ideas\nfor this project\n\n\nThanks to Mehmet Nevvaf Timur for his work on the SCOOP integration and the 'NETQUEUE' feature\n\n\nThanks to Henri Bunting for his work on the BRIAN2 subpackage\n\n\nThanks to the BCCN Berlin (http:\/\/www.bccn-berlin.de),\nthe Research Training Group GRK 1589\/1, and the\nNeural Information Processing Group ( http:\/\/www.ni.tu-berlin.de) for support\n\n\nTests\nTests can be found in pypet\/tests.\nNote that they involve heavy file I\/O and you need privileges\nto write files to a temporary folder.\nThe tests suite will make use of the tempfile.gettempdir() function to\ncreate such a temporary folder.\nEach test module can be run individually, for instance $ python trajectory_test.py.\nYou can run all tests with $ python all_tests.py which can also be found under\npypet\/tests.\nYou can pass additional arguments as $ python all_tests.py -k --folder=myfolder\/\nwith -k to keep the HDF5 and log files created by the tests\n(if you want to inspect them, otherwise they will be deleted after the completed tests),\nand --folder= to specify a folder where to store the HDF5 files instead of the temporary one.\nIf the folder cannot be created, the program defaults to tempfile.gettempdir().\nRunning all tests can take up to 20 minutes. The test suite encompasses more than 1000 tests\nand has a code coverage of about 90%!\nMoreover, pypet is constantly tested with Python 3.7 and 3.8 for Linux using\nTravis-CI. Testing for Windows platforms is performed via Appveyor.\nThe source code is available at https:\/\/github.com\/SmokinCaterpillar\/pypet\/.\nLicense\nBSD, please read LICENSE file.\nLegal Notice\npypet was created by Robert Meyer at the Neural Information Processing Group (TU Berlin),\nsupported by the Research Training Group GRK 1589\/1.\nContact\nrobert.meyer (at) alcemy.tech\nalcemy GmbH\nChoriner Str. 83\n10119 Berlin, Germany\n","84":"ParamNoise\nA comparison of parameter space noise methods for exploration in deep reinforcement learning\nNOTE: This project is not maintained.  Reach out if you'd like to help reboot it.\nLinks to papers\nParameter Space Noise for Exploration : https:\/\/openreview.net\/forum?id=ByBAl2eAZ&noteId=ByBAl2eAZ\nNoisy Networks For Exploration : https:\/\/openreview.net\/forum?id=rywHCPkAW&noteId=rywHCPkAW\nResources\n\nOpenAI Baselines for useful Atari wrappers and replay buffer\nbearpaw's pytorch-classification repo for utilities, logging, training framework\nikostrikov's PPO implementation for other utilities and PPO guidance\npytorch-rl for DQN help\nPyTorch DQN tutorial for PyTorch tricks\nOriginal DQN paper since both papers use the original hyperparameters, for the most part\n\nTODOs\n\nImplement PPO and MuJoCo env handling\nRevisit logging; make sure everything is there to reproduce results in papers\nImplement plotting (matplotlib is in Logger object; maybe try out visdom)\nMore tests (figure out different combinations of arguments to ensure everything's interacting well)\nBegin experiments (start with Mujoco; it's cheaper)\n\nAtari Games to Test\n\nAlien: Adaptive helps a lot, learned shows no improvement\nEnduro: Both methods improve\nSeaquest: Adaptive helps, learned performs worse than baseline\nSpace Invaders: Adaptive helps, but learned helps more\nWizardOfWor: Adaptive worse than baseline, but learned helps a lot\n\nMuJoCo enviroments to test\n\nHopper\nWalker2d\nHalfCheetah\nSparse versions of these? (from rllab)\n\n","85":"PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\nCode accompanying CVPR'20 paper of the same title. Paper link: https:\/\/arxiv.org\/abs\/2003.03808\n\n\n\nTable of Contents\n\nPULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\nTable of Contents\n\nWhat does it do?\nHow do I use it?\nUsage\n\nPrereqs\nData\nApplying PULSE\n\n\n\n\n\nWhat does it do?\nGiven a low-resolution input image, PULSE searches the outputs of a generative model (here, StyleGAN) for high-resolution images that are perceptually realistic and downscale correctly.\n\nHow do I use it?\nThe easiest way to apply PULSE to your own images is with our interactive demo, found at https:\/\/colab.research.google.com\/drive\/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true.\nIf you want to try using this codebase, continue on.\nUsage\nThe main file of interest for applying PULSE is run.py. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.\nPrereqs\nYou will need to install cmake first (required for dlib, which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux. For the full set of required Python packages, create a Conda environment from the provided YAML, e.g.\nconda create -f pulse.yml\n\nFinally, you will need an internet connection the first time you run the code as it will automatically download the relevant pretrained model from Google Drive (if it has already been downloaded, it will use the local copy).\nData\nBy default, input data for run.py should be placed in .\/input\/ (though this can be modified). However, this assumes faces have already been aligned and downscaled. If you have data that is not already in this form, place it in realpics and run align_face.py which will automatically do this for you. (Again, all directories can be changed by command line arguments if more convenient.) You will at this stage pic a downscaling factor.\nNote that if your data begins at a low resolution already, downscaling it further will retain very little information. In this case, you may wish to bicubically upsample (usually, to 1024x1024) and allow align_face.py to downscale for you.\nThe dataset we evaluated on was CelebA-HQ, but in our experience PULSE works with any picture of a realistic face.\nApplying PULSE\nOnce your data is appropriately formatted, all you need to do is\npython run.py\n\nEnjoy!\nContact both Sachit Menon and Alex Damian (sachit.menon@duke.edu and alexandru.damian@duke.edu) for questions regarding this work.\n","86":"BOOM\nAn easy-to-use multi-process Configuration Space Exploration pipeline framework.\nFeatures\n\nEasy to use: you only need to write your configuration file and modules, we will handle everything else!\nFlexible: we offer common modules for QA pipelines, and it is very easy to develop your own modules.\nParameter tuning: automatically run on all possible parameter combinations and saves the results.\nHigh efficiency: we use multiple processes to run the whole pipeline.\nCompatibility: only compatible with Python 2.\n\nInstallation\nFirst, install RabbitMQ and pip.\nThen, clone the repository, run\nmake install\n\nIt will install dependencies using pip, and install this framework to your PATH.\nRun\nWe offer a command-line executable program boom.\nWhen executing, it will load conf.yaml from the current directory.\nYou can also specify the configuration file to use by adding option -conf PATH_TO_CONF_FILE.\nFor more options, run boom --help.\nDocker image\nTo build the docker image, run make docker.\nTutorials\nPlease check out the two tutorials in examples folder.\nQuick tutorails\nConfiguation Space Exploration\nA QA pipeline may be consisted with several modules, each module may have some parameters.\nEach combination of parameters corresponds to a path in the parameter space.\nBOOM exhaustively run the pipeline on every possible parameter combinations, saves all intermediate results and final results.\nThe following figure shows a pipeline which has several modules.\nThe execution path is a tree which every level corresponds to a module, and each node stands for a different parameter value.\nThe leaf nodes are metric values.\nRed arrows belongs to the best parameter combination.\n\nComponents\nThere are two main components to a BOOM pipeline: the modules and the configuration file.\nEach pipeline can have an arbitrary number of modules (n >= 1) but there is only one configuration file that defines the pipeline.\nBOOM works by instantiating each module and passing data along from one module to the next, allowing each to process and transform the data along the way.\nModules\nThe building block of a BOOM pipeline is the Module class. Each module in the pipeline takes in the data in the exact format returned by the previous module and return the data for the next module in its process() method. At a minimum, each user-defined module must subclass Module and implement the __init__() and process() methods.\nConfiguration Files\nThe configuration file defines the structure and composition of the pipeline and allows the user to define a parameter space for the pipeline to be executed over. The configuration is written is a YAML file and contains two core components: pipeline, where pipeline metadata is declared, and modules, where the pipeline composition is defined.\nFollowing is the pipeline section of the toy example's configuration file:\npipeline:\n    name: toy_pipeline\n    rabbitmq_host: 127.0.0.1\n    clean_up: false\n    use_mongodb: false\n    mongodb_host: 127.0.0.1\n\nUnder the pipeline key, there are 5 key-value pairs that need to be declared:\nname\nrabbitmq_host\nclean_up\nuse_mongodb\nmongodb_host\n\nname allows the user to declare a name for the pipeline. rabbitmq_host and mongodb_host are simply the host addresses for RabbitMQ and MongoDB, respectively. clean_up is a boolean value that will delete intermediate output files if declared true. use_mongodb is a boolean value that will write data to MongoDB instead of files if declared true.\nFollowing is the modules section of the toy example's configuration file:\npipeline: # Pipeline section, defines pipeline's properties\n    mode: docker # Running mode, local or docker, default local\n    name: toy_pipeline # Name of the pipeline\n    rabbitmq_host: 127.0.0.1 # RabbitMQ's host uri\n    clean_up: false # Whether the pipeline cleans up after finished running, true or false\n    use_mongodb: true # Whether to use MongoDB, true or false, default false\n    mongodb_host: 127.0.0.1 # MongoDB's host\n\nmodules:\n    -   name: module_1 # Name of the module\n        type: Sample # Type of the module\n        input_file: data.json # Input file's uri\n        output_module: module_2 # The following module's name\n        instances: 1 # Number of instances of this module\n        params:\n            -   name: p1\n                type: collection # Type of the param, int, float or collection\n                values: # Possible vaules for collection param\n                    - val1\n                    - val2\n                    - val3\n\n            -   name: p2\n                type: int\n                start: 0\n                end: 20\n                step_size: 20\n\n    -   name: module_2\n        type: Sample\n        output_module: module_3\n        instances: 1\n        params:\n            -   name: p\n                type: float\n                start: 0.0\n                end: 80.0\n                step_size: 40.0\n        \n    -   name: module_3\n        type: Sample\n        output_module: module_4\n        instances: 1\n\n    -   name: module_4\n        type: CSVWriter\n        output_file: results.csv \n        instances: 1\n\nThe modules section of the configuration file should contain a list of modules. Each module consists of a set of key-value pairs which  must include name, type, input_file (first module only), output_module (or output_file for the last module), instances, and (optionally) params. params is a list of parameters, defined by a name, type (float, int, or collection). If the parameter is a float or int, the param should also contain start, end, and step_size. If the parameter is of type collection, then it should contain a values list.\nAPI documentation\nYou can find the API documentation here.\nWarning\nThis framework is still under heavy development,\nplease be careful.\n","87":"atiamML\nChemla - Latent representations for real-time synthesis space exploration\n\n\ncode\/ contains project's code along with simple scripts that demonstrate the use of the developped methodologies - see code\/README.md\nreport\/ contains the report in PDF format along with the LaTeX source and eventual figures - see report\/README.md\ntoy\/ contains toy datasets, along with the procedural scripts to generate it - see toy\/README.md\n\n","88":"Safe-Explorer\nIntroduction\nThis repository contains Pytorch implementation of paper \"Safe Exploration in Continuous Action Spaces\" [Dalal et al.] along with \"Continuous Control With Deep Reinforcement\nLearning\" [Lillicrap et al.]. Dalal et al. present a closed form analytically optimal solution to ensure safety in continuous action space. The proposed \"safety layer\",\nmakes the smallest possible perturbation to the original action such that safety constraints are satisfied.\n\nDalal et al. also propose two new domains BallND and Spaceship which are governed by first and second order dynamics respectively. In Spaceship domain agent receives a reward only on task completion, while BallND has continuous reward based distance from the target. Implementation of both of these tasks extend OpenAI gym's environment interface (gym.Env).\nSetup\nThe code requires Python 3.6+ and is tested with torch 1.1.0. To install dependencies run,\npip install -r requirements.txt\nTraining\nTo obtain list of parameters and their default values run,\npython -m safe_explorer.main --help\nTrain the model by simply running,\nBallND\npython -m safe_explorer.main --main_trainer_task ballnd\nSpaceship\npython -m safe_explorer.main --main_trainer_task spaceship\nMonitor training with Tensorboard,\ntensorboard --logdir=runs\nResults\nTo be updated.\nAcknowledgement\nSome modifications in DDPG implementation are based OpenAI Spinning Up implement.\nReferences\n\n\nLillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971 (2015).\n\n\nDalal, Gal, et al. \"Safe exploration in continuous action spaces.\" arXiv preprint arXiv:1801.08757 (2018).\n\n\n","89":"DeepChem\n\n\n\n\n\nWebsite | Documentation | Colab Tutorial | Discussion Forum | Gitter\nDeepChem aims to provide a high quality open-source toolchain\nthat democratizes the use of deep-learning in drug discovery,\nmaterials science, quantum chemistry, and biology.\nTable of contents:\n\nRequirements\nInstallation\n\nStable version\nNightly build version\nDocker\nFrom source\n\n\nGetting Started\nContributing to DeepChem\n\nCode Style Guidelines\nDocumentation Style Guidelines\nGitter\n\n\nAbout Us\nCiting DeepChem\n\nRequirements\nDeepChem currently supports Python 3.6 through 3.7 and requires these packages on any condition.\n\njoblib\nNumPy\npandas\nscikit-learn\nSciPy\nTensorFlow\n\ndeepchem>=2.4.0 requires tensorflow v2\ndeepchem<2.4.0 requires tensorflow v1\n\n\n\nSoft Requirements\nDeepChem has a number of \"soft\" requirements.\nIf you face some errors like ImportError: No module named XXXX, you may need to install some packages.\nPlease check the document about soft requirements.\nInstallation\nStable version\nCaution!! : The latest stable version was published nearly a year ago. If you are a pip user or you face some errors, we recommend the nightly build version.\nRDKit is a soft requirement package, but many useful methods like molnet depend on it. We recommend installing RDKit with deepchem.\npip install tensorflow==1.14\nconda install -y -c conda-forge rdkit deepchem==2.3.0\nIf you want GPU support:\npip install tensorflow-gpu==1.14\nconda install -y -c conda-forge rdkit deepchem==2.3.0\nNightly build version\nYou install the nightly build version via pip. The nightly version is built by the HEAD of DeepChem.\npip install tensorflow==2.3.0\npip install --pre deepchem\nRDKit is a soft requirement package, but many useful methods like molnet depend on it. We recommend installing RDKit with deepchem if you use conda.\nconda install -y -c conda-forge rdkit\nDocker\nIf you want to install deepchem using a docker, you can pull two kinds of images.\nDockerHub : https:\/\/hub.docker.com\/repository\/docker\/deepchemio\/deepchem\n\ndeepchemio\/deepchem:x.x.x\n\nImage built by using a conda package manager (x.x.x is a version of deepchem)\nThe x.x.x image is built when we push x.x.x. tag\nDockerfile is put in docker\/conda-forge directory\n\n\ndeepchemio\/deepchem:latest\n\nImage built by the master branch of deepchem source codes\nThe latest image is built every time we commit to the master branch\nDockerfile is put in docker\/master directory\n\n\n\nYou pull the image like this.\ndocker pull deepchemio\/deepchem:2.3.0\nIf you want to know docker usages with deepchem in more detail, please check the document.\nFrom source\nIf you try install all soft dependencies at once or contribute to deepchem, we recommend you should install deepchem from source.\nPlease check this introduction.\nGetting Started\nThe DeepChem project maintains an extensive collection of tutorials. All tutorials are designed to be run on Google colab (or locally if you prefer). Tutorials are arranged in a suggested learning sequence which will take you from beginner to proficient at molecular machine learning and computational biology more broadly.\nAfter working through the tutorials, you can also go through other examples. To apply deepchem to a new problem, try starting from one of the existing examples or tutorials and modifying it step by step to work with your new use-case. If you have questions or comments you can raise them on our gitter.\nGitter\nJoin us on gitter at https:\/\/gitter.im\/deepchem\/Lobby. Probably the easiest place to ask simple questions or float requests for new features.\nAbout Us\nDeepChem is managed by a team of open source contributors. Anyone is free to join and contribute!\nCiting DeepChem\nIf you have used DeepChem in the course of your research, we ask that you cite the \"Deep Learning for the Life Sciences\" book by the DeepChem core team.\nTo cite this book, please use this bibtex entry:\n@book{Ramsundar-et-al-2019,\n    title={Deep Learning for the Life Sciences},\n    author={Bharath Ramsundar and Peter Eastman and Patrick Walters and Vijay Pande and Karl Leswing and Zhenqin Wu},\n    publisher={O'Reilly Media},\n    note={\\url{https:\/\/www.amazon.com\/Deep-Learning-Life-Sciences-Microscopy\/dp\/1492039837}},\n    year={2019}\n}\n\nVersion\n2.4.0-rc\n","90":"PySB\n\n\n\nPython Systems Biology modeling framework\nhttp:\/\/pysb.org\/\nPySB (pronounced \"Pie Ess Bee\") is a framework for building rule-based\nmathematical models of biochemical systems. It works nicely with\nscientific Python libraries such as NumPy, SciPy and SymPy for model\nsimulation and analysis.\n\nInstallation\nPySB depends on the following:\n\n\nnumpy\nscipy\nsympy\nPerl - http:\/\/www.perl.org\/get.html\nBioNetGen - http:\/\/bionetgen.org\/\n\n\nFor full instructions, see the Installation chapter of the manual at\nhttp:\/\/docs.pysb.org\/en\/latest\/installation.html\n\nDocumentation\nThe manual is available online at http:\/\/docs.pysb.org\/. You can also\ngenerate the documentation locally by installing Sphinx and running\nthe following commands:\n$ cd doc\n$ make html\n\nThen open _build\/html\/index.html in your web browser.\n","91":"biology\none key generate biology 3D modul by blender,such as animal,plant,micra\n\n","92":"Tools for Data Science in Genomic Sequencing.\nThis repository includes projects using Python, R Language, Bioconductor, and Galaxy to understand, analyze, and interpret data from next-generation sequencing experiments.\nThe following projects are included:\n1. Galaxy for Next Generation Sequencing\nSet up a workflow to analyse DNA polymorphic sites of father-mother-child sequencing samples\n2. Python for DNA Sequencing Analysis\nThis project includes a set of tools for DNA sequence analysis\n(\na. check records in file (count_records),\nb. compute the length of each DNA sequence (check_length),\nc. identify open read frame in each DNA sequence (orf_identifier),\nd. identify repeated motif in sequence (repeats_identifier)\n).\n3. Linux Command Line Tools for Data Science in Biology\nThis project apply bowtie2, samtools, bedtools and bcftools to:\n\nAnalyze RNA-seq data to determine sets of genes that are expressed in the various tissues;\ncataloging genetic variation using SAMtools and BEDtools as well as other Unix commands;\ndevelop a pipeline for variant calling in a given genome; 4) perform the bioinformatics analysis to determine genes that      are differentially expressed at different experimental conditions with Linux command line.\n\n4. Sequencing Alignment and Genome Assembly\nPython is used to implement key algorithms and data structures to analyze real genomes and DNA sequencing datasets.\n5. R and Bioconductor\nUse tools from the Bioconductor project and R programming language to analyze genomic data.\n6. Statistics with R for Genomic Data\nAn introduction to the statistics for analysis of genomic data.\n","93":"\nAbout OpenWorm\nOpenWorm aims to build the first comprehensive computational model of Caenorhabditis elegans (C. elegans), a microscopic roundworm. With only a thousand cells, it solves basic problems such as feeding, mate-finding and predator avoidance. Despite being extremely well-studied in biology, a deep, principled understanding of the biology of this organism remains elusive.\nWe are using a bottom-up approach, aimed at observing the worm behaviour emerge from a simulation of data derived from scientific experiments carried out over the past decade. To do so, we are incorporating the data available from the scientific community into software models. We are also forging new collaborations with universities and research institutes to collect data that fill in the gaps.\nYou can earn a badge with us simply by trying out this package! Click on the image below to get started.\n\nQuickstart\nWe have put together a Docker container that pulls together the major components of our simulation and runs it on your machine.  When you get it all running it does the following:\n\nRun our nervous system model, known as c302, on your computer.\nIn parallel, run our 3D worm body model, known as Sibernetic, on your computer, using the output of the nervous system model.\nProduce graphs from the nervous system and body model that demonstrate its behavior on your computer for you to inspect.\nProduce a movie showing the output of the body model.\n\nExample Output\n\n\nNOTE: Running the simulation for the full amount of time would produce content like the above.  However, in order to run in a reasonable amount of time, the default run time for the simulation is limited.  As such, you will see only a partial output, equivalent to about 5% of run time, compared to the examples above.  To extend the run time, use the -d argument as described below.\nInstallation\nPre-requisites:\n\nYou should have at least 60 GB of free space on your machine and at least 2GB of RAM\nYou should be able to clone git repositories on your machine. Install git, or this GUI may be useful.\n\nTo Install:\n\nInstall Docker on your system.\nIf your system does not have enough free space, you can use\nan external hard disk.  On MacOS X, the location for image storage\ncan be specified in the Advanced Tab in Preferences.  See this thread\nin addition for Linux instructions.\n\nRunning\n\nEnsure the Docker daemon is running in the background (on MacOS\/Windows there should be an icon with the Docker whale logo showing in the menu bar\/system tray).\nOpen a terminal and run: git clone http:\/\/github.com\/openworm\/openworm; cd openworm\nOptional: Run .\/build.sh (or build.cmd on Windows). If you skip this step, it will download the latest released Docker image from the OpenWorm Docker hub.\nRun .\/run.sh (or run.cmd on Windows).\nAbout 5-10 minutes of output will display on the screen as the steps run.\nThe simulation will end.  Run stop.sh (stop.cmd on Windows) on your system to clean up the running container.\nInspect the output in the output directory on your local machine.\n\nAdvanced\nArguments\n\n-d [num] : Use to modify the duration of the simulation in milliseconds.  Default is 15.  Use 5000 to run for time to make the full movie above (i.e. 5 seconds).\n\nOther things to try\n\nOpen a terminal and run .\/run-shell-only.sh (or run-shell-only.cmd on Windows).  This will let you log into the container before it has run master_openworm.py.  From here you can inspect the internals of the various checked out code bases and installed systems and modify things. Afterwards you'll still need to run .\/stop.sh to clean up.\nIf you wish to modify what gets installed, you should modify Dockerfile.  If you want to modify what runs, you should modify master_openworm.py.  Either way you will need to run build.sh in order to rebuild the image locally.  Afterwards you can run normally.\n\nFAQ\nWhat is the Docker container?\nThe Docker container is a self-contained environment in which you can run OpenWorm simulations.  It's fully set up to get you started by following the steps above. At the moment,\nit runs simulations and produces visualizations for you, but these visualizations must be viewed outside of the Docker container. While you do not need to know\nmuch about Docker to use OpenWorm, if you are planning on working extensively with the platform, you may benefit\nfrom understanding some basics. Docker Curriculum\nis an excellent tutorial for beginners that is straightforward to work through (Sections 1 - 2.5 are plenty sufficient).\nIs it possible to modify the simulation without having to run build.sh?\nYes, but it is marginally more complex.  The easiest way is to modify anything in the Docker container once you are inside of it - it will work just like a bash shell.  If you want to modify any code in the container, you'll need to use an editor that runs in the terminal, like nano.  Once you've modified something in the container, you don't need to re-build.  However, if you run stop.sh once you exit, those changes will be gone.\nHow do I access more data than what is already output?\nThe simulation by default outputs only a few figures and movies to your home system (that is, outside of the Docker container).  If you want to access the entire output of the simulation, you will need to copy it from the Docker container.\nFor example, say you want to extract the worm motion data.  This is contained in the file worm_motion_log.txt, which is found in the \/home\/ow\/sibernetic\/simulations\/[SPECIFIC_TIMESTAMPED_DIRECTORY]\/worm_motion_log.txt.  The directory [SPECIFIC_TIMESTAMPED_DIRECTORY] will have a name like C2_FW_2018_02-12_18-36-32, and its name can be found by checking the output directory.  This is actually the main output directory for the simulation, and contains all output, including cell modelling and worm movement.\nOnce the simulation ends and you exit the container with exit, but before you run stop.sh, run the following command from the openworm-docker-master folder:\ndocker cp openworm:\/home\/ow\/sibernetic\/simulations\/[SPECIFIC_TIMESTAMPED_DIRECTORY]\/worm_motion_log.txt .\/worm_motion_log.txt\nThis will copy the file from the Docker container, whose default name is openworm.  It is crucial that you do not run stop.sh before trying to get your data out (see below)\nWhat is the difference between exit and stop.sh?\nWhen you are in the Docker Container openworm, and are done interacting with it, you type exit to return to your system's shell.  This stops execution of anything in the container, and that container's status is now Exited.  If you try to re-start the process using run-shell-only.sh, you will get an error saying that the container already exists.  You can choose, at this point, to run stop.sh.  Doing so will remove the container and any files associated with it, allowing you to run a new simulation.  However, if you don't want to remove that container, you will instead want to re-enter it.\nHow do I enter a container I just exited?\nIf you run stop.sh you'll delete your data and reset the container for a new run.  If, however, you don't want to do that, you can re-enter the Docker container like this:\ndocker start openworm                 # Restarts the container\ndocker exec -it openworm \/bin\/bash    # Runs bash inside the container\n\nThis tells Docker to start the container, to execute commands (exec) with an interactive, tty (-it)  bash (bash) shell in the container openworm.\nYou'll be able to interact with the container as before.\nDocumentation\nFind out more about OpenWorm.  Documentation is available at http:\/\/docs.openworm.org.  Join us on Slack.\nThis repository references:\n\nProject-wide tracking via high-level issues and milestones\n\n","94":"Programming For Biology 2019\nprogrammingforbiology.org\nInstructors\nSimon Prochnik\nSofia Robb\n\nBig Picture\n\nWhy?\nHelpful Tips\n\n\nUnix\n\nUnix 1\n\nUnix Overview\n\nWhat is the Command-Line?\n\n\nThe Basics\n\nLogging into Your Workstation\nBringing up the Command-Line\nOK. I've Logged in.  What Now?\nCommand-Line Prompt\nIssuing Commands\nCommand-Line Editing\nWildcards\nHome Sweet Home\nGetting Around\nEssential Unix Commands\nGetting Information About Commands\nFinding Out What Commands are on Your Computer\nArguments and Command Line Switches\nSpaces and Funny Characters\nUseful Commands\nManipulating Directories\nNetworking\nStandard I\/O and Redirection\nA Simple Example\nRedirection Meta-Characters\nFilters, Filenames, and Standard Input\nStandard I\/O and Pipes\nMore Pipe Idioms\n\n\nMore Unix\nLink to Unix 1 Problem Set\n\n\nUnix 2\n\nText Editors\nIntroduction to vi\n\nGetting Started with vi\nCreating, Writing, And Saving a File Walk through\nCommon Activities and vi Commands\nOther Useful Tips\nMug of vi\n\n\n\n\nGit for Beginners\n\nThe Big Picture.\n\nCollaboration\nStoring Versions\nRestoring Previous Versions\nBackup\nThe Details\n\n\nThe Basics\nCreating a new repository\n\nKeeping track of differences between local and remote repositories\nDeleting and moving files\nGet a copy of file on your remote\nTips\n\n\nCloning a Repository\n\nBringing Changes in from the Remote Repository to your Local Repository\nLinks to slightly less basic topics\n\n\nLink To Unix 2 Problem Set\n\n\n\n\nPython Lectures\n\nPython 1\n\nPython Overview\nRunning Python\n\nInteractive Interpreter\nPython Scripts are Text Files\nRunning Python Scripts\nA quicker\/better way to run python scripts\n\n\nSyntax\n\nPython Variable Names\nNaming conventions for Python Variable Names\nReserved Words\nLines and Indentation\nComments\nBlank Lines\n\n\nData Types and Variables\n\nNumbers and Strings\nLists\nTuples\nDictionary\nCommand line parameters: A Special Built-in List\nWhat kind of object am I working with?\n\n\nLink to Python 1 Problem Set\n\n\nPython 2\n\nOperators\n\nArithmetic Operators\nAssignment Operators\nComparison Operators\nLogical Operators\nMembership Operators\nOperator Precedence\n\n\nTruth\n\nUse bool() to test for truth\n\n\nLogic: Control Statements\n\nIf Statement\nif\/elif\n\n\nNumbers\n\ninteger\nfloating point number\ncomplex number\nConversion functions\nNumeric Functions\n\n\nComparing two numbers\nLink to Python 2 Problem Set\n\n\nPython 3\n\nSequences\nWhat functions go with my object?\nStrings\n\nQuotation Marks\nStrings and the print() function\nprint() and Common Errors\nSpecial\/Escape Characters\nConcatenation\nThe difference between string   and integer  \nDetermine the length of a string\nChanging String Case\nFind and Count\nReplace one string with another\nExtracting a Substring, or Slicing\nReverse a string or a list\nOther String Methods\n\n\nString Formatting\n\nThe format() mini-language\nSummary of special formatting symbols so far\nWhat's the point?\n\n\nLink to Python 3 Problem Set\n\n\nPython 4\n\nLists and Tuples\n\nLists\nTuples\nBack to Lists\nAccessing Values in Lists\nChanging Values in a List\nExtracting a Subset of a List, or Slicing\nList Operators\nList Functions\nList Methods\nBuilding a List one Value at a Time\n\n\nLoops\n\nWhile loop\nWhile Loop Syntax\nInfinite Loops\nFor Loops\nFor Loop Syntax\nLoop Control\nLoop Control: Break\nLoop Control: Continue\nIterators\nList Comprehension\n\n\nLink to Python 4 Problem Set\n\n\nPython 5\n\nDictionaries\n\nCreating a Dictionary\nAccessing Values in Dictionaries\nChanging Values in a Dictionary\nAccessing Each Dictionary Key\/Value\nBuilding a Dictionary one Key\/Value at a Time\nChecking That Dictionary Keys Exist\nDictionary Operators\nBuilding a Dictionary one Key\/Value at a Time using a loop\nSorting Dictionary Keys\nDictionary Functions\nDictionary Methods\n\n\nSets\n\nSet Operators\nSet Functions\nSet Methods\nBuild a dictionary of NT counts using a set and loops\n\n\nLink to Python 5 Problem Set\n\n\nPython 6\n\nI\/O and Files\n\nWriting to the Screen\nReading input from the keyboard\nReading from a File\nOpen a File\nReading the contents of a file\nOpening a file with with open() as fh:\nWriting to a File\nBuilding a Dictionary from a File\n\n\nLink to Python 6 Problem Set\n\n\nPython 7\n\nRegular Expressions\n\nIndividual Characters\nCharacter Classes\nAnchors\nQuantifiers\nVariables and Patterns\nEither Or\nSubpatterns\nUsing Subpatterns Inside the Regular Expression Match\nUsing Subpatterns Outside the Regular Expression\nGet position of the subpattern with finditer()\nSubpatterns and Greediness\nPractical Example: Codons\nTruth and Regular Expression Matches\nUsing Regular expressions in substitutions\nUsing subpatterns in the replacement\nRegular Expression Option Modifiers\n\n\nHelpful Regex tools\nLink to Python 7 Problem Set\n\n\nPython 8\n\nData Structures\n\nList of lists\nLists of dictionaries\nDictionaries of lists\nDictionaries of dictionaries\nBuilding Complex Datastructures\n\n\nLink to Python 8 Problem Set\n\n\nPython 9\n\nExceptions\n\ntry\/except\/else\/finally\nGetting more information about an exception\nRaising an Exception\nCreating Custom Exceptions\n\n\nLink to Python 9 Problem Set\n\n\nPython 10\n\nFunctions\n\nDefining a Function that calculates GC Content\nUsing your function to calculate GC content\nThe details\nNaming Arguments\nKeyword Arguments\nDefault Values for Arguments\nLambda expressions\n\n\nScope\n\nLocal Variables\nGlobal\n\n\nModules\n\nGetting information about modules with pydoc\nos.path\nos.system\nsubprocess\n\nCapturing output from a shell pipeline\nCapturing output the long way (for a single command)\nCheck the exit status of a command\nRun a command with that redirects stdout to a file using python subprocess\n\n\nsys\nre\ncollections\ncopy\nmath\nrandom\nstatistics\nglob\nargparse\n\n\nMany more modules that do many things\nLink to Python 10 Problem Set\n\n\nPython 11\n\nClasses\n\nYou have been using classes to create objects\nattributes and methods\nCreating a Class\nCreating a DNARecord Object\nRetrieving attribute values\nUsing class methods\nGetting data into a new instance of our class\n\ninit\n\n\n\n\nLink to Python 11 Problem Set\n\n\n\n\nWorkshops\n\n","95":"Chainer Chemistry: A Library for Deep Learning in Biology and Chemistry\n\n\n\n\n\n\n\nChainer Chemistry is a deep learning framework (based on Chainer) with\napplications in Biology and Chemistry. It supports various state-of-the-art\nmodels (especially GCNN - Graph Convolutional Neural Network) for chemical property prediction.\nFor more information, please refer to the documentation.\nAlso, a quick introduction to deep learning for molecules and Chainer Chemistry\nis available here.\nDependencies\nChainer Chemistry depends on the following packages:\n\nchainer\npandas\nscikit-learn\ntqdm\nh5py\n\nThese are automatically added to the system when installing the library via the\npip command (see Installation). However, the following  needs to be\ninstalled manually:\n\nrdkit (release 2019.03.2.0)\n\nPlease refer to the RDKit documentation\nfor more information regarding the installation steps.\nNote that only the following versions of Chainer Chemistry's dependencies are\ncurrently supported:\n\n\n\nChainer Chemistry\nChainer\nRDKit\nPython\n\n\n\n\nv0.1.0 ~ v0.3.0\nv2.0 ~ v3.0\n2017.09.3.0\n2.7, 3.5, 3.6\n\n\nv0.4.0\nv3.0 ~ v4.0 *1\n2017.09.3.0\n2.7, 3.5, 3.6\n\n\nv0.5.0\nv3.0 ~ v5.0 *2\n2017.09.3.0\n2.7, 3.5, 3.6\n\n\nv0.6.0\nv6.0 ~      *3\n2017.09.3.0\n2.7, 3.5, 3.6\n\n\nv0.7.0 ~ v0.7.1\nv7.0 ~\n2019.03.2.0\n3.6, 3.7      *4\n\n\nmaster branch    *5\nv7.0 ~\n2019.03.2.0\n3.6, 3.7\n\n\n\n[Footnote]\n*1: We used FunctionNode in this PR,\nwhich is introduced after chainer v3. See this issue for details.\n*2: Saliency modules only work after chainer v5.\n*3: Chainer v6 is released and ChainerX is newly introduced.\nIn order to support this new feature & API, we broke backward compatibility for chainer chemistry v0.6.0 release.\nSee ChainerX Documentation for details.\n*4: python 2.x support is dropped, following the same policy with chainer and rdkit.\n*5: As announced in chainer blog,\nfurther development will be limited to only serious bug-fixes and maintenance.\nInstallation\nChainer Chemistry can be installed using the pip command, as follows:\npip install chainer-chemistry\n\nExample to install rdkit with conda:\n# newer conda version is necessary to install rdkit 2019.03.2.0\nconda install -n base conda==4.6.14\nconda install -c rdkit rdkit==2019.03.2.0\nIf you would like to use the latest sources, please checkout the master branch\nand install with the following commands:\ngit clone https:\/\/github.com\/pfnet-research\/chainer-chemistry.git\npip install -e chainer-chemistry\n\nSample Code\nSample code is provided with this repository. This includes, but is not limited\nto, the following:\n\nTraining a new model on a given dataset\nPerforming inference on a given dataset, using a pretrained model\nEvaluating and reporting performance metrics of different models on a given\ndataset\n\nPlease refer to the examples directory for more information.\nSupported Models\nThe following graph convolutional neural networks are currently supported:\n\nNFP: Neural Fingerprint [2, 3]\nGGNN: Gated Graph Neural Network [4, 3]\nWeaveNet [5, 3]\nSchNet [6]\nRSGCN: Renormalized Spectral Graph Convolutional Network [10]\n* The name is not from the original paper - see PR #89 for the naming convention.\nRelGCN: Relational Graph Convolutional Network [14]\nGAT: Graph Attention Networks [15]\nGIN: Graph Isomorphism Networks [17]\nMPNN: Message Passing Neural Networks [3]\nSet2Set [19]\nGNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation [20]\nMEGNet: MatErials Graph Network [24]\nCGCNN: Crystal Graph Convolutional Neural Networks [25]\n\nWe test supporting the brand-new Graph Warp Module (GWM) [18]-attached models for:\n\nNFP ('nfp_gwm')\nGGNN ('ggnn_gwm')\nRSGCN ('rsgcn_gwm')\nGIN ('gin_gwm')\n\nIn the directory examples\/molnet_wle, we have implemented the new preprocessing ''Weisfeiler-Lehman Embedding for Molecular Graph Neural Networks'' [26] for several GNN architectures. Please find the Readme in that directory for the usage and the details.\nSupported Datasets\nThe following datasets are currently supported:\nChemical\n\nQM9 [7, 8]\nTox21 [9]\nMoleculeNet [11]\nZINC (only 250k dataset) [12, 13]\nUser (own) dataset\n\nNetwork\n\ncora [21]\nciteseer [22]\nreddit [23]\n\nResearch Projects\nIf you use Chainer Chemistry in your research, feel free to submit a\npull request and add the name of your project to this list:\n\nBayesGrad: Explaining Predictions of Graph Convolutional Networks (paper, code)\nGraph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks (paper, code)\nGraphNVP: An Invertible Flow Model for Generating Molecular Graphs (paper, code)\nGraph Residual Flow for Molecular Graph Generation (paper)\n\nUseful Links\nChainer Chemistry:\n\nDocumentation\nResearch Blog\n\nOther Chainer frameworks:\n\nChainer: A Flexible Framework of Neural Networks for Deep Learning\nChainerRL: Deep Reinforcement Learning Library Built on Top of Chainer\nChainerCV: A Library for Deep Learning in Computer Vision\nChainerMN: Scalable Distributed Deep Learning with Chainer\nChainerUI: User Interface for Chainer\n\nLicense\nThis project is released under the MIT License. Please refer to the\nthis page\nfor more information.\nPlease note that Chainer Chemistry is still in experimental development.\nWe continuously strive to improve its functionality and performance, but at\nthis stage we cannot guarantee the reproducibility of any results published in\npapers. Use the library at your own risk.\nReferences\n[1] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of Workshop on Machine Learning Systems (LearningSys) in Advances in Neural Information Processing System (NIPS) 28, 2015.\n[2] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems (NIPS) 28, pages 2224\u20132232. Curran Asso- ciates, Inc., 2015.\n[3] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\n[4] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.\n[5] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):595\u2013608, 2016.\n[6] Kristof Sch\u00fctt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Rober M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems (NIPS) 30, pages 992\u20131002. Curran Associates, Inc., 2017.\n[7] Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17. Journal of chemical information and modeling, 52(11):2864\u20132875, 2012.\n[8] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1:140022, 2014.\n[9] Ruili Huang, Menghang Xia, Dac-Trung Nguyen, Tongan Zhao, Srilatha Sakamuru, Jinghua Zhao, Sampada A Shahane, Anna Rossoshek, and Anton Simeonov. Tox21challenge to build predictive models of nuclear receptor and stress response pathways as mediated by exposure to environmental chemicals and drugs. Frontiers in Environmental Science, 3:85, 2016.\n[10] Kipf, Thomas N. and Welling, Max. Semi-Supervised Classification with Graph Convolutional Networks. International Conference on Learning Representations (ICLR), 2017.\n[11] Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017.\n[12] J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman. Zinc: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52(7):1757\u20131768, 2012.\n[13] Preprocessed csv file downloaded from https:\/\/raw.githubusercontent.com\/aspuru-guzik-group\/chemical_vae\/master\/models\/zinc_properties\/250k_rndm_zinc_drugs_clean_3.csv\n[14] Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling. Modeling Relational Data with Graph Convolutional Networks. Extended Semantic Web Conference (ESWC), 2018.\n[15] Veli\u010dkovi\u0107, P., Cucurull, G., Casanova, A., Romero, A., Li\u00f2, P., & Bengio, Y. (2017). Graph Attention Networks. arXiv preprint arXiv:1710.10903.\n[16] Dan Busbridge, Dane Sherburn, Pietro Cavallo and Nils Y. Hammerla. (2019). Relational Graph Attention Networks. https:\/\/openreview.net\/forum?id=Bklzkh0qFm\n[17] Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, ``How Powerful are Graph Neural Networks?'', \tarXiv:1810.00826 [cs.LG], 2018 (to appear at ICLR19).\n[18] K. Ishiguro, S. Maeda, and M. Koyama, ``Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks'', arXiv:1902.01020 [cs.LG], 2019.\n[19] Oriol Vinyals, Samy Bengio, Manjunath Kudlur. Order Matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015.\n[20] Marc Brockschmidt, ``GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation'', arXiv:1906.12192 [cs.ML], 2019.\n[21] McCallum, Andrew Kachites and Nigam, Kamal and Rennie, Jason and Seymore, Kristie, Automating the Construction of Internet Portals with Machine Learning. Information Retrieval, 2000.\n[22] C. Lee Giles and Kurt D. Bollacker and Steve Lawrence, CiteSeer: An Automatic Citation Indexing System. Proceedings of the Third ACM Conference on Digital Libraries, 1998.\n[23] William L. Hamilton and Zhitao Ying and Jure Leskovec, Inductive Representation Learning on Large Graphs. Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017\n[24] Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong. Graph networks as a universal machine learning framework for molecules and crystals. Chemistry of Materials, 31(9):3564\u20133572, 2019.\n[25] Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Physical review letters, 120(14):145301, 2018.\n[26] Katsuhiko Ishiguro, Kenta Oono, and Kohei Hayashi, \"Weisfeiler-Lehman Embedding for Molecular Graph Neural Networks\", arXiv: 2006.06909, 2020. paper link\n","96":"\n\n\n\n\n\n\nBiotite project\nBiotite is your Swiss army knife for bioinformatics.\nWhether you want to identify homologous sequence regions in a protein family\nor you would like to find disulfide bonds in a protein structure: Biotite\nhas the right tool for you.\nThis package bundles popular tasks in computational molecular biology\ninto a uniform Python library.\nIt can handle a major part of the typical workflow\nfor sequence and biomolecular structure data:\n\n\nSearching and fetching data from biological databases\nReading and writing popular sequence\/structure file formats\nAnalyzing and editing sequence\/structure data\nVisualizing sequence\/structure data\nInterfacing external applications for further analysis\n\n\nBiotite internally stores most of the data as NumPy ndarray objects,\nenabling\n\n\nfast C-accelerated analysis,\nintuitive usability through NumPy-like indexing syntax,\nextensibility through direct access of the internal NumPy arrays.\n\n\nAs a result the user can skip writing code for basic functionality (like\nfile parsers) and can focus on what their code makes unique - from\nsmall analysis scripts to entire bioinformatics software packages.\nIf you use Biotite in a scientific publication, please cite:\n\nKunzmann, P. & Hamacher, K. BMC Bioinformatics (2018) 19:346.\nhttps:\/\/doi.org\/10.1186\/s12859-018-2367-z\n\n\nInstallation\nBiotite requires the following packages:\n\n\nnumpy\nrequests\nmsgpack\n\n\nSome functions require some extra packages:\n\n\nmdtraj - Required for trajetory file I\/O operations.\nmatplotlib - Required for plotting purposes.\n\n\nBiotite can be installed via Conda...\n$ conda install -c conda-forge biotite\n... or pip\n$ pip install biotite\n\nUsage\nHere is a small example that downloads two protein sequences from the\nNCBI Entrez database and aligns them:\nimport biotite.sequence.align as align\nimport biotite.sequence.io.fasta as fasta\nimport biotite.database.entrez as entrez\n\n# Download FASTA file for the sequences of avidin and streptavidin\nfile_name = entrez.fetch_single_file(\n    uids=[\"CAC34569\", \"ACL82594\"], file_name=\"sequences.fasta\",\n    db_name=\"protein\", ret_type=\"fasta\"\n)\n\n# Parse the downloaded FASTA file\n# and create 'ProteinSequence' objects from it\nfasta_file = fasta.FastaFile.read(file_name)\navidin_seq, streptavidin_seq = fasta.get_sequences(fasta_file).values()\n\n# Align sequences using the BLOSUM62 matrix with affine gap penalty\nmatrix = align.SubstitutionMatrix.std_protein_matrix()\nalignments = align.align_optimal(\n    avidin_seq, streptavidin_seq, matrix,\n    gap_penalty=(-10, -1), terminal_penalty=False\n)\nprint(alignments[0])\nMVHATSPLLLLLLLSLALVAPGLSAR------KCSLTGKWDNDLGSNMTIGAVNSKGEFTGTYTTAV-TA\n-------------------DPSKESKAQAAVAEAGITGTWYNQLGSTFIVTA-NPDGSLTGTYESAVGNA\n\nTSNEIKESPLHGTQNTINKRTQPTFGFTVNWKFS----ESTTVFTGQCFIDRNGKEV-LKTMWLLRSSVN\nESRYVLTGRYDSTPATDGSGT--ALGWTVAWKNNYRNAHSATTWSGQYV---GGAEARINTQWLLTSGTT\n\nDIGDDWKATRVGINIFTRLRTQKE---------------------\n-AANAWKSTLVGHDTFTKVKPSAASIDAAKKAGVNNGNPLDAVQQ\n\nMore documentation, including a tutorial, an example gallery and the API\nreference is available at https:\/\/www.biotite-python.org\/.\n\nContribution\nInterested in improving Biotite?\nHave a look at the\ncontribution guidelines.\nFeel free to join or community chat on Discord.\n","97":"DGL-LifeSci\nDocumentation | Discussion Forum\nWe also have a slack channel for real-time discussion. If you want to join the channel, contact mufeili1996@gmail.com.\nIntroduction\nDeep learning on graphs has been an arising trend in the past few years. There are a lot of graphs in\nlife science such as molecular graphs and biological networks, making it an import area for applying\ndeep learning on graphs. DGL-LifeSci is a DGL-based package for various applications in life science\nwith graph neural networks.\nWe provide various functionalities, including but not limited to methods for graph construction,\nfeaturization, and evaluation, model architectures, training scripts and pre-trained models.\nFor a list of community contributors, see here.\nFor a full list of work implemented in DGL-LifeSci, see here.\nInstallation\nRequirements\nDGL-LifeSci should work on\n\nall Linux distributions no earlier than Ubuntu 16.04\nmacOS X\nWindows 10\n\nDGL-LifeSci requires python 3.6+, DGL 0.5.2+ and PyTorch 1.5.0+.\nInstall pytorch\nInstall dgl\nAdditionally, we require RDKit 2018.09.3 for utils related to cheminformatics. We recommend installing it with\nconda install -c rdkit rdkit==2018.09.3\n\nFor other installation recipes for RDKit, see the official documentation.\nPip installation for DGL-LifeSci\npip install dgllife\n\nConda installation for DGL-LifeSci\npip install hyperopt\nconda install -c dglteam dgllife\n\nInstallation from source\nIf you want to try experimental features, you can install from source as follows:\ngit clone https:\/\/github.com\/awslabs\/dgl-lifesci.git\ncd dgl-lifesci\/python\npython setup.py install\n\nVerifying successful installation\nOnce you have installed the package, you can verify the success of installation with\nimport dgllife\n\nprint(dgllife.__version__)\n# 0.2.6\nIf you are new to DGL, the first time you import dgl a message will pop up as below:\nDGL does not detect a valid backend option. Which backend would you like to work with?\nBackend choice (pytorch, mxnet or tensorflow):\n\nand you need to enter pytorch.\nExample Usage\nTo apply graph neural networks to molecules with DGL, we need to first construct DGLGraph --\nthe graph data structure in DGL and prepare initial node\/edge features. Below gives an example of\nconstructing a bi-directed graph from a molecule and featurizing it with atom and bond features such\nas atom type and bond type.\nfrom dgllife.utils import smiles_to_bigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n\n# Node featurizer\nnode_featurizer = CanonicalAtomFeaturizer(atom_data_field='h')\n# Edge featurizer\nedge_featurizer = CanonicalBondFeaturizer(bond_data_field='h')\n# SMILES (a string representation for molecule) for Penicillin\nsmiles = 'CC1(C(N2C(S1)C(C2=O)NC(=O)CC3=CC=CC=C3)C(=O)O)C'\ng = smiles_to_bigraph(smiles=smiles, \n                      node_featurizer=node_featurizer,\n                      edge_featurizer=edge_featurizer)\nprint(g)\n\"\"\"\nDGLGraph(num_nodes=23, num_edges=50,\n         ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n         edata_schemes={'h': Scheme(shape=(12,), dtype=torch.float32)})\n\"\"\"\nWe implement various models that users can import directly. Below gives an example of defining a GCN-based model\nfor molecular property prediction.\nfrom dgllife.model import GCNPredictor\n\nmodel = GCNPredictor(in_feats=1)\nFor a full example of applying GCNPredictor, run the following command\npython examples\/property_prediction\/classification.py -m GCN -d Tox21\nFor more examples on molecular property prediction, generative models, protein-ligand binding affinity\nprediction and reaction prediction, see examples.\nWe also provide pre-trained models for most examples, which can be used off-shelf without training from scratch.\nBelow gives an example of loading a pre-trained model for GCNPredictor on a molecular property prediction dataset.\nfrom dgllife.data import Tox21\nfrom dgllife.model import load_pretrained\nfrom dgllife.utils import smiles_to_bigraph, CanonicalAtomFeaturizer\n\ndataset = Tox21(smiles_to_bigraph, CanonicalAtomFeaturizer())\nmodel = load_pretrained('GCN_Tox21') # Pretrained model loaded\nmodel.eval()\n\nsmiles, g, label, mask = dataset[0]\nfeats = g.ndata.pop('h')\nlabel_pred = model(g, feats)\nprint(smiles)                   # CCOc1ccc2nc(S(N)(=O)=O)sc2c1\nprint(label_pred[:, mask != 0]) # Mask non-existing labels\n# tensor([[ 1.4190, -0.1820,  1.2974,  1.4416,  0.6914,  \n# 2.0957,  0.5919,  0.7715, 1.7273,  0.2070]])\nSimilarly, we can load a pre-trained model for generating molecules. If possible, we recommend running\nthe code block below with Jupyter notebook.\nfrom dgllife.model import load_pretrained\n\nmodel = load_pretrained('DGMG_ZINC_canonical')\nmodel.eval()\nsmiles = []\nfor i in range(4):\n    smiles.append(model(rdkit_mol=True))\n\nprint(smiles)\n# ['CC1CCC2C(CCC3C2C(NC2=CC(Cl)=CC=C2N)S3(=O)=O)O1',\n# 'O=C1SC2N=CN=C(NC(SC3=CC=CC=N3)C1=CC=CO)C=2C1=CCCC1', \n# 'CC1C=CC(=CC=1)C(=O)NN=C(C)C1=CC=CC2=CC=CC=C21', \n# 'CCN(CC1=CC=CC=C1F)CC1CCCN(C)C1']\nIf you are running the code block above in Jupyter notebook, you can also visualize the molecules generated with\nfrom IPython.display import SVG\nfrom rdkit import Chem\nfrom rdkit.Chem import Draw\n\nmols = [Chem.MolFromSmiles(s) for s in smiles]\nSVG(Draw.MolsToGridImage(mols, molsPerRow=4, subImgSize=(180, 150), useSVG=True))\n\nSpeed Reference\nBelow we provide some reference numbers to show how DGL improves the speed of training models per epoch in seconds.\n\n\n\nModel\nOriginal Implementation\nDGL Implementation\nImprovement\n\n\n\n\nGCN on Tox21\n5.5 (DeepChem)\n1.0\n5.5x\n\n\nAttentiveFP on Aromaticity\n6.0\n1.2\n5x\n\n\nJTVAE on ZINC\n1826\n743\n2.5x\n\n\nWLN for reaction center prediction\n11657\n858 (1 GPU) \/ 134 (8 GPUs)\n13.6x (1GPU) \/ 87.0x (8GPUs)\n\n\nWLN for candidate ranking\n40122\n22268\n1.8x\n\n\n\n","98":"ssbio: A Framework for Structural Systems Biology\n\nIntroduction\nThis Python package provides a collection of tools for people with questions in the realm of structural systems biology. The main goals of this package are to:\n\nProvide an easy way to map hundreds or thousands of genes to their encoded protein sequences and structures\nDirectly link protein structures to genome-scale metabolic models\nDemonstrate fully-featured Python scientific analysis environments in Jupyter notebooks\n\nExample questions you can (start to) answer with this package:\n\nHow can I determine the number of protein structures available for my list of genes?\nWhat is the best, representative structure for my protein?\nWhere, in a metabolic network, do these proteins work?\nWhere do popular mutations show up on a protein?\nHow can I compare the structural features of entire proteomes?\nHow do structural properties correlate with my experimental datasets?\nHow can I improve the contents of my metabolic model with structural data?\n\n\nTry it without installing\n\n\n\nNote\nBinder notebooks are still in beta, but they mostly work! Third-party programs are also preinstalled in the Binder notebooks except I-TASSER and TMHMM due to licensing restrictions.\n\n\nInstallation\nFirst install NGLview using pip, then install ssbio\npip install nglview\njupyter-nbextension enable nglview --py --sys-prefix\npip install ssbio\n\nUpdating\npip install ssbio --upgrade\n\nUninstalling\npip uninstall ssbio\n\nDependencies\nSee: Software for a list of external programs to install, along with the functionality that they add. Most of these additional programs are used to predict or calculate properties of proteins, and are only required if you desire to calculate the described properties.\n\nTutorials\nCheck out some Jupyter notebook tutorials for a single Protein and or for many in a GEM-PRO model. See a list of all Tutorials.\n\nCitation\nThe manuscript for the ssbio package can be found and cited at [1].\n\n\n[1]Mih N, Brunk E, Chen K, Catoiu E, Sastry A, Kavvas E, Monk JM, Zhang Z, Palsson BO. 2018. ssbio: A Python Framework for Structural Systems Biology. Bioinformatics. https:\/\/academic.oup.com\/bioinformatics\/advance-article\/doi\/10.1093\/bioinformatics\/bty077\/4850940.\n\n\n","99":"[toc]\nEasySports\n\u58f0\u660e\n\u672c\u9879\u76ee\u90e8\u5206API\u6765\u81eaNBA\u4e2d\u6587\u5b98\u7f51\u4e0e\u864e\u6251\u4f53\u80b2\uff0c\u7eaf\u7ec3\u624b\u4e4b\u4f5c\uff0c\u4e2a\u4eba\u672a\u4ece\u4e2d\u83b7\u53d6\u4efb\u4f55\u5229\u76ca\uff0c\u5176\u6240\u6709\u5185\u5bb9\u5747\u53ef\u5728NBA\u4e2d\u6587\u5b98\u7f51\u4e0e\u864e\u6251\u4f53\u80b2\u83b7\u53d6\u3002\n\u6570\u636e\u7684\u83b7\u53d6\u4e0e\u5171\u4eab\u53ef\u80fd\u4f1a\u4fb5\u72af\u5230NBA\u4e2d\u6587\u5b98\u7f51\u4e0e\u864e\u6251\u4f53\u80b2\u7684\u6743\u76ca\uff0c\u82e5\u88ab\u544a\u77e5\u9700\u505c\u6b62\u5171\u4eab\u4e0e\u4f7f\u7528\uff0c\u672c\u4eba\u4f1a\u7acb\u5373\u5220\u9664\u6574\u4e2a\u9879\u76ee\u3002\n\u7b80\u4ecb\n\u9879\u76ee\u91c7\u7528 MaterialDesign + MVP + Retrofit2 + RxJava\u5f00\u53d1\n\u5927\u90e8\u5206\u529f\u80fd\u5df2\u7ecf\u5b8c\u6210\uff0c\u5982\u679c\u5bf9\u4f60\u6709\u5e2e\u52a9\u7684\u8bdd\u4e0d\u59a8star\u4e00\u4e2ao(\uffe3\u25bd\uffe3)\u30d6\n\u4e0b\u9762\u6211\u5c31\u6765\u603b\u7ed3\u4e00\u4e0b\u5427\u54c8\u54c8\u54c8\u54c8\n\u4e0b\u8f7d\u5730\u5740\n\u73b0\u5728\u5df2\u7ecf\u5728\u9177\u5b89\u7f51\u4e0a\u67b6\u5566\n\u53ec\u5524\u672f\uff1a\u4f20\u9001\u95e8\uff01\uff01\n\u5e94\u7528\u90e8\u5206\u622a\u56fe\n\n\n\n\n\n\n\n\n\u4e00\u4e9b\u6536\u83b7\n\u79fb\u52a8\u7aef\u548c\u540e\u7aef\u4e00\u628a\u6293\uff0c\u5b66\u4e60\u4e86\u5f88\u591a\n\n\u5c3d\u53ef\u80fd\u7b80\u6d01\u5e72\u51c0\u7684\u7f16\u7801\u89c4\u8303\n\u81ea\u8ba4\u4e3a\u826f\u597d\u7684\u4e1a\u52a1\u5206\u5305\u4e0e\u529f\u80fd\u89e3\u8026\uff0c\u6e05\u6670\u7684\u7f51\u7edc\u8bf7\u6c42\n\u5bf9\u4e09\u65b9\u670d\u52a1\u548c\u6846\u67b6\u7684\u4e8c\u6b21\u5c01\u88c5\n\u7b26\u5408Material Design\u7684\u754c\u9762\uff0c\u826f\u597d\u7684\u52a8\u753b\u8fc7\u5ea6\n\n\u5c01\u88c5\n\u5bf9\u65b9\u6cd5\u7684\u4f7f\u7528\u6216\u8005\u7b2c\u4e09\u65b9\u670d\u52a1\u6216\u662f\u6846\u67b6\u7684\u4f7f\u7528\u90fd\u7528\u5230\u4e86\u5c01\u88c5\n\u8fd9\u91cc\u53ea\u8bf4\u5176\u4e2d\u51e0\u70b9\uff08\u4e5f\u5305\u62ec\u4e00\u4e0b\u81ea\u5b9a\u4e49View\uff09\n\n\u4e00\u4e2a\u57fa\u4e8eMVP\u7684\u5feb\u901f\u5f00\u53d1\u57fa\u7c7b\u5e93rbase\n\nActivity\u548cFragment\u7684\u5e38\u7528\u5c01\u88c5\uff0c\u7ed3\u5408RxJava\n\u5f02\u5e38\u6536\u96c6\u7c7b\u7684\u5c01\u88c5\u5e76\u63d0\u4f9b\u5bf9\u5916\u7684\u670d\u52a1\u5668\u4e0a\u4f20\u5904\u7406\u63a5\u53e3\n\u5de5\u5177\u7c7b\u7684\u5c01\u88c5\uff1aPermissionManager \u3001SPManager\u7b49\n\u7f51\u7edc\u8bf7\u6c42\u7684\u7edf\u4e00\u7ba1\u7406\n\n\n\u597d\u7528\u7684\u5f39\u7a97\u5e93\u7684\u5c01\u88c5dialoglib\n\u7f51\u9875\u6d4f\u89c8\u7684\u5c01\u88c5RWebActivity,\u5176\u5185\u90e8\u96c6\u6210\u4e86\u817e\u8baf\u5f00\u6e90\u6846\u67b6VasSonic\n\u5bf9ZXing\u5e93\u7684\u4e8c\u6b21\u5c01\u88c5\u548c\u81ea\u5b9a\u4e49\u5b9e\u73b0\nProgressLayout \u5e38\u7528\u7684\u52a0\u8f7d\u3001\u9519\u8bef\u5e03\u5c40\nTitleItemDecoration \u597d\u7528\u7684\u7c98\u6027\u5934\u90e8\n\n\u7b2c\u4e09\u65b9\u670d\u52a1\n\nShareSDK\n\u963f\u91cc\u70ed\u4fee\u590d\n\u817e\u8baf\u76f4\u64ad\n\nps:\u672c\u6765\u4e5f\u5f15\u5165\u4e86Bmob\u6784\u5efa\u7528\u6237\u7cfb\u7edf\u548c\u5f02\u5e38\u53cd\u9988\u7cfb\u7edf\uff0c\u5728\u5f15\u5165\u81ea\u5df1\u642d\u5efa\u7684\u540e\u53f0\u63a5\u53e3\u4ee5\u540e\u5c31\u5e9f\u5f03\u6389\u4e86\uff0cbmob\u8fd8\u662f\u6709\u70b9\u5751\u7239\u7684\u3002\n\u6846\u67b6\n\u611f\u8c22\u8fd9\u4e9b\u5f00\u6e90\u6846\u67b6\u7684\u5927\u529b\u652f\u6301\n\nBRAVH \uff1a \u529f\u80fd\u5f3a\u5927\u7684RecyclerViewAdapter\u5c01\u88c5\u5e93\nglide : \u56fe\u7247\u52a0\u8f7d\nGreenDAO : \u6570\u636e\u5e93\u6846\u67b6\nRetrofit : \u4ee3\u7801\u7b80\u6d01\uff0c\u63a5\u53e3\u89e3\u8026\nOkHttp : \u7f51\u7edc\u8bf7\u6c42\nRxJava : \u5feb\u6377\u7684\u7ebf\u7a0b\u5207\u6362\uff0c\u7b80\u6d01\u7684\u4ee3\u7801\uff0c\u6e05\u6670\u7684\u903b\u8f91\uff0c\u548cRetrofit\u914d\u5408\u5f88\u723d\nphotoView : \u56fe\u7247\u7684\u64cd\u4f5c\neventbus \uff1a\u65f6\u95f4\u603b\u7ebf\uff0c\u7ec4\u4ef6\u4e4b\u95f4\u7684\u7b80\u4fbf\u901a\u4fe1\ngson \uff1a JSON\u5e8f\u5217\u5316\nVasSonic : Tecent\u5f00\u6e90\uff0c\u63d0\u5347web\u7684\u9996\u5c4f\u8bf7\u6c42\u901f\u5ea6\nstetho : Facebook\u5f00\u6e90\u7684\u975e\u5e38\u597d\u7528\u8c03\u8bd5\u6846\u67b6\nBottomNavigation : \u5e95\u90e8\u5bfc\u822a\u680f\nJieCaoPlayer : \u64ad\u653e\u5668\uff0c\u5f15\u5165\u5230\u9879\u76ee\u4e2d\uff0c\u5e76\u5728\u5176\u57fa\u7840\u4e0a\u81ea\u5b9a\u4e49\u529f\u80fd\n\n\u63a5\u53e3\n\u5e94\u7528\u7684\u6570\u636e\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u81ea\u5df1\u642d\u5efa\u548c\u6570\u636e\u6293\u53d6\uff08\u6211\u7684\u535a\u5ba2\u7684\u6293\u53d6\u6559\u7a0b\uff09\n\n\u81ea\u5df1\u642d\u5efa\u7684SSM\u540e\u53f0\u7cfb\u7edf\u4f20\u9001\u95e8\uff0c\u5e76\u4e14\u5df2\u7ecf\u53d1\u5e03\u5728\u963f\u91cc\u4e91\u670d\u52a1\u5668\u4e0a\u5566\uff0c\u5e72\u6d3b\u6ee1\u6ee1\u7684\u54df\uff08\u8981\u8138(\u273f\u25e1\u203f\u25e1)\uff09\n\n\u76f4\u64ad\u63a5\u5165\n\u7248\u672c\u66f4\u65b0\n\u7528\u6237\u7ba1\u7406\u7cfb\u7edf\n\u5d29\u6e83\u4fe1\u606f\u91c7\u96c6\n\u7528\u6237\u4fe1\u606f\u53cd\u9988\n\n\n\u5269\u4e0b\u7684\u63a5\u53e3\u662f\u4ece\u864e\u6251\u4f53\u80b2\u3001\u817e\u8baf\u89c6\u9891\u4e2d\u722c\u53d6\u7684\uff0c\u76f8\u5f53\u96f6\u6563\uff0c\u76f8\u5f53\u7e41\u6742\uff0c\u5177\u4f53\u53ef\u4ee5\u770bJsonParser.java\uff0c\u91cc\u9762\u6709\u4e00\u4e9b\u5341\u5206\u5201\u94bb\u7684Json\u6570\u636e\u7684\u5904\u7406\n\nTODO\n\n\u6781\u5149\u63a8\u9001\u524d\u7aef\u548c\u540e\u7aef\u7684\u96c6\u6210\n\u89c6\u9891\u64ad\u653e\u5668\u7684\u66ff\u6362\n\u63d2\u4ef6\u5316\n\n\u6700\u540e\n\u4e2a\u4eba\u535a\u5ba2\n\u672c\u9879\u76ee\u7684\u540e\u53f0\n","100":"SportsTracker\nSportsTracker is an application for people who want to track their sporting\nactivities. It is not bound to a specific kind of sport, the user can create\ncategories for all sport types such as cycling, running, swimming or tennis.\nThe main advantage is a good overview of your exercises, you can easily create\ndiagrams and statistics for specific time ranges and sport types. In the\ncalendar you can also track your body weight or create note entries, e.g. the\ntraining plan or upcoming sport events.\nAll the application data is stored in XML files. So it is very easy to access\nthem with other tools or to write importers and exporters for other\napplications.\nIf you own a heartrate monitor with a computer interface you can import the\nrecorded exercise files and evaluate the diagrams with the integrated\nExerciseViewer application.\nLinks\n\nSportsTracker website: https:\/\/www.saring.de\/sportstracker\/\nSourceForge project site: https:\/\/sourceforge.net\/projects\/sportstracker\/\n\nprovides package downloads and a forum for discussions\n\n\nGitHub project site: https:\/\/github.com\/ssaring\/sportstracker\/\n\nprovides source code management, an issue tracker and pull request processing\n\n\nTravis Continuous Integration: \nSportsTracker documentation: README.txt\nSportsTracker license: GPL v2\n\n","101":"RunMap\n\n\n\n\u622a\u56fe\n\n\n\n\u94fe\u63a5:\nBlog\uff1ahttps:\/\/github.com\/stdnull\/StudyNotes\nLicense\nCopyright 2017 stdnull\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","102":"SportChef\n           \nSports Competition Management Software with a modern and fast architecture. Java based backend with a RESTful JSON API and a HTML 5 client with a modern UI.\nCopyright (C) 2015, 2016 Marcus Fihlon\nThis program is free software: you can redistribute it and\/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.\nYou should have received a copy of the GNU Affero General Public License along with this program.  If not, see http:\/\/www.gnu.org\/licenses\/.\nRunning SportChef\nUsing Docker\nThe SportChef docker image is available on DockerHub. To run SportChef, you have to specify a port mapping to map the ports of the application server inside the container (8080) to a port on your machine (e.g. 80) and you have to specify a folder on your machine to store the permanent data. The complete docker call looks like this:\ndocker run -p [local port]:8080 -v [local data path]:\/root\/.sportchef -it mcpringle\/sportchef\nExample:\ndocker run -p 80:8080 -v \/home\/mcpringle\/.sportchef:\/root\/.sportchef -it mcpringle\/sportchef\nRelease Notes\nVersion 1.1.1\n\nBugfix for infrequent NullPointerException after creating a new event\n\nVersion 1.1\n\nDefault images are assigned to events without an image\nThe imprint now opens in a dialog instead of a separate page\nStarted with integration builds\nAdded a lot of automated tests\nMeasure the test coverage\nStatic code analysis to increase code quality\nFixed a lot of small bugs\nIncreased code quality\n\nVersion 1.0\n\nList all events on one page\nUse persistence framework (no database)\nAdd imprint page\nNew administration page\nCustom configuration file\nResize event images on upload\nCalculate background color of events based on the event image\n\nTechnology\nServer\nThe saver is base on Java EE technology and provides high-performance, RESTful web services. Everything is tied together using a Maven build.\nClient\nFor the client we decided wo go with HTML5 and Google Polymer.\nWebsite\nWe'll use GitHub Pages for hosting our upcoming project website. We only need static content, so we decided to go with Hugo, a fast and modern static website engine. Our content will be written in Markdown syntax.\nTools\nWe use a Maven build to tie everything together. As a result this project is IDE independent (every state-of-the-art IDE should be able to import and\/or use a Maven project). VersionEye is used to track out of date dependencies. To track our reaction time on issues and pull requests we use Issue Stats.\nHow to contribute to SportChef\nContributors\nA huge thank you to all the contributors! All contributors are listed below (sorted alphabetically by GitHub username):\n\nCoalaJoe\nInteractiondesigner\nItchy\njarekratajski\nMcPringle\nPReimers\n\nIf you are a contributor and you are missing on this list, please add your entry yourself and create a pull request or create an issue.\nMilestones\n\n\n\nVersion\nPlanned Release Date\nStatus\n\n\n\n\nv1.0\nEnd of January 2016\nreleased\n\n\nv1.1\nEnd of February 2016\nreleased\n\n\nv1.2\nEnd of May 2016\nwork in progress\n\n\nv1.3\n2016\/Q3\nplanning\n\n\n\nThroughput\n\nOverview of issues\nWe use Waffle.io for a really useful overview of our issues. Our issues can have one of five states. The states are:\n\n\n\nStatus\nExplanation\n\n\n\n\nBacklog\nAll open issues not ready to be worked on.\n\n\nReady\nAll open issues ready to be worked on.\n\n\nIn Progress\nAll issues, somebody is working on.\n\n\nIn Review\nImplementation is finished but should be reviewed or is waiting as a pull request.\n\n\nDone\nIssues closed in the last week.\n\n\n\nIf you would like to contribute to SportChef, please choose an issue from the Ready column and add a comment, so that we know what issue you choosed and we're able to update the status of the issue.\nIf you prefer an issue which is not yet ready and listed in the Backlog, please add a comment before starting to work on it, so we can discuss the issue beforehand and take it to the Ready state.\nIf there is an issue already In Progress and you would like to help, please add a comment to the issue to get in contact with the assigned developer.\nSource code management\nGIT Workflow Rules\nWe use the GitFlow workflow for SportChef. You can read a really good explanation of GitFlow on the Blog of Vincent Driessen: A successful Git branching model\n\nImportant: If you start to work on a new feature, please start on the develop branch!\nKeep your fork in sync\nIf you fork this repository, GitHub will not keep your fork in sync with this repository. You have to do it on your own.\n\nIf not already done, add this repository as an upstream to your repository:git remote add upstream https:\/\/github.com\/McPringle\/sportchef.git\nVerify that this repository was added successfully:git remote -v\nFetch branches and commits from this repository to your local repository:git fetch upstream\nIf you are not on your local develop branch, check it out:git checkout develop\nMerge the changes from this repositories develop branch into your repository):git merge upstream\/develop\nPush your updated repository to your GitHub fork:git push origin develop\n\nIf you want to merge changes from a different branch (e.g. master, release, etc), simply replace the branch name develop in the above command line examples with the branch name, you want to merge.\nFrequently Asked Questions\n\nWhen I try to push, I get a non-fast-forward updates were rejected error.Your local copy of a repository is out of sync with, or behind the upstream repository, you are pushing to. You must retrieve the upstream changes, before you are able to push your local changes.\n\n","103":"The Carolina Roller Derby Scoreboard is a browser-based scoreboard solution that also provides overlays for video production and the ability to track penalties.\nThe topics on the Scoreboard Wiki Main Page are the primary documentation for the scoreboard. In addition to the wiki topics, the Derby Scoreboard Facebook group is very active and currently the best way to reach other users and developers.\nA mailing list and wiki were available on SourceForge (the original location for this project) but they are not currently used. Subscribing to the SourceForge mailing list and consulting the wiki there is not recommended.\nInstalling the Scoreboard Software\nThese are instructions for getting the software installed and running on a standalone computer to provide a functioning scoreboard. If you have already done this, see Quick Start Guide to Operating the Scoreboard below.\nHardware Requirements\nMost Apple or Windows computers that have been manufactured in the last ten years should be able to handle the scoreboard well on a standalone setup. In general, a machine with at least a dual-core 64-bit processor and 2 gigabytes of RAM should be sufficient. Using the scoreboard to provide video overlays or in a networked setup that includes a scoreboard assistant or penalty tracker typically requires more computing power.\nChromebooks that have been modified to run Linux distributions have been used to host the scoreboard but hardware limitations (lack of a suitable display output or low-powered CPUs) may cause issues.\nThere are experimental versions of the scoreboard available that will run on Android devices. Contact the developers for more information.\nSoftware Requirements\nThe scoreboard should be unzipped into a folder on the local machine. The user running the software requires write access to this folder. Do not put the scoreboard in a folder that requires administrator privileges to write to unless you intend to run the software as an administrator.\nWeb Browser\nGoogle Chrome and Microsoft Edge (as well as their open-source parent Chromium are recommended for running the software. Some known issues may occur when using Mozilla Firefox or Apple Safari. Microsoft Internet Explorer is not recommended.\nJava\nJava is required for providing a Java Runtime Environment (JRE) version 1.7.0 or newer. Installing the latest version of Oracle's Java is recommended.\n\n\nWindows users can install the standard Java for Windows package that is available when clicking on Free Java Download from Oracle\u2019s Java site.\n\n\nApple users must install the complete Java Platform (JDK), which includes the JRE, to run the scoreboard properly.\n\n\nLinux users may already have a JRE from the OpenJDK project installed, if not, OpenJDK can be obtained from their repositories.\n\n\nDownloading the Scoreboard\nThe project is currently hosted on GitHub, and ZIP files can be downloaded from the GitHub Releases Page. It is recommended that you use the version labeled \"Latest release\" (green box). The \"Pre-release\" (orange box) versions are currently in development and testing, and are not recommended for sanctioned games or tournaments.\nPlease note that an older version of the project is still hosted on SourceForge and it is no longer maintained there.\nSetting up the Scoreboard\nOnce Chrome and Java are installed, use your file manager to navigate to the scoreboard folder and run the scoreboard background script by double-clicking on it.\n\n\nWindows users: Run scoreboard-Windows.exe to start the script.\n\n\nApple users: Run scoreboard.sh to start the script. (If clicking doesn't work, try pressing command+i (or right click on the file and select \"Get info\"). In the new info dialog in section \"open with\" select Terminal.app. (If it's not listed, choose other and navigate to \/Applications\/Utilities\/Terminal.app.)\n\n\nLinux users: Run scoreboard.sh to start the script. If you are unable to start it, you may have to allow script files to be executable as programs.\n\n\nOnce it starts successfully, the scoreboard script will open a new window and display a series of status messages. You must keep this script running in order for the scoreboard to function, so do not close the window. You may minimize the window without effect.\nIn your file manager, open start.html with the recommended browser. You may need to right-click on the file and choose the Open With option. The browser will open to localhost:8000 where several options are presented.\nAssuming that your scoreboard computer is set up with a monitor\/laptop screen as a primary display for the operator, and a separate projector as a second display, right-click on the second link for Main Scoreboard and choose Open link in new window. Drag the new window with the main scoreboard onto the second display, click inside the window, and press the F11 key to make the window full screen. In the first browser window that you opened on the primary display, click on one of the documentation links. It will open in a new tab. Back in the original tab click on Main Operator Control Panel.\nWhen the control panel displays, it will ask you for an operator name. Enter your name and click Login. This operator name is used to store your personalized settings such as key controls.\nNow you can go to the tab with the documentation and either go to the Quick Start Guide or dive in deep right away and proceed with the section on the Team\/Time page.\n","104":"StravaZpot \nA fluent API to integrate with Strava on Android apps.\nUsage\nThis document explains how to use StravaZpot in your Android app. For additional questions, you may want to check Strava official documentation here.\nAuthentication\nLogin button\nStravaZpot includes a custom view to include a login button according to Strava guidelines. To do that, you can add the following code to your XML layout:\n<com.sweetzpot.stravazpot.authenticaton.ui.StravaLoginButton\n    xmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n    android:id=\"@+id\/login_button\"\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:layout_gravity=\"center\"\n    app:type=\"orange\"\/>\nThe custom attribute type accepts two different values: orange and light (default). StravaLoginButton makes use of vector drawables in the support library. Thus, if you are targeting version prior to Android API 21, you would need to add the following code to your Activity:\nstatic {\n    AppCompatDelegate.setCompatVectorFromResourcesEnabled(true);\n}\nLogin Activity\nStrava uses OAuth 2.0 to authenticate users, and then request them to grant permission to your app. To get a Client ID and Secret from Strava for your app, follow this link.\nOnce you have your app credentials, you can get an Intent to launch the login activity for your app. You can easily do it with:\nIntent intent = StravaLogin.withContext(this)\n                  .withClientID(<YOUR_CLIENT_ID>)\n                  .withRedirectURI(<YOUR_REDIRECT_URL>)\n                  .withApprovalPrompt(ApprovalPrompt.AUTO)\n                  .withAccessScope(AccessScope.VIEW_PRIVATE_WRITE)\n                  .makeIntent();\nstartActivityForResult(intent, RQ_LOGIN);\nYou need to notice several things with this call:\n\n<YOUR_CLIENT_ID> must be replaced with the Client ID provided by Strava when you registered your application.\n<YOUR_REDIRECT_URL> must be in the domain you specified when you registered your app in Strava.\nRefer to ApprovalPrompt enum to get more options for this parameter.\nRefer to AccessScope enum to get more options for this parameter.\nYou need to launch the intent with startActivityForResult since the login activity will return a value that you will need later to obtain a token. If login to Strava was successful and the user granted permission to your app, you will receive a code that you can retrieve with:\n\n@Override\nprotected void onActivityResult(int requestCode, int resultCode, Intent data) {\n    super.onActivityResult(requestCode, resultCode, data);\n\n    if(requestCode == RQ_LOGIN && resultCode == RESULT_OK && data != null) {\n        String code = data.getStringExtra(StravaLoginActivity.RESULT_CODE);\n        \/\/ Use code to obtain token\n    }\n}\nFinally, you need to add the login activity to your manifest:\n<activity\n    android:name=\"com.sweetzpot.stravazpot.authenticaton.ui.StravaLoginActivity\"\n    android:label=\"@string\/login_strava\" \/>\nObtain a Token\nEvery Strava API call needs a token to prove the user is authenticated and the app has permission to access the API. After you have obtained the code from user login, you need to exchange it with Strava to get a token. You can do it with the following code:\nAuthenticationConfig config = AuthenticationConfig.create()\n                                                  .debug()\n                                                  .build();\nAuthenticationAPI api = new AuthenticationAPI(config);\nLoginResult result = api.getTokenForApp(AppCredentials.with(CLIENT_ID, CLIENT_SECRET))\n                        .withCode(CODE)\n                        .execute();\n\nNotice that in this call you must provide the Client ID and Secret provided by Strava when you registered your application, and the code obtained during the login process. Also, the execution of the previous code involves a network request; you are responsible for calling this code in a suitable thread, outside the UI thread. Otherwise, you will get an exception.\nIf the previous request is successful, you will get a LoginResult, which has a Token that you can use in your subsequent API calls, and an Athlete instance, representing the authenticated user.\nDeauthorize\nAuthenticationAPI api = new AuthenticationAPI(config);\nauthenticationAPI.deauthorize()\n                 .execute();\nAthlete API\nStravaConfig\nBefore introducing the Athlete API, we have to talk about StravaConfig. StravaConfig is a class required by all the APIs in StravaZpot to configure the way it is going to interact with Strava. You can create a single instance of StravaConfig as soon as you obtain a token, and reuse it during your app lifecycle. To create an instance of StravaConfig:\nStravaConfig config = StravaConfig.withToken(TOKEN)\n                                  .debug()\n                                  .build();\nYou must provide the token obtained during the authentication process. The call to debug() method will show in the Android Monitor what is going on when you do the network requests.\nOnce you have the configuration object, you can proceed to use all the APIs.\nCreate the Athlete API object\nAthleteAPI athleteAPI = new AthleteAPI(config);\nRetrieve current athlete\nAthlete athlete = athleteAPI.retrieveCurrentAthlete()\n                            .execute();\nRetrieve another athlete\nAthlete athlete = athleteAPI.retrieveAthlete(ATHLETE_ID)\n                            .execute();\nUpdate an athlete\nAthlete athlete = athleteAPI.updateAthlete()\n                            .newCity(CITY)\n                            .newState(STATE)\n                            .newCountry(COUNTRY)\n                            .newSex(Gender.FEMALE)\n                            .newWeight(WEIGHT)\n                            .execute();\nRetrieve athlete's zones\nZones zones = athleteAPI.getAthleteZones()\n                        .execute();\nRetrieve athlete's totals and stats\nStats stats = athleteAPI.getAthleteTotalsAndStats(ATHLETE_ID)\n                        .execute();\nList athlete K\/QOMs\/CRs\nList<SegmentEffort> koms = athleteAPI.listAthleteKOMS(ATHLETE_ID)\n                                     .inPage(PAGE)\n                                     .perPage(ITEMS_PER_PAGE)\n                                     .execute();\nFriend API\nCreate the Friend API object\nFriendAPI friendAPI = new FriendAPI(config);\nList user's friends\nList<Athlete> friends = friendAPI.getMyFriends()\n                                 .inPage(PAGE)\n                                 .perPage(ITEMS_PER_PAGE)\n                                 .execute();\nList another athlete's friends\nList<Athlete> friends = friendAPI.getAthleteFriends(ATHLETE_ID)\n                                 .inPage(PAGE)\n                                 .perPage(ITEMS_PER_PAGE)\n                                 .execute();\nList user's followers\nList<Athlete> followers = friendAPI.getMyFollowers()\n                                   .inPage(PAGE)\n                                   .perPage(ITEMS_PER_PAGE)\n                                   .execute();\nList another athlete's followers\nList<Athlete> followers = friendAPI.getAthleteFollowers(123456)\n                                   .inPage(2)\n                                   .perPage(10)\n                                   .execute();\nList common following athletes between two users\nList<Athlete> followers = friendAPI.getBothFollowing(ATHLETE_ID)\n                                   .inPage(PAGE)\n                                   .perPage(PER_PAGE)\n                                   .execute();\nActivity API\nCreate the Activity API object\nActivityAPI activityAPI = new ActivityAPI(config);\nCreate an activity\nActivity activity = activityAPI.createActivity(ACTIVITY_NAME)\n                               .ofType(ActivityType.RUN)\n                               .startingOn(START_DATE)\n                               .withElapsedTime(Time.seconds(SECONDS))\n                               .withDescription(ACTIVITY_DESCRIPTION)\n                               .withDistance(Distance.meters(METERS))\n                               .isPrivate(false)\n                               .withTrainer(true)\n                               .withCommute(false)\n                               .execute();\nRetrieve an activity\nActivity activity = activityAPI.getActivity(ACTIVITY_ID)\n                               .includeAllEfforts(true)\n                               .execute();\nUpdate an activity\nActivity activity = activityAPI.updateActivity(ACTIVITY_ID)\n                               .changeName(ACTIVITY_NAME)\n                               .changeType(ActivityType.RIDE)\n                               .changePrivate(true)\n                               .changeCommute(true)\n                               .changeTrainer(true)\n                               .changeGearID(GEAR_ID)\n                               .changeDescription(ACTIVITY_DESCRIPTION)\n                               .execute();\nDelete an activity\nactivityAPI.deleteActivity(321934)\n           .execute();\nList user's activities\nList<Activity> activities = activityAPI.listMyActivities()\n                                       .before(Time.seconds(BEFORE_SECONDS))\n                                       .after(Time.seconds(AFTER_SECONDS))\n                                       .inPage(PAGE)\n                                       .perPage(ITEMS_PER_PAGE)\n                                       .execute();\nList user's friends' activities\nList<Activity> activities = activityAPI.listFriendActivities()\n                                       .before(Time.seconds(BEFORE_SECONDS))\n                                       .inPage(PAGE)\n                                       .perPage(ITEMS_PER_PAGE)\n                                       .execute();\nList related activities\nList<Activity> activities = activityAPI.listRelatedActivities(ACTIVITY_ID)\n                                       .inPage(PAGE)\n                                       .perPage(ITEMS_PER_PAGE)\n                                       .execute();\nList activity zones\nList<ActivityZone> activityZones = activityAPI.listActivityZones(ACTIVITY_ID)\n                                              .execute();\nList activity laps\nList<ActivityLap> laps = activityAPI.listActivityLaps(ACTIVITY_ID)\n                                    .execute();\nComment API\nCreate the Comment API object\nCommentAPI commentAPI = new CommentAPI(config);\nList activity comments\nList<Comment> comments = commentAPI.listActivityComments(ACTIVITY_ID)\n                                   .inPage(PAGE)\n                                   .perPage(ITEMS_PER_PAGE)\n                                   .execute();\nKudos API\nCreate the Kudos API object\nKudosAPI kudosAPI = new KudosAPI(config);\nList activity kudoers\nList<Athlete> athletes = kudosAPI.listActivityKudoers(ACTIVITY_ID)\n                                 .inPage(PAGE)\n                                 .perPage(ITEMS_PER_PAGE)\n                                 .execute();\nPhoto API\nCreate the Photo API object\nPhotoAPI photoAPI = new PhotoAPI(config);\nList activity photos\nList<Photo> photos = photoAPI.listAcivityPhotos(ACTIVITY_ID)\n                             .execute();\nClub API\nCreate the Club API object\nClubAPI clubAPI = new ClubAPI(config);\nRetrieve a club\nClub club = clubAPI.getClub(CLUB_ID)\n                   .execute();\nList club announcements\nList<Announcement> announcements = clubAPI.listClubAnnouncements(CLUB_ID)\n                                          .execute();\nList club group events\nList<Event> events = clubAPI.listClubGroupEvents(CLUB_ID)\n                            .execute();\nList user's clubs\nList<Club> clubs = clubAPI.listMyClubs()\n                          .execute();\nList club members\nList<Athlete> athletes = clubAPI.listClubMembers(CLUB_ID)\n                                .inPage(PAGE)\n                                .perPage(ITEMS_PER_PAGE)\n                                .execute();\nList club admins\nList<Athlete> athletes = clubAPI.listClubAdmins(CLUB_ID)\n                                .inPage(PAGE)\n                                .perPage(ITEMS_PER_PAGE)\n                                .execute();\nList club activities\nList<Activity> activities = clubAPI.listClubActivities(CLUB_ID)\n                                   .before(BEFORE)\n                                   .inPage(PAGE)\n                                   .perPage(PER_PAGE)\n                                   .execute();\nJoin a club\nJoinResult joinResult = clubAPI.joinClub(123456)\n                               .execute();\nLeave a club\nLeaveResult leaveResult = clubAPI.leaveClub(123456)\n                                 .execute();\nGear API\nCreate the Gear API object\nGearAPI gearAPI = new GearAPI(config);\nRetrieve gear\nGear gear = gearAPI.getGear(GEAR_ID)\n                   .execute();\nRoute API\nCreate the Route API\nRouteAPI routeAPI = new RouteAPI(config);\nRetrieve a route\nRoute route = routeAPI.getRoute(ROUTE_ID)\n                      .execute();\nList athlete's routes\nList<Route> routes = routeAPI.listRoutes(ATHLETE_ID)\n                             .execute();\nSegment API\nCreate the Segment API object\nSegmentAPI segmentAPI = new SegmentAPI(config);\nRetrieve a segment\nSegment segment = segmentAPI.getSegment(SEGMENT_ID)\n                            .execute();\nList user's starred segments\nList<Segment> segments = segmentAPI.listMyStarredSegments()\n                                   .inPage(PAGE)\n                                   .perPage(ITEMS_PER_PAGE)\n                                   .execute();\nList another athlete's starred segments\nList<Segment> segments = segmentAPI.listStarredSegmentsByAthlete(ATHLETE_ID)\n                                   .inPage(PAGE)\n                                   .perPage(PER_PAGE)\n                                   .execute();\nStar a segment\nSegment segment = segmentAPI.starSegment(SEGMENT_ID)\n                            .execute();\nUnstar a segment\nSegment segment = segmentAPI.unstarSegment(SEGMENT_ID)\n                            .execute();\nList segment efforts\nList<SegmentEffort> efforts = segmentAPI.listSegmentEfforts(SEGMENT_ID)\n                                        .forAthlete(ATHLETE_ID)\n                                        .startingOn(START_DATE)\n                                        .endingOn(END_DATE)\n                                        .inPage(PAGE)\n                                        .perPage(ITEMS_PER_PAGE)\n                                        .execute();\nRetrieve segment leaderboard\nLeaderboard leaderboard = segmentAPI.getLeaderboardForSegment(SEGMENT_ID)\n                                    .withGender(Gender.FEMALE)\n                                    .inAgeGroup(AgeGroup.AGE_25_34)\n                                    .inWeightClass(WeightClass.KG_75_84)\n                                    .following(true)\n                                    .inClub(CLUB_ID)\n                                    .inDateRange(DateRange.THIS_WEEK)\n                                    .withContextEntries(CONTEXT_ENTRIES)\n                                    .inPage(PAGE)\n                                    .perPage(ITEMS_PER_PAGE)\n                                    .execute();\nExplore segments\nList<Segment> segments = segmentAPI.exploreSegmentsInRegion(Bounds.with(Coordinates.at(SW_LAT, SW_LONG), Coordinates.at(NE_LAT, NE_LONG)))\n                                   .forActivityType(ExploreType.RUNNING)\n                                   .withMinimumClimbCategory(MIN_CLIMB_CATEGORY)\n                                   .withMaximumClimbCategory(MAX_CLIMB_CATEGORY)\n                                   .execute();\nSegment Effort API\nCreate the Segment Effort API object\nSegmentEffortAPI segmentEffortAPI = new SegmentEffortAPI(config);\nRetrieve a segment effort\nSegmentEffort segmentEffort = segmentEffortAPI.getSegmentEffort(SEGMENT_EFFORT_ID)\n                                              .execute();\nStream API\nCreate the Stream API object\nStreamAPI streamAPI = new StreamAPI(config);\nRetrieve activity streams\nList<Stream> streams = streamAPI.getActivityStreams(ACTIVITY_ID)\n                                .forTypes(StreamType.LATLNG, StreamType.DISTANCE)\n                                .withResolution(Resolution.LOW)\n                                .withSeriesType(SeriesType.DISTANCE)\n                                .execute();\nRetrieve segment effort streams\nList<Stream> streams = streamAPI.getSegmentEffortStreams(SEGMENT_EFFORT_ID)\n                                .forTypes(StreamType.LATLNG, StreamType.DISTANCE)\n                                .withResolution(Resolution.LOW)\n                                .withSeriesType(SeriesType.DISTANCE)\n                                .execute();\nRetrieve segment streams\nList<Stream> streams = streamAPI.getSegmentStreams(SEGMENT_ID)\n                                .forTypes(StreamType.LATLNG, StreamType.DISTANCE)\n                                .withResolution(Resoulution.LOW)\n                                .withSeriesType(SeriesType.DISTANCE)\n                                .execute();\nRetrieve route streams\nList<Stream> streams = streamAPI.getRouteStreams(ROUTE_ID)\n                                .execute();\nUpload API\nCreate the Upload API object\nUploadAPI uploadAPI = new UploadAPI(config);\nUpload a file\nStrava allows you to upload files with formats GPX, FIT or TCX. We recommend to use TCXZpot in order to generate TCX files that can be uploaded to Strava.\nUploadStatus uploadStatus = uploadAPI.uploadFile(new File(<path_to_file>))\n                                     .withDataType(DataType.FIT)\n                                     .withActivityType(UploadActivityType.RIDE)\n                                     .withName(\"A complete ride around the city\")\n                                     .withDescription(\"No description\")\n                                     .isPrivate(false)\n                                     .hasTrainer(false)\n                                     .isCommute(false)\n                                     .withExternalID(\"test.fit\")\n                                     .execute();\nCheck upload status\nUploadStatus uploadStatus = uploadAPI.checkUploadStatus(16486788)\n                                     .execute();\nThreading\nAll the APIs in StravaZpot perform network requests in a synchronous manner and without switching to a new thread. Therefore, it is up to the user of the library to invoke the API in a suitable thread, and outside the Android UI thread in order to avoid NetworkOnMainThreadException.\nExceptions\nStravaZpot methods do not have any checked exceptions, but users of the library should be prepared for them to happen. In particular, the following scenarios can arise:\n\nStrava may return a 401 Unauthorized response code. In that case, the network request will throw a StravaUnauthorizedException. It is up to the user of the library to reuthenticate with Strava to get a new token and retry the request.\nIf any other network error happen, or the request is not successful, it will throw a StravaAPIException.\n\nDownload\nYou can get StravaZpot from JCenter using Gradle. Just add this line to your build file:\ncompile 'com.sweetzpot.stravazpot:lib:1.3.1'\nLicense\nCopyright 2018 SweetZpot AS\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","105":"PerfectBody\nThat is the Android app, which is useful for people who do or want to begin to do sports. You can find a chronometer, BMI calculator, the descriptions of the exercises and the diets, personal profile and much more in this app...\n","106":"PerfectBody\nThat is the Android app, which is useful for people who do or want to begin to do sports. You can find a chronometer, BMI calculator, the descriptions of the exercises and the diets, personal profile and much more in this app...\n","107":"Daily-Quotes\nAndroid app to get the latest quotes: Inspirational, Motivational, Love, Funny, Inspirational, Sports ... Quotes to bring positive into your life.\n\nPreview\n\n\n\nFeatures\n\nNew Quotes everyday\nSharing on Social Media (Facebook, Twitter, Instragram, Google+, Whatsapp)\nNotification\nDifferent quotes categories\n\nHelp\nIf you run into issues, please don't hesitate to find help on the GitHub project.\nLicense\nThe Daily Quotes project is covered by the MIT License.\nThe MIT License (MIT)\nCopyright (c) 2016 Mohamed Labouardy and contributors to the Daily Quotes project.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","108":"Sports Leagues App\nThis sports app will show latest sports news, history and records of the world\u2019s major sports leagues.\nOptions to filter out by sports also give the latest articles.\nThe article list are fully clickable and will bring you to the web page showing the full article.\n\nFeatures:\nNotifications\nSocial Share Articles\nSave an article to read later with no network connection\n\n\n\n\n","109":"Socrata Datasync\nLast updated: June 2, 2017\nLooking for the latest release? Get it here: https:\/\/github.com\/socrata\/datasync\/releases\nGeneral Information\nDataSync is an executable Java application which serves as a general solution to automate publishing data on the\nSocrata platform. It can be used through a easy-to-use graphical interface or as a command-line tool ('headless mode').\nWhether you are a non-technical user, developer, or ETL specialist DataSync makes data publishing simple and reliable.\nDataSync takes a CSV or TSV file on a local machine or networked hard drive and publishes it to a Socrata dataset so\nthat the Socrata dataset stays up-to-date. DataSync can also publish geospatial files such as zipped shapefiles,\ngeoJSON, KML, and KMZ files. DataSync jobs can be integrated into an ETL process, scheduled using a tool\nsuch as the Windows Task Scheduler or Cron, or used to perform updates or create new datasets in batches. DataSync\nworks on any platform that runs Java version 1.8 or higher (i.e. Windows, Mac, and Linux). This simple, yet powerful\npublishing tool lets you easily update Socrata datasets programmatically and automatically (scheduled), without\nwriting a single line of code.\nComprehensive DataSync Documentation\nThe Socrata University Class: Socrata Introduction to Integration\nStandard Jobs\nStandard jobs can be set up to take a CSV data file from a local machine or networked folder and publish it to a specific dataset. A job can be automated easily using the Windows Task Scheduler or similar tool to run the job at specified intervals (i.e. once per day).\n\nGIS Jobs\nGIS jobs can be set up to handle geospatial datasets, such as zipped shapefiles, geoJSON, KML, or KMZ files and replace specific datasets on Socrata. The job can then be automated in a similar fashion to standard jobs.\nPort Jobs\nPort jobs are used for moving data around that is already on the Socrata platform. Users that have publisher rights can make copies of datasets through this tool. Port jobs allow the copying of both dataset schemas (metadata and columns) and data (rows).\n\nDevelopers\nThis repository is our development basecamp. If you find a bug or have questions, comments, or suggestions, you can contribute to our issue tracker.\nApache Maven\nDataSync uses Maven for building and package management. For more information: What is Maven?\nTo build the project, first you'll need to create an application token on your profile page.  Put the random string it produces in a file called \"api-key.txt\" in the root directory of this project, then run\nmvn clean install\n\nTo compile the project into an executable JAR file (including all dependencies) run:\nmvn clean compile -Dmaven.test.skip=true assembly:single\n\nThis puts the JAR file into the \"target\" directory inside the repo.  So to open DataSync, simply:\ncd target\njava -jar DataSync-1.8.2-jar-with-dependencies.jar\n\nJava SDK\nDataSync can be used as a Java SDK, for detailed documentation refer\nto:\nhttp:\/\/socrata.github.io\/datasync\/guides\/datasync-library-sdk.html\n","110":"data_engineering\n\u0110\u00e2y l\u00e0 d\u1ef1 \u00e1n th\u1ef1c h\u00e0nh c\u00e1c v\u1ea5n \u0111\u1ec1 th\u1ef1c t\u1ebf c\u1ee7a BeeCost.Com khi l\u00e0m Data Engineering tr\u00ean d\u1eef li\u1ec7u l\u1edbn c\u1ee7a c\u00e1c trang web E-Commerce.\nB\u1ea1n c\u00f3 th\u1ec3 tho\u1ea3i m\u00e1i s\u1eed d\u1ee5ng hay chia s\u1ebb t\u00e0i nguy\u00ean t\u1ea1i project n\u00e0y, tr\u1eeb c\u00e1c h\u00ecnh \u1ea3nh thu\u1ed9c v\u1ec1 th\u01b0\u01a1ng hi\u1ec7u c\u1ee7a BeeCost.\nBeeCost Team c\u0169ng r\u1ea5t mong c\u00e1c nh\u1eadn x\u00e9t v\u00e0 \u0111\u00f3ng g\u00f3p t\u1eeb c\u00e1c b\u1ea1n. \u0110\u00e2y l\u00e0 c\u00e1c th\u00f4ng tin b\u1ea1n s\u1ebd c\u1ea7n t\u00ecm hi\u1ec3u th\u00eam:\n1. C\u00e1c ph\u1ea7n m\u1ec1m c\u1ea7n thi\u1ebft\n2. C\u00e1ch Download project v\u1ec1 m\u00e1y\n3. C\u00e1ch Build code\n4. Wiki v\u00e0 c\u00e1c b\u00e0i th\u1ef1c h\u00e0nh\nProject code n\u00e0y l\u00e0 m\u1ed9t c\u00e2u chuy\u1ec7n d\u00e0i 05 n\u0103m t\u1eeb khi ch\u00fang m\u00ecnh kh\u1edfi nghi\u1ec7p. BeeCost l\u00e0 m\u1ed9t team \u0111am m\u00ea khoa h\u1ecdc d\u1eef li\u1ec7u (Data Science).\nTh\u00e0nh vi\u00ean team m\u00ecnh t\u1eebng th\u1eafng Nh\u00e2n T\u00e0i \u0110\u1ea5t Vi\u1ec7t 2017 cho s\u1ea3n ph\u1ea9m Ch\u1ed1ng \u0111\u1ea1o v\u0103n s\u1eed d\u1ee5ng AI \u0111ang \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng t\u1ea1i c\u00e1c tr\u01b0\u1eddng \u0111\u1ea1i h\u1ecdc. M\u1ed9t th\u00e0nh vi\u00ean kh\u00e1c l\u00e0 Data Scientist 12 n\u0103m kinh nghi\u1ec7m, l\u00e0 l\u1edbp k\u0129 s\u01b0 n\u00f2ng c\u1ed1t \u0111\u1ea7u ti\u00ean c\u1ee7a C\u1ed1c C\u1ed1c Search, t\u1eebng b\u00e1n \u0111\u01b0\u1ee3c 2 c\u00f4ng ty d\u1eef li\u1ec7u v\u00e0 AI, t\u00e1c gi\u1ea3 c\u1ee7a ChatBot ng\u00e2n h\u00e0ng MBBank v\u00e0 v\u00ed \u0111i\u1ec7n t\u1eed BUNO c\u1ee7a BIDV Bank.\nTr\u01b0\u1edbc khi \u0111\u1ebfn v\u1edbi BeeCost, ch\u00fang m\u00ecnh l\u00e0 \u0111\u1ed3ng s\u00e1ng l\u1eadp m\u1ea1ng x\u00e3 h\u1ed9i Ph\u1eadt t\u1eed butta.vn c\u00f9ng Gi\u00e1o H\u1ed9i Ph\u1eadt Gi\u00e1o Vi\u1ec7t Nam, ra m\u1eaft t\u1ea1i Vesak 2019: s\u1ef1 ki\u1ec7n c\u1ee7a Li\u00ean H\u1ee3p Qu\u1ed1c v\u1edbi h\u01a1n 20 n\u01b0\u1edbc tham gia.\nNh\u1eefng g\u00ec ch\u00fang m\u00ecnh l\u00e0m l\u00e0 d\u00f9ng c\u00f4ng ngh\u1ec7 v\u00e0 d\u1eef li\u1ec7u \u0111\u1ec3 ph\u1ee5c v\u1ee5 ng\u01b0\u1eddi d\u00f9ng internet, BeeCost.Com c\u0169ng kh\u00f4ng ph\u1ea3i l\u00e0 ngo\u1ea1i l\u1ec7, n\u00f3 l\u00e0 m\u1ed9t tr\u1ee3 l\u00fd mua s\u1eafm Online, mang tinh hoa c\u1ee7a team m\u00ecnh t\u1eeb c\u00e1c d\u1ef1 \u00e1n \u0111\u00e3 l\u00e0m.\nV\u00e0, project m\u1edf n\u00e0y l\u00e0 nh\u1eefng ph\u1ea7n c\u1ed1t l\u00f5i \u0111\u1ea7u ti\u00ean nh\u1ea5t \u0111\u00e3 x\u00e2y d\u1ef1ng n\u00ean BeeCost.\nB\u1ea1n c\u00f3 th\u1ec3 th\u1eed d\u00f9ng \u0111\u1ec3 th\u1ea5y nh\u1eefng d\u00f2ng code \u0111\u00e3 t\u1ea1o n\u00ean BeeCost.Com nh\u01b0 th\u1ebf n\u00e0o nha.\n","111":"Big Data Engineering Lab - BDElab\nIn verschiedenen \u00dcbungsaufgaben werden Konzepte aus der Vorlesung Big Data Engineering an praktischen Beispielen nachvollzogen.\nDie einzelnen Aufgaben befinden sich in Unterverzeichnissen:\n\nLab 1 (Fakten-\/Graphmodell, Hadoop HDFS, Pail)\nLab 2 (Batch Processing mit Hadoop MapReduce)\nLab 3 (Komplexe Batch Layer Pipelines mit JCascalog)\nLab 4 (Stream Processing mit Apache Kafka und Storm)\n\nAllgemeine Vorbereitung\nDie Aufgaben k\u00f6nnen auf allen Rechnern im Pool LI 137 (LKIT) bearbeitet werden. Die notwendige Umgebung variiert und wird in den Aufgaben erl\u00e4utert. Grunds\u00e4tzlich gilt:\n\nDie Basis aller Aufgaben bildet das vorliegende Git Repository auf github.\nAlle Aufgaben basieren auf Java. Hierzu wird eine Entwicklungsumgebung mit Java 1.7+ SDK (oder neuer), Eclipse JEE (oder eine andere Java IDE ihrer Wahl), Maven, und Git ben\u00f6tigt. Diese Umgebung sollte lokal und nativ auf dem Entwicklungsrechner vorliegen (auf den LKIT-Rechnern ist dies gegeben).\n\nLKIT Hadoop Cluster\nEin Hadoop Cluster besteht aus Master- und Slave-Knoten. Im LKIT gibt es einen Master und 20 Slaves. Die Adressen folgen dem folgenden Schema:\n\nDer Cluster dient aktuell als verteiltes Dateisystem (HDFS - Hadoop Distributed File System) und verteilter Ressourcen Manager (Hadoop YARN - Yet Another Resource Negotiator) f\u00fcr verteilte Anwendungen (auf Basis von Hadoop Map Reduce und Cascalog).\nDer Master Knoten dient im HDFS als sog. NameNode zur Verwaltung von Namen und Bl\u00f6cken. Zudem ist er YARN ResourceManager zur Verwaltung verteilter Rechenauftr\u00e4ge. Die Slave Knoten dienen als HDFS DataNodes und YARN NodeManager.\nNutzung des Clusters\nDie Nutzung des Clusters erfolgt \u00fcber die Kommandozeile (Shell). Hierzu dient das Skript hadoop. Im LKIT ist das Skript unter \/usr\/local\/opt\/hadoop-2.7.3\/bin\/hadoop installiert. Sie k\u00f6nnen es zur einfacheren Nutzung der $PATH Umgebungsvariable hinzuf\u00fcgen. Erg\u00e4nzen sie dazu folgende Zeile in ihrer ~\/.profile Datei:\nPATH=\"$PATH:\/usr\/local\/opt\/hadoop-2.7.3\/bin\"\n\nHDFS Bereiche im LKIT\nF\u00fcr das Labor wird je Teilnehmer ein Verzeichnis auf dem HDFS zur Verf\u00fcgung gestellt. Dieses hat folgendes Muster:\n\/user\/<IZ-Name>\/\n\nDas Verzeichnis muss f\u00fcr jeden Teilnehmer erst eingerichtet werden. Bitte wenden sie sich dazu an einen Betreuer.\nHadoop Web Anwendungen\nIn Hadoop gibt es einige Web-basierte Werkzeuge. Die folgenden Links funktionieren nur im Intranet der Hochschule (auch \u00fcber eine VPN Verbindung von au\u00dfen).\n\n\nDie NameNode Web UI zeigt den Status und Inhalt des HDFS.\n\nLKIT NameNode Web UI\n\n\n\nDie YARN Web UI dient zur Beobachtung von MapReduce und anderen YARN Anwendungen.\n\nLKIT YARN Web UI\n\n\n\nAnmeldung am Cluster von Remote per VPN und SSH\nFalls Sie die Laboraufgaben nicht im LKIT bearbeiten m\u00f6chten, k\u00f6nnen Sie dies per VPN und SSH auch von au\u00dferhalb durchf\u00fchren.\n\n\n\u00d6ffnen Sie eine Konsole und starten Sie eine SSH Verbindung zum Login Server der Hochschule.\nssh <IZ-Name>@login.hs-karlsruhe.de\n\n\n\nVom Login Server aus k\u00f6nnen Sie sich nun wiederum mittels SSH auf einem beliebigen LKIT Knoten einw\u00e4hlen.\nssh <IZ-Name>@iwi-lkit3-<Range-Number>.hs-karlsruhe.de\n\nDie Range-Number ist eine Zahl zwischen 01-20.\nSie k\u00f6nnen auch eine VPN Verbindung zum Hochschulnetz aufbauen (Server: vpn.hs-karlsruhe.de) und sich dann von au\u00dferhalb direkt auf den LKIT Rechnern per SSH anmelden, ohne den Login Server zu benutzen.\n\n\nAlternative: Einbindung eines Remote Windows 10 Rechners in den Cluster\nFolgender Gist beschreibt, wie die Hadoop Anwendungen der Labor\u00fcbungen auf einem entfernten Windows 10 Rechner gestartet werden k\u00f6nnen und von dort auf den LKIT-Cluster zugreifen:\nhttps:\/\/gist.github.com\/SheepSays\/b96397ea6d93738e47304e7a129f15a0\nDies funktioniert z.B. vom Windows Rechner zuhause mit VPN Verbindung zur Hochschule.\n","112":"Data-Structures\n\nThis contains all the programs for data structures that are a part of the syllabus of MAKAUT 2nd year Computer Science and engineering course.\n\nC\nC++\nJava\nJavascript\nPython \ud83d\udc0d\n\n\nStack\nStack is a linear data structure which follows a particular order in which the operations are performed. The order may be LIFO(Last In First Out) or FILO(First In Last Out).\nMain methods\n\nPush(new item)\n\nInserts new element at the \"top\" of the Stack.\nNeed to send the new item as argument.\n\n\nPop()\n\nTakes out the most top element from the Stack.\nNo arguments needed.\n\n\n\n\nLinked List\nA linked list is a sequence of data structures, which are connected together via links.\nLinked List is a sequence of links which contains items. Each link contains a connection to another link. Linked list is the second most-used data structure after array.\nMain methods\n\nGet\n\ngetFirst()\n\nGets the first element of the List.\n\n\ngetLast()\n\nGets the last element of the List.\n\n\ngetAt(index)\n\nGets the element at the index provided or null otherwise.\n\n\n\n\nInsert\n\ninsertFirst(new_item)\n\nInserts the new item in the first index of the List, the previous first takes second place.\n\n\ninsertLast(new_item)\n\nInserts the new item at the new last index (previous last index + 1) of the List.\n\n\ninsertAt(new_item, index)\n\nInserts the new item at the provided index if size_of_list >= index and index > 0.\n\n\n\n\nRemove\n\nremoveFirst()\n\nRemoves the item on the first index of the List, so the item on the second index takes the first place.\n\n\nremoveLast()\n\nRemoves the item in the last index of the List\n\n\nremoveAt(index)\n\nRemoves the item at the provided index if size_of_list >= index and index > 0.\n\n\n\n\n\nSubtypes\nDouble Linked List\nIt's a subtype of Linked List where each node is not only connected to the next node, but also to the previous node. Everything else is the same as a Linked List.\nCircular Linked List\nA linked list is a sequence of data structures, which are connected together via links. Linked List is a sequence of links which contains items. Each link contains a connection to another link. The main property of the Circular Linked List, which is also the main difference with the Linked List, is that the last nodes is linked to the first one.\nCircular Double Linked List\nTakes the best of the two previous subtypes, the first and last nodes are linked, and links between the nodes are in both ways.\n\nQueue\nA Queue is a linear structure which follows a particular order in which the operations are performed. The order is First In First Out (FIFO).\nMain methods\n\nEnqueue(new_item)\n\nInserting the new item at the beginning of the Queue.\n\n\nDequeue()\n\nTaking out the last item of the Queue.\n\n\n\n\nTree \ud83c\udf33\nTree is a widely used abstract data type that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.\nMain methods\n\nInsert(new_item)\n\nInserts the new item in the correct tree node\n\n\nGet\n\nPre-Order()\nIn-Order()\nPost-Order()\n\n\n\n\nContributing \nTo start contributing, check out CONTRIBUTING.md. New contributors are always welcome to support this project. Check out issues labelled as Hacktoberfest if you are up for some grabs! :)\n","113":"de-hours-with-experts\nData Engineering Hours With Experts Coding Challenge!\nProblem\nFor a given number, return the next largest number that can be created by rearranging that number's digits.\nIf no larger number can be created, return -1\nExample inputs\/outputs are shown below:\n\n\n\nInput\nOutput\n\n\n\n\n12\n21\n\n\n21\n-1\n\n\n12345678\n12345687\n\n\n34535762\n34536257\n\n\n45590051\n45590105\n\n\n987654321\n-1\n\n\n\nYou can create your solution in Java, Scala, or Python. Some files and tests have been created for you as a starting point. Instructions for importing the Java and Scala projects into Intellij are included.\nThings not to worry about\n\nValidating command line arguments - you can just assume each program takes 1 argument which is always a valid number\n\n","114":"NoSQLDataEngineering\nThis repository stores the last versions of all the projects available to infer a schema from an Aggregate-oriented NoSQL database. It uses a model-driven engineering approach based on Eclipse Modeling Framework. It also contains several utility projects used to visualize, classify and generate code from a NoSQL schema model.\nA NoSQL schema is a schema used to define which data and in which format is being stored in a NoSQL database. This schema is not explicitly defined but inferred by a data-oriented infererence process.\nTable of contents\n\nSchema models\nInference process\nNoSQLSchema metamodel\nNoSQL import\nJson to DBSchema\n\n\n\nDatabase import\nRandom data generation\n\n\n\nSchema evolution\n\n\n\nObject Document mappers\n\n\n\nNoSQL Schema visualization\nData visualization\n\n\n\nDecision tree\n\n\nSchema models\nProjects:   es.um.nosql.examples, es.um.nosql.orchestrator.test\nTODO...\nInference process\nThe inference process assumes the following initial scenario:\n\nA document-based NoSQL database is running. At the moment supported databases are MongoDB and CouchDB. If this requirement is not met or your data is in another format, maybe this will help you.\nThe database must be divided into several tables or collections or each object must contain a type attribute indicating its collection. Each object must also contain a unique _id attribute.\nThe user has installed the Eclipse Modeling Framework (EMF, http:\/\/www.eclipse.org\/modeling\/emf\/)\n\nThe following projects are needed to execute the inference process:\n\nNoSQL Schema metamodel: es.um.nosql.s13e, es.um.nosql.s13e.edit and es.um.nosql.s13e.editor.\nNoSQL import: es.um.nosql.s13e.nosqlimport.\nJson to DBSchema: es.um.nosql.s13e.json2dbschema.\n\nSome examples of the inference process may be found on the es.um.nosql.orchestrator.test project. The process is resumed as follows:\n\nFirst of all the NoSQL import project is used and a database importer is created. Then a convenient method is used to infer from the database a minimum set of objects describing the existing versions:\nSystem.out.println(\"Starting inference...\");\nMongoDBImport inferrer = new MongoDBImport();\nJsonArray jArray = inferrer.mapRed2Array(MONGODB_IP, TABLENAME, MONGODB_MAPREDUCE_FOLDER);\nSystem.out.println(\"Inference finished.\");\n\n\nThen the Json2DBSchema project is used, and the result of the last step is supplied as an input. In our case we will commonly use a JsonArray with the minimum objects from the last step as an input:\nSystem.out.println(\"Starting BuildNoSQLSchema...\");\nBuildNoSQLSchema builder = new BuildNoSQLSchema();\nbuilder.buildFromGsonArray(TABLENAME, jArray, OUTPUT_MODEL_NAME);\n\n\nThe result of the inference process will be a NoSQLSchema saved in the OUTPUT_MODEL_NAME route.\nNow this model may be visualized or used to generate some code using any other project from this repository.\n\n\nNoSQL Schema metamodel\nThe projects involved in the NoSQL Schema metamodel definition are the following ones:\n\nes.um.nosql.s13e: This projects stores the metamodel definition in an ecore file, stored in model\/nosqlschema.ecore. It also contains the generated java classes for this metamodel, as well as some utilities such as a Model loader used to load model definitions in a XMI file.\nes.um.nosql.s13e.edit and es.um.nosql.s13e.editor: These projects are used to define a generated editor in order to manipulate NoSQL schema models with the Ecore interface.\n\nThe most important elements of this projects are, apart from the metamodel definition, the Model loader, which will be used in several other projects to start some processes from a model file located somewhere in the file system. It also contains a PrettyPrinter class which can take a NoSQL schema model and print it to the standard input in a human-readable way.\nThe NoSQL schema metamodel\n\n\n\nThe NoSQLSchema metamodel has NoSQLSchema as a root element with a certain name and containing several Entities.\nAn Entity describes an object of the real world such as Movie, Prize, Director or Criticism. It contains, at the same time, several version definitions.\nAn EntityVersion is a certain version of an Entity, with an identifier, a count attribute (indicating how many objects in the bd correspond to this version) and a boolean root attribute, indicating if this version is a root element or is embedded into some other object. An EntityVersion may contain several properties.\nAn Attribute is a kind of Property with a name and an associated type. This type may be at the same time a Primitive type such as a String, Number or Boolean, or a Tuple containing several Tuples and Primitive types inside of it.\nA Reference is a kind of Property with a cardinality, a name and is associated with an Entity. This way an object inside a database may be associated with another object by pointing to its id on this field.\nAn Aggregate is a kind of Property with a cardinality, a name and potentially several associated EntityVersions. This way an object may embed several other objects inside of it.\n\n\n\nIn the image above there is an example of a NoSQLSchema model based on a 'Movies' domain. It is divided into several parts:\n\nEntity Movie: With three EntityVersions, each one of them containing several Attributes, References and Aggregates. Some of these references are associated with Director, and some aggregations embed other entity versions such as Criticism.\nEntity Director: Divided in two versions with references to Movie.\nEntity Criticism: Divided in two versions with several simple Attributes of Primitive Type.\nEntity Rating: With just one entity version and a couple of string Attributes.\nEntity Prize: With another entity version, and a couple Attributes, being one of them a Tuple.\n\nA NoSQLSchema model may be obtained by aplying the inference process described here. Once a NoSQLSchema model is obtained, it may be used for several things:\n\nIt can be visualized by using the NoSQL Schema visualization tool (link)\nIt can be used to generate a differentiation model used to classify objects and visualize them in D3.js (link)\nIt can be used to generate a Decision tree to effectively classify objects of this database. Projects related are 'Entity Differentiation'(link) and 'Decision tree'(link)\n...\n\n\nNoSQL import\nThis project is aimed to import JSON data from a NoSQL database. It is provided with a MapReduce process and applies this process to the given database. As a result of the MapReduce process it obtains a subset of representative database objects which describes all the entities and versions found on that database. It supports MongoDB and CouchDB databases, but this list will be updated on the near future with more document based databases. As an input for this project to work the user needs to provide the following inputs:\n\nA fully working MongoDB or CouchDB database with some data on it.\nA database name to which the process will infer representative JSON objects and apply the MapReduce process.\nA MapReduce folder containing a map.js file and a reduce.js file. These files contain JavaScript code, they may be found on the corresponding folder and are not likely to change.\n\n\n\nAn example of its usage is just based on the creation of a MongoDB\/CouchDBImport object and an execution of a suitable method. The process goes as follow:\n\nA MongoDB\/CouchDBImport object is created and there are a couple of map.js and reduce.js on a folder.\nThe created object will apply the MapReduce files to a given database and extract a minimum representation of the Json objects stored on the database.\nThe object now will apply an homogeneization to to each object in order to remove unnecessary attributes (such as _rev, _id, etc).\nThe object will finally apply the desired output format and return the result to the client.\n\nThe Java project involved in the NoSQL import project is the following one:\n\nes.um.nosql.s13e.nosqlimport: This project contains a definition of two database importers able to interact with MongoDB and CouchDB. It also contains some helper classes in order to return a suitable output as a Stream<JsonObject>, JsonArray or a Json file which will be used on the schema extraction process (see here). As more databases are supported new classes will be added to this project.\n\n\nJson to DBSchema\nTODO:\n\nDatabase import\nThis project is used to fill a database with raw data from a file. It may be not necessary if the input for the inference process is an already filled database, but in some cases the user will have just a JSON, XML or CSV file, and so this project might come handy to import the relevant files into a database.\nAs an example of the usage the user may check the Main class. The process goes as follow:\n\nA DbController object is created with a certain DbType as a parameter. This way the controller will create a client for the specified database.\nAn input method is selected by calling to a certain method of the DbController: model2Db, xml2Db or json2Db methods are given as examples.\n\nThe model2Db method expects a NoSQLSchema model and then it will generate random JSON information according to the given model. Then it will proceed to insert the JSON data on the database. More details may be checked on the Model2Db and JsonGenerator classes.\nThe xml2Db method makes use of the XML2Db class and expects a XML file with the following format:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<collectionName>\n    <row <parameter 1>=<value 1> <parameter 2>=<value 2> ... ... <parameter N>=<value N>\/>\n    <row <parameter 1>=<value 1> <parameter 2>=<value 2> ... ... <parameter N>=<value N>\/>\n    <row <parameter 1>=<value 1> <parameter 2>=<value 2> ... ... <parameter N>=<value N>\/>\n<\/collectionName>\n\n\nThe json2Db method makes use of the Json2Db class and expects a route to a JSON file as well as a database name. It will insert the content found into the database with an appropiate collection name.\nThe user will probably need to create its own interface class if the implemented ones are not enough for its requirements, but the implementation shouldn't be too difficult.\n\n\nNow the controller will apply the input method with the given client, filling the database.\n\n\n\nThe Java project involved in the Database import project is the following one:\n\nes.um.nosql.s13e.db: This project contains a definition of a database controller able to fill MongoDB and CouchDB databases given a certain XML, JSON or model file. It is also possible to implement classes to process data files into the database, if needed.\n\n\nRandom data generation\nThe purpose for this project is to provide the user a meaningful dataset of a certain volume to which he can execute queries and perform different data analyses, since sometimes the data available in a dataset is not sufficient to apply certain processes. Because of this, this project may be used to generate random data content using a NoSQLSchema model as an input file. A YAML configuration file should also be defined with certain parameters and conditions so the data generated meets certain criteria defined by the user.\nThe general structure of the project is as follows:\n\n\nFirst of all a NoSQLSchema model may be defined by using the Ecore editor in Eclipse, or by inferring it from a database using the inference process described here, so there is no need to start from an inferred schema, the user may define one by himself. Then a YAML configuration file should be provided. This file contains several definitions and keys detailed just below:\nThe YAML configuration file\nThe configuration file is divided into some sections each one of them accepting several parameters:\n\ninput section:\n\nmodel (REQUIRED): The input model conforming the NoSQLSchema metamodel.\ndebug: Boolean value indicating if debug messages will be printed during the execution.\nsplits (REQUIRED): The iterations in which the generation will be performed. This parameter needs to be adjusted in order to not fill all the memory available.\n\n\nentities section:\n\nversions section:\n\nminInstances and maxInstances: The range of objects generated randomly for each existing version entity.\n\n\nincludeType: Boolean value which tells if a type attribute should be inserted in each object indicating the entity it belongs.\ntimestamp: The timestamp mark from where objects are being generated. Useful for evolution studies purposes.\ndateFormat: The format in which the previous timestamp is defined, such as dd\/MM\/yyyy.\n\n\nattributes section:\n\nprimitiveTypes section:\n\nstrangeTypesProbability: The probability to generate another type instead of the expected type.\nnullProbability: The probability to generate a NULL value instead of the expected value.\nstringType: The type of strings being generated, which may be name, name_surname, random, large, word, phrase, word_number or nonsense.\nstringNamesFile: The file containing the names to be randomly selected.\nstringSurnamesFile: The file containing the surnames to be randomly selected.\nstringWordsFile: The file containing the words to be randomly selected.\nminIntegerAllowed and maxIntegerAllowed: The range of ints to be randomly generated.\nminDoubleAllowed and maxDoubleAllowed: The range of doubles to be randomly generated.\ndoubleDecimalsAllowed: The decimals allowed each time a double is generated.\n\n\ntuples section:\n\nminTupleElements and maxTupleElements: The range of elements being generated on each tuple.\nstrangeTypesProbability: The probability to generate another tuple type instead of the expected type.\nnullProbability: The probability to generate a NULL value instead of the expected tuple value.\n\n\n\n\nassociations section:\n\nreferences section:\n\nminReferenceAllowed and maxReferenceAllowed: The range of elements referred in a single reference.\nstrangeTypesProbability: The probability to generate another reference type instead of the expected type.\n\n\naggregates section:\n\nminAggregateAllowed and maxAggregateAllowed: The range of elements agregated in a single aggregation.\n\n\n\n\noutput section (AT LEAST ONE REQUIRED):\n\ndatabase: The IP in which the database is running.\ndatabaseCollection: The name of the collection being inserted in the database.\nfolder: The folder to which the output should generate files.\nconsole: Boolean value indicating if the output should be printed to the console.\n\n\n\nA lot of these options are just optional, only the input model and an available output are required to be defined. This subsection will be extended on the future when more options are defined on request. An example of this file can be found in es.um.nosql.s13e.db.gen\/config\/config.yaml.\nThe project\nThe process is defined in several steps explained below:\n\nFirst of all several splits are defined, and the same number of iterations are queued.\nFor each split the generation is divided into two steps:\n\nThe first step generates the required objects and fills them with primitive types and tuples.\nThe second step fills the references and aggregates with objects generated in that same split.\n\n\nFinally the objects generated are stored in the selected outputs.\nThe data structures are flushed and a new split may begin.\n\nThe Java project involved in the Random data generation project is the following one:\n\nes.um.nosql.s13e.db.gen: This project contains several classes to be used as POJOs for the YAML configuration file, the data generator itself and the output module in which several output modes are defined. There is also a main class and a controller used as a running example of the project.\n\n\nSchema evolution\nThis project is intended to analyze a NoSQLSchema model, its entities and its variations, in order to classify variations depending on their count as outliers or non-outliers. The count attribute stores, for each variation, how many objects of that variation exist on the database. Once variations are classified then several analysis may be performed: Outliers may be transformed to non-outliers variations by proposing migrations, and non-outliers variations will be studied in the future TODO.\nThere are several independent processes on this project. We list and explain them below:\n\nA process is presented to generate some MapReduce templates in order to launch the inference process with different options to catch the timestamp field.\nAnother process is available to generate a CSV file from an input NoSQLSchema.\nA third process allows the user to detect outliers and propose transformations from a NoSQLSchema.\nThe last process takes a NoSQLSchema model as an input, removes outliers, and then studies the remaining variations, in order to catch dependencies.\n\nAs explained, one of the generated outputs for a NoSQLSchema model when analyzing outliers is a CSV file in which for each Entity, its count number and its timestamps are stored in columns. By doing this, it is fairly easy to create a plot in Python in which variations with their lifelines may be presented, as seen in the next Figure.\n\n\nThe project also contains a utility to suggest some transformations in order to make outliers dissapear, by migrating outlier variations to non-outlier variations. The criteria for this suggestions is as follows:\n\nFor any outlier variation, search for the non-outlier variations with most properties in common.\nFrom all these non-outlier variations, select the one with less properties not in common with the outlier.\nThis way the proposed migration will try to perform as less changes as possible to each variation. An example of these suggestions may be seen on the Figure below:\n\n\n\nThe Java project involved in the Schema evolution analysis project is the following one:\n\nes.um.nosql.s13e.evolution: This project contains some MapReduce templates to be used on the MapReduce inference process in order to extract the initial and last timestamps for each variation. It also contains the executables to extract and analyze outliers from a NoSQLSchema model. Lastly it contains some Python utilities to create charts from an outlier CSV file. There are several Main classes, each one of them launches a certain process.\n\n\nObject document mappers\nTODO:\n\nNoSQL Schema visualization\nThis tool is designed to represent schemas and schema versions in a friendly way using Sirius (https:\/\/eclipse.org\/sirius\/). Sirius is a modeling tool used to generate graphical DSLs in an easy way, but also to define a suitable representation for a given model. As a result of the inference process described above (link) a NoSQLSchema model is given, and that model is the input for the visualization process.\nAt the moment there are defined the following viewpoints:\n\nA tree viewpoint in which all schema versions are listed as well as the entity versions (ordered alphabetically). This tree representation is divided into three branches, each one of them showing a certain aspect: A Schemabranch in which schema versions are shown ordered by the Entity to which its root EntityVersion belongs, an Inverted Index in which all the EntityVersions are listed as well as the schema versions in which they are involved, and finally a Dictionary branch in which each Entity is displayed with its EntityVersions and Properties.\n\n\n\n\nA global schema diagram representation to visualize the Entities, EntityVersions and Properties. Each one of the elements shows a defined style. This diagram shows, in a friendly way, the same information as the last branch of the tree viewpoint.\n\n\n\n\n\nTwo diagram representations associated with each schema version.\n\n\nA diagram representation associated to each Entity, as well as each element associated, in some way, with the EntityVersions of that Entity.\n\n\n\n\nThe projects involved in this visualization tool are the following ones:\n\nes.um.nosql.s13e.design: The Sirius workbench project in which the viewpoints are defined.\nes.um.nosql.s13e.visualization.sdk: The feature project in which the Sirius visualization plugin is stored.\n\nHowever: Please keep in mind this section was developed in 2016 for a M.Sc.Thesis (found here) and since then several changes have been performed on the inference process. Because of that we have been wanting to redefine all the viewpoints available, since we now think the available viewpoints are not fully efficient on showing the information we want. For example, the viewpoints do not satisfy us when we use as an input really big models (hundreds of EntityVersions). That's why we will be redefining the implemented viewpoints and you should expect drastic changes on the viewpoints available soon(tm).\n\nData visualization\n\n\n\nhttps:\/\/www.youtube.com\/watch?v=933V5AiwAgM\nThis visualization tool is designed to draw different graphics representing the data classification of a NoSQL database using D3.js (https:\/\/d3js.org\/). D3.js is a JavaScript library for manipulating documents based on data. As a result of the inference process described above (link) a NoSQL_Schema model is given, and HTML5\/CSS3\/JavaScript code is generated as an output of the visualization process.\nOnce the code is generated the user just needs to provide a JSON file with the database objects, and make sure that these objects have a defined id field of some kind as well as a \"type\" attribute indicating its type (the entity it belongs to).\n\n\nThe NoSQL_Schema model will then be transformed by a m2m transformation into a Version_Diff model which defines, for each version entity (and each JSON object) a collection of HasField and HasNotField defining how the version is composed, by which properties and with which types. Now a m2t transformation can be executed which in the end will generate JavaScript.\n\n\nOnce the m2t transformation is finished a HTML5\/JS project is created with an index.html. Giving a proper JSON object collection file it is possible to classify this collection into its entities and versions and represent the results with D3.js graphics.\n\n\n\n\nThe project involved in this visualization tool is the following one:\n\nes.um.nosql.s13e.datavisualization: Project with the Version_Diff metamodel definition. It also contains all the m2m and m2t transformations, and references the NoSQLSchema project.\n\nAlso in the NoSQLDataIndex project several examples can be found. Each example is contained in a folder with the JavaScript code already generated. To visualize each example, just open the index.html file in a web browser.\nPlease keep in mind this section was developed in 2016 for a M.Sc.Thesis (found here), and so it will become obsolete and need some refinements at some point.\n\nDecision tree\n\n","115":"DataStructures_Algorithms\nRuntime Analysis\nBig O Notation\n\nBig O Notation is used to describe the upper bound of a particular algorithm. Big O is used to describe worst case scenarios\n\n\nLittle O Notation\n\nLittle O Notation is also used to describe an upper bound of a particular algorithm; however, Little O provides a bound that is not asymptotically tight\n\nBig \u03a9 Omega Notation\n\nBig Omega Notation is used to provide an asymptotic lower bound on a particular algorithm\n\n\nLittle \u03c9 Omega Notation\n\nLittle Omega Notation is used to provide a lower bound on a particular algorithm that is not asymptotically tight\n\nTheta \u0398 Notation\n\nTheta Notation is used to provide a bound on a particular algorithm such that it can be \"sandwiched\" between two constants (one for an upper limit and one for a lower limit) for sufficiently large values\n\n\nData Structures\nLinked List\n\nA Linked List is a linear collection of data elements, called nodes, each pointing to the next node by means of a pointer. It is a data structure consisting of a group of nodes which together represent a sequence.\nSingly-linked list: linked list in which each node points to the next node and the last node points to null\nDoubly-linked list: linked list in which each node has two pointers, p and n, such that p points to the previous node and n points to the next node; the last node's n pointer points to null\nCircular-linked list: linked list in which each node points to the next node and the last node points back to the first node\nTime Complexity:\n\nAccess: O(n)\nSearch: O(n)\nInsert: O(1)\nRemove: O(1)\n\n\n\nStack\n\nA Stack is a collection of elements, with two principle operations: push, which adds to the collection, and pop, which removes the most recently added element\nLast in, first out data structure (LIFO): the most recently added object is the first to be removed\nTime Complexity:\n\nAccess: O(n)\nSearch: O(n)\nInsert: O(1)\nRemove: O(1)\n\n\n\nQueue\n\nA Queue is a collection of elements, supporting two principle operations: enqueue, which inserts an element into the queue, and dequeue, which removes an element from the queue\nFirst in, first out data structure (FIFO): the oldest added object is the first to be removed\nTime Complexity:\n\nAccess: O(n)\nSearch: O(n)\nInsert: O(1)\nRemove: O(1)\n\n\n\n","116":"A Hanborq optimized Hadoop Distribution, especially with high performance of MapReduce. It's the core part of HDH (Hanborq Distribution with Hadoop for Big Data Engineering).\nHere is our presentation: Hanborq Optimizations on Hadoop MapReduce\nHDH (Hanborq Distribution with Hadoop)\nHanborq, a start-up team focuses on Cloud & BigData products and businesses, delivers a series of software products for Big Data Engineering, including a optimized Hadoop Distribution.\nHDH delivers a series of improvements on Hadoop Core, and Hadoop-based tools and applications for putting Hadoop to work solving Big Data problems in production. HDH is ideal for enterprises seeking an integrated, fast, simple, and robust Hadoop Distribution. In particular, if you think your MapReduce jobs are slow and low performing, the HDH may be you choice.\nHanborq optimized Hadoop\nIt is a open source distribution, to make Hadoop Fast, Simple and Robust.\n- Fast: High performance, fast MapReduce job execution, low latency.\n- Simple: Easy to use and develop BigData applications on Hadoop.\n- Robust: Make hadoop more stable.\nMapReduce Benchmarks\nThe Testbed: 5 node cluster (4 slaves), 8 map slots and 2 reduce slots per node.\n1. MapReduce Runtime Environment Improvement\nIn order to reduce job latency, HDH implements Distributed Worker Pool like Google Tenzing. HDH MapReduce framework does not spawn new JVM processes for each job\/task, but instead keep the slot processes running constantly.\nAdditionally, there are many other improvements at this aspect.\nbin\/hadoop jar hadoop-examples-0.20.2-hdh3u3.jar sleep -m 32 -r 4 -mt 1 -rt 1\nbin\/hadoop jar hadoop-examples-0.20.2-hdh3u3.jar sleep -m 96 -r 4 -mt 1 -rt 1\n\n2. MapReduce Processing Engine Improvement\nMany improvements are applied on Hadoop MapReduce Processing engine, such as shuffle, sort-avoidance, etc.\n\nPlease refer to the page MapReduce Benchmarks for detail.\nFeatures\nMapReduce\n- Fast job launching: such as the time of job lunching drop from 20 seconds to 1 second.\n- Low latency: not only job setup, job cleanup, but also data shuffle, etc.\n- High performance shuffle: low overhead of CPU, network, memory, disk, etc.\n- Sort avoidance: some case of jobs need not sorting, which result in too many unnecessary system overhead and long latency.\n... and more and continuous ...\nHow to build?\n$ cd cloudera\/maven-packaging  \n$ mvn -Dnot.cdh.release.build=true -Dmaven.test.skip=true -DskipTests=true clean package  \n\nThen use this package: build\/hadoop-{main-version}-hdh{hdh-version}, for example: build\/hadoop-0.20.2-hdh3u2\nCompatibility\nThe API, configuration, scripts are all back-compatible with Apache Hadoop and Cloudera Hadoop(CDH). The user and developer need not to study new, except new features.\nInnovations and Inspirations\nThe open source community and our real enterprise businesses are the strong source of our continuous innovations.\nGoogle, the great father of MapReduce, GFS, etc., always outputs papers and experiences that bring us inspirations, such as:\nMapReduce: Simplified Data Processing on Large Clusters\nMapReduce: A Flexible Data Processing Tool\nTenzing: A SQL Implementation On The MapReduce Framework\nDremel: Interactive Analysis of Web-Scale Datasets\n... and more and more ...\nOpen Source License\nAll Hanborq offered code is licensed under the Apache License, Version 2.0. And others follow the original license announcement.\n","117":"csc-bdse\n\u0411\u0430\u0437\u043e\u0432\u044b\u0439 \u043f\u0440\u043e\u0435\u043a\u0442 \u0434\u043b\u044f \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0438 \u043f\u043e \u043a\u0443\u0440\u0441\u0443 \"\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u043d\u0430\u044f \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u0438\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\".\n\u0417\u0430\u0434\u0430\u043d\u0438\u044f\n\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430\n\u0427\u0430\u0441\u0442\u043e \u0437\u0430\u0434\u0430\u0432\u0430\u0435\u043c\u044b\u0435 \u0432\u043e\u043f\u0440\u043e\u0441\u044b\n\u0417\u0430\u0434\u0430\u043d\u0438\u0435 1\n\u0417\u0430\u0434\u0430\u043d\u0438\u0435 2\n\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3\n\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4\n","118":"SnowGraph is depressed now and has been migrated to linzeqipku\/intellide-graph\nSoftware Knowledge Graph\n\nSnowGraph is a software data analytics platform.\nFeatures\n\n\nData format support\n\nSource code, version control, issue tracking, mailing lists, documentation, online discussions.\n\n\n\nTrace recovery\n\nEntities extracted from different data sources are linked automatically through multiple traceability recovery techniques.\n\n\n\nData analytics\n\nSoftware data are stored in a neo4j graph database.\nWe can use Neo4j's graph query language, Cypher, to query software data.\nDifferent data analytics methods can be implemented as a reusable component using a uniform interface.\n\n\n\nIn-dev Applications\n\nNatural language interface for querying the graph database\nAPI-aware text understanding\nQAbot based on text analysis\nQAbot based on program analysis\n\n\n\nDevelopment Environment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Graph\nExample input data:\n\nLucene-data.zip\n\nedit snowgraph-builder.yml, like:\ngraphPath: E:\/SnowGraphData\/lucene\/graphdb-tmp\n\nextractors:\n    - graphdb.extractors.parsers.javacode.JavaCodeExtractor E:\/SnowGraphData\/lucene\/sourcecode\n    - graphdb.extractors.parsers.git.GitExtractor E:\/SnowGraphData\/lucene\/git\n    - graphdb.extractors.parsers.stackoverflow.StackOverflowExtractor E:\/SnowGraphData\/lucene\/stackoverflow\n    - graphdb.extractors.parsers.jira.JiraExtractor E:\/SnowGraphData\/lucene\/jira\n    - graphdb.extractors.parsers.mail.MailListExtractor E:\/SnowGraphData\/lucene\/mbox\n    - graphdb.extractors.miners.text.TextExtractor\n    - graphdb.extractors.miners.codeembedding.line.LINEExtractor\n    - graphdb.extractors.linkers.apimention.ApiMentionExtractor\n    - graphdb.extractors.linkers.ref.ReferenceExtractor\n\nRun graphdb.framework.SnowGraphBuilder (VM arguments: -Xms2000m -Xmx2000m).\nThis process may take a long time.\nWith the above property file, the graph database is generated in E:\/SnowGraphData\/lucene\/graphdb-tmp.\n","119":"Artificial Intelligence for Humans  - Code Examples\nThese examples are part of a series of books that is currently under development.  Check the above website to see which volumes have been completed and are available.  The planned list is shown here.\nThe following volumes are planned for this series:\n\nVolume 0: Introduction to the Math of AI\nVolume 1: Fundamental Algorithms\nVolume 2: Nature Inspired Algorithms\nVolume 3: Neural Networks and Deep Learning\n\nQuestions?\nIf you have a question, or wish to discuss something related to these books, you can find my Google Group here:\nhttps:\/\/groups.google.com\/forum\/#!forum\/jeffheatons-ai-group\nStaying Up to Date\nThis appendix describes how to obtain the \u201cArtificial Intelligence for Humans\u201d (AIFH) book series examples.\nThis is probably the most dynamic area of the book.  Computer languages are always changing and adding new versions.  I will update the examples as this becomes necessary.  There are also bugs and corrections.  You are encouraged to always make sure you are using the latest version of the book examples.\nBecause this area is so dynamic, this file may have become out of date. You can always find the latest version of this file at the following location.\nhttps:\/\/github.com\/jeffheaton\/aifh\nThis book\u2019s examples are provided in a number of computer programming languages.  Core example packs are provided for Java, C#, C\/C++, Python and R for most volumes.  The community may have added other languages as well.  All examples can be found at the GitHub repository.\nDownload ZIP File\nGithub provides an icon that allows you to simply download a ZIP file that contains all of the example code for the series.  A single ZIP file is used to contain all of the examples for the series.  Because of this, the contents of this ZIP are frequently updated.  If you are starting a new volume, it is very important that you make sure you have the latest copy.  The download can be performed from the following URL.\nhttps:\/\/github.com\/jeffheaton\/aifh\nYou can see the download link in Figure 1.\nFigure 1: GitHub\n\nClone the Git Repository\nAll examples can be obtained using the source control program git, if it is installed on your system. The following command clones the examples to your computer.  Cloning simply refers to the process of copying the example files.\ngit clone https:\/\/github.com\/jeffheaton\/aifh.git\n\nYou can also pull the latest updates using the following command.\ngit pull\n\nIf you would like an introduction to git refer to the following URL.\nhttp:\/\/git-scm.com\/docs\/gittutorial\nExample Contents\nThe entire \u201cArtificial Intelligence for Humans\u201d series is contained in one download.  This download is a zip file.\nOnce you open the examples file you will see the contents see in Figure 2.\nFigure 2: Examples Download\n\nThe license file describes the license used for the book examples. All of the examples for this series are released under the Apache 2 License license.  This is a Free and open-source software (FOSS) license.  This means that I do retain a copyright to the files.  However, you can freely reuse these files in both commercial and non-commercial projects without further permission.\nWhile the book source code is provided free, the book text is not provided free.  These books are commercial products that I sell through a variety of means.  You may not redistribute the actual books.  This includes the PDF, MOBI, EPUB and any other format the book might be converted to. I do, however, provide all books in DRM-free form. Your support of this policy is greatly appreciated and does contribute to the future growth of these books.\nThere are also two README files included in the download.  The README.md is a \u201cmarkdown\u201d file that contains images and formatting.  The README.txt file is plane text.  Both files contain the same information. For more information on MD files, refer to the following URL.\nhttps:\/\/help.github.com\/articles\/github-flavored-markdown\nYou will find README files at several levels of the examples download. The README file contained in the examples root (seen above) contains information about the book series.\nYou will also notice the individual volume folders contained in the download.  These are named vol1, vol2, etc.  You may not see all of the volumes in the download.  Not all of the volumes have been written yet!  All of the volumes have the same format.  For example, if you were to open Volume 1, you would see the contents listed in Figure 3.\nFigure 3: Inside Volume 1\n\nAgain, you see the two README files.  These files contain information unique to this particular volume.  The most important information contained in the volume level README files is the current status of the examples.  The community often contributes example packs.  This means that some of the example packs may not be complete.  The README for the volume will let you know this important information. The volume README.also contains the errata and FAQ for a volume.\nYou should also see a file named \u201cchart.R\u201d.  This file contains the source code that I used to create many of the charts in the book.  I use the R programming language to produce nearly all graphs and charts seen in the book.  This allows you to see the equations behind the pictures.  I do not translate this file to other programming languages.  R is simply what I use in the production of the book.  If I used another language, like Python, to produce some of the charts, you would see a \u201cchart.py\u201d along with the R code.\nYou can see that the above volume contains examples for C, C#, Java, Python and R. These are the core languages that I try to always ensure complete examples for.  However, you may see other languages added.  Again, always check the README file for the latest information on language translations.\nFigure 4 shows the contents of a typical language pack.\nFigure 4: The Java Language Pack\n\nNotice the README files again?  The README files inside of a language folder are VERY important.  Inside the above two README files you will find information about using the examples with Java.  If you are having trouble using the book\u2019s examples with a particular language, the README file should be your first stop.  The other files seen above are all unique to Java.  The README file above describes these in much greater detail.\nContributing to the Project\nDo you want to translate the examples to a new language? Have you found something broken, misspelled, or otherwise botched? You probably have. Fork the project and push a commit revision to GitHub. You will be credited among the growing number of contributors.\nThe process begins with a fork.  You create an account on GitHub and fork the AIFH project.  This creates a new project, with a copy of the AIFH files.  You will then clone your new for, in a similar way as was described for cloning the main AIFH repositiory.  Once you make your changes you submit a \u201cpull request\u201d.  Once I get this request I will evaluate your changes\/additions and merge it with the main project.\nA much more detailed article on contributing through GitHub can be found here.\nhttps:\/\/help.github.com\/articles\/fork-a-repo\nGetting Help\n\nQ&A Forum on Google Groups\n\nCitations\nIf you would like to cite Volume 1: please use:\nBibtex:\n@book{Heaton13a,\n\tAuthor = {Jeff Heaton},\n\tTitle = {Artificial Intelligence for Humans, Volume 1: Fundamental Algorithms},\n\tPublisher = {CreateSpace Independent Publishing Platform},\n\tYear = {2013},\n\tISBN = {1493682229}\n}\n\nEndnote:\n%0 Book\n%T Artificial Intelligence for Humans, Volume 1: Fundamental Algorithms\n%A Heaton, J.\n%N v. 1\n%@ 9781493682225\n%D 2013\n%I CreateSpace Independent Publishing Platform\n\nIf you would like to cite Volume 2: please use:\nBibtex:\n@book{Heaton14a,\n\tAuthor = {Jeff Heaton},\n\tTitle = {Artificial Intelligence for Humans, Volume 2: Nature Inspired Algorithms},\n\tPublisher = {CreateSpace Independent Publishing Platform},\n\tYear = {2014},\n\tISBN = {1499720572}\n}\n\nEndnote:\n%0 Book\n%T Artificial Intelligence for Humans, Volume 2: Nature Inspired Algorithms\n%A Heaton, J.\n%N v. 1\n%@ 1499720572\n%D 2014\n%I CreateSpace Independent Publishing Platform\n\n","120":"SUSI.AI Server\n\n   \n \n  \nSUSI.AI is an intelligent Open Source personal assistant. It is capable of chat and voice interaction by using APIs to perform actions such as music playback, making to-do lists, setting alarms, streaming podcasts, playing audiobooks, and providing weather, traffic, and other real-time information. Additional functionalities can be added as console services using external APIs. SUSI.AI is able to answer questions and depending on the context will ask for additional information in order to perform the desired outcome. The core of the assistant is the SUSI.AI server that holds the \"intelligence\" and \"personality\" of SUSI.AI. The Android and web applications make use of the APIs to access information from a hosted server.\nDeployments\nDevelopment: An automatic deployment from the development branch at GitHub is available for tests at https:\/\/susi-server.herokuapp.com\nMaster: The master branch is planned to be deployed on https:\/\/api.susi.ai. Currently, the deployment is taking place each hour at xx.45 using the development branch. We are planning to switch to the Master branch for production soon.\nDeployment branches\nThere are two branches targetting deployment, \"dev-dist\" and \"stable-dist\".\nThe former is intended to be used in conjunction with the development versions\nof \"susi_linux\", while the later with the stable (from the \"master\" branch).\nThese two branches are currently updated manually from \"susi_server_binary_latest.tar.gz\"\n(see below), but future integration will update \"dev-dist\" regularly.\n\nCommunication\nPlease join our mailing list to discuss questions regarding the project: https:\/\/groups.google.com\/forum\/#!forum\/opntec-dev\/\nOur chat channel is on gitter here: https:\/\/gitter.im\/fossasia\/susi_server\n\nHow do I install Susi: Download, Build, Run\n\nNote\n\nYou must be logged in to Docker Cloud for the button to work correctly. If you are not logged in, you'll see a 404 error instead.\n\n\n\n \n \n\nAt this time, SUSI.AI is not provided in the compiled form, you easily build it yourself. It's not difficult and done in one minute! The source code is hosted at https:\/\/github.com\/fossasia\/susi_server, you can download it and run SUSI.AI with (Before installation you must have \"Java Development Kit\" latest version at http:\/\/openjdk.java.net\/install\/ & \"Gradle\" latest version at https:\/\/gradle.org\/install\/):\n\nNote\n\nFor Armv6 processors (e.g. Raspberry Pi Zero \/ Zero W\/ Zero WH\/ 1A \/ 1B), please make sure that your system is using Java 8 (Oracle or OpenJDK) as there are some compatibility issues for Armv6 processors.\nYou may use the following command to install OpenJDK's Java 8 JRE and JDK:\n\n\n> sudo apt install openjdk-8-jdk-headless\n\n> git clone https:\/\/github.com\/fossasia\/susi_server.git\n> cd susi_server\n> .\/gradlew build\n> bin\/start.sh\n\nFor Windows Users (who are using GitBash\/Cygwin or any terminal):\n> git clone https:\/\/github.com\/fossasia\/susi_server.git\n> cd susi_server\n> git checkout master\n> ant jar\n> java -jar dist\/susiserver.jar\n> git checkout development\n> ant jar\n> java -jar dist\/susiserver.jar\n\nTo stop:\n> Press Ctrl+C\n\nAfter all server processes are running, SUSI.AI tries to open a browser page itself. If that does not happen, just open http:\/\/localhost:4000; if you made the installation on a headless or remote server, then replace 'localhost' with your server name.\nTo stop SUSI.AI, run: (this will block until the server has actually terminated)\n> bin\/stop.sh\n\nA self-upgrading process is available which must be triggered by a shell command. Just run:\n> bin\/upgrade.sh\n\n\nWhere can I download ready-built releases of SUSI.AI?\nThe latest binary built can be downloaded from\nhttp:\/\/download.susi.ai\/susi_server\/susi_server_binary_latest.tar.gz\nTo run susi, do:\ntar xfz susi_server_binary_latest.tar.gz\ncd susi_server_binary_latest\njava -server -Xmx200m -jar build\/libs\/susi_server-all.jar\n\nHow do I install SUSI.AI with Docker on Google Cloud?\nTo install SUSI.AI with Docker on Google Cloud please refer to the Susi Docker installation readme.\n\nHow do I install SUSI.AI with Docker on AWS?\nTo install SUSI.AI with Docker on AWS please refer to the Susi Docker installation readme.\n\nHow do I install SUSI.AI with Docker on Bluemix?\nTo install SUSI.AI with Docker on Bluemix please refer to the Susi Docker installation readme.\n\nHow do I install SUSI.AI with Docker on Microsoft Azure?\nTo install SUSI.AI with Docker on Azure please refer to the Susi Docker installation readme.\n\nHow do I install SUSI.AI with Docker on Digital Ocean?\nTo install SUSI.AI with Docker on Digital Ocean please refer to the Susi Docker installation readme.\n\nHow do I deploy SUSI.AI with Heroku?\nYou can easily deploy to Heroku by clicking the Deploy to Heroku button above. To install SUSI.AI using Heroku Toolbelt, please refer to the Susi Heroku installation readme.\n\nHow do I deploy SUSI.AI with cloud9?\nTo install SUSI.AI with cloud9 please refer to the Susi cloud9 installation readme.\n\nHow do I setup SUSI.AI on Eclipse?\nTo install SUSI.AI on Eclipse, please refer to the Susi Eclipse\nreadme.\n\nHow to setup auto deployment to a VPS using travis\nTo auto deploy SUSI.AI to a VPS using travis, please refer to the readme file.\n\nHow do I run SUSI.AI?\n\nbuild Susi (you need to do this only once, see above)\nrun bin\/start.sh\nopen http:\/\/localhost:4000 in your browser\nto shut down Susi, run bin\/stop.sh\n\n\nHow do I configure SUSI.AI?\nThe basis configuration file is in conf\/config.properties. To customize these settings place a file customized_config.properties to the path data\/settings\/\n\nHow to compile using Gradle?\n\nTo install Gradle on Ubuntu:\n$ sudo add-apt-repository ppa:cwchien\/gradle\n$ sudo apt-get update\n$ sudo apt-get install gradle\n\n\nTo install Gradle on Mac OS X with homebrew\nbrew install gradle\n\n\nTo compile, first, create dir necessary for Gradle\n.\/gradle_init.sh\n\nCompile the source to classes and a jar file\ngradle assemble\n\nThe compiled file can be found in build dir Last, clean up so that we can\nstill build the project using Ant\n.\/gradle_clean.sh\n\n\n\n\nHow do I develop Skills (AI Conversation Rules) for SUSI.AI?\nThe SUSI.AI skill language is described in the Skill Development\nTutorial.\n\nHow to utilize Susi skill data in SUSI.AI server?\nIf you simply want to add your skill to the SUSI.AI online service, please go to https:\/\/skills.susi.ai and add your skill.\nFor your own deployments: The Susi skill data is the storage place for the Susi skills. To make Susi server utilize these skills, clone Susi skill data alongside Susi server.\ngit clone https:\/\/github.com\/fossasia\/susi_skill_data.git\n\nIf you want to create private skills in your local server, you should create a local git repository susi_private_skill_data alongside Susi server. Then you must create a local git host:\n> cd <above susi home>\n> mkdir susi_private_skill_data_host\n> cd susi_private_skill_data_host\n> git init \u2014bare\n> cd ..\/susi_private_skill_data\n> git remote add origin <path to susi_private_skill_data_host>\n> git push --set-upstream origin master\n\n\nWhy should I use SUSI.AI?\nIf you like to create your own AI, then you may consider SUSI.AI.\n\nWhere can I find API documentation?\nThe Documentation can be found here.\n\nWhere do I find the javadocs?\nYou can build them via 'ant\njavadoc'\n\nWhere can I report bugs and make feature requests?\nThis project is considered a community work. The development team consists of you too. We are very thankful for the pull request. So if you discovered that something can be enhanced, please do it yourself and make a pull request. If you find a bug, please try to fix it. If you report a bug to us, We will possibly consider it but at the very end of a giant, always growing heap of work. The best chance for you to get things done is to try it yourself. Our issue tracker is\nhere.\n\nWhat is the Development Workflow?\n\nFixing issues\n\nStep 1: Pick an issue to fix\nAfter selecting the issue\n\nComment on the issue saying you are working on the issue.\nWe expect you to discuss the approach either by commenting or on Gitter Chat.\nUpdates or progress on the issue would be nice.\n\n\nStep 2: Branch policy\nStart off from your development branch and make sure it is up-to-date with the latest version of the committer repo's development branch. Make sure you are working in development branch only. git pull upstream development\nIf you have not added upstream follow the steps given\nhere.\n\nStep 3: Coding Policy\n\nPlease help us follow the best practice to make it easy for the\nreviewer as well as the contributor. We want to focus on the code\nquality more than on managing pull request ethics.\nSingle commit per pull request\nFor writing commit messages please adhere to the Commit style guidelines.\nFollow uniform design practices. The design language must be consistent throughout the app.\nThe pull request will not get merged until and unless the commits are squashed. In case there are multiple commits on the PR, the commit author needs to squash them and not the maintainers cherry-picking and merging squashes.\nIf you don't know what does squashing of commits is read from\nhere.\nIf the PR is related to any front end change, please attach relevant screenshots in the pull request description\n\n\nStep 4: Submitting a PR\nOnce a PR is opened, try and complete it within 2 weeks, or at least stay actively working on it. Inactivity for a long period may necessitate a closure of the PR. As mentioned earlier updates would be nice.\n\nStep 5: Code Review\nYour code will be reviewed, in this sequence, by:\n\nTravis CI: by building and running tests. If there are failed tests, the build will be marked as a failure. You can consult the CI log to find which tests. Ensure that all tests pass before triggering\nanother build.\nThe CI log will also contain the command that will enable running the failed tests locally.\nReviewer: A core team member will be assigned to the PR as its reviewer, who will approve your PR or he will suggest changes.\n\n\nWhat is the software license?\nLGPL 2.1\n","121":"AIMA3e-Java (JDK 8+)  \nJava implementation of algorithms from Russell and Norvig's Artificial Intelligence - A Modern Approach 3rd Edition. You can use this in conjunction with a course on AI, or for study on your own. We're looking for solid contributors to help.\nGetting Started Links\n\n\nOverview of Project\n\n\nInterested in Contributing\n\n\nSetting up your own workspace\n\n\nComments on architecture and design\n\n\nDemo Applications that can be run from your browser (unfortunately not up to date)\n\n\nJavadoc for the aima-core project (outdated)\n\n\nDownload the latest official (but outdated) version  = 1.9.1 (Dec 18 2016)\n\n\nLatest Maven Information (for integration as a third party library)\n<dependency>\n    <groupId>com.googlecode.aima-java<\/groupId>\n    <artifactId>aima-core<\/artifactId>\n    <version>3.0.0<\/version>\n<\/dependency>\n\n\n\nIndex of Implemented Algorithms\n\n\n\nFigure\nPage\nName (in 3rd edition)\nCode\n\n\n\n\n2\n34\nEnvironment\nEnvironment\n\n\n2.1\n35\nAgent\nAgent\n\n\n2.3\n36\nTable-Driven-Vacuum-Agent\nTableDrivenVacuumAgent\n\n\n2.7\n47\nTable-Driven-Agent\nTableDrivenAgentProgram\n\n\n2.8\n48\nReflex-Vacuum-Agent\nReflexVacuumAgent\n\n\n2.10\n49\nSimple-Reflex-Agent\nSimpleReflexAgentProgram\n\n\n2.12\n51\nModel-Based-Reflex-Agent\nModelBasedReflexAgentProgram\n\n\n3\n66\nProblem\nProblem\n\n\n3.1\n67\nSimple-Problem-Solving-Agent\nSimpleProblemSolvingAgent\n\n\n3.2\n68\nRomania\nSimplifiedRoadMapOfRomania\n\n\n3.7\n77\nTree-Search\nTreeSearch\n\n\n3.7\n77\nGraph-Search\nGraphSearch\n\n\n3.10\n79\nNode\nNode\n\n\n3.11\n82\nBreadth-First-Search\nBreadthFirstSearch\n\n\n3.14\n84\nUniform-Cost-Search\nUniformCostSearch\n\n\n3\n85\nDepth-first Search\nDepthFirstSearch\n\n\n3.17\n88\nDepth-Limited-Search\nDepthLimitedSearch\n\n\n3.18\n89\nIterative-Deepening-Search\nIterativeDeepeningSearch\n\n\n3\n90\nBidirectional search\nBidirectionalSearch\n\n\n3\n92\nBest-First search\nBestFirstSearch\n\n\n3\n92\nGreedy best-First search\nGreedyBestFirstSearch\n\n\n3\n93\nA* Search\nAStarSearch\n\n\n3.26\n99\nRecursive-Best-First-Search\nRecursiveBestFirstSearch\n\n\n4.2\n122\nHill-Climbing\nHillClimbingSearch\n\n\n4.5\n126\nSimulated-Annealing\nSimulatedAnnealingSearch\n\n\n4.8\n129\nGenetic-Algorithm\nGeneticAlgorithm\n\n\n4.11\n136\nAnd-Or-Graph-Search\nAndOrSearch\n\n\n4\n147\nOnline search problem\nOnlineSearchProblem\n\n\n4.21\n150\nOnline-DFS-Agent\nOnlineDFSAgent\n\n\n4.24\n152\nLRTA*-Agent\nLRTAStarAgent\n\n\n5.3\n166\nMinimax-Decision\nMinimaxSearch\n\n\n5.7\n170\nAlpha-Beta-Search\nAlphaBetaSearch\n\n\n6\n202\nCSP\nCSP\n\n\n6.1\n204\nMap CSP\nMapCSP\n\n\n6.3\n209\nAC-3\nAC3Strategy\n\n\n6.5\n215\nBacktracking-Search\nAbstractBacktrackingSolver\n\n\n6.8\n221\nMin-Conflicts\nMinConflictsSolver\n\n\n6.11\n224\nTree-CSP-Solver\nTreeCspSolver\n\n\n7\n235\nKnowledge Base\nKnowledgeBase\n\n\n7.1\n236\nKB-Agent\nKBAgent\n\n\n7.7\n244\nPropositional-Logic-Sentence\nSentence\n\n\n7.10\n248\nTT-Entails\nTTEntails\n\n\n7\n253\nConvert-to-CNF\nConvertToCNF\n\n\n7.12\n255\nPL-Resolution\nPLResolution\n\n\n7.15\n258\nPL-FC-Entails?\nPLFCEntails\n\n\n7.17\n261\nDPLL-Satisfiable?\nDPLLSatisfiable\n\n\n7.18\n263\nWalkSAT\nWalkSAT\n\n\n7.20\n270\nHybrid-Wumpus-Agent\nHybridWumpusAgent\n\n\n7.22\n272\nSATPlan\nSATPlan\n\n\n9\n323\nSubst\nSubstVisitor\n\n\n9.1\n328\nUnify\nUnifier\n\n\n9.3\n332\nFOL-FC-Ask\nFOLFCAsk\n\n\n9.6\n338\nFOL-BC-Ask\nFOLBCAsk\n\n\n9\n345\nCNF\nCNFConverter\n\n\n9\n347\nResolution\nFOLTFMResolution\n\n\n9\n354\nDemodulation\nDemodulation\n\n\n9\n354\nParamodulation\nParamodulation\n\n\n9\n345\nSubsumption\nSubsumptionElimination\n\n\n10.9\n383\nGraphplan\nGraphPlan\n\n\n11.5\n409\nHierarchical-Search\nHierarchicalSearchAlgorithm\n\n\n11.8\n414\nAngelic-Search\n---\n\n\n13.1\n484\nDT-Agent\nDT-Agent\n\n\n13\n484\nProbability-Model\nProbabilityModel\n\n\n13\n487\nProbability-Distribution\nProbabilityDistribution\n\n\n13\n490\nFull-Joint-Distribution\nFullJointDistributionModel\n\n\n14\n510\nBayesian Network\nBayesianNetwork\n\n\n14.9\n525\nEnumeration-Ask\nEnumerationAsk\n\n\n14.11\n528\nElimination-Ask\nEliminationAsk\n\n\n14.13\n531\nPrior-Sample\nPriorSample\n\n\n14.14\n533\nRejection-Sampling\nRejectionSampling\n\n\n14.15\n534\nLikelihood-Weighting\nLikelihoodWeighting\n\n\n14.16\n537\nGIBBS-Ask\nGibbsAsk\n\n\n15.4\n576\nForward-Backward\nForwardBackward\n\n\n15\n578\nHidden Markov Model\nHiddenMarkovModel\n\n\n15.6\n580\nFixed-Lag-Smoothing\nFixedLagSmoothing\n\n\n15\n590\nDynamic Bayesian Network\nDynamicBayesianNetwork\n\n\n15.17\n598\nParticle-Filtering\nParticleFiltering\n\n\n16.9\n632\nInformation-Gathering-Agent\nInformationGatheringAgent\n\n\n17\n647\nMarkov Decision Process\nMarkovDecisionProcess\n\n\n17.4\n653\nValue-Iteration\nValueIteration\n\n\n17.7\n657\nPolicy-Iteration\nPolicyIteration\n\n\n17.9\n663\nPOMDP-Value-Iteration\nPOMDPValueIteration\n\n\n18.5\n702\nDecision-Tree-Learning\nDecisionTreeLearner\n\n\n18.8\n710\nCross-Validation-Wrapper\nCrossValidation\n\n\n18.11\n717\nDecision-List-Learning\nDecisionListLearner\n\n\n18.24\n734\nBack-Prop-Learning\nBackPropLearning\n\n\n18.34\n751\nAdaBoost\nAdaBoostLearner\n\n\n19.2\n771\nCurrent-Best-Learning\nCurrentBestLearning\n\n\n19.3\n773\nVersion-Space-Learning\nVersionSpaceLearning\n\n\n19.8\n786\nMinimal-Consistent-Det\nMinimalConsistentDet\n\n\n19.12\n793\nFOIL\nFOIL\n\n\n21.2\n834\nPassive-ADP-Agent\nPassiveADPAgent\n\n\n21.4\n837\nPassive-TD-Agent\nPassiveTDAgent\n\n\n21.8\n844\nQ-Learning-Agent\nQLearningAgent\n\n\n22.1\n871\nHITS\nHITS\n\n\n23.5\n894\nCYK-Parse\nCYK\n\n\n25.9\n982\nMonte-Carlo-Localization\nMonteCarloLocalization\n\n\n\nIndex of implemented notebooks\n\n\n\nChapter No\nName\nStatus (in 3rd edition)\nStatus (in 4th edition)\n\n\n\n\n3\nSolving Problems by Searching\nIn Progress\nNot started\n\n\n6\nConstraint Satisfaction Problems\nIn Progress\n---\n\n\n12\nKnowledge Representation\nDone\n---\n\n\n13\nQuantifying Uncertainty\nDone\n---\n\n\n14\nProbabilistic Reasoning\nIn Progress\n---\n\n\n\nBefore starting to work on a new notebook:\n\nOpen a new issue with the following heading: **Notebook: Chapter Name - Version **. Check that the issue is not assigned to anyone.\nMention a topics list of what you will be implementing in the notebook for that particular chapter. You can iteratively refine the list once you start working.\nStart a discussion on what can go in that particular notebook.\n\n\"---\" indicates algorithms yet to be implemented.\nIndex of data structures\nHere is a table of the data structures yet to be implemented.\n\n\n\nFig\nPage\nName (in book)\nCode\n\n\n\n\n9.8\n341\nAppend\n---\n\n\n10.1\n369\nAIR-CARGO-TRANSPORT-PROBLEM\n---\n\n\n10.2\n370\nSPARE-TIRE-PROBLEM\n---\n\n\n10.3\n371\nBLOCKS-WORLD\n---\n\n\n10.7\n380\nHAVE-CAKE-AND-EAT-CAKE-TOO-PROBLEM\n---\n\n\n11.1\n402\nJOB-SHOP-SCHEDULING-PROBLEM\n---\n\n\n11.4\n407\nREFINEMENT-HIGH-LEVEL-ACTIONS\n---\n\n\n23.6\n895\nSENTENCE-TREE\n---\n\n\n29.1\n1062\nPOWERS-OF-2\n---\n\n\n\n","122":"Welcome to the AIDR repository\nAIDR\u2014Artificial Intelligence for Digital Response\u2014is a free and open-source platform to filter and classify social media messages related to natural disasters and humanitarian crises. AIDR uses human and machine intelligence to collect and automatically tag up to thousands of messages per minute.\nIf you want to see AIDR in operation, go to: http:\/\/aidr.qcri.org\/\nDocumentation for Developers\nTo build AIDR using maven use:\nmvn install -P <profile>\nThe <profile> can be either dev or prod.\nWe are always looking for great developers who are passionate about humanitarian applications. Start here: Wiki Home\nCredits\n","123":"Code for the 2020 edition of \"Practical Artificial Intelligence With Java\"\nThe previous edition was released in 2013. The new 2020 edition is largely a rewrite of older material with the addition of new material. The 2020 edition was published July 28, 2020 and this repository was updated to remove all old code and add new and modified examples. See below for information on getting the old code and the PDF for the 2013 edition.\nLeanpub Link for latest edition\nThis book is a combination of\n\nnew coverage of deep learning\nnew material: creating and using knowledge graphs\nexamples from my discontinued book \"Power Java\": anomaly detection, linked data, using DBPedia, OpenNLP, and web scraping\nexamples from the original editions of this book: genetic algorithms and search algorithms\na few examples updated from my discontinued book \"Practical Semantic Web and Linked Data Applications, Java Edition\"\n\nYou can find the older code for the 2013 4th edition here: https:\/\/github.com\/mark-watson\/Java-AI-Book-Code_4th_edition\n","124":"ABAGAIL\n\nThe library contains a number of interconnected Java packages that implement machine learning and artificial intelligence algorithms. These are artificial intelligence algorithms implemented for the kind of people that like to implement algorithms themselves.\nUsage\n\nSee FAQ\nSee Wiki\n\nIssues\nSee Issues page.\nContributing\n\nFork it.\nCreate a branch (git checkout -b my_branch)\nCommit your changes (git commit -am \"Awesome feature\")\nPush to the branch (git push origin my_branch)\nOpen a Pull Request\nEnjoy a refreshing Diet Coke and wait\n\nFeatures\nHidden Markov Models\n\nBaum-Welch reestimation algorithm, scaled forward-backward algorithm, Viterbi algorithm\nSupport for Input-Output Hidden Markov Models\nWrite your own output or transition probability distribution or use the provided distributions, including neural network based conditional probability distributions\nNeural Networks\n\nFeed-forward backpropagation neural networks of arbitrary topology\n\nConfigurable error functions with sum of squares, weighted sum of squares\nMultiple activation functions with logistic sigmoid, linear, tanh, and soft max\nChoose your weight update rule with standard update rule, standard update rule with momentum, Quickprop, RPROP\nOnline and batch training\nSupport Vector Machines\n\nFast training with the sequential minimal optimization algorithm\n\nSupport for linear, polynomial, tanh, radial basis function kernels\nDecision Trees\n\nInformation gain or GINI index split criteria\n\nBinary or all attribute value splitting\nChi-square signifigance test pruning with configurable confidence levels\nBoosted decision stumps with AdaBoost\nK Nearest Neighbors\n\nFast kd-tree implementation for instance based algorithms of all kinds\n\nKNN Classifier with weighted or non-weighted classification, customizable distance function\nLinear Algebra Algorithms\n\nBasic matrix and vector math, a variety of matrix decompositions based on the standard algorithms\n\nSolve square systems, upper triangular systems, lower triangular systems, least squares\nSingular Value Decomposition, QR Decomposition, LU Decomposition, Schur Decomposition, Symmetric Eigenvalue Decomposition, Cholesky Factorization\nMake your own matrix decomposition with the easy to use Householder Reflection and Givens Rotation classes\nOptimization Algorithms\n\nRandomized hill climbing, simulated annealing, genetic algorithms, and discrete dependency tree MIMIC\n\nMake your own crossover functions, mutation functions, neighbor functions, probability distributions, or use the provided ones.\nOptimize the weights of neural networks and solve travelling salesman problems\nGraph Algorithms\n\nKruskals MST and DFS\n\nClustering Algorithms\n\nEM with gaussian mixtures, K-means\n\nData Preprocessing\n\nPCA, ICA, LDA, Randomized Projections\n\nConvert from continuous to discrete, discrete to binary\nReinforcement Learning\n\nValue and policy iteration for Markov decision processes\n","125":"[INACTIVE REPOSITORY]\nThis repository is not maintained anymore, it simply contains old AI assignments solutions.\n","126":"Artificial Intelligence State Space Search\n- Introduction\nState space search is a process used in the field of computer science, including artificial intelligence (AI), in which successive configurations or states of an instance are considered, with the goal of finding a goal state with a desired property. [1]\n- Searches Covered Here\nThe following are the searches with their frontiers as:\n\nDepth First Search (DFS) - Stack\/Recursion\nBreadth First Search (BFS) - Queue\nIterative Deepening Search (IDS) - Stack\/Recursion + Queue\nGreedy Best First Search (Greedy) - Priority Queue\nA Star Search (A*) - Priority Queue\n\n- Problem Statement\nWe are given a 2D grid (N x M dimensions) in which our goal is to reach from source to destination using the minimum (optimal) path.\nExample:\n\nFor Grid:\n0 0 0 0 0 \n1 0 1 1 0\n1 1 0 1 0\n1 1 1 0 0\n1 1 1 1 0\n\nSource: (0, 0)\nDestination: (4, 4)\nOptimal Path Cost: 5\nOptimal Path: {(0, 0) -> (1, 1) -> (2, 2) -> (3, 3) -> (4, 4)}\n\nEach Problem will run with their own time complexity but will give the same output as above for the same configuration\n\nSolution\nThe solution is for N x M dimensional grid. Algorithms optimize their route to find the shortest path as fast as possible. There are 3 input files given that can be tested, whilst other (your own or you friends) test cases can run aswell.\nTo create the testcase file, the format is as:\nN M\nsource_x source_y\ndestination_x destination_y\ngrid of NxM dimensions\n\n- Animations\n\nDFS vs BFS Animation\n\n\n\nIterative Deepening Search Animation\n\n\n\nA Animation*\n\n\n- References\n[1] Wikipedia - State Space Search\n","127":"Bot Libre\nAn open source platform for artificial intelligence, nlp, chat bots, virtual agents, social media automation, and live chat automation.\n\nhttp:\/\/www.botlibre.org\nhttps:\/\/www.botlibre.com\nhttps:\/\/www.botlibre.biz\nhttps:\/\/sourceforge.net\/projects\/botlibre\n\nComponents\n\nbotlibre-web : a web platform for developing and hosting bots for the web, mobile, and social media\nai-engine : artificial intelligence\/nlp engine, Java library\nai-engine-test : JUnit test cases, Java, test GUI\nsdk : Android, iOS, web SDK, (Java, objective C, JavaScript)\n\n","128":"ChatBot\nDescription\nAndroid chat experiment with an A.L.I.C.E. artificial intelligence.\nA.L.I.C.E. (Artificial Linguistic Internet Computer Entity) is an award-winning free natural language artificial intelligence chat robot engine.\nScreenshots\n\n\n\n","129":"\n\n\n\nOpenDSE - Open Design Space Exploration Framework\nOpenDSE is a design space exploration framework for embedded systems, written in Java.\nIt follows the Y-chart approach where an application consisting of data-dependent tasks is mapped to an architecture consisting of resources.\nThe separate modules are available at Maven: See net.sf.opendse.\nThe documentation of the framework is based on a tutorial, javadocs, and source code. Learn how to use OpenDSE from the following links:\n\nA tutorial shows the basic model, XML and optimization.\nThe Javadocs give an overview of the structure of the framework and its elements.\n\nHistory\nOpenDSE was formerly hosted at https:\/\/sourceforge.net\/projects\/opendse\/\nCredits\nBrought to you by\n\nMartin Lukasiewycz\nFelix Reimann\nFedor Smirnov\nFalko Hoefte\n\nThis project uses\n\nopt4j for metaheuristic optimization\ngoogle\/guice for dependency injection\njrtom\/jung as graph library for the model\nelharo\/xom for XML processing\njmpi for solving the realtime analysis\n\n","130":"cse-framework\n\nThe Configuration Space Exploration Framework, is a project to enable large-scale distributed experimentation.\nThe CSE is a core component of the OAQA project, its objective is to build a framework for searching\noptimal experiment configurations, given time and resource constraints.\nFor more information please refer to the OAQA tutorial.\n","131":"a space project\nWelcome to A Space Project. A project involving space...duh.\n\nEnjoy Galactic Space Exploration in a sate-of-the-art, hyper-realistic physics simulation of the entire universe!\nGet a realistic sense of the cosmic scale; there's literally dozens of planets and traveling between them could take up to minutes!\nYou want Faster Than Light Travel, you got it. (yeah it's real bro cuz like quantum anti-dark matter n' stuff yo)\nDuring your explorations you could find various astronomical bodies including:\n\nUnique planetary systems\nBinary star systems, Trinary star systems, and even quadri... quatro? quadrino-ary? as-many-as-you-want star systems!\nDiscover lonely rogue planets who lost their sun. :(\n\n\nPilot different spaceships and combat dumb AI written by an even dumber human\nThen when your bored of that you can land on a planet I guess. The worlds are flat (Ha! take that round Earthers)\n\nOk, seriously: The are two main views. Space & World.\nSpace is a finite galaxy, generated by a set of points. Each point is a collection of stars and\/or planets.\nWorlds are also finite. If you move continuously in one direction you will eventually return to where you started. Wrapped via a 4D torus.\nThis game is still in pre-alpha prototype phase. There's not much content yet but I have plans. Stay tuned.\nFeatures\n\nExplore a random Galaxy in a spaceship\nDiscover various planetary systems and cellestial bodies\nLand on planets and explore.\nFly different ships\nFight against other ship (in progress)\nDestroy asterdoids (todo)\nMining and base-building (todo)\nOpen Source bruh!\nCross-Platform Desktop and Mobile Support\n\nWindows, OSX, Linux, Android, IOS\n\n\nController support (todo)\nDeveloper Tools & Mod Support (in progress)\nMultiplayer\n\nhaha, just kidding. submit a pull request. do I have to do everything around here?\n\n\nUnit Tests\n\npfft... my code is perfect, the first time. every time.\n\n\nNo Nonsense\n\nDRM-Free! Ad-Free! Telemetry-Free! Cloud-Free! Microtransaction-Free!\n\n\nFeature Creep and Unrealistic Scope!\n\nCurrent Status\nWork in progress. Currently in early development. More of an engine than a game at this point as there is not much content, just scaffolding for what will hopefully one day be a game. The code is bit rough in some places, littered with todo's, half-baked features, and of course the occasional bug. Also, forgive my programmer graphics.\n\nActively...and painfully slowly...in progress... It doesn't even have sound and I have been poking at this for over 6 years now... Oh well. For what I am currently working on and just general notes to myself see todo.txt\nControls\n\n\n\nControl\nDesktop\nMobile (iOS, Android)\nController\/Gamepad\n\n\n\n\nMovement\nWASD\nLeft Joystick\n\n\n\nAim\nMouse\nLeft Joystick\n\n\n\nAttack \/ Shoot\nRight-Click\nbottom right button\n\n\n\nDefense \/ Shield\nAlt\ntodo\n\n\n\nDefense \/ Dodge (Barrel Roll)\nShift + A\/D\ntodo\n\n\n\nToggle HyperDrive\n1\ntodo\n\n\n\nLand\/Take Off\nT\ntop center when over planet\n\n\n\nEnter\/Exit vehicle\nG\nbottom right small button when in\/near vehicle\n\n\n\nZoom\nScroll Wheel\nPinch Zoom (not implemented yet)\n\n\n\nReset Zoom\nMiddle-Click\n\n\n\n\nToggle Map State\nM\ntop left corner button\n\n\n\nFull screen\nF11\nn\/a\n\n\n\nMenu (Pause)\nEscape\ntop right corner button\n\n\n\nVsync\nF8\n\n\n\n\nECS Debug Viewer\nF9\n\n\n\n\nMisc debug keys I am too lazy to document rn and won't be permanent anyway\n\n\n\n\n\n\nLicense\nMIT: https:\/\/github.com\/0XDE57\/SpaceProject\/blob\/master\/LICENSE.md\nLibraries\n\nlibGDX\nAshley\nOpenSimplexNoise\nVisUI\n\nBuilding\nGeneral\n\nSet up your Development Environment\nMake sure Android SDK is installed.\nImport project in IDE of choice using gradle.\nIf a \"File not found\" error occurs, check the working directory. Append \"android\\assets\" to the working directory in run configurations.\n\nAndroid Studio\n\nDesktop\n\ncreate Run Configuration\nmain class = com.spaceproject.desktop.DesktopLauncher\nuse classpath of module 'desktop'\nworking directory = ...\\SpaceProject\\android\\assets\n(must ensure working directory includes assets so data like fonts and configs can be loaded)\nbuild and run!\n\n\nAndroid\n\nenable dev options, enable usb debugging\nconnect phone, android studio should detect it\nbuild and run!\n\n\nIOS\n\nhttps:\/\/github.com\/libgdx\/libgdx\/wiki\/Ios-notes\n\n\n\n","132":"Zollern Galaxy\nAn adventurous space exploration add-on mod for Galacticraft version 4.\nFor Minecraft version 1.12.2.\n\nCopyright 2015-2025 Zollern Wolf\n\n\nZollern Galaxy\n\n\nGalacticraft Add-On Mod\nYou CAN:\n\n\nLearn from it\n\n\n\n\nUse it to get ideas and concepts\n\n\nYou CAN'T:\n\n\nRedistribute it\n\n\n\n\nClaim it as your own\n\n\n\nSteve Kung's \"More Planets\" mod was a big help.\n\"MJRLegends\" was a huge help as well.\n\nYou may use this in a modpack without contacting me. Credit would be nice but is not required.\nShould work with most space mods, just may need some IDs changed in the configs.\n\n","133":"hellobioqa\nA \"hello world\" question answering pipeline for biomedical questions based on configuration space exploration framework\n","134":"TURNUS\n \nA design space exploration and optimization framework for dynamic dataflow programs.\nDownload\nThe Eclipse plugins can be downloaded from this repository:\nhttp:\/\/scistimm.epfl.ch\/turnus\/eclipse\/\n\nPlease make sure to use the latest Eclipse Neon for RCP and RAP Developers.\nBuild from sources\nIf you want to build the latest version from the sources you should clone all the repositories and successively use maven to build all the plugins. Here an example:\nmkdir turnus\ncd turnus\ngit clone https:\/\/github.com\/turnus\/turnus.git\ngit clone https:\/\/github.com\/turnus\/turnus.orcc.git\ngit clone https:\/\/github.com\/turnus\/turnus.p2.git\ncd turnus.p2\nmvn clean install\n\n\nHow to cite\n@article{casalebrunet,\n      title = {Analysis and optimization of dynamic dataflow programs},\n      author = {Casale-Brunet, Simone},\n      institution = {IEL},\n      publisher = {EPFL},\n      address = {Lausanne},\n      year = {2015},\n}\n\n@inproceedings{turnus, \n\tauthor={S. Casale-Brunet and M. Mattavelli and J. Janneck}, \n\tbooktitle={2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)}, \n\ttitle={{TURNUS}: A design exploration framework for dataflow system design}, \n\tyear={2013},  \n\tpages={654-654},\n\tdoi={10.1109\/ISCAS.2013.6571927}, \n\tISSN={0271-4302}, \n\tmonth={May},\n}\n\n\n","135":"TURNUS\n \nA design space exploration and optimization framework for dynamic dataflow programs.\nDownload\nThe Eclipse plugins can be downloaded from this repository:\nhttp:\/\/scistimm.epfl.ch\/turnus\/eclipse\/\n\nPlease make sure to use the latest Eclipse Neon for RCP and RAP Developers.\nBuild from sources\nIf you want to build the latest version from the sources you should clone all the repositories and successively use maven to build all the plugins. Here an example:\nmkdir turnus\ncd turnus\ngit clone https:\/\/github.com\/turnus\/turnus.git\ngit clone https:\/\/github.com\/turnus\/turnus.orcc.git\ngit clone https:\/\/github.com\/turnus\/turnus.p2.git\ncd turnus.p2\nmvn clean install\n\n\nHow to cite\n@article{casalebrunet,\n      title = {Analysis and optimization of dynamic dataflow programs},\n      author = {Casale-Brunet, Simone},\n      institution = {IEL},\n      publisher = {EPFL},\n      address = {Lausanne},\n      year = {2015},\n}\n\n@inproceedings{turnus, \n\tauthor={S. Casale-Brunet and M. Mattavelli and J. Janneck}, \n\tbooktitle={2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)}, \n\ttitle={{TURNUS}: A design exploration framework for dataflow system design}, \n\tyear={2013},  \n\tpages={654-654},\n\tdoi={10.1109\/ISCAS.2013.6571927}, \n\tISSN={0271-4302}, \n\tmonth={May},\n}\n\n\n","136":"space\nSpace Exploration Mod\n","137":"space\nSpace Exploration Mod\n","138":"space\nSpace Exploration Mod\n","139":"About\nCyberBiology - life simulator on computer\nBuild\nLinux\nBuild artifact:\nmake\nYou can run after build:\njava .\/build\/world.jar\n\n","140":"Caleydo - Visualization for Molecular Biology\n\nCaleydo is a visualization framework for molecular biology data. It is targeted at analyzing multiple heterogeneous but related tabular datasets (e.g.,  mRNA expression, copy number status and clinical variables), stratifications or clusters in these datasets and their relationships to biological pathways.\nFor user documentation please refer to the Caleydo Help. For general information and downloads based on binaries please use the Caleydo Website. This guide assumes that you want to install Caleydo from source.\nInstallation\nCaleydo uses Java, OpenGL and the Eclipse Rich Client Platform (RCP). Things you need to install before being able to run Caleydo:\n\nEclipse Kepler for RCP and RAP Developers, which you can get from the eclipse download page. Other Eclipse versions won't work.\nInstall EGit in Eclipse using software updates.\nJava SDK >= 1.7\n\nTo install Caleydo use EGit within Eclipse and clone the repository. Each directory in the caleydo-dev folder corresponds to an Eclipse project. Here is a good tutorial on how to import Eclipse projects from git.\nIf you want to use ssh (instead of https) for communicating with github out of eclipse follow these instructions.\nYou will have to generate a new RSA key and save it to you ~\/.ssh folder. Remeber to set a passphrase for you key. This will result in a file ida_rsa and ida_rsa.pub turning up in your ssh folder.\nSave your public rsa key with your eclipse account folder.\nWhen cloning the repository follow the above tutorial. Don't change the username \"git\" to your username!\nTeam\nCaleydo is an academic project currently developed by members of\n\nInstitute for Computer Graphics and Vision at Graz University of Technology, Austria\nInstitute of Computer Graphics at Johannes Kepler University Linz, Austria\nPfister Lab at the School of Engineering and Applied Sciences, Harvard University, Cambridge, USA\nPark Lab at Harvard Medical School, Boston, USA\n\nAcknowledgements\nCaleydo makes use of a range of open source tools, bioinformatics resources and pre-packages several datasets, which we gratefully acknowledge here.\nSoftware Libraries\n\nBubble Sets - A Java implementation of the visualization technique.\nCDK - The Chemistry Development Kit.\nJGrahT - A graph library.\nJogl - Java bindings for OpenGL.\nPathVisio - loading and parsing WikiPathways.\nWordHoard - statistical utilities.\n\nBioinformatics Resources\n\nDavid Bioinformatics Resources - Gene ID mapping.\nKEGG - Pathways.\nWikiPathways - Pathways.\n\nDatasets\n\nCCLE - BROAD Institute Cancer Cell Line Encyclopedia.\nTCGA - The Cancer Genome Atlas.\n\nResources\n\nGitHub - Hosting our source code.\nWebStorm - Free developer licenses for our web projects from JetBrains.\n\n","141":"20n\/act: An open source platform for bioengineering\n20n\/act is the data aggregation and prediction system for bioengineering. For a target molecule, 20n\/act predicts DNA insertions into cells (usually a microbe such as E. coli or S. cerevisiae) that modify the cell. These modified cells make the target molecule by fermentation from sugar. We call these \"target molecules\/chemicals\" the bioreachables. The system predicted\/invented the first bio-route to Acetaminophen\/Tylenol\/APAP. Read more on our blog post. The technical details of the APAP work can be found in patents applications on coli and yeast fermentation.\nGetting started\nLive preview\nSee predicted DNA for 11 sample molecules at Bioreachables Preview (Login:Pass = public:preview). Due to limitations we can only make a preview version available. If you'd like the full version please contact us.\nBuilding the project\nCheckout the repo. Follow instructions to run to create the database and prediction corpus. If you'd rather get a pre-packaged DB without creating it yourself please contact us. The codebase is public to further the state-of-the-art in automating biological engineering\/synthetic biology. Some modules are specific to microbes, but most of the predictive stack deals with host-agnostic enzymatic biochemistry.\nComponents of 20n\/act\nPredictor stack\nAnswers \"what DNA do I insert if I want to make my chemical?\"\n\n\n\n\nModule\nFunction\nCode\n\n\n\n\n1\nInstaller\nIntegrates heterogeneous raw data\nCode:com.act.reachables.initdb Run:Instructions\n\n\n2\nReaction operator (RO) inference\nMines rules of enzymatic catalysis\nCode:biointerpretation module\n\n\n2\nStructure Activity Relationship (SAR) inference\nMines substrate specificities\nCode:biointerpretation module\n\n\n3\nBiointerpretation\nMechanistic validation of enzymatic transforms (using ROs)\nCode:com.act.biointerpretation.BiointerpretationDriver  Run:Instructions\n\n\n4\nReachables computation\nExhaustively enumerates all biosynthesizable chemicals\nCode:com.act.reachables.reachablesCode:com.act.reachables.postprocess_reachablesRun:Instructions\n\n\n5\nCascades computation\nExhaustively enumerates all enzymatic routes from metabolic natives to bioreachable target\nCode:com.act.reachables.cascadesRun:Instructions\n\n\n6\nDNA designer\nComputes protein & DNA design (coli specific) for each non-natural enzymatic path\nCode:org.twentyn.proteintodna.ProteinToDNADriverRun:Instructions\n\n\n7\nApplication miner\nMines chemical applications using web searches [Bing]\nCode:act.installer.bing.BingSearcherRun:Instructions\n\n\n8\nEnzymatic biochemistry NLP\nText -> Chemical tokens -> Biologically feasible reactions using ROs\nCode:act.shared.TextToRxnsFrontend:TextToRxnsUI\n\n\n9\nPatent search\nChemical -> Patents\nCode:act.installer.reachablesexplorer.PatentFinderRun:Instructions\n\n\n10\nBioreachables wiki\nAggregates reachables, cascades, use cases, protein and DNA designs into a user friendly wiki interface\nDocumentation\n\n\n\n  \nAnalytics\nAnswers \"Is my bio-engineered cell doing what I want it to?\"\n\n\n\n\nModule\nFunction\nCode\n\n\n\n\n1\nLCMS: Untargeted metabolomics\nDeep-learnt signal processing to identify all chemical [side]effects of DNA engineering on cell\nCode:DeepLearningLcmsPeakCode:com.act.lcms.UntargetedMetabolomics\n\n\n2\nLCMS: Comparative visualization\nVisualizing traces side-by-side from untargeted evaluation of over and underexpressed peaks\nDoc:LCMSDataVisualisation\n\n\n\n  \nUnit economics of bioproduction\nAnswers \"Can I use bio-production to make this chemical at scale?\"\n\n\n\n\nModule\nFunction\nCode\n\n\n\n\n1\nCost model: Manufacturing unit economics for large scale production\nIt backcalculates cell efficiency (yield, titers, productivity) objectives based on given COGS ($ per ton) of target chemical. From cell efficiency objectives it guesstimates the R&D investment (money and time) and ROI expectations\nCode:act.installer.bing.CostModelCode (viz server):costModelUISource model:XLS\n\n\n\nLicense and Contributing\nCode licensed under the GNU General Public License v3.0.\nIf an alternative license is desired, please contact 20n.\nOriginal Authors\n\nSaurabh Srivastava\nJ. Christopher Anderson\nMark T. Daly\nMichael Lampe\nThomas Legrand\nVijay Ramakrishnan\nGil Goldshlager\nNishant Kakar\n\n","142":"This is the official University of Minnesota Populus Git Repository.\nAbout\nPopulus is a package of educational software allowing students to manipulate ecological and evolutionary models, producing graphical representations of their dynamics.  It also contains an integrated help system discussing each of the models.\nPlease go to https:\/\/www.cbs.umn.edu\/populus\/ for more information about Populus.\nCopyright\nDon Alstad \nDepartment of Ecology, Evolution & Behavior \nUniversity of Minnesota \n1987 Upper Buford Circle \nSt. Paul, MN 55108-6097\nHow to run\nInstallers will be available on the main Populus page, https:\/\/www.cbs.umn.edu\/populus\/.\nTo build and run from source, use the gradle wrapper in the top directory:\n$ .\/gradlew build\n$ .\/gradlew run\nIf you are using JDK 14 or later, you can also create installer\/packager:\n$ .\/gradlew jpackage\nThe images and installers will be in the build directory.\nNote that for Windows, you'll need to run gradlew.bat instead, and need to install WiX to package.\nFeedback\nIf you find bugs, irregularities, places for improvement, or have other comments, please email populus@umn.edu.\nLanguage Support\nSpanish translations of some of the more basic models are provided. We would be interested in corresponding with people who would be able to help with other translations.  If interested, please email populus@umn.edu.\nProgramming Credits\nJava versions: Amos Anderson, Lars Roe, Sharareh Noorbaloochi \nDOS versions: Chris Bratteli\nLicense\n\n","143":"\nlibSBOLj provides the core Java interfaces and their implementation for\nthe Synthetic Biology Open Language (SBOL). The library provides an API to\nwork with SBOL objects, the functionality to read and write SBOL documents as XML\/RDF files, and a validator to check the\ncorrectness of SBOL models.\nUsing the libSBOLj library\nIn a Maven project:\nIn a Maven project that utilizes the libSBOLj library, add a dependency in the Maven project's pom.xml file.\n<dependency>\n\t<groupId>org.sbolstandard<\/groupId>\n\t<artifactId>libSBOLj<\/artifactId>\n\t<version>2.4.0<\/version>\n<\/dependency>\n\nIn a non-Maven project:\nDownload libSBOLj-<version>-withDependencies.jar from the latest github release.\nRunning in the command line:\nlibSBOLj comes with a command-line interface (CLI) that can be used to validate SBOL files. You can execute\nlibSBOLj-<version>-withDependencies.jar to validate and convert files as follows.\njava -jar libSBOLj-<version>-withDependencies.jar <inputFile> -l <language>\n\nIf validation\/conversion is successful, the program will print the contents of the input file in the specified language (SBOL1, SBOL2, GenBank, and FASTA). You can also output the result to a file.\njava -jar libSBOLj-<version>-withDependencies.jar <inputFile> -l <language> -o <outputFile>\n\nOne can also provide a URI using the -s flag for a TopLevel object, and only that object and all its dependencies will be output.\nIf validation fails with an error, there will be a message printed about the validation error.  The -f flag can be used to indicate that validation should continue after the first error, while the -d flag will provide a detailed error trace on a validation error.\nIn addition to checking all required validation rules, it will also check if the URIs are compliant and whether the SBOL document is complete (i.e., all referenced objects are contained within the file).  These validation checks can be turned off with the -n and -i flags, respectively.  It is also possible to turn-on best practices checking using the -b flag.\nWhen the input file is being converted into SBOL 2.0, the conversion should be provided a default URI prefix.  It can also be provided a default version, if desired.  Finally, the -t flag will insert the type of top level objects into the URI during conversion, if desired.\njava -jar libSBOLj-<version>-withDependencies.jar <inFile> -o <outFile> -p <URIprefix> -v <version>\n\nFinally, it can be used to compare the equality of the contents of two SBOL files using the command below:\njava -jar libSBOLj-<version>-withDependencies.jar <firstSBOLFile> -e <secondSBOLFile>\n\nUsing the latest libSBOLj SNAPSHOT\nGetting the libSBOLj source\n\nCreate a GitHub account.\nSetup Git on your machine.\nClone the libSBOLj GitHub repository to your machine.\nRetrieve the SBOLTestSuite Submodule using the instructions below.\n\nRetrieving SBOLTestSuite Submodule\ngit submodule update --init --recursive\n\nCompiling and Packaging libSBOLj\n\n\nSetup Apache Maven. A tutorial on using Apache Maven is provided here.\n\n\nIn the command line, change to the libSBOLj directory (e.g. cd \/path\/to\/libSBOLj) and execute the following command\n\n\nmvn package\n\nThis will compile the libSBOLj source files, package the compiled source into a libSBOLj JAR file (libSBOLj-<version>-SNAPSHOT-withDependencies.jar), and place the JAR file into the core2\/target sub-directory.\n","144":"JVARKIT\nJava utilities for Bioinformatics\n\n\nDocumentation\nDocumentation is available at: http:\/\/lindenb.github.io\/jvarkit\/\nCompilation\nFebruary 2019. I'm moving to java OpenJdk. See the [[NEWS]] file.\nEach tool is compiled independently of each other.\nSee the documentation for each tool at http:\/\/lindenb.github.io\/jvarkit\/. All the pages should include a paragraph titled 'Download and Compile'\nYou shouldn't try to compile all the tools because some of them are not tested, deprecated, or just too specific to my lab.\nAuthor\nPierre Lindenbaum PhD\nhttp:\/\/plindenbaum.blogspot.com\n@yokofakun\n","145":"This repo contains code samples, data and problem solutions for the\nbook \"Computational Biology with Java\".\nSee petergarst.com for more information.\n","146":"\nBioL\u00e6r er en native Android quiz-applikation med fokus p\u00e5 molekyl\u00e6rbiologi. I denne f\u00f8rste udgave af app'en er det laboratoriumanalysen ELISA, der kan quizzes i. Dog er systemet konstrueret s\u00e5ledes, at det let senere kan udvides til andre fagomr\u00e5der. Applikationen er udviklet i forbindelse med f\u00f8rste\u00e5rsprojektet p\u00e5 datamatikeruddannelsen. I projektets Wiki-sektion kan du l\u00e6se meget mere om tilblivelsen, den tilh\u00f8rende rapport, rettigheder mv.\nEnglish description\n\nThis project includes a native Android Application about laboratory analysis ELISA.\nThe idea was to make a biology application to help students understand the term ELISA.\nDeveloped in Java and XML with Android Studio in the context of a school project.\n\nAuthors\n\nSebastian Ougter Olsen\nMathias Elholm Blomgaard\nThomas Christensen\nDaniel Lyck\nMichael Trans\n\n\n","147":"BiologyAnalyze\n\u86cb\u767d\u8d28\u7f51\u7edc\u3001\u57fa\u56e0\u6d4b\u5e8f\u76f8\u5173\u6570\u636e\u5206\u6790\n","148":"\n\n\n\niBioSim is a computer-aided design (CAD) tool aimed for the modeling, analysis, and design of genetic circuits.\nWhile iBioSim primarily targets models of genetic circuits, models representing metabolic networks, cell-signaling pathways,\nand other biological and chemical systems can also be analyzed.\niBioSim also includes modeling and visualization support for multi-cellular and spatial models as well.\nIt is capable of importing and exporting models specified using the Systems Biology Markup Language (SBML).\nIt can import all levels and versions of SBML and is able to export Level 3 Version 1.\nIt supports all core SBML modeling constructs except some types of fast reactions, and also has support for the\nhierarchical model composition, layout, flux balance constraints, and arrays packages.\nIt has also been tested successfully on the stochastic benchmark suite and the curated models in the BioModels database.\niBioSim also supports the Synthetic Biology Open Language (SBOL), an emerging standard for information exchange in synthetic\nbiology.\nWebsite: iBioSim\nVideo Demo: Tools Workflow\nContact: Chris Myers (@cjmyers) myers@ece.utah.edu\nContributor(s): Nathan Barker, Pedro Fontanarrosa, Scott Glass, Kevin Jones, Hiroyuki Kuwahara, Curtis Madsen, Nam Nguyen, Tramy Nguyen, Tyler Patterson, Nicholas Roehner, Jason Stevens, Leandro Watanabe, Michael Zhang, Zhen Zhang, and Zach Zundel.\nActive Developer(s): Pedro Fontanarrosa, Chris Myers, Tramy Nguyen, Leandro Watanabe.\nRunning iBioSim\n\nDownload the iBioSim tool from the release page here:\nAfter downloading the tool, run the corresponding start-up script:\n\nWindows: iBioSim.bat\nMac OS X: iBioSim.mac64\nLinux: iBioSim.linux64\n\n\n\n[Optional] Installing iBioSim for Development\nPre-installation Requirements\n\nCreate a GitHub account.\nSetup Git on your machine.\nInstall Maven plugin on your machine.\nInstall Eclipse IDE  for Java.\nInstall libSBML for validation and flattening.\nClone the iBioSim GitHub repository to your machine\n\nImporting iBioSim to Eclipse\n\nClone the iBioSim (https:\/\/github.com\/MyersResearchGroup\/iBioSim.git) project (e.g. git clone https:\/\/github.com\/MyersResearchGroup\/iBioSim.git) to a location of your preference.\nOpen up your Eclipse workspace that you want to import your iBioSim project to.\nSelect Import from the File Menu.\nWhen given the option to select which project import, select Existing Maven Projects under Maven\n\nSet Maven Projects:\n\nRoot Directory: full path to your iBioSim project (i.e. path\/to\/iBioSim)\nOnce root directory is set, all the pom.xml should be displayed under Projects. Select all pom.xml files.\nAll installation should be complete so click Finish\n\n\n\n\n\nSetting up iBioSim Configurations in Eclipse\n\nOpen up iBioSim Run Configurations window and create a new Java Application in your Eclipse workspace\n\n\nGive the java application a name (i.e. iBioSim_GUI)\nSet the Main tab to the following information:\n\nProject: iBioSim-gui\nMain class: edu.utah.ece.async.ibiosim.gui.Gui\n\n\nSet the Environment tab to the following information:\n\nCreate variables with the corresponding value:\n\nBIOSIM: full path to your iBioSim project (i.e. path\/to\/iBioSim)\nPATH: append your copy of iBioSim bin directory to whatever existing PATH already supplied to the value of this variable (i.e. $PATH:path\/to\/iBioSim\/bin).\n\n\n\n\nSet Arguments tab to the following information:\n\nProgram arguments: -Xms2048 -Xms2048 -XX:+UseSerialGC -Djava.library.path=\/path\/to\/lib\/\nNote: for the java library path, \/path\/to\/lib\/ is the location where libSBML is installed. The libSBML is installed by default in \/usr\/local\/lib in Linux and Mac OS X machines and libSBML-5.17.0-win64 in Windows 64-bit machines.\n\n\nIf you are running on Mac OS X, also set the following:\n\nVM arguments: -Dapple.laf.useScreenMenuBar=true -Xdock:name=\"iBioSim\" -Xdock:icon=$BIOSIM\/src\/resources\/icons\/iBioSim.jpg\n\n\nAll run configurations are complete. Make sure to apply all your changes.\n\nBuilding iBioSim\n\nGo to the directory where the iBioSim is checked out and perform mvn clean install (NOTE: if you do not want to generate javadocs, use the flag -Dmaven.javadoc.skip=true).\n\n[Optional] Building reb2sac and GeneNet dependencies\n\niBioSim incorporates tools that are not Java-based, and therefore, have to be installed separately.\nThe easiest way to install reb2sac and GeneNet is to simply download the pre-compiled binaries for your operating system below:\n\nreb2sac\nGeneNet\n\n\nAnother way to install them is to compile these tools on your machine following the instructions below:\n\nreb2sac\nGeneNet\n\n\nAfter compiling or downloading reb2sac and GeneNet, copy the compiled binaries into the bin directory in the local copy of your iBioSim.\n\n","149":"\n\n\n\niBioSim is a computer-aided design (CAD) tool aimed for the modeling, analysis, and design of genetic circuits.\nWhile iBioSim primarily targets models of genetic circuits, models representing metabolic networks, cell-signaling pathways,\nand other biological and chemical systems can also be analyzed.\niBioSim also includes modeling and visualization support for multi-cellular and spatial models as well.\nIt is capable of importing and exporting models specified using the Systems Biology Markup Language (SBML).\nIt can import all levels and versions of SBML and is able to export Level 3 Version 1.\nIt supports all core SBML modeling constructs except some types of fast reactions, and also has support for the\nhierarchical model composition, layout, flux balance constraints, and arrays packages.\nIt has also been tested successfully on the stochastic benchmark suite and the curated models in the BioModels database.\niBioSim also supports the Synthetic Biology Open Language (SBOL), an emerging standard for information exchange in synthetic\nbiology.\nWebsite: iBioSim\nVideo Demo: Tools Workflow\nContact: Chris Myers (@cjmyers) myers@ece.utah.edu\nContributor(s): Nathan Barker, Pedro Fontanarrosa, Scott Glass, Kevin Jones, Hiroyuki Kuwahara, Curtis Madsen, Nam Nguyen, Tramy Nguyen, Tyler Patterson, Nicholas Roehner, Jason Stevens, Leandro Watanabe, Michael Zhang, Zhen Zhang, and Zach Zundel.\nActive Developer(s): Pedro Fontanarrosa, Chris Myers, Tramy Nguyen, Leandro Watanabe.\nRunning iBioSim\n\nDownload the iBioSim tool from the release page here:\nAfter downloading the tool, run the corresponding start-up script:\n\nWindows: iBioSim.bat\nMac OS X: iBioSim.mac64\nLinux: iBioSim.linux64\n\n\n\n[Optional] Installing iBioSim for Development\nPre-installation Requirements\n\nCreate a GitHub account.\nSetup Git on your machine.\nInstall Maven plugin on your machine.\nInstall Eclipse IDE  for Java.\nInstall libSBML for validation and flattening.\nClone the iBioSim GitHub repository to your machine\n\nImporting iBioSim to Eclipse\n\nClone the iBioSim (https:\/\/github.com\/MyersResearchGroup\/iBioSim.git) project (e.g. git clone https:\/\/github.com\/MyersResearchGroup\/iBioSim.git) to a location of your preference.\nOpen up your Eclipse workspace that you want to import your iBioSim project to.\nSelect Import from the File Menu.\nWhen given the option to select which project import, select Existing Maven Projects under Maven\n\nSet Maven Projects:\n\nRoot Directory: full path to your iBioSim project (i.e. path\/to\/iBioSim)\nOnce root directory is set, all the pom.xml should be displayed under Projects. Select all pom.xml files.\nAll installation should be complete so click Finish\n\n\n\n\n\nSetting up iBioSim Configurations in Eclipse\n\nOpen up iBioSim Run Configurations window and create a new Java Application in your Eclipse workspace\n\n\nGive the java application a name (i.e. iBioSim_GUI)\nSet the Main tab to the following information:\n\nProject: iBioSim-gui\nMain class: edu.utah.ece.async.ibiosim.gui.Gui\n\n\nSet the Environment tab to the following information:\n\nCreate variables with the corresponding value:\n\nBIOSIM: full path to your iBioSim project (i.e. path\/to\/iBioSim)\nPATH: append your copy of iBioSim bin directory to whatever existing PATH already supplied to the value of this variable (i.e. $PATH:path\/to\/iBioSim\/bin).\n\n\n\n\nSet Arguments tab to the following information:\n\nProgram arguments: -Xms2048 -Xms2048 -XX:+UseSerialGC -Djava.library.path=\/path\/to\/lib\/\nNote: for the java library path, \/path\/to\/lib\/ is the location where libSBML is installed. The libSBML is installed by default in \/usr\/local\/lib in Linux and Mac OS X machines and libSBML-5.17.0-win64 in Windows 64-bit machines.\n\n\nIf you are running on Mac OS X, also set the following:\n\nVM arguments: -Dapple.laf.useScreenMenuBar=true -Xdock:name=\"iBioSim\" -Xdock:icon=$BIOSIM\/src\/resources\/icons\/iBioSim.jpg\n\n\nAll run configurations are complete. Make sure to apply all your changes.\n\nBuilding iBioSim\n\nGo to the directory where the iBioSim is checked out and perform mvn clean install (NOTE: if you do not want to generate javadocs, use the flag -Dmaven.javadoc.skip=true).\n\n[Optional] Building reb2sac and GeneNet dependencies\n\niBioSim incorporates tools that are not Java-based, and therefore, have to be installed separately.\nThe easiest way to install reb2sac and GeneNet is to simply download the pre-compiled binaries for your operating system below:\n\nreb2sac\nGeneNet\n\n\nAnother way to install them is to compile these tools on your machine following the instructions below:\n\nreb2sac\nGeneNet\n\n\nAfter compiling or downloading reb2sac and GeneNet, copy the compiled binaries into the bin directory in the local copy of your iBioSim.\n\n","150":"Introducing Scorum\nScorum platform has three core functions:\n\nBlogging platform where authors and readers will be rewarded for creating and engaging with content\nStatistical centers where fans can browse and authors can use Microsoft\u2019s Power BI tool to integrate data-rich visuals into their content\nCommission-free betting exchange where fans can place bets against each other using Scorum Coins (SCR)\nScorum\u2019s blockchain protocol is built on the Graphene Framework and utilizes a delegated proof of stake consensus.\n\nPublic Announcement & Discussion\nThe Scorum team has been hard at work developing the blogging platform and the statistics center.\nFind out more as we take the project public through the following channels:\n\nGet the latest updates and chat with us on Telegram, Facebook, and Twitter\nRead more about our vision on Steemit\nJoin our affiliate program to get Scorum Coins for free or apply for whitelist to get coins with a discount\n\nNo Support & No Warranty\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\nIN THE SOFTWARE.\nBlockchain consensus rules\nRather than attempt to describe the rules of the blockchain, it is up to\neach individual to inspect the code to understand the consensus rules.\nQuickstart\nJust want to get up and running quickly?  Try deploying a prebuilt\ndockerized container.\nDockerized Node\nSee this guide for detailed instructions including commands for Ubuntu  16.04 (LTS).\nSeed Nodes\nA list of some seed nodes to get you started can be found in\nseed-nodes. This list is embedded into default config.ini.\nBuilding\nSee doc\/building.md for detailed build instructions, including\ncompile-time options, and specific commands for Ubuntu 16.04 (LTS).\nSystem Requirements\nFor a full node, you need 10GB of space available. Scorumd uses a memory mapped file which currently holds 2GB of data and by default is set to use up to 10GB. It's highly recommended to run scorumd on a fast disk such as an SSD or by placing the shared memory files in a ramdisk and using the shared-file-dir config (or command line) option to specify where. Any CPU with decent single core performance should be sufficient.\nMain net chain_id\ngenesis.json hash sum: db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1\nTest net chain_id\ngenesis.testnet.json hash sum: d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2\n","151":"Introducing Scorum\nScorum platform has three core functions:\n\nBlogging platform where authors and readers will be rewarded for creating and engaging with content\nStatistical centers where fans can browse and authors can use Microsoft\u2019s Power BI tool to integrate data-rich visuals into their content\nCommission-free betting exchange where fans can place bets against each other using Scorum Coins (SCR)\nScorum\u2019s blockchain protocol is built on the Graphene Framework and utilizes a delegated proof of stake consensus.\n\nPublic Announcement & Discussion\nThe Scorum team has been hard at work developing the blogging platform and the statistics center.\nFind out more as we take the project public through the following channels:\n\nGet the latest updates and chat with us on Telegram, Facebook, and Twitter\nRead more about our vision on Steemit\nJoin our affiliate program to get Scorum Coins for free or apply for whitelist to get coins with a discount\n\nNo Support & No Warranty\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\nIN THE SOFTWARE.\nBlockchain consensus rules\nRather than attempt to describe the rules of the blockchain, it is up to\neach individual to inspect the code to understand the consensus rules.\nQuickstart\nJust want to get up and running quickly?  Try deploying a prebuilt\ndockerized container.\nDockerized Node\nSee this guide for detailed instructions including commands for Ubuntu  16.04 (LTS).\nSeed Nodes\nA list of some seed nodes to get you started can be found in\nseed-nodes. This list is embedded into default config.ini.\nBuilding\nSee doc\/building.md for detailed build instructions, including\ncompile-time options, and specific commands for Ubuntu 16.04 (LTS).\nSystem Requirements\nFor a full node, you need 10GB of space available. Scorumd uses a memory mapped file which currently holds 2GB of data and by default is set to use up to 10GB. It's highly recommended to run scorumd on a fast disk such as an SSD or by placing the shared memory files in a ramdisk and using the shared-file-dir config (or command line) option to specify where. Any CPU with decent single core performance should be sufficient.\nMain net chain_id\ngenesis.json hash sum: db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1\nTest net chain_id\ngenesis.testnet.json hash sum: d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2\n","152":"\n\n\u041f\u0435\u0440\u0435\u0439\u0442\u0438 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a\nThis project is devoted to development of the electronic timing system for orienteering with inexpensive base stations and cheap tags.\nIt is also possible to use one on rogaining events, adventure races, trail running, wherever time keeping is required.\nHere are hardware and firmware parts of the timing system.\nLinks to data processing software are placed below.\nDownload latest release\nManual\nThis project is open and free. Whoever is not afraid of difficulties can try doing it oneself. Just follow the instructions from the Manual.\nThe low cost of the components can be worth your efforts (about USD $10 for one base station and $0.2 per RFID tag).\nThis development is a hobby.\nNo guarantees are given, various kinds of problems are possible during reproduction.\nSupport is also not guaranteed. So, act at your own risk.\nVersion\nThe version consists of three numbers. The first number indicates the version of the hardware.\nIf any changes are made to the circuit or to the PCB, this number is incremented by 1.\nThe second and third numbers indicates the version of the firmware.\nIf any new function is added to the firmware, the second number is incremented by 1.\nIf the firmware just fixes bugs, the third number in the version is incremented by 1.\nWhen a new version of the firmware is released with new functions, the third number is reset to 0.\nThe base station and the master station have their own versions. The release version is the largest of these two numbers.\nThe current release version is 3.7.0\nThe current base station version is 3.7.0\nThe current master station version is 1.7.0\nChangelog\nBuild the firmware of the base station with #define HW_VERS 1 to install the firmware vX.6.X or greater on the PCB v1 or v2.\nReporting Issues and Asking for Help\nIssues and suggested improvements can be posted on Issues page.\nPlease make sure you provide all relevant information about your problem or idea.\nWe also have our Telegram chat where you can ask any questions about the system.\nContributing\nYou can contribute by writing code.\nWe welcome software for working with the system on a PC via USB and on Android via Bluetooth or NFC.\nThe data transfer protocol and commands are described in the Manual.\nWith pleasure we will add a link to your developments working with Sportiduino.\nPull requests, creation of forks, developing any new ideas are welcome.\nYou can also help by improving documentation and its translation.\nParts of the system\nCards\nThe system uses cards NTAG213\/215\/216.\nAs stickers on Chinese web marketplaces they cost about $0.1, 0.2 and 0.4, respectively.\nAs key fobs the cost is doubled.\nMemory of these cards can keep 32, 120 and 216 marks, respectively.\nAlso it is possible to use Mifare Classic 1K and 4K cards.\nCards 1K are also cheap and come bundled with the RC522 module.\nThe memory of these chips is enough for 42 marks. They work a little slower than NTAG.\nThe system automatically detects the type of used cards.\nRead more\nBase stations\nThe main components of the station are the ATmega328P microcontroller and the MFRC522 module,\nwhich operates at a frequency of 13.56 MHz, real-time clock DS3231SN.\nAll powered by 3 AA batteries through the MCP1700T-33 stabilizer.\nThe capacity of the kit of three alkaline AA batteries should be enough for a year of active use.\nTested at ambient temperatures from -20 to +50 degrees Celcius.\nTotally, the initial components for one base station and the consumables cost about $10 (in 2019).\nRead more\nMaster station\nWith the master station you can read and write tags and configure base stations.\nIt is simpler than the base station.\nIt consists of Arduino Nano, RFID module, LED and buzzer.\nIt connects with a PC via USB.\nRead more\nThere is also a wireless station with the Bluetooth module.\nData processing\nSportiduinoPQ\nCards and stations are configured by SportiduinoPQ program.\nThe program is written on Python, based on PyQt5 and SportiduinoPython module.\nSportOrg\nReading cards is implemented in the SportOrg program.\n\nThis system and its variants have been used in Russia at a number of events\nup to approx. 1400 participants and approx. 70 check points.\n\nAvailable from:  https:\/\/github.com\/sportiduino\/sportiduino\nLicense:         GNU GPLv3\n","153":"Sports Recognition in Videos\nThis repository builds a classifier to predict the sport being played using Dense Trajectory Features. It works\non the UCF Sports dataset and builds a multi-class classifier based on SVM using chi-squared kernel. The paper behind the project is:\nHeng Wang, Alexander Kl\u00e4ser, Cordelia Schmid, Liu Cheng-Lin. Action Recognition by Dense Trajectories. CVPR 2011 - IEEE Conference on Computer Vision & Pattern Recognition, Jun 2011\nPrerequisites\n\nThe UCF Sports Dataset, a\nstripped down version (with no image dumps) can be found\nhere\nDense Trajectory Features,\nthis code has been included in dense_trajectory_release_v1.2 folder\n\nProject Setup\nRun setup.sh to install all dependencies. It also builds a DenseTrack executable\nwhich gives out the features of all videos\nCode Flow\n\nThe DenseTrack executable computes a large feature vector comprising of\nHOG + HOF + MBH descriptors concatenated with each other\nThe data is split into train and test with a ratio test_size\nA codebook of size k using k-means clustering is generated in attempts\nA bag-of-visual-words representation is created for each video using the\nhistogram built using the above clustering\nAll the bag-of-visual-words are fed into the SVM using chi-squared kernel\nand classified using a One-Vs-Rest Classifier\n\nExecution\nTo run the code, run Driver.py which generates the One-Vs-Rest Classifier and\ndumps in a model.p file and along with the codebook centers for bag-of-visual-words\nin centers.p.\nEvaluation\nThe code has been evaluated on accuracy of predictions after the test and train\nsplit ratio of 0.3. Following classes have been used from the UCF Sports Dataset:\n\nDiving-Side (7 videos)\nKicking-Front (10 videos)\nRiding-Horse (12 videos)\nRun-Side (13 videos)\nSkateBoarding-Front (12 videos)\nSwing-Bench (20 videos)\nSwing-SideAngle (13 videos)\nWalk-Front (22 videos)\n\nAfter preliminary evaluation, we achieved a result of around 29%.\nContact\nThe repository has been made available at\nhttps:\/\/github.com\/ChinmayJindal\/sports-recognition\n","154":"two_point_calib\nThis is an implementation of \"A Two-point Method for PTZ camera Calibration in Sports\" (WACV2018)\nDependences:\n\nOpenCV 3.1 or later.\nEigen 3.2.6 or later.\nflann 1.8.4 or later.\nmatio: https:\/\/github.com\/tbeu\/matio\n\nThe code is tested on Xcode 6.4 on a Mac 10.10.5 system. But the code has minimum dependence on compile and system, so it should work well on linux and windows as well.\nFile structure:\nmatlab: synthetic example of the two-point calibration method.\nIn the src\/pan_tilt_forest folder\nbt_dtr: a general implementation of regression forest with back tracking\ncvx_gl and cvx_pgl: geometry and project geometry files\ndt_util: decision tree utility\nutil: pan-tilt-zoom camera pose estimation given ray-3D correspondences\nTodo: cmake file and training parameters\n","155":"Pekka Piril\u00e4n tulospalveluohjelma (sports time keeping program)\nBriefly in Finnish \/ Lyhyesti suomeksi\nT\u00e4ss\u00e4 ovat Pekka Piril\u00e4n (1945-2015) tulospalveluohjelmien l\u00e4hdekoodit. Mukana\novat kaikki muunnelmat: teksti- ja Windows-versio sek\u00e4 henkil\u00f6kohtaisesta ett\u00e4\nviestiohjelmasta. Pekan perhe julkaisi l\u00e4hdekoodin avoimen l\u00e4hdekoodin\nGPLv3-lisenssin alaiseksi. Lisenssi tarkoittaa karkeasti sit\u00e4, ett\u00e4 kuka vain\nvoi muuttaa ja k\u00e4ytt\u00e4\u00e4 ohjelmaa vapaasti kaikissa tilanteissa. Ohjelman\nk\u00e4ytt\u00f6\u00f6n liittyvi\u00e4 palveluita ja jopa kopioita ohjelmasta saa myyd\u00e4, mutta\nmuutetun version levitt\u00e4j\u00e4 sitoutuu julkaisemaan versiostaan my\u00f6s l\u00e4hdekoodin\nja ostaja saa j\u00e4lleen tehd\u00e4 kopiollaan mit\u00e4 haluaa.\nValmiiksi k\u00e4\u00e4nnetyt ja paketoidut versiot sek\u00e4 ohjeet l\u00f6ytyv\u00e4t edelleen\nosoitteesta http:\/\/www.pirila.fi\/ohj\/index.html.\nPekan perheen toiveena on, ett\u00e4 ohjelmasta olisi hy\u00f6ty\u00e4 urheiluyhteis\u00f6lle viel\u00e4\nvuosien ajan.\nDescription\nThis is the source code of a suite of sports time keeping programs Pekka Piril\u00e4\n(1945-2015) started developing in around 1986. The program originally\nspecialized in orienteering, but was later amended to support additional\nsports. The user interface is in Finnish and there are Finnish language\nvariables and comments throughout the source code. The source code is released\nunder GPLv3. More information in Finnish at http:\/\/www.pirila.fi\/ohj\/index.html.\nConsole programs\nRequired tools\nThe console program's project files are for Visual Studio. They were\nsuccessfully compiled with Visual Studio Express 2013 for Windows Desktop,\nbut Visual Studio 2010 and anything newer can probably be made to work. The\nlanguage is probably compliant with C++03.\nCompile\n\nOpen TPsource\\V52\\VS\\Libs\\tputilv2.sln\nCompile by pressing F7\n\nTwo new folders will be created next to TPsource: vc10 and TPexe\n\n\nOpen TPsource\\V52\\VS\\Hk\\HkMaali520.sln\nCompile by pressing F7\n\nA stand-alone executable TPexe\\Hk\\V521\\HkMaali.exe is built and ready\nto use\n\n\nTo build the relay version, repeat with TPsource\\V52\\VS\\V\\JukMaali520.sln\n\nWindows programs\nRequired tools\nThe Windows programs are made with\nEmbarcadero C++ Builder.\nVersion 10.1 Berlin was successfully used to compile and run the program.\nSecureBridge 7.1 for RAD Studio 10.1 Berlin\nis an add-on that is required to compile and run the program. NOTE: At least\nin Windows 10 you must edit one of SecureBridge's header files to be able to\ncompile this program. Open\n\"Program Files (x86)\\Devart\\SecureBridge for RAD Studio 10\\Include\\Win32\\ScSSHSocket.hpp\"\nand change Winapi::Winsock::PSockAddrIn to Winapi::Winsock2::PSockAddrIn.\nCompile\n\nOpen TPsource\\V52\\RADStudio10\\DBboxm-XE.cbproj\nRight-click on DBboxm-XE.lib in Project Manager and select Make\nOpen TPsource\\V52\\RADStudio10\\Tputil-XE.cbproj\nRight-click on Tputil-XE.lib in Project Manager and select Make\nOpen TPsource\\V52\\RADStudio10\\HkKisaWin.cbproj and Run to start the\nprogram for individual competitions\nOpen TPsource\\V52\\RADStudio10\\ViestiWin.cbproj and Run to start the\nrelay program\n\nHeap errors during compilation\nIf you get heap errors with linking, you can try these things\n\nhttp:\/\/stackoverflow.com\/questions\/28929516\/c-builder-xe7-lme288-error\n\nRun command prompt as Administrator.\nType (without quotes) \"bcdedit \/set IncreaseUserVa 3072\"\nReboot computer.\n\n\nEmpty %TEMP%, reboot, try again, repeat\nRun C++ Builder as admin\n\n","156":"DFSCoin development tree\nDFSCoin is a PoS-based cryptocurrency.\nDevelopment process\nDevelopers work in their own trees, then submit pull requests when\nthey think their feature or bug fix is ready.\nThe patch will be accepted if there is broad consensus that it is a\ngood thing.  Developers should expect to rework and resubmit patches\nif they don't match the project's coding conventions (see coding.txt)\nor are controversial.\nThe master branch is regularly built and tested, but is not guaranteed\nto be completely stable. Tags are regularly created to indicate new\nstable release versions of DFSCoin.\nFeature branches are created when there are major new features being\nworked on by several people.\nFrom time to time a pull request will become outdated. If this occurs, and\nthe pull is no longer automatically mergeable; a comment on the pull will\nbe used to issue a warning of closure. The pull will be closed 15 days\nafter the warning if action is not taken by the author. Pull requests closed\nin this manner will have their corresponding issue labeled 'stagnant'.\nIssues with no commits will be given a similar warning, and closed after\n15 days from their last activity. Issues closed in this manner will be\nlabeled 'stale'.\n","157":"SportsCash Core integration\/staging repository\nDash forked Bitcoin - PIVX Forked Dash - SPORTSCASH Forked PIVX\nProject INFO\n\nBitcointalkhttps:\/\/bitcointalk.org\/index.php?topic=4356707.msg38922386#msg38922386\nTelegramhttps:\/\/t.me\/joinchat\/GzldMxHltxGz_ZqHSFlGPg\nDiscordhttps:\/\/discord.gg\/GfuxwGB\nTwitterhttps:\/\/twitter.com\/SportsCashCoin\nReddithttps:\/\/www.reddit.com\/r\/SportsCashCoin\/\nWebsitehttp:\/\/sportscash.co\n\nCoin Specs\n\nAlgoQuark\nBlock Time60 Seconds\nDifficulty RetargetingEvery Block\nMax Coin Supply (PoS)24,000,000 SCC\nPremine1,000,000 SCC\nMasternode Collateral10,000 SCC\nPort Collateral33001\nRPCPort Collateral33002\n\nPoS Rewards Breakdown\n\nBlock HeightRewardMasternodesStakers\n<= 2,0001 SCC0.9 SCC0.1 SCC\n2001-1,144,8577 SCC6.3 SCC0.7 SCC\n1,144,858-2,744,8575 SCC4.5 SCC0.5 SCC\n>2744858-54115233 SCC2.7 SCC0.3 SCC\n5411523- 0 SCC0 SCC0 SCC\n\n","158":"sports\nAnalytics, tools and puzzles applicable to a variety of sports.\n","159":"Coursera-Data Structures and Algorithms Specialization\nThis specialization is a mix of theory and practice: you will learn algorithmic techniques for solving various computational problems and will implement about 100 algorithmic coding problems in a programming language of your choice. No other online course in Algorithms even comes close to offering you a wealth of programming challenges that you may face at your next job interview. To prepare you, we invested over 3000 hours into designing our challenges as an alternative to multiple choice questions that you usually find in MOOCs. Sorry, we do not believe in multiple choice questions when it comes to learning algorithms...or anything else in computer science! For each algorithm you develop and implement, we designed multiple tests to check its correctness and running time \u2014 you will have to debug your programs without even knowing what these tests are! It may sound difficult, but we believe it is the only way to truly understand how the algorithms work and to master the art of programming. The specialization contains two real-world projects: Big Networks and Genome Assembly. You will analyze both road networks and social networks and will learn how to compute the shortest route between New York and San Francisco (1000 times faster than the standard shortest path algorithms!) Afterwards, you will learn how to assemble genomes from millions of short fragments of DNA and how assembly algorithms fuel recent developments in personalized medicine.\nData Structures and Algorithms Specialization\nSkills Gained:\nAlgorithms Data Structure Debugging Graph Theory Software Testing Binary Search Tree Computer Programming\nCourse 1 - Algorithmic Toolbox\nThe course covers basic algorithmic techniques and ideas for computational problems arising frequently in practical applications: sorting and searching, divide and conquer, greedy algorithms, dynamic programming. We will learn a lot of theory: how to sort data and how it helps for searching; how to break a large problem into pieces and solve them recursively; when it makes sense to proceed greedily; how dynamic programming is used in genomic studies. You will practice solving computational problems, designing new algorithms, and implementing solutions efficiently (so that they run in less than a second).\nCourse 2 - Data Structures\nA good algorithm usually comes together with a set of good data structures that allow the algorithm to manipulate the data efficiently. In this course, we consider the common data structures that are used in various computational problems. You will learn how these data structures are implemented in different programming languages and will practice implementing them in our programming assignments. This will help you to understand what is going on inside a particular built-in implementation of a data structure and what to expect from it. You will also learn typical use cases for these data structures.\nA few examples of questions that we are going to cover in this class are the following:\n\nWhat is a good strategy of resizing a dynamic array?\nHow priority queues are implemented in C++, Java, and Python?\nHow to implement a hash table so that the amortized running time of all operations is O(1) on average?\nWhat are good strategies to keep a binary tree balanced?\n\nCourse 3 - Algorithms on Graphs\nIf you have ever used a navigation service to find optimal route and estimate time to destination, you've used algorithms on graphs. Graphs arise in various real-world situations as there are road networks, computer networks and, most recently, social networks! If you're looking for the fastest time to get to work, cheapest way to connect set of computers into a network or efficient algorithm to automatically find communities and opinion leaders in Facebook, you're going to work with graphs and algorithms on graphs.\nIn this course, you will first learn what a graph is and what are some of the most important properties. Then you'll learn several ways to traverse graphs and how you can do useful things while traversing the graph in some order. We will then talk about shortest paths algorithms \u2014 from the basic ones to those which open door for 1000000 times faster algorithms used in Google Maps and other navigational services. You will use these algorithms if you choose to work on our Fast Shortest Routes industrial capstone project. We will finish with minimum spanning trees which are used to plan road, telephone and computer networks and also find applications in clustering and approximate algorithms.\nCourse 4 - Algorithms on Strings\nWorld and internet is full of textual information. We search for information using textual queries, we read websites, books, e-mails. All those are strings from the point of view of computer science. To make sense of all that information and make search efficient, search engines use many string algorithms. Moreover, the emerging field of personalized medicine uses many search algorithms to find disease-causing mutations in the human genome.\nCourse 5 - Advanced Algorithms and Complexity\nYou've learned the basic algorithms now and are ready to step into the area of more complex problems and algorithms to solve them. Advanced algorithms build upon basic ones and use new ideas. We will start with networks flows which are used in more typical applications such as optimal matchings, finding disjoint paths and flight scheduling as well as more surprising ones like image segmentation in computer vision. We then proceed to linear programming with applications in optimizing budget allocation, portfolio optimization, finding the cheapest diet satisfying all requirements and many others. Next we discuss inherently hard problems for which no exact good solutions are known (and not likely to be found) and how to solve them in practice. We finish with a soft introduction to streaming algorithms that are heavily used in Big Data processing. Such algorithms are usually designed to be able to process huge datasets without being able even to store a dataset.\nCourse 6 - Genome Assembly Programming Challenge\nIn Spring 2011, thousands of people in Germany were hospitalized with a deadly disease that started as food poisoning with bloody diarrhea and often led to kidney failure. It was the beginning of the deadliest outbreak in recent history, caused by a mysterious bacterial strain that we will refer to as E. coli X. Soon, German officials linked the outbreak to a restaurant in L\u00fcbeck, where nearly 20% of the patrons had developed bloody diarrhea in a single week. At this point, biologists knew that they were facing a previously unknown pathogen and that traditional methods would not suffice \u2013 computational biologists would be needed to assemble and analyze the genome of the newly emerged pathogen.\nTo investigate the evolutionary origin and pathogenic potential of the outbreak strain, researchers started a crowdsourced research program. They released bacterial DNA sequencing data from one of a patient, which elicited a burst of analyses carried out by computational biologists on four continents. They even used GitHub for the project: https:\/\/github.com\/ehec-outbreak-crowdsourced\/BGI-data-analysis\/wiki\nThe 2011 German outbreak represented an early example of epidemiologists collaborating with computational biologists to stop an outbreak. In this Genome Assembly Programming Challenge, you will follow in the footsteps of the bioinformaticians investigating the outbreak by developing a program to assemble the genome of the E. coli X from millions of overlapping substrings of the E.coli X genome.\n","160":"NewQuant\nNewQuant is a C++ library for data analysis and financial engineering computation. It is in building now, not a completed version.\n01.ExceptionClass is finished\uff0cit is a self-defined exception class.\n02.MathematicsExpression is finished\uff0cit helps users to build numerical functors in a convenient way.\n03.MatrixComputation\uff0cthe most important module of NewQuant\uff0cis finished mostly\uff0cit includes kinds of matrices and kinds of linear-equations solvers. Users can use it to do basic matrix computation\uff0cto solve linear-equation\uff0cto do matrix decomposition(such as LU decomposition)\uff0cto solve least square problem.\n04.MonteCarlo\uff0cthe majority part is finished\uff0cit includes kinds of SDEsolvers\uff0cusers can use it to simulate kinds of SDEs\uff0cfor example GBM. This module is very useful for financial engineering.\n05.Regression is in building now\uff0cit is the base of econometrics.\n06.SpecialFunction is in building now\uff0cit is the base of StatisticsComputation module.\n07.StatisticsComputation\uff0cis finished partly\uff0cit includes kinds of computation about pdf\uff0ccdf and quantile now\uff0cit is also the base of econometrics.\nNewQuant is released under BSD license.\n","161":"FEAT\n\n\nFEAT is a feature engineering automation tool that learns new representations of raw data\nto improve classifier and regressor performance. The underlying methods use Pareto\noptimization and evolutionary computation to search the space of possible transformations.\nFEAT wraps around a user-chosen ML method and provides a set of representations that give the best\nperformance for that method. Each individual in FEAT's population is its own data representation.\nFEAT uses the Shogun C++ ML toolbox to fit models.\nCheck out the documentation for installation and examples.\nCite\nLa Cava, W., Singh, T. R., Taggart, J., Suri, S., & Moore, J. H.. Learning concise representations for regression by evolving networks of trees. ICLR 2019. arxiv:1807.0091\nBibtex:\n@inproceedings{la_cava_learning_2019,\n    series = {{ICLR}},\n    title = {Learning concise representations for regression by evolving networks of trees},\n    url = {https:\/\/arxiv.org\/abs\/1807.00981},\n    language = {en},\n    booktitle = {International {Conference} on {Learning} {Representations}},\n    author = {La Cava, William and Singh, Tilak Raj and Taggart, James and Suri, Srinivas and Moore, Jason H.},\n    year = {2019},\n}\n\nContact\nMaintained by William La Cava (lacava at upenn.edu)\nAcknowledgments\nThis work is supported by grant K99-LM012926 from the National Library of Medicine.\nFEAT is being developed to study human disease in the Epistasis Lab\nat UPenn.\nLicense\nGNU GPLv3, see LICENSE\n","162":"FEAT\n\n\nFEAT is a feature engineering automation tool that learns new representations of raw data\nto improve classifier and regressor performance. The underlying methods use Pareto\noptimization and evolutionary computation to search the space of possible transformations.\nFEAT wraps around a user-chosen ML method and provides a set of representations that give the best\nperformance for that method. Each individual in FEAT's population is its own data representation.\nFEAT uses the Shogun C++ ML toolbox to fit models.\nCheck out the documentation for installation and examples.\nCite\nLa Cava, W., Singh, T. R., Taggart, J., Suri, S., & Moore, J. H.. Learning concise representations for regression by evolving networks of trees. ICLR 2019. arxiv:1807.0091\nBibtex:\n@inproceedings{la_cava_learning_2019,\n    series = {{ICLR}},\n    title = {Learning concise representations for regression by evolving networks of trees},\n    url = {https:\/\/arxiv.org\/abs\/1807.00981},\n    language = {en},\n    booktitle = {International {Conference} on {Learning} {Representations}},\n    author = {La Cava, William and Singh, Tilak Raj and Taggart, James and Suri, Srinivas and Moore, Jason H.},\n    year = {2019},\n}\n\nContact\nMaintained by William La Cava (lacava at upenn.edu)\nAcknowledgments\nThis work is supported by grant K99-LM012926 from the National Library of Medicine.\nFEAT is being developed to study human disease in the Epistasis Lab\nat UPenn.\nLicense\nGNU GPLv3, see LICENSE\n","163":"data-structures-and-algorithms\nThis project contains learning materials for the Data Structures and Algorithms course for students from Software Engineering program at Sofia University, Faculty of Mathematics and informatics\n","164":"TH10_DataReversing\nUsing binary reverse engineering techniques to extract the memory data of the Touhou 10th game, Mountain of Faith.\nThese are some AI projects about MoF:\ntwinject by Netdex\nTH10AI by DREAMWORLDVOID\ntouhou10-dqn by actumn\n","165":"game-data-reverse-engineering\nReverse engineering resources for data files from various video games I like\n","166":"Engineering\nMy academic adventures.\nThis repository contains all my projects during my graduation.\n","167":"libSPRITE\n#Installation\nTo install, type make install as root.\nThis will copy headers files to \/usr\/local\/include\/SPRITE\/ and static library\nto \/usr\/local\/lib\/SPRITE by default.\nTo uninstall, type make uninstall as root.\nLua Paths\nlibSPRITE assumes the Lua headers are install in \/usr\/local\/include. Some\ndistributions place it elsewhere. You can change where the Makefile looks for\nthe Lua include files by specifying the LUA_INCLUDE variable in the arguments\nto make.\nex.: make LUA_INCLUDE=\/usr\/include\/lua5.2\nYou can also change the Lua library path by setting the LUA_LIB variable.\nTesting\n'make test' will build the unit tests for this package. You must have cppunit\ninstalled to compile and run the unit tests. After compiling, run '.\/run_test'\nto execute the unit tests. You will have to run as root to execute all test cases.\nNOTE: You must start from a clean systems ('make clean') before running 'make test'. Otherwise, some tests that depend on compile time assertions will fail.\nMakefile Overrides\nBy default, libSPRITE sends output to stdout and stderr for info, warnings, and errors. To supress these messages, you can specifiy -DNO_PRINT_INFO -DNO_PRINT_WARNING -DNO_PRINT_ERROR. The best way to do that is by appending to these options to the USER_CFLAGS. For example:\nmake USER_CFLAGS='-DNO_PRINT_INFO -DNO_PRINT_WARNING'\n\nThere are also overides for CPPFLAGS (USER_CPPFLAGS) and LDFLAGS (USER_LDFLAGS).\nUsing CMake\nTo build with cmake, create a directory called build, cd to the build directory and type cmake ..\/.\nTo specify the build prefix, using the CMAKE_INSTALL_PREFIX macro. Example\ncmake -DCMAKE_INSTALL_PREFIX=\/usr\/local\/\n\nBy default, the build type is Release. To build unit tests us the CMAKE_BUILD_TYPE macro. Example:\ncmake -DCMAKE_BUILD_TYPE=Test\n\nAfter specifying cmake with this macro, make will create a run_test executable in each folder. Execute the run_test executable for the folder you wish to test.\nRun cmake with the -DCMAKE_BUILD_TYPE=Release option to switch back to the normal build.\nDocumentation\nDocumentation can be found on the Wiki\nTutorial\nA growing Tutorial for developing applications using libSPRITE can be found here\n","168":"Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we\u2019ll help you sort it out.\n","169":"Artificial intelligence library\nMy C++ deep learning framework (with GPU support) & other machine learning algorithms implementations\nDeepLearning Operations\n\nConvolution\nDropout\nSoftmax\nRecurrent\nLinear\nSigmoid, Tanh, Relu, Selu activations\nLayer normalization\nAddition\nConcatenation\nMaxpooling\nAveragepooling\nResidualBlock\nAutoencoder\nL1 e L2 regularizations\nGradient clipping\n\nNeural network optimizers\n\nStochastic gradient descent with minibatch and momentum\nDirect Feedback Alignment\n\nDeep Reinforcement Learning agents\n\nDeep Qlearning Agents\n\nDeep Reinforcement Learning environments\n\nKbandits\nTicTacToe\n\nOptimization algorithms\n\nGenetic algorithms (with multicore features for high performance)\nParticle Swarm Optimization\n\nOther machine learning algorithms\n\nlinear regression\nlogistic regression\ngenetic programming\n\nData Mining\n\nK-means clustering\n\nVisualization\n\nBitmap class for loading, saving and processing multiple image formats\nVisualizaiton namespace for fast data visualization\n\nLicense\nThe MIT License (MIT)\nCopyright (c) 2015 Carlo Meroni\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n","170":"About This Repository\nThis repository should now be considered a historical curiosity only.\nThe original version of this code was developed between 2002-2004 and included free with the book \"Artificial Intelligence for Games\". Over the intervening years, the code has become a less useful reference. The third edition of the textbook is considerably larger, and this code did not keep up with errata or improvements in the algorithms. I have not use the code in this repository for my consulting work in over a decade.\nThe third edition of the textbook does not mention this code, but includes considerably expanded and corrected listings in the text in a language neutral format. I would recommend that version.\n\nHistorical Information\nThe Artificial Intelligence for Games system.\nCopyright (c) Ian Millington 2003-2009. All Rights Reserved.\nThis software is distributed under licence. Use of this software\nimplies agreement with all terms and conditions of the accompanying\nsoftware licence.\nThis code also contains portions of the AI Core engine.\nCopyright (c) Icosagon Limited 2003-2007. All Rights Reserved.\nPlease see accompanying LICENSE file.\nInstallation\nThe code can be extracted to any directory.\nPlatform Compatibility\nThe software has been designed for platform indepedence as much as\npossible. The only file that may need altering for your platform is\n.\/src\/timing.cpp which currently wraps the windows multimedia timer.\nBuilding\nBuilding with Scons\nThe code can be built using SCONS, available from\nhttp:\/\/www.scons.org\/. Simply cd (change directory) into the build\ndirectory and type scons.\n\ncd build\nscons\n\nTo remove intermediate files after building (but leaving the library\nand demos):\n\nscons -c ..\/src\n\nBuilding with Microsoft Visual Studio 8 Professional\nSolution and project files are included for use with Microsoft visual\nstudio 8 professional. They may also work with Express edition, but\nI've not tested that.\nBuilding with CMake on Linux\nThe code can be built on Linux (and possibly on other platforms)\nusing CMake:\ncd build\ncmake .\nmake\nDocumentation\nTo build the documentation (see below) you must have doxygen\ninstalled (it is available from http:\/\/www.stack.nl\/~dimitri\/doxygen\/\nSimply cd into the .\/doc\/build\/doxygen directory, then type:\n\ndoxygen aicore.config\n\nto build the documentation.\nLayout\nThe build process creates a statically linked library in .\/lib which\ncan be used with the include headers in .\/include. The demo programs\nare built and placed in the .\/bin directory.\nSource code is contained in the .\/src directory, and documentation\nis in the .\/doc directory, in particular the reference documentation\nis in the .\/doc\/ref directory.\nDocumentation\nThe source code is heavily documented, and the contents correspond to\nthe discussion in the \"Artificial Intelligence for Games\" book.\nIt is possible to create 'doxygen' documentation with the tags in the\nsource code files, and a configuration for building the documentation\nis provided in the .\/doc\/build\/doxygen directory. The doxygen\nconfiguration supplied provides only html output, since other output\nformats depend on how your machine is configured.\nThis is not currently targeted from the scons configuration, because\nscons suport for doxygen depends on where doxygen is installed on your\nmachine.\nDemos\nTo run the demos you will require OpenGL and GLUT installed on your\nmachine, and the relevant DLLs or shared objects on the path.\n","171":"OpenNERO\n   \nOpenNERO is an open source software platform designed for\nresearch and education in Artificial Intelligence. The project is based on the\nNeuro-Evolving Robotic Operatives (NERO) game developed by graduate\nand undergraduate students at the  Neural Networks Research Group and\nDepartment of Computer Science at the\nUniversity of Texas at Austin.\nIn particular, OpenNERO has been used to implement several demos and exercises for Russell\nand Norvig's textbook Artificial Intelligence: A Modern Approach.  These\ndemos and exercises illustrate AI methods such as brute-force search, heuristic search, scripting,\nreinforcement learning, and evolutionary computation, and AI problems such as maze running,\nvacuuming, and robotic battle. The methods and problems are implemented in several different\nenvironments (or \"mods\"), as described below.\nMore environments, problems, and methods, as well as demos and exercises illustrating them, will\nbe added in the future. The current ones are intended to serve as a starting point on which new\nones can be built, by us, but also by the community at large. If you have questions or would like to contribute, check out the OpenNERO Google Group.\nGet Started\n\nGet OpenNERO\n\nOpenNERO Dependencies\nDownload a Binary\n\nLinux\nWindows\nMac OS X\n\n\nBuild from Source\n\nLinux\nWindows\nMac OS X\n\n\n\n\nDemonstrations\n\nSearch\n\nThe Maze Environment\nFirst Person Search\nUninformed Search\nHeuristic Search\n\n\nPlanning\n\nTower of Hanoi Environment\nSymbolic Planning\n\n\nNatural Language Processing\n\nTower of Hanoi Environment\nNatural Language Processing\n\n\nReinforcement Learning\n\nThe Maze Environment\nQ-learning\n\n\nEvolutionary Computation\n\nThe Roomba Environment\nNeuroevolution\n\n\nMulti-Agent Systems\n\nNERO Environment\nNERO Machine Learning Game\n\n\nVision\n\nThe Vision Environment\nVision: Edge Detection\n\n\n\n\nExercises\n\nOpenNero Setup\nAdding Stuff\nCreate Roomba Agent\nMaze Generator\nMaze Solver\nAI Exercises\nHeuristic Search\nPlanning\nNLPExercise\nQLearningExercise\nMaze Learner\nAdvanced Maze\nNero Tournament\nSample Tournament Results\nObject Recognition\n\n\nSystem Documentation\nHeadless Mode\nSystem Overview\n\nContributors\nMany people have contributed to OpenNERO, including Igor V. Karpov, John B. Sheblak, Adam Dziuk, Minh Phan, Dan Lessin, Wes Tansey, Reza Mahjourian, Risto Miikkulainen, members of the Neural Networks Research Group at UT Austin, students and alumni of the Computational Intelligence and Game Design stream of the Freshman Research Initiative at UT Austin.\n\nNOTE: as with any active project, OpenNERO is a work in progress and many updates are frequently being made. If you have trouble with OpenNERO, check the discussion group and then consider submitting an issue. And of course, if you would like to contribute, let us know!\n","172":"AIKIDO - AI for KIDO   \n\n\u26a0\ufe0f Warning: AIKIDO is under heavy development. These instructions are\nprimarily for reference by the developers.\n\nAIKIDO is a C++ library, complete with Python bindings, for solving robotic motion\nplanning and decision making problems. This library is tightly integrated with\nDART for kinematic\/dynamics calculations and OMPL for motion planning. AIKIDO\noptionally integrates with ROS, through the suite of aikido_ros packages, for\nexecution on real robots.\nInstallation\nOn Ubuntu Trusty using apt-get\nAIKIDO depends on ROS. You should install ROS by adding the ROS repository to your sources.list as follows. We encourage users to install indigo.\n$ sudo sh -c 'echo \"deb http:\/\/packages.ros.org\/ros\/ubuntu $(lsb_release -sc) main\" > \/etc\/apt\/sources.list.d\/ros-latest.list'\n$ sudo apt-key adv --keyserver 'hkp:\/\/keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654\n$ sudo apt-get update\n$ sudo apt-get install ros-indigo-actionlib ros-indigo-geometry-msgs ros-indigo-interactive-markers ros-indigo-roscpp ros-indigo-std-msgs ros-indigo-tf ros-indigo-trajectory-msgs ros-indigo-visualization-msgs\nOnce ROS is installed, you can install AIKIDO from the Personal Robotics Lab PPA:\n$ sudo add-apt-repository ppa:libccd-debs\/ppa\n$ sudo add-apt-repository ppa:fcl-debs\/ppa\n$ sudo add-apt-repository ppa:dartsim\/ppa\n$ sudo add-apt-repository ppa:personalrobotics\/ppa\n$ sudo apt-get update\n$ sudo apt-get install libaikido-all-dev\nOn macOS using Homebrew\n# Install the Homebrew package manager\n$ \/usr\/bin\/ruby -e \"$(curl -fsSL https:\/\/raw.githubusercontent.com\/Homebrew\/install\/master\/install)\"\n# Add Homebrew tap for Personal Robotics Lab software\n$ brew tap personalrobotics\/tap\n# Install AIKIDO\n$ brew install aikido\n\nNote: While ROS seems to be available on macOS, we haven't tested it with AIKIDO. For now, brew install aikido installs AIKIDO without the ROS-dependent components.\n\nBuilding from Source\nDependencies\nAIKIDO depends on CMake, Boost, DART (version 6.3 or above), OMPL, yaml-cpp, tinyxml2, pr-control-msgs, libmicrohttpd, and the\nPython development headers (python-dev on Debian systems). DART and AIKIDO both\nmake heavy use of C++14 and require a modern compiler.\nOn Ubuntu Trusty using CMake\nYou should install the ROS packages as described above to build all the ROS-dependent AIKIDO components (e.g., aikido-control-ros).\nInstall the other dependencies:\n$ sudo add-apt-repository ppa:libccd-debs\/ppa\n$ sudo add-apt-repository ppa:fcl-debs\/ppa\n$ sudo add-apt-repository ppa:dartsim\/ppa\n$ sudo add-apt-repository ppa:personalrobotics\/ppa\n$ sudo apt-get update\n$ sudo apt-get install cmake build-essential libboost-filesystem-dev libdart6-optimizer-nlopt-dev libdart6-utils-dev libdart6-utils-urdf-dev libmicrohttpd-dev libompl-dev libtinyxml2-dev libyaml-cpp-dev pr-control-msgs\nOnce the dependencies are installed, you can build and install AIKIDO using CMake:\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make  # you may want to build AIKIDO using multi-core by executing `make -j4`\n$ sudo make install\nAIKIDO includes several optional components that depend on ROS. While we\nsuggest building AIKIDO in a Catkin workspace (see below) to enable the ROS\ncomponents, it is also possible to build those components in a standalone\nbuild. To do so, source the setup.bash file in your Catkin workspace before\nrunning the above commands, e.g.:\n$ . \/path\/to\/my\/workspace\/setup.bash\nOn Ubuntu Trusty using Catkin\nIt is also possible to build AIKIDO as a third-party package inside a\nCatkin workspace. To do so, clone AIKIDO into your Catkin\nworkspace and use the catkin build command like normal.\nIf you are using the older catkin_make command, then you must build your workspace\nwith catkin_make_isolated. This may dramatically increase your build time, so we\nstrongly recommend that you use catkin build, which is provided by the\ncatkin_tools package, if possible.\nOn macOS using CMake\nPlease install Homebrew as described above, then you can easily install the dependencies as follows:\n$ cd <aikido_directory>\n$ brew bundle\nOnce the dependencies are installed, you can build and install AIKIDO using CMake:\n$ cd <aikido_directory>\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make  # you may want to build AIKIDO using multi-core by executing `make -j4`\n$ sudo make install\nCode Style\nPlease follow the AIKIDO style guidelines when making a contribution.\nLicense\nAIKIDO is licensed under a BSD license. See LICENSE for more\ninformation.\nAuthors\nAIKIDO is developed by the\nPersonal Robotics Lab in the\nPaul G. Allen School of Computer Science and Engineering at\nthe University of Washington.\nThe library was started by\nMichael Koval (@mkoval)\nand Pras Velagapudi (@psigen).\nIt has received major contributions from\nShushman Choudhury (@Shushman),\nBrian Hou (@brianhou),\nAaron Johnson (@aaronjoh),\nJennifer King (@jeking),\nGilwoo Lee (@gilwoolee),\nJeongseok Lee (@jslee02),\nand Clint Liddick (@ClintLiddick).\nWe also would like to thank\nMichael Grey (@mxgrey)\nand Jeongseok Lee (@jslee02)\nfor making changes to DART to better support AIKIDO.\n","173":"\u26a1\u7535\u78c1AI\u667a\u80fd\u8f66\n\n\n\n\n\n\n\u91c7\u96c6\u6570\u636e\u53d1\u9001\u81f3\u4e0a\u4f4d\u673a\u3001\u8fd0\u884c\u8bad\u7ec3\u597d\u7684\u6a21\u578b\n\u2728FEATURE\n\n\ud83d\ude80\u7eaf\u7535\u78c1\u56db\u8f6e\n\ud83d\ude91\u57fa\u4e8e\u72b6\u6001\u673a\u7684\u51fa\u8f68\u3001\u5835\u8f6c\u4fdd\u62a4\n\ud83c\udfa81.3\u540bOLED\u4e0e6\u4e2a\u6309\u952e\u7684\u4e2d\u6587GUI\n\ud83d\udca5\u4e00\u952e\u5207\u6362AI\u6a21\u5f0f\u4e0ePID\u6a21\u5f0f\uff01\n\ud83d\uddc3\ufe0f\u53ef\u5728\u8fd0\u884c\u65f6\u5207\u6362AI\u6a21\u578b\n\ud83d\udd27\u5728Keil\u4e2d\u901a\u8fc7GUI\u914d\u7f6e\u5168\u8f66\u53c2\u6570\n\u26a1PID\u91c7\u7528CMSIS-DSP\u5e93\u5b9e\u73b0\n\u26a1\u517c\u987e\u6548\u7387\u4e0e\u7075\u6d3b\u6027\u7684\u6ed1\u52a8\u5e73\u5747\u6ee4\u6ce2\u5668\n\ud83d\udd12\u8fd0\u884c\u65f6\u9519\u8bef\u68c0\u67e5\uff1aCAR_ERROR_CHECK\n\u2934\ufe0f\u6781\u7b80\u8c03\u5ea6\u5668\n\ud83d\udd52\u57fa\u4e8e\u5e73\u8861\u4e8c\u53c9\u6811\u7684\u8f6f\u4ef6\u5b9a\u65f6\u5668\n\ud83d\udd0b\u4f4e\u7535\u91cf\u81ea\u52a8\u5173\u673a\n\u267b\ufe0fstd::set_new_handler\n\n\ud83d\udc40PREVIEW\n\u63a7\u5236\u53f0\n\n\u590d\u4f4d\u540e\u6253\u5370\u7684\u5185\u5bb9\n\n\n\u5f00\u59cb\n\n\u56fe\u4e2d\u67f1\u5b50\u662f\u7535\u6c60\u7535\u91cf\uff0c\u6570\u5b57\u662f\u201cCPU\u5360\u7528\u7387\u201d\n\n\n\u4e3b\u9875\n\n\u53f3\u4e0a\u89d2\u6570\u5b57\u4ee3\u8868\u201cCPU\u5360\u7528\u7387\u201d\u7684\u4f4d\u6570\n\n\n\u6a21\u578b\u9009\u62e9\n\n\n\u9884\u5b58\u4e863\u4e2amodel.nncu.c\uff0c\u53ef\u5728\u8fd0\u884c\u65f6\u5207\u6362\u3002\n\n\u63a7\u5236\u9762\u677f\n\n\n\u7535\u673a\u8bbe\u7f6e\n\n\n\u65b9\u5411\u63a7\u5236\n\n\u6570\u5b57\u7535\u4f4d\u5668\n\n\u7535\u78c1\u4f20\u611f\u5668\n\n\n\n\u5355\u51fb\u5207\u6362\u663e\u793a\u65b9\u5f0f\n\n\u663e\u793a\u6ce2\u5f62\n\n\n\u914d\u7f6e\u53c2\u6570\n\nLIBRARY\n\n\u81f4\u656c\u5f00\u6e90\uff01\n\n\nnncu\nSimpleGUI\n\u9010\u98de\u79d1\u6280RT1064\u5f00\u6e90\u5e93\nStateMachineCompact\nMCUXpressoSDK\nDSP\n\n\ud83d\udcccDEPENDENCE\n\n\u5f00\u53d1\u73af\u5883\uff1aKeil 5.31\n\u7f16\u8bd1\u5668\uff1aArmClang V6.14\n\u4f7f\u7528MCUXpresso Config Tool\u521d\u59cb\u5316\u65f6\u949f\u548c\u5916\u8bbe\n\u8bf7\u4f7f\u7528Keil\u6253\u5f00car_config.h\n\n\u5341\u4e94\u5c4a\u5fc5\u80dc \ud83c\udf89 \ud83c\udf89 \ud83c\udf89\n","174":"Welcome to the modern version of Polyworld, an Artificial Life\nsystem designed as an approach to Artificial Intelligence.\nDocumentation is hosted on our wiki: https:\/\/github.com\/polyworld\/polyworld\/wiki\nFor installation instructions, please refer to the wiki page for your OS:\n\nLinux Installation\nMac Installation\n\nNote that the Github repository at https:\/\/github.com\/polyworld\/polyworld is the official\nhome of the Polyworld project, which was formerly hosted at\nhttp:\/\/sourceforge.net\/projects\/polyworld.\n","175":"Quackle   \n\n\nCrossword game artificial intelligence and analysis tool.\nSee LICENSE in this directory.\nBuilding Quackle:\nQuackle is built and tested with the latest releases of Qt 5.12 and 5.13.\nSee README.MacOS and README.Windows for platform-specific instructions.  Generally:\nClone the repo or download the tarball and untar.  Use qmake to build quackle.pro and quackleio\/quackleio.pro:\nqmake quackle.pro && make\ncd quackleio && qmake && make && cd ..\n\nFinally, build the main binary.\ncd quacker && qmake && make\n\nThe binary will build as 'Quackle'.  It might be found in the quacker directory or in the release subdirectory.\nFile organization:\n\nquackle\/ - libquackle sources.  libquackle is the engine, and can be linked to any convenient interface.  It does not use Qt.\nquackle\/quackleio\/ - I\/O library for Quackle.  Implements stuff for accessing dictionaries, serializing GCG files, etc.  Also, command-line option handling.  This does have some modest dependencies on Qt.\nquackle\/quacker\/ - code for full Quackle UI.  Written in Qt, and requires libquackleio and libquackle.\nquackle\/makeminidawg\/ - standalone console program for building Quackle dictionaries.\nquackle\/makegaddag\/ - standalone console program for building gaddag files.\nquackle\/data\/ - lexicons, strategy files, and alphabet resources for Quackle.\nIn this directory is libquackle. Run qmake and then run make in this directory. Then cd to quackle\/quackleio\/, run qmake, and then run make.\n\nolaughlin@gmail.com\njasonkatzbrown@gmail.edu\njfultz@wolfram.com\nmatt.liberty@gmail.com\n","176":"Physis Shard\nPhysis Shard is a framework for developing learning systems.\nIt defines basically two simple interfaces that allows easy addition of agents and problems.\nIn other words, to add agents or problems they only need to implement these interfaces.\nContents\nAgents:\n\nSpectrum-diverse Unified Neuron Evolution Architecture (SUNA)\nRandom agent (named Mysterious_Agent)\nDummy agent (the user define its output every iteration)\n\nEnvironments (i.e., Problems):\n\nMountain Car\nDouble Cart Pole (with and without velocities)\nFunction Approximation\nMultiplexer\nSingle Cart Pole\n\nInstall\nThis library depends on the zweifel library.\nYou can install it from the git address:\nhttps:\/\/github.com\/zweifel\/zweifel\nAfter installing the zweifel library, its full path need to be defined in the Physis Shard's Makefile.\nIn other words, change the following variable in the Makefile to point to zweifel library's correct installing location:\nPATH_TO_ZWEIFEL_LIBRARY=\/home\/user\/zweifel\n\nRun make afterwards:\nmake\n\nIt should produce two executables: rl and rl_live.\nTo run tests run:\n.\/rl\n\nAt the end of the test, the best solution's dna will be saved with the name dna_best_individual\nTo test this final solution run:\n.\/rl_live dna_best_individual\n\nChanging Environments\nEnvironment can be changed in main.cpp.\nFor example commenting out where the Reinforcement_Environment is defined and\nuncommenting the line with:\nReinforcement_Environment* env= new Double_Cart_Pole(random);\n\nIf the environment should be terminated when the maximum steps is reached\nuncomment the following in parameters.h:\n#define TERMINATE_IF_MAX_STEPS_REACHED\t\t\n\nDo not forget to comment it out when surpassing the maximum number of steps is\nnot a termination condition! For example, montain car does not need it while\ndouble cart pole does.\nChanging Parameters\nMany parameters of the environment as well as of the agent can be changed by modifying some definitions\nin parameters.h\nRunning Experiments\nTo run a trial until its maximum number of trials defined in main.cpp, run:\n.\/rl\n\nTo test the best individual, run:\n.\/rl_live dna_best_individual\n\nA series of trials can be run by using the script mean_curve.sh\nAdding Agents or Problems\nAn agent needs to implement the Reinforcement_Agent.h while a problem needs to implement the Reinforcement_Environment.h.\nThere are simple examples of agents and problems inside respectively the agents\/ and environments\/ directories.\nMost of the examples were built with the general reinforcement learning in mind, however they can be applied to supervised learning as well as unsupervised learning (e.g., consider the reward from the system as an error).\nLicense\nApache License Version 2.0\n","177":"Russian AI Cup (not only Russian!)\nArtificial intelligence programming contest. Official website: http:\/\/russianaicup.ru\nRussian AI Cup \u2014 intitiative of the company Mail.Ru Group within the IT-oriented competitions.In this championship participants compete in skills of creating an artificial intelligence on example of game stratefies. Organizers are Mail.Ru Group and Codeforces. Best participants will receive special prizes.\nRussian AI Cup \u2014 largest annual artificial intelligence programming contest in Russia, and third open competition for talented IT-specialists that is part of Mail.Ru Group strategy of forming and developing competitive Russian IT-industry on a global scale.\nRAIC 2018: CodeBall\nWe are pleased to welcome all the participants of the Russian AI Cup: CodeBall 2018 open beta!\nThe beta test will run until 21:00 UTC on December 23. Please note that at this time we can make a significant changes in the rules, scoring system and any other aspects of the championship. This week we will try to fix possible mistakes, optimize performance and make any other necessary improvements. The ratings will be reset after this week of beta.\nSome links on the website are not working. We will fix it within a few days.\nGood luck and have fun!\nUPDATE 19.12.18\nHere you can read about updates.\nUseful links\n\nAbout CodeBall. News, Notifications;\nQuick start instructions. Other sources will be published soon on this repository and here;\nPlay CodeBall in the browser.\n\nCommunity for English speakers (Discord)\nWe are HERE. Join us!\nCommunity for Russian speakers (Telegram)\nHERE. Join us!\nOfficial Contacts\nAlso, we've email cups@corp.mail.ru. If you have any private questions, you can ask us. We always check our inbox and reply to all.\n","178":"Russian AI Cup (not only Russian!)\nArtificial intelligence programming contest. Official website: http:\/\/russianaicup.ru\nRussian AI Cup \u2014 intitiative of the company Mail.Ru Group within the IT-oriented competitions.In this championship participants compete in skills of creating an artificial intelligence on example of game stratefies. Organizers are Mail.Ru Group and Codeforces. Best participants will receive special prizes.\nRussian AI Cup \u2014 largest annual artificial intelligence programming contest in Russia, and third open competition for talented IT-specialists that is part of Mail.Ru Group strategy of forming and developing competitive Russian IT-industry on a global scale.\nRAIC 2018: CodeBall\nWe are pleased to welcome all the participants of the Russian AI Cup: CodeBall 2018 open beta!\nThe beta test will run until 21:00 UTC on December 23. Please note that at this time we can make a significant changes in the rules, scoring system and any other aspects of the championship. This week we will try to fix possible mistakes, optimize performance and make any other necessary improvements. The ratings will be reset after this week of beta.\nSome links on the website are not working. We will fix it within a few days.\nGood luck and have fun!\nUPDATE 19.12.18\nHere you can read about updates.\nUseful links\n\nAbout CodeBall. News, Notifications;\nQuick start instructions. Other sources will be published soon on this repository and here;\nPlay CodeBall in the browser.\n\nCommunity for English speakers (Discord)\nWe are HERE. Join us!\nCommunity for Russian speakers (Telegram)\nHERE. Join us!\nOfficial Contacts\nAlso, we've email cups@corp.mail.ru. If you have any private questions, you can ask us. We always check our inbox and reply to all.\n","179":"Endless Sky\nExplore other star systems. Earn money by trading, carrying passengers, or completing missions. Use your earnings to buy a better ship or to upgrade the weapons and engines on your current one. Blow up pirates. Take sides in a civil war. Or leave human space behind and hope to find some friendly aliens whose culture is more civilized than your own...\n\nEndless Sky is a sandbox-style space exploration game similar to Elite, Escape Velocity, or Star Control. You start out as the captain of a tiny space ship and can choose what to do from there. The game includes a major plot line and many minor missions, but you can choose whether you want to play through the plot or strike out on your own as a merchant or bounty hunter or explorer.\nSee the player's manual for more information, or the home page for screenshots and the occasional blog post.\nInstalling the game\nOfficial releases of Endless Sky are available on Steam and as direct downloads from GitHub. A PPA is available for Ubuntu and for Debian. Other package managers may also include the game, though the specific version provided may not be up-to-date.\nSystem Requirements\nEndless Sky has very minimal system requirements, meaning most systems should be able to run the game. The most restrictive requirement is likely that your device must support at least OpenGL 3.\n\n\n\n\nMinimum\nRecommended\n\n\n\n\nRAM\n350 MB\n750 MB\n\n\nGraphics\nOpenGL 3.0\nOpenGL 3.3\n\n\nStorage Free\n120 MB\n300 MB\n\n\n\nBuilding from source\nWhile most development is done on Linux using the SCons build tool to compile the project, IDE-specific files are provided for XCode and Code::Blocks to simplify the installation on Mac OS and Windows. It is possible to use other IDEs or build systems to compile the game, but support is not provided.\nFor full installation instructions, consult the Build Instructions wiki page.\nContributing\nAs a free and open source game, Endless Sky is the product of many peoples' work. Contributions of artwork, storylines, and other writing are most in-demand, while there is a loosely defined development roadmap. Those who wish to contribute are encouraged to review the wiki, and to post in the discussion forum.\nLicensing\nEndless Sky is a free, open source game. The source code is available under the GPL v3 license, and all the artwork is either public domain or released under a variety of Creative Commons licenses. (To determine the copyright status of any of the artwork, consult the copyright file.)\n","180":"\n\n\n\nPioneer Space Simulator\n\nPioneer is a space adventure game set in the Milky Way galaxy at the turn of\nthe 31st century.\nThe game is open-ended, and you are free to explore the millions of star\nsystems in the game. You can land on planets, slingshot past gas giants, and\nburn yourself to a crisp flying between binary star systems. You can try your\nhand at piracy, make your fortune trading between systems, or do missions for\nthe various factions fighting for power, freedom or self-determination.\nFor more information, see:\nhttp:\/\/pioneerspacesim.net\/\nCommunity\nCome by #pioneer at irc.freenode.net and say hi to the team:\nhttp:\/\/pioneerspacesim.net\/irc\nBugs? Please log an issue:\nhttp:\/\/pioneerspacesim.net\/issues\nFollow Pioneer on Twitter:\nhttps:\/\/twitter.com\/pioneerspacesim\/\nPioneer wiki\nhttp:\/\/pioneerwiki.com\/wiki\/Pioneer_Wiki\nJoin the player's forum:\nhttp:\/\/spacesimcentral.com\/community\/pioneer\/\nJoin the development forum:\nhttp:\/\/pioneerspacesim.net\/forum\nManual\nManual can be found at:\nhttp:\/\/pioneerwiki.com\/wiki\/Manual\nBasic flight:\nhttps:\/\/pioneerwiki.com\/wiki\/Basic_flight\nKeyboard and mouse control is found at:\nhttp:\/\/pioneerwiki.com\/wiki\/Keyboard_and_mouse_controls\nFAQ\nFor frequently asked questions, please see\nhttp:\/\/pioneerwiki.com\/wiki\/FAQ\nBUG Reporting\nPlease see the section of the FAQ pertaining to bugs, crashs and reporting other issues: Bug Reporting FAQs.\nPlease do your best to fill out the issue template as completely as possible, especially when you're reporting a crash bug or a graphical issue. Having system information including graphics drivers and the method you used to install Pioneer helps immensely to diagnose and fix these kinds of issues.\nContributing\nIf you are hungry to contribute, more information can be found here:\nhttp:\/\/pioneerwiki.com\/wiki\/How_you_can_contribute\nIf you have a contribution you want to share, and want to learn how to make a\npull request, see:\nhttp:\/\/pioneerwiki.com\/wiki\/Using_git_and_GitHub\nLocalization\nLocalization for Pioneer is handled trough Transifex, and pulled to the source from there automatically. Because of this please don't make pull requests for translations. You can find the localization project here.\nYou need to register at transifex to be able to access the translations.\nIf you want a new language introduced, please request it on the Freenode IRC channel of Pioneer, or here by making an issue for it.\nGetting Pioneer\nLatest build is available at\nhttps:\/\/pioneerspacesim.net\/page\/download\/\nFor compiling from source, please see COMPILING.txt\nChangelog\nPlease see Changelog.txt\n","181":"Woozoolike\nA simple space exploration roguelike for 7DRL 2017.\nScreenshots\n\n\nBuilds (Windows)\n\n7DRL Version(English) - Download\nLatest Version(English) - Download\nLatest Version(Korean) - Download\n\nContact\n\nDiscord: https:\/\/discord.gg\/RhH3vyn\n\n","182":"OSP (OpenGL + C++ Experiments)\nThis repo hopes to become a space exploration game along the lines of Squad's Kerbal Space Program; as of today it's a place to experiment implementing different features in C++\/OpenGL.\nFeatures\n(Those marked with a \u2714\ufe0f are already implemented)\n(Those marked with a \u267b\ufe0f are ongoing or started)\n(The list may be expanded at any time)\n\u2714\ufe0f Keplerian Orbit Simulator for on-rails solar system\n\n\u2714\ufe0f Simulating elliptic orbits\n\u267b\ufe0f Simulating parabolic and hyperbolic orbits (Not really neccesary for now)\n\nVessel Building and Controlling\n\nVessel assembly from a list of parts\n\n\u267b\ufe0f Procedural parts\n\n\u267b\ufe0f Procedural Engines\n\n\u2714\ufe0f Nozzle simulator\nLiquid fueled engines\nSolid fueled engines\n\n\nProcedural structures (tanks, fuselage...)\n\n\nPre-made parts\n\n\nVessel controlling\n\n\u267b\ufe0f Navball\n\n\u2714\ufe0f Navball aligns with vessel\n\u267b\ufe0f Navball aligns to reference frames\n\n\nS.A.S\n\n\n\n\u267b\ufe0f Planetary Surfaces\n\nEither very big view distance or joining together multiple scaled cameras\nRendering of planets from far away\n\nSimple billboard shader\n\n\n\u267b\ufe0f Rendering of planets from the surface and near space\n\n\u267b\ufe0f Near space rendering\n\n\u267b\ufe0f Rocky bodies\n\n\u267b\ufe0f Cubesphere rendering\n\nVery complex cubespheres for asteroids and weird-shaped bodies\nNot spheric planets (see above, could be related)\nVery big scale shadow rendering (Per vertex?)\n\n\nLOD\n\nSeamless LOD transitions\nRemoving seams between quads\n\n\nAtmospheres\n\nAtmospheric shader\nSeamless transition from space to ground\nClouds \/ Cloud shadows\n\n\n\n\nOther bodies\n\nGas body rendering\n\nClouds and animations?\n\n\n\n\n\n\n\n\n\nIn-Vessel physics\n\nRigidbody physics simulator\nExtraction of acceleration and rotation from the simulation\nApplying forces to the vessel from outside (Gravitational gradient?)\nSeamless transition from in-vessel physics to the orbit simulator\n\nMaybe they don't need to be separated, but perfomance could suffer\n\n\nInteraction with the terrain system\n\n\u267b\ufe0f Newtonian Orbit Simulator for vessels\n\n\u2714\ufe0f Orbit propagation\n\u2714\ufe0f Orbit predictor (threaded)\n\u267b\ufe0f Maneouver planning\nImproving perfomance so many vessels can be simulated at high warp speeds\n\nModding\nThe engine is data-driven, so pretty much no hardcoded stuff.\nCustom behaviour may be implemented using a scripting language (lua, squirrel...), or even a bigger language like C# (Mono)\nScreenshots (may be outdated)\n\n\n\n","183":"Light-HLS: Fast, Accurate and Convenient\nLet's try to make HLS developemnt easier for everyone~ ^_^.\nLight-HLS is a light weight high-level synthesis (HLS) framework for academic exploration and evaluation, which can be called to perform various design space exploration (DSE) for FPGA-based HLS design. It covers the abilities of previous works, overcomes the existing limitations and brings more practical features. Light-HLS is modularized and portable so designers can use the components of Light-HLS to conduct various DSE procedures.  Light-HLS gets rid of RTL code generation so it will not suffer from the time-consuming synthesis of commercial HLS tools like VivadoHLS, which involves many detailed operations in both its frond-end and back-end, but can accurately estimate timing, resource and some other results of commercial tools for applications.\nIf Light-HLS helps for your works, please cite our paper in ICCAD 2019 ^_^:\nT. Liang, J. Zhao, L. Feng, S. Sinha and W. Zhang, \"Hi-ClockFlow: Multi-Clock Dataflow Automation and Throughput Optimization in High-Level Synthesis,\" 2019 IEEE\/ACM International Conference on Computer-Aided Design (ICCAD), Westminster, CO, USA, 2019, pp. 1-6. doi: 10.1109\/ICCAD45719.2019.8942136\n\nA well-organzied Wiki can be find here. Since we are still developing this project and there could be some bugs and issues, if you have any problems, PLEASE feel free to let us know: ( tliang@connect.ust.hk ), for which we will sincerely appreciate ^_^. We strongly recommand to you send us an email so we can add you into our maillist for the latest information of Light-HLS, because Light-HLS is a young tool and continuously updated to add features and fix bugs. This is a young project and if you want to join us, we are happy to make it a better one togather! \\^_^\/ Here are some known issues raised by our users, which we are handling one by one.\n\nLight-HLS Frond-End\nThe goal of Light-HLS frond-end is to generate IR code close enough to those generated via commercial tools, like VivadoHLS, for DSE purpose. In the front-end of Light-HLS, initial IR codes generated via Clang will be processed by HLS optimization passes consisted of three different levels: (a) At instruction level, Light-HLS modifies, removes or reorders the instructions, e.g. reducing bitwidth,  removing redundant instruction  and reordering computation. (b) At loop\/function level, functions will be instantiated and loops may be extracted into sub-functions. (c) As for memory access level, redundant load\/store instructions will be removed based on dependency analysis.\nLight-HLS Back-End\nThe back-end of Light-HLS is developed to schedule and bind for the optimized IR codes, so it can predict the resultant performance and resource cost accurately based on the given settings. The IR instructions can be automatically characterized by Light-HLS and a corresponding library, which records the timing and resource of different types of operations, will be generated. For scheduling, based on the generated library, Light-HLS maps most operations to corresponding cycles based on as-soon-as-possible (ASAP) strategy. For some pipelined loops, the constraints of the port number of the BRAMs and loop-carried dependencies are considered. Moreover, some operations might be scheduled as late as possible (ALAP) to lower the II. \tAs for resource binding, Light-HLS accumulates the resource cost by each operation and the chaining of operations is considered. The reusing of hardware resource is an important feature in HLS and Light-HLS reuses resources based on more detailed rules, e.g. the source and type of input.\nLight-HLS Application Scenariors\nLet's first see what we can do with Light-HLS in the research about HLS.\n\n\nHLS designs can be set with various configurations, leading to different results of performance and resource. To find the optmial solution, designers can determine the configuration and call Light-HLS to predict the result in tens milliseconds, which will be close to the result in VivadoHLS. An example is Hi-ClockFlow, a tool which searches for the configuration of clock settings and HLS directives for the multi-clock dataflow.\n\n\nHLS designs are sensitive to the source codes, some of which are friendly to FPGA while the others are not. If researchers want to analyze and optimize the design at source code level, Light-HLS have accomplished the back-tracing from back-end, to front-end, to source code, so researchers can find out which part of source code have some interesting behaviors causing problems. In the example Hi-ClockFlow, Light-HLS helps to partition the source code and map the performance and resource to the different parts of the source code.\n\n\nIn the front-end of HLS, source code will be processed by a series of LLVM Passes for analysis and optimization. However, for most of researchers, even if they come up with an idea for the front-end processing, they can hardly estimate the exact outcome of the solution if it can be applied to the commercial HLS tools. Currently, Light-HLS can generate IR code similar to the one generated by VivadoHLS and provide accurate scheduling and resource binding in back-end. Therefore, researchers might implement a Pass, plug it into the front-end of Light-HLS (Yes, just plug ^_^), and evaluate it with the back-end of Light-HLS to see the effect.\n\n\nIn the back-end of HLS, the IR instructions\/blocks\/loops\/functions are scheduled and binded to specific hardware resource on FPGA. Based on the IR code similar to the one generated by commercial tools, how to properly schedule the source code and bind the resource can be tested and analyzed with Light-HLS. Currently, Light-HLS can provide the estimated performance and resource cost close to those from VivadoHLS 2018.2. (We will catch up the version of 2019.2 recently.) Light-HLS can generate the library of the timing and resource cost of all the IR instructions, e.g. add, fmul, MAC, fptoui and etc, for a specified devive, like Zedboard. Researchers can change the original scheme of Light-HLS's scheduling and binding to see the effect.\n\n\nCategory:\nInstallation of Light-HLS\nUsage of Light-HLS\nImplementation of Light-HLS and Further development\nNotes for You to Create Your Own HLS Tools with LLVM\nGood Good Study Day Day Up (^o^)\/~\n","184":"Molpher-lib: Introduction\nThis C++\/Python library is a chemical space exploration software. It is based on the Molpher program which introduced a chemical space exploration method called molecular morphing. The original Molpher method uses stochastic optimization to traverse chemical space between two existing molecules. The main promise of this algorithm is that a virtual library enriched in compounds with improved biological activity could be generated in this way.\nThe purpose of Molpher-lib is to bring molecular morphing closer to the cheminformatics community, but also offer new features that go beyond the capabilities of the original Molpher program. Molpher-lib makes it possible to roam the chemical universe freely and with little constraints on the inputs. For example, we could just use a carbon atom as a starting point and have Molpher-lib autonomously evolve it into a complete molecular structure. To ensure that the generated molecules have required properties, Molpher-lib also helps with implementation of custom rules and constraints. If you want to know more about Molpher-lib and its usage, make sure to check out some examples on the website. We also have some Jupyter notebooks with examples\nthat you can explore.\nIf you would like to participate in the development or just check out the current features of the library, there is extensive documentation which can help you. A big part of the documentation is dedicated to a detailed tutorial that should introduce the philosophy of Molpher-lib in more detail and give you a good idea of what it is currently capable of.\nThe library is actively developed and many new features are planned to be added. The long-term goal is to make Molpher-lib a universal and easy-to-use de novo drug design framework. Ideas, comments and feature requests are more than welcome and can be submitted to the issue tracker. You can also subscribe to the RSS feed of the dev branch for development updates. If you want to know what is new in the current version, you can look at the changelog.\nInstallation\nSupported platforms:\n\nLinux 64-bit\n\nAt the moment, the library binaries are only compiled for 64-bit Linux systems. However, development for other platforms is also planned. If you manage to compile the library on a different platform, consider making a pull request or comment on the issue tracker. Any help is much appreciated.\nInstallation with Anaconda\nMolpher-lib is distributed as a conda package. At the moment, this is the preferred way to install and use the library. All you need to do is get the full Anaconda distribution or its lightweight variant, Miniconda. It is essentially a Python distribution, package manager and virtual environment in one and makes setting up a development environment for any project very easy. After installing Anaconda\/Miniconda you can run the following in the Linux terminal:\nconda install -c rdkit -c lich molpher-lib\nThis will automatically download the latest version of the library and install everything to the currently active environment (for more information on environments and the conda command see Conda Test Drive). The library depends on the popular cheminformatics toolkit RDKit so do not forget to add the rdkit channel.\nIf you are interested in the development snapshots of the library\n(most up to date code, but can contain bugs)\n, you can use the dev channel instead:\nconda install -c rdkit -c lich\/label\/dev molpher-lib\nAfter that the library should import in your environment and you should be able to successfully run the integrated unit tests:\nfrom molpher.tests import run\n\nrun()\nYou can also check out the Jupyter notebooks with examples from the documentation.\nCompiling from Source\nCompiling and installing from source is a little bit more elaborate. This process is described in detail in the documentation, but in the simplest case the following should work:\n# get dependencies\nsudo apt-get install git build-essential python3-dev python3-numpy cmake python3-setuptools\n\n# clone the repo\ngit clone https:\/\/github.com\/lich-uct\/molpher-lib.git\ngit checkout dev # or the branch\/tag\/commit you want\nREPOSITORY_ROOT=`pwd`\/molpher-lib\n\n# this might take a while, but you if you are lucky, \n# cmake might be able to find dependencies \n# if you already have them somewhere on your system\n# so you can skip this step if you have TBB, Boost and RDKit\n# installed at standard locations\ncd ${REPOSITORY_ROOT}\/deps\n.\/build_deps.sh --all\n\n# finally, build the library itself\ncd ${REPOSITORY_ROOT}\nmkdir cmake-build\ncd cmake-build\ncmake .. -DCMAKE_BUILD_TYPE=Debug -DPYTHON_EXECUTABLE=python3\nmake molpher_install_python\nAfter setting the appropriate variables:\nexport CMAKE_INSTALL_PREFIX=\"${REPOSITORY_ROOT}\/dist\"\nexport DEPS_DIR=${CMAKE_INSTALL_PREFIX}\/..\/deps\nexport PYTHONPATH=${DEPS_DIR}\/rdkit\/:${CMAKE_INSTALL_PREFIX}\/lib\/python3.5\/site-packages\nexport LD_LIBRARY_PATH=${DEPS_DIR}\/tbb\/lib\/intel64\/gcc4.7:${DEPS_DIR}\/rdkit\/lib\/:${DEPS_DIR}\/boost\/stage\/lib:${CMAKE_INSTALL_PREFIX}\/lib\nyou should be good to go:\npython3\nfrom molpher.tests import run\n\nrun()\nThis will run the integrated unit tests. They should all pass without problems.\nIf you want to explore some example code from the documentations, there are\na few Jupyter notebooks located under doc\/notebooks. You can create\nthe needed conda environment\nand launch your Jupyter server as follows:\ncd ${REPOSITORY_ROOT}\nconda env create -f \"environment.yml\"\n. source_2_activate\npython setup.py build_ext --inplace\ncd doc\/notebooks\/\njupyter-notebook\nNote that you will need to have the library already compiled and installed in the standard\n${REPOSITORY_ROOT}\/dist directory.\nThis installation process has been tested on common Debian-based systems so experience on other Linux flavors may differ. If you run into problems, report them to the issue tracker and hopefully someone will be able to help.\n","185":"#SYNCHROTRACE (Now deprecated. Please use https:\/\/github.com\/dpac-vlsi\/SynchroTrace-gem5 )\nThere are two tools which together form the prototype SynchroTrace simulation flow built into Gem5.\n1) Sigil - Multi-threaded Trace Capture Tool\n2) Replay - Event-Trace Replay Framework\nThis code base includes (2) Replay.\nCurrently, the Sigil version required to generate traces is provided separately from the same github user \"dpac-vlsi\".\nThe logical steps to using this simulation environment for design space exploration or CMP simulation is as follows:\n1)\na) Generate Multi-threaded Event Traces for the program binary you are testing (See Sigil documentation for further information):\n-Use the Sigil wrapping script (runsigil_and_gz_newbranch.py) with necessary options on your binary\nOR\nb) Use previously generated or sample traces\n2) Compile SynchroTrace (For a list of dependencies, please look in the Additional Notes section below)\n3) Run SynchroTrace with necessary options on the generated traces\n#####Installing SynchroTrace and Running the 8 Thread FFT Example\n\n\nCheck Necessary Dependencies:\ngcc-4.4.7\ngmp-5.1.1\nmpc-1.0\nmpfr-3.1.2\nswig-2.0.1\npython-2.7.6\nscons-2.3.0\nzlib-dev or zlib1g-dev\nm4\n\nPlease note:\na) Runtime problems have been encountered when using later versions of swig and gcc.\n\nb) Please ensure \/usr\/bin\/gcc-4.4 is symbolically linked to your defaultgcc (\/usr\/bin\/gcc)\nc) gmp, mpc, and mpfr are packaged with gcc when using package installer such as apt.\nd) swig-2.0.1 can be found at http:\/\/sourceforge.net\/projects\/swig\/files\/swig\/swig-2.0.1\/\ne) Currently we do not provide any means to automatically install the missing packages.\n\n\nBuild SynchroTrace\n\n\na) If not done so, clone the SynchroTrace repo from GitHub:\n$ git clone https:\/\/github.com\/dpac-vlsi\/SynchroTrace\nb) Go to the base SynchroTrace directory\nc) Run the following command (Note that the number of jobs refers to the number of cores available for compilation):\n     scons build\/X86_MESI_CMP_directory\/gem5.opt --jobs=6\nAt this point, the gem5 executable should be built with integrated Trace Replay in the location specified in the command above.\ngem5 is usually run with a configuration script that hooks up the various architecture models packaged with the gem5 framework.\nWe have written a SynchroTrace configuration script to which arguments can be passed to configure the desired system.\nThis script can be found at <SYNCHROTRACE_FOLDER>\/configs\/synchrotrace\/synchrotrace.py\nThe run_synchrotrace_fft.pl run script can be used to run the FFT example simply as follows:\n$ .\/run_synchrotrace_fft.pl\nThis script can be modified and emulated to run your own configuration.\nA different trace location by changing the $eventDir variable.\nThis design being simulated is set by changing the arguments provided in the $synchrotracecmd variable.\nFor a list of valid arguments, run the following from the main SynchroTrace folder:\n$ .\/build\/X86_MESI_CMP_directory\/gem5.opt .\/configs\/synchrotrace\/synchrotrace.py --help\nd) Once the run is completed, simulated metrics can be found in m5out\/stats.txt and m5out\/ruby.stats\n####################################################################################################################################\n#####Additional Notes:\n\n\nSample Sigil Traces are located in $BASESYNCHROTRACEDIR\/sample_sigil_traces\n\n\nSynchroTrace configurations are located in $BASESYNCHROTRACEDIR\/configs\/synchrotrace\/synchrotrace.py and $BASESYNCHROTRACEDIR\/configs\/ruby\/Ruby.py\n\n\nThe run_synchrotrace_fft.pl run script has a section for debug flags. The following is a list of the available debug flags used by SynchroTrace with brief descriptions.\n-DebugFlag('mutexLogger') - Prints order of threads obtaining mutex lock\n-DebugFlag('printEvent') - Prints EventID# for specific thread before\/after event started\/completed. This debug flag makes the simulation time very slow.\n-DebugFlag('printEventFull') - Prints EventIDs for Threads, Threads on what Cores every 50k cycles\n-DebugFlag('cacheMiss') - Prints out cache misses as they happen and address\n-DebugFlag('memoryInBarrier') - prints memory reads, writes, read bytes, write bytes every barrier\n-DebugFlag('flitsInBarrier') - prints flits generated every barrier\n-DebugFlag('l1MissesInBarrier') - prints l1 misses per thread every barrier\n-DebugFlag('latencyInBarrier') - prints 3 lines. # packets in barrier, Accumulated queueing delay in barrier, Accumulated network latency in Barrier.\n-DebugFlag('powerStatsInBarrier') - prints the total router power specifically for that barrier, i.e. not a rolling average.\n-DebugFlag('roi') - Prints out the cycle when we reach the parallel region in Debate. Prints out when the threads all join up.\n-DebugFlag('netMessages') - Prints the network packet messages out at 10k cycle buckets.\n-DebugFlag('amTrace') - Original default debug flag.\n\n\nAn example of this command with a debug flag is as follows:\n\n\n.\/build\/X86_MESI_CMP_directory\/gem5.opt --debug-flags=printEventFull .\/configs\/synchrotrace\/synchrotrace.py --garnet-network=fixed --topology=Mesh --mesh-rows=8 --eventDir=$eventDir --outputDir=$outputDir --num-cpus=8 --num_threads=8 --num-dirs=8 --num-l2caches=8 --l1d_size=8kB --l1d_assoc=16 --l1i_size=8kB --l1i_assoc=2 --l2_size=128kB --l2_assoc=4 --cpi_iops=1 --cpi_flops=2 --bandwidth_factor=4 --l1_latency=3 --masterFreq=1 2> fft.err\";\nwhere the $eventDir points to the directory of the traces and $outputDir points to the desired output directory path.\n","186":"Finite Galaxy\nFinite Galaxy is a free and open source space exploration game; the repository containing all files is located at https:\/\/github.com\/finite-galaxy\/finite-galaxy\/\nIt is derived from Endless Sky, a game created by Michael Zahniser, which is located at https:\/\/github.com\/endless-sky\/endless-sky\/\nBoth games can be installed alongside and played independently of each other. Although derived from the same source code and basically using the same content, Finite Galaxy and Endless Sky have diverged and are no longer compatible. If you transpose a save game from one to the other, you are likely to encounter hundreds of errors.\nTable of contents\n\nInstallation\n\nGNU\/Linux\nApple\/Mac OS X\nMicrosoft Windows\n\n\nIntroduction\nChanges\n\nMajor changes\nMinor changes\nNot yet implemented ideas\n\n\nContributing\n\nHelp wanted\nPosting issues\nPosting pull requests\n\n\nReveal entire map\n\nInstallation\nGNU Linux\nOpen your terminal and enter:\n\nto install dependencies:\n\non ArchLinux: pacman -S --needed git gcc scons sdl2 libpng libjpeg-turbo mesa glew openal libmad pango ttf-linux-libertine\non Debian\/Ubuntu: sudo apt-get install git g++ scons libsdl2-dev libpng-dev libjpeg-dev libgl1-mesa-dev libglew-dev libopenal-dev libmad0-dev libpango fonts-linuxlibertine\non Fedora\/RHEL\/CentOS: sudo dnf install git gcc-c++ scons SDL2-devel libpng-devel libjpeg-turbo-devel mesa-libGL-devel glew-devel openal-soft-devel libmad-devel pango linux-libertine-fonts (replace dnf with yum on some versions).\n\n\ngit clone https:\/\/github.com\/finite-galaxy\/finite-galaxy.git to get a local copy of the repository.\ncd finite-galaxy\/ to open the directory.\ngit pull to update the game.\nscons to compile the game.\n.\/finite-galaxy to run the game.\n\nFor more help, consult the man page (the finite-galaxy.6 file).\n(return to top)\nApple Mac OS X\nIf you have trouble compiling or encounter errors, please post here.\nTo build Finite Galaxy, you will first need to download Xcode from the App Store.\nNext, install Homebrew (from http:\/\/brew.sh).\nOnce Homebrew is installed, use it to install the libraries you will need:\nbrew install libpng\nbrew install libjpeg-turbo\nbrew install libmad\nbrew install sdl2\nbrew install pango\n\nIf the versions of those libraries are different from the ones that the Xcode project is set up for, you will need to modify the file paths in the \u201cFrameworks\u201d section in Xcode.\nIt is possible that you will also need to modify the \u201cHeader Search Paths\u201d and \u201cLibrary Search Paths\u201d in \u201cBuild Settings\u201d to point to wherever Homebrew installed those libraries.\nLibrary paths\nTo create a Mac OS X binary that will work on systems other than your own, you may also need to use install_name_tool to modify the libraries so that their location is relative to the @rpath.\nsudo install_name_tool -id \"@rpath\/libpng16.16.dylib\" \/usr\/local\/lib\/libpng16.16.dylib\nsudo install_name_tool -id \"@rpath\/libmad.0.2.1.dylib\" \/usr\/local\/lib\/libmad.0.2.1.dylib\nsudo install_name_tool -id \"@rpath\/libturbojpeg.0.dylib\" \/usr\/local\/opt\/libjpeg-turbo\/lib\/libturbojpeg.0.dylib\nsudo install_name_tool -id \"@rpath\/libSDL2-2.0.0.dylib\" \/usr\/local\/lib\/libSDL2-2.0.0.dylib\nsudo install_name_tool -id \"@rpath\/pango-1.44.7.dylib\" \/usr\/local\/lib\/pango-1.44.7.dylib\n\n(return to top)\nMicrosoft Windows\nIf you have trouble compiling or encounter errors, please post here.\n\nAcquire the files with git clone https:\/\/github.com\/finite-galaxy\/finite-galaxy.git or click \u201cDownload ZIP\u201d and extract it.\nIf you don't have it already, open finite-galaxy\/fonts\/LinLibertine_DRah.ttf and install the font (or copy it to the appropiate location).\nFor building the game, see https:\/\/github.com\/endless-sky\/endless-sky\/wiki\/BuildInstructions#Windows\n\n(return to top)\nIntroduction\nWhy did I start this project? Why not contribute to Endless Sky instead?\n\nAlthough I like Endless Sky as a whole, it also contains things I don't like. Conversation scenes, news and portraits, tribute, and plundering of installed outfits are just a few examples.\nThe original creator, Michael Zahniser, seemed to disappear and pace of development appeared to slow down: there were only nine commits in September 2018 and zero in October. (I started Finite Galaxy on October 18.)\nNumerous pull requests over there have been open for over a year, reviews are haphazard, there are many lengthy discussions on unimportant things, while useful proposals were often ignored.\nThe direction and vision is not always clear.\nSupport for plug-ins is rather limited.\n\nIn short, I consider it a better use of time to work on a project where I can incorporate most of my ideas, where I can remove things I dislike, and where I can contribute whenever I like, without having to wait weeks or months for a review or wasting my time on something that won't be included.\nFinite Galaxy is very much a work in progress. Nevertheless, it can be compiled and played without errors. Feel free to try it out yourself!\n(return to top)\nChanges\nMajor changes\n\nHyperjump fuel is based on your ship's effective mass (including cargo and carried ships)\nHyperjump fuel is no longer free, its price depends on the planet (when landing) or the government (when hailing ships in space)\nShips continuously consume energy, based upon the number of bunks, to represent life-support\nShip categories are based on total mass\n\nship mass = hull mass + outfit space + cargo space\n\n\nIntroduced core space, reserved for energy generators, shields and hull systems, and hyperdrives\n\noutfit space = core space + engine space + weapon space\n\n\nInstalled outfits can no longer be plundered by default; outfits in cargo still can\nMinimum depreciation value raised to 50%, time lowered to one year\nWeapon projectile damage is a random number between damage and damage + random damage\nGuns fire in parallel by default, i.e. no harmonized angle convergence.\nShip info display shows more stats\nRedistributed most human ships and many outfits to have more regional specialization\nRemoved tribute from planets (relevant code is still present for plug-ins)\nRemoved news and portraits (relevant code is still present for plug-ins)\nDistances from planets to the sytem's centre are trebled; as a result space feels larger, thrusters are more desirable, and players won't always land immediately in the middle of a space fight\nNon-missile weapons have their weapon range increased by about a third\nExploding ships are significantly more dangerous\nAdd support for Unicode and different writing directions\n\n(return to top)\nMinor changes\nSee changelog.txt, ship_overview.txt, and https:\/\/github.com\/finite-galaxy\/finite-galaxy\/commits\/master\n(return to top)\nNot yet implemented ideas\n\nAdd quotation mark preference\nAdd \u201cLicences\u201d tab in player info panel\nAdd \u201cTribute\u201d tab in player info panel\nAdd \u201cManufacturer\u201d to ships\nAdd functionality to deposit credits at the bank for a fixed time (e.g. one year), receiving either the sum plus interest when it expires, or the sum minus a penalty when you claim it beforehand\nAdd planet landing fees support\nAllow friendly fire\nAllow sorting available jobs (by e.g. cargo size, distance, name, payment, etc.)\nAllow sorting outfits by cost, mass, name\nAllow sorting ships by cost, mass, name, outfit space, shields, etc.\nDisplay flagship speed by default and display target's speed with tactical scanner\nDe-hardcode jump radius\nIncrease jump radius if you have multiple jump drives installed, perhaps 100*(jump drive)^0.5\nLimit the commodities for sale on specific planets\nMake ship explosion \u2018weapon\u2019 automatically proportional to mass (base, empty, or total mass)\nSeparate fleet overview column in outfitter and shipyard from ship info display\nSeparate slots for guns and missile launchers\nShips entering a system from hyperspace should be positioned near the system's centre, instead of near the first inhabited planet\nhttps:\/\/github.com\/endless-sky\/endless-sky\/wiki\/DevelopmentRoadmap\n\n(return to top)\nContributing\nContributions are welcome; anyone can contribute; feel free to open issues or make pull requests.\nHelp wanted\nCode:\n\nDe-hardcode Drone\/Fighter classes (currently a boolean is used), to allow for multiple, customizable fighter\/bay types (e.g. small, medium, large).\nDe-hardcode hardpoint slots (currently a boolean is used), to allow for multiple, customizable hardpoint types (e.g. gun slots, missile bays, turret mounts).\nImplementing the ideas listed above.\nOther new mechanics that make the game more enjoyable.\nUpdate code to C++17.\n\nArt:\nBecause my Blender skills are non-existent, I could use help from people who are capable and willing to:\n\nMake new outfit sprites and turret hardpoints (e.g. six-gun blaster turret).\nModify existing ship sprites and thumbnails.\nProduce ship thumbnails for ships that only have sprites.\nCreate new ships.\nSee open issues.\n\nMiscellaneous:\nUnfortunately I'm unable to test things on platforms other than my own (Fedora Linux). BSD, MacOS X, and Windows users could help by trying compiling and running the game, and if necessary, correct the appropiate files accordingly.\n(return to top)\nPosting issues\nThe issues page on GitHub is for tracking bugs and for art, content, and feature requests. When posting a new issue, please:\n\nBe polite and always assume good faith.\nCheck to make sure it's not a duplicate of an existing issue.\nCreate a separate \u201cissue\u201d for each bug, problem, question, or request.\n\nIf requesting a new feature, first ask yourself: will this make the game more fun or interesting? Remember that this is a game, not a simulator. Changes will not be made purely for the sake of realism, especially if they introduce needless complexity or aggravation.\nIf you believe your issue has been resolved, you can close the issue yourself.\n(return to top)\nPosting pull requests\nIf you are posting a pull request, please:\n\nDo not combine multiple unrelated changes into a single pull.\nCheck the diff and make sure the pull request does not contain unintended changes.\nIf proposing a major pull request, start by posting an issue and discussing the best way to implement it. Often the first strategy that occurs to you will not be the cleanest or most effective way to implement a new feature.\ncode\/:\n\nfollow the coding standard.\nC++14\ndo not use tabs; use two spaces instead\nmake numbers with many digits easier to read for humans by inserting '\n\n(decimal numbers) at intervals of three digits if there are more than four in a row\n(hexadecimal numbers) at intervals of four digits if there are more than six in a row\n\n\nuse Oxford English\n\n\ndata\/:\n\ndo not use tabs; use two spaces instead\nuse Oxford spelling (the variant of English used by many scientific journals and international organizations such as the United Nations), instead of American, British, Canadian, or other national varieties.\nno diacritics in English:\n\n\u00e1, \u00e0, \u00e2 \u2192 a; same for other vowels\n\u00e5 \u2192 aa\n\u00e6, \u00e4 \u2192 ae\n\u0153, \u00f8, \u00f6 \u2192 oe\n\u00fc \u2192 ue\n\u0130\/\u0131 \u2192 I\/i\n\u00e7 \u2192 c\n\u010d, \u0107, \u010b \u2192 ch\n\u0161, \u015b, \u015f \u2192 sh\n\u00f1 \u2192 nh\n\u00df \u2192 ss\n\n\ntext strings (conversations, descriptions, mission dialogues, tooltips, etc.):\n\navoid abbreviations (e.g., i.e., etc.); contractions (isn't) are fine\nuse an Oxford comma when giving more than two items (e.g. one, two, and three; not one, two and three)\nuse U+2013 \u2013 en-dash for number ranges (e.g. 10\u201312) and for parenthetical expressions \u2013 like this \u2013 instead of parentheses, em-dashes, or hyphens\nuse U+2026 \u2026 horizontal ellipsis instead of three full stops\nuse U+202F \u202f narrow non-breaking space as a thousands separator for numbers with five or more digits (e.g. 12\u202f345)\nuse U+2212 \u2212 minus sign for negative numbers, subtractions, and deductions\n\n\nuse the \" quote for direct speech and ' apostrophe within direct speech; the source code replaces these with proper \u201cprimary\u201d and \u2018secondary\u2019 opening and closing quotation marks; surround such strings with ` backticks\n\n\nrepeatedly check and double check any new or changed strings to avoid unnecessary typos; e.g. mind the difference between it's (cf. he's, she's) and its (cf. his, her).\n\n\nimages\/:\n\nfile names are lower case and use underscores instead of spaces\nadd both normal and @2x versions\n\nfor ships, also create thumbnails\nfor turrets, also create hardpoints\n\n\ninsert yourself in the copyright.txt file\ninclude all assets (Blender, GIMP, other files) in the opening post\n\n\nsounds\/:\n\nfile names are lower case and use underscores instead of spaces\ninsert yourself in the copyright.txt file\n\n\n\n(return to top)\nReveal entire map\nPart of the fun of the game is travelling around and exploring. However, if you don't have time for that and simply want to reveal everything in the entire galaxy, then open your save game, find # What you know: and insert the following lines directly afterwards:\nvisited \"1 Axis\"\nvisited \"10 Pole\"\nvisited \"11 Autumn Above\"\nvisited \"11 Spring Below\"\nvisited \"12 Autumn Above\"\nvisited \"14 Pole\"\nvisited \"14 Summer Above\"\nvisited \"14 Winter Below\"\nvisited \"16 Autumn Rising\"\nvisited \"3 Axis\"\nvisited \"3 Pole\"\nvisited \"3 Spring Rising\"\nvisited \"4 Axis\"\nvisited \"4 Spring Rising\"\nvisited \"4 Summer Rising\"\nvisited \"4 Winter Rising\"\nvisited \"5 Axis\"\nvisited \"5 Spring Below\"\nvisited \"5 Summer Above\"\nvisited \"5 Winter Above\"\nvisited \"7 Autumn Rising\"\nvisited \"8 Winter Below\"\nvisited \"9 Spring Above\"\nvisited \"Ablodab\"\nvisited \"Ablub\"\nvisited \"Acamar\"\nvisited \"Achernar\"\nvisited \"Acrux\"\nvisited \"Adhara\"\nvisited \"Aescolanus\"\nvisited \"Al Dhanab\"\nvisited \"Albaldah\"\nvisited \"Albireo\"\nvisited \"Alcyone\"\nvisited \"Aldebaran\"\nvisited \"Alderamin\"\nvisited \"Aldhibain\"\nvisited \"Algedi\"\nvisited \"Algenib\"\nvisited \"Algenubi\"\nvisited \"Algieba\"\nvisited \"Algol\"\nvisited \"Algorel\"\nvisited \"Alheka\"\nvisited \"Alhena\"\nvisited \"Alioth\"\nvisited \"Alkaid\"\nvisited \"Almaaz\"\nvisited \"Almach\"\nvisited \"Alnair\"\nvisited \"Alnasl\"\nvisited \"Alnilam\"\nvisited \"Alnitak\"\nvisited \"Alniyat\"\nvisited \"Alpha Arae\"\nvisited \"Alpha Centauri\"\nvisited \"Alpha Hydri\"\nvisited \"Alphard\"\nvisited \"Alphecca\"\nvisited \"Alpheratz\"\nvisited \"Altair\"\nvisited \"Aludra\"\nvisited \"Ancient Hope\"\nvisited \"Ankaa\"\nvisited \"Answer\"\nvisited \"Antares\"\nvisited \"Antevorta\"\nvisited \"Ap'arak\"\nvisited \"Arcturus\"\nvisited \"Arculus\"\nvisited \"Arneb\"\nvisited \"Ascella\"\nvisited \"Asikafarnut\"\nvisited \"Aspidiske\"\nvisited \"Atria\"\nvisited \"Avior\"\nvisited \"Aya'k'k\"\nvisited \"Beginning\"\nvisited \"Bellatrix\"\nvisited \"Belonging\"\nvisited \"Belug\"\nvisited \"Belugt\"\nvisited \"Beta Lupi\"\nvisited \"Betelgeuse\"\nvisited \"Bloptab\"\nvisited \"Blubipad\"\nvisited \"Blugtad\"\nvisited \"Boral\"\nvisited \"Bore Fah\"\nvisited \"Bote Asu\"\nvisited \"Bright Void\"\nvisited \"Broken Bowl\"\nvisited \"Caeculus\"\nvisited \"Canopus\"\nvisited \"Capella\"\nvisited \"Caph\"\nvisited \"Cardax\"\nvisited \"Cardea\"\nvisited \"Castor\"\nvisited \"Cebalrai\"\nvisited \"Celeborim\"\nvisited \"Chalawan\"\nvisited \"Charm\"\nvisited \"Chikatip\"\nvisited \"Chimitarp\"\nvisited \"Chirr'ay'akai\"\nvisited \"Chornifath\"\nvisited \"Chy'chra\"\nvisited \"Cinxia\"\nvisited \"Coluber\"\nvisited \"Companion\"\nvisited \"Convector\"\nvisited \"Cor Caroli\"\nvisited \"Da Ent\"\nvisited \"Da Lest\"\nvisited \"Dabih\"\nvisited \"Danoa\"\nvisited \"Dark Hills\"\nvisited \"Debrugt\"\nvisited \"Delta Capricorni\"\nvisited \"Delta Sagittarii\"\nvisited \"Delta Velorum\"\nvisited \"Deneb\"\nvisited \"Denebola\"\nvisited \"Diphda\"\nvisited \"Dokdobaru\"\nvisited \"Dschubba\"\nvisited \"Dubhe\"\nvisited \"Due Yoot\"\nvisited \"Durax\"\nvisited \"Eber\"\nvisited \"Eblumab\"\nvisited \"Edusa\"\nvisited \"Ehma Ti\"\nvisited \"Ek'kek'ru\"\nvisited \"Ekuarik\"\nvisited \"Elnath\"\nvisited \"Eltanin\"\nvisited \"Eneremprukt\"\nvisited \"Enif\"\nvisited \"Es'sprak'ai\"\nvisited \"Eshkoshtar\"\nvisited \"Eteron\"\nvisited \"Fah Root\"\nvisited \"Fah Soom\"\nvisited \"Fala\"\nvisited \"Fallen Leaf\"\nvisited \"Far Horizon\"\nvisited \"Farbutero\"\nvisited \"Farinus\"\nvisited \"Faronektu\"\nvisited \"Fasitopfar\"\nvisited \"Fell Omen\"\nvisited \"Feroteri\"\nvisited \"Ferukistek\"\nvisited \"Fingol\"\nvisited \"Flugbu\"\nvisited \"Fomalhaut\"\nvisited \"Fornarep\"\nvisited \"Four Pillars\"\nvisited \"Furmeliki\"\nvisited \"Gacrux\"\nvisited \"Gamma Cassiopeiae\"\nvisited \"Gamma Corvi\"\nvisited \"Gienah\"\nvisited \"Girtab\"\nvisited \"Glubatub\"\nvisited \"Gomeisa\"\nvisited \"Good Omen\"\nvisited \"Gorvi\"\nvisited \"Graffias\"\nvisited \"Gupta\"\nvisited \"Hadar\"\nvisited \"Hamal\"\nvisited \"Han\"\nvisited \"Hassaleh\"\nvisited \"Hatysa\"\nvisited \"Heia Due\"\nvisited \"Hesselpost\"\nvisited \"Hevru Hai\"\nvisited \"Hi Yahr\"\nvisited \"Hintar\"\nvisited \"Holeb\"\nvisited \"Homeward\"\nvisited \"Host\"\nvisited \"Hunter\"\nvisited \"Ik'kara'ka\"\nvisited \"Ildaria\"\nvisited \"Imo Dep\"\nvisited \"Insitor\"\nvisited \"Io Lowe\"\nvisited \"Io Mann\"\nvisited \"Ipsing\"\nvisited \"Iyech'yek\"\nvisited \"Izar\"\nvisited \"Ka'ch'chrai\"\nvisited \"Ka'pru\"\nvisited \"Kaliptari\"\nvisited \"Kappa Centauri\"\nvisited \"Kashikt\"\nvisited \"Kasikfar\"\nvisited \"Kaus Australis\"\nvisited \"Kaus Borealis\"\nvisited \"Ki War Ek\"\nvisited \"Kiro'ku\"\nvisited \"Kiru'kichi\"\nvisited \"Kochab\"\nvisited \"Kor Ak'Mari\"\nvisited \"Kor En'lakfar\"\nvisited \"Kor Fel'tar\"\nvisited \"Kor Men\"\nvisited \"Kor Nor'peli\"\nvisited \"Kor Tar'bei\"\nvisited \"Kor Zena'i\"\nvisited \"Kornephoros\"\nvisited \"Korsmanath\"\nvisited \"Kraz\"\nvisited \"Kugel\"\nvisited \"Kursa\"\nvisited \"Last Word\"\nvisited \"Lesath\"\nvisited \"Levana\"\nvisited \"Limen\"\nvisited \"Lolami\"\nvisited \"Lom Tahr\"\nvisited \"Lone Cloud\"\nvisited \"Lucina\"\nvisited \"Lurata\"\nvisited \"Makferuti\"\nvisited \"Markab\"\nvisited \"Markeb\"\nvisited \"Matar\"\nvisited \"Mebla\"\nvisited \"Mebsuta\"\nvisited \"Meftarkata\"\nvisited \"Mei Yohn\"\nvisited \"Mekislepti\"\nvisited \"Membulem\"\nvisited \"Men\"\nvisited \"Menkalinan\"\nvisited \"Menkar\"\nvisited \"Menkent\"\nvisited \"Merak\"\nvisited \"Mesuket\"\nvisited \"Miaplacidus\"\nvisited \"Miblulub\"\nvisited \"Mimosa\"\nvisited \"Minkar\"\nvisited \"Mintaka\"\nvisited \"Mirach\"\nvisited \"Mirfak\"\nvisited \"Mirzam\"\nvisited \"Mizar\"\nvisited \"Moktar\"\nvisited \"Mora\"\nvisited \"Muhlifain\"\nvisited \"Muphrid\"\nvisited \"Naos\"\nvisited \"Naper\"\nvisited \"Nashira\"\nvisited \"Nenia\"\nvisited \"Nihal\"\nvisited \"Nocte\"\nvisited \"Nunki\"\nvisited \"Oblate\"\nvisited \"Orbona\"\nvisited \"Orvala\"\nvisited \"Ossipago\"\nvisited \"Over the Rainbow\"\nvisited \"Pantica\"\nvisited \"Parca\"\nvisited \"Peacock\"\nvisited \"Pelubta\"\nvisited \"Peragenor\"\nvisited \"Peresedersi\"\nvisited \"Perfica\"\nvisited \"Persian\"\nvisited \"Persitar\"\nvisited \"Phact\"\nvisited \"Phecda\"\nvisited \"Pherkad\"\nvisited \"Phurad\"\nvisited \"Pik'ro'iyak\"\nvisited \"Plort\"\nvisited \"Polaris\"\nvisited \"Pollux\"\nvisited \"Porrima\"\nvisited \"Prakacha'a\"\nvisited \"Procyon\"\nvisited \"Pug Iyik\"\nvisited \"Quaru\"\nvisited \"Rajak\"\nvisited \"Rasalhague\"\nvisited \"Rastaban\"\nvisited \"Rati Cal\"\nvisited \"Regor\"\nvisited \"Regulus\"\nvisited \"Remembrance\"\nvisited \"Rigel\"\nvisited \"Ruchbah\"\nvisited \"Rutilicus\"\nvisited \"Ruwarku\"\nvisited \"Sabik\"\nvisited \"Sabriset\"\nvisited \"Sadachbia\"\nvisited \"Sadalmelik\"\nvisited \"Sadalsuud\"\nvisited \"Sadr\"\nvisited \"Sagittarius A*\"\nvisited \"Saiph\"\nvisited \"Salipastart\"\nvisited \"Salm\"\nvisited \"Sargas\"\nvisited \"Sarin\"\nvisited \"Sayaiban\"\nvisited \"Scheat\"\nvisited \"Schedar\"\nvisited \"Segesta\"\nvisited \"Seginus\"\nvisited \"Seketra\"\nvisited \"Sepetrosk\"\nvisited \"Sepriaptu\"\nvisited \"Sevrelect\"\nvisited \"Shaula\"\nvisited \"Sheratan\"\nvisited \"Si'yak'ku\"\nvisited \"Sich'ka'ara\"\nvisited \"Silikatakfar\"\nvisited \"Silver Bell\"\nvisited \"Silver String\"\nvisited \"Similisti\"\nvisited \"Sirius\"\nvisited \"Situla\"\nvisited \"Skeruto\"\nvisited \"Sko'karak\"\nvisited \"Sobarati\"\nvisited \"Sol\"\nvisited \"Sol Arachi\"\nvisited \"Sol Kimek\"\nvisited \"Sol Saryds\"\nvisited \"Solifar\"\nvisited \"Sospi\"\nvisited \"Speloog\"\nvisited \"Spica\"\nvisited \"Steep Roof\"\nvisited \"Stercutus\"\nvisited \"Suhail\"\nvisited \"Sumar\"\nvisited \"Sumprast\"\nvisited \"Tais\"\nvisited \"Talita\"\nvisited \"Tania Australis\"\nvisited \"Tarazed\"\nvisited \"Tarf\"\nvisited \"Tebuteb\"\nvisited \"Tejat\"\nvisited \"Terminus\"\nvisited \"Terra Incognita\"\nvisited \"Torbab\"\nvisited \"Tortor\"\nvisited \"Turais\"\nvisited \"Ula Mon\"\nvisited \"Ultima Thule\"\nvisited \"Umbral\"\nvisited \"Unagi\"\nvisited \"Unukalhai\"\nvisited \"Uwa Fahn\"\nvisited \"Vega\"\nvisited \"Vindemiatrix\"\nvisited \"Volax\"\nvisited \"Wah Ki\"\nvisited \"Wah Oh\"\nvisited \"Wah Yoot\"\nvisited \"Waypoint\"\nvisited \"Wazn\"\nvisited \"Wei\"\nvisited \"Wezen\"\nvisited \"World's End\"\nvisited \"Ya Hai\"\nvisited \"Yed Prior\"\nvisited \"Zaurak\"\nvisited \"Zeta Aquilae\"\nvisited \"Zeta Centauri\"\nvisited \"Zosma\"\nvisited \"Zuba Zub\"\nvisited \"Zubenelgenubi\"\nvisited \"Zubenelhakrabi\"\nvisited \"Zubeneschamali\"\n\"visited planet\" \"Ember Reaches\"\n\"visited planet\" \"Ember Threshold\"\n\"visited planet\" \"Ember Wormhole\"\n\"visited planet\" \"Hai Wormhole\"\n\"visited planet\" \"Pirate Wormhole\"\n\"visited planet\" \"Pug Wormhole\"\n\"visited planet\" \"Quarg Wormhole\"\n\"visited planet\" \"Remnant Wormhole\"\n\"visited planet\" \"Rim Wormhole\"\n\"visited planet\" \"The Eye\"\n\n(return to top)\n","187":"DeSyDe\nDeSyDe is a design space exploration tool developed at KTH (ForSyDe research group).\nReleases:\n\n\nlatest:\n\nRelease for our DSD'18 publication + user tutorial\n\n\n\nprevious:\n\nRelease for our TODAES article\nRelease for our RAPIDO'17 publication\n\n\n\n(Almost) hassle-free installation\nYou need to install DeSyDe via the automated build scripts. We have\ntried assuring an (almost) fully-automated installation process,\nespecially for Linux machines. The idea is quite simple: the script\ndownloads almost everything necessary so that nothing on your system\nis touched and then proceeds to compile everything. This sandboxing\ncomes with the cost of added compilation time, but since this should\nbe a one-time process, the larger time frame is a good trade-off for\nflexibility.\nThe only dependency that is not cloned directly from its repo and\ncompiled alongside DeSyDe is Qt, as DeSyDe currently does not make\nGecode's Gist optional. Please ensure that you have the basic\ndevelopment files for Qt installed and reachable in your machine. In\nfuture releases this necessity will be removed.\nIf you are on any debian-based distro with reasonably updated\npackages, you should be good to go by issuing the following install\ncommand (do not forget to prepend sudo if necessary):\napt install automake libtool qt5-default\n\nAs of 2019-06-05, it seems from user feedback that on ubuntu and other\nderived distros not all dependencies are pulled with these commands, so\nit may be necessary to install qtcreator to be able to compile DeSyDe\n(do not forget to prepend sudo if necessary):\napt install qtcreator\n\nThen, a make followed by make install should do the trick. Tested\non Linux Mint 18.3 and Debian 10.\nUsage\nPlease follow the tutorial for more details on how\nto use the tool and how to interpret its output.\nRunning the Experiments\nThe experiments provided in the examples folder represent those that are still functional and were\nused as proof of concepts into previous papers this project was involved. For a step-by-step tutorial\non how to setup your own experiment, check out the tutorial provided in this repo.\nIncluded examples\n\nDSD18: experiments from our DSD'18 dealing with TDN NoCs exploration that optimize power while respecting real time constraints.\nScalAnalysis: folder containing scripts that generates experiments for different sized NoCs platforms based on a template extracted from DSD18.\ntutorial: the files used for the user tutorial.\n\nPublications\nKathrin Rosvall, Tage Mohammadat, George Ungureanu, Johnny \u00d6berg, and Ingo Sander. \u201cExploring Power and Throughput for Dataflow Applications on Predictable NoC Multiprocessors,\u201d 719\u201326, 2018.\nKathrin Rosvall, Nima Khalilzad, George Ungureanu, and Ingo Sander. Throughput propagation in constraint-based design space exploration for mixed-criticality systems. In Proceedings of the 2017 Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools (RAPIDO '17), Stockholm, Sweden. ACM, January 2017.\nNima Khalilzad, Kathrin Rosvall, and Ingo Sander. A modular design space exploration framework for multiprocessor real-time systems. In Forum on specification & Design Languages (FDL '16), Bremen, Germany. IEEE, September 2016.\nKathrin Rosvall and Ingo Sander. A constraint-based design space exploration framework for real-time applications on MPSoCs. In Design Automation and Test in Europe (DATE '14), Dresden, Germany, Mar. 2014.\n","188":"Tux in Space\nTux in Space: space exploration game\nThis program is a simulation game. The program simulates the moving, under\nthe phisic's laws, of planets, spaceships, suns, and everything else in\ndeep space.\nHighlighted features:\n\nA wide gerarchic collection of various object types with different\nbehaviours\nA phisic engine that simulates the gravity force and different types of\nimpacts between objects\nSpace Monsters with basic AI\n\ntest\/use the program:\nThere is an executable file, compiled in a 64 bit Linux; Maybe can run\nin other computers as well, but you can easily compile the program with\nthe makefile (just run 'make' in the master directory).\nThe program is written for linux only, but with very little work or maybe no work at all could also\nrun on other operative systems. (See information file)\nOfficially supported compiler is gcc >= 6.3.\nFor developers is suggested to use the latest version in the\nmaster branch, for normal users the latest release.\nStatus\nThe program is far far away from being complete.\nLicense\nTux in Space - space exploration game\nCopyright (C) 2016-2017 emanuele.sorce@hotmail.com\nThis program is free software; you can redistribute it and\/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, version 3 or compatible.\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty or\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\nYou should have received a copy of the GNU General Public License\nalong with this program; if not, write to the Free Software\nFoundation, Inc.\n","189":"turbo-ros-pkg\nROS software repository, Robotics & Biology Laboratory, TU Berlin\n","190":"DIY spectrophotometer\nHow to use it ?\nPlease check the userguide to obtain all the useful information about the user interface.\nGoal of the project\nThis project was done at the Hackuarium association, a DIY biology hacking-space and open laboratory located in Ecublens (Switzerland). Feel free to contact us if you would like additional information or would like to buy a fully built and tested spectrophotometer.\nRecently, the open-source and open-hardware community has been involved in the creation of open scientific tools. In this context, a few spectrophotometer projects emerged. However, none of them was sufficiently evolved to be casually used in a laboratory. It is for this reason that we started this project.\nOur goal was to create a simple spectrophotometer that measures the\nabsorbance of a sample for 3 different light colors (red, green, blue), that is\nself-contained (battery powered) and displays the result on a LCD screen. In addition, the tool had to be reliable, precise and cheap.\nThe initial idea was that you could teach spectrophotometry without\nhaving to buy an expensive instrument, since you can find experiments\nthat do not require a specific light wavelength. This includes\noptical density for bacteria culture, determination of a pigment's concentration in\na solution, determination of the kinetic of a reaction and many others.\nThe device is rather cheap, for you can buy all the components on AliExpress\n(but one) and either 3D print or laser cut the case (MDF or acrylic glass). When constructing 10, the price\nper spectrophotometer is around $30. However if you want to have a final\nproduct with an aluminium case and PCBs already assembled, the cost would rather be $90 each if you order 20 of them.\n\nCan we do science with this ?\nBefore trying to explain how it is done more in detail maybe the first\nquestion that we should answer is : can we do science with this tool?\nWhen thinking about the design of the spectro, we found an light sensor that converts the light energy to frequency and that is linear on a range of nearly 10^6. This means that the luminosity range that we can measure precisely is really big. Only the precision of the component is therefore really promising and shows that we could do something that gives good results.\nAnd indeed, the final product gives really accurate results in fields like:\n\nChemistry\nBiology\n\nAnd other, which you can see on our lab notebook.\nSome of our results\nPatent blue V\nTo begin we did a very simple experiment that allows to measure the absorbance of solutions containing different known concentrations of patent blue V (E131), a blue pigment that is used in various blue candies.\n\nThe result is rather good knowing that the solutions were simply prepared by adding various volumes (0 to 2 mL) of a concentrated pigmented solution with a 1mL seringe to 100mL of water. Not the most accurate method...\nEscherichia coli\nAnother feature of the simple-spectrophotometer is that it works on a rechargeable battery (autonomy of 48 hours). Consequently, you can measure a kinetic by placing the device directly inside an incubator, which is not feasible with a regular commercial spectrophotometer.\nOptical density (OD) at 600nm is often used in biology to determine the biomass increase versus time. In this experiment we have placed Escherichia coli with cell growing media directly in the spectrophotometer cuvette and placed the spectrophotometer itself on a shaker in the incubator.\nWe can then program it to measure the absorbance every X seconds. In our case we measured the absorbance every 30 minutes (up to 40 measures). The following curve was obtained.\n\nThis experiment clearly shows the log phase and the stationary phase. But the growth was faster than expected. An acquisition every 15 minutes would have been a better choice.\nUsing the simple spectrophotometer to teach science\nThe use of the spectrophotometer is rather simple. Just put a blank, press the button, wait 10s and put the sample. The result is displayed on the screen. In addition, it could be open so that students could see how it is inside and could really understand how the tool works (avoiding the \"black box\" effect). Also, since it is inexpensive, having ten of these for a classroom would be possible, which would allow students to have one instrument per pair.\nUsing the spectrophotometer\nFor all this reason, we also consider our instrument as a pedagogical tool. This allowed us to use it to teach some basic scientific concepts to groups of children and teenagers.\nSchool in Mondomo (Colombia)\nHow to make it ?\nThe electronic design is compatible with the Arduino platform. We use an\nATMEGA32U4 microcontroller which is directly connected to the peripherals we need for this spectrophotometer.\nWe prefer to design the full PCB rather than to make a shield for an existing Arduino board. Indeed, the extra work to add the microcontroller is rather limited, so it does not make sense from our point of view to create a shield. Also, the resulting board is smaller and more reliable.\nAll the components we use but one are mainstream and can be found on eBay or AliExpress. The only special component is the light to frequency converter TSL235R\nthat we buy on Mouser website.\n\nFor more details about the construction process, follow one of the links underneath.\n\nThe PCB (using eagle)\nThe case (using FreeCAD and OpenSCAD)\nThe software (using Arduino)\n\nExperiments\n\nBacteria growth\nPigment concentration\nMeasure of the fluorescence\nKinetic of a reaction\n\nTests\nUnderneath, you will find some of the tests we have run to verify the quality of our spectrophotometer.\n\nBattery\nReproducibility of results\n\nWorkshop\nWe regularly organize one day workshops \"Build your own spectrophotometer\" during which the participants learn all the process we went through to obtain the final product and build their own spectrophotometer. Please contact us if you are interested by this kind of workshop.\nHere are the slides of the presentation we make during the workshop.\n\nSlides\n\nCloning the project\nThis project uses SUBMODULES.\nIn order to clone it use:\ngit clone --recurse-submodules https:\/\/github.com\/hackuarium\/simple-spectro\nTo update the submodules:\ngit submodule update --recursive --remote\n","191":"LibAPR - The Adaptive Particle Representation Library\nLibrary for producing and processing on the Adaptive Particle Representation (APR) (For article see: https:\/\/www.nature.com\/articles\/s41467-018-07390-9).\n\nLabeled Zebrafish nuclei: Gopi Shah, Huisken Lab (MPI-CBG, Dresden and Morgridge Institute for Research, Madison); see also Schmid et al., Nature Communications 2017\n\n\nDependencies\n\nHDF5 1.8.20 or higher\nOpenMP > 3.0 (optional, but suggested)\nCMake 3.6 or higher\nLibTIFF 4.0 or higher\n\nNB: This update to 2.0 introduces changes to IO and iteration that are not compatable with old versions.\nBuilding\nThe repository requires sub-modules, so the repository needs to be cloned recursively:\ngit clone --recursive https:\/\/github.com\/cheesema\/LibAPR\n\nIf you need to update your clone at any point later, run\ngit pull\ngit submodule update\n\nBuilding on Linux\nOn Ubuntu, install the cmake, build-essential, libhdf5-dev and libtiff5-dev packages (on other distributions, refer to the documentation there, the package names will be similar). OpenMP support is provided by the GCC compiler installed as part of the build-essential package.\nIn the directory of the cloned repository, run\nmkdir build\ncd build\ncmake ..\nmake\n\nThis will create the libapr.so library in the build directory, as well as all of the examples.\nDocker build\nWe provide a working Dockerfile that install the library within the image on a separate repo.\nBuilding on OSX\nOn OSX, install the cmake, hdf5 and libtiff homebrew packages and have the Xcode command line tools installed.\nIf you want to compile with OpenMP support, also install the llvm package (this can also be done using homebrew), as the clang version shipped by Apple currently does not support OpenMP.\nIn the directory of the cloned repository, run\nmkdir build\ncd build\ncmake ..\nmake\n\nThis will create the libapr.dylib library in the build directory, as well as all of the examples.\nIn case you want to use the homebrew-installed clang (OpenMP support), modify the call to cmake above to\nCC=\"\/usr\/local\/opt\/llvm\/bin\/clang\" CXX=\"\/usr\/local\/opt\/llvm\/bin\/clang++\" LDFLAGS=\"-L\/usr\/local\/opt\/llvm\/lib -Wl,-rpath,\/usr\/local\/opt\/llvm\/lib\" CPPFLAGS=\"-I\/usr\/local\/opt\/llvm\/include\" cmake ..\n\nBuilding on Windows\nThe simplest way to utilise the library from Windows 10 is through using the Windows Subsystem for Linux; see: https:\/\/docs.microsoft.com\/en-us\/windows\/wsl\/install-win10 then follow linux instructions.\nCompilation only works with mingw64\/clang or the Intel C++ Compiler, with Intel C++ being the recommended way\nThe below instructions for VS can be attempted; however they have not been reproduced.\nYou need to have Visual Studio 2017 installed, with the community edition being sufficient. LibAPR does not compile correctly with the default Visual Studio compiler, so you also need to have the Intel C++ Compiler, 18.0 or higher installed. cmake is also a requirement.\nFurthermore, you need to have HDF5 installed (binary distribution download at The HDF Group and LibTIFF (source download from SimpleSystems. LibTIFF needs to be compiled via cmake. LibTIFF's install target will then install the library into C:\\Program Files\\tiff.\nIn the directory of the cloned repository, run:\nmkdir build\ncd build\ncmake -G \"Visual Studio 15 2017 Win64\" -DTIFF_INCLUDE_DIR=\"C:\/Program Files\/tiff\/include\" -DTIFF_LIBRARY=\"C:\/Program Files\/tiff\/lib\/tiff.lib \" -DHDF5_ROOT=\"C:\/Program Files\/HDF_Group\/HDF5\/1.8.17\"  -T \"Intel C++ Compiler 18.0\" ..\ncmake --build . --config Debug\n\nThis will set the appropriate hints for Visual Studio to find both LibTIFF and HDF5. This will create the apr.dll library in the build\/Debug directory, as well as all of the examples. If you need a Release build, run cmake --build . --config Release from the build directory.\nExamples and Documentation\nThese examples can be turned on by adding -DAPR_BUILD_EXAMPLES=ON to the cmake command.\nThere are nine basic examples, that show how to generate and compute with the APR:\n\n\n\nExample\nHow to ...\n\n\n\n\nExample_get_apr\ncreate an APR from a TIFF and store as hdf5.\n\n\nExample_apr_iterate\niterate through a given APR.\n\n\nExample_neighbour_access\naccess particle and face neighbours.\n\n\nExample_compress_apr\nadditionally compress the intensities stored in an APR.\n\n\nExample_random_access\nperform random access operations on particles.\n\n\nExample_ray_cast\nperform a maximum intensity projection ray cast directly on the APR data structures read from an APR.\n\n\nExample_reconstruct_image\nreconstruct a pixel image from an APR.\n\n\n\nAll examples except Example_get_apr require an already produced APR, such as those created by Example_get_apr.\nFor tutorial on how to use the examples, and explanation of data-structures see the library guide.\nLibAPR Tests\nThe testing framework can be turned on by adding -DAPR_TESTS=ON to the cmake command. All tests can then be run by executing on the command line your build folder.\nctest\n\nPlease let us know by creating an issue, if any of these tests are failing on your machine.\nPython support\nNote: These have been updated and externalised, and will be released shortly.\nJava wrappers\nBasic Java wrappers can be found at LibAPR-java-wrapper\nComing soon\n\nmore examples for APR-based filtering and segmentation\ndeployment of the Java wrappers to Maven Central so they can be used in your project directly\nsupport for loading the APR in Fiji, including scenery-based 3D rendering\nimproved java wrapper support\nCUDA GPU-accelerated APR generation and processing\nBlock based decomposition for extremely large images.\nTime series support.\n\nContact us\nIf anything is not working as you think it should, or would like it to, please get in touch with us!! Further, if you have a project, or algorithm, you would like to try using the APR for also please get in contact we would be glad to help!\n\nCiting this work\nIf you use this library in an academic context, please cite the following paper:\n\nCheeseman, G\u00fcnther, Gonciarz, Susik, Sbalzarini: Adaptive Particle Representation of Fluorescence Microscopy Images (Nature Communications, 2018) https:\/\/doi.org\/10.1038\/s41467-018-07390-9\n\n","192":"All the C program code are translated to C++ STL. Smart pointers are used to avoid danger pointers and memory leak.\nIt is rewritten to support MVC design pattern required by MFC under Visual Studio 2017. It would be a good code reference for C++ programmers. The \"model\" folder in source code should be portable to other C++ compilers. As mentioned above, to understand the code to display the Windows UI required the fundamental knowledge of Microsoft Foundation Class (MFC) Library. C++ Beginners might find it difficult to understand the code of some essential UI controls. Many thanks to the MFC experts provided the source code of such advanced controls.\nWelcome for C++ experts for further improvement or contribution of coding enhancement.\n","193":"Cyclops LED Driver\nPrecision, wide-bandwidth current source with optional optical feedback mode\nfor driving high-power LEDs and laser diodes. Good for sneaking optogenetic\nstimuli between fast things (e.g. galvo flyback on a 2P system). Good for\nreally controlling the amount of light you deliver during 1P imaging or\noptogenetic stimulation.\nIf you have questions or comments, please come talk on the open-ephys slack\nin the #cyclops channel.\nFeatures\n\nUltra-precise\nHigh power\nUp to 1.5A per LED\nWide bandwidth\n\n~2.5 MHz -3 dB bandwidth\nMaximum 100 ns 1.0A rise and fall times\n\n\nCurrent and optical feedback modes\nBuilt-in waveform generation\nOver-current protection\nModular\n\nArduino compatible: internal waveform generation\nAlso, accepts external analog, gate, or trigger inputs\n\n\n\nStimulus generation options\n\nExternal stimulus sequencer\nExternal digital trigger\n\nTTL logic level\n\n\nExternal analog waveform generator\n\n0-5V analog signals\n\n\nInternal 12-bit DAC\n\nSynchronized across up to 4 drivers\nArduino library\nProgrammable triggering logic\nRespond to USB input\n\n\n\n\nBuying one\nYou can purchase a fully assembled cyclops driver from the open-ephys\nstore. All profits go towards continued\noperation of open-ephys.\nDocumentation\nDocumentation and usage information are here: MANUAL.pdf. If you\nhave questions concerning usage, performance, etc., please direct them toward\nthe Open Ephys forum.\nHardware Licensing\nCopyright Jonathan P. Newman 2020.\nThis work is licensed under CC BY-NC-SA 4.0. To view a copy of this license,\nvisit https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\nNote: This license applies to hardware designs and documentation which reside\nin the 'device', 'experimental', 'resources' folders of this repository along\nwith information in 'MANUAL.md' and 'MANUAL.pdf'\nSoftware Licensing\nCopyright (c) Jonathan P. Newman 2017. All right reserved.\nThe code associated with the Cyclops project is free software: you can\nredistribute it and\/or modify it under the terms of the GNU General Public\nLicense as published by the Free Software Foundation, either version 3 of the\nLicense, or (at your option) any later version.\nThe code associated with the Cyclops project is distributed in the hope that it\nwill be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General\nPublic License for more details.\nYou should have received a copy of the GNU General Public License along with\nthis code.  If not, see http:\/\/www.gnu.org\/licenses\/.\nNote: This license applies to software\/fireware source code which resides in in\nthe 'lib' folder of this repository\n","194":"PhysiBoSS\nMultiscale simulation of multi-cellular system\nOverview:\n\nPresentation\nUsage\nDocumentation\nReferences\nRemarks\n\nPresentation\nPhysiBoSS (PhysiCell-MaBoSS) is C++ software for multiscale simulation of heterogeneous multi-cellular system. It integrates together cell's internal signalling pathway model (boolean formalism), physical representation of cell (agent-based) and extra-cellular matrix diffusing or fixed entities.\nIt is adapted from PhysiCell sources, with the boolean network computation inside each cell from MaBoSS software.\n\nUsage\nCompiling PhysiBoSS\nPhysiBoSS should run and be easily installed on Linux and MacOS system.\nIt requires moderatly recent version of C++ (at least c++11) and OpenMP support. Compilation of MaBoSS library requires flex and bison library, usually already present (and can be easily installed on e.g. Linux ubuntu with sudo apt-get install bison flex). We also provide a Docker image of PhysiBoSS that can be used if it cannot be installed in your machine. It can also be used without any installation via a Web interface for specific simulations on nanohub.\nTo install it on Linux system, from a Terminal:\nClone the repository on your local machine, and go inside the main directory. Type make install, which will install and compile MaBoSS then PhysiBoSS. The executables will be created in the 'bin' directory if all goes well.\nIt can be compiled in 'Debug', 'Release' or 'Proliling' modes, to set in the 'Makefile' file. Default is 'Release' mode (fastest).\nYou might also have to change your c++ compiler in the Makefile according to your operating system.\nCommands list:\ngit clone https:\/\/github.com\/gletort\/PhysiBoSS.git\ncd PhysiBoSS\nmake install\nIf errors happened during the compilation, please refer to the installation page.\nRunning one simulation\nTo run a simulation, you need (at least) a XML parameter file indicating the conditions of the simulation, and the networks file (you can find some on MaBoSS website and on our logical modelling pipeline repository).\nOther options are possible, cf the code-documentation or this repository wiki for more informations.\nExample of a parameter file (with only few parameters shown):\n  <?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n \n  <simulation>\n \t\t<time_step> 0.2 <\/time_step>\n \t\t<mechanics_time_step> 0.1 <\/mechanics_time_step>\n \t\t....\n  <\/simulation>\n \n  <cell_properties>\n \t\t<mode_motility> 1 <\/mode_motility>\n \t\t<polarity_coefficient> 0.5 <\/polarity_coefficient>\n \t\t...\n  <\/cell_properties>\n \n  <network>\n \t\t<network_update_step> 10 <\/network_update_step>\n \t\t...\n  <\/network>\n \n  <initial_configuration>\n \t\t<load_cells_from_file> init.txt <\/load_cells_from_file>\n \t\t...\n  <\/initial_configuration>\nImage and analyse a simulation\nTo visualize graphically the result of a simulation, with use the software Paraview (or you can also generate a .svg snapshot of the simulation). Analysis of the result files were done with python scripts proposed in this directory. For documentation on how to use Paraview to set-up the rendering of PhysiBoSS outputs, see here, with the explication on how to draw spheres from a set of points (x, y, z, radius).\nNanohub\nPhysiBoSS can be directly used via a Web interface on nanohub. This allows to run it without any installation and running directly on the server and can be used without any coding skills. The parameters of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server. It just required a nanohub account.\nAvailable simulations tools of PhysiBoSS can be found on https:\/\/nanohub.org\/resources\/tools, under the keywords PhysiBoSS or PhysiBoSSa.\nA model of tumors cell spheroid growing and invading into the surrounding extra-cellular matrix (ECM) is currently available PhysiBoSSa_ECM. Various parameters as the density of the extracellular matrix, the cell motility, ECM degradation from the cells, TGF-beta production... can be tuned by the user.\nDocumentation\nCode-oriented documentation can be generated with Doxygen:\nmake doc\nin the main directory.\nIt can be configured in the Doxyfile file present in this directory.\nIt will generate the html documentation in the doc\/html directory.\nYou can visualize it in a browser, e.g.:\nfirefox doc\/html\/index.html &\nYou can also refer to (future) publications with PhysiBoSS for scientific applications of this software and description of the models.\nStep-by-step examples with the necessary files to run them are also proposed in the 'examples' directory and on the Wiki of this repository.\nReferences\n For PhysiBoSS: \n\nPhysiBoSS publication: Letort G, Montagud A, Stoll G, Heiland R, Barillot E, Macklin P, Zinovyev A, Calzone L .  PhysiBoSS: a multi-scale agent-based modelling framework integrating physical dimension and cell signalling.  Bioinformatics, bty766, doi:10.1093\/bioinformatics\/bty766\n\n\nFor PhysiCell: \nPaul Macklin's lab website  \nPhysiCell publication: A. Ghaffarizadeh, S.H. Friedman, S.M. Mumenthaler, and P. Macklin, PhysiCell: an Open Source Physics-Based Cell Simulator for 3-D Multicellular Systems, bioRxiv 088773, 2016. DOI: 10.1101\/088773. \nBioFVM website  \nBioFVM publication: A. Ghaffarizadeh, S.H. Friedman, and P. Macklin. BioFVM: an efficient, parallelized diffusive transport solver for 3-D biological simulations. Bioinformatics, 2015. \n\n\nFor MaBoSS:\nMaBoSS website  \nMaBoSS publication: Stoll G, Viara E, Barillot E, Calzone L. Continuous time Boolean modeling for biological signaling: application of Gillespie algorithm. BMC Syst Biol. 2012 Aug 29;6:116. doi: 10.1186\/1752-0509-6-116. \n\n\nRemarks\nPlease, refer to the Wiki of this repository for a much more extended documentation, with step by step examples instructions.\nPhysiCell is developed in Paul Macklin's lab.\nMaBoSS and PhysiBoSS are developed in the Computational Systems Biology of Cancer group at Institut Curie (Paris, France).\nWe invite you to use PhysiBoSS for you research and give feedbacks to us. Any help in developing it further is more than welcome.\nDo not hesitate to contact us for any comments or difficulties in using PhysiBoSS: physiboss@gmail.com.\nWishing you to enjoy using PhysiBoSS,\nPhysiBoSS's team.\n","195":"PhysiBoSS\nMultiscale simulation of multi-cellular system\nOverview:\n\nPresentation\nUsage\nDocumentation\nReferences\nRemarks\n\nPresentation\nPhysiBoSS (PhysiCell-MaBoSS) is C++ software for multiscale simulation of heterogeneous multi-cellular system. It integrates together cell's internal signalling pathway model (boolean formalism), physical representation of cell (agent-based) and extra-cellular matrix diffusing or fixed entities.\nIt is adapted from PhysiCell sources, with the boolean network computation inside each cell from MaBoSS software.\n\nUsage\nCompiling PhysiBoSS\nPhysiBoSS should run and be easily installed on Linux and MacOS system.\nIt requires moderatly recent version of C++ (at least c++11) and OpenMP support. Compilation of MaBoSS library requires flex and bison library, usually already present (and can be easily installed on e.g. Linux ubuntu with sudo apt-get install bison flex). We also provide a Docker image of PhysiBoSS that can be used if it cannot be installed in your machine. It can also be used without any installation via a Web interface for specific simulations on nanohub.\nTo install it on Linux system, from a Terminal:\nClone the repository on your local machine, and go inside the main directory. Type make install, which will install and compile MaBoSS then PhysiBoSS. The executables will be created in the 'bin' directory if all goes well.\nIt can be compiled in 'Debug', 'Release' or 'Proliling' modes, to set in the 'Makefile' file. Default is 'Release' mode (fastest).\nYou might also have to change your c++ compiler in the Makefile according to your operating system.\nCommands list:\ngit clone https:\/\/github.com\/gletort\/PhysiBoSS.git\ncd PhysiBoSS\nmake install\nIf errors happened during the compilation, please refer to the installation page.\nRunning one simulation\nTo run a simulation, you need (at least) a XML parameter file indicating the conditions of the simulation, and the networks file (you can find some on MaBoSS website and on our logical modelling pipeline repository).\nOther options are possible, cf the code-documentation or this repository wiki for more informations.\nExample of a parameter file (with only few parameters shown):\n  <?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n \n  <simulation>\n \t\t<time_step> 0.2 <\/time_step>\n \t\t<mechanics_time_step> 0.1 <\/mechanics_time_step>\n \t\t....\n  <\/simulation>\n \n  <cell_properties>\n \t\t<mode_motility> 1 <\/mode_motility>\n \t\t<polarity_coefficient> 0.5 <\/polarity_coefficient>\n \t\t...\n  <\/cell_properties>\n \n  <network>\n \t\t<network_update_step> 10 <\/network_update_step>\n \t\t...\n  <\/network>\n \n  <initial_configuration>\n \t\t<load_cells_from_file> init.txt <\/load_cells_from_file>\n \t\t...\n  <\/initial_configuration>\nImage and analyse a simulation\nTo visualize graphically the result of a simulation, with use the software Paraview (or you can also generate a .svg snapshot of the simulation). Analysis of the result files were done with python scripts proposed in this directory. For documentation on how to use Paraview to set-up the rendering of PhysiBoSS outputs, see here, with the explication on how to draw spheres from a set of points (x, y, z, radius).\nNanohub\nPhysiBoSS can be directly used via a Web interface on nanohub. This allows to run it without any installation and running directly on the server and can be used without any coding skills. The parameters of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server. It just required a nanohub account.\nAvailable simulations tools of PhysiBoSS can be found on https:\/\/nanohub.org\/resources\/tools, under the keywords PhysiBoSS or PhysiBoSSa.\nA model of tumors cell spheroid growing and invading into the surrounding extra-cellular matrix (ECM) is currently available PhysiBoSSa_ECM. Various parameters as the density of the extracellular matrix, the cell motility, ECM degradation from the cells, TGF-beta production... can be tuned by the user.\nDocumentation\nCode-oriented documentation can be generated with Doxygen:\nmake doc\nin the main directory.\nIt can be configured in the Doxyfile file present in this directory.\nIt will generate the html documentation in the doc\/html directory.\nYou can visualize it in a browser, e.g.:\nfirefox doc\/html\/index.html &\nYou can also refer to (future) publications with PhysiBoSS for scientific applications of this software and description of the models.\nStep-by-step examples with the necessary files to run them are also proposed in the 'examples' directory and on the Wiki of this repository.\nReferences\n For PhysiBoSS: \n\nPhysiBoSS publication: Letort G, Montagud A, Stoll G, Heiland R, Barillot E, Macklin P, Zinovyev A, Calzone L .  PhysiBoSS: a multi-scale agent-based modelling framework integrating physical dimension and cell signalling.  Bioinformatics, bty766, doi:10.1093\/bioinformatics\/bty766\n\n\nFor PhysiCell: \nPaul Macklin's lab website  \nPhysiCell publication: A. Ghaffarizadeh, S.H. Friedman, S.M. Mumenthaler, and P. Macklin, PhysiCell: an Open Source Physics-Based Cell Simulator for 3-D Multicellular Systems, bioRxiv 088773, 2016. DOI: 10.1101\/088773. \nBioFVM website  \nBioFVM publication: A. Ghaffarizadeh, S.H. Friedman, and P. Macklin. BioFVM: an efficient, parallelized diffusive transport solver for 3-D biological simulations. Bioinformatics, 2015. \n\n\nFor MaBoSS:\nMaBoSS website  \nMaBoSS publication: Stoll G, Viara E, Barillot E, Calzone L. Continuous time Boolean modeling for biological signaling: application of Gillespie algorithm. BMC Syst Biol. 2012 Aug 29;6:116. doi: 10.1186\/1752-0509-6-116. \n\n\nRemarks\nPlease, refer to the Wiki of this repository for a much more extended documentation, with step by step examples instructions.\nPhysiCell is developed in Paul Macklin's lab.\nMaBoSS and PhysiBoSS are developed in the Computational Systems Biology of Cancer group at Institut Curie (Paris, France).\nWe invite you to use PhysiBoSS for you research and give feedbacks to us. Any help in developing it further is more than welcome.\nDo not hesitate to contact us for any comments or difficulties in using PhysiBoSS: physiboss@gmail.com.\nWishing you to enjoy using PhysiBoSS,\nPhysiBoSS's team.\n","196":"\n\n\nCore Principles\nThese are the core principles of object-oriented approach to the current state of artificial neural networks that is inspired by synaptic plasticity between biological neurons:\n\nUnlike the current ANN implementations, neurons must be objects not tensors between matrices.\nJust the current ANN implementations, neurons should be GPU accelerated (ideally) to provide the necessary parallelism.\nWhile the current ANN implementations can only create special cases, a Plexus Network must be architecture-free (i.e. adaptive) to create a generalized solution of all machine learning problems.\nInstead of dealing with decision of choosing an ANN layer combination(such as Convolution, Pooling or Recurrent layers), the network must have a layerless design.\nThere must be fundamentally two types of neurons: sensory neuron, interneuron.\nInput of the network must be made of sensory neurons. Any interneuron can be picked as a motor neuron (an element of the output). There are literally no difference between an interneuron and a motor neuron except the intervene of the network for igniting the wick of learning process through the motor neurons. Any non-motor interneuron can be assumed as a cognitive neuron which collectively forms the cognition of network.\nThere can be arbitrary amount of I\/O groups in a single network.\nInstead of batch size, iteration, and epoch concepts, training examples must be fed on time basis with a manner like; learn first sample for X seconds, OK done? then learn second sample for Y seconds. By this approach, you can assign importance factors to your samples with maximum flexibility.\nNetwork must be retrainable.\nNetwork must be modular. In other words: You must be able to train a small network and then plug that network into a bigger network (we are talking about some kind of self-fusing here).\nNeurons must exhibit the characteristics of cellular automata just like Conway's Game of Life.\nNumber of neurons in the network can be increased or decreased (scalability).\nThere must be no need for a network-wide oscillation. Yet the execution of neurons should follow a path very similar to flow of electric current nevertheless.\nNetwork should use randomness and\/or uncertainty principle flawlessly. Consciousness is an emergent property from cellular level to macro scale, the network. But it's also an emergent property for the neuron from quantum level uncertainty to cellular mechanisms. In such a way that randomness is the cause of the illusion of consciousness.\nMost importantly, the network must and can not iterate through the whole dataset. Besides that, it's also generally impossible to iterate the whole dataset on real life situations if the system is continuous like in robotics. Because of that; the network must be designed to handle such a continuous data stream that literally endless and must be designed to handle that data stream chunk by chunk. Therefore, when you are feeding the network, use a diverse feed but not a grouped feed (like 123123123123123123 but not like 111111222222333333).\n\nActivation function\nThe activation function that used by Plexus is Sigmoid:\n\n\n\nand the derivative of the Sigmoid function:\n\n\n\nImplementation of this algorithm in Python programming language is publicly accessible through this link: https:\/\/github.com\/mertyildiran\/Plexus\/blob\/master\/plexus\/plexus.py\nYou can directly skip to Application part if you are not willing to understand the mathematical and algorithmic background.\nBasics\nPlexus Network has only two classes; Network and Neuron. In a Plexus Network, there are many instances of Neuron class but there is only one instance of Network class.\nWhen you crate a new Plexus Network you give these five parameters to the Network class: size of the network, input dimension, output dimension, connectivity rate, precision. The network accordingly builds itself.\n\n\n\n\nsize is literally equal to total number of neurons in the network. All neurons are referenced in an instance variable called Network.neurons\ninput dimension specifies the number of sensory neurons. Sensory neurons are randomly selected from neurons.\noutput dimension specifies the number of motor neurons. Motor neurons are randomly selected from non-sensory neurons.\nnumber of neurons multiplied by connectivity rate gives the average number of subscriptions made by a single neuron.\n\n\n\n\n\nprecision simply defines the precision of the all calculations will be made by neurons (how many digits after the decimal point).\n\nAfter the network has been successfully created. It will ignite itself automatically. Ignition in simple terms, no matter if you have plugged in some data or not, it will fire the neurons with using some mechanism very similar to flow of electric current (will be explained later on this paper).\nAnatomy of a Single Neuron\nA single neuron in a Plexus Network, takes the network as the only parameter and stores these seven very important information (in it's instance variables): subscriptions, publications, potential, desired_potential, loss and type\n\n\n\nThere are eventually there types of neurons:\n\nNeuron.type = 1 means it's a sensory neuron.\nNeuron.type = 2 means it's a motor neuron.\nNeuron.type = 0 means it's neither a sensory nor a motor neuron. It means it's an cognitive interneuron (or just cognitive neuron).\n\nFunctionality of a neuron is relative to its type.\nsubscriptions is neuron's indirect data feed. Each non-sensory neuron subscribes to some other neurons of any type. For sensory neurons, subscriptions are completely meaningless and empty by default because it gets its data feed from outside world by assignments of the network. Subscriptions are literally the Plexus Network equivalent of Dendrites in biological neurons. subscriptions is a dictionary that holds Neuron(reference) as key and Weight as value.\npublications holds literally the mirror data of subscriptions in the target neurons. In other words; any subscription creates also a publication reference in the target neuron. Similarly, publications is the Plexus Network equivalent of Axons in biological neurons.\npotential p is the overall total potential value of all subscriptions multiplied by the corresponding weights. Only in sensory neurons, it is directly assigned by the network. Value of potential may only be updated by the neuron's itself and its being calculated by this simple formula each time when the neuron is fired:\n\n\n\n\n\n\ndesired_potential p' is the ideal value of the neuron's potential that is desired to eventually reach. For sensory neurons, it is meaningless. For motor neurons, it is assigned by the network. If it's None then the neuron does not learn anything and just calculates potential when it's fired.\nloss l is calculated not just at the output but in every neuron except sensory ones and it is equal to absolute difference (distance) between desired potential and current potential.\n\n\n\nAll numerical values inside a neuron are floating point numbers and all the calculations obey to the precision that given at start.\nSensory and Motor Neurons\nInput Layer in classical neural networks renamed as Sensory Neurons in Plexus networks, and Target\/Output Layer renamed as Motor Neurons. This naming convention is necessary cause the built of the relevance of artificial neural networks with biological neural networks and Neuroscience.\nThe difference of sensory neurons from the cognitive neurons (that neither sensory nor motor ones) is, they do not actually fire. They just stand still for the data load. They do not have any subscriptions to the other neurons (literally no subscriptions). But they can be subscribed by the other neurons, including motor ones. They do not learn, they do not consume any CPU resources. They just stored in the memory. You can assign an image, a frame of a video, or a chunk of an audio to a group of sensory neurons.\nThe difference of motor neurons form the other neurons is, they are only responsible to the network. They act as the fuse of the learning and calculation of the loss. The network dictates a desired potential on each motor neuron. The motor neuron calculates its potential, compares it with desired potential, calculates the loss then tries to update its weights randomly many times and if it fails, it blames its subscriptions. So just like the network, motor neurons are also able to dictate a desired potential on the other non-motor neurons. This is why any neuron holds an additional potential variable called desired_potential.\nPartially Subscribe\nOn the second phase of the network initiation, any non-sensory neurons are forced to subscribe to some non-motor neurons which are selected by random sampling. Length of this sample is also selected by random sampling (rounds to nearest integer) is done from a normal distribution. Such a normal distribution that, the average number of subscriptions is the mean, and square root of the mean is the standard deviation. (e.g. if neurons on average has 100 subscriptions then the mean is 100 and the standard deviation is 10)\n\n\n\nAlgorithm\nEven so the Python implementation of Plexus Network is easy to understand, it will be helpful for readers to explain the algorithm in pseudocode;\nInitiation\nprocedure initiate the network is\n    connectivity \u2190 size * connectivity_rate;\n    connectivity_sqrt \u2190 sqrt(connectivity);\n    connectivity_sqrt_sqrt \u2190 sqrt(connectivity_sqrt);\n    for item in size, do\n        create neuron;\n    end\n    pick sensory neurons randomly;\n    pick motor neurons randomly;\n    determine non-sensory neurons;\n    determine non-motor neurons;\n    initiate subscriptions;\n    initiate instance variables;\n    ignite the network;\nInitiation is nothing more than a make the assignments for once phase until the ignition. The final step (ignition) never stops but can be paused (if user wants).\nInitiate Subscriptions\nprocedure initiate subscriptions is\n    for neuron in neurons, do\n        if neuron is not a sensory neuron, then\n            call neuron.partially_subscribe();\n        end\n    end\n    return True;\nPartially Subscribe\nprocedure partially subscribe is\n    sample \u2190 randomly sample approximately \"connectivity\" units of a neuron from within all non-motor neurons;\n    for neuron in sample, do\n        if neuron is not self, then\n            establish a subscription;    \/\/ weight is randomly assigned\n            establish a publication;\n        end\n    end\n    return True;\nThe time complexity of the procedure initiate subscriptions is O(n2), so this may take a while if the size of the network and connectivity is big.\nIgnite\nprocedure ignite subscriptions is\n    create an empty ban_list;\n    while network is not frozen, do\n        if next_queue is empty, then\n            get the output of network and print it;\n            increase the wave_counter;\n            if first_queue is empty, then\n                for neuron in sensory neurons, do\n                    for target_neuron in neuron.publications, do\n                        append target_neuron to first_queue;\n                    end\n                end\n                copy first_queue to next_queue;\n            end\n        end\n        copy next_queue to current_queue;\n        empty next_queue;\n        for neuron in ban_list, do\n            if neuron.ban_counter > connectivity_sqrt_sqrt, then\n                remove the neuron from current_queue;\n            end\n        end\n        while current_queue is not empty, do\n            neuron \u2190 select a random neuron from current_queue;\n            remove the neuron from current_queue;\n            if neuron.ban_counter <= connectivity_sqrt_sqrt, then\n                call neuron.fire();\n                append the neuron to ban_list;\n                increase neuron.ban_counter;\n                for target_neuron in neuron.publications, do\n                    append target_neuron to next_queue;\n                end\n            end\n        end\n    end\nProcedure ignite regulates the firing order of neurons and creates an effect very similar to flow of electric current, network wide. It continuously runs until the network frozen, nothing else can stop it. It fires the neurons step by step through adding them to a queue.\nIt generates its first queue from the publications of sensory neurons. Time complexity of if next_queue is empty, then block is O(n2) but it can be ignored (unless there are too many sensory neurons) because it runs once per wave.\nIt eliminates banned neurons with for neuron in ban_list, do block. Function of ban_counter is giving neurons connectivity_sqrt_sqrt amount of chance after they added to ban_list. Then it fires the neurons inside current_queue, one by one, choosing them randomly.\nAfter a neuron fired, it adds the fired neuron to ban_list and lastly copies the publications of that neuron to next_queue so execution(firing process) can follow the path through the connections.\nEach execution from first sensory neuron to last motor neuron symbolizes one wave. Every time a wave finished, procedure falls into if next_queue is empty, then block so wave starts over from the sensory neurons.\nban_counter and connectivity_sqrt_sqrt comparison creates execution loops inside cognitive neurons and these loops act like memory units which is a pretty important concept. Because loops create the relation between currently fed data and previously learned data. Without these loops the network fails on both classification and regression problems.\nBecause neuron.fire() has a time complexity of O(n2), each turn inside while network is not frozen, do block, has a time complexity of O(n4). But don't worry because it will approximate to O(n3) because of the probabilistic nature of fire function and the network will fire more than a million of neurons per minute. By the way, while network is not frozen, do block is ignored because it's an endless loop under normal conditions.\nFire\nprocedure fire is\n    if self is not a sensory neuron, then\n        potential \u2190 calculate potential;\n        increase fire counter;\n        if desired_potential is not None, then\n\n            loss \u2190 calculate loss;\n            if loss = 0, then\n                desired_potential \u2190 None;\n                return True;\n            end\n            if blame_lock is not empty, then\n                if (wave_counter - blame_lock) < connectivity, then\n                    return True;\n                else\n                    blame_lock \u2190 None;\n            end\n\n            try connectivity times:\n                generate new weights randomly;\n                calculate new potential and new loss according to these weights;\n                if loss_new < loss_current, then return True;\n            end\n\n            try sqrt(connectivity) times:\n                generate hypothetical potentials for neurons in subscriptions randomly;\n                calculate new potential and new loss according to these hypothetical potentials;\n                if loss_new < loss_current, then\n                    apply these hypothetical potentials as \"desired_potential\"s;\n                    return True;\n                end\n            end\n\n            if (still) not improved, then\n                either create some new subscriptions;\n                or break some of the subscriptions;\n                return True;\n            end\n\n        end\n    end\nProcedure fire handles all feedforwarding, backpropagation and learning process by itself. fire function is an instance method of Neuron class. This procedure is by far the most important one in the Plexus Network. It's basically the core function and CPU spends most of its time to execute fire functions again and again.\nIf desired_potential is not assigned to a value, then it just calculates the potential and finishes.\nIf desired_potential is assigned to a value, then first it calculates the loss. If loss is equal to zero, then the current state of the neuron is perfectly well and there is nothing to learn.\nIf blame_lock is not empty, then it will pass this function connectivity times with this control statement: if blame_lock is not empty, then.\n\n\n\nIt tries to improve the current state of the neuron by updating its weights randomly, connectivity times. If it's improved, then break.\nIt tries to improve the current state of the neuron by dictating randomly generated hypothetical potentials over the subscriptions, square root of connectivity times. If it's improved, then break.\nIf it still is not improved, then it either creates some new subscriptions or breaks some of the subscriptions it currently has and hopes it will lead the neuron to new improvements in the future.\nOn the first wave, the fire function is only meaningful for motor neurons but after the first wave desired_potential dictation will spread throughout the cognitive neurons.\nLoad\nprocedure load (input, output) is\n    if output is None, then\n        for neuron in motor neurons, do\n            neuron.desired_potential \u2190 None;\n        end\n    end\n    if (number of sensory neurons is not equal to input length), then\n        raise an error but do not interrupt the network;\n    else\n        for neuron in sensory neurons, do\n            neuron.potential \u2190 load from the input;\n        end\n    end\n    if (number of motor neurons is not equal to output length), then\n        raise an error but do not interrupt the network;\n    else\n        for neuron in motor neurons, do\n            neuron.desired_potential \u2190 load from the output;\n        end\n    end\nProcedure load is the only method that you can feed your data to the network. You should call that function and load your data in real time. Also you should do it periodically and continuously, like every 3 seconds. If you leave second parameter empty then this procedure will automatically assume that you are testing the network, so it will replace desired_potential values of motor neurons with None. Otherwise, it means you are training the network so it will load the input data to sensory neurons and it will load the output data to desired_potential values of motor neurons.\nApplication\nInstallation of the Python Package\npip install plexus\nIf you want to install Plexus on development mode:\ngit clone https:\/\/github.com\/mertyildiran\/Plexus.git\ncd Plexus\/\npip install -e .\nor alternatively:\nmake dev\nand test the installation with:\nmake cpp\nExamples\nBinary Classification Example\n(you can alternatively run this example with python3 examples\/classification_binary.py command using a pre-written script version of below commands)\nSuppose you need to train the network to figure out that the elements of given arrays are bigger than 0.5 or not (like [0.9, 0.6, 1.0, 0.8] or [0.1, 0.3, 0.0, 0.4]) and suppose it's a 4-element array. So let's create a network according to your needs:\nimport cplexus as plexus\n\nSIZE = 14\nINPUT_SIZE = 4\nOUTPUT_SIZE = 2\nCONNECTIVITY = 1\nPRECISION = 2\nRANDOMLY_FIRE = False\nDYNAMIC_OUTPUT = True\nVISUALIZATION = False\nnet = plexus.Network(\n    SIZE,\n    INPUT_SIZE,\n    OUTPUT_SIZE,\n    CONNECTIVITY,\n    PRECISION,\n    RANDOMLY_FIRE,\n    DYNAMIC_OUTPUT,\n    VISUALIZATION\n)\nIf you want to visualize the network using PyQtGraph enable VISUALIZATION = False. Because our network is automatically initiated and ignited, now all we have to do is training the network. So let's train our network with 80 samples:\nPRECISION = 2\nTRAINING_SAMPLE_SIZE = 20\nfor i in range(1, TRAINING_SAMPLE_SIZE):\n    if (i % 2) == 0:\n        output = [1.0, 0.0]\n        generated_list = generate_list_bigger()\n        notify_the_load(generated_list, output, TRAINING_DURATION)\n        net.load(generated_list, output)\n    else:\n        output = [0.0, 1.0]\n        generated_list = generate_list_smaller()\n        notify_the_load(generated_list, output, TRAINING_DURATION)\n        net.load(generated_list, output)\n    time.sleep(TRAINING_DURATION)\nYou should load your data one by one from each kind, respectively. Because it will prevent over fitting to one specific kind. You must wait a short time like TRAINING_DURATION = 0.01 seconds (which is a reasonable duration in such a case), after each load.\noutput[0] will converge to detect bigger than 0.5 inputs.\noutput[1] will converge to detect smaller than 0.5 inputs.\nBefore the testing you should define a criteria called DOMINANCE_THRESHOLD so you can catch the decision making. Now let's test the network:\nerror = 0\nerror_divisor = 0\nfor i in repeat(None, TESTING_SAMPLE_SIZE):\n    binary_random = random.randint(0, 1)\n    if binary_random == 0:\n        generated_list = generate_list_bigger()\n        expected = [1.0, 0.0]\n    else:\n        generated_list = generate_list_smaller()\n        expected = [0.0, 1.0]\n\n    net.load(generated_list)\n    time.sleep(TRAINING_DURATION)\n\n    output = net.output\n    error += abs(expected[0] - output[0])\n    error += abs(expected[1] - output[1])\n    error_divisor += 2\nWith the while loop given above, you will be able to check the output by giving enough time to propagate your input throught the network. By giving net.load() only one parameter here, you automatically disable the training.\nNow freeze your network and calculate the overall error:\nnet.freeze()\nerror = error \/ error_divisor\nwhich outputs:\nOverall error: 0.010249996604397894\n\nClassifying Prime Numbers Example\nThis example is quite simple to the previous example but this time we are teaching the network to understand if the given number is prime or not. Which is a relatively complex problem.\nRun python3 examples\/classification_prime.py 1 -l cpp to see the result. You will observe that the network is able to learn the solution for such a complex problem in the matter of seconds.\nSequence Basic Example\nIn this example, instead of classification, we will train the network to detect a pattern in given sequence. The magic here is; without even changing anything related to network, just by changing logic we feed the data into the network, the network automatically turns into a Recurrent Neural Network.\nRun python3 examples\/sequence_basic.py to see the output. This is the output you should see:\n___ PLEXUS NETWORK BASIC SEQUENCE RECOGNITION EXAMPLE ___\n\nCreate a Plexus network with 14 neurons, 4 of them sensory, 1 of them motor, 1 connectivity rate, 2 digit precision\n\nPrecision of the network will be 0.01\nEach individual non-sensory neuron will subscribe to 14 different neurons\n14 neurons created\n4 neuron picked as sensory neuron\n1 neuron picked as motor neuron\nNetwork has been ignited\n\n*** LEARNING ***\n\nGenerate The Dataset (20 Items Long) To Recognize a Sequence & Learn for 0.1 Seconds Each\nLoad Input: [1.0, 0.0, 0.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 1.0, 0.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 0.0, 1.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 0.0, 0.0, 1.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [1.0, 0.0, 0.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 1.0, 0.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 0.0, 1.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 0.0, 0.0, 1.0]\tOutput: [1.0]\tand wait 0.1 seconds\nLoad Input: [1.0, 0.0, 0.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 1.0, 0.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 0.0, 1.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 0.0, 0.0, 1.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [1.0, 0.0, 0.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 1.0, 0.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 0.0, 1.0, 0.0]\tOutput: [0.0]\tand wait 0.1 seconds\nLoad Input: [0.0, 0.0, 0.0, 1.0]\tOutput: [1.0]\tand wait 0.1 seconds\n...\n\nby looking at this output, you should be able to see the pattern. Now on testing stage you can see how successful the network is on detecting the pattern:\n*** TESTING ***\n\nTest the network with random data (20 times)\nLoad Input: ([1.0, 0.0, 0.0, 0.0], [0.0])\tRESULT: 0.019999999552965164\nLoad Input: ([0.0, 1.0, 0.0, 0.0], [0.0])\tRESULT: 0.0\nLoad Input: ([0.0, 0.0, 1.0, 0.0], [0.0])\tRESULT: 0.019999999552965164\nLoad Input: ([0.0, 0.0, 0.0, 1.0], [0.0])\tRESULT: 0.019999999552965164\nLoad Input: ([1.0, 0.0, 0.0, 0.0], [0.0])\tRESULT: 0.0\nLoad Input: ([0.0, 1.0, 0.0, 0.0], [0.0])\tRESULT: 0.0\nLoad Input: ([0.0, 0.0, 1.0, 0.0], [0.0])\tRESULT: 0.05999999865889549\nLoad Input: ([0.0, 0.0, 0.0, 1.0], [1.0])\tRESULT: 0.6600000262260437\nLoad Input: ([1.0, 0.0, 0.0, 0.0], [0.0])\tRESULT: 0.019999999552965164\nLoad Input: ([0.0, 1.0, 0.0, 0.0], [0.0])\tRESULT: 0.0\nLoad Input: ([0.0, 0.0, 1.0, 0.0], [0.0])\tRESULT: 0.05999999865889549\nLoad Input: ([0.0, 0.0, 0.0, 1.0], [0.0])\tRESULT: 0.6600000262260437\nLoad Input: ([1.0, 0.0, 0.0, 0.0], [0.0])\tRESULT: 0.019999999552965164\nLoad Input: ([0.0, 1.0, 0.0, 0.0], [0.0])\tRESULT: 0.0\nLoad Input: ([0.0, 0.0, 1.0, 0.0], [0.0])\tRESULT: 0.0\nLoad Input: ([0.0, 0.0, 0.0, 1.0], [1.0])\tRESULT: 0.9800000190734863\n...\n\nand the overall error:\nNetwork is now frozen\n\n1786760 waves are executed throughout the network\n\nIn total: 66110093 times a random non-sensory neuron is fired\n\n\nOverall error: 0.040124996623490006\n\nCatDog Example\n(you can alternatively run this example with python3 examples\/catdog.py command using a pre-written script version of below commands)\nSuppose you need to train the network to figure out that the given image (32x32 RGB) is an image of a cat or a dog and map them to blue and red respectively. So let's create a network according to your needs:\nSIZE = 32 * 32 * 3 + 3 + 256\nINPUT_SIZE = 32 * 32 * 3\nOUTPUT_SIZE = 3\nCONNECTIVITY = 0.005\nPRECISION = 3\nTRAINING_DURATION = 3\nRANDOMLY_FIRE = False\nDYNAMIC_OUTPUT = False\nVISUALIZATION = False\nnet = plexus.Network(\n    SIZE,\n    INPUT_SIZE,\n    OUTPUT_SIZE,\n    CONNECTIVITY,\n    PRECISION,\n    RANDOMLY_FIRE,\n    DYNAMIC_OUTPUT,\n    VISUALIZATION\n)\nWe will plug in 32x32 RGB to the network so we need 3072 sensory neurons. 3 motor neurons for see how RGB our result is and 256 cognitive neurons to train. We need 3 digits precision because we need to store 255 different values between 0.0 and 1.0 range.\nExplaining the answer of How to load CIFAR-10 dataset and use it is out of the scope of this paper but you can easily understand it by reading the code: examples\/catdog.py Once you get the numpy array of CIFAR-10 (or any other image data) just normalize it and load:\nTRAINING_SAMPLE_SIZE = 20\nfor i in range(1, TRAINING_SAMPLE_SIZE):\n    if (i % 2) == 0:\n        cat = random.sample(cats, 1)[0]\n        cat_normalized = np.true_divide(cat, 255).flatten()\n        blue_normalized = np.true_divide(blue, 255).flatten()\n        cv2.imshow(\"Input\", cat)\n        net.load(cat_normalized, blue_normalized)\n    else:\n        dog = random.sample(dogs, 1)[0]\n        dog_normalized = np.true_divide(dog, 255).flatten()\n        red_normalized = np.true_divide(red, 255).flatten()\n        cv2.imshow(\"Input\", dog)\n        net.load(dog_normalized, red_normalized)\n    show_output(net)\nYou will get an Overall error as the result very similar to examples above although this time the input length was 768 times bigger. This is because Plexus Network amalgamates the problems from all levels of difficulty on a single medium. It makes easy problems relatively hard, and hard problems relatively easy.\nWhen you run this example, you will get a slightly better result when compared to flipping a coin. You will most likely get an Overall error between 0.35 - 0.45 which is the proof that the network is able to learn something.\nBy the way, don't forget that; Plexus Network does not iterate over the dataset and furthermore it runs in real-time. Also you have trained the network just for 4-5 minutes. Now let's see what happens if we train our network for a long period of time:\nNote\nImplementation of GPU acceleration and saving the trained network to disk are in work-in-progress (WIP) state. Therefore some parts of the implementation are subject to change.\n","197":"CATH Tools  \nProtein structure comparison tools such as SSAP, as used by the Orengo Group in curating CATH.\n\n\n\n\n\n\n\n\n\n\n\n\nExecutable DOWNLOADS   (for Linux\/Mac; chmod them to be executable)\nDocs   \u00a0 \nCode   \u00a0 \nBuilds   \u00a0 \nExtras repo   \u00a0 \n\n\n\nTools\n\n\n\n\n\n\n\n\n\n\n cath-cluster   Complete-linkage cluster arbitrary data.\n\n\n\n cath-map-clusters   Map names from previous clusters to new clusters based on (the overlaps between) their members (which may be specified as regions within a parent sequence). Renumber any clusters with no equivalents.\n\n\n\n cath-resolve-hits   Collapse a list of domain matches to your query sequence(s) down to the non-overlapping subset (ie domain architecture) that maximises the sum of the hits' scores.\n\n\n\n cath-ssap   Structurally align a pair of proteins.\n\n\n\n cath-superpose   Superpose two or more protein structures using an existing alignment.\n\n\n\nExtra Tools\n\nbuild-test          Perform the cath-tools tests (which should all pass, albeit with a few warnings)\ncath-assign-domains Use an SVM model on SSAP+PRC data to form a plan for assigning the domains to CATH superfamilies\/folds\ncath-refine-align   Iteratively refine an existing alignment by attempting to optimise SSAP score\ncath-score-align    Score an existing alignment using structural data\n\nAuthors\nThe SSAP algorithm (cath-ssap) was devised by Christine A Orengo and William R Taylor.\nPlease cite: Protein Structure Alignment, Taylor and Orengo, Journal of Molecular Biology 208, 1-22, PMID: 2769748. (PubMed, Elsevier)\nSince then, many people have contributed to this code, most notably:\n\nTony E Lewis             (2011\u2013\u2026)\nOliver C Redfern                                          (~2003\u20132011)\nJames E Bray, Ian Sillitoe (~2000\u20132003)\nAndrew C R Martin    (considerable edits around 2001)\n\nAcknowledgements\ncath-ssap typically uses DSSP, either by reading DSSP files or via its own implementation of the DSSP algorithms.\ncath-cluster uses Fionn Murtagh's reciprocal-nearest-neighbour algorithm (see Multidimensional clustering algorithms, volume 4 of Compstat Lectures.\nPhysica-Verlag, W\u00fcrzburg\/ Wien, 1985. ISBN 3-7051-0008-4) as described and refined in Daniel M\u00fcllner's Modern hierarchical, agglomerative clustering algorithms (2011, arXiv:1109.2378).\nFeedback\nPlease tell us about your cath-tools bugs\/suggestions here.\nIf you find this software useful, please spread the word and star the GitHub repo.\n","198":"skew-biology\nSoftware for my DIY spectrometers, sensors, pcr, and incubators.\nSpectrometer Software\nSoftware for reading a spectrum from a DIY spectrometer using OpenCV,\ncalibrating it using non-linear regression to the function with the gnu scientific library.\nExample DIY Lego Spectrometer\n\n\n1k diffraction grating\nSony IMX179 8MP CCD\nVelcro, aluminum foil, and a lot of black legos\nCCD at m=0\n\nUsage:\nspec [roi x y w h] [cal C0 C1 C2]\nThe ROI and constants for calibration can be specified on the command line. The calculated intensity and wavelength can be saved to csv alongside the calibration file for analysis in other programs.\nCalibration Details\nThis software currently uses a second-order polynomial for calibration off a minimum of three points. There may be value in moving to a cubic function.\nnm = C0 + C1p + C2p^2\nCalibration References\n\nCalibrating the Wavelength of Your Spectrometer\nOceanOptics Cubic Calibration\nPublicLab Linear Calibration\n\nSelecting a Region of Interest\nOn startup the region of interest can be selected with a mouse. The pixels in this region of interest are summed to calculate intensity.\nCommands:\n\n\n\nCommand\nNote\n\n\n\n\ntop\nlists the top wavelengths, intensities, and pixel indicies\n\n\ncal\nenters calibration mode\n\n\nsave\nsaves to csv along with the calibration\n\n\n\nBuilding:\ncmake .\nmake\n\nExample usage:\nCalibrating a CFL using the mercury peaks terbium peaks. Appears to accurately predict europium peaks at 612nm.\nCFL Spectrum Reference\n\nIncubator\nPID controled incubator w\/ IR\nRGB\nsimple rgb sensor control for recording the rgb values of a sample\nbeing incubated by a connected computer\n","199":"Kalei - Style guide\nThis project aims at making sure your style sheets are fully documented whilst being synchronized with your webpages styles. To do this it actually uses your live stylesheets in so that at anytime you can review how your styleguide looks.\nMain goals and benefits\n\nFully documented CSS - No need to explain the benefits\nNo dependencies, simply download the repository and run in your browser\nAutomatic generation of demo UI components\nEasy access for anyone, designer, developer, manager and users\nRapid development of projects by allowing developers to find the correct CSS and HTML for any given UI component\nOpen sourced so that all great ideas can be included\n\nGetting started\n\nDownload the repository (git clone git:\/\/github.com\/kaleistyleguide\/kaleistyleguide.git)\nServe it on a HTTP server and it should work!\nEdit js\/config.js to point at your own styles.css\n\nAuthor\nThomas Davis\nContributors\nLuke Brooker\nRichard Barret\nSam Pospischil\nInspiration\nKalei is heavily influenced by the following projects and blog posts.\nPea.rs\nKSS\nStyleDocco\nRJ Metrics\nAnchoring Your Design Language in a Live Style Guide\nTechnologies\n\nmarked\njscssp\nLESS\ncssbeautify\nfixie\nhighlight\nbackbone\nunderscore\njquery\n\nLicense\nPublic domain: http:\/\/unlicense.org\/\n","200":"bui\n\u57fa\u4e8ejQuery\u7684\u5bcc\u5ba2\u6237\u7aef\u63a7\u4ef6\u5e93\n\n\u6587\u6863\u5e93\u5730\u5740\n\u5e94\u7528\u4ee3\u7801\nAPI\u4ee3\u7801\nLicense\n\u63d0\u4ea4\u4ee3\u7801\u6d41\u7a0b\n\n\u6587\u4ef6\u7ed3\u6784\n\nassets : css\u6587\u4ef6\uff0c\u57fa\u4e8ebootstrap\u7684css\u6837\u5f0f\uff0c\u53ef\u4ee5\u81ea\u5df1\u5728\u6b64\u57fa\u7840\u4e0a\u7f16\u8bd1\u51fa\u65b0\u7684\u7248\u672c\nbuild : js \u548c css\u6587\u4ef6\u6253\u5305\u597d\u7684\u76ee\u5f55\nsrc: js \u7684\u6e90\u6587\u4ef6\ntest: \u5355\u5143\u6d4b\u8bd5\uff0c\u6240\u6709\u63a7\u4ef6\u7684\u5355\u5143\u6d4b\u8bd5\u90fd\u5728\u5185\u90e8\uff0c\u4ee5php\u7684\u65b9\u5f0f\u63d0\u4f9b\ntools : \u6587\u4ef6\u6253\u5305\uff0c\u4ee5\u53ca\u751f\u6210\u6587\u4ef6\u7684\u5de5\u5177\ndocs \uff1a \u6e90\u6587\u4ef6\u4e2d\u672a\u63d0\u4f9b\uff0c\u4f46\u662f\u53ef\u4ee5\u81ea\u5df1\u6267\u884c tools\/jsduck\/run.bat\u6587\u4ef6\uff0c\u8bf7\u4e0d\u8981\u63d0\u4ea4\u6b64\u6587\u4ef6\u5939\n\n\u6253\u5305\n\u6e90\u6587\u4ef6\u7684\u7f16\u8bd1\u5305\u62ec\uff1a\n\n\u5408\u5e76js\uff0c\u538b\u7f29js\n\u7f16\u8bd1less\u751f\u6210 css,\u538b\u7f29css\n\u590d\u5236\u6587\u4ef6\uff0c\u5c06\u6240\u6709js\u5408\u5e76\u6210\u4e00\u4e2abui.js\n\u6267\u884cbuild.bat\u6587\u4ef6\n\n\u751f\u6210\u6587\u6863\uff1a\n\n\u4f7f\u7528jsduck \u8fdb\u884c\u7f16\u8bd1\u6587\u6863\uff0ctools\/jsduck\/run.bat\n\u914d\u7f6e\u6587\u4ef6\u5728tools\/jsduck\/config.json\n\u5982\u679c\u4e0d\u60f3\u914d\u7f6e\u73af\u5883\uff0c\u8bf7\u4e0b\u8f7d\u6587\u6863API\n\n\u6587\u6863\u5730\u5740\n\ndpl \u5730\u5740\n\u63a7\u4ef6\u5e93demo\n\u63a7\u4ef6\u5e93API\n\u96c6\u6210\u7684\u5e94\u7528\n\n\u63d0\u4ea4\u95ee\u9898\n\u63d0\u95ee\n\u8054\u7cfb\u6211\u4eec\n\n\u8bba\u575b\uff1ahttp:\/\/bbs.builive.com\n\u65fa\u65fa\u7fa4\u53f7\uff1a 778141976\nQQ\u7fa4\uff1a138692365\n\n","201":"JQuery EasyTabs Plugin\nTabs with(out) style.\nEasyTabs creates tabs with all the functionality, no unwanted changes\nto your markup, and no hidden styling.\nUnlike jQuery UI tabs, which style and arrange your tabs and panels for you, this plugin handles only the functionality of the tabs. By leaving the styling and layout up to you, it is much easier to style and arrange your tabs the way you want.\nWhat EasyTabs Does:\n\nCreates tabs from an unordered list, which link to divs on the page\nAllows complete customization of appearance, layout, and style via CSS\nSupports forward- and back-button in browsers\nTabs are bookmarkable and SEO-friendly\nTabs can be cycled at a specified interval\n\nWhat EasyTabs Does NOT Do:\n\nStyle your tabs in any way (though sensible CSS defaults can be found\nin the demos)\n\nShow Your Support\n\n\n\nShow your support for jQuery EasyTabs, by helping us raise money for the Karmanos Cancer\nInstitute.\n\n\n\n\n\n\nDocumentation\n\nInstallation\nStylization\nConfiguration Options\nDemos\n\nInstallation\nThe HTML\nUnlike JQuery UI tabs, the HTML markup for your tabs and content can be arranged however you want. At the minimum, you need a container, an unordered list of links for your tabs, and matching divs for your tabbed content.\n<div id=\"tab-container\">\n  <ul>\n    <li><a href=\"#tab-1-div\">Tab 1<\/a><\/li>\n    <li><a href=\"#that-other-tab\">The Second Tab<\/a><\/li>\n    <li><a href=\"#lastly\">Tab C<\/a><\/li>\n  <\/ul>\n  <div id=\"tab-1-div\">\n    <h2>Heading 1<\/h2>\n    <p>This is the content of the first tab.<\/p>\n  <\/div>\n    <div id=\"that-other-tab\">\n    <h2>Heading 2<\/h2>\n    <p>Stuff from the second tab.<\/p>\n  <\/div>\n  <div id=\"lastly\">\n    <h2>Heading 3<\/h2>\n    <p>More stuff from the last tab.<\/p>\n  <\/div>\n<\/div>\n\nThe Javascript\nTo enable back- and forward-button support for the users' browsers, be sure to include either the jQuery HashChange plugin (recommended) or the Address plugin before including the EasyTabs plugin. There is no other configuration required, it will just work!\n<script src=\"\/javascripts\/jquery.js\" type=\"text\/javascript\"><\/script> \n<script src=\"\/javascripts\/jquery.hashchange.js\" type=\"text\/javascript\"><\/script> \n<script src=\"\/javascripts\/jquery.easytabs.js\" type=\"text\/javascript\"><\/script>  \n\n<script type=\"text\/javascript\"> \n  $(document).ready(function(){ $('#tab-container').easytabs(); });\n<\/script> \n\nI varied the tab ids and names just to show you how flexible this is. There is no magic going on with this plugin; it's not trying to guess the order of your tabs or what tab is associated with which <div>. Just make the id of the content <div> match the href of the tab link.\nRequired Markup\nThe only rules you need to follow are these:\n\ncontaining <div> with a unique id\nthe container <div>\u00a0contains an unordered list <ul>\u00a0of links <a>\n\n(UPDATE: As of version 1.1, this is no longer the case. You can now include your tabs anywhere within the container. It can be a <ul>, <ol>, <div>, or anything you want. The default is still a top-level <ul>, so to change it you just specify your selector with the new \"tabs\" option.)\n\nthe container div also contains content divs (for the tabbed content), each div has a unique id\u00a0that matches the href property of a link in the unordered list\n\nOther than that, go nuts. The order of the elements does NOT matter. Your <ul> could be before or after the content divs (or even between them). You can put non-tabbed content between the elements. It doesn't matter. The most common structure (for inspiration's sake) is something like this:\ndiv#tab-container ul > ( li > a[href=\"tab-1\"], li > a[href=\"second-tab\"] )\ndiv#tab-container div#tab-1\ndiv#tab-container div#second-tab\n\n+---------------------------------------------------------------------------+\n|                              div#tab-container                            |\n|  +---------------------------------------------------------------------+  |\n|  |                                  ul                                 |  |\n|  |  +-----------------------------+    +----------------------------+  |  |\n|  |  |             li              |    |             li             |  |  |\n|  |  |  +-----------------------+  |    |  +----------------------+  |  |  |\n|  |  |  |    a[href=\"tab-1\"]    |  |    |  | a[href=\"second-tab\"] |  |  |  |\n|  |  |  +-----------------------+  |    |  +----------------------+  |  |  |\n|  |  +-----------------------------+    +----------------------------+  |  |\n|  +---------------------------------------------------------------------+  |\n|                                                                           |\n|  +---------------------------------------------------------------------+  |\n|  |                               div#tab-1                             |  |\n|  +---------------------------------------------------------------------+  |\n|                                                                           |\n|  +---------------------------------------------------------------------+  |\n|  |                             div#second-tab                          |  |\n|  +---------------------------------------------------------------------+  |\n|                                                                           |\n+---------------------------------------------------------------------------+\n\n\nFor stylization, configuration options, and live demos, see the EasyTabs homepage.\n\nLinks\n\nFull Documentation and Demos\nUpdates and new features for v1.1.2\nUpdates and new features for v2.0\nUpdates and new features for v2.1.2\nDownload jQuery EasyTabs\nFork and view source code\n\nInfo\n\nAuthor: Steve Schwartz\nCompany: Alfa Jango, LLC\nLicense: Dual licensed under the MIT and GPL licenses.\n\n","202":"\nVisit the Mimosa Website for all sorts of Mimosa documentation goodness.\nQuestions?\n\n\nGoogle Group\nTwitter @mimosajs\n\nQuick Start\nThere are a ton of docs on the site to get you started, but here's a quick start.\n\nInstall node.js.  A .10 version.\nnpm install -g mimosa\nmimosa new testproject\nFollow the prompts to choose some assets\ncd testproject\nmimosa watch -s\nhttp:\/\/localhost:3000\n\nNow you are ready to rock!\nMaybe you want to start with something other than an empty app?  Maybe you want to start with a preconfigured Ember app for instance?\nmimosa skel:list\nThat will get you a list of the skeletons currently available for Mimosa that might help jumpstart your app.\nLicense\n(The MIT License)\nCopyright (c) 2014 David Bashford\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n'Software'), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and\/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","203":"Sketch Mate\nThese plugins will make you best friends with Sketch.\nPlugin Directory\nArtboards\n\nDuplicate Artboard shift + \u2318 + D\nFit Artboard\nFit Artboard Height ctrl + shift + A\nRemove Artboard \u2318 + \u232b\nSort Artboards\n\nMisc\n\nGoto Page\nReplace Layer \u2318 + \u2325 + R\nSet Line Height\nToggle Click Through\nWrap in Bounding Box\n\nSmart Align\n\nDistribute Horizontally control + \u2318 + \u2325 + ,\nDistribute Vertically control + \u2318 + \u2325 + .\nSmart Align Horizontally \u2318 + \u2325 + ,\nSmart Align Vertically \u2318 + \u2325 + .\nSmart Align Bottom control + \u2318 + \u2193\nSmart Align Left control + \u2318 + \u2190\nSmart Align Right control + \u2318 + \u2192\nSmart Align Top control + \u2318 + \u2191\nSpace Horizontally\nSpace Vertically\nStretch Height \u2318 + \u2325 + e\nStretch Width \u2318 + e\n\nSmart Move\n\nDelete and Pull\nPull Left shift + \u2318 + \u2325 + \u2190\nPull Up shift + \u2318 + \u2325 + \u2191\nPush Down shift + \u2318 + \u2325 + \u2193\nPush Right shift + \u2318 + \u2325 + \u2192\nSet Increments shift + \u2318 + \u2325 + I\n\nSort\n\nReverse Layer Order\nReverse Positions\nSort Artboards by Name\nSort Layers ctrl + \u2318 + \u2325 + S\n\nInstallation\nTo install all plugins, download them all first, unzip the archive, and place the folder contents in your Sketch Plugins folder by navigating to Sketch > Plugins > Reveal Plugins Folder\u2026\nTo install only a selection of plugins, you will first need to place the library file inventory.js in the root of your Sketch Plugins directory. This is very important as all plugins rely on its functionality.\nYou can then install selected plugins by double-clicking the file, or alternatively, drag and drop the file onto the Sketch app icon. This will automatically copy the plugin to your Sketch Plugins folder.\nKeyboard Shortcuts\nMost plugins have a pre-defined keyboard shortcut. You can always change it by editing the shortcut written in parenthesis at the end of the first line of a plugin.\nFor example, the first line of Duplicate Artboard.sketchplugin:\n\n\/\/ Duplicates the current artboard right next to it. (shift command d)\n\nYou can use modifier keys such as option, command, control, shift\nArtboards\nDuplicate Artboard (next to the current artboard)\nThis improves the built in behavior of artboard duplication in Sketch. If the selected artboard is in the middle of other artboards, all artboards on the right side will be shifted to the right before the artboard is duplicated. Requires any layer of an artboard to be selected.\nShortcut: shift + \u2318 + D\n\nRemove Artboard\nThis improves the built in behavior of artboard removal in Sketch. If the selected artboard is in the middle of other artboards, all artboards on the right side will be shifted to the left after the artboard has been removed. Requires any layer of an artboard to be selected.\nShortcut: \u2318 + \u232b (Backspace)\n\nFit Artboard\nResizes the artboard to fit its layers.\nFit Artboard Height\nResizes the artboard to fit the height of its layers.\nShortcut: shift + ctrl + A\n\nSort Artboards\nSorts selected artboard layers by their horizontal position. Useful when your layer list does not reflect the artboard arrangement on your canvas.\nText\nSet Line Height\nPlugin that allows you to set the line height of a text layer as a multiple of the font size. It\u2019s like using em in CSS. Supports multiple selections.\nShortcut: \u2318 + L\n\nMisc\nReplace Layer\nReplaces the selected layer with the content in the clipboard. Basically this plugin does paste in place while removing the original selection.\nShortcut: \u2325 + cmd + R\n\nSmart Align\nDistribute Horizontally\nCalls the menu command \"Distribute Horizontally\". Just for shortcut purposes.\nShortcut: ctrl + \u2318 + \u2325 + ,\nDistribute Vertically\nCalls the menu command \"Distribute Vertically\". Just for shortcut purposes.\nShortcut: ctrl + \u2318 + \u2325 + .\nSmart Align Horizontally\nexperimental Aligns the selected layer relative to its parent group.\nShortcut: \u2318 + \u2325 + ,\nSmart Align Vertically\nexperimental Aligns the selected layer relative to its parent group.\nShortcut: \u2318 + \u2325 + .\nSpace Horizontal\nDistributes the selected elements horizontally, with the same distante beetween them. If only one layer is selected, the layer will be moved by the spacing that has been input.\n\nSpace Vertical\nDistributes the selected elements vertically, with the same distante beetween them. If only one layer is selected, the layer will be moved by the spacing that has been input.\nSmart Move\nexperimental Allows you to pull or push layers in relation to the selected layer.\n\nPull Left shift + \u2318 + \u2325 + \u2190\nPull Up shift + \u2318 + \u2325 + \u2191\nPush Down shift + \u2318 + \u2325 + \u2193\nPush Right shift + \u2318 + \u2325 + \u2192\n\n\nSorting\nSort Layers\nThere are also plugins to reverse the order of the layers in the layer list and a plugin that reverses the position of the selected layers on the artboard.\nOptions: Text (A->Z), Text (Z->A), Layer Name (A->Z), Layer Name (Z->A), Top, Left, Random\nShortcut: ctrl + \u2318 + \u2325 + S\nSorting Layers by text, visually\n\nSorting Layers in the layer list by position\n\n","204":"Meteor Collection Helpers\nCollection helpers automatically sets up a transformation on your collections using Meteor's Mongo.Collection transform option, allowing for simple models with an interface that's similar to template helpers.\nInstallation\n$ meteor add dburles:collection-helpers\nUsage\nWrite your helpers somewhere seen by both client and server.\nBooks = new Mongo.Collection('books');\nAuthors = new Mongo.Collection('authors');\n\nBooks.helpers({\n  author() {\n    return Authors.findOne(this.authorId);\n  }\n});\n\nAuthors.helpers({\n  fullName() {\n    return `${this.firstName} ${this.lastName}`;\n  },\n  books() {\n    return Books.find({ authorId: this._id });\n  }\n});\nThis will then allow you to do:\nBooks.findOne().author().firstName; \/\/ Charles\nBooks.findOne().author().fullName(); \/\/ Charles Darwin\nAuthors.findOne().books()\nOur relationships are resolved by the collection helper, avoiding unnecessary template helpers. So we can simply write:\nTemplate.books.helpers({\n  books() {\n    return Books.find();\n  }\n});\n...with the corresponding template:\n<template name=\"books\">\n  <ul>\n    {{#each books}}\n      <li>{{name}} by {{author.fullName}}<\/li>\n    {{\/each}}\n  <\/ul>\n<\/template>\nMeteor.users\nYou can also apply helpers to the Meteor.users collection\nMeteor.users.helpers({\n  \/\/ ...\n});\nApplying the transformation function\nSometimes it may be useful to apply the transformation directly to an object.\nvar doc = {\n  firstName: 'Charles',\n  lastName: 'Darwin'\n};\n\nvar transformedDoc = Authors._transform(doc);\n\ntransformedDoc.fullName(); \/\/ Charles Darwin\nLicense\nMIT\n","205":"**NOTE: this project is no longer actively maintained and not recommended for use. It is left here for reference. **\nKanso\nSimple, distributable JavaScript apps using CouchDB\n\nKanso is a set of tools and packages for creating JavaScript apps that run\ndirectly on CouchDB. Your app and related code can be easily packaged and shared\nwith the community, or deployed to a number of cloud-hosted services.\nThe command-line tool is used for building and deploying these applications, as\nwell as providing some useful utilities for working with CouchDB and JSON data.\nWhy develop apps with Kanso?\n\nScalability: easily grow from hobby-project to high-demand app with CouchDB\nDeployment: effortless to deploy, to the cloud or local machines, promoting\nquick iterations\nMulti-platform: runs anywhere with CouchDB (Windows, OSX, Linux, Android and\niOS)\nKeep your data yours: now you can keep sensitive data in-house, and avoid\nsharing it with cloud services\nEasy to distribute: apps are easy to share and distribute, between people and\nservers\nAvoid lock-in: easy to deploy and based on open-source, don't be held hostage\nto a proprietary API\nHomogeneity: your development environment matches your production environment\nOne language to rule them all: with just CouchDB and the browser, all you\nneed to speak is JavaScript!\n\nGet started\nsudo npm install -g kanso\n\nLearn more: Simplest possible app\nDeveloper community\nKanso is an open-source project written by developers using CouchApps everyday, in\nreal-world projects. Kanso provides the tools to share code and resources,\nregardless of the way in which your app is built. It's easy to use coffee-script,\nless stylesheets, or any number of JavaScript frameworks in your app, and still\nbenefit from the wealth of packages provided by the community.\nFind other Kanso developers in #kansojs on FreeNode, or on the mailing list.\nFind out more\nFor more information on the project, check out the Kanso website.\n","206":"http-client  \nhttp-client lets you compose HTTP clients using JavaScript's fetch API. This library has the following goals:\n\nPreserve the full capabilities of the fetch API\nProvide an extendable middleware API\nUse the same API on both client and server\n\nInstallation\nUsing npm:\n$ npm install --save http-client\n\nhttp-client requires you to bring your own global fetch function (for convenience when using the top-level createFetch function). isomorphic-fetch is a great polyfill if you need to support environments that don't already have a global fetch function.\nThen, use as you would anything else:\n\/\/ using ES6 modules\nimport { createFetch } from 'http-client'\n\n\/\/ using CommonJS modules\nvar createFetch = require('http-client').createFetch\nThe UMD build is also available on unpkg:\n<script src=\"https:\/\/unpkg.com\/http-client\/umd\/http-client.min.js\"><\/script>\nYou can find the library on window.HTTPClient.\nUsage\nhttp-client simplifies the process of creating flexible HTTP clients that work in both node and the browser. You create your own fetch function using the createFetch method, optionally passing middleware as arguments.\nimport { createFetch, base, accept, parse } from 'http-client'\n\nconst fetch = createFetch(\n  base('https:\/\/api.stripe.com\/v1'),  \/\/ Prefix all request URLs\n  accept('application\/json'),         \/\/ Set \"Accept: application\/json\" in the request headers\n  parse('json')                       \/\/ Read the response as JSON and put it in response.body\n)\n\nfetch('\/customers\/5').then(response => {\n  console.log(response.jsonData)\n})\nTop-level API\ncreateFetch(...middleware)\nCreates a fetch function that uses some middleware. Uses the global fetch function to actually make the request.\ncreateStack(...middleware)\nCombines several middleware into one, in the same order they are provided as arguments. Use this function to create re-usable middleware stacks or if you don't want to use a global fetch function.\nenableRecv(fetch)\nReturns an \"enhanced\" version of the given fetch function that knows how to run response handlers registered using recv. This is only really useful when using stacks directly instead of createFetch.\nMiddleware\nhttp-client provides a variety of middleware that may be used to extend the functionality of the client. Out of the box, http-client ships with the following middleware:\naccept(contentType)\nAdds an Accept header to the request.\nimport { createFetch, accept } from 'http-client'\n\nconst fetch = createFetch(\n  accept('application\/json')\n)\nauth(value)\nAdds an Authorization header to the request.\nimport { createFetch, auth } from 'http-client'\n\nconst fetch = createFetch(\n  auth('Bearer ' + oauth2Token)\n)\nbase(baseURL)\nAdds the given baseURL to the beginning of the request URL.\nimport { createFetch, base } from 'http-client'\n\nconst fetch = createFetch(\n  base('https:\/\/api.stripe.com\/v1')\n)\n\nfetch('\/customers\/5') \/\/ GET https:\/\/api.stripe.com\/v1\/customers\/5\nbody(content, contentType)\nSets the given content string as the request body.\nimport { createFetch, body } from 'http-client'\n\nconst fetch = createFetch(\n  body(JSON.stringify(data), 'application\/json')\n)\ndebug()\nAdds a debug property to the response or error object so you can inspect them. Mainly useful for testing\/debugging (should run after all other middleware).\nimport { createFetch, debug } from 'http-client'\n\nconst fetch = createFetch(\n  \/\/ ... other middleware\n  debug()\n)\n\nfetch(input).then(response => {\n  console.log(response.debug.input, response.debug.options)\n})\nheader(name, value)\nAdds a header to the request.\nimport { createFetch, header } from 'http-client'\n\nconst fetch = createFetch(\n  header('Content-Type', 'application\/json')\n)\ninit(propertyName, value)\nSets the value of an arbitrary property in the options object.\nimport { createFetch, init } from 'http-client'\n\nconst fetch = createFetch(\n  init('credentials', 'include')\n)\njson(object)\nAdds the data in the given object as JSON to the request body.\nmethod(verb)\nSets the request method.\nimport { createFetch, method } from 'http-client'\n\nconst fetch = createFetch(\n  method('POST')\n)\nparams(object)\nAdds the given object to the query string of GET\/HEAD requests and as a x-www-form-urlencoded payload on all others.\nimport { createFetch, method, params } from 'http-client'\n\n\/\/ Create a client that will append hello=world to the URL in the query string\nconst fetch = createFetch(\n  params({ hello: 'world' })\n)\n\n\/\/ Create a client that will send hello=world as POST data\nconst fetch = createFetch(\n  method('POST'),\n  params({ hello: 'world' })\n)\nparse(parser, as = 'body')\nReads the response body to completion, parses the response, and puts the result on response.body (or whatever as is). parser must be the name of a valid Body parsing method. The following parsers are available in the spec:\n\narrayBuffer\nblob\nformData\njson\ntext\n\nimport { createFetch, parse } from 'http-client'\n\nconst fetch = createFetch(\n  parse('json')\n)\n\nfetch(input).then(response => {\n  console.log(response.body)\n})\nNote: Some parsers may not be available when using a fetch polyfill. In particular if you're using node-fetch, you should be aware of its limitations.\nquery(object)\nAdds the data in the given object (or string) to the query string of the request URL.\nrecv(handler)\nUsed to handle the response in some way. The handler function should return the new response value, or a promise for it. Response handlers run in the order they are defined.\nimport { createFetch, recv } from 'http-client'\n\nconst fetch = createFetch(\n  recv(response => (console.log('runs first'), response)),\n  recv(response => (console.log('runs second'), response))\n)\nStacks\nMiddleware may be combined together into re-usable middleware \"stacks\" using createStack. A stack is itself a middleware that is composed of one or more other pieces of middleware. Thus, you can pass a stack directly to createFetch as if it were any other piece of middleware.\nThis is useful when you have a common set of functionality that you'd like to share between several different fetch methods, e.g.:\nimport { createFetch, createStack, header, base, parse, query } from 'http-client'\n\nconst commonStack = createStack(\n  header('X-Auth-Key', key),\n  header('X-Auth-Email', email),\n  base('https:\/\/api.cloudflare.com\/client\/v4'),\n  parse('json')\n)\n\n\/\/ This fetch function can be used standalone...\nconst fetch = createFetch(commonStack)\n\n\/\/ ...or we can add further middleware to create another fetch function!\nconst fetchSinceBeginningOf2015 = createFetch(\n  commonStack,\n  query({ since: '2015-01-01T00:00:00Z' })\n)\nStacks are also useful when you don't have a global fetch function, e.g. in node. In those cases, you can still use http-client middleware and supply your own fetch (we recommend node-fetch) function directly, but make sure you \"enhance\" it first:\nconst { createStack, enableRecv, header, base } = require('http-client')\n\n\/\/ We need to \"enhance\" node-fetch so it knows how to\n\/\/ handle responses correctly. Specifically, enableRecv\n\/\/ gives a fetch function the ability to run response\n\/\/ handlers registered with recv (which parse, used below,\n\/\/ uses behind the scenes).\nconst fetch = enableRecv(\n  require('node-fetch')\n)\n\nconst stack = createStack(\n  header('X-Auth-Key', key),\n  header('X-Auth-Email', email),\n  base('https:\/\/api.cloudflare.com\/client\/v4'),\n  parse('json')\n)\n\nstack(fetch, input, options)\n","207":"TouchScroll\nTouchScroll is a JavaScript\/CSS 3-based scrolling layer for Webkit Mobile, espeacially iPhone, Android, and iPad. It allows to configure scrolling behaviour in many ways and to use fixed interface elements.\nDependencies\nTouchScroll depends on css-beziers, a library for computations on cubic bezier curves.\nUsage\nTo use TouchScroll you need an element with fixed height. Have a look at the demo for an elegant solution using display: -webkit-box.\nThe stylesheet is mandatory at the moment. It will be made optional in the future for cases when scrollbars aren\u2019t needed.\n<link rel=\"stylesheet\" src=\"touchscroll.css\">\n<!-- \u2026 -->\n<div id=\"scroller\">\n    <!-- contents go here -->\n<\/div>\n<script src=\"css-beziers.js\"><\/script>\n<script src=\"touchscroll.js\"><\/script>\n<script>\n    var scroller = new TouchScroll(document.querySelector(\"#scroller\"));\n<\/script>\n\nTo enable the elasticity\/bouncing effect, add {elastic: true} as second parameter to the instantiation:\n<script>\n    var scroller = new TouchScroll(document.querySelector(\"#scroller\"), {elastic: true});\n<\/script>\n\nSet the scroller to overflow: auto to enable scrolling in other environments.\nThe scroller automatically adapts its size to content changes and window resizes\/orientation changes.\nLimitations\/Known Issues\n\nTouchScroll currently doesn\u2019t work well with forms on Android.\nThe scroller element shouldn\u2019t have any padding.\nBecause two wrapper <div>s are inserted inside of the scroller, the CSS\nchild selector (#scroller > foo) might not work as expected.\nWhen a scroller is invisible, it can\u2019t adapt its size correctly. Call its setupScroller method to fix that (e.g. after making a scroller visible by setting display: block on it).\nTapping the status bar on iPhone doesn\u2019t trigger \u201cscroll to top\u201d.\nSelecting text doesn\u2019t work on the iPad and on some iPhone versions (OS 4.0b2) \u2013 an issue with cancelling events?\n\nTo Do\n\nKeep the scrollbars round while bouncing \u2013 I already know how to do this.\nInvestigate whether support for tapping the status bar on iPhone can be added.\nInvestigate how selecting text and using the context menu can be re-enabled on iPhone\/iPad.\nAdd an option to completely switch off scrollbars.\nFind a solution to the event problems on Android \u2013 help greatly appreciated!\n\nContact\nE-Mail: da AT uxebu.com\nTwitter: @void_0\n","208":"Keshif\nThis repostory is not actively maintained.\nTo access the most recent version and the online platform, visit www.keshif.me\nKeshif is a web-based visualization and analytics tool that lets you explore datasets quickly.\nLicense\nBSD 3 clause (c) University of Maryland 2014-2016\nAuthor\nMehmet Adil Yalcin @ HCIL, University of Maryland, College Park\nFunded in part by Huawei (2013-2014).\n","209":"Xiki plugin for Sublime Text 2\n\nBefore use: Install Xiki on your machine http:\/\/xiki.org\/\nInstall Package Control if you don't have it.\nInstall the  SublimeXiki package.\nTo use: open the command pallete (cmd+shift+p or ctrl+shift+p) and use Create Xiki Buffer\n\nHotkeys:\n\ncmd+enter: run or collapse the highlighted command\/menu.\ncmd+shift+enter: run the current command, and place the cursor after the output.\n\nIf used on a directory or file, will indent to the subdirectory level and create a command prompt ($)\nIf used on a command prompt, will maintain the current indentation and create a prompt ($$ or $)\n\n\n\nUseful SublimeXiki commands:\n\n\/ or ~: start a directory transversal tree.\n\n~ starts at your home directory\nYou can also type a more complete path like \/path\/to\/dir or ~\/path\n\n\n$: run a command directly (does not invoke a shell)\n$$: run a command using your default shell (allows pipes, redirection, logic, etc)\n\nUseful Xiki commands:\n\ndocs\nmysql\nmongo\n\n","210":"DidIStealThis\nA small script made to idenitfy wether or not code is stolen\n","211":"HttpProxyMiddleware\nA middleware for scrapy. Used to change HTTP proxy from time to time.\nInitial proxyes are stored in a file. During runtime, the middleware\n  will fetch new proxyes if it finds out lack of valid proxyes.\nRelated blog: http:\/\/www.kohn.com.cn\/wordpress\/?p=208\nfetch_free_proxyes.py\nUsed to fetch free proxyes from the Internet. Could be modified by\n  youself.\nUsage\nsettings.py\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 351,\n    # put this middleware after RetryMiddleware\n    'crawler.middleware.HttpProxyMiddleware': 999,\n}\n\nDOWNLOAD_TIMEOUT = 10           # 10-15 second is an experienmental reasonable timeout\nchange proxy\nOften, we wanna change to use a new proxy when our spider gets banned.\n  Just recognize your IP being banned and yield a new Request in your\n  Spider.parse method with:\nrequest.meta[\"change_proxy\"] = True\nSome proxy may return invalid HTML code. So if you get any exception\n  during parsing response, also yield a new request with:\nrequest.meta[\"change_proxy\"] = True\nspider.py\nYour spider should specify an array of status code where your spider\n  may encouter during crawling. Any status code that is not 200 nor in\n  the array would be treated as a result of invalid proxy and the proxy\n  would be discarded. For example:\nwebsite_possible_httpstatus_list = [404]\nThis line tolds the middleware that the website you\u2019re crawling would\n  possibly return a response whose status code is 404, and do not\n  discard the proxy that this request is using.\nTest\nUpdate HttpProxyMiddleware.py path in\n  HttpProxyMiddlewareTest\/settings.py.\ncd HttpProxyMiddlewareTest\nscrapy crawl test\nThe testing server is hosted on my VPS, so take it easy\u2026 DO NOT\n  waste too much of my data plan.\nYou may start your own testing server using IPBanTest which is powered\n  by Django.\n","212":"Using IBM Watson's Speech-to-Text API to do Multi-Threaded Transcription of Really Long and Talky Videos, Such as Presidential Debates\nA demonstration of how to use Python and IBM Watson's Speech-to-Text API to do some decently accurate transcription of real-world video and audio, at amazingly fast speeds.\nNote:_ I'm just spit-balling code here, not making a user-friendly package.  I'm focused on making an automated workflow to create fun supercuts of \"The Wire\"...and will polish the scripts and implementation later. These notes and scripts (and data files) are merely for your reference.\ntl;dr\nIBM Watson offers a REST-based Speech to Text API that allows free usage for the first 1,000 minutes each month (and $0.02 for each additional minute):\n\nWatson Speech to Text can be used anywhere there is a need to bridge the gap between the spoken word and its written form. This easy-to-use service uses machine intelligence to combine information about grammar and language structure with knowledge of the composition of an audio signal to generate an accurate transcription. It uses IBM's speech recognition capabilities to convert speech in multiple languages into text. The transcription of incoming audio is continuously sent back to the client with minimal delay, and it is corrected as more speech is heard.\n\nIn my preliminary tests, it's not quite as good as Google Translate in terms of pure accuracy, but it's more than good enough for finding key words, whether they be relatively common verbs like \"fight\", \"death\", \"kill\" or proper nouns, such as Obama and countries of the world.\nBut it doesn't do too badly on very common (and aurally-ambiguous) short words such as pronouns and articles. Because Watson provides a confidence level for each word, it's possible to write scripts to programmatically filter out ambiguous words.\nHere's a YouTube playlist of some automated supercuts I've created from the U.S. presidential primary debates. My favorite is probably this supercut of Senator Sanders and Secretary Clinton saying fighting words.\n\nHere's the JSON returned from Watson, which includes word-by-word timestamps and confidence levels. Here's a simplified version of it, in which the JSON is just a flat list of words.\nIBM Watson's API is robust enough to accept many concurrent requests. In the sample scripts I've included in this repo, I was able to break up a 90 minute debate into 5 minute segments and send them up to Watson simultaneously...resulting in a 6 to 7 minute processing time for the entire 90 minutes.\nSome non-presidential examples:\n\nAttempting to transcribe the profanities in The Wire's \"Old Cases\" episode -- (youtube supercut, obviously nsfw)\nAttempting to transcribe a ProPublica podcast\n\nQuick *nix check!\nBefore you look at the scary Python framework I've built for myself, you should first if you can work with movie\/audio files and connect to Watson, using nothing but Unix tools: ffmpeg, and good ol' curl: check out this brief walkthrough\nSupercut fun\nYou probably want to see the final product. I'm too lazy to document all the code and haven't organized it yet, but here's one result: making supercuts by grepping the Watson Speech to Text data for certain words. For example, to find all \"fighting words\", e.g. war, wars, warriors, fight, bomb, kill, threat, terror, death, murder, torture:\n python supercut.py republican-debate-sc-2016-02-13 '\\bwar(?:riors?|s)?\\b|fight|bomb|kill|threat|terror|death|murder|tortur'\nHere's a playlist of sample supercuts of presidential people:\nRepublican Debate, South Carolina, 2016-02-13:\n\nPEOPLE\nAmerica, and other geopolitical words\nObama, Clinton, and Bush\nFighting words (fight, bomb, kill)\nNegative words (wrong, bad)\nPositive words (good, best)\n\nDemocratic Debate, Wisconsin, 2016-02-11:\n\nPEOPLE\nAmerica, and other geopolitical words\nObama, Clinton, and Bush\nFighting words (fight, bomb, kill)\nBut, Why, Not, Yes, Now\n\nObama weekly address\n\ncriminal, justice, reform\nwho, what, when, where, why, how (original video)\n\n\nThe technical details\nHow it works\nAfter you've downloaded a video file to disk, the assorted scripts and commands in this repo will:\n\nConvert the file to mp4 if necessary\nCreate a project subfolder to store the video file and all derived audio and transcripts file\nExtract the audio from as 16-bit, 16khz WAV files\nSplit the audio into segments (300 seconds each, by default)\nSend each of those segments to Watson's API to be analyzed and transcribed.\nSaves the raw responses from Watson's API for each audio file\nCompiles all of the resulting responses into one data file, as if you had sent the entire audio file to be analyzed in a single go.\n\nThe advantages of splitting up the audio is that it allows the transcription to be done in parallel. An hour-long audio track would take probably an hour to get a response back (if your internet connection doesn't fail), whereas 60 parallel requests to analyze 1-minute each will take roughly...1 minute to complete.\nI haven't tested the upper-bounds in concurrent requests to Watson's API, though I was able to send around 30 5-minute requests all at once without getting an errors.\nHere are some sample results in the projects\/ folder:\n\nThe Republican Presidential Debate, South Carolina, Feb. 13, 2016\nDonald Trump's \"Live Free or Die\" commercial\nPresident Obama's Weekly Video Address, Oct. 31, 2015\n\nRequirements\nIBM Watson\nThe transcription power comes from IBM Watson's Speech-to-Text REST API. After cutting up a video into 5-minute segments, I then upload all of the audio files in parallel to Watson, which can complete the entire batch in nearly just 5 minutes.\n\nLive Watson Speech-to-Text demo\nWatson's Speech-to-Text documentation\nAPI reference\n\nGetting started with IBM Bluemix\nYou have to sign up for an IBM Bluemix account, which is free and doesn't require a credit card for the first month.\nAfter signing up for Bluemix, you can find the console page for the speech-to-text API here, where you can get user credentials. This repo contains a sample file: credsfile_watson.SAMPLE.json\nThe pricing is pretty generous, in terms of testing things out: 1,000 minutes free each month. Every additional minute is $0.02 -- i.e. transcribing an hour's worth of audio will cost $1.20.\nQuickie Watson Testy!\nBefore you get into the Python stuff, you should see if you are properly initialized with Watson by making contact with it from the command-line (i.e. bash, i.e. uh not sure if it will work on Windows like this):\nIf you don't have a WAV file at hand, you can install the youtube-dl command-line tool:\n$ pip install youtube-dl\n\nAnd then download Trump's Live Free or Die commercial. The following command downloads a movie file, bb4TxjvQlh0.mkv, and extracts a WAV file named bb4TxjvQlh0.wav:\nyoutube-dl \"https:\/\/www.youtube.com\/watch?v=bb4TxjvQlh0\" \\\n  --keep-video \\\n  --extract-audio \\\n  --audio-format wav \\\n  --audio-quality 16K \\\n  --id\nIn the next step, I assume you have a file named bb4TxjvQlh0.wav, but you are free to use any WAV audio file.\n(Note: the whole movie-file thing is totally ancillary...Watson doesn't care if the audio file comes from a movie or you recording into your microphone or whatever. But people like to transcribe videos, which is why I include the step.)\nThis next step is what contacts Watson's API. Replace USERNAME and PASSWORD with whatever credentials you got from the IBM Bluemix Developer Panel.\nThe --data-binary flag wants a file name (prepended with @).\nWhen the audio file is uploaded and Watson returns a response, it will be saved to transcript.json\ncurl -X POST \\\n     -u USERNAME:PASSWORD     \\\n     -o transcript.json        \\\n     --header \"Content-Type: audio\/wav\"    \\\n     --header \"Transfer-Encoding: chunked\" \\\n     --data-binary \"@bb4TxjvQlh0.wav\"        \\\n     \"https:\/\/stream.watsonplatform.net\/speech-to-text\/api\/v1\/recognize?continuous=true&timestamps=true&word_confidence=true&profanity_filter=false\"\nIf this doesn't work for you, then either your Internet is down, Watson is down, or you don't have the proper user\/password credentials.\nPython stuff\nThis project uses:\n\nAnaconda 3-2.4.0\nPython 3.5.1\nRequests\nmoviepy - currently, just being used as a very nice wrapper around ffmpeg, to do audio-video conversion and extraction. But has a lot of potential for laughter and games via programmatic editing.\n\nmoviepy will install ffmpeg if you don't already have it installed\n\n\n\nDemonstrations\nRepublican Debate in South Carolina, Feb. 13, 2016\nCheck out the projects\/republican-debate-sc-2016-02-13 folder in this repo to see the raw JSON response files and their corresponding .WAV audio, as extracted from the Feb. 13, 2016 Republican Presidential Candidate debate in South Carolina:\n\n\n\nDonald Trump \"Live Free or Die\" commercial (39 seconds)\nThe commercial can be seen here on YouTube:\n\n\n\nThe project directory generated: projects\/trump-nh\/\nBecause the video is so short, the directory includes the video file, the extracted audio, as well as the segmented audio and raw Watson JSON responses. For this example, I made the segments 10 seconds long.\nTo compile the transcript text:\nimport json\nfrom glob import glob\nfilenames = glob(\".\/projects\/trump-nh\/transcripts\/*.json\")\n\nfor fn in filenames:\n  with open(fn, 'r') as t:\n      data = json.loads(t.read())\n      for x in data['results']:\n          best_alt = x['alternatives'][0]\n          print(best_alt['transcript'])\nThe result:\n\nthis great slogan of the Hampshire live free or die means so much\nso many people all over the world they use that expression it means liberty it means freedom it means free enterprise\nmean safe\nthe insecurity it means borders it means strong strong military where nobody's going to mess with us it means taking care of our vets\nwhat a great slogan congradulations New Hampshire\n\n\nwonderful job dnmt\nI\nand\n\nNote that the last 3 tokens, dmnt I and, are a result of the Watson API getting confused by the dramatic music that closes the commercial. Luckily, the JSON response includes, among timestamp data for each work, a confidence level as well.\nIt actually is spot on for Trump's full closing sentence (not sure why \"congradulations\" is used...)...the confidence levels for dmnt I and were very low comparatively...I think dmnt is some kind of code word used by the API to indicate something, not that Watson thought that dmnt was actually said (see the full JSON response here)\n{\n    \"word_confidence\": [\n        [\n            \"what\",\n            0.9999999999999674\n        ],\n        [\n            \"a\",\n            0.9999999999999672\n        ],\n        [\n            \"great\",\n            0.999999999999967\n        ],\n        [\n            \"slogan\",\n            0.9964234383591973\n        ],\n        [\n            \"congradulations\",\n            0.7798716606178608\n        ],\n        [\n            \"New\",\n            0.9999999999999933\n        ],\n        [\n            \"Hampshire\",\n            0.9845177369977128\n        ]\n    ]\n}\nPresident Obama weekly address for October 31, 2015 (3 minutes)\nHere's a quick demonstration of Watson's accuracy given a weekly video address from President Obama (~3 minutes):\n\nVideo landing page at Whitehouse.gov\nVideo file: 103115_WeeklyAddress.mp4\nAudio file: 00000-00190.wav\nWatson JSON response: 00000-00190.json\nThe produced file folder: projects\/obama-weekly-address-2015-10-31\/\n\n(because President Obama's video address is just about 3 minutes long, only audio file is extracted, and only one call to Watson's API is made)\nRight now there's just a bunch of sloppy scripts that need to be refactored. There's a script named init.py that you can run from the command-line that will read an existing video file, create a project folder, cut up the audio, and do the transcriptions. It assumes that you have a file named credsfile_watson.json relative to init.py.\nSome code for the commandline, to download the file, then to run init.py:\ncurl -o \"\/tmp\/obama-weekly-address-2015-10-31.mp4\" \\\n  https:\/\/www.whitehouse.gov\/WeeklyAddress\/2015\/103115-QREDSC\/103115_WeeklyAddress.mp4\n\npython init.py \/tmp\/obama-weekly-address-2015-10-31.mp4\nThe output produced by init.py:\n[MoviePy] Writing audio in \/Users\/dtown\/watson-word-watcher\/projects\/obama-weekly-address-2015-10-31\/full-audio.wav\n[MoviePy] Done.                                                                                            \n[MoviePy] Writing audio in \/Users\/dtown\/watson-word-watcher\/projects\/obama-weekly-address-2015-10-31\/audio-segments\/00000-00190.wav\n[MoviePy] Done.  \n\nTranscribe\nThe biggest bottleneck is transcribing the audio. The transcribe.py script does all the transcription in one big go:\npython transcribe.py projects\/obama-weekly-address-2015-10-31\n\nSending to Watson API:\n   \/Users\/dtown\/watson-word-watcher\/projects\/obama-weekly-address-2015-10-31\/audio-segments\/00000-00190.wav\nTranscribed:\n   \/Users\/dtown\/watson-word-watcher\/projects\/obama-weekly-address-2015-10-31\/transcripts\/00000-00190.json\n\nAnd then run these scripts for a quickie processing of the JSON transcript:\npython compile.py projects\/obama-weekly-address-2015-10-31\npython rawtext.py projects\/obama-weekly-address-2015-10-31\npython analyze.py projects\/obama-weekly-address-2015-10-31\n\nThe output:\n\nhi everybody today there are two point two million people behind bars in America and millions more on parole or probation\nevery year we spend eighty billion\nin taxpayer dollars\nkeep people incarcerated\nmany are nonviolent offender serving unnecessarily long sentences\nI believe we can disrupt the pipeline from underfunded schools overcrowded jails\nI believe we can address the disparities in the application of criminal justice from arrest rates to sentencing to incarceration\nand I believe we can help those who have served their time and earned a second chance\nget the support they need to become productive members of society\nthat's why over the course of this year I've been talking to folks around the country about reforming our criminal justice system\nto make it smarter fairer and more effective\nin February I sat down in the oval office with police officers from across the country\nin the spring\nI met with police officers and young people in Camden New Jersey where they're using community policing and data to drive down crime\nover the summer I visited a prison in Oklahoma to talk with inmates and correction officers about rehabilitating prisoners\npreventing more people from ending up there in the first place\ntwo weeks ago I visit West Virginia to meet with families battling prescription drug heroin abuse\nas well as people who are working on new solutions for treatment and rehabilitation\nlast week I traveled to Chicago to thank police chiefs from across the country for all that their officers do to protect Americans\nto make sure they get the resources they need to get the job done\nand to call for common sense gun safety reforms that would make officers and their communities safe\nwe know that having millions of people in the criminal justice system without any ability to find a job after release is unsustainable\nit's bad for communities and it's bad for our economy\nso on Monday I'll travel to Newark New Jersey to highlight efforts to help Americans\npaid their debt to society re integrate back into their communities\neveryone has a role to play for businesses that are hiring ex offenders\nto philanthropies they're supporting education and training programs\nand I'll keep working with people in both parties to get criminal justice reform bills to my desk\nincluding a bipartisan bill that would reduce mandatory minimums for nonviolent drug offenders and reward prisoners\nshorter sentences if they complete programs that make them less likely\ncommit a repeat offense\nthere's a reason good people across the country are coming together to reform our criminal justice system\nbecause it's not about politics\nit's about whether we as a nation live up to our founding ideals of liberty and justice for all\nand working together we can make sure that we do\nthanks everybody have a great weekend and have a safe and happy Halloween\n\nYou can compare it to the transcript here.\n","213":"Think of it as flatpages for small bits of reusable content you might want to insert into your templates and manage from the admin interface.\nThis is really nothing more than a model and a template tag.\nBy adding chunks to your installed apps list in your Django project and performing a .\/manage.py syncdb, you'll be able to add as many \"keyed\" bits of content chunks to your site.\nThe idea here is that you can create a chunk of content, name it with a unique key (for example: home_page_left_bottom) and then you can call this content from a normal template.\nWhy would anyone want this?\nWell it essentially allows someone to define \"chunks\" (I had wanted to call it blocks, but that would be very confusing for obvious reasons) of content in your template that can be directly edited from the awesome Django admin interface.  Throwing a rich text editor control on top of it make it even easier.\nUsage:\n{% load chunks %}\n<html>\n  <head>\n    <title>Test<\/title>\n  <\/head>\n  <body>\n    <h1> Blah blah blah<\/h1>\n    <div id=\"sidebar\">\n        ...\n    <\/div>\n    <div id=\"left\">\n        {% chunk \"home_page_left\" %}\n    <\/div>\n    <div id=\"right\">\n        {% chunk \"home_page_right\" %}\n    <\/div>\n  <\/body>\n<\/html>\n\nif you need the Chunk object in the template (maybe you've added some generic relations to it) you should use the {% get chunk %} templatetag:\n{% load chunks %}\n\n{% get_chunk \"home_page_left\" as chunk_obj %}\n\n<!-- ... use the Chuck object, then display it's contents -->\n<div>{{ chuck_obj.content }}<\/div>\n\nThis is really helpful in those cases where you want to use django.contrib.flatpages but you need multiple content areas.  I hope this is helpful to people and I'll be making minor edits as I see them necessary.\n","214":"Docker cookbooks\nDocker cookbooks is a collection of Dockerfiles and configs to build images the way you need them.\n","215":"Faster R-CNN\nThis repo has been deprecated. Here is the complete codes for training Faster-RCNN on your data and using the pre-trained Faster-RCNN model for new data: ChainerCV\nThis is an experimental implementation of Faster R-CNN in Chainer based on Ross Girshick's work: py-faster-rcnn codes.\nRequirement\nUsing anaconda is strongly recommended.\n\n\nPython 2.7.6+, 3.4.3+, 3.5.1+\n\nChainer 1.22.0+\nNumPy 1.9, 1.10, 1.11\nCython 0.25+\nOpenCV 2.9+, 3.1+\n\n\n\nInstallation of dependencies\npip install numpy\npip install cython\npip install chainer\npip install chainercv\n# for python3\nconda install -c https:\/\/conda.binstar.org\/menpo opencv3\n# for python2\nconda install opencv\n\nFor Windows users\nThere's a known problem in cpu_nms.pyx. But a workaround has been posted here (and see also the issue posted to the original py-faster-rcnn).\nSetup\n1. Build extensions\npython setup.py build_ext -i\n\nInference\n1. Download pre-trained model\nif [ ! -d data ]; then mkdir data; fi\ncurl https:\/\/dl.dropboxusercontent.com\/u\/2498135\/faster-rcnn\/VGG16_faster_rcnn_final.model?dl=1 -o data\/VGG16_faster_rcnn_final.model\n\nNOTE: The model definition in faster_rcnn.py has been changed, so if you already have the older pre-trained model file, please download it again to replace the older one with the new one.\n2. Use forward.py\ncurl -O http:\/\/vision.cs.utexas.edu\/voc\/VOC2007_test\/JPEGImages\/004545.jpg\npython forward.py --img_fn 004545.jpg --gpu 0\n\n--gpu 0 turns on GPU. When you turn off GPU, use --gpu -1 or remove --gpu option.\n\nLayers\nSummarization of Faster R-CNN layers used during inference\nRPN\nThe region proposal layer (RPN) is consisted of AnchorTargetLayer and ProposalLayer. RPN takes feature maps from trunk network like VGG-16, and performs 3x3 convolution to it. Then, it applies two independent 1x1 convolutions to the output of the first 3x3 convolution. Resulting outputs are rpn_cls_score and rpn_bbox_pred.\n\nThe shape of rpn_cls_score is (N, 2 * n_anchors, 14, 14) because each pixel on the feature map has n_anchors bboxes and each bbox should have 2 values that mean object\/background.\nThe shape of rpn_bbox_pred is (N, 4 * n_anchors, 14, 14) because each pixel on the feature map has n_anchors bboxes, and each bbox is represented with 4 values that mean left top x and y, width and height.\n\nTraining\n1. Make sure chainercv has been installed\nChainerCV is a utility library enables Chainer to treat various datasets easily. It also provides some transformation utility for data augmentation, and includes some standard algorithms for some comptuer vision tasks. Check the repo to know details. Here I use (VOCDetectionDataset)[http:\/\/chainercv.readthedocs.io\/en\/latest\/reference\/datasets.html#vocdetectiondataset] of ChainerCV. Anyway, before starting training of FasterRCNN, please install ChainerCV via pip.\npip install chainercv\n\n2. Start training\npython train_rpn.py\n\nFaster R-CNN Architecture\nNote that it is a visualization of the workflow DURING INFERENCE\n\n","216":"Attention-Based Summarization\nTensorflow implementation of A Neural Attention Model for Abstractive Summarization. The original code of author can be found here.\n\nPrerequisites\n\nPython 2.7 or Python 3.3+\nTensorflow\nGensim\n\nUsage\nTo train a model with duc2013 dataset:\n$ python main.py --dataset duc2013\n\nTo test an existing model:\n$ python main.py --dataset duc2014 --forward_only True\n\n(This is still in progress and currently have no access to summarization dataset)\nReferences\n\nEMNLP 2015 slide\n\nAuthor\nTaehoon Kim \/ @carpedm20\n","217":"Densely Connected Convolutional Network (DenseNet)\nThis repository contains the caffe version code for the paper Densely Connected Convolutional Networks.\nFor a brief introduction of DenseNet, see our original Torch implementation.\nImageNet Pretrained Models\nSee https:\/\/github.com\/shicai\/DenseNet-Caffe for caffe prototxt and pre-trained models.\nSee https:\/\/github.com\/liuzhuang13\/DenseNet for Torch pre-trained models.\nSee http:\/\/pytorch.org\/docs\/torchvision\/models.html?highlight=densenet for directly using the pretrained models in PyTorch.\nNote\n\nThe models in this repo are for CIFAR datasets only (input 32x32). If you feed images with larger resolution (e.g., ImageNet images), you need to use a different downsampling strategy to keep the memory usage reasonable. See our paper or Torch code for details on ImageNet models.\nThe code in this repo doesn't support BC-structres. However, it should be easy to modify.\nThis code is not the code we use to obtain the results in the original paper, the details (such as input preprocessing, data augmentation, training epochs) may be different. To reproduce the results reported in our paper, see our original Torch implementation.\n\nResults\nThe default setting (L=40, k=12, dropout=0.2) in the code yields a 7.09% error rate on CIFAR10 dataset (without any data augmentation).\nUsage\n\nGet the CIFAR data prepared following the Caffe's official CIFAR tutorial.\nmake_densenet.py contains the code to generate the network and solver prototxt file. First change the data path in function make_net() and preprocessing mean file in function densenet() to your own path of corresponding data file.\nBy default make_densenet.py generates a DenseNet with Depth L=40, Growth rate k=12 and Dropout=0.2. To experiment with different settings, change the code accordingly (see the comments in the code). Example prototxt files are already included. Use python densenet_make.py to generate new prototxt files.\nChange the caffe path in train.sh. Then use sh train.sh to train a DenseNet.\n\nContact\nliuzhuangthu at gmail.com\ngh349 at cornell.edu\nAny discussions, suggestions and questions are welcome!\n","218":"PYCCURACY IS LOOKING FOR A NEW MAINTAINER!\nIf you wish to be the new maintainer leave a message in an issue.\nPyccuracy\nPyccuracy is a Behaviour-Driven-Development-style tool written in Python that aims to make it easier to write automated acceptance tests. It improves the readability of those tests by using a structured natural language \u2013 and a simple mechanism to extend this language \u2013 so that both developers and customers can collaborate and understand what the tests do.\nHelp\nPlease check our documentation. For quick usage help use the pyccuracy_help command line tool.\nMailing list\nJoin the Pyccuracy mailing list at Google Groups to discuss and get support from the community and team.\nGet in touch with the team\nIf you have further questions, please contact the team:\n\nBernardo Heynemann (@heynemann)\nClaudio Figueiredo (@jcfigueiredo)\nGabriel Falc\u00e3o (@gabrielfalcao)\nGuilherme Chapiewski (@gchapiewski)\n","219":"RapidDevelop-Android\u5feb\u901f\u5f00\u53d1\u6846\u67b6\n\n\u6846\u67b6\u6301\u7eed\u66f4\u65b0\u4e2d\n\u8fd9\u4e2a\u6846\u67b6\u662f\u4ece\u5e73\u65f6\u9879\u76ee\u91cc\u7528\u7684\u6bd4\u8f83\u591a\u7684\u6846\u67b6\u91cc\u6574\u5408\u800c\u6765\n\u5bf9\u672c\u9879\u76ee\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u4e00\u8d77\u7814\u7a76\u559c\u6b22\u7684\u670b\u53cb\u6b22\u8fcestar\n\u540c\u65f6\u4e5f\u6b22\u8fce\u5927\u5bb6\u7684\u5b9d\u8d35\u610f\u89c1issues\n\u5982\u679c\u5927\u5bb6\u5bf9MVP\u6a21\u5f0f\u7684\u5f00\u53d1 \u7f51\u7edc\u722c\u866b\u4ee5\u53ca\u7f13\u5b58\u7b56\u7565\u611f\u5174\u8da3\u7684\u8bdd\u53ef\u4ee5\u770b\u770b\u6211\u6700\u65b0\u5199\u7684Freebook\n\u90ae\u7bb1:mychinalance@gmail.com\nAPI\u5730\u5740\n\u4e0b\u8f7dAPK\n\n###English\n##\u529f\u80fd\u8bf4\u660e\n\n\u5f02\u5e38\u5d29\u6e83\u7edf\u4e00\u7ba1\u7406\nretrofit rxjava okhttp rxcache------------------------------\u7f51\u7edc\u8bf7\u6c42\u4ee5\u53ca\u7f51\u7edc\u7f13\u5b58\nDemo\u91c7\u7528MVP\u6a21\u5f0f\u5f00\u53d1------------------------------------\u6570\u636e\u903b\u8f91\u590d\u7528,\u4fbf\u4e8e\u7ef4\u62a4\u5347\u7ea7\n\u4e0b\u62c9\u5237\u65b0 \u4e0a\u62c9\u52a0\u8f7d \u53ca\u81ea\u52a8\u52a0\u8f7d---------------------------\u5b9e\u73b0\u76d1\u542c\u65b9\u4fbf\u5feb\u6377\nRecyclerView\u8bbe\u914d\u5668------------------------------------------\u518d\u4e5f\u4e0d\u9700\u8981\u5199ViewHolder\nRecyclerView item\u52a0\u8f7d\u52a8\u753b--------------------------------\u591a\u79cd\u52a8\u753b\u6548\u679c\u4e00\u884c\u4ee3\u7801\u89e3\u51b3\n\u9875\u9762\u72b6\u6001\u7edf\u4e00\u7ba1\u7406 \u52a0\u8f7d\u4e2d  \u65e0\u6570\u636e  \u65e0\u7f51\u7edc-------------\u6240\u6709\u9875\u9762\u5747\u53ef\u6dfb\u52a0\n\u56fe\u7247\u663e\u793a\u4e0e\u7f13\u5b58 GIF\u56fe\u7247\u663e\u793a\nTab+Fragment\u5feb\u901f\u5b9e\u73b0\n\u89c6\u9891\u64ad\u653e(\u4effQQ\u7a7a\u95f4,\u79d2\u62cd\u7b49List\u64ad\u653e)\n\n\n##\u6548\u679c\u56fe\u5c55\u793a\n     \n\n##\u4f7f\u7528\u8bf4\u660e\n\u5bfc\u5165 lcrapiddeveloplibrary \u5230\u9879\u76ee\n\u5728 build.gradle \u7684 dependencies \u6dfb\u52a0:\ndependencies {\ncompile fileTree(include: ['*.jar'], dir: 'libs')\n....\ncompile project(':lcrapiddeveloplibrary')\n}\n\n##\u8f7b\u677e\u5b9e\u73b0\u5f02\u5e38\u7edf\u4e00\u7ba1\u7406\nMyApplication\u91cc\u9762\u521d\u59cb\u5316\u5c31\u53ef\u4ee5\u4e86\npublic class MyApplication extends Application {\n\n   \n    @Override\n    public void onCreate() {\n        super.onCreate();\n       \n        \/\/\u521d\u59cb\u5316\u5f02\u5e38\u7ba1\u7406\u5de5\u5177\n        Recovery.getInstance()\n                .debug(true)\/\/\u5173\u95ed\u540e \u5728\u9519\u8bef\u7edf\u4e00\u7ba1\u7406\u9875\u9762\u4e0d\u663e\u793a\u5f02\u5e38\u6570\u636e\n                .recoverInBackground(false)\n                .recoverStack(true)\n                .mainPage(WelcomeActivity.class)\/\/\u6062\u590d\u9875\u9762\n                .init(this);\n    }\n}\n\n##\u8f7b\u677e\u5b9e\u73b0 \u72b6\u6001\u9875\u9762 \u4e0b\u62c9\u5237\u65b0 \u81ea\u52a8\u52a0\u8f7d item\u52a8\u753b\n\u9996\u5148layout.xml\u91cc\u9762\u7684\u7f16\u5199\u5566 \u5217\u8868\u9875\u9762\u57fa\u672c\u90fd\u662f\u8fd9\u4e2a\u5957\u8def\n<!--ProgressActivity\u7528\u4e8e\u72b6\u6001\u9875\u7684\u63a7\u5236 \u6bd4\u5982\u52a0\u8f7d\u4e2d  \u7f51\u7edc\u5f02\u5e38  \u65e0\u6570\u636e  \u9002\u5408\u4efb\u4f55\u9875\u9762-->\n<com.xiaochao.lcrapiddeveloplibrary.viewtype.ProgressActivity\n    xmlns:progressActivity=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n    android:id=\"@+id\/progress\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n    <LinearLayout\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:orientation=\"vertical\"\n        >\n        <!--SpringView\u4e0b\u62c9\u5237\u65b0-->\n        <com.xiaochao.lcrapiddeveloplibrary.widget.SpringView\n            android:id=\"@+id\/springview\"\n            android:layout_width=\"match_parent\"\n            android:layout_height=\"match_parent\"\n            android:background=\"#FFFFFF\"\n            >\n            <android.support.v7.widget.RecyclerView\n                android:id=\"@+id\/rv_list\"\n                android:layout_width=\"match_parent\"\n                android:layout_height=\"match_parent\"\n                android:background=\"#eeeeee\"\/>\n        <\/com.xiaochao.lcrapiddeveloplibrary.widget.SpringView>\n\n    <\/LinearLayout>\n<\/com.xiaochao.lcrapiddeveloplibrary.viewtype.ProgressActivity>\n\n\u7136\u540e\u5c31\u662fActivity\u91cc\u9762\u7684\u7f16\u5199\u4e86 \u8fd9\u4e2a\u4f8b\u5b50\u91cc\u4f7f\u7528MVP\u6a21\u5f0f\u7f16\u5199\u611f\u5174\u8da3\u7684\u770b\u6211\u6700\u65b0\u5199\u7684Freebook\npublic class ListvViewActivity extends AppCompatActivity implements BaseQuickAdapter.RequestLoadMoreListener,SpringView.OnFreshListener,SchoolListView {\n\n    RecyclerView mRecyclerView;\n    ProgressActivity progress;\n    private Toolbar toolbar;\n    private BaseQuickAdapter mQuickAdapter;\n    private int PageIndex=1;\n    private SpringView springView;\n    private SchoolListPresent present;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_listv_view);\n        initView();\n    }\n\n\n    private void initView() {\n        present = new SchoolListPresent(this);\n        mRecyclerView = (RecyclerView) findViewById(R.id.rv_list);\n        springView = (SpringView) findViewById(R.id.springview);\n        \/\/\u8bbe\u7f6e\u4e0b\u62c9\u5237\u65b0\u76d1\u542c\n        springView.setListener(this);\n        \/\/\u8bbe\u7f6e\u4e0b\u62c9\u5237\u65b0\u6837\u5f0f\n        springView.setHeader(new RotationHeader(this));\n        \/\/springView.setFooter(new RotationFooter(this));mRecyclerView\u5185\u90e8\u96c6\u6210\u7684\u81ea\u52a8\u52a0\u8f7d  \u4e0a\u5566\u52a0\u8f7d\u7528\u4e0d\u4e0a   \u5728\u5176\u4ed6View\u4f7f\u7528\n        progress = (ProgressActivity) findViewById(R.id.progress);\n        \/\/\u8bbe\u7f6eRecyclerView\u7684\u663e\u793a\u6a21\u5f0f  \u5f53\u524dList\u6a21\u5f0f\n        mRecyclerView.setLayoutManager(new LinearLayoutManager(this));\n        \/\/\u5982\u679cItem\u9ad8\u5ea6\u56fa\u5b9a  \u589e\u52a0\u8be5\u5c5e\u6027\u80fd\u591f\u63d0\u9ad8\u6548\u7387\n        mRecyclerView.setHasFixedSize(true);\n        \/\/\u8bbe\u7f6e\u9875\u9762\u4e3a\u52a0\u8f7d\u4e2d..\n        progress.showLoading();\n        \/\/\u8bbe\u7f6e\u9002\u914d\u5668\n        mQuickAdapter = new ListViewAdapter(R.layout.list_view_item_layout,null);\n        \/\/\u8bbe\u7f6e\u52a0\u8f7d\u52a8\u753b\n        mQuickAdapter.openLoadAnimation(BaseQuickAdapter.SCALEIN);\n        \/\/\u8bbe\u7f6e\u662f\u5426\u81ea\u52a8\u52a0\u8f7d\u4ee5\u53ca\u52a0\u8f7d\u4e2a\u6570\n        mQuickAdapter.openLoadMore(6,true);\n        \/\/\u5c06\u9002\u914d\u5668\u6dfb\u52a0\u5230RecyclerView\n        mRecyclerView.setAdapter(mQuickAdapter);\n         \/\/\u8bbe\u7f6e\u81ea\u52a8\u52a0\u8f7d\u76d1\u542c\n        mQuickAdapter.setOnLoadMoreListener(this);\n        \/\/\u8bf7\u6c42\u7f51\u7edc\u6570\u636e\n        present.LoadData(PageIndex,12,false);\n    }\n   \/\/\u81ea\u52a8\u52a0\u8f7d\n    @Override\n    public void onLoadMoreRequested() {\n        PageIndex++;\n        present.LoadData(PageIndex,12,true);\n    }\n    \/\/\u4e0b\u62c9\u5237\u65b0\n    @Override\n    public void onRefresh() {\n        PageIndex=1;\n        present.LoadData(PageIndex,12,false);\n    }\n  \n    \/*\n    * MVP\u6a21\u5f0f\u7684\u76f8\u5173\u72b6\u6001\n    *\n    * *\/\n    @Override\n    public void showProgress() {\n        progress.showLoading();\n    }\n\n    @Override\n    public void hideProgress() {\n        progress.showContent();\n    }\n\n    @Override\n    public void newDatas(List<UniversityListDto> newsList) {\n        \/\/\u8fdb\u5165\u663e\u793a\u7684\u521d\u59cb\u6570\u636e\u6216\u8005\u4e0b\u62c9\u5237\u65b0\u663e\u793a\u7684\u6570\u636e\n        mQuickAdapter.setNewData(newsList);\/\/\u65b0\u589e\u6570\u636e\n        mQuickAdapter.openLoadMore(10,true);\/\/\u8bbe\u7f6e\u662f\u5426\u53ef\u4ee5\u4e0b\u62c9\u52a0\u8f7d  \u4ee5\u53ca\u52a0\u8f7d\u6761\u6570\n        springView.onFinishFreshAndLoad();\/\/\u5237\u65b0\u5b8c\u6210\n    }\n\n    @Override\n    public void addDatas(List<UniversityListDto> addList) {\n        \/\/\u65b0\u589e\u81ea\u52a8\u52a0\u8f7d\u7684\u7684\u6570\u636e\n        mQuickAdapter.notifyDataChangedAfterLoadMore(addList, true);\n    }\n\n    @Override\n    public void showLoadFailMsg() {\n        \/\/\u8bbe\u7f6e\u52a0\u8f7d\u9519\u8bef\u9875\u663e\u793a\n        progress.showError(getResources().getDrawable(R.mipmap.monkey_cry), Constant.ERROR_TITLE, Constant.ERROR_CONTEXT, Constant.ERROR_BUTTON, new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                PageIndex=1;\n                present.LoadData(PageIndex,12,false);\n            }\n        });\n    }\n\n    @Override\n    public void showLoadCompleteAllData() {\n        \/\/\u6240\u6709\u6570\u636e\u52a0\u8f7d\u5b8c\u6210\u540e\u663e\u793a\n        mQuickAdapter.notifyDataChangedAfterLoadMore(false);\n        View view = getLayoutInflater().inflate(R.layout.not_loading, (ViewGroup) mRecyclerView.getParent(), false);\n        mQuickAdapter.addFooterView(view);\n    }\n\n    @Override\n    public void showNoData() {\n        \/\/\u8bbe\u7f6e\u65e0\u6570\u636e\u663e\u793a\u9875\u9762\n        progress.showEmpty(getResources().getDrawable(R.mipmap.monkey_cry),Constant.EMPTY_TITLE,Constant.EMPTY_CONTEXT);\n    }\n}\n\n##\u8f7b\u677e\u5b9e\u73b0\u89c6\u9891\u5217\u8868\u64ad\u653e\n\u5217\u8868\u90e8\u5206\u548c\u4e0a\u9762\u7684\u4e00\u6837\u5c31\u4e0d\u8bf4\u4e86,\u6211\u8fd9\u8fb9\u4e3b\u8981\u63cf\u53d9\u89c6\u9891\u64ad\u653e\u7684\u90e8\u5206 \u662f\u5728\u4e0d\u61c2\u5f97\u53ef\u4ee5clone\u5230\u672c\u5730\u4ed3\u5e93\u8dd1\u4e00\u8fb9\nitem_layout.xml\n<LinearLayout\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:padding=\"5dp\"\n        android:orientation=\"vertical\">\n        <com.xiaochao.lcrapiddeveloplibrary.Video.JCVideoPlayerStandard\n            android:id=\"@+id\/video_list_item_playr\"\n            android:layout_width=\"match_parent\"\n            android:layout_height=\"wrap_content\"\/>\n        <LinearLayout\n            android:layout_width=\"match_parent\"\n            android:layout_height=\"match_parent\"\n            android:orientation=\"horizontal\"\n            android:padding=\"5dp\"\n            android:gravity=\"center_vertical\">\n            <ImageView\n                android:id=\"@+id\/video_list_item_image\"\n                android:layout_width=\"100dp\"\n                android:layout_height=\"70dp\"\n                android:src=\"@mipmap\/def_head\"\/>\n            <LinearLayout\n                android:layout_width=\"0dp\"\n                android:layout_weight=\"1\"\n                android:layout_height=\"wrap_content\"\n                android:layout_marginLeft=\"15dp\"\n                android:layout_marginTop=\"5dp\"\n                android:layout_marginBottom=\"5dp\"\n                android:layout_marginRight=\"10dp\"\n                android:orientation=\"vertical\">\n                <TextView\n                    android:id=\"@+id\/video_list_item_text_title\"\n                    android:layout_width=\"wrap_content\"\n                    android:layout_height=\"wrap_content\"\n                    android:textColor=\"#666666\"\n                    android:text=\"\u6807\u9898\"\n                    android:textSize=\"15dp\"\/>\n                <TextView\n                    android:id=\"@+id\/video_list_item_text_context\"\n                    android:layout_width=\"wrap_content\"\n                    android:layout_marginTop=\"5dp\"\n                    android:textColor=\"#999999\"\n                    android:textSize=\"13dp\"\n                    android:text=\"\u5185\u5bb9\"\n                    android:lines=\"3\"\n                    android:ellipsize=\"end\"\n                    android:layout_height=\"wrap_content\"\/>\n            <\/LinearLayout>\n        <\/LinearLayout>\n\n    <\/LinearLayout>\n\n\u7136\u540e\u5c31\u662fadapter\u91cc\u9762\u5bf9\u89c6\u9891\u63a7\u4ef6\u7684\u8d4b\u503c\u5904\u7406\npublic class VideoLisViewAdapter extends BaseQuickAdapter<VideoListDto> {\n\n    public VideoLisViewAdapter(int layoutResId, List<VideoListDto> data) {\n        super(layoutResId, data);\n    }\n\n    public VideoLisViewAdapter(List<VideoListDto> data) {\n        super(data);\n    }\n\n    public VideoLisViewAdapter(View contentView, List<VideoListDto> data) {\n        super(contentView, data);\n    }\n\n    @Override\n    protected void convert(BaseViewHolder helper, VideoListDto item) {\n        helper.setText(R.id.video_list_item_text_title,item.getTitle()).setText(R.id.video_list_item_text_context,item.getIntroduction());\n        \/\/Glide\u52a0\u8f7d\u56fe\u7247  \u5e76\u4e14\u652f\u6301gif\u52a8\u56fe\n        Glide.with(mContext)\n                .load(item.getPictureUrl())\n                .crossFade()\n                .placeholder(R.mipmap.def_head)\n                .into((ImageView) helper.getView(R.id.video_list_item_image));\n        \/\/\u5bf9\u89c6\u9891\u7684\u8d4b\u503c \u6dfb\u52a0\u89c6\u9891\u64ad\u653e\u5730\u5740(\u4f7f\u7528\u539f\u5730\u5740  .mp4\u4e4b\u7c7b\u7684  \u8fd9\u4e2a\u8981\u6ce8\u610f)\u548c\u6807\u9898\n        ((JCVideoPlayerStandard)helper.getView(R.id.video_list_item_playr)).setUp(item.getAppVideoUrl(),item.getTitle());\n        Glide.with(mContext)\n                .load(item.getPictureUrl())\n                .crossFade()\n                .placeholder(R.mipmap.main_mini_m)\n                .into((((JCVideoPlayerStandard) helper.getView(R.id.video_list_item_playr)).thumbImageView));\n    }\n}\n\n###Tab+Fragment\u5feb\u901f\u5b9e\u73b0\n\u8fd8\u662f\u539f\u6765\u7684\u914d\u65b9 layout.xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<LinearLayout xmlns:android=\"http:\/\/schemas.android.com\/apk\/res\/android\"\n    xmlns:tools=\"http:\/\/schemas.android.com\/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    xmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n    android:orientation=\"vertical\"\n    tools:context=\"com.xiaochao.lcrapiddevelop.UI.Tab.TabActivity\">\n    <android.support.v7.widget.Toolbar\n        android:id=\"@+id\/toolbar\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"?attr\/actionBarSize\"\n        android:background=\"?attr\/colorPrimary\"\n        app:popupTheme=\"@style\/AppTheme.PopupOverlay\" \/>\n    <!--\u663e\u793a\u5934\u90e8\u6ed1\u5757-->\n    <FrameLayout\n        android:id=\"@+id\/tab\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:background=\"#fff\"\n        \/>\n    <android.support.v4.view.ViewPager\n        android:id=\"@+id\/viewpager\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        \/>\n<\/LinearLayout>\n\n\u7136\u540e\u5c31\u662f\u5934\u90e8\u7684xml\u7f16\u5199\u4e86\n<com.xiaochao.lcrapiddeveloplibrary.SmartTab.SmartTabLayout\n    xmlns:android=\"http:\/\/schemas.android.com\/apk\/res\/android\"\n    xmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n    android:id=\"@+id\/viewpagertab\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"40dp\"\n    android:background=\"#FFFFFF\"\n\n    app:stl_defaultTabTextColor=\"@color\/custom_tab\"\n    app:stl_distributeEvenly=\"true\"\n    app:stl_defaultTabTextHorizontalPadding=\"5dp\"\n    app:stl_indicatorColor=\"@color\/title_bag\"\n    app:stl_indicatorCornerRadius=\"0dp\"\n    app:stl_indicatorInterpolation=\"smart\"\n    app:stl_indicatorThickness=\"3dp\"\n    app:stl_defaultTabTextSize=\"13dp\"\n    app:stl_dividerColor=\"@color\/bag_gray\"\n    app:stl_dividerThickness=\"1dp\"\n    app:stl_overlineColor=\"@color\/bag_gray\"\n    app:stl_underlineColor=\"#00000000\"\n    app:stl_defaultTabBackground=\"@color\/bag_gray_transparent\"\n    \/>\n\n\u5b8c\u5168\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u60f3\u8981\u7684\u98ce\u683c\u73a9  \u4e0b\u9762\u8868\u683c\u4e3a \u53ef\u8bbe\u7f6e\u7684\u5c5e\u6027\n\n\n\n\nattr\n\u63cf\u8ff0\n\n\n\n\nstl_indicatorAlwaysInCenter\n\u5982\u679c\u8bbe\u7f6e\u4e3a\u771f,\u6709\u6e90\u6807\u7b7e\u603b\u662f\u663e\u793a\u5728\u4e2d\u5fc3(\u5982\u62a5\u644agoogle app),\u9ed8\u8ba4\u7684\u9519\u8bef\n\n\nstl_indicatorWithoutPadding\n\u5982\u679c\u8bbe\u7f6e\u4e3atrue,\u753b\u7684\u6307\u6807\u6ca1\u6709\u586b\u5145\u9009\u9879\u5361\u4e2d,\u9ed8\u8ba4\u7684\u9519\u8bef\n\n\nstl_indicatorInFront\n\u753b\u524d\u7684\u6307\u793a\u5668\u4e0b\u5212\u7ebf,\u9ed8\u8ba4\u7684\u9519\u8bef\n\n\nstl_indicatorInterpolation\n\u884c\u4e3a\u7684\u6307\u6807:\u201c\u7ebf\u6027\u201d\u6216\u201c\u667a\u80fd\u201d\n\n\nstl_indicatorGravity\n\u56fe\u7684\u4f4d\u7f6e\u6307\u793a\u5668:\u201c\u5e95\u201d\u6216\u201c\u524d\u201d\u6216\u201c\u4e2d\u5fc3\u201d,\u9ed8\u8ba4\u201c\u5e95\u201d\n\n\nstl_indicatorColor\n\u6807\u5fd7\u7684\u989c\u8272\n\n\nstl_indicatorColors\n\u591a\u79cd\u989c\u8272\u7684\u6307\u6807,\u53ef\u4ee5\u8bbe\u7f6e\u6bcf\u4e2a\u9009\u9879\u5361\u7684\u989c\u8272\n\n\nstl_indicatorThickness\n\u539a\u5ea6\u6307\u6807\n\n\nstl_indicatorWidth\n\u7684\u5bbd\u5ea6\u6307\u6807,\u9ed8\u8ba4\u201c\u6c7d\u8f66\u201d\n\n\nstl_indicatorCornerRadius\n\u5706\u89d2\u534a\u5f84\u7684\u6307\u6807\n\n\nstl_overlineColor\n\u9876\u7ebf\u7684\u989c\u8272\n\n\nstl_overlineThickness\n\u9876\u7ebf\u7684\u539a\u5ea6\n\n\nstl_underlineColor\n\u989c\u8272\u7684\u5e95\u7ebf\n\n\nstl_underlineThickness\n\u539a\u5ea6\u7684\u5e95\u7ebf\n\n\nstl_dividerColor\n\u989c\u8272\u4e4b\u95f4\u7684\u5206\u9694\u5668\u9009\u9879\u5361\n\n\nstl_dividerColors\n\u591a\u79cd\u989c\u8272\u7684\u9009\u9879\u5361\u4e4b\u95f4\u7684\u5206\u9694\u5668,\u53ef\u4ee5\u8bbe\u7f6e\u6bcf\u4e2a\u9009\u9879\u5361\u7684\u989c\u8272\n\n\nstl_dividerThickness\n\u5206\u9891\u5668\u7684\u539a\u5ea6\n\n\nstl_defaultTabBackground\n\u80cc\u666f\u53ef\u62c9\u7684\u6bcf\u4e2a\u9009\u9879\u5361\u3002 \u4e00\u822c\u8bbe\u7f6eStateListDrawable\n\n\nstl_defaultTabTextAllCaps\n\u5982\u679c\u8bbe\u7f6e\u4e3a\u771f,\u6240\u6709\u9009\u9879\u5361\u6807\u9898\u5927\u5199,\u8fdd\u7ea6\u4e8b\u5b9e\n\n\nstl_defaultTabTextColor\n\u6587\u672c\u7684\u989c\u8272\u5305\u62ec\u9ed8\u8ba4\u7684\u9009\u9879\u5361\n\n\nstl_defaultTabTextSize\n\u6587\u672c\u5305\u62ec\u9ed8\u8ba4\u7684\u9009\u9879\u5361\u7684\u5927\u5c0f\n\n\nstl_defaultTabTextHorizontalPadding\n\u6587\u672c\u5e03\u5c40\u586b\u5145\u9ed8\u8ba4\u7684\u9009\u9879\u5361\u5305\u62ec\n\n\nstl_defaultTabTextMinWidth\n\u6700\u5c0f\u5bbd\u5ea6\u7684\u6807\u7b7e\n\n\nstl_customTabTextLayoutId\n\u5e03\u5c40ID\u5b9a\u4e49\u81ea\u5b9a\u4e49\u9009\u9879\u5361\u3002 \u5982\u679c\u4f60\u4e0d\u6307\u5b9a\u4e00\u4e2a\u5e03\u5c40,\u4f7f\u7528\u9ed8\u8ba4\u9009\u9879\u5361\n\n\nstl_customTabTextViewId\n\u6587\u672c\u89c6\u56feID\u5728\u4e00\u4e2a\u81ea\u5b9a\u4e49\u9009\u9879\u5361\u5e03\u5c40\u3002 \u5982\u679c\u4f60\u4e0d\u4e0ecustomTabTextLayoutId\u5b9a\u4e49,\u4e0d\u5de5\u4f5c\n\n\nstl_distributeEvenly\n\u5982\u679c\u8bbe\u7f6e\u4e3a\u771f,\u6bcf\u4e2a\u9009\u9879\u5361\u90fd\u7ed9\u51fa\u540c\u6837\u7684\u91cd\u91cf,\u9ed8\u8ba4\u7684\u9519\u8bef\n\n\nstl_clickable\n\u5982\u679c\u8bbe\u7f6e\u4e3afalse,\u7981\u7528\u9009\u62e9\u9009\u9879\u5361\u5355\u51fb,\u8fdd\u7ea6\u4e8b\u5b9e\n\n\nstl_titleOffset\n\u5982\u679c\u8bbe\u7f6e\u4e3a\u201cauto_center\u201d,\u4e2d\u95f4\u7684\u5e7b\u706f\u7247\u7684\u4f4d\u7f6e\u9009\u9879\u5361\u4e2d\u5fc3\u5c06\u7ee7\u7eed\u3002 \u5982\u679c\u6307\u5b9a\u4e00\u4e2a\u7ef4\u5ea6\u5c06\u62b5\u6d88\u4ece\u5de6\u8fb9\u7f18,\u9ed8\u8ba424 dp\n\n\nstl_drawDecorationAfterTab\n\u753b\u88c5\u9970(\u6307\u793a\u5668\u548c\u7ebf)\u7ed8\u56fe\u9009\u9879\u5361\u540e,\u9ed8\u8ba4\u7684\u9519\u8bef\n\n\n\n\n\u597d\u4e86\u63a5\u4e0b\u6765\u5c31TabActivity\npublic class TabActivity extends AppCompatActivity {\n\n    ViewGroup tab;\n    ViewPager viewpager;\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_tab);\n        initView();\n    }\n\n    private void initView() {\n        tab = (ViewGroup) findViewById(R.id.tab);\n        viewpager = (ViewPager) findViewById(R.id.viewpager);\n        \/\/\u4f7f\u7528\u65b9\u624d\u5b9a\u4e49\u5934\u90e8\n        tab.addView(LayoutInflater.from(this).inflate(R.layout.tab_top_layout, tab, false));\n    \n        SmartTabLayout viewPagerTab = (SmartTabLayout) findViewById(R.id.viewpagertab);\n        \n        FragmentPagerItems pages = new FragmentPagerItems(this);\n        \n        \/\/\u6dfb\u52a0Fragment  FragmentPagerItem.of(\"\u5934\u90e8\u663e\u793a\u6807\u9898\", \"\u5efa\u7acb\u7684fragment\",\"\u9700\u8981\u4f20\u503c\u7684\u53ef\u4ee5\u4f20Bundle\")\n        for (int i=0;i<4;i++) {\n            pages.add(FragmentPagerItem.of(\"Tab\"+i, TabFragment.class));\n        }\n\n        FragmentPagerItemAdapter adapter = new FragmentPagerItemAdapter(\n                getSupportFragmentManager(), pages);\n\n        viewpager.setAdapter(adapter);\n        viewPagerTab.setViewPager(viewpager);\n    }\n}\n\n\n##\u7279\u522b\u611f\u8c22\n\nJieCaoVideoPlayer\nSpringView\nSmartTabLayout\nBaseRecyclerViewAdapterHelper\nRecovery\n\n","220":"About\nstorm-contrib is a community repository for modules to use with Storm. These include a variety of spouts\/bolts for integrating with other systems (Redis, Kafka, MongoDB, etc), as well as code for common tasks a Storm developer encounters.\nFor more information about Storm itself, see the Storm GitHub repository.\nOrganization\nstorm-contrib is organized as a \"super-project\" with a sub-folder for each module. Each module is distributed independently and module owners are responsible for distribution.\n##Git Submodules\nSome storm-contrib modules are git submodules (links to external github repositories). This allows storm-contrib sub-projects to be maintained externally (so those projects can maintain branches and tags independently), but also included in storm-contrib to increase community visibility.\nMore information about how git submodules work can be found in the online git documentation on submodules.\nInitializing storm-contrib submodules\nWhen you clone storm-contrib, the modules that are git submodules will appear as empty directories.\nTo initialize the git submodules use the following command:\ngit submodule init\n\nTo pull down the latest versions of submodules:\ngit submodule update\n\nContributing\nIf you're interested in contributing a module to storm-contrib, send an email to the Storm mailing list. You will then be given commit rights to storm-contrib. The advantage of having your module be part of storm-contrib instead of your own project is more visibility for your code. However, if you'd rather maintain your module as its own project that's fine too!\nAdding your storm-related project as a git submodule\nOnce you have signed the contributor licence agreement and been granted commit rights to storm-contrib, you can add your project as a git submodule with the following command:\ngit submodule add git:\/\/github.com\/[username]\/[projectname].git\n\n","221":"android_volley_examples\nProject with examples how to use the new Volley networking framework\n","222":"jclouds moved to Apache in 2013 and archived this repository.  Please visit our\nnew JIRA issue tracker,\nGitHub repository, and\nweb page.\n","223":"Android-ColorTrackView\n\u5b57\u4f53\u6216\u8005\u56fe\u7247\u53ef\u4ee5\u9010\u6e10\u67d3\u8272\u548c\u9010\u6e10\u892a\u8272\u7684\u52a8\u753b\u6548\u679c\n\u4f7f\u7528\n <com.zhy.view.ColorTrackView\n        android:id=\"@+id\/id_changeTextColorView\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_centerInParent=\"true\"\n        android:layout_centerVertical=\"true\"\n        android:padding=\"20dp\"\n        android:background=\"#44ff0000\"\n        android:gravity=\"center_vertical\"\n        zhy:progress=\"0\"\n        zhy:direction=\"left\"\n        zhy:text=\"\u5f20\u9e3f\u6d0b\"\n        zhy:text_change_color=\"#ffff0000\"\n        zhy:text_origin_color=\"#ff000000\"\n        zhy:text_size=\"60sp\" \/>\n\u6ce8\uff1azhy\u4e3a\u547d\u540d\u7a7a\u95f4\uff0cxmlns:zhy=\"http:\/\/schemas.android.com\/apk\/res-auto\"\uff0c\u53ef\u81ea\u7531\u4fee\u6539\u3002\n\nprogress  [0.0f , 1.0f]\ntext \u7ed8\u5236\u7684\u6587\u672c\ntext_change_color\u76ee\u6807\u989c\u8272\ntext_origin_color\u539f\u59cb\u989c\u8272\ntext_size\u5b57\u4f53\u5927\u5c0f\ndirection \u65b9\u5411\uff0c\u679a\u4e3e\u7c7b\u578b\uff0c\u652f\u6301\uff1aleft,right,top,bottom\n\n\u6548\u679c\u56fe\n\u7b80\u5355\u4f7f\u7528\n\n\u7ed3\u5408ViewPager\n\n\u5173\u4e8e\u6211\n\u6211\u7684\u535a\u5ba2\u5730\u5740\n","224":" \n\nAll available icons (744)\n\nIf, like me, you're tired of copying 5 images (ldpi, mdpi, hdpi, xhdpi, xxhdpi) for each icon you want to use in your app, for each color you want to use them with android-material-icons can help you.\n\nAbout\nandroid-material-icons allows you to include any of the Material Design 2.1.1 icons by Google packed by Sergey Kupletsky in your texts, your ActionBar, and even in your EditTexts. Icons are infinitely scalable, and customizable with shadows and everything you can do on texts.\nSpecial thanks to Joan Zapata for his android-iconify project since this is mostly a copy :)\nGet started #1\nIf you need icons on a TextView, use the { } syntax. You can put any text around it and have more than one icon in the text. Note that the shadows apply to the icons as well.\n<IconTextView\n    android:text=\"{zmdi-android}\"\n    android:shadowColor=\"#22000000\"\n    android:shadowDx=\"3\"\n    android:shadowDy=\"3\"\n    android:shadowRadius=\"1\"\n    android:textSize=\"90dp\"\n    android:textColor=\"#FF33B5E5\"\n    ... \/>\n\nYou can either use IconTextView \/ ButtonTextView or use any TextView and then programmatically call Iconify.addIcons(myTextView);.\n\nGet started #2\nIf you need an icon in an ImageView or in your ActionBar, then you should use IconDrawable. Again, icons are infinitely scalable and will never get fuzzy!\n\/\/ Set an icon in the ActionBar\nmenu.findItem(R.id.share).setIcon(\n   new IconDrawable(this, IconValue.zmdi_share)\n   .colorRes(R.color.ab_icon)\n   .actionBarSize());\nDesign-time preview (maybe working)\n\nCopy material font file (do not rename it) to your $ANDROID_SDK\/platforms\/android-$N\/data\/fonts\/ folder for each platform $N available.\nAssign attribute hacky_preview like this:\n\n<com.malinskiy.materialicons.widget.IconTextView\n            xmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n            android:text=\"{zmdi-android}\"\n            android:textSize=\"48dp\"\n            app:hacky_preview=\"true\"\/>\n\nGet it\nGradle:\nrepositories {\n    ...\n    mavenCentral()\n    ...\n}\n...\ndependencies {\n    ...\n    compile 'com.malinskiy:materialicons:1.0.2'\n    ...\n}\nLicense\nCopyright 2013 Joan Zapata\nCopyright 2014 Anton Malinskiy\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nIt uses Google's material font licensed under OFL 1.1, which is compatible\nwith this library's license.\n\n    http:\/\/scripts.sil.org\/cms\/scripts\/render_download.php?format=file&media_id=OFL_plaintext&filename=OFL.txt\n    \n\n","225":"RaiflatButton\nA raised button that lowers down to 0dp of elevation. From my blog post: https:\/\/rubensousa.github.io\/2016\/10\/raiflatbutton\nIt behaves like a normal button on APIs < 21\n\nBuild\ndependencies {\n   compile 'com.github.rubensousa:raiflatbutton:0.1'\n}\nHow to\nJust add the following xml to your layout:\n<com.github.rubensousa.raiflatbutton.RaiflatButton\n    android:id=\"@+id\/normalButton\"\n    style=\"@style\/Base.Widget.AppCompat.Button.Colored\"\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:text=\"Colored\" \/>\n    \n<com.github.rubensousa.raiflatbutton.RaiflatImageButton\n    android:id=\"@+id\/imageButton\"\n    style=\"@style\/Base.Widget.AppCompat.Button\"\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:src=\"@drawable\/ic_android\" \/>\nIf you want to resume the normal button behavior, just use:\nbutton.setFlatEnabled(false);\nStyling\nSince RaiflatButton and RaiflatImageButton both extend AppCompatButton and AppCompatImageButton, you can reuse the same AppCompat styles.\nExample:\n<style name=\"RaiflatButtonPrimaryStyle\" parent=\"Base.Widget.AppCompat.Button.Colored\">\n    <item name=\"android:colorControlNormal\">?attr\/colorPrimary<\/item>\n<\/style>\nLicense\nCopyright 2016 R\u00faben Sousa\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","226":"Vertical-Intro\nVertical intro allows you to integrate material vertical intro to your app\n\nThe current minSDK version is API level 14 Android 4.0 (ICE CREAM SANDWICH).\n \nDownload sample apk\n#YouTube demo\n\n#Installation\nGradle:\ncompile 'com.github.armcha:Vertical-Intro:2.0.0'\nSetup and usage\n\nStep 1:\nYour activity must extends from VerticalIntro activity\npublic class TestActivity extends VerticalIntro\nStep 2:\nAdd activity to manifest with defined theme:\n<activity\nandroid:name=\".TestActivity\"\nandroid:theme=\"@style\/VerticalIntroStyle\" \/>\nStep 3:\nAdd items in init\naddIntroItem(new VerticalIntroItem.Builder()\n                .backgroundColor(R.color.my_color)\n                .image(R.drawable.my_drawable)\n                .title(\"Lorem Ipsum Lorem Ipsum\")\n                .text(\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\" +\n                        \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\" +\n                        \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\")\n                .textColor(R.color.your_color)\n                .titleColor(R.color.your_color)\n                .textSize(14) \/\/ in SP\n                .titleSize(17) \/\/ in SP\n                .nextTextColor(R.color.color1)\n                .build());\nReturn color for last item bottom view background color\n @Override\n    protected Integer setLastItemBottomViewColor() {\n        return R.color.my_second_color;\n    }\nCustomize\nNote: You must do all customizations inside init method\nEnable or disable skip button\nsetSkipEnabled(true); \n\nEnable or disable vibrate \ud83d\udcf3 and set vibrate intensity\nsetVibrateEnabled(true);\nsetVibrateIntensity(20);\n\nSet your texts\nsetNextText(\"OK\");\nsetDoneText(\"FINISH HIM\");\nsetSkipText(\"GO GO\");\n\nSet custom font\nsetCustomTypeFace(Typeface.createFromAsset(getAssets(), \"fonts\/NotoSans-Regular.ttf\"));\n#\n\nSet text color\n.textColor(R.color.your_color)\n\nSet title color\n.titleColor(R.color.your_color)\n\nSet text size in SP\n.textSize(14)\n\nSet title size in SP\n.titleSize(17)\n\nSet skip button text color\nsetSkipColor(R.color.your_color);\n\nContact\nPull requests are more than welcome.\nPlease fell free to contact me if there is any problem when using the library.\n\nEmail: armcha01@gmail.com\nFacebook: https:\/\/web.facebook.com\/chatikyana\nGoogle +: https:\/\/plus.google.com\/+ArmanChatikyan\nWebsite: https:\/\/armcha.github.io\n\nLicense\n  Vertical Intro library for Android\n  Copyright (c) 2017 Arman Chatikyan (https:\/\/github.com\/armcha\/Vertical-Intro).\n  \n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n\n","227":"Disclaimer: unfortunately, I don't do Android dev anymore, so I can't maintain this library.\n\nPanesLibrary\nThis library makes it easy to make native Android apps with multi-pane tablet layouts. On the phone, the app appears as a conventional app with a sliding menu and a content pane where fragments are stacked on top of each other. On the tablet, the menu and all other fragments appear in dynamically added panes of varying sizes.\nExample\nExampleActivity demonstrates all the necessary configuration for using PanesActivity.\nExampleFragment uses a layout with two TextViews and a Button. The TextViews hold the index of the fragment and the length of time it's been alive. This is used to demonstrate that fragments are correctly retained on activity restarts. The Button has a callback which creates a new fragment and adds it to the parent activity.\nUsing PanesLibrary\nMake sure you have the most up-to-date support library and ActionBarSherlock.\nHave your activity extend PanesActivity. Then, add fragments to the activity using the following functions:\n\nsetMenuFragment(fragment): set the menu\naddFragment(prevFragment, newFragment): adds newFragment after the prevFragment (clobbering any fragments that used to be after prevFragment)\nclearFragments(): removes all the fragments except the menu fragment\ngetMenuFragment(): get the menu fragment\ngetTopFragment(): get the fragment on the top of the stack\n\nYou also need to provide a PaneSizer. This object allows you to programatically set the width of each pane based on the type of Fragment\/View you want to place inside the pane.\nYou should also implement updateFragment(...). This is run whenever a fragment is added (even on activity restarts).\nFragments are added onto a stack where the 0th fragment is the menu. Here's an example of what this means:\n> setMenuFragment(A)\nstack: A\n\n> addFragment(A, B)\nstack: A, B\n\n> addFragment (B, C)\nstack: A, B, C\n\n> addFragment (C, D)\nstack: A, B, C, D\n\n> addFragment(B, E)\nstack: A, B, E\n\n>clearFragments()\nstack: A\n\nNote: PanesActivity requires you to use fragments. If you don't use fragments, you can still use PanesLayout manually.\nPanesLayout (this controls the tablet layout)\nThe hierarchy of a PanesLayout looks like this:\n|--------------------------------|\n| PanesLayout                    |\n| |----------------------------| |\n| | PaneScrollView             | |\n| | |------------------------| | |\n| | | PaneView |-----------| | | |\n| | |          | (content) | | | |\n| | |          |-----------| | | |\n| | |------------------------| | |\n| |----------------------------| |\n|--------------------------------|\n\nA PanesLayout can hold any number of panes. Each pane is made up of a PaneScrollView (which allows the pane contents to slide left & right), a PaneView (which provides padding on the left of the content), and some content (i.e. fragment).\nVariables associated with each pane:\n\ntype: the possible values of type are defined by the user. For example, in ExampleActivity, the only possible type is DEFAULT_PANE_TYPE. This variable is used to size the pane by the PaneSizer\nfocused: if true, then PanesLayout will never intercept touch events associated with this pane.\nindex: the index of this pane-- starting at 0, going to # of panes - 1.\n\nTo add\/remove\/get panes:\n\naddPane(int type)\naddPane(int type, boolean focused)\nremovePanes(int i): removes all panes after the ith index\ngetNumPanes(): the number of panes\ngetCurrentIndex(): the current top index\nsetIndex(int index): scroll to the pane associated with index.\n\nListeners\/delegates:\n\nPaneSizer: determines what the type\/focus of a pane should be based on some associated object (i.e. Fragment or View)\nOnIndexChangedListener: this is fired whenever the visible panes change.\n\nCopyright 2013 Kenrick Rilee\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n","228":"Lighthouse\nPlease note: this project is no longer maintained.\nLighthouse is a decentralised, peer to peer crowdfunding application that uses the smart contracts features of the\nBitcoin protocol. It lets you create projects and pledge to those projects.\nget involved\nMailing lists:\n\nUser discussion and questions\nDevelopment talk\n\nOr sign up for occasional email newsletters on vinumeris.com.\nYou can chat to us in the #lighthouse chatroom on Freenode IRC.\nIf you're a developer, build instructions are at the end of this README.\nhow to tweak the user interface\nYou can do some work on the UI without being a Java developer. If you have web design experience modifying the Lighthouse\nUI will seem quite familiar. Ask for a binary build if you don't want to compile the app yourself, and then run\nLighthouse with the --resdir=\/path\/to\/ui\/files flag. It should point to the client\/src\/main\/resources\/lighthouse\ndirectory from this git repository. You can use the Shortcut+S key combination to reload the UI whilst the app\nis running. Shortcut is the Cmd key on a Mac and Control key on Windows\/Linux. The UI is defined by the CSS files\nand FXML files (similar to HTML but different language).\nDocumentation:\n\nJavaFX CSS\nFXML is best learned by poking around in the existing code, or reviewing the JavaFX docs in general.\n\nYou can also use the Scene Builder\ntool to do visual UI design. It can't load main.fxml but everything else should open just fine.\nlicense and legal stuff\nThe Lighthouse code is under the Apache license, which allows for commercial derivatives.\nHowever the Lighthouse logo and name are not open. In future the logo images and name may be taken\nout of this repository and put into a proprietary branch. This would be a similar arrangement to Firefox and Chrome.\nThe purpose of this is to ensure that if someone downloads an app called \"Lighthouse\" they know what they are getting\nand that it is not, for example, a fork or patched version of the app that may be broken in some way. If someone were\nto distribute a fork of the app, they would have to create a new name and logo so users can tell the fork apart from\nthe original.\nThus to avoid confusion if you'd like to distribute a version of the project that doesn't match the upstream sources,\nplease invent a new name and logo for it first. Thanks.\nbuilding from source\nBuilding Lighthouse from source requires the Maven build tool and the bitcoinj library.\nThe latest version of bitcoinj should be installed from source:\n$ git clone https:\/\/github.com\/bitcoinj\/bitcoinj\n$ cd bitcoinj\n$ mvn clean install\n\nCompile Lighthouse with:\n$ git clone https:\/\/github.com\/vinumeris\/lighthouse\n$ cd lighthouse\n$ mvn clean package\n\nRun Lighthouse with:\n$ java -jar client\/target\/shaded.jar\n\n","229":"LightQ\n\nIt is a high performance,  brokered messaging queue which supports transient (1M msg\/sec with microseconds latency)  and durable (~300K msg\/sec with milliseconds latency) queues. Durable queues are similar to Kafka  where data are written to the file and consumers consume from the file.\n###Features:\n\nHigh Performance - 1M+ msg\/sec\nLow Latency - in microseconds\nTransient and durable queue (similar to Kafka where producer writes to the file, consumer reads from the file)\nAuthentication per topic (userid\/password validation)\nHeader only project (embed within your project)\nConsumer in load balancing mode(pipeline):  One of the consumer gets a message mostly in round robin)\nConsumer as Subscribers (Each consumer gets a copy of a message)\nBoth subscriber and pipelining mode are supported for a single topic\nMulti Producers\/Consumers for a single topic\nUnlimited* topics per broker\nJSON protocol to create topic and join topic (at runtime)\nC++11 support\/require\nLogging support\nDynamic port allocation for topic and consumer\/producer bind uri\nApache License\nCluster support (todo)\nClient API (todo): C, Go, Java, Rust, Lua, Ruby\n\nIt is mostly header only project with main.cpp as an example for broker, producer and consumer.\nNOTE: This is an initial version and may not be ready for production use.\n##Protocol:\nCreate a Topic:\n(Admin userid and password must be passed to create a topic. Also we need to define userid\/password per topic which consumer\/produder need to pass for authentication)\n Send a request  to the broker\n{\n \"admin_password\": \"T0p$3cr31\",\n \"admin_user_id\": \"lightq_admin\",\n \"broker_type\": \"queue\",\n \"cmd\": \"create_topic\",\n \"password\": \"T0p$3cr31\",\n \"topic\": \"test\",\n \"user_id\": \"test_admin\"\n}\n\n\nResponse: \n{\n   \"cmd\": \"create_topic\",\n   \"description\": \"topic created successfully\",\n   \"status\": \"ok\"\n}\n\n###Join Topic (Consumer):\n(Need to pass userid\/password for topic 'test')\nRequest:\n{\n   \"cmd\": \"join\",\n   \"connection_type\": \"zmq\",\n   \"password\": \"T0p$3cr31\",\n   \"topic\": \"test\",\n   \"type\": \"pull\",\n   \"user_id\": \"test_admin\"\n}\nResponse: \n{\n   \"bind_uri\": \"tcp:\/\/127.0.0.1:5002\",\n   \"cmd\": \"join\",\n   \"status\": \"ok\",\n   \"topic\": \"test\"\n}\n\nJoin Topic (Producer):\n(Need to pass userid\/password for topic 'test')\nRequest:\n{\n   \"cmd\": \"join\",\n   \"connection_type\": \"zmq\",\n   \"password\": \"T0p$3cr31\",\n   \"topic\": \"test\",\n   \"type\": \"pub\",\n   \"user_id\": \"test_admin\"\n }\nResponse:\n{\n  \"bind_uri\": \"tcp:\/\/127.0.0.1:5003\",\n  \"cmd\": \"join\",\n  \"status\": \"ok\",\n  \"topic\": \"test\"\n}\n\nGet the statistics about the topic\nRequest:\n{\n   \"cmd\": \"stats\",\n   \"password\": \"T0p$3cr31\",\n   \"topic\": \"test\",\n   \"user_id\": \"test_admin\"\n}\nResponse:\n{\n  \"cmd\": \"stats\",\n  \"messages_received\": 9499570,\n  \"messages_sent\": 9491554,\n  \"publishers_count\": 1,\n  \"queue_size\": 8016,\n  \"status\": \"ok\",\n  \"subscribers_count\": 1,\n  \"total_bytes_read\": 0,\n  \"total_bytes_written\": 0\n\n}\n#Performance:\nLaptop hardware:\nMacBook Pro (Retina, 15-inch, Late 2013)\nProcessor 2.3 GHz Intel Core i7\nMemory 16 GB 1600 MHz DDR3\n\nBroker Type: Transient\n##100 bytes, 10M messages\nProducer:\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 100 event\nTotal Messages:10000000, Time Taken:8.46577 seconds.\nStart Time: 1427658489112, End Time:1427658497577\n1181227 messages per seconds.\n1000000000 bytes sent\n112.6507 MB per second\n\nConsumer:\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer queue zmq  pull event\nTotal Messages:10000001, Time Taken:8.47781 seconds.\nStart Time: 1427658489122, End Time:1427658497600\n1179550 messages per seconds.\n1000000004 bytes received\n112.4907 MB per second.\n\n##256 bytes 10M Messages\nProducer:\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 256 event\nTotal Messages:10000000, Time Taken:9.2752 seconds.\nStart Time: 1427658738559, End Time:1427658747834\n1078143 messages per seconds.\n2560000000 bytes sent\n263.2186 MB per second.\n\nConsumer:\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer queue zmq  pull event\nTotal Messages:10000001, Time Taken:9.30292 seconds.\nStart Time: 1427658738562, End Time:1427658747865\n1074931 messages per seconds.\n2560000004 bytes received\n262.4345 MB per second.\n\n512 bytes 10M Messages\nProducer:\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 512 event\nTotal Messages:10000000, Time Taken:10.5182 seconds.\nStart Time: 1427658940094, End Time:1427658950612\n950734 messages per seconds.\n5120000000 bytes sent\n464.2258 MB per second.\n\nConsumer:\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer queue zmq  pull event\nTotal Messages:10000001, Time Taken:10.5296 seconds.\nStart Time: 1427658940097, End Time:1427658950627\n949706 messages per seconds.\n5120000004 bytes received\n463.7239 MB per second.\n\n1024 bytes 10M Messages\nProducer:\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 1024 event\nTotal Messages:10000000, Time Taken:19.8285 seconds.\nStart Time: 1427659063592, End Time:1427659083420\n504324 messages per seconds.\n10240000000 bytes sent\n492.5049 MB per second.\n\nConsumer:\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer queue zmq  pull event\nTotal Messages:10000001, Time Taken:19.8222 seconds.\nStart Time: 1427659063603, End Time:1427659083425\n504485 messages per seconds.\n10240000004 bytes received\n492.6617 MB per second.\n\n#Performance: (Durable broker: file)\n##100 bytes, 10M messages\nProducer:\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 100 event\nTotal Messages:10000000, Time Taken:25.0786 seconds.\nStart Time: 1427659575158, End Time:1427659600236\n398746 messages per seconds.\n1000000000 bytes sent\n38.0275 MB per second.\n\nConsumer:\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer file socket  pull event\nTotal Messages:10000001, Time Taken:25.0945 seconds.\nStart Time: 1427659575170, End Time:1427659600264\n398493 messages per seconds.\n1000000004 bytes received\n38.0033 MB per second.\n\n##256 bytes 10M Messages\nProducer:\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 256 event\nTotal Messages:10000000, Time Taken:28.3802 seconds.\nStart Time: 1427659339399, End Time:1427659367779\n352358 messages per seconds.\n2560000000 bytes sent\n86.0250 MB per second.\n\nConsumer:\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer file socket  pull event\nTotal Messages:10000001, Time Taken:28.3939 seconds.\nStart Time: 1427659339410, End Time:1427659367804\n352188 messages per seconds.\n2560000004 bytes received\n85.9834 MB per second.\n\n512 bytes 10M Messages\nProducer:\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 512 event\nTotal Messages:10000000, Time Taken:31.0832 seconds.\nStart Time: 1427890737326, End Time:1427890768409\n321716 messages per seconds.\n5120000000 bytes sent\n157.0884 MB per second.\n\nConsumer:\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer file socket  pull event\nTotal Messages:10000001, Time Taken:31.0935 seconds.\nStart Time: 1427890737329, End Time:1427890768423\n321610 messages per seconds.\n5120000004 bytes received\n157.0363 MB per second.\n\n1024 bytes 10M Messages\nProducer:\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 1024 event\nTotal Messages:10000000, Time Taken:37.4027 seconds.\nStart Time: 1427890878446, End Time:1427890915848\n267360 messages per seconds.\n10240000000 bytes sent\n261.0942 MB per second.\n\nConsumer:\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer file socket  pull event\nTotal Messages:10000001, Time Taken:37.4166 seconds.\nStart Time: 1427890878455, End Time:1427890915871\n267260 messages per seconds.\n10240000004 bytes received\n260.9970 MB per second.\n\nPerformance Test with 100M messages: https:\/\/github.com\/rohitjoshi\/LightQ\/blob\/master\/PerfTest100M.md\n###Example: (transient broker)\nStart Broker: (broker type:  queue, logging level: event)\n.\/dist\/Release\/GNU-MacOSX\/lightq queue event\n\nStart Consumer: (client: consumer,  broker type: queue, client socket: zmq, logging level: event)\n.\/dist\/Release\/GNU-MacOSX\/lightq consumer queue zmq  pull event\n\nStart Producer:  (client: producer, number of messages 10M, payload size: 100 bytes, logging level: event)\n.\/dist\/Release\/GNU-MacOSX\/lightq producer 10000000 100 event\n\n##License : \nDependecies:\nZeroMQ LGPL: \n","230":"Hexagen\nTrue coroutines for Swift, and several familiar concurrency structures built on top of them.\nFeatures\n\n\nVery little boilerplate for most use cases. (Largely made possible by Swift's type inference.)\n\n\nSimple unidirectional and bidirectional generator functions, as in Python, C#, ECMAscript 6, etc.:\nlet counter = { (n: Int) in Gen<Int> { yield in\n    for i in 0..<n {\n        yield(i)\n    }\n}}\n\nfor i in counter(5) {\n    println(i)\n}\n\n\nInterruptible Grand Central Dispatch task API allowing you to write asynchronous code in straightforward blocking style. When a task is waiting on some event (a timer firing, I\/O availability or completion, etc.), instead of blocking the thread, it will suspend itself so its dispatch queue can continue processing tasks. When the event arrives, a block to resume the task is added to its dispatch queue.\n\n\nIncluded abstractions that know how to seamlessly suspend and resume tasks as needed:\n\nChannel: Supports a style of communication between tasks largely inspired by Go's channels and Goroutines.\nPromise: Allows any number of tasks to await a potentially pending result and awakens all of them when one becomes available.\n\nTimer: A Promise that is marked as fulfilled at a specified time.\nFeed: A lazily constructed open-ended series of Promises of a given type wrapped in Optional: after receiving a value from a Promise obtained from a Feed, you can get its successor and await the next value, and repeat until nil is returned, indicating that the feed has ended and won't contain any further values. (Feed implements Sequence so you can iterate over it with a for loop, this is often the most straightforward way to use it.)\n\nAsyncGen: A Task subclass with additional generator-like behavior \u2014 the body function receives a \"post\" function which is used somewhat like yield, but doesn't actually suspend the task; instead it sends values to an internal Feed, which other tasks can subscribe to by iterating over the task object.\n\n\n\n\n\n\n\n\n\n97% elegant! Very minimal abomination content, you should almost never have to encounter it.\n\n\nWarnings\n\n\nHexagen is in early development and pretty experimental to begin with, don't count on the API not changing drastically.\n\n\nYour coroutines should always exit by returning \u2014 you can leave them hanging but you will leak memory. With Swift's lack of exceptions and use of ARC instead of garbage collection, I don't currently see a way to unilaterally tell a coroutine to terminate but still clean up after itself.\n\n\nThe task API needs to account for Cocoa APIs with thread-local behavior in order to make them work coherently and currently doesn't. This may be tricky in the cases of components that don't expose their thread-local variables in any directly manipulable form. Expect it to interact badly with autorelease pools.\n\n\nMore generally, this approach has turned out to be very surprisingly low on complications so far, but the whole thing is still a sketchy self-indulgent hack that violates some basic assumptions that almost all existing Objective-C and Swift code can expect to safely make. It's hard to say what potential interactions I might be overlooking, particularly given that the Swift toolchain is still closed-source. For now, I strongly discourage using this in production code unless you are very, very silly and reasonably confident that you are already going to hell.\n\n\nIf you are under the age of 180\u00ba or find this framework offensive, please don't look at it.\n\n\nNotes\n\n\nHexagen is written for Swift 1.2, first available in Xcode 6.3.\n\n\nCurrently Promises as implemented here are fulfill-only, i.e. there isn't a separate path for errors to take, like there tends to be in other languages' implementations of Promises. This is meant to mirror Swift's overall approach to error handling: to the extent that you need to write Promises that can express error conditions, you should encode that in your own types.\n\n\nIdeas\/Todo\n\nLibrary components\n\nSelect\nTimeouts\nRead-only and write-only views of channels (and promises?)\nElegant task-aware I\/O API\nSubscribe to Cocoa events, notifications, key-value observing, etc. via Promises\/Feeds\nBridges to and from Hexagen features for existing widely-used Swift\/Objective-C concurrency libraries\/frameworks\/approaches\nTask-local storage API\n\n\nInternals\n\nAdapt to use Boost.Context directly once the upcoming version with execution_context is released and Xcode is shipping with usable support for thread_local in clang\/libc++?\n\n\nProject quality\n\nUnit tests\nBenchmarks\nMore examples, better organized examples\n\n\n\nExtra Credit\n\nSide project: implement an alternative framework based on stackless generators (like e.g. Python's built-in yield: a function can only yield from itself to the function that called or most recently reentered it, because it's implemented more like an ordinary function call, getting its own frame on top of the current stack while it's running rather than having a separate stack to switch to).\nImplement exception handling in pure Swift using Hexagen. (Completing this successfully is worth negative points, and I will grudgingly respect you but never fully trust you.)\n\nColophon\nHexagen is released under an MIT license (see LICENSE.md), so you can pretty freely incorporate and redistribute it wherever. The internal context-switching primitive is a thin wrapper around Boost.Coroutine. Boost is free software under a permissive MIT-style license and the parts of it used by Hexagen are already included in this repository.\nMy name is Alice Atlas and I wrote the rest of this, I did it on purpose and I'm not sorry dad\n","231":"shok\nhttp:\/\/shok.io\nThe shok command shell is a non-POSIX interactive command language interpreter with an expressive scripting language.  It intends to be a modern, discoverable shell intended for every-day command invocation, process management, and filesystem manipulation.\nstatus\nshok is in its early stages of initial development.  It has the framework for a shell, a lexer, a parser, and an \"evaluator\" (type-checking, AST execution, program invocation).  If it compiles, it may let you change directories and run commands but not write programs, or much else really.  Most core features have yet to be implemented.  All language attributes are suitable for discussion and replacement.  Nevertheless, it is progressing quickly and steadily.\nGet involved!  See http:\/\/shok.io for details.  Description of the code layout and steps forward are coming soon.\ntodo\nImmediate hacking priorities:\n\n\nfunctions \/ methods\n\n\ntrivial implementations of a few basic objects\n\n\nstring literals\n\n\nbasic interactive niceties: left\/right\/home\/end, ^L\n\n\ncomments\n\n\n","232":"What is VoodooSpark?\nVoodooSpark is a customized firmware build for Particle's Spark Core and Photon devices to allow a remote interface definition of the firmware API over a local TCP connection. The intent is to allow client-side programs to directly control the Particle devices in real-time regardless of their programming language. The interface exposed is directly mapped to the one provided by Particle available in their docs section.\nThe VoodooSpark uses the Particle Cloud and its REST API to provide IP address and port information to the local Particle device. It will then initiate a direct connection to the host machine, on which will need to be a TCP server. Once the connection has been made, the host machine can drive the Particle devices using the binary protocol defined below to effectively execute firmware API level commands dynamically.\nLoading the Firmware\nWith your Particle device connected to a Wifi network and has already gone through the \"claim\"\/ownership process:\n\nOpen the Particle.io Editor with the credentials used when going through the claiming process.\nCopy and paste the entire contents of firmware\/voodoospark.cpp into the editor window.\nClick \"Verify\"\nClick \"Flash\"\nOnce the flashing process is complete, close the Particle.io Editor.\n\nAlternately, the firmware may be loaded using the Particle CLI (particle-cli) instead of the Particle.io Editor:\nnpm install -g particle-cli\nparticle cloud login\nparticle cloud flash PARTICLE_DEVICE_ID firmware\/voodoospark.cpp\nNow your Particle device is running VoodooSpark, lets connect to it!\nConnecting the Particle device to You!\nThe way VoodooSpark works is to use the Particle Cloud as a channel to identify where to initiate the TCP connection to the device from your host machine.\nIn order to connect the Particle device to your computer, you will first need to issue an HTTP GET request to the Particle Cloud. This can be done via any programming language, but for this example we are using a simple CURL command. You will need some information outlined with curly braces below, please note the {DEVICE-ID} and {ACCESS-TOKEN} are available from the Particle.io Editor\ncurl https:\/\/api.particle.io\/v1\/devices\/{DEVICE-ID}\/endpoint?access_token={ACCESS-TOKEN}\n\nThis should return a JSON document that looks similar to this:\n{\n  \"cmd\": \"VarReturn\",\n  \"name\": \"endpoint\",\n  \"result\": \"192.168.1.10:48879\",\n  \"coreInfo\": {\n    \"last_app\": \"\",\n    \"last_heard\": \"2014-05-08T02:51:48.826Z\",\n    \"connected\": true,\n    \"deviceID\": \"{DEVICE-ID}\"\n  }\n}\n\nThe \"result\" value is the IP address of the Particle Device on your local network and the part after the colon (:) is the port that the server is currently listening on. This port will by default be 48879 (0xBEEF), but can be changed in the voodoospark firmware. Please do not hardcode the port for this reason, rather use the data returned back as the response.\nWith the IP Address and TCP port information, use your favorite language or TCP client to connect to the device (even telnet will work) and send it the necessary BINARY protocol commands to trigger the desired API interactions as defined in our API Command guide.\nHow to Debug\nIn case you want to see what is going inside the VoodooSpark in real-time, we have built in a lot of debug hooks for you. You will need a USB cable and the screen or minicom utilities (one of them) on unix. Modify the firmware loaded in the Particle build system to convert the line:\n#define DEBUG 0\n\nto the following definition:\n#define DEBUG 1\n\nThis will enable debug mode, boot a serial port connection on the device and present on your computer for you to watch the inside voodoo. Be sure to flash the new firmware to your device, this is very important and easy to miss. Once the flashing finishes, do an\nls \/dev\n\nLook for something similar to tty.usbmodem1411 yours may be different, but will be something like tty.usbmodemABCD. Now using your favorite term app connect to that port using the baud rate of 115200.\nFor screen this command will look like:\nscreen \/dev\/tty.usbmodem1411 115200\n\nReference Implementations\n\nParticle-io node.js\nVspark Go\n\nLicense\nSee LICENSE file.\nMade With Voodoo\nThis firmware is made and cared for by the following awesome people:\n\nChris Williams https:\/\/github.com\/voodootikigod\nRick Waldron https:\/\/github.com\/rwaldron\nDavid Resseguie https:\/\/github.com\/Resseguie\nBrian Genisio https:\/\/github.com\/BrianGenisio\n\n","233":"Tesseract Lib for iOS\nAbout\nThis project contains only the leptonica and tesseract-ocr libraries compiled for iOS.\nThere is no support for armv6, so it won't work with iPhone 1st Gen and iPhone 3G.\nUsage\nYou might want to use this tesseract-ios to include Tesseract in your iOS project. For a more advanced usage, you can use the raw library with regular C++ code.\nDon't forget to rename your implementation classes with .mm instead of .m as it uses C++ code.\nCode Sample\nFollow this blog post for more informations.\n","234":"PROJECT ON HOLD - Waiting for Jupyter ascending\nThe IPython project is undergoing heavy development at the moment as it is split into the Jupyter environment for interactive computing with IPython as just one of many supported kernels. As such, developing a desktop wrapped around the project means tracking a rapidly moving target. Therefore, I've decided to wait until things settle down a bit - likely the split will be more or less stabilised for version 4.0 which is planned for release later this year, at which point development of IPython desktop can start up again.\nIPython Notebook Desktop\nThis is a proof of concept desktop interface for the IPython Notebook.\nWhat's new\nThe latest revision improves ipython configuration and process handling. It will now try to automatically figure out the location of your ipython install and the url where the server is available when launched.\nConcept\nIt's well established that IPython is awesome.\nMost IPython users end up using local installs of the IPython notebook in their browser. However this is somewhat clunky, mixing the browser interface and the notebook interface and generally requiring a trip to the command line to get the server running.\nThe IPython Notebook Desktop wraps the webapp in a more friendly interface, powered by node-webkit. You can configure a notebook to run to power the interface (optionally have it run on startup).\nWhat this does NOT do is provide you with an IPython installation. This is deliberate, since people have different needs and tastes with regards to their Python installs. Some people want to use the Python bundled with their operating system, others use Python distributions like Canopy or Anaconda. With IPython Desktop the Python distribution and the interface are separate, but you must configure IPython desktop to use your IPython installation.\nThe IPython Notebook Desktop doesn't aim to make it easier to install a scientific python environment, but should be easy enough to get by itself. It could eventually be a candidate for bundling with existing packages or with IPython itself.\nPretty pictures\nIPython embedded\n\nStart Screen:\n\nConfig Screen:\n\nGet it!\nApp bundle for Mac\nYou also need to have IPython installed. My personal recommendation is the Anaconda python distribution if you are mainly doing science and engineering work.\nComing sometime - binary bundles for each platform. Contributions welcome\nConfiguration\nIPython desktop can either launch the IPython notebook server for you or connect to an existing URL.\nTo launch a server you must specify the location of your IPython executable, by default this is pre-filled (using the output of the command which ipython). You can optionally specify a Profile to use (which will be used with --profile=...)\nIMPORTANT - you must supply the full path to your IPython install otherwise it will fail to launch the ipython server\nWARNING: ipython-desktop is by no means idiot proof at the moment. If you don't configure it correctly the page will simply fail to load without explanation. This should improve in future versions.\nURL only\nIf you set the \"remote\" option in the config you can simply type in the URL of your IPython server including http:\/\/ at the front! handy if you just want a nicer interface for a remote system or just for testing.\nBuilding ipython-desktop\nIn theory, the following steps should work (on Mac):\nRequirements\n\nXcode developer tools installed\nnode (nodejs) with npm, if you use Homebrew (and you should) just do brew install node.\ngrunt and grunt-cli (npm install -g grunt grunt-cli normally you will have to use sudo)\n\nSet up the project\nIn the terminal, cd into the source folder. Run npm install, grunt nodewebkit, grunt install to set up the dependencies for ipython desktop.\nFINALLY you should be able to run grunt run and see you shiny new ipython-desktop app, ready to configure.\nKnown Issues\n\nIMPORTANT - you must configure ipython desktop with the FULL PATH of your ipython executable\nCertain combinations of starting\/stopping ipython servers and opening\/closing windows might leave orphaned IPython processes (especially if you force quit the app)\n\nTODO\n\nBundles for all OSes\nAdd fault tolerance e.g. for missing or misconfigured Ipython\nMore user friendly configuration of ipython\nbetter integration with ipython notebooks - start\/stop events, clean shutdown\nIntegration with Native menus!\nTry to find the current iPython install using \"which iPython\" -> Done!\nTry to auto-config profiles using \"ipython profile locate\"\nGet url\/port of running ipython using json from profile folder\n\nSimilar Work\n\nCanopy: Enthought provide their Canopy desktop interface with IPython notebook integration. However this ties you into the EPD distribution. The IPython Notebook Desktop aims to be a lighter, more versatile solution\nIPython notebook Works in a similar vein, though is Mac only. It also differs in aim, since it bundles the essentials for scientific python computing. My aim with this project is to allow the interface to work with different Python installs, making it possible to use different python version and different virtual environments.\nIPyApp Another project that uses node-webkit to wrap IPython notebook, but embeds the full python executable environment in the app.\n\nCredits\nIPython desktop is powered by Node Webkit and makes use of the angular-desktop-app template. Icon is IPython faenza\nLICENCE\nThis software is currently under LGPL licence.\n","235":"PROJECT ON HOLD - Waiting for Jupyter ascending\nThe IPython project is undergoing heavy development at the moment as it is split into the Jupyter environment for interactive computing with IPython as just one of many supported kernels. As such, developing a desktop wrapped around the project means tracking a rapidly moving target. Therefore, I've decided to wait until things settle down a bit - likely the split will be more or less stabilised for version 4.0 which is planned for release later this year, at which point development of IPython desktop can start up again.\nIPython Notebook Desktop\nThis is a proof of concept desktop interface for the IPython Notebook.\nWhat's new\nThe latest revision improves ipython configuration and process handling. It will now try to automatically figure out the location of your ipython install and the url where the server is available when launched.\nConcept\nIt's well established that IPython is awesome.\nMost IPython users end up using local installs of the IPython notebook in their browser. However this is somewhat clunky, mixing the browser interface and the notebook interface and generally requiring a trip to the command line to get the server running.\nThe IPython Notebook Desktop wraps the webapp in a more friendly interface, powered by node-webkit. You can configure a notebook to run to power the interface (optionally have it run on startup).\nWhat this does NOT do is provide you with an IPython installation. This is deliberate, since people have different needs and tastes with regards to their Python installs. Some people want to use the Python bundled with their operating system, others use Python distributions like Canopy or Anaconda. With IPython Desktop the Python distribution and the interface are separate, but you must configure IPython desktop to use your IPython installation.\nThe IPython Notebook Desktop doesn't aim to make it easier to install a scientific python environment, but should be easy enough to get by itself. It could eventually be a candidate for bundling with existing packages or with IPython itself.\nPretty pictures\nIPython embedded\n\nStart Screen:\n\nConfig Screen:\n\nGet it!\nApp bundle for Mac\nYou also need to have IPython installed. My personal recommendation is the Anaconda python distribution if you are mainly doing science and engineering work.\nComing sometime - binary bundles for each platform. Contributions welcome\nConfiguration\nIPython desktop can either launch the IPython notebook server for you or connect to an existing URL.\nTo launch a server you must specify the location of your IPython executable, by default this is pre-filled (using the output of the command which ipython). You can optionally specify a Profile to use (which will be used with --profile=...)\nIMPORTANT - you must supply the full path to your IPython install otherwise it will fail to launch the ipython server\nWARNING: ipython-desktop is by no means idiot proof at the moment. If you don't configure it correctly the page will simply fail to load without explanation. This should improve in future versions.\nURL only\nIf you set the \"remote\" option in the config you can simply type in the URL of your IPython server including http:\/\/ at the front! handy if you just want a nicer interface for a remote system or just for testing.\nBuilding ipython-desktop\nIn theory, the following steps should work (on Mac):\nRequirements\n\nXcode developer tools installed\nnode (nodejs) with npm, if you use Homebrew (and you should) just do brew install node.\ngrunt and grunt-cli (npm install -g grunt grunt-cli normally you will have to use sudo)\n\nSet up the project\nIn the terminal, cd into the source folder. Run npm install, grunt nodewebkit, grunt install to set up the dependencies for ipython desktop.\nFINALLY you should be able to run grunt run and see you shiny new ipython-desktop app, ready to configure.\nKnown Issues\n\nIMPORTANT - you must configure ipython desktop with the FULL PATH of your ipython executable\nCertain combinations of starting\/stopping ipython servers and opening\/closing windows might leave orphaned IPython processes (especially if you force quit the app)\n\nTODO\n\nBundles for all OSes\nAdd fault tolerance e.g. for missing or misconfigured Ipython\nMore user friendly configuration of ipython\nbetter integration with ipython notebooks - start\/stop events, clean shutdown\nIntegration with Native menus!\nTry to find the current iPython install using \"which iPython\" -> Done!\nTry to auto-config profiles using \"ipython profile locate\"\nGet url\/port of running ipython using json from profile folder\n\nSimilar Work\n\nCanopy: Enthought provide their Canopy desktop interface with IPython notebook integration. However this ties you into the EPD distribution. The IPython Notebook Desktop aims to be a lighter, more versatile solution\nIPython notebook Works in a similar vein, though is Mac only. It also differs in aim, since it bundles the essentials for scientific python computing. My aim with this project is to allow the interface to work with different Python installs, making it possible to use different python version and different virtual environments.\nIPyApp Another project that uses node-webkit to wrap IPython notebook, but embeds the full python executable environment in the app.\n\nCredits\nIPython desktop is powered by Node Webkit and makes use of the angular-desktop-app template. Icon is IPython faenza\nLICENCE\nThis software is currently under LGPL licence.\n","236":"PROJECT ON HOLD - Waiting for Jupyter ascending\nThe IPython project is undergoing heavy development at the moment as it is split into the Jupyter environment for interactive computing with IPython as just one of many supported kernels. As such, developing a desktop wrapped around the project means tracking a rapidly moving target. Therefore, I've decided to wait until things settle down a bit - likely the split will be more or less stabilised for version 4.0 which is planned for release later this year, at which point development of IPython desktop can start up again.\nIPython Notebook Desktop\nThis is a proof of concept desktop interface for the IPython Notebook.\nWhat's new\nThe latest revision improves ipython configuration and process handling. It will now try to automatically figure out the location of your ipython install and the url where the server is available when launched.\nConcept\nIt's well established that IPython is awesome.\nMost IPython users end up using local installs of the IPython notebook in their browser. However this is somewhat clunky, mixing the browser interface and the notebook interface and generally requiring a trip to the command line to get the server running.\nThe IPython Notebook Desktop wraps the webapp in a more friendly interface, powered by node-webkit. You can configure a notebook to run to power the interface (optionally have it run on startup).\nWhat this does NOT do is provide you with an IPython installation. This is deliberate, since people have different needs and tastes with regards to their Python installs. Some people want to use the Python bundled with their operating system, others use Python distributions like Canopy or Anaconda. With IPython Desktop the Python distribution and the interface are separate, but you must configure IPython desktop to use your IPython installation.\nThe IPython Notebook Desktop doesn't aim to make it easier to install a scientific python environment, but should be easy enough to get by itself. It could eventually be a candidate for bundling with existing packages or with IPython itself.\nPretty pictures\nIPython embedded\n\nStart Screen:\n\nConfig Screen:\n\nGet it!\nApp bundle for Mac\nYou also need to have IPython installed. My personal recommendation is the Anaconda python distribution if you are mainly doing science and engineering work.\nComing sometime - binary bundles for each platform. Contributions welcome\nConfiguration\nIPython desktop can either launch the IPython notebook server for you or connect to an existing URL.\nTo launch a server you must specify the location of your IPython executable, by default this is pre-filled (using the output of the command which ipython). You can optionally specify a Profile to use (which will be used with --profile=...)\nIMPORTANT - you must supply the full path to your IPython install otherwise it will fail to launch the ipython server\nWARNING: ipython-desktop is by no means idiot proof at the moment. If you don't configure it correctly the page will simply fail to load without explanation. This should improve in future versions.\nURL only\nIf you set the \"remote\" option in the config you can simply type in the URL of your IPython server including http:\/\/ at the front! handy if you just want a nicer interface for a remote system or just for testing.\nBuilding ipython-desktop\nIn theory, the following steps should work (on Mac):\nRequirements\n\nXcode developer tools installed\nnode (nodejs) with npm, if you use Homebrew (and you should) just do brew install node.\ngrunt and grunt-cli (npm install -g grunt grunt-cli normally you will have to use sudo)\n\nSet up the project\nIn the terminal, cd into the source folder. Run npm install, grunt nodewebkit, grunt install to set up the dependencies for ipython desktop.\nFINALLY you should be able to run grunt run and see you shiny new ipython-desktop app, ready to configure.\nKnown Issues\n\nIMPORTANT - you must configure ipython desktop with the FULL PATH of your ipython executable\nCertain combinations of starting\/stopping ipython servers and opening\/closing windows might leave orphaned IPython processes (especially if you force quit the app)\n\nTODO\n\nBundles for all OSes\nAdd fault tolerance e.g. for missing or misconfigured Ipython\nMore user friendly configuration of ipython\nbetter integration with ipython notebooks - start\/stop events, clean shutdown\nIntegration with Native menus!\nTry to find the current iPython install using \"which iPython\" -> Done!\nTry to auto-config profiles using \"ipython profile locate\"\nGet url\/port of running ipython using json from profile folder\n\nSimilar Work\n\nCanopy: Enthought provide their Canopy desktop interface with IPython notebook integration. However this ties you into the EPD distribution. The IPython Notebook Desktop aims to be a lighter, more versatile solution\nIPython notebook Works in a similar vein, though is Mac only. It also differs in aim, since it bundles the essentials for scientific python computing. My aim with this project is to allow the interface to work with different Python installs, making it possible to use different python version and different virtual environments.\nIPyApp Another project that uses node-webkit to wrap IPython notebook, but embeds the full python executable environment in the app.\n\nCredits\nIPython desktop is powered by Node Webkit and makes use of the angular-desktop-app template. Icon is IPython faenza\nLICENCE\nThis software is currently under LGPL licence.\n","237":"\nlibsvm-ruby-swig\u00b6 \u2191\n\nRuby interface to LIBSVM (using SWIG)\n\nwww.tomzconsulting.com\n\ntweetsentiments.com\n\nDESCRIPTION:\u00b6 \u2191\nThis is the Ruby port of the LIBSVM Python SWIG (Simplified Wrapper and  Interface Generator) interface.\nA slightly modified version of LIBSVM 2.9 is included, it allows turrning on\/off the debug log. You don't need your own copy of SWIG to use this library - all  needed files are generated using SWIG already.\nLook for the README file in the ruby subdirectory for instructions. The binaries included were built under Ubuntu Linux 2.6.28-18-generic x86_64, you should run make under the libsvm-2.9 and libsvm-2.9\/ruby  directories to regenerate the executables for your environment.\nLIBSVM is in use at tweetsentiments.com - A Twitter \/ Tweet sentiment analysis application\nINSTALL:\u00b6 \u2191\nCurrently the gem is available on linux only(tested on Ubuntu 8-9 and Fedora 9-12, and on OS X by danielsdeleo), and you will need g++ installed to compile the  native code. \nsudo gem sources -a http:\/\/gems.github.com   (you only have to do this once)\nsudo gem install tomz-libsvm-ruby-swig\nSYNOPSIS:\u00b6 \u2191\nQuick Interactive Tutorial using irb (adopted from the python code from Toby Segaran's \u201cProgramming Collective Intelligence\u201d book):\nirb(main):001:0> require 'svm'\n=> true\nirb(main):002:0> prob = Problem.new([1,-1],[[1,0,1],[-1,0,-1]])\nirb(main):003:0> param = Parameter.new(:kernel_type => LINEAR, :C => 10)\nirb(main):004:0> m = Model.new(prob,param)\nirb(main):005:0> m.predict([1,1,1])\n=> 1.0\nirb(main):006:0> m.predict([0,0,1])\n=> 1.0\nirb(main):007:0> m.predict([0,0,-1])\n=> -1.0\nirb(main):008:0> m.save(\"test.model\")\nirb(main):009:0> m2 = Model.new(\"test.model\")\nirb(main):010:0> m2.predict([0,0,-1])\n=> -1.0\nAUTHOR:\u00b6 \u2191\nTom Zeng\n\ntwitter.com\/tomzeng\n\nwww.tomzconsulting.com\n\nwww.linkedin.com\/in\/tomzeng\n\ntom.z.zeng at gmail dot com\n\n","238":"\nlibsvm-ruby-swig\u00b6 \u2191\n\nRuby interface to LIBSVM (using SWIG)\n\nwww.tomzconsulting.com\n\ntweetsentiments.com\n\nDESCRIPTION:\u00b6 \u2191\nThis is the Ruby port of the LIBSVM Python SWIG (Simplified Wrapper and  Interface Generator) interface.\nA slightly modified version of LIBSVM 2.9 is included, it allows turrning on\/off the debug log. You don't need your own copy of SWIG to use this library - all  needed files are generated using SWIG already.\nLook for the README file in the ruby subdirectory for instructions. The binaries included were built under Ubuntu Linux 2.6.28-18-generic x86_64, you should run make under the libsvm-2.9 and libsvm-2.9\/ruby  directories to regenerate the executables for your environment.\nLIBSVM is in use at tweetsentiments.com - A Twitter \/ Tweet sentiment analysis application\nINSTALL:\u00b6 \u2191\nCurrently the gem is available on linux only(tested on Ubuntu 8-9 and Fedora 9-12, and on OS X by danielsdeleo), and you will need g++ installed to compile the  native code. \nsudo gem sources -a http:\/\/gems.github.com   (you only have to do this once)\nsudo gem install tomz-libsvm-ruby-swig\nSYNOPSIS:\u00b6 \u2191\nQuick Interactive Tutorial using irb (adopted from the python code from Toby Segaran's \u201cProgramming Collective Intelligence\u201d book):\nirb(main):001:0> require 'svm'\n=> true\nirb(main):002:0> prob = Problem.new([1,-1],[[1,0,1],[-1,0,-1]])\nirb(main):003:0> param = Parameter.new(:kernel_type => LINEAR, :C => 10)\nirb(main):004:0> m = Model.new(prob,param)\nirb(main):005:0> m.predict([1,1,1])\n=> 1.0\nirb(main):006:0> m.predict([0,0,1])\n=> 1.0\nirb(main):007:0> m.predict([0,0,-1])\n=> -1.0\nirb(main):008:0> m.save(\"test.model\")\nirb(main):009:0> m2 = Model.new(\"test.model\")\nirb(main):010:0> m2.predict([0,0,-1])\n=> -1.0\nAUTHOR:\u00b6 \u2191\nTom Zeng\n\ntwitter.com\/tomzeng\n\nwww.tomzconsulting.com\n\nwww.linkedin.com\/in\/tomzeng\n\ntom.z.zeng at gmail dot com\n\n","239":"A Lineman JS Template using Angular\n\nThis is a project template for Angular JS applications using Lineman.\nIt includes the following features:\n\nTemplate Precompilation into Angulars $templateCache using grunt-angular-templates\nA basic login, logout service bound to sample routes inside config\/server.js\nA router, and 2 views home and login\nA directive that shows a message on mouseover\n2 Controllers, for home and login, with $scope variables set and bound\nA working, bound login form (username\/password don't matter, but are required)\nConfigured grunt-ng-annotate so you don't have to fully qualify angular dependencies.\nAuto generated sourcemaps with inlined sources via grunt-concat-sourcemap (you'll need to enable sourcemaps in Firefox\/Chrome to see this)\nUnit Tests and End-to-End Tests\nConfiguration to run Protractor for End-to-End Tests\n\nInstructions\n\ngit clone https:\/\/github.com\/linemanjs\/lineman-angular-template.git my-lineman-app\ncd my-lineman-app\nsudo npm install -g lineman\nnpm install\nlineman run\nopen your web browser to localhost:8000\n\nRunning Tests\nThis template was used as the basis of @davemo's Testing Strategies for Angular JS screencast, and contains all the tests we wrote in the screencast and a few more!\nTo run the unit tests:\n\nlineman run from 1 terminal window\nlineman spec from another terminal window, this will launch Testem and execute specs in Chrome\n\nTo run the end-to-end tests:\nEnd-to-End Tests\n\nnpm install protractor\n.\/node_modules\/protractor\/bin\/webdriver-manager update\nMake sure you have chrome installed.\nlineman run from 1 terminal window\nlineman grunt spec-e2e from another terminal window\n\nDefining your apps angular.module in CoffeeScript\nIf you are using Coffeescript to define the angular.module for your app, you will need to swap the concat order in config\/application.js such that coffeescript files are included before javascript. (If you are using JavaScript for defining the angular.module the default concat order is fine).\nAdd the following concat_sourcemap block to config\/application.js if you want to define your app module in coffeescript:\nmodule.exports = function(lineman) {\n  return {\n\n    concat_sourcemap: {\n      js: {\n        src: [\n          \"<%= files.js.vendor %>\",\n          \"<%= files.coffee.generated %>\",\n          \"<%= files.js.app %>\",\n          \"<%= files.ngtemplates.dest %>\"\n        ]\n      }\n    }\n\n  };\n};\nHopefully this helps you get up and running with AngularJS!\n","240":"Red Dwarf (LOOKING FOR A NEW MAINTAINER)\n\nAbout\nRed Dwarf is a heatmap visualization of GitHub repository stargazers.\nPlay with the live demo.\nHow it Works\nRed Dwarf uses the GitHub API to determine the locations of people who have starred a given repository. Then, using the Google Maps API, these locations are translated into geocoordinates and fed into a heatmap. The result is a beautiful and detailed visualization of global positions of a repository's stargazers.\nGetting Started\nRed Dwarf depends on Google Maps for geocoding and mapping. You must get a Google Maps API key to access these services.\nUsage\nImport the Google Maps JavaScript API. Appending libraries=visualization to the source path will ensure that you have the heatmap library available. Replace YOUR_KEY with the API key provided to you by Google.\n<script src=\"http:\/\/maps.googleapis.com\/maps\/api\/js?key=YOUR_KEY&sensor=false&libraries=visualization\"><\/script>\nInstantiate a new RedDwarf object, the constructor of which accepts a configuration object (see Settings).\nvar stars = new RedDwarf({\n\tuser: config.user,\n\trepository: config.repository,\n\tmap_id: config.map_id\n});\nSettings\n\nuser (required)\nThe GitHub user login of the repository owner.\nrepository (required)\nThe GitHub repository name.\nmap_id (required)\nThe ID of the HTML element in which to draw the map.\ncache_location\nThe path (relative or absolute) of a JSON file containing precomputed geolocation data. If omitted, Red Dwarf will compute all data from scratch (see Performance). Note: this file's contents are equivalent to the output of the toJSON method.\nmap_zoom The initial zoom level of the heatmap. Default: 2. (more info)\nmap_lat The initial latitude position on which the heatmap is centered. Default: 20. (20 degrees north of the equator) (more info)\nmap_lng The initial longitude position on which the heatmap is centered. Default: 0. (Prime Meridian) (more info)\nmap_type The initial type of the heatmap: road, satellite, hybrid, or terrain. Default: \"roadmap\". (more info)\n\nMethods\n\ntoJSON Returns a JSON representation of a mapping of string locations to geocodes, the number of repository stars, and a mapping of stargazers' user logins to their respective user objects.\n\nEvents\nRed Dwarf will trigger each of the following events during processing. Arguments passed to the event handlers are listed below the event name (where applicable).\nEvent handlers are defined by including functions keyed by the respective event name in the settings object.\n\nonRepositoryLoaded Fired after successfully loading repository info from the GitHub API.\n\ndata The data object returned by GitHub.\n\n\nonRepositoryError Fired after unsuccessfully loading repository info from the GitHub API.\n\nmessage The error message returned by GitHub.\n\n\nonCacheLoaded Fired after successfully loading the JSON cache file.\nonStargazersUpdated Fired after processing a chunk of at most 100 repository stargazers.\n\nnum_stargazers The number of stargazers processed so far.\n\n\nonStargazersLoaded Fired after successfully loading all stargazers.\nonLocationUpdated Fired after successfully loading a single stargazer's profile.\n\nnum_resolved_stargazers The number of stargazers whose profiles have been loaded so far.\nnum_stargazers The total number of profiles to load.\n\n\nonLocationLoaded Fired after successfully loading all stargazers' profiles.\nonPointsUpdated Fired after geocoding a chunk of at most 10 stargazer locations.\n\nnum_resolved_points The number of locations geocoded so far.\nnum_stargazers The total number of locations to geocode.\n\n\nonPointsLoaded Fired after successfully geocoding all locations and drawing the heatmap.\n\nPerformance\nIt's important to pre-cache the geolocation data because the Google Maps geocoding API places strict limits on the frequency of requests. Trial and error indicates that this limit is in the neighborhood of 40 requests per minute. This means a repository with 200 stars would take 5 minutes to get all geocoordinates. By default, Red Dwarf rate limits requests using a technique based on work by Nicholas Zakas.\nAlso note that repositories with many thousands of stargazers will likely hit usage limits on GitHub's API, because each stargazer's profile must be queried in order to get their location. Rate limiting is currently in development for this scenario.\nFor these reasons, it's best to pre-cache as frequently as possible. This will prevent each page load from incurring the usage limit penalizations.\nRed Dwarf will only make API calls for data not already in the cache.\nPrivacy\nRed Dwarf uses publicly-available stargazer information including login names and locations via the GitHub API. Locations are only derived from stargazers who have opted in to making their location public in their GitHub profile.\nMIT License\nCopyright (c) 2012 Rick Viscomi (rviscomi@gmail.com)\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","241":"Rucksack\nRucksack is a jquery plugin to arrange elements that can fit in the given width. It relies on the knapsack algorithm.\nUsage:\n$('#container').rucksack({\n \u00a0 \u00a0width: 'width to fit in. defaults to 960px',\n \u00a0 \u00a0class: 'class name to be given to the div that will contain the elements of one row. defaults to rucksack'\n})\n\n\nExample\nHere's a working example by @omgmog\nhttp:\/\/jsfiddle.net\/vXbCY\/\nAnother example by ctcherry\nhttp:\/\/jsfiddle.net\/URMuA\/\nContributors:\n\ngauravsc\n\n","242":"jquery.nstSlider.js\nFully customizable with CSS, Single\/Double handles, Touch-enabled, IE 7+ Compatibility, Custom Digit Rounding, Non linear step increments!\n\nExample\nInitialize with:\n$(\".mySlider\").nstSlider({\n    \"left_grip_selector\": \".leftGrip\",\n    \"right_grip_selector\": \".rightGrip\",\n    \"value_bar_selector\": \".bar\",\n    \"value_changed_callback\": function(cause, minValue, maxValue, prevMinValue, prevMaxValue) {\n        \/\/ show the suggested values in your min\/max labels elements\n    }\n});\nMethod call:\n$(\".mySlider\").nstSlider(\"set_position\", 10 \/* min *\/, 90 \/* max *\/);\nDestroy with:\n$(\".mySlider\").nstSlider(\"teardown\");\nDemo\nFor live demos please visit the project webpage:\nhttp:\/\/lokku.github.io\/jquery-nstslider\/\nFor a Quick Start have a look at the source html of the following file:\nhttps:\/\/github.com\/lokku\/jquery-nstslider\/blob\/master\/demo\/index.html\nOptions\n\n\n\nOption\nType\nDefault\nDescription\n\n\n\n\nanimating_css_class\nstring\nnst-animating\nthe css class to be used when the slider is to be animated (this happens when a certain min\/max value is being set for example).\n\n\ntouch_tolerance_value_bar_x\nnumber\n15\nthe horizontal tolerance in pixels by which a handle of the slider should be grabbed if the user touches outside the slider bar area.\n\n\ntouch_tolerance_value_bar_y\nnumber\n30\nthe vertical tolerance in pixels by which a handle of the slider should be grabbed if the user touches outside the slider bar area.\n\n\nleft_grip_selector\nstring\n.nst-slider-grip-left\nthe selector of the left grip handle. The left grip element must exist in the page when the slider is initialized.\n\n\nright_grip_selector\nstring\nundefined\nthe selector of the right grip handle. This is optional. A single handler bar is assumed if this selector is not specified.\n\n\nvalue_bar_selector\nstring\nundefined\nthe selector of the value bar. If not specified assumes a value bar representing the selection is not wanted.\n\n\nrounding\nobject or number\n1\nthe rounding for a certain value displayed on the slider. This rounds the values returned in the value_changed_callback as roundedValue : int(actualValue \/ rounding) * rounding. The rounding parameter can be a number (i.e., fixed rounding) or can depend on actualValue (i.e., dynamic rounding). To perform dynamic rounding an object must be passed instead of a value. For example, passing rounding : { '1' : '100', '10' : '1000', '50' : '10000' } will use rounding = 1 when actualValue <= 100, rounding = 10 when 100 < actualValue <= 1000 and so on...\n\n\ncrossable_handles\nboolean\ntrue\nAllow handles to cross each other while one of them is being dragged. This option is ignored if just one handle is used.\n\n\nvalue_changed_callback\nfunction\nfunction(cause, curMin, curMax, prevMin, prevMax) { return; }\na callback called whenever the user drags one of the handles.\n\n\nuser_mouseup_callback\nfunction\nfunction(vmin, vmax, left_grip_moved) { return; }\na callback called whenever the mouse button pressed while dragging a slider grip is released\n\n\nuser_drag_start_callback\nfunction\nfunction () { return; }\na callback called before the user drags one of the handles\n\n\n\nMethods\nWhen calling methods, use positional arguments. For example, for the\nset_position method, call:\n$(\".mySlider\").nstSlider(\"set_position\", 10 \/* min *\/, 90 \/* max *\/);\ndo not call:\n$(\".mySlider\").nstSlider(\"set_position\", { min: 10, max: 90 });\nunless the documentation says that the first argument is an object.\n\n\n\nMethod\nArguments (positional)\nDescription\n\n\n\n\nget_range_min\nNone\nreturn the current minimum range of the slider\n\n\nget_range_max\nNone\nreturn the current maximum range of the slider\n\n\nget_current_min_value\nNone\nreturn the current minimum value of the slider\n\n\nget_current_max_value\nNone\nreturn the current maximum value of the slider\n\n\nis_handle_to_left_extreme\nNone\nreturn a boolean indicating whether or not the left handler is moved all the way to the left\n\n\nis_handle_to_right_extreme\nNone\nreturn a boolean indicating whether or not the right handler is moved all the way to the right\n\n\nrefresh\nNone\nforce a refresh of the slider\n\n\ndisable\nNone\ndisable the slider (i.e., user cannot move the handles)\n\n\nenable\nNone\nenable the slider (i.e., user can move the handles)\n\n\nis_enabled\nNone\nreturn a boolean indicating whether or not the slider can be moved by the user\n\n\nset_position\nmin: number, max: number\nset the handles at the specified min and max values\n\n\nset_step_histogram\nhistogram : array of numbers\nuse a non-linear step increment for the slider that is stretched where the histogram provided counts more items\n\n\nunset_step_histogram\nNone\nuse a linear scale of increments for the slider\n\n\nset_range\nrangeMin : number, rangeMax : number\nset the minimum and the maximum range of values the slider\n\n\nset_rounding\nrounding: number or object\nset the rounding for the slider\n\n\nget_rounding\nNone\nreturn the current rounding of the slider\n\n\nteardown\nNone\nremove all data stored in the slider\n\n\nvalue_to_px\nnumber\ngiven a value in the range of the slider, returns the corresponding value in pixel relative to the slider width\n\n\n\nDependencies\njQuery 1.6.4+\nLicense\nCopyright (c) 2014 Lokku Ltd.\nLicensed under the MIT license.\n","243":"ReplayLastGoal\nAutomatically create and tweet a video with the latest goal at the world cup.\nNotice: It is your responsiblity to make sure that you stay within the limits of \"Fair Use\". Laws might be different in your country. The author and contributors of this project decline all responsibility.\nFor reference: Copyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"fair use\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\nTry it!\nJust follow @ReplayLastGoal on Twitter.\nWebhooks\nYou can add webhooks to automatically receive a notification when a goal is scored along with the animated gif and a link to the video replay. We support classic webhooks, Hipchat and Slack. Go to\nhttp:\/\/ReplayLastGoal.com\/hooks\/add to configure them.\n\nHow does it work?\nIt connects to a video live stream (that you need to provide) and keeps a buffer of about one minute worth of video. When a given twitter account tweets (by default @GoalFlash), it uses the buffer to generate a video of the goal and then tweets it. Videos are saved in the videos\/ directory.\nPress & testimonials\n\nReplayLastGoal lets you relive the agony and ecstasy of the World Cup in GIFs - TheNextWeb\nReplayLastGoal Instantly Tweets Video Of The Latest World Cup Goal - Techcrunch\nMissed a World Cup gooooooooooaaaaalll? A new Twitter bot will catch you up - NiemanLab\nBest of the reactions on Twitter\nReactions to the take down notice from FIFA\n\nRequirements\nA live video stream\nThere are a few public television channels in Europe who are live streaming the world cup. The only caveat is that your server needs to be in that country (but it's not too complicated to work around it using the cloud.)\nLive streaming works either with Flash which is hard to reverse engineer or with HTML5 for mobile devices. So open the web page with the live stream with Safari and change your user agent to the iPad and you will be able to get the HTML5 version of the live stream. Then you can inspect network traffic to identify the URL of the live stream. It should end with the extension .m3u8. Use that URL in settings.json.\nAudio Video Converter\nYou need to have ffmpeg 2.2x installed on your machine. On a Mac, it's a piece of cake. Install brew and then do brew install ffmpeg and you are done.\nOn Ubuntu, it's a pain. This bash script should help you.\nInstall\ngit clone https:\/\/github.com\/xdamman\/ReplayLastGoal.git\nnpm install\n\nEdit settings.json and save it as settings.development.json (or settings.production.json for production environment as set by the NODE_ENV variable.)\nYou are now ready to start the application:\nnpm start\n\nLike it? Love it?\nShare the love by tweeting or favoriting this repo!\nOh, and pull requests are more than welcome! :-)\nContributors\n\nLaurent VB (facebook integration)\nJonathan Kupferman (better gif quality)\n\nSpecial thanks to Benjamin Goering (@bengo) for his help.\nTODO\n(Pull requests welcome)\n\n Better pin point when the goal happens in the 20s window to trim down the duration of the video and gif (we could use a VU meter to identify when the sound level peaks)\n Support for multichannels when there is more than one match at once\n Better test coverage\n Interface to manage webhooks and allow anyone to add their own webhook\n Hipchat Add On\n Slack add on\n Generate image to send with the tweet (finding the right one might be tricky)\n Refactoring to start streaming the video of the goal as soon as we start recording\n Automatically turn on\/off the input stream when there is a match\n Automatically create a video summary with all the goals after the game ends\n\n","244":"Emmet (ex-Zen Coding)\nEmmet is a toolkit for high-speed HTML, XML, XSL (or any other structured code format) coding and editing. The core of this plugin is a powerful abbreviation engine which allows you to expand expressions\u2014similar to CSS selectors\u2014into HTML code. For example:\ndiv#page>div.logo+ul#navigation>li*5>a\n\u2026can be expanded into:\n\n<div id=\"page\">\n        <div class=\"logo\"><\/div>\n        <ul id=\"navigation\">\n                <li><a href=\"\"><\/a><\/li>\n                <li><a href=\"\"><\/a><\/li>\n                <li><a href=\"\"><\/a><\/li>\n                <li><a href=\"\"><\/a><\/li>\n                <li><a href=\"\"><\/a><\/li>\n        <\/ul>\n<\/div>\n\nRead more about current Emmet syntax\nInstallation\n\nGo to Help > Install New Software\u2026 in your Eclipse IDE\nAdd http:\/\/emmet.io\/eclipse\/updates\/ in update sites\nCheck Emmet for Eclipse group in available plugins list, click Next button and follow the installation instructions\nRestart Eclipse\n\nPlugin Overview\nThis plugin provides the features:\n\nExpand abbreviations by Tab key\nTab stops and linked mode support\nSimple install and update process\nChange action shortcuts in Eclipse\u2019s Keys preferences page\nWorks across all Eclipse editors\nPreferences support to fine-tune output for each syntax and add new abbreviations and snippets\n\n\nAptana 3 users: since Aptana 3 can also expand snippets by Tab key, there might be collisions in expanded result (for example, for div tag). You can remove unused snippets for Aptana bundles in order to make Emmet plugin work properly.\nContributions\nDjango snippets","245":"Whiskey\nWhiskey is a powerful test runner for Node.js applications and a process\norchestration framework which makes running integration tests with a lot of\nservice \/ process dependencies easier.\nFeatures\n\nEach test file runs isolated in a separate process\nSupport for running multiple tests in parallel in a single suite (--concurrency option)\nSupport for running multiple suites in parallel (--independent-tests option)\nSupport for a test initialization function which is run before running the tests in a test file\nSupport for a test file timeout\nPer-test setUp \/ tearDown function support\nPer-suite (test file) initialize \/ finalize function support\nPer-session, or global, setUp \/ tearDown function support\nSupport for different test reporters (cli, tap)\nSupport for code coverage (cli reporter, html reporter)\nSupport for reporting variables which have leaked into a global scope\nNicely formatted reports (colors!)\nIntegration with node debugger\nSupport for generating Makefiles with different Whiskey targets\n\nChanges\nFor changes please see CHANGES.md file.\nInstallation\nInstall it using npm:\nnpm install whiskey\n\nUsage\nwhiskey [options] --tests \"<test files>\"\n\nwhiskey [options] --independent-tests \"<test files>\"\n\nwhiskey [options] --tests \"<test files>\"  --independent-tests \"<test files>\"\n\nAvailable options\n\n-t, --tests - Whitespace separated list of test suites to run sequentially\n-T, --independent-tests - Whitespace separated list of test suites to run concurrently\n-m, --max-suites NUMBER - The number of concurrently executing independent test suites (defaults to 5)\n-ti, --test-init-file - A path to the initialization file which must export\ninit function and it is called in a child process *before running the tests in\neach test file\n-c, --chdir - An optional path to which the child process will chdir to before\nrunning the tests\n-g, --global-setup-teardown STRING - Specifies the file containing the globalSetUp and globalTearDown procedures.\n--timeout [NUMBER] - How long to wait for tests to complete before timing\nout\n--failfast - Stop running the tests on a first failure or a timeout\n--no-styles - Don't use styles and colors\n--concurrency [NUMBER] - Maximum number of tests which will run in parallel (defaults to 1)\n--quiet - Don't print stdout and stderr\n--real-time - Print stdout and stderr as soon as it comes in\n--test-reporter [cli,tap] - Which test reporter to use (defaults to cli)\n--coverage - Use this option to enable the test coverage\n--coverage-reporter [cli,html] - Which coverage reporter to use (defaults to cli)\n--coverage-dir - Directory where the coverage HTML report is saved\n--scope-leaks - Record which variables were leaked into a global scope\n--scope-leaks-reporter [cli] - Which scope leak reporter to use (defaults\nto cli)\n--debug NUMBER - Attach a debugger to a test process listening on the specified port number\n--report-timing - Report each test run time\n--dependencies STRING - Specify path to the dependencies file for the\nprocess runner. More information about the process runner can be found at\nPROCESS_RUNNER.md\n--only-essential-dependencies - Only start dependencies required by the tests\nfiles which are ran. This option is only applicable if --dependencies option\nis used.\n\nNote: When specifying multiple test a list with the test paths must be quoted,\nfor example: whiskey --tests \"tests\/a.js tests\/b.js tests\/c.js\"\nA Note about setUp and tearDown\nPresently, two kinds of setup and teardown procedures exist with Whiskey.\nsetUp and tearDown work on a per-test basis; that is, Whiskey invokes setUp\nbefore running a test in a given Javascript file, called a suite and tearDown\nis invoked after a test run has finished.\nIf you run multiple suites in parallel (e.g., via the\n-T\/--independent-tests option), you'll get concurrent execution of setups and\nteardowns as well.\nSometimes, though, you need longer-lived environmental configurations, or you\nneed safe resource sharing between entire batches of independently running\ntests. For these, you'll want to use globalSetUp and globalTearDown.\n\nWhen do I use setUp \/ tearDown?\n\nWhen a suite's runtime environment does not influence other running suites.\n\n\nWhen do I use globalSetUp \/ globalTearDown ?\n\nWhen a suite's runtime environment can potentially interfere with other, concurrently running suites.\nExample: Attempting to run multiple suites in parallel which rely on a Cassandra schema being in place, and each attempting to reset the schema to a known state on a single Cassandra instance, you'll get Cassandra schema version errors.  Using globalSetUp prevents this by running the schema reset code exactly once for all tests.\n\n\n\nTest File Examples\nA simple example (success):\nvar called = 0;\n\nexports.test_async_one_equals_one = function(test, assert) {\n  setTimeout(function() {\n    assert.equal(1, 1);\n    called++;\n    test.finish();\n  }, 1000);\n};\n\nexports.tearDown = function(test, assert) {\n  assert.equal(called, 1);\n  test.finish();\n};\nA simple example (skipping a test):\nvar dbUp = false;\n\nexports.test_query = function(test, assert) {\n  if (!dbUp) {\n    test.skip('Database is not up, skipping...');\n    return;\n  }\n\n  assert.equal(2, 1);\n  test.finish();\n};\nA simple example (failure):\nexports.test_two_equals_one = function(test, assert) {\n  assert.equal(2, 1);\n  test.finish();\n};\nA simple example using the optional BDD module:\nvar bdd = require('whiskey').bdd.init(exports);\nvar describe = bdd.describe;\n\ndescribe('the bdd module', function(it) {\n  it('supports it(), expect(), and toEqual()', function(expect) {\n    expect(true).toEqual(true);\n  });\n});\nA simple example demonstrating how to use global setup and teardown functionality:\nexports['globalSetUp'] = function(test, assert) {\n  \/\/ Set up database schema here...\n  \/\/ Push known data set to database here...\n  test.finish();\n}\n\nexports['globalTearDown'] = function(test, assert) {\n  \/\/ Drop database here...\n  test.finish();\n}\nFor more examples please check the example\/ folder, and the test\/run.sh script.\nBuild status\n\nRunning Whiskey test suite\nTo run the Whiskey test suite, run the following command in the repository root\ndirectory.\nnpm test\nIf all the tests have sucessfully passed, the process should exit with a zero\nstatus code and you should see * * * Whiskey test suite PASSED. * * *\nmessage.\nContributing\nTo contribute, fork the repository, create a branch with your changes and open a\npull request.\nDebugging\nIf you want to debug your test, you can use the --debug option. This will\ncause Whiskey to start the test process with the V8 debugger functionality.\nYou then need to manually connect to the debugger to control it (i.e. using\nnode repl or node-inspector).\nWhiskey will also by default set a breakpoint at the beginning of your test\nfile.\nNote: This option can only be used with a single test file.  Further, you\ncannot use the --debug and --independent-tests options together.  The\nsemantics just don't make any sense.  To debug a test, make sure you invoke it\nwith --tests instead.\nTroubleshooting\nI use long-stack-straces module in my own code and all of the tests get reported as succeeded\nLong stack traces modules intercepts the default Error object and throws a custom\none. The problem with this is that Whiskey internally relies on attaching the\ntest name to the Error object so it can figure out to which test the exception\nbelongs. long-stack-traces throws a custom Error object and as a consequence test\nname attribute gets lost so Whiskey thinks your test didn't throw any exceptions.\nThe solution for this problem is to disable long-stack-trace module when running\nthe tests. This shouldn't be a big deal, because Whiskey internally already uses\nlong-stack-traces module which means that you will still get long stack traces\nin the exceptions which were thrown in your tests.\nMy test gets reported as \"timeout\" instead of \"failure\"\nIf your test gets reported as \"timeout\" instead of \"failure\" your test code most\nlikely looks similar to the one below:\nexports.test_failure = function(test, assert){\n  setTimeout(function() {\n    throw \"blaaaaah\";\n    test.finish();\n  },200);\n};\nThe problem with this is that if you run tests in parallel (--concurrency > 1)\nand you don't use a custom assert object which gets passed to each test function,\nWhiskey can't figure out to which test the exception belongs. As a consequence,\nthe test is reported as \"timed out\" and the exception is reported as \"uncaught\".\nThe solution for this problem is to run the tests in sequential mode (drop the\n--concurrency option).\nLicense\nApache 2.0, for more info see LICENSE.\n","246":"flux-router-component\nNotice: This package is deprecated in favor of fluxible-router.\n\n\n\n\n\nProvides navigational React components (NavLink), router mixin (RouterMixin), and action navigateAction for applications built with Flux architecture.  Please check out examples of how to use these components.\nContext and Expected Context Methods\nBefore we explain how to use NavLink and RouterMixin, lets start with two methods they expect:\n\nexecuteAction(navigateAction, payload) - This executes navigate action, switches the app to the new route, and update the url.\nmakePath(routeName, routeParams) - This is used to generate url for a given route.\n\nThese two methods need to be available in:\n\nthe React context of the component (access via this.context in the component), or\nthe context prop of the component (this.props.context)\nIf exists in both this.context and this.props.context, the one in this.context takes higher precedence.\n\nAn example of such context is the ComponentContext provided by fluxible-plugin-routr, which is a plugin for fluxible.  We have a more sophisticated example application, fluxible-router, showing how everything works together.\nNote that React context is an undocumented feature, so its API could change without notice.  Here is a blog from Dave King that explains what it is and how to use it.\nNavLink\nDocs\nRouterMixin\nDocs\nnavigateAction\nDocs\nHistory Management (Browser Support and Hash-Based Routing)\nConsidering different application needs and different browser support levels for pushState, this library provides the following options for browser history management:\n\nUse History provided by this library (Default)\nUse HistoryWithHash provided by this library\nIn addition, you can also customize it to use your own\n\nHistory\nThis is the default History implementation RouterMixin uses.  It is a straight-forward implementation that:\n\nuses pushState\/replaceState when they are available in the browser.\nFor the browsers without pushState support, History simply refreshes the page by setting window.location.href = url for pushState, and calling window.location.replace(url) for replaceState.\n\nHistoryWithHash\nUsing hash-based url for client side routing has a lot of known issues.  History.js describes those issues pretty well.\nBut as always, there will be some applications out there that have to use it.  This implementation provides a solution.\nIf you do decide to use hash route, it is recommended to enable checkRouteOnPageLoad.  Because hash fragment (that contains route) does not get sent to the server side, RouterMixin will compare the route info from server and route in the hash fragment.  On route mismatch, it will dispatch a navigate action on browser side to load the actual page content for the route represented by the hash fragment.\nuseHashRoute Config\nYou can decide when to use hash-based routing through the useHashRoute option:\n\nuseHashRoute=true to force to use hash routing for all browsers, by setting useHashRoute to true when creating the HistoryWithHash instance;\nunspecified, i.e. omitting the setting, to only use hash route for browsers without native pushState support;\nuseHashRoute=false to turn off hash routing for all browsers.\n\n\n\n\n\nuseHashRoute = true\nuseHashRoute = false\nuseHashRoute unspecified\n\n\n\n\nBrowsers with pushState support\nhistory.pushState with \/home#\/path\/to\/pageB\nhistory.pushState with \/path\/to\/pageB\nSame as useHashRoute = false\n\n\nBrowsers without pushState support\npage refresh to \/home#\/path\/to\/pageB\npage refresh to \/path\/to\/pageB\nSame as useHashRoute = true\n\n\n\nCustom Transformer for Hash Fragment\nBy default, the hash fragments are just url paths.  With HistoryWithHash, you can transform it to whatever syntax you need by passing props.hashRouteTransformer to the base React component that RouterMixin is mixed into.  See the example below for how to configure it.\nExample\nThis is an example of how you can use and configure HistoryWithHash:\nvar RouterMixin = require('flux-router-component').RouterMixin;\nvar HistoryWithHash = require('flux-router-component\/utils').HistoryWithHash;\n\nvar Application = React.createClass({\n    mixins: [RouterMixin],\n    ...\n});\n\nvar appComponent = Application({\n    ...\n    historyCreator: function historyCreator() {\n        return new HistoryWithHash({\n            \/\/ optional. Defaults to true if browser does not support pushState; false otherwise.\n            useHashRoute: true,\n            \/\/ optional. Defaults to '\/'. Used when url has no hash fragment\n            defaultHashRoute: '\/default',\n            \/\/ optional. Transformer for custom hash route syntax\n            hashRouteTransformer: {\n                transform: function (original) {\n                    \/\/ transform url hash fragment from '\/new\/path' to 'new-path'\n                    var transformed = original.replace('\/', '-').replace(\/^(\\-+)\/, '');\n                    return transformed;\n                },\n                reverse: function (transformed) {\n                    \/\/ reverse transform from 'new-path' to '\/new\/path'\n                    var original = '\/' + (transformed && transformed.replace('-', '\/'));\n                    return original;\n                }\n            }\n        });\n    }\n});\nProvide Your Own History Manager\nIf none of the history managers provided in this library works for your application, you can also customize the RouterMixin to use your own history manager implementation.  Please follow the same API as History.\nAPI\nPlease use History.js and HistoryWithHash.js as examples.\n\non(listener)\noff(listener)\ngetUrl()\ngetState()\npushState(state, title, url)\nreplaceState(state, title, url)\n\nExample:\nvar RouterMixin = require('flux-router-component').RouterMixin;\nvar MyHistory = require('MyHistoryManagerIsAwesome');\n\nvar Application = React.createClass({\n    mixins: [RouterMixin],\n    ...\n});\n\nvar appComponent = Application({\n    ...\n    historyCreator: function historyCreator() {\n        return new MyHistory();\n    }\n});\nScroll Position Management\nRouterMixin has a built-in mechanism for managing scroll position upon page navigation, for modern browsers that support native history state:\n\nreset scroll position to (0, 0) when user clicks on a link and navigates to a new page, and\nrestore scroll position to last visited state when user clicks forward and back buttons to navigate between pages.\n\nIf you want to disable this behavior, you can set enableScroll prop to false for RouterMixin.  This is an example of how it can be done:\nvar RouterMixin = require('flux-router-component').RouterMixin;\n\nvar Application = React.createClass({\n    mixins: [RouterMixin],\n    ...\n});\n\nvar appComponent = Application({\n    ...\n    enableScroll: false\n});\nonbeforeunload Support\nThe History API does not allow popstate events to be cancelled, which results in window.onbeforeunload() methods not being triggered.  This is problematic for users, since application state could be lost when they navigate to a certain page without knowing the consequences.\nOur solution is to check for a window.onbeforeunload() method, prompt the user with window.confirm(), and then navigate to the correct route based on the confirmation.  If a route is cancelled by the user, we reset the page URL back to the original URL by using  the History pushState() method.\nTo implement the window.onbeforeunload() method, you need to set it within the components that need user verification before leaving a page.  Here is an example:\ncomponentDidMount: function() {\n  window.onbeforeunload = function () {\n    return 'Make sure to save your changes before leaving this page!';\n  }\n}\nPolyfills\naddEventListener and removeEventListener polyfills are provided by:\n\nCompatibility code example on Mozilla Developer Network\nA few DOM polyfill libaries listed on Modernizer Polyfill wiki page.\n\nArray.prototype.reduce and Array.prototype.map (used by dependent library, query-string) polyfill examples are provided by:\n\nMozilla Developer Network Array.prototype.reduce polyfill\nMozilla Developer Network Array.prototype.map polyfill\n\nYou can also look into this polyfill.io polyfill service.\nCompatible React Versions\n\n\n\nCompatible React Version\nflux-router-component Version\n\n\n\n\n0.12\n>= 0.4.1\n\n\n0.11\n< 0.4\n\n\n\nLicense\nThis software is free to use under the Yahoo! Inc. BSD license.\nSee the LICENSE file for license text and copyright information.\nThird-pary open source code used are listed in our package.json file.\n","247":"PowerArray\nTurns out that you can re-write some of the methods of Array to obtain a much better performance than the native methods.\nIn particular, Array.forEach seems to perform pretty badly.\n\nNote: the overridden methods of PowerArray break compliance, the focus is on performance so take a look at the caveat section below.\n\nIt looks as if a for loop with cached length is the fastest way of iterating.\nvar i, len = array.length;\nfor (i = 0; i < len; i += 1) {\n  someFun(array[i]);\n}\nSo I rewrote the Array class as PowerArray and implemented the above mechanism in PowerArray.forEach with surprising results.\nThe results are as follows:\nPowerArray.forEach is averagely 5 times faster than native Array.\nThis is only a proof of concept.\nInstall with npm install powerarray\nProposed Usage\nParticularly useful for arrays that need processing on all elements often, or for numeric arrays utilized as indexes for Collections of data.\nMethods\nAll Array native methods are available through PowerArray. The following methods are either extending or overriding the native Array class.\nPowerArray.forEach: utilizes a for loop for iteration, takes a callback which receives an element and the index of that element.\nPowerArray.map: utilizes a for loop to return a PowerArray of mapped values, takes a callback processing function argument.\nPowerArray.binarySearch: performs a binary search on the elements of the array, only relevant if the array only consists of numbers. Thanks to Oliver Caldwell's post for a quick version of the algorithm. Also note the contribution of Yehonatan and other authors of comments to the post which helped to optimise the implementation of binary search further.\nPowerArray.numericSort: sorts array (if array only contains integers), useful for utilizing binarySearch. Optional sorting function argument.\nPowerArray.addAndSort: adds a new value and sorts the array automatically\nContribution\nPull requests are more than welcome, just make sure to add a test in tests\/test.js (and that it passes it obviously).\nCaveats\nThanks to David Souther for documenting these:\n\nNo this context in fn calls, handle your own binding.\nNo determination if i is a member of PowerArray (eg for sparse arrays, [2, 4, , 6])\nNo exception is thrown when the callback isn't callable.\n\nThere may be more, please feel free to flag those or include them yourself through a pull request.\n","248":"angular-load\nDynamically load scripts and CSS stylesheets in your Angular.JS app.\nCopyright (C) 2014, 2015, Uri Shaked uri@urish.org\n\n\nInstallation\nYou can choose your preferred method of installation:\n\nThrough bower: bower install angular-load --save\nThrough npm: npm install angular-load --save\nDownload from github: angular-load.js\n\nUsage\nInclude angular-load.js in your application.\n<script src=\"bower_components\/angular-load\/dist\/angular-load.min.js\"><\/script>\nAdd the module angularLoad as a dependency to your app module:\nvar myapp = angular.module('myapp', ['angularLoad']);\nYou can also use ES6 or CommonJS\nimport angularLoad from 'angular-load';\n\/\/ or\nvar angularLoad = require('angular-load');\n\nvar myapp = angular.module('myapp', [angularLoad]);\nangularLoad service directive\nThe angularLoad service provides three methods: loadScript(), loadCSS() and unloadCSS().\nCall the loadScript() and loadCSS() methods to load a script\nor a CSS stylesheet asynchronously into the current page. Both methods return a promise that will be resolved\nonce the resource (script or stylesheet) has been loaded. In case of an error (e.g. HTTP 404) the promise will be\nrejected.\nCall the unloadCSS() method to unload a CSS stylesheet from the current page. This method return boolean value, specifying whether the given stylesheet URL could be located in the page and has been unloaded. In case of trying to remove non-existent resource the function will return false\nUsage example:\nangularLoad.loadScript('https:\/\/mysite.com\/someplugin.js').then(function() {\n\t\/\/ Script loaded succesfully.\n\t\/\/ We can now start using the functions from someplugin.js\n}).catch(function() {\n    \/\/ There was some error loading the script. Meh\n});\nCollaborators\nUri Shaked uri@urish.org, Colm Seale colm.seale@gmail.com\nLicense\nReleased under the terms of MIT License:\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n'Software'), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and\/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","249":"django-mediasync\nOne of the more significant development roadblocks we have relates to local vs.\ndeployed media. Ideally all media (graphics, css, scripts) development would\noccur locally and not use production media. Then, when ready to deploy, the\nmedia should be pushed to production. That way there can be significant changes\nto media without disturbing the production web site.\nThe goal of mediasync is to develop locally and then flip a switch in production\nthat makes all the media URLs point to remote media URLs instead of the local\nmedia directory.\nAll code is under a BSD-style license, see LICENSE for details.\nSource: http:\/\/github.com\/sunlightlabs\/django-mediasync\/\n\nRequirements\n\ndjango >= 1.0\nboto >= 1.8d\nslimmer == 0.1.30 (optional)\npython-cloudfiles == 1.7.5 (optional, for Rackspace Cloud Files backend)\n\n\nUpgrading from mediasync 1.x\n\nUpdate your mediasync settings as described in the next section.\n\nRun .\/manage.py syncmedia --force to force updates of all files:\n\ngzip instead of deflate compression\nsync both compressed and original versions of files\n\n\n\n\nadd \"django.core.context_processors.request\" to TEMPLATE_CONTEXT_PROCESSORS\n\n\nAn important note about Django 1.3\nWhen DEBUG = True and the project is run with manage.py runserver, Django 1.3\nautomatically adds django.views.static.serve to urlpatterns. While this feature\nmakes local development easier for most people, it screws everything up if\nyou've added mediasync.urls to urlpatterns. As of now, the only way I can find\nto disable the automatic addition of django.views.static.serve is to use a full\nURL for STATIC_URL instead of just a path:\nSTATIC_URL = \"http:\/\/localhost:8000\/static\/\"\n\n\nConfiguration\n\nsettings.py\nAdd to INSTALLED_APPS:\n'mediasync'\n\nAdd to TEMPLATE_CONTEXT_PROCESSORS:\n'django.core.context_processors.request'\n\nMake sure your STATIC_ROOT setting is the correct path to your media:\nSTATIC_ROOT = '\/path\/to\/media'\n\nWhen media is being served locally (instead of from S3 or Cloud Files),\nmediasync serves media through a Django view. Set your STATIC_URL to what\nyou'd like that local media URL to be. This can be whatever you'd like, as long\nas you're using the {% media_url %} tag (more details on this later):\nSTATIC_URL = 'http:\/\/localhost:8000\/devmedia\/'\n\nSTATIC_URL is the URL that will be used in debug mode. Otherwise,\nthe STATIC_URL will be loaded from the backend settings. Please see\nAn important note about Django 1.3.\nThe following settings dict must also be added:\nMEDIASYNC = {\n    'BACKEND': 'path.to.backend',\n}\n\nIf you want to use a different media URL than that specified\nin settings.STATIC_URL, you can add STATIC_URL to the MEDIASYNC\nsettings dict:\nMEDIASYNC = {\n    ...\n    'STATIC_URL': '\/url\/to\/media\/', # becomes http:\/\/yourhost.com\/url\/to\/media\/\n    ...\n}\n\nSame goes for STATIC_ROOT:\nMEDIASYNC = {\n    ...\n    'STATIC_ROOT': '\/path\/to\/media\/',\n    ...\n}\n\nmediasync supports pluggable backends. Please see below for information on\nthe provided backends as well as directions on implementing your own.\n\nMedia expiration\nIf the client supports media expiration, all files are set to expire 365 days\nafter the file was synced. You may override this value by adding\nEXPIRATION_DAYS to the MEDIASYNC settings dict.\n# Expire in 10 years.\nMEDIASYNC['EXPIRATION_DAYS'] = 365 * 10\n\n\nServing media remote (S3\/Cloud Files) or locally\nThe media URL is selected based on the SERVE_REMOTE attribute in the\nMEDIASYNC dict in settings.py. When False, media will be served locally\ninstead of from S3.\n# This would force mediasync to serve all media through the value\n# specified in settings.STATIC_URL.\nMEDIASYNC['SERVE_REMOTE'] = False\n\n# This would serve all media through S3\/Cloud Files.\nMEDIASYNC['SERVE_REMOTE'] = True\n\n# This would serve media locally while in DEBUG mode, and remotely when\n# in production (DEBUG == False).\nMEDIASYNC['SERVE_REMOTE'] = not DEBUG\n\nWhen serving files locally, you can emulate the CSS\/JS combo\/minifying\nbehavior we get from using media processors by specifying the following.\nMEDIASYNC['SERVE_REMOTE'] = False\nMEDIASYNC['EMULATE_COMBO'] = True\n\nNote that this will only work if your STATIC_URL is pointing at your\nDjango dev server. Also keep in mind that some processors may take a while,\nand is best used to check things over before rolling out to production.\n\nDOCTYPE\nlink and script tags are written using XHTML syntax. The rendering can be\noverridden by using the DOCTYPE setting. Allowed values are 'html4',\n'html5', or 'xhtml'. The default in mediasync 2.0 is html5, just as\nthe DOCTYPE on your site should be.\nMEDIASYNC['DOCTYPE'] = 'html5'\n\nFor each doctype, the following tags are rendered:\n\nhtml4\n<link rel=\"stylesheet\" href=\"...\" type=\"text\/css\" media=\"...\">\n<script type=\"text\/javascript\" charset=\"utf-8\" src=\"...\"><\/script>\n\n\nhtml5\n<link rel=\"stylesheet\" href=\"...\" media=\"...\">\n<script src=\"...\"><\/script>\n\n\nxhtml\n<link rel=\"stylesheet\" href=\"...\" type=\"text\/css\" media=\"...\" \/>\n<script type=\"text\/javascript\" charset=\"utf-8\" src=\"...\"><\/script>\n\n\nSSL\nmediasync will attempt to intelligently determine if your media should be\nserved using HTTPS. In order to use automatic SSL detection,\ndjango.core.context_processors.request must be added to\nTEMPLATE_CONTEXT_PROCESSORS in settings.py:\nTEMPLATE_CONTEXT_PROCESSORS = (\n    ...\n    'django.core.context_processors.request',\n    ...\n)\n\nThe USE_SSL mediasync setting can be used to override the SSL\nURL detection.\n# Force HTTPS.\nMEDIASYNC['USE_SSL'] = True\n\nor\n# Force HTTP.\nMEDIASYNC['USE_SSL'] = False\n\nSome backends will be unable to use SSL. In these cases USE_SSL and SSL\ndetection will be ignored.\n\nurls.py\nTo serve local media through mediasync, add a reference to mediasync.urls in\nyour main urls.py file.\nurlpatterns = ('',\n    ...\n    url(r'^', include('mediasync.urls)),\n    ...\n)\n\n\nBackends\nmediasync now supports pluggable backends. A backend is a Python module that\ncontains a Client class that implements a mediasync-provided BaseClient class.\n\nS3\nMEDIASYNC['BACKEND'] = 'mediasync.backends.s3'\n\n\nSettings\nThe following settings are required in the mediasync settings dict:\nMEDIASYNC = {\n    'AWS_KEY': \"s3_key\",\n    'AWS_SECRET': \"s3_secret\",\n    'AWS_BUCKET': \"bucket_name\",\n}\n\nOptionally you may specify a path prefix:\nMEDIASYNC['AWS_PREFIX'] = \"key_prefix\"\n\nAssuming a correct DNS CNAME entry, setting AWS_BUCKET to\nassets.sunlightlabs.com and AWS_PREFIX to myproject\/media would\nsync the media directory to http:\/\/assets.sunlightlabs.com\/myproject\/media\/.\nAmazon allows users to create DNS CNAME entries to map custom domain names\nto an AWS bucket. MEDIASYNC can be configured to use the bucket as the media\nURL by setting AWS_BUCKET_CNAME to True.\nMEDIASYNC['AWS_BUCKET_CNAME'] = True\n\nIf you would prefer to not use gzip compression with the S3 client, it can be\ndisabled:\nMEDIASYNC['AWS_GZIP'] = False\n\n\nTips\nSince files are given a far future expires header, one needs a way to do\n\"cache busting\" when you want the browser to fetch new files before the expire\ndate arrives.  One of the best and easiest ways to accomplish this is to alter\nthe path to the media files with some sort of version string using the key\nprefix setting:\nMEDIASYNC['AWS_PREFIX'] = \"myproject\/media\/v20001201\"\n\nGiven that and the above DNS CNAME example, the media directory URL would end\nup being http:\/\/assets.sunlightlabs.com\/myproject\/media\/v20001201\/.  Whenever\nyou need to update the media files, simply update the key prefix with a new\nversioned string.\nA CACHE_BUSTER settings can be added to the main MEDIASYNC settings\ndict to add a query string parameter to all media URLs. The cache buster can\neither be a value or a callable which is passed the media URL as a parameter.\nMEDIASYNC['CACHE_BUSTER'] = 1234567890\n\nThe above setting will generate a media path similar to:\nhttp:\/\/yourhost.com\/url\/to\/media\/image.png?1234567890\n\nAn important thing to note is that if you're running your Django site in a\nmulti-threaded or multi-node setup, you'll want to be careful about using a\ntime-based cache buster value. Each worker\/thread will probably have a slightly\ndifferent value for datetime.now(), which means your users will find themselves\nhaving cache misses randomly from page to page.\n\nRackspace Cloud Files\nMEDIASYNC['BACKEND'] = 'mediasync.backends.cloudfiles'\n\n\nSettings\nThe following settings are required in the mediasync settings dict:\nMEDIASYNC = {\n    'CLOUDFILES_CONTAINER': 'container_name',\n    'CLOUDFILES_USERNAME': 'cf_username',\n    'CLOUDFILES_API_KEY': 'cf_apikey',\n}\n\n\nTips\nThe Cloud Files backend lacks support for the following features:\n\nsetting HTTP Expires header\nsetting HTTP Cache-Control header\ncontent compression (gzip)\nSSL support\nconditional sync based on file checksum\n\n\nCustom backends\nYou can create a custom backend by creating a Python module containing a Client\nclass. This class must inherit from mediasync.backends.BaseClient. Additionally,\nyou must implement two methods:\ndef remote_media_url(self, with_ssl):\n    ...\n\nremote_media_url returns the full base URL for remote media. This can be\neither a static URL or one generated from mediasync settings:\ndef put(self, filedata, content_type, remote_path, force=False):\n    ...\n\nput is responsible for pushing a file to the backend storage.\n\nfiledata - the contents of the file\ncontent_type - the mime type of the file\nremote_path - the remote path (relative from remote_media_url) to which\nthe file should be written\nforce - if True, write file to remote storage even if it already exists\n\nIf the client supports gzipped content, you will need to override supports_gzip\nto return True:\ndef supports_gzip(self):\n        return True\n\n\nFile Processors\nFile processors allow you to modify the content of a file as it is being\nsynced or served statically.\nMediasync ships with three processor modules:\n\nslim is a minifier written in Python and requires the\nslimmer Python package. The Python package can be found here:\nhttp:\/\/pypi.python.org\/pypi\/slimmer\/\nyuicompressor is a minifier written in Java and can be downloaded\nfrom YUI's download page: http:\/\/developer.yahoo.com\/yui\/compressor\/.\nThis processor also requires an additional setting, as defined below.\nyuicompressor is new and should be considered experimental until\nthe mediasync 2.1 release.\nclosurecompiler is a javascript compiler provided by Google.\n\nCustom processors can be specified using the PROCESSORS entry in the\nmediasync settings dict. PROCESSORS should be a list of processor entries.\nEach processor entry can be a callable or a string path to a callable. If the\npath is to a class definition, the class will be instantiated into an object.\nThe processor callable should return a string of the processed file data, None\nif it chooses to not process the file, or raise mediasync.SyncException if\nsomething goes terribly wrong. The callable should take the following arguments:\ndef proc(filedata, content_type, remote_path, is_active):\n        ...\n\n\nfiledata\nthe content of the file as a string\ncontent_type\nthe mimetype of the file being processed\nremote_path\nthe path to which the file is being synced (contains the file name)\nis_active\nTrue if the processor should... process\n\nIf the PROCESSORS setting is used, you will need to include the defaults\nif you plan on using them:\n'PROCESSORS': (\n    'mediasync.processors.slim.css_minifier',\n    'mediasync.processors.slim.js_minifier',\n        ...\n),\n\nmediasync will attempt to use slimmer by default if you have the package\ninstalled and do not use the PROCESSORS setting.\n\nGoogle Closure Compiler\nGoogle's JavaScript Closure Compiler provides an API that allows files to be\ncompressed without installing anything locally. To use the service:\n'PROCESSORS': ('mediasync.processors.closurecompiler.compile',)\n\n\nYUI Compressor\nTo configure YUI Compressor you need to define a PROCESSORS and\nYUI_COMPRESSOR_PATH as follows, assuming you placed the \".jar\" file in\nyour ~\/bin path:\n'PROCESSORS': ('mediasync.processors.yuicompressor.css_minifier',\n               'mediasync.processors.yuicompressor.js_minifier'),\n'YUI_COMPRESSOR_PATH': '~\/bin\/yuicompressor.jar',\n\n\nFeatures\n\nIgnored Directories\nAny directory in STATIC_ROOT that is hidden or starts with an underscore\nwill be ignored during syncing.\n\nTemplate Tags\nWhen referring to media in HTML templates you can use custom template tags.\nThese tags can by accessed by loading the media template tag collection.\n{% load media %}\n\nAny tag that has a path argument can use either a string or a variable:\n{% media_url \"images\/avatar.png\" }\n{% media_url user.profile.avatar_path %}\n\nSome backends (S3) support https URLs when the requesting page is secure.\nIn order for the https to be detected, the request must be placed in the\ntemplate context with the key 'request'. This can be done automatically by\nadding 'django.core.context_processors.request' to TEMPLATE_CONTEXT_PROCESSORS\nin settings.py\n\nmedia_url\nRenders the STATIC_URL from settings.py with trailing slashes removed.\n<img src=\"{% media_url %}\/images\/stuff.png\">\n\nSTATIC_URL takes an optional argument that is the media path. Using the argument\nallows mediasync to add the CACHE_BUSTER to the URL if one is specified.\n<img src=\"{% media_url '\/images\/stuff.png' %}\">\n\nIf CACHE_BUSTER is set to 12345, the above example will render as:\n<img src=\"http:\/\/assets.example.com\/path\/to\/media\/images\/stuff.png?12345\">\n\nNOTE: Don't use this tag to serve CSS or JS files. Use the js and css tags\nthat were specifically designed for the purpose.\n\njs\nRenders a script tag with the correct include.\n{% js \"myfile.js\" %}\n\n\ncss\nRenders a <link> tag to include the stylesheet. It takes an optional second\nparameter for the media attribute; the default media is \"screen, projector\".\n{% css \"myfile.css\" %}\n{% css \"myfile.css\" \"screen\" %}\n\n\ncss_print\nShortcut to render as a print stylesheet.\n{% css_print \"myfile.css\" %}\n\nwhich is equivalent to\n{% css \"myfile.css\" \"print\" %}\n\n\nWriting Style Sheets\nUsers are encouraged to write stylesheets using relative URLS. The media\ndirectory is synced with S3 as is, so relative local paths will still work\nwhen pushed remotely.\nbackground: url(..\/images\/arrow_left.png);\n\n\nJoined files\nWhen serving media in production, it is beneficial to combine JavaScript and\nCSS into single files. This reduces the number of connections the browser needs\nto make to the web server. Fewer connections can dramatically decrease page\nload times and reduce the server-side load.\nJoined files are specified in the MEDIASYNC dict using JOINED. This is\na dict that maps individual media to an alias for the joined files.\n'JOINED': {\n    'styles\/joined.css': ['styles\/reset.css','styles\/text.css'],\n    'scripts\/joined.js': ['scripts\/jquery.js','scripts\/processing.js'],\n},\n\nFiles listed in JOINED will be combined and pushed to S3 with the name of\nthe alias. The individual CSS files will also be pushed to S3. Aliases must end\nin either .css or .js in order for the content-type to be set appropriately.\nThe existing template tags may be used to refer to the joined media. Simply use\nthe joined alias as the argument:\n{% css_print \"joined.css\" %}\n\nWhen served locally, template tags will render an HTML tag for each of the files\nthat make up the joined file:\n<link rel=\"stylesheet\" href=\"\/media\/styles\/reset.css\" type=\"text\/css\" media=\"screen, projection\" \/>\n<link rel=\"stylesheet\" href=\"\/media\/styles\/text.css\" type=\"text\/css\" media=\"screen, projection\" \/>\n\nWhen served remotely, one HTML tag will be rendered with the name of the joined file:\n<link rel=\"stylesheet\" href=\"http:\/\/bucket.s3.amazonaws.com\/styles\/joined.css\" type=\"text\/css\" media=\"screen, projection\" \/>\n\n\nSmart GZIP for S3\nIn previous versions of mediasync's S3 client, certain content was always pushed\nin a compressed format. This can cause major issues with clients that do not\nsupport gzip. New in version 2.0, mediasync will push both a gzipped and an\nuncompressed version of the file to S3. The template tags look at the request\nand direct the user to the appropriate file based on the ACCEPT_ENCODING HTTP\nheader. Assuming a file styles\/layout.css, the following would be synced to S3:\nstyles\/layout.css\nstyles\/layout.css.gzt\n\nNote the altered use of the .gz extension. Some versions of the Safari browser\non OSX ignore the Content-Type header for files ending in .gz and treat them\ninstead as files to download. This altered extension allows Safari to deflate\nand utilize the files correctly without affecting functionality in any other\ntested browsers.\n\nSignals\nmediasync provides two signals that allow you to hook into the syncing\nprocess. pre_sync is sent after the client is opened, but before the first\nfile is synced. post_sync is sent after the last file is synced, but before\nthe client is closed. This allows you to call commands on the client without\nhaving to worry about its state. The signals allow you to do common tasks such\nas calling Django 1.3's collectstatic command, process SASS stylesheets, or\nclean up files generated during a pre_sync process.\n\ncollectstatic receiver\nA receiver for calling the collectstatic management command is provided:\nfrom mediasync.signals import pre_sync, collectstatic_receiver\n\n# run collectstatic before syncing media\npre_sync.connect(collectstatic_receiver)\n\n\nSASS receiver\nA receiver for compiling SASS into CSS is provided:\nfrom mediasync.signals import pre_sync, sass_receiver\n\n# compile SASS files before syncing media\npre_sync.connect(sass_receiver)\n\nAny file in static root that has the sass or scss file extension will be\ncompiled into CSS. The compiled CSS file will be placed in the same directory\nand the original extension will be replaced with css. If a file exists with\nthe same css extension, it will be overwritten.\nBy default mediasync uses the sass command with no options. If you would\nlike to specify your own command, specify SASS_COMMAND in settings:\nMEDIASYNC = {\n    ...\n    'SASS_COMMAND': 'sass -scss -l',\n    ...\n}\n\n\nRunning MEDIASYNC\n.\/manage.py syncmedia\n\n\nChange Log\n\n2.2.0\n\nadded pre_sync and post_sync signals\nprovide basic receiver for calling collectstatic before syncing\nprovide receiver for compiling SASS before syncing\nshow media directory listing when serving locally in debug mode\nadd processor for Google's Closure Compiler API for JavaScript\ntemplate tags can now take a variable as the path argument\n\n\n2.1.0\n\ndefault to using STATIC_URL and STATIC_ROOT (Django 1.3), falling back\nto MEDIA_URL and MEDIA_ROOT if the STATIC_* settings are not set\nadd AWS_GZIP setting to optionally disable gzip compression in S3 client\n\nThanks to Rob Hudson and Dolan Antenucci for their contributions to this\nrelease.\n\n2.0.0\n\nupdated Rackspace Cloud Files backend\nuse gzip instead of deflate for compression (better browser support)\nsmart gzip client support detection\nadd pluggable backends\nadd pluggable file processors\nexperimental YUI Compressor\nsettings refactor\nallow override of settings.MEDIA_URL\nImprovements to the logic that decides which files to sync. Safely ignore\na wider variety of hidden files\/directories.\nMake template tags aware of whether the current page is SSL-secured. If it\nis, ask the backend for an SSL media URL (if implemented by your backend).\nmade SERVE_REMOTE setting the sole factor in determining if\nmedia should be served locally or remotely\nadd many more tests\ndeprecate CSS_PATH and JS_PATH\n\nThanks to Greg Taylor, Peter Sanchez, Jonathan Drosdeck, Rich Leland,\nand Rob Hudson for their contributions to this release.\n\n1.0.1\n\nadd application\/javascript and application\/x-javascript to JavaScript\nmimetypes\nbreak out of CSS and JS mimetypes\nadd support for HTTPS URLs to S3\nallow for storage of S3 keys in ~\/.boto configuration file\n\nThanks to Rob Hudson and Peter Sanchez for their contributions.\n\n1.0.0\nInitial release.\n","250":"django-mediasync\nOne of the more significant development roadblocks we have relates to local vs.\ndeployed media. Ideally all media (graphics, css, scripts) development would\noccur locally and not use production media. Then, when ready to deploy, the\nmedia should be pushed to production. That way there can be significant changes\nto media without disturbing the production web site.\nThe goal of mediasync is to develop locally and then flip a switch in production\nthat makes all the media URLs point to remote media URLs instead of the local\nmedia directory.\nAll code is under a BSD-style license, see LICENSE for details.\nSource: http:\/\/github.com\/sunlightlabs\/django-mediasync\/\n\nRequirements\n\ndjango >= 1.0\nboto >= 1.8d\nslimmer == 0.1.30 (optional)\npython-cloudfiles == 1.7.5 (optional, for Rackspace Cloud Files backend)\n\n\nUpgrading from mediasync 1.x\n\nUpdate your mediasync settings as described in the next section.\n\nRun .\/manage.py syncmedia --force to force updates of all files:\n\ngzip instead of deflate compression\nsync both compressed and original versions of files\n\n\n\n\nadd \"django.core.context_processors.request\" to TEMPLATE_CONTEXT_PROCESSORS\n\n\nAn important note about Django 1.3\nWhen DEBUG = True and the project is run with manage.py runserver, Django 1.3\nautomatically adds django.views.static.serve to urlpatterns. While this feature\nmakes local development easier for most people, it screws everything up if\nyou've added mediasync.urls to urlpatterns. As of now, the only way I can find\nto disable the automatic addition of django.views.static.serve is to use a full\nURL for STATIC_URL instead of just a path:\nSTATIC_URL = \"http:\/\/localhost:8000\/static\/\"\n\n\nConfiguration\n\nsettings.py\nAdd to INSTALLED_APPS:\n'mediasync'\n\nAdd to TEMPLATE_CONTEXT_PROCESSORS:\n'django.core.context_processors.request'\n\nMake sure your STATIC_ROOT setting is the correct path to your media:\nSTATIC_ROOT = '\/path\/to\/media'\n\nWhen media is being served locally (instead of from S3 or Cloud Files),\nmediasync serves media through a Django view. Set your STATIC_URL to what\nyou'd like that local media URL to be. This can be whatever you'd like, as long\nas you're using the {% media_url %} tag (more details on this later):\nSTATIC_URL = 'http:\/\/localhost:8000\/devmedia\/'\n\nSTATIC_URL is the URL that will be used in debug mode. Otherwise,\nthe STATIC_URL will be loaded from the backend settings. Please see\nAn important note about Django 1.3.\nThe following settings dict must also be added:\nMEDIASYNC = {\n    'BACKEND': 'path.to.backend',\n}\n\nIf you want to use a different media URL than that specified\nin settings.STATIC_URL, you can add STATIC_URL to the MEDIASYNC\nsettings dict:\nMEDIASYNC = {\n    ...\n    'STATIC_URL': '\/url\/to\/media\/', # becomes http:\/\/yourhost.com\/url\/to\/media\/\n    ...\n}\n\nSame goes for STATIC_ROOT:\nMEDIASYNC = {\n    ...\n    'STATIC_ROOT': '\/path\/to\/media\/',\n    ...\n}\n\nmediasync supports pluggable backends. Please see below for information on\nthe provided backends as well as directions on implementing your own.\n\nMedia expiration\nIf the client supports media expiration, all files are set to expire 365 days\nafter the file was synced. You may override this value by adding\nEXPIRATION_DAYS to the MEDIASYNC settings dict.\n# Expire in 10 years.\nMEDIASYNC['EXPIRATION_DAYS'] = 365 * 10\n\n\nServing media remote (S3\/Cloud Files) or locally\nThe media URL is selected based on the SERVE_REMOTE attribute in the\nMEDIASYNC dict in settings.py. When False, media will be served locally\ninstead of from S3.\n# This would force mediasync to serve all media through the value\n# specified in settings.STATIC_URL.\nMEDIASYNC['SERVE_REMOTE'] = False\n\n# This would serve all media through S3\/Cloud Files.\nMEDIASYNC['SERVE_REMOTE'] = True\n\n# This would serve media locally while in DEBUG mode, and remotely when\n# in production (DEBUG == False).\nMEDIASYNC['SERVE_REMOTE'] = not DEBUG\n\nWhen serving files locally, you can emulate the CSS\/JS combo\/minifying\nbehavior we get from using media processors by specifying the following.\nMEDIASYNC['SERVE_REMOTE'] = False\nMEDIASYNC['EMULATE_COMBO'] = True\n\nNote that this will only work if your STATIC_URL is pointing at your\nDjango dev server. Also keep in mind that some processors may take a while,\nand is best used to check things over before rolling out to production.\n\nDOCTYPE\nlink and script tags are written using XHTML syntax. The rendering can be\noverridden by using the DOCTYPE setting. Allowed values are 'html4',\n'html5', or 'xhtml'. The default in mediasync 2.0 is html5, just as\nthe DOCTYPE on your site should be.\nMEDIASYNC['DOCTYPE'] = 'html5'\n\nFor each doctype, the following tags are rendered:\n\nhtml4\n<link rel=\"stylesheet\" href=\"...\" type=\"text\/css\" media=\"...\">\n<script type=\"text\/javascript\" charset=\"utf-8\" src=\"...\"><\/script>\n\n\nhtml5\n<link rel=\"stylesheet\" href=\"...\" media=\"...\">\n<script src=\"...\"><\/script>\n\n\nxhtml\n<link rel=\"stylesheet\" href=\"...\" type=\"text\/css\" media=\"...\" \/>\n<script type=\"text\/javascript\" charset=\"utf-8\" src=\"...\"><\/script>\n\n\nSSL\nmediasync will attempt to intelligently determine if your media should be\nserved using HTTPS. In order to use automatic SSL detection,\ndjango.core.context_processors.request must be added to\nTEMPLATE_CONTEXT_PROCESSORS in settings.py:\nTEMPLATE_CONTEXT_PROCESSORS = (\n    ...\n    'django.core.context_processors.request',\n    ...\n)\n\nThe USE_SSL mediasync setting can be used to override the SSL\nURL detection.\n# Force HTTPS.\nMEDIASYNC['USE_SSL'] = True\n\nor\n# Force HTTP.\nMEDIASYNC['USE_SSL'] = False\n\nSome backends will be unable to use SSL. In these cases USE_SSL and SSL\ndetection will be ignored.\n\nurls.py\nTo serve local media through mediasync, add a reference to mediasync.urls in\nyour main urls.py file.\nurlpatterns = ('',\n    ...\n    url(r'^', include('mediasync.urls)),\n    ...\n)\n\n\nBackends\nmediasync now supports pluggable backends. A backend is a Python module that\ncontains a Client class that implements a mediasync-provided BaseClient class.\n\nS3\nMEDIASYNC['BACKEND'] = 'mediasync.backends.s3'\n\n\nSettings\nThe following settings are required in the mediasync settings dict:\nMEDIASYNC = {\n    'AWS_KEY': \"s3_key\",\n    'AWS_SECRET': \"s3_secret\",\n    'AWS_BUCKET': \"bucket_name\",\n}\n\nOptionally you may specify a path prefix:\nMEDIASYNC['AWS_PREFIX'] = \"key_prefix\"\n\nAssuming a correct DNS CNAME entry, setting AWS_BUCKET to\nassets.sunlightlabs.com and AWS_PREFIX to myproject\/media would\nsync the media directory to http:\/\/assets.sunlightlabs.com\/myproject\/media\/.\nAmazon allows users to create DNS CNAME entries to map custom domain names\nto an AWS bucket. MEDIASYNC can be configured to use the bucket as the media\nURL by setting AWS_BUCKET_CNAME to True.\nMEDIASYNC['AWS_BUCKET_CNAME'] = True\n\nIf you would prefer to not use gzip compression with the S3 client, it can be\ndisabled:\nMEDIASYNC['AWS_GZIP'] = False\n\n\nTips\nSince files are given a far future expires header, one needs a way to do\n\"cache busting\" when you want the browser to fetch new files before the expire\ndate arrives.  One of the best and easiest ways to accomplish this is to alter\nthe path to the media files with some sort of version string using the key\nprefix setting:\nMEDIASYNC['AWS_PREFIX'] = \"myproject\/media\/v20001201\"\n\nGiven that and the above DNS CNAME example, the media directory URL would end\nup being http:\/\/assets.sunlightlabs.com\/myproject\/media\/v20001201\/.  Whenever\nyou need to update the media files, simply update the key prefix with a new\nversioned string.\nA CACHE_BUSTER settings can be added to the main MEDIASYNC settings\ndict to add a query string parameter to all media URLs. The cache buster can\neither be a value or a callable which is passed the media URL as a parameter.\nMEDIASYNC['CACHE_BUSTER'] = 1234567890\n\nThe above setting will generate a media path similar to:\nhttp:\/\/yourhost.com\/url\/to\/media\/image.png?1234567890\n\nAn important thing to note is that if you're running your Django site in a\nmulti-threaded or multi-node setup, you'll want to be careful about using a\ntime-based cache buster value. Each worker\/thread will probably have a slightly\ndifferent value for datetime.now(), which means your users will find themselves\nhaving cache misses randomly from page to page.\n\nRackspace Cloud Files\nMEDIASYNC['BACKEND'] = 'mediasync.backends.cloudfiles'\n\n\nSettings\nThe following settings are required in the mediasync settings dict:\nMEDIASYNC = {\n    'CLOUDFILES_CONTAINER': 'container_name',\n    'CLOUDFILES_USERNAME': 'cf_username',\n    'CLOUDFILES_API_KEY': 'cf_apikey',\n}\n\n\nTips\nThe Cloud Files backend lacks support for the following features:\n\nsetting HTTP Expires header\nsetting HTTP Cache-Control header\ncontent compression (gzip)\nSSL support\nconditional sync based on file checksum\n\n\nCustom backends\nYou can create a custom backend by creating a Python module containing a Client\nclass. This class must inherit from mediasync.backends.BaseClient. Additionally,\nyou must implement two methods:\ndef remote_media_url(self, with_ssl):\n    ...\n\nremote_media_url returns the full base URL for remote media. This can be\neither a static URL or one generated from mediasync settings:\ndef put(self, filedata, content_type, remote_path, force=False):\n    ...\n\nput is responsible for pushing a file to the backend storage.\n\nfiledata - the contents of the file\ncontent_type - the mime type of the file\nremote_path - the remote path (relative from remote_media_url) to which\nthe file should be written\nforce - if True, write file to remote storage even if it already exists\n\nIf the client supports gzipped content, you will need to override supports_gzip\nto return True:\ndef supports_gzip(self):\n        return True\n\n\nFile Processors\nFile processors allow you to modify the content of a file as it is being\nsynced or served statically.\nMediasync ships with three processor modules:\n\nslim is a minifier written in Python and requires the\nslimmer Python package. The Python package can be found here:\nhttp:\/\/pypi.python.org\/pypi\/slimmer\/\nyuicompressor is a minifier written in Java and can be downloaded\nfrom YUI's download page: http:\/\/developer.yahoo.com\/yui\/compressor\/.\nThis processor also requires an additional setting, as defined below.\nyuicompressor is new and should be considered experimental until\nthe mediasync 2.1 release.\nclosurecompiler is a javascript compiler provided by Google.\n\nCustom processors can be specified using the PROCESSORS entry in the\nmediasync settings dict. PROCESSORS should be a list of processor entries.\nEach processor entry can be a callable or a string path to a callable. If the\npath is to a class definition, the class will be instantiated into an object.\nThe processor callable should return a string of the processed file data, None\nif it chooses to not process the file, or raise mediasync.SyncException if\nsomething goes terribly wrong. The callable should take the following arguments:\ndef proc(filedata, content_type, remote_path, is_active):\n        ...\n\n\nfiledata\nthe content of the file as a string\ncontent_type\nthe mimetype of the file being processed\nremote_path\nthe path to which the file is being synced (contains the file name)\nis_active\nTrue if the processor should... process\n\nIf the PROCESSORS setting is used, you will need to include the defaults\nif you plan on using them:\n'PROCESSORS': (\n    'mediasync.processors.slim.css_minifier',\n    'mediasync.processors.slim.js_minifier',\n        ...\n),\n\nmediasync will attempt to use slimmer by default if you have the package\ninstalled and do not use the PROCESSORS setting.\n\nGoogle Closure Compiler\nGoogle's JavaScript Closure Compiler provides an API that allows files to be\ncompressed without installing anything locally. To use the service:\n'PROCESSORS': ('mediasync.processors.closurecompiler.compile',)\n\n\nYUI Compressor\nTo configure YUI Compressor you need to define a PROCESSORS and\nYUI_COMPRESSOR_PATH as follows, assuming you placed the \".jar\" file in\nyour ~\/bin path:\n'PROCESSORS': ('mediasync.processors.yuicompressor.css_minifier',\n               'mediasync.processors.yuicompressor.js_minifier'),\n'YUI_COMPRESSOR_PATH': '~\/bin\/yuicompressor.jar',\n\n\nFeatures\n\nIgnored Directories\nAny directory in STATIC_ROOT that is hidden or starts with an underscore\nwill be ignored during syncing.\n\nTemplate Tags\nWhen referring to media in HTML templates you can use custom template tags.\nThese tags can by accessed by loading the media template tag collection.\n{% load media %}\n\nAny tag that has a path argument can use either a string or a variable:\n{% media_url \"images\/avatar.png\" }\n{% media_url user.profile.avatar_path %}\n\nSome backends (S3) support https URLs when the requesting page is secure.\nIn order for the https to be detected, the request must be placed in the\ntemplate context with the key 'request'. This can be done automatically by\nadding 'django.core.context_processors.request' to TEMPLATE_CONTEXT_PROCESSORS\nin settings.py\n\nmedia_url\nRenders the STATIC_URL from settings.py with trailing slashes removed.\n<img src=\"{% media_url %}\/images\/stuff.png\">\n\nSTATIC_URL takes an optional argument that is the media path. Using the argument\nallows mediasync to add the CACHE_BUSTER to the URL if one is specified.\n<img src=\"{% media_url '\/images\/stuff.png' %}\">\n\nIf CACHE_BUSTER is set to 12345, the above example will render as:\n<img src=\"http:\/\/assets.example.com\/path\/to\/media\/images\/stuff.png?12345\">\n\nNOTE: Don't use this tag to serve CSS or JS files. Use the js and css tags\nthat were specifically designed for the purpose.\n\njs\nRenders a script tag with the correct include.\n{% js \"myfile.js\" %}\n\n\ncss\nRenders a <link> tag to include the stylesheet. It takes an optional second\nparameter for the media attribute; the default media is \"screen, projector\".\n{% css \"myfile.css\" %}\n{% css \"myfile.css\" \"screen\" %}\n\n\ncss_print\nShortcut to render as a print stylesheet.\n{% css_print \"myfile.css\" %}\n\nwhich is equivalent to\n{% css \"myfile.css\" \"print\" %}\n\n\nWriting Style Sheets\nUsers are encouraged to write stylesheets using relative URLS. The media\ndirectory is synced with S3 as is, so relative local paths will still work\nwhen pushed remotely.\nbackground: url(..\/images\/arrow_left.png);\n\n\nJoined files\nWhen serving media in production, it is beneficial to combine JavaScript and\nCSS into single files. This reduces the number of connections the browser needs\nto make to the web server. Fewer connections can dramatically decrease page\nload times and reduce the server-side load.\nJoined files are specified in the MEDIASYNC dict using JOINED. This is\na dict that maps individual media to an alias for the joined files.\n'JOINED': {\n    'styles\/joined.css': ['styles\/reset.css','styles\/text.css'],\n    'scripts\/joined.js': ['scripts\/jquery.js','scripts\/processing.js'],\n},\n\nFiles listed in JOINED will be combined and pushed to S3 with the name of\nthe alias. The individual CSS files will also be pushed to S3. Aliases must end\nin either .css or .js in order for the content-type to be set appropriately.\nThe existing template tags may be used to refer to the joined media. Simply use\nthe joined alias as the argument:\n{% css_print \"joined.css\" %}\n\nWhen served locally, template tags will render an HTML tag for each of the files\nthat make up the joined file:\n<link rel=\"stylesheet\" href=\"\/media\/styles\/reset.css\" type=\"text\/css\" media=\"screen, projection\" \/>\n<link rel=\"stylesheet\" href=\"\/media\/styles\/text.css\" type=\"text\/css\" media=\"screen, projection\" \/>\n\nWhen served remotely, one HTML tag will be rendered with the name of the joined file:\n<link rel=\"stylesheet\" href=\"http:\/\/bucket.s3.amazonaws.com\/styles\/joined.css\" type=\"text\/css\" media=\"screen, projection\" \/>\n\n\nSmart GZIP for S3\nIn previous versions of mediasync's S3 client, certain content was always pushed\nin a compressed format. This can cause major issues with clients that do not\nsupport gzip. New in version 2.0, mediasync will push both a gzipped and an\nuncompressed version of the file to S3. The template tags look at the request\nand direct the user to the appropriate file based on the ACCEPT_ENCODING HTTP\nheader. Assuming a file styles\/layout.css, the following would be synced to S3:\nstyles\/layout.css\nstyles\/layout.css.gzt\n\nNote the altered use of the .gz extension. Some versions of the Safari browser\non OSX ignore the Content-Type header for files ending in .gz and treat them\ninstead as files to download. This altered extension allows Safari to deflate\nand utilize the files correctly without affecting functionality in any other\ntested browsers.\n\nSignals\nmediasync provides two signals that allow you to hook into the syncing\nprocess. pre_sync is sent after the client is opened, but before the first\nfile is synced. post_sync is sent after the last file is synced, but before\nthe client is closed. This allows you to call commands on the client without\nhaving to worry about its state. The signals allow you to do common tasks such\nas calling Django 1.3's collectstatic command, process SASS stylesheets, or\nclean up files generated during a pre_sync process.\n\ncollectstatic receiver\nA receiver for calling the collectstatic management command is provided:\nfrom mediasync.signals import pre_sync, collectstatic_receiver\n\n# run collectstatic before syncing media\npre_sync.connect(collectstatic_receiver)\n\n\nSASS receiver\nA receiver for compiling SASS into CSS is provided:\nfrom mediasync.signals import pre_sync, sass_receiver\n\n# compile SASS files before syncing media\npre_sync.connect(sass_receiver)\n\nAny file in static root that has the sass or scss file extension will be\ncompiled into CSS. The compiled CSS file will be placed in the same directory\nand the original extension will be replaced with css. If a file exists with\nthe same css extension, it will be overwritten.\nBy default mediasync uses the sass command with no options. If you would\nlike to specify your own command, specify SASS_COMMAND in settings:\nMEDIASYNC = {\n    ...\n    'SASS_COMMAND': 'sass -scss -l',\n    ...\n}\n\n\nRunning MEDIASYNC\n.\/manage.py syncmedia\n\n\nChange Log\n\n2.2.0\n\nadded pre_sync and post_sync signals\nprovide basic receiver for calling collectstatic before syncing\nprovide receiver for compiling SASS before syncing\nshow media directory listing when serving locally in debug mode\nadd processor for Google's Closure Compiler API for JavaScript\ntemplate tags can now take a variable as the path argument\n\n\n2.1.0\n\ndefault to using STATIC_URL and STATIC_ROOT (Django 1.3), falling back\nto MEDIA_URL and MEDIA_ROOT if the STATIC_* settings are not set\nadd AWS_GZIP setting to optionally disable gzip compression in S3 client\n\nThanks to Rob Hudson and Dolan Antenucci for their contributions to this\nrelease.\n\n2.0.0\n\nupdated Rackspace Cloud Files backend\nuse gzip instead of deflate for compression (better browser support)\nsmart gzip client support detection\nadd pluggable backends\nadd pluggable file processors\nexperimental YUI Compressor\nsettings refactor\nallow override of settings.MEDIA_URL\nImprovements to the logic that decides which files to sync. Safely ignore\na wider variety of hidden files\/directories.\nMake template tags aware of whether the current page is SSL-secured. If it\nis, ask the backend for an SSL media URL (if implemented by your backend).\nmade SERVE_REMOTE setting the sole factor in determining if\nmedia should be served locally or remotely\nadd many more tests\ndeprecate CSS_PATH and JS_PATH\n\nThanks to Greg Taylor, Peter Sanchez, Jonathan Drosdeck, Rich Leland,\nand Rob Hudson for their contributions to this release.\n\n1.0.1\n\nadd application\/javascript and application\/x-javascript to JavaScript\nmimetypes\nbreak out of CSS and JS mimetypes\nadd support for HTTPS URLs to S3\nallow for storage of S3 keys in ~\/.boto configuration file\n\nThanks to Rob Hudson and Peter Sanchez for their contributions.\n\n1.0.0\nInitial release.\n","251":"sublime-laravelgenerator\nA Sublime Text plugin that allows you to make use of the Laravel 4\nGenerators by Jeffrey\nWay directly within Sublime Text.\nInstallation\n\nInstall the  Laravel 4\ngenerator commands through Composer.\nInstall the ST plugin through Package Control: Sublime Laravel Generator\nIf you are on Windows or php executable is not in PATH, please specify the path to it in laravelgenerator.sublime-settings. To do so, copy laravelgenerator.sublime-settings from this\nplugin to <Packages_Directory>\/Users\/ and make the edits to that file.\n\nUsage\n\nOpen a Laravel Project\nOpen the command palette (Ctrl+Shift+P)\nExecute any of the available Generate commands\nSee here for a basic workflow video\n\nNote: artisan needs to be in the project root.\nCustomization\nThe plugin is quite extensible. Interested users can extend the plugin for more\nartisan commands by adding the appropriate entries in\nDefault.sublime-commands.\nCredits\n\nJeffrey Way: for the idea and testing this\nplugin throughout the development.\n\n\nThis is a work in progress. Feedback is appreciated. Feel free to report any\nissues you come across\n","252":"Dagny\nDagny is a Django adaptation of Ruby on Rails\u2019s Resource-Oriented\nArchitecture (a.k.a. \u2018RESTful Rails\u2019).\nDagny makes it really easy to build resourceful web applications.\nYou can read the full documentation here.\nAt present, this project is in an experimental phase, so APIs are very liable to\nchange. You have been warned.\nP.S.: the name is a reference.\nMotivation\nRails makes building RESTful web applications incredibly easy, because\nresource-orientation is baked into the framework\u2014it\u2019s actually harder to make\nyour app unRESTful.\nI wanted to build a similar system for Django; one that made it incredibly\nsimple to model my resources and serve them up with the minimum possible code.\nOne of the most important requirements was powerful yet simple content\nnegotiation: separating application logic from the rendering of responses makes\nwriting an API an effortless task.\nFinally, as strong as Rails\u2019s inspiration was, it still needed to be consistent\nwith the practices and idioms of the Django and Python ecosystems. Dagny doesn\u2019t\nuse any metaclasses (yet), and the code is well-documented and readable by most\nPythonista\u2019s standards.\nAppetizer\nDefine a resource:\nfrom dagny import Resource, action\nfrom django.shortcuts import get_object_or_404, redirect\nfrom polls import forms, models\n\nclass Poll(Resource):\n    \n    @action\n    def index(self):\n        self.polls = models.Poll.objects.all()\n    \n    @action\n    def new(self):\n        self.form = forms.PollForm()\n    \n    @action\n    def create(self):\n        self.form = forms.PollForm(self.request.POST)\n        if self.form.is_valid():\n            self.poll = self.form.save()\n            return redirect(\"Poll#show\", self.poll.id)\n        \n        return self.new.render()\n    \n    @action\n    def edit(self, poll_id):\n        self.poll = get_object_or_404(models.Poll, id=int(poll_id))\n        self.form = forms.PollForm(instance=self.poll)\n    \n    @action\n    def update(self, poll_id):\n        self.poll = get_object_or_404(models.Poll, id=int(poll_id))\n        self.form = forms.PollForm(self.request.POST, instance=self.poll)\n        if self.form.is_valid():\n            self.form.save()\n            return redirect(\"Poll#show\", self.poll.id)\n        \n        return self.edit.render()\n    \n    @action\n    def destroy(self, poll_id):\n        self.poll = get_object_or_404(models.Poll, id=int(poll_id))\n        self.poll.delete()\n        return redirect(\"Poll#index\")\n\nCreate the templates:\n<!-- polls\/index.html -->\n<ol>\n  {% for poll in self.polls %}\n    <li><a href=\"{% url Poll#show poll.id %}\">{{ poll.name }}<\/a><\/li>\n  {% endfor %}\n<\/ol>\n<p><a href=\"{% url Poll#new %}\">Create a poll<\/a><\/p>\n\n<!-- polls\/new.html -->\n<form method=\"post\" action=\"{% url Poll#create %}\">\n  {% csrf_token %}\n  {{ self.form.as_p }}\n  <input type=\"submit\" value=\"Create Poll\" \/>\n<\/form>\n\n<!-- polls\/show.html -->\n<p>Name: {{ self.poll.name }}<\/p>\n<p><a href=\"{% url Poll#edit self.poll.id %}\">Edit this poll<\/a><\/p>\n\n<!-- polls\/edit.html -->\n<form method=\"post\" action=\"{% url Poll#update self.poll.id %}\">\n  {% csrf_token %}\n  {{ self.form.as_p }}\n  <input type=\"submit\" value=\"Update poll\" \/>\n<\/form>\n\nSet up the URLs:\nfrom django.conf.urls.defaults import *\nfrom dagny.urls import resources\n\nurlpatterns = patterns('',\n    (r'^polls\/', resources('polls.resources.Poll', name='Poll')),\n)\n\nDone.\nExample Project\nThere\u2019s a more comprehensive example project which showcases a user\nmanagement app, built in very few lines of code on top of the standard\ndjango.contrib.auth app.\nTo get it running:\ngit clone 'git:\/\/github.com\/zacharyvoase\/dagny.git'\ncd dagny\/\npip install -r REQUIREMENTS  # Installs runtime requirements\npip install -r REQUIREMENTS.test  # Installs testing requirements\ncd example\/\n.\/manage.py syncdb  # Creates db\/development.sqlite3\n.\/manage.py test users  # Runs all the tests\n.\/manage.py runserver\n\nThen just visit http:\/\/localhost:8000\/users\/ to see it in action!\nLicense\nThis is free and unencumbered software released into the public domain.\nAnyone is free to copy, modify, publish, use, compile, sell, or distribute this\nsoftware, either in source code form or as a compiled binary, for any purpose,\ncommercial or non-commercial, and by any means.\nIn jurisdictions that recognize copyright laws, the author or authors of this\nsoftware dedicate any and all copyright interest in the software to the public\ndomain. We make this dedication for the benefit of the public at large and to\nthe detriment of our heirs and successors. We intend this dedication to be an\novert act of relinquishment in perpetuity of all present and future rights to\nthis software under copyright law.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\nCONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nFor more information, please refer to http:\/\/unlicense.org\/\n","253":"logstash_formatter: JSON logs for logstash\nThis library is provided to allow standard python logging to output log data\nas json objects ready to be shipped out to logstash.\nThis project has been originally open sourced by exoscale (which is a great hosting service btw),\nthanks to them.\nInstalling\nUsing pip (PyPI):\npip install logstash_formatter\n\nManual:\ngit clone https:\/\/github.com\/ulule\/python-logstash-formatter.git\ncd python-logstash-formatter\npython setup.py install\n\nUsage\nJson outputs are provided by the LogstashFormatter logging formatter.\nimport logging, sys\nfrom logstash_formatter import LogstashFormatterV1\n\n# configure logging\nhandler = logging.StreamHandler(stream=sys.stdout)\nhandler.setFormatter(LogstashFormatterV1())\nlogging.basicConfig(handlers=[handler], level=logging.INFO)\n\n# use it\nlogging.info(\"my log\")\nThe LogstashFormatter may take the following named parameters:\n\nfmt: Config as a JSON string that supports:\n\nextra: provide extra fields always present in logs.\nsource_host: override source host name.\n\n\njson_cls: JSON encoder to forward to json.dump.\njson_default: Default JSON representation for unknown types,\nby default coerce everything to a string.\n\nLogstashFormatterV1 adheres to the more 1.2.0 schema and will not update\nfields, apart from a special handling of msg which will be updated to\nmessage when applicable.\nYou can also add extra fields to your json output by specifying a dict in place of message, or by specifying\nthe named argument extra as a dictionary. When supplying the exc_info named argument with a truthy value,\nand if an exception is found on the stack, its traceback will be attached to the payload as well.\nlogger.info({\"account\": 123, \"ip\": \"172.20.19.18\"})\nlogger.info(\"classic message for account: {account}\", extra={\"account\": account})\n\ntry:\n    h = {}\n    h['key']\nexcept:\n    logger.info(\"something unexpected happened\", exc_info=True)\nSample output for LogstashFormatter\nThe following keys will be found in the output JSON:\n\n@source_host: source hostname for the log\n@timestamp: ISO 8601 timestamp\n@message: short message for this log\n@fields: all extra fields\n\n{\n    \"@fields\": {\n        \"account\": \"pyr\",\n        \"args\": [],\n        \"created\": 1367480388.013037,\n        \"exception\": [\n            \"Traceback (most recent call last):\\n\",\n            \"  File \\\"test.py\\\", line 16, in <module>\\n    k['unknown']\\n\",\n            \"KeyError: 'unknown'\\n\"\n        ],\n        \"filename\": \"test.py\",\n        \"funcName\": \"<module>\",\n        \"levelname\": \"WARNING\",\n        \"levelno\": 30,\n        \"lineno\": 18,\n        \"module\": \"test\",\n        \"msecs\": 13.036966323852539,\n        \"name\": \"root\",\n        \"pathname\": \"test.py\",\n        \"process\": 1819,\n        \"processName\": \"MainProcess\",\n        \"relativeCreated\": 18.002986907958984,\n        \"thread\": 140060726359808,\n        \"threadName\": \"MainThread\"\n    },\n    \"@message\": \"TEST\",\n    \"@source_host\": \"phoenix.spootnik.org\",\n    \"@timestamp\": \"2013-05-02T09:39:48.013158\"\n}\nSample output for LogstashFormatterV1\nThe following keys will be found in the output JSON:\n\n@timestamp: ISO 8601 timestamp\n@version: Version of the schema\n\n{\"@version\": 1,\n \"account\": \"pyr\",\n \"lineno\": 1,\n \"levelno\": 30,\n \"filename\": \"test.py\",\n \"thread\": 140566036444928,\n \"@timestamp\": \"2015-03-30T09:46:23.000Z\",\n \"threadName\": \"MainThread\",\n \"relativeCreated\": 51079.52117919922,\n \"process\": 10787,\n \"source_host\": \"phoenix.spootnik.org\",\n \"processName\": \"MainProcess\",\n \"pathname\": \"test.py\",\n \"args\": [],\n \"module\": \"test\",\n \"msecs\": 999.9005794525146,\n \"created\": 1427708782.9999006,\n \"name\": \"root\",\n \"stack_info\": null,\n \"funcName\": \"<module>\",\n \"levelname\": \"WARNING\",\n \"message\": \"foo\"}\n","254":"A simple django middleware that logs request times using the Django 1.3 logging support, and a management command to analyze the resulting data. Once installed and configured you can run a command line these:\npython manage.py analyze_timelog\npython manage.py analyze_timelog --noreverse\nAnd generate useful tabular data like this:\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\n| view                     | method | status | count | minimum | maximum | mean  | stdev           |\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\n| boxes.viewsBoxDetailView | GET    | 200    | 9430  | 0.14    | 0.28    | 0.21  | 0.0700037118541 |\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\n| boxes.viewsBoxListView   | GET    | 200    | 66010 | 0.17    | 0.28    | 0.232 | 0.0455415351076 |\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\n| django.views.staticserve | GET    | 200    | 61295 | 0.00    | 0.02    | 0.007 | 0.0060574669888 |\n+--------------------------+--------+--------+-------+---------+---------+-------+-----------------+\n\nThis project was heavily influenced by the Rails Request log analyzer.\nInstallation\npip install django-timelog\nOnce installed you need to do a little configuration to get things working. First add the middleware to your MIDDLEWARE_CLASSES in your settings file.\nMIDDLEWARE_CLASSES = (\n  'timelog.middleware.TimeLogMiddleware',\nNext add timelog to your INSTALLED_APPS list. This is purely for the management command discovery.\nINSTALLED_APPS = (\n  'timelog',\nThen configure the logger you want to use. This really depends on what you want to do, the django 1.3 logging setup is pretty powerful. Here\u2019s how I\u2019ve got logging setup as an example:\nTIMELOG_LOG = '\/path\/to\/logs\/timelog.log'\n\nLOGGING = {\n  'version': 1,\n  'formatters': {\n    'plain': {\n      'format': '%(asctime)s %(message)s'},\n    },\n  'handlers': {\n    'timelog': {\n      'level': 'DEBUG',\n      'class': 'logging.handlers.RotatingFileHandler',\n      'filename': TIMELOG_LOG,\n      'maxBytes': 1024 * 1024 * 5,  # 5 MB\n      'backupCount': 5,\n      'formatter': 'plain',\n    },\n  },\n  'loggers': {\n    'timelog.middleware': {\n      'handlers': ['timelog'],\n      'level': 'DEBUG',\n      'propogate': False,\n     }\n  }\n}\nLastly, if you have particular URIs you wish to ignore you can define them using basic regular expressions in the TIMELOG_IGNORE_URIS list in settings.py:\nTIMELOG_IGNORE_URIS = (\n    '^\/admin\/',         # Ignores all URIs beginning with '\/admin\/'\n    '^\/other_page\/$',   # Ignores the URI '\/other_page\/' only, but not '\/other_page\/more\/'.\n    '.jpg$',            # Ignores all URIs ending in .jpg\n)","255":"Hooked\nHow to use\nHooked is a python library for managing git hooks. It adds a plugin system to the git hooks which allow us to write simple python scripts to run for the different phases in git.\nThere are 3 different actions that can be done with hooked.py.\nTo demonstrate lets create a test git directory:\n$ mkdir \/tmp\/testgit\n$ cd \/tmp\/testgit\n$ git init\n\nNone of the hooks are enabled in a normal git directory:\n$ ls .git\/hooks\/\napplypatch-msg.sample  pre-applypatch.sample      pre-rebase.sample\ncommit-msg.sample      pre-commit.sample      update.sample\npost-update.sample     prepare-commit-msg.sample\n\nIn the hooked directory, we initialize the hooked system for that git repository.\n$ python hooked.py --git-root=\/tmp\/testgit\n\nNow, there is a few additions to the hooks directory:\n$ ls .git\/hooks\/\naction             post-update.sample     prepare-commit-msg\napplypatch-msg.sample  pre-applypatch.sample  prepare-commit-msg.sample\ncommit-msg         pre-commit         pre-rebase.sample\ncommit-msg.sample      pre-commit.sample      update.sample\n\nAt time of writing, hooked.py creates prepare-commit-msg, commit-msg and pre-commit.\nIt also creates the action directory.\n$ ls .git\/hooks\/action\/\nconfig.json  __init__.py  other_hook.py  test_hook.py\n\nBy default, it includes some example hooks but they are disabled. The config.json is a simple json configuration which lists which hooks are installed. By default, none are installed.\n$ cat .git\/hooks\/action\/config.json \n{ \"hooks\": [] }\n\nLets install some hooks, so hooks are stored outside the hooked repository as they are on a per user basis. lets create a git_hooks directory:\n$ mkdir \/tmp\/git_hooks\n$ cd \/tmp\/git_hooks\n\nAnd lets make a python hook to demonstrate:\n$ cat - > new_hook.py\ndef precommit(git_state):\n    for fname in git_state[\"files\"]:\n        if \"action\" in fname:\n            print \"ARGGGGGHHHHHH\"\n            return False\n    return True\n\nThis simply shouts if the word action is in the filename. Lets inject this into our git repo hooks. we can inject a directory or just a file. If you use a directory it will pull out all python files from the directory and all hooks that are copied will be turned on by default.\n$ python hooked.py --git-root=\/tmp\/testgit --inject=\/tmp\/git_hooks\/new_hook.py\n\nNow lets look at the action folder and the contents of config.json:\n$ ls .git\/hooks\/action\/\nconfig.json  __init__.py  new_hook.py  other_hook.py  test_hook.py\n$ cat .git\/hooks\/action\/config.json \n{\"hooks\": [\"new_hook\"]}\n\nYour hooks have now been installed. If in the future you want to completely remove the usage of hooked.py you can do the following:\n$ python hooked.py --git-root=\/tmp\/testgit --clean\n\n","256":"PyUnit provides a basic set of assertions which can get you started with unit testing python, but it\u2019s always useful to have more. Django also has a few specific requirements and common patterns when it comes to testing. This set of classes aims to provide a useful starting point for both these situations.\nThe application also overrides the default Django test runner, adding a few useful features:\nInstallation\nJust add the project to your INSTALLED_APPS.\nINSTALLED_APPS = (\n\t'test_extensions',\n)\nNote that this application steals the test command from django, overriding it with extra toys. If another application in your INSTALLED_APPS does this too then the last one in the list will win. South migrations does this in order to use the django syncdb command for testing, which test_extensions does too. As of 0.4 test_extensions works with South, just as long as you include it after south in the list of installed apps.\nAssertions\nSee the examples directory in the src\/test_extensions directory for details of a large number of useful assertions for testing django apps:\n\nassert_response_contains\nassert_response_doesnt_contain\nassert_regex_contains\nassert_render_matches\nassert_code\nassert_render\nassert_render_matches\nassert_doesnt_render\nassert_render_contains\nassert_render_doesnt_contain\n\nTest Runners\nXMLUnit\nSometimes it\u2019s nice to have a file reporting the results of a test run. Some applications such as CruiseControl can use this to display the results in a user interface.\npython manage.py test --xml\nCode Coverage\nIf you want to know what code is being run when you run your test suite then codecoverage is for you. These two flags use two different third party libraries to calculate coverage statistics. The first dumps the results to stdout, \u2014xmlcoverage creates a cobertura-compatible xml output, and the last one creates a series of files displaying the results.\npython manage.py test --coverage\npython manage.py test --xmlcoverage\npython manage.py test --figleaf\nNo Database\nSometimes your don\u2019t want the overhead of setting up a database during testing, probably because your application just doesn\u2019t use it.\npython manage.py test --nodb\npython manage.py test --nodb --coverage\npython manage.py test --nodb --xmlcoverage\nWARNING Don\u2019t use this if you use the ORM in your app. An outstanding issue means that you can get into trouble. Your tests will still hit the database, but it will be your non test data.\nLocal Continuous Integration Command\nThanks to Roberto Aguilar (http:\/\/github.com\/rca) for providing a auto-reloading version of the test runner. Run the runtester command and it should run your test suite whenever you change a file (similar to how runserver reloads the server each time you change something.)\nSee this thread\n from the Django Developer list of more information and discussion.\npython manage.py runtester\nLicence\nXMLUnit is included out of convenience. It was written by Marc-Elian Begin <Marc-Elian.Begin@cern.ch> and is Copyright \u00a9 Members of the EGEE Collaboration. 2004. http:\/\/www.eu-egee.org\nThe rest of the code is licensed under an MIT license:\nCopyright \u00a9 2008 Gareth Rushgrove <gareth@morethanseven.net>\nPermission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \u201cSoftware\u201d), to deal in the Software without\nrestriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following\nconditions:\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.","257":"Hookah\nThe HTTP event engine\nCurrent Features\n\nInterface is 100% HTTP (use easily from any language)\nDispatches POST requests (webhooks) asynchronously with retry\nProvides publish\/subscribe interface using PubSubHubbub protocol (experimental)\nProvides \"Twitter Stream API\"-style long-polling interface for topics (super experimental)\n\nAbout\nHookah was originally created to ease the implementation of webhooks in your web systems. While webhooks are still at the core, it's becoming a scalable HTTP event engine with HTTP pubsub and long-polling event streaming. And of course, webhooks. Any system with webhooks or looking to implement webhooks will benefit from Hookah.\nRequirements\nHookah currently depends on Twisted.\nUsage\nHookah is a simple, lightweight standalone web server that you run locally alongside your existing web stack. Starting it from the command line is simple:\n    twistd hookah --port 8080\n\nUsing the Dispatcher\nPosting to \/dispatch with a _url POST parameter will queue that POST request for that URL and return immediately. This allows you to use Hookah as an outgoing request queue that handles retries, etc. Using HTTP means you can do this easily from any language using a familiar API.\nPosting to \/dispatch with a _topic POST parameter will broadcast that post to any callbacks subscribed to that topic (see following PubSub section), or any stream consumers with a long-running request on that topic.\nUsing PubSub\nRefer to the PubSubHubbub spec, as Hookah is currently quite compliant with this excellent protocol. The hub endpoint is at \/hub, but this multiplexes (based on 'hub.mode' param) between \/publish for publish pings, and \/subscribe for subscription requests.\nThis feature is still very early and as a result it is incomplete. The main caveat is that there is no permanent storage of subscription data or of the queues. This means if you were to restart Hookah, all subscriptions would have to be made again.\nUsing Streams\nHookah implements a long-running stream API, modeled after Twitter's Stream API. Just do a GET request to \/stream with a topic parameter, and you'll get a persistent, chunked HTTP connection that will send you messages published to that topic as they come in. Refer to the Twitter Stream API docs to get a better feel for this pragmatic Comet streaming technique.\nTodo\n\nPersistent storage (SQLite, MySQL, CouchDB) and queuing (in memory, Kestrel, RabbitMQ) backends\nConfiguration\nBacklog\/history with resend\n\"Errback\" webhook\nAsync response handling\n\nLicense\nHookah is released under the MIT license, which can be found in the LICENSE file.\nContributors\n\nyou?\n\nAuthor\nJeff Lindsay progrium@gmail.com\nLearn more about web hooks\nhttp:\/\/webhooks.org\n","258":"\ncensus.ire.org\nA nationwide census browser for 2000 and 2010 census data.\n\nDependencies\nYou will need Python 2.7, the PostGIS stack, virtualenv and virtualenvwrapper. Mac Installation instructions at: http:\/\/blog.apps.chicagotribune.com\/2010\/02\/17\/quick-install-pythonpostgis-geo-stack-on-snow-leopard\/):\nOther required software:\n\nmongodb\nwget\nmdbtools.\n\nOn a Mac you can get these with Brew:\nbrew install mongodb\nbrew install wget\nbrew install mdbtools\n\n\nBootstrapping the webapp\nTo get the web application running:\ncd censusweb\nmkvirtualenv --no-site-packages censusweb\npip install -r requirements.txt\npython manage.py runserver\n\n\nConfiguring the webapp\nBy default the webapp is going to use the data published to the IRE test site, which may not be accessible to you. To use your own data open censusweb\/config\/settings.py and modify the following line:\nAPI_URL = 'http:\/\/s3.amazonaws.com\/census-test'\n\nSee the next section to learn how to deploy data to your custom S3 bucket.\n\nLoading data\nOnce you've setup the webapp you will have the requirements needed to load data. If you want to load embargoed data you will need to define environment variables for your username and password:\nCENSUS_USER=cgroskopf@tribune.com\nCENSUS_PASS=NotMyRealPassword\n\nYou will also need to have defined your Amazon Web Services credentials so that you can upload the rendered data files to S3:\nexport AWS_ACCESS_KEY_ID=\"foo\"\nexport AWS_SECRET_ACCESS_KEY=\"bar\"\n\nYou will also need to modify the load configuration to point at the same S3 bucket you configured for the webapp. Open dataprocessing\/config.py and modify the following lines:\nS3_BUCKETS = {\n    'staging': 'census-test',\n    'production': 'censusdata.ire.org',\n}\n\nTo load SF1 data for Hawaii make sure you have Mongo running and then execute the following commands:\ncd dataprocessing\n.\/batch_sf.sh Hawaii staging\n\n\nCredits\nThis application was a project of Investigative Reporters and Editors \/ National Institute for Computer-Assisted Reporting. Funding was generously provided by The Reynolds Journalism Institute.\nThe following journalists and nerds contributed to this project:\n\nJeremy Ashkenas (New York Times)\nBrian Boyer (Chicago Tribune)\nJoe Germuska (Chicago Tribune)\nChristopher Groskopf (Chicago Tribune)\nMark Horvit (IRE)\nRyan Mark (Chicago Tribune)\nCurt Merrill (CNN)\nPaul Overberg (USA Today)\nTed Peterson (IRE)\nAron Pilhofer (New York Times)\nMike Tigas (Spokesman-Review)\nMatt Waite (University of Nebraska)\n\n\nLicense\nThis software is licensed under the permissive MIT license. See COPYING for details.\n","259":"\u8fd9\u662f2015\u5e74\u7684\u5199\u7684\u9879\u76ee\uff0c\u5df2\u7ecf\u4e0d\u518d\u7ef4\u62a4\uff0c\u529f\u80fd\u53ef\u80fd\u5df2\u7ecf\u5931\u6548\uff0c\u540e\u671f\u6709\u65f6\u95f4\u4f1a\u7528kotlin\u91cd\u5199\u4e00\u904d\uff0c\u65e0\u53c2\u8003\u4ef7\u503c\npedometer\nthis is a pedometer demo,I will rewrite with kotlin\n","260":"Oh snap!\nToolbarMenudrawer 2.0 is here :D\n","261":"Socialize SDK Android\n\nIntegrating Socialize into your App\nCheck out the full documentation to learn how to integrate Socialize into your app:\nhttp:\/\/socialize.github.io\/socialize-sdk-android\/\n\nBuilding Socialize from Source\nNote: This is not required if you simply want to integrate Socialize into your app\n\nPrerequisites\nMake sure you have the following installed on your local machine:\n\nAndroid SDK (https:\/\/developer.android.com\/sdk\/)\n\nPip:\nsudo easy_install pip\n\n\nSphinx 1.2.2:\nsudo pip install sphinx\n\n\nANT:\nbrew install ant\n\n\n\nAlso make sure you have the following versions of the Android SDK installed:\n\nAndroid 2.2 (API 8)\nAndroid 4.4 (API 19)\n\nThese are installed using the Android SDK manager:\nhttp:\/\/developer.android.com\/tools\/help\/sdk-manager.html\n\nBuilding Socialize\nFirst clone this repo:\ngit clone git@github.com:socialize\/socialize-sdk-android.git\n\nSocialize depends on 3 external library projects:\ngit clone git@github.com:socialize\/android-ioc.git\ngit clone git@github.com:socialize\/loopy-sdk-android.git\ngit clone git@github.com:facebook\/facebook-android-sdk.git\n\n\nSetup the Facebook SDK for Build\nSwitch to the verified (tested) version of Facebook:\ncd facebook-android-sdk\ngit checkout sdk-version-3.17.2\ncd ..\/\n\nThis version of the Facebook SDK (3.17.2) has some compilation warnings which are treated\nas errors by facebook.  To override this, we need to change the compiler arguments in the\nfacebook ant.properties:\nvim facebook-android-sdk\/facebook\/ant.properties\n\nReplace the following line:\njava.compilerargs=-Xlint -Werror\n\nWith:\njava.compilerargs=-Xlint\n\nNow you can build the SDK distribution:\ncd socialize-sdk-android\/sdk\nant -Dsdk.dir=\/usr\/local\/android clean build\n\nMake sure you replace \/usr\/local\/android with your local path to the Android SDK\n\nBuilding the Demo App\nTo build and test the demo app from the command line:\ncd socialize-sdk-android\/demo\nant -Dsdk.dir=\/usr\/local\/android clean release\n\nMake sure you replace \/usr\/local\/android with your local path to the Android SDK\nNow you can install the demo app:\n\/usr\/local\/android\/platform-tools\/adb uninstall com.socialize.demo\n\/usr\/local\/android\/platform-tools\/adb install bin\/socialize-demo-release.apk\n\nThe demo app is called, Socialize Demos\n\nBuilding the Documentation\nNote: Sphinx 1.2.2 is required to generate docs:\nsudo pip install sphinx\n\nTo build the html version of the documentation:\ncd socialize-sdk-android\/sdk\nant -Dsdk.dir=\/usr\/local\/android doc\n\nMake sure you replace \/usr\/local\/android with your local path to the Android SDK\nNow you can browse the documentation:\nopen build\/docs\/user_guide\/index.html\n\n\nRunning the Tests\nIn order to run the tests you need either an Android 4.4 device or emulator.  We recommend using the\nGenymotion Android virtualization platform available here: http:\/\/www.genymotion.com\/\nEnsure the device\/emulator is connected and available:\n\/usr\/local\/android\/platform-tools\/adb devices\n\nIf you do not see any devices listed, try restarting the adb server:\n\/usr\/local\/android\/platform-tools\/adb kill-server\n\/usr\/local\/android\/platform-tools\/adb start-server\n\nPrior to running the tests you MUST run an sdk cleanup so that the stage server has its state reset.\nThis is a python script located in the test folder:\ncd socialize-sdk-android\/test\n\npython sdk-cleanup.py <consumer-key> <consumer-secret> \\\n<http:\/\/stage.api.socialize.com\/v1> \\\n[facebook_user_id] [facebook_token]\n\nTo run the tests:\nant -propertyfile ant.global.properties -Dsdk.dir=\/usr\/local\/android test-with-results\n\nMake sure you replace \/usr\/local\/android with your local path to the Android SDK\nNow you can browse the coverage results:\nopen coverage-results\/coverage.html\n\n\nBuilding the Distro\nTo build the distributable SDK (zip):\ncd socialize-sdk-android\/sdk\nant -Dsdk.dir=\/usr\/local\/android clean build\n\nMake sure you replace \/usr\/local\/android with your local path to the Android SDK\n","262":"Presentation\nAn architecture for Android as a replacement of MVC.\nWhy should I use Presentation?\nBecause you want to have more readable, testable code.\nAvoid \"God Objects\", mainly your Activities or Fragments.\nHow does it work?\nSeparation of responsibilities by module:\n\nPresenter: Get a Business Object from the DataProvider and give instructions to the ViewProxy\nDataProvider: Communicate with the \"outside\" to set and get the data, following the instructions of the Presenter\nViewProxy: Convert Presenter instructions and set values to Android Views.\n\nArchitecture\n\nLeak safe.\nDon't hold a strong reference to the DataProvider, Presenter, or ViewProxy.\nSample\nThe goal is to make this application:\n\nEach public method of the modules are defined into an interface:\npublic interface FormDef {\n\n    interface IPresenter extends Base.IPresenter {\n\n        void onClickSaveButton(String value);\n    }\n\n    interface IDataProvider extends Base.IDataProvider {\n\n        String getValueSaved();\n\n        void saveValue(String value);\n    }\n\n    interface IView extends Base.IView {\n\n        void setValueSaved(String text);\n    }\n\n}\n\nThen you have your:\n\nFormPresenter that extends BasePresenter and implements FormDef.IPresenter\nFormDataProvider that extends BaseDataProvider and implements FormDef.IDataProvider\nFormViewProxy that extends BaseViewProxy and implements FormDef.IView\n\nThe easiest way to use this architecture is to override BaseActivity, even if it's NOT mandatory.\npublic class FormActivity extends BaseActivity<FormPresenter, FormViewProxy> {\n\n    @Override\n    public int getContentView() {\n        return R.layout.activity_form;\n    }\n\n    @Override\n    public Class<FormPresenter> getPresenterClass() {\n        return FormPresenter.class; \/\/ the class of your presenter to be used within this activity\n    }\n\n    @Override\n    public FormViewProxy newViewProxy(FormPresenter presenter, Bundle savedInstanceState) {\n        return new FormViewProxy(this); \/\/ the view proxy, with the Activity in param\n    }\n}\n\nNote: it also exists a BaseFragment to do exactly the same but within a Fragment.\nGo further\nRe-use Presenter after Activity destruction\nBecause BasePresenter extends ViewModel (from Android Architecture), it is possible to re-use the same Presenter after a rotation for example.\nWhen you are using BaseActivity, your presenters will be automatically re-used.\nNote: in this case, the DataProvider will also be kept, but you will have to re-create the ViewProxy.\nPresenter into an Adapter\nThis library has a dependency on Efficient Adapter to use the same view cache mechanism.\nThis allows to apply the Presentation pattern to object in an Adapter as well. Your ViewHolder should extend PresenterViewHolder and your Presenter should extend BaseItemPresenter.\nProguard\nNothing special needed.\nGradle\ndependencies {\n    compile 'com.skocken:presentation:2.5.0'\n}\n\nAndroid Support library\nIf you are still using the deprecated Android Support Library (instead of AndroidX), please use the dependency 2.4.X instead.\nLicense\n\nApache 2.0\n\nContributing\nPlease fork this repository and contribute back using\npull requests.\nAny contributions, large or small, major features, bug fixes, unit\/integration tests are welcomed and appreciated\nbut will be thoroughly reviewed and discussed.\n","263":"\n\u5230\u8fbe\u9876\u90e8\u6216\u5e95\u90e8\u7ee7\u7eed\u62c9\u52a8\u65f6\uff0c\u5b9e\u73b0Item\u95f4\u7684\u76f8\u4e92\u5206\u79bb\uff0c\u6709\u4e24\u79cd\u6a21\u5f0f\uff1a\n1.\u5168\u90e8\u5206\u79bb\u7684\u6a21\u5f0f\uff0c\u5373\u5c4f\u5e55\u5185\u6240\u6709Item\u90fd\u4f1a\u5206\u79bb\n2.\u90e8\u5206\u5206\u79bb\u6a21\u5f0f\uff0c\u4ee5\u70b9\u51fb\u4f4d\u7f6e\u4e3a\u5206\u754c\u70b9\uff0c\u90e8\u5206item\u5206\u79bb\n\u7528\u6cd5\n\u5728\u4ee3\u7801\u4e2d\nPullSeparateListView lv = (PullSeparateListView) findViewById(R.id.pullExpandListView);\n\/\/\u5168\u90e8\u5206\u79bb\u8bbe\u7f6e\u4e3atrue,\u90e8\u5206\u5206\u79bb\u8bbe\u7f6e\u4e3afalse\u3002\u9ed8\u8ba4\u4e3afalse\nlv.setSeparateAll(true);\n\n\u5728xml\u4e2d\n<com.chiemy.pullseparate.PullSeparateListView\n    android:id=\"@+id\/pullExpandListView1\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:layout_below=\"@+id\/is_separateAll_cb\"\n\tapp:separate_all=\"true\"\n    >\n<\/com.chiemy.pullseparate.PullSeparateListView>\n\n\u53e6\u5916\u8fd8\u6dfb\u52a0\u4e86\u70b9\u51fb\u7f29\u653e\u7684\u6548\u679c\n","264":"[DEPRECATED] RxAssertions\n\n\nThis project is no longer maintened \/ active. Thanks for all OSS people that contributed and\/or provided feedback !!!\n\nRxAssertions is a simple idea for better RxJava assertions.\nI found the original idea from the guys of Ribot : in fact, I think this a good idea and helps to keep tests clean.\nHowever, Ribot guys deprecated their original repo some time ago in favor of vanilla TestSubscriber, so I decided to take my own shot on this.\nThis library mimics and improves the original Ribot`s idea with the following goodies :\n\nAssertJ powered assertions for RxJava (as the original one)\nAll tests rely on BlockingObservable internally\nInternal API rely 100% on TestSubscriber\nImproved public API, covering most of TestSubscriber provided assertions\nImproved Assertions entry points from factory methods, with Observable, BlockingObservable, Single and Completable support\n\nLets see some code diet :\nRegular assertions with TestSubscriber\nTestSubscriber<String> testSubscriber = new TestSubscriber<>();\nObservable.just(\"RxJava\", \"Assertions\").toBlocking().subscribe(testSubscriber);\ntestSubscriber.assertCompleted();\ntestSubscriber.assertNoErrors();\ntestSubscriber.assertValues(\"RxJava\", \"Assertions\");\nAssertions with RxAssertions\nRxAssertions.assertThat(Observable.just(\"RxJava\", \"Assertions\"))\n\t\t.completes()\n\t\t.withoutErrors()\n\t\t.expectedValues(\"RxJava\", \"Assertions\");\nor\nassertThat(Observable.empty())\n\t\t.emitsNothing()\n\t\t.completes()\n\t\t.withoutErrors();\nor\nSingle<String> single = Single.fromCallable(() -> \"RxJava\");\nassertThat(single).completes().expectedSingleValue(\"RxJava\");\nYou can find other examples at test folder\nSetup\nAdd it in your build.gradle\nrepositories {\n\t...\n\tmaven { url \"https:\/\/jitpack.io\" }\n\n}\nAdd the dependency\ndependencies {\n\t...\n\ttestCompile 'com.github.ubiratansoares:rxassertions:$version'\n}\nCheck the releases tab for the latest version.\nRxAssertions uses RxJava 1.1.9 and AssertJ 2.5.0 as dependencies.\nExperimental\nSince v0.3.0, we have some assertions leveraging on AssertJ Conditions API. This kind of assertion offers flexible matching both for emissions and error checks. You can find samples at test folder.\nContributing\nPRs are wellcome. \ud83d\ude80\nCredits\n\nRibot guys for the original idea\nRxJava and AssertJ guys for these awesome libraries\n\nLicense\nCopyright (C) 2016 Ubiratan Soares\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","265":"play2-crud\n\nPowerful CRUD & DAO implementation with REST interface for play framework 2.x\nFor the Typesafe Activator check play2-crud-activator.\nSome screenshots\n\n\nindex page\n\n\n\ncreate page\n\n\n\nlist page\n\n\n\nQuick Start\nFollow these steps to use play2-crud. You can also use it partially just for DAO or CRUD controllers. If you think any part needs further explanation, please report a new issue.\nAdd play2-crud dependency\nYou can begin with adding play2-crud dependency inside conf\/Build.scala file.\n\nAdd app dependency:\n\n    val appDependencies = Seq(\n        javaCore, javaJdbc, javaEbean,\n        \"play2-crud\" % \"play2-crud_2.10\" % \"0.7.0\"\n    )\n\n\n\n\nDependency version is for version 0.7.0 defined, but you can use the latest version.\n\n\nAdd custom maven repositories:\n\n\n    val main = play.Project(appName, appVersion, appDependencies).settings(\n        \/\/maven repository\n        resolvers += \"release repository\" at  \"http:\/\/hakandilek.github.com\/maven-repo\/releases\/\",\n        resolvers += \"snapshot repository\" at \"http:\/\/hakandilek.github.com\/maven-repo\/snapshots\/\"\n    )\n\n\nAssociate Global settings\nDirect reference\nIf you don't want to override the play application launcher, you just have to notice to play that the class to use as launcher is now GlobalCRUDSettings. Change the application.global configuration key in the conf\/application.conf file, and use play.utils.crud.GlobalCRUDSettings:\n...\napplication.global=play.utils.crud.GlobalCRUDSettings\n...\n\n\nDefine routes\n# CRUD Controllers\n->     \/app             play.crud.Routes\n\n# REST API\n->     \/api             play.rest.Routes\n\n\nDefine model\n\nModel class has to implement play.utils.dao.BasicModel with the type parameter indicating the type of the @Id field.\n\n@Entity\npublic class Sample extends Model implements BasicModel<Long> {\n\n   @Id\n   private Long key;\n\n   @Basic\n   @Required\n   private String name;\n\n   public Long getKey() {\n      return key;\n   }\n\n   public void setKey(Long key) {\n      this.key = key;\n   }\n\n   public String getName() {\n      return name;\n   }\n\n   public void setName(String name) {\n      this.name = name;\n   }\n}\n\nHere the Sample model class implements BasicModel<Long> where key field indicated with @Id is Long.\n\n... call http:\/\/localhost:9000\/app and voila!\nSamples\n\nSample with basic dynamic CRUD controllers\nSample with custom views is a full featured sample.\nFull featured sample with DAO and DAOListeners\nSample with Cache usage\n\nHOW-TO\nHere you can find some HOW-TO documents introducing some powerful functionality:\n\nHOW-TO use simple CRUD\nHOW-TO define a custom DAO\nHOW-TO define a custom Controller\nHOW-TO use DAO Listeners\nHOW-TO use dynamic REST Controllers\nHOW-TO use custom REST Controllers\nHOW-TO Override Play Launcher\n\n","266":"WatchTower\n\n\n\nNote: In order to use this app, you'll need to use the Google API Console to register your SHA1 token along with the package name, this app won't work otherwise.\nWatchTower is a simple application which was created to test and explore the functionality of the new Proximity Beacon API. The application can be used to try out:\n\nRegistering Beacons\nUpdating Beacons\nViewing Beacons\nViewing Beacon Diagnostics\nViewing Beacon Attachments\nAdding Beacon Attachments\nDeleting Single Beacon Attachments\nDeleting Batch Beacon Attachments by Type\n\nSome features are not very practical within a mobile-app (for example, adding json data to attachments), so these have not been included.\n\n\n\nNote: This was built quickly to simply test the APIs functionality. If you come across any bugs please feel free to submit them as an issue, or open a pull request ;)\nFor further information, please read the supporting blog post.\nRequirements\n\nAndroid SDK.\nAndroid 5.1 (API 22) .\nAndroid SDK Tools\nAndroid SDK Build tools 22.0.1\nAndroid Support Repository\nAndroid Support library\nEnabled [Unit Test support] (http:\/\/tools.android.com\/tech-docs\/unit-testing-support)\n\nBuilding\nTo build, install and run a debug version, run this from the root of the project:\n.\/gradlew installRunDebug\n\nTesting\nFor Android Studio to use syntax highlighting for Automated tests and Unit tests you must switch the Build Variant to the desired mode.\nTo run unit tests on your machine using [Robolectric] (http:\/\/robolectric.org\/):\n.\/gradlew testDebug\n\nTo run automated tests on connected devices:\n.\/gradlew connectedAndroidTest\n\nAttributions\nThanks to the following for icons off of Noun Project:\n\nSt\u00e9phanie Rusch - Beacon Icon \nAbraham - Cloud Icon\nS.Shohei - Battery Icon\nJuergen Bauer - Alert Icon\nPham Thi Dieu Linh - Attachment Icon\n","267":"\u8fd9\u662f\u4e00\u4e2a\u6781\u7b80\u7684HelloWorld\u5e94\u7528\uff0c\u4e3b\u8981\u7528\u6765\u5c55\u793a\u5982\u4f55\u5728Android\u5e73\u53f0\u67b6\u6784Flux\u5e94\u7528\u3002\u5e76\u63d0\u4f9b\u4e00\u4e9b\u57fa\u7840\u4ee3\u7801\uff0c\u65b9\u4fbf\u5f00\u53d1\u8005\u76f4\u63a5Copy\u8fd9\u4e9b\u4ee3\u7801\u5230\u81ea\u5df1\u7684\u5de5\u7a0b\u4e2d\uff0c\u7701\u6389\u91cd\u65b0\u9020\u8f6e\u5b50\u7684\u8fc7\u7a0b\u3002\u63a5\u4e0b\u6765\u4f1a\u4e00\u6b65\u6b65\u7684\u89e3\u91ca\u8fd9\u4e2a\u5e94\u7528\u662f\u5982\u4f55\u6784\u5efa\u7684\u3002\nDemo\u7a0b\u5e8f\u662f\u7528AndroidStudio\u5f00\u53d1\u7684\uff0c\u5047\u8bbe\u4f60\u5df2\u7ecf\u4e86\u89e3Android\u548cAndroidStudioIDE\uff0c\u5982\u679c\u4f60\u5df2\u7ecf\u5f88\u719f\u6089Android\u5e94\u7528\u7684\u5f00\u53d1\uff0c\u770b\u5b8cAndroidFlux\u4e00\u89c8\u6216\u8bb8\u5df2\u7ecf\u53ef\u4ee5\u5f00\u53d1\u51fa\u57fa\u4e8eFlux\u6846\u67b6\u7684\u5e94\u7528\uff0c\u5982\u679c\u4f60\u5e76\u4e0d\u719f\u6089Flux\u6216\u8005Android\uff0c\u52a1\u5fc5\u5148\u8bfb\u5b8c\u8fd9\u7bc7\u6587\u6863\n\u6e90\u7801\u7ed3\u6784\n\u672c\u7740\u67b6\u6784\u5373\u76ee\u5f55\u7684\u601d\u60f3\uff0c\u8ba9\u6211\u4eec\u5148\u770b\u4e00\u4e0b\u6e90\u7801\u7ed3\u6784\uff0c\u6574\u4e2a\u7684\u6e90\u7801\u7ed3\u6784\u662f\u8fd9\u6837\u7684\uff1a\n\u279c  tree .\n.\n\u251c\u2500\u2500 MainActivity.java\n\u251c\u2500\u2500 actions\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Action.java\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ActionsCreator.java\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 MessageAction.java\n\u251c\u2500\u2500 dispatcher\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Dispatcher.java\n\u251c\u2500\u2500 model\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Message.java\n\u2514\u2500\u2500 stores\n    \u251c\u2500\u2500 MessageStore.java\n    \u2514\u2500\u2500 Store.java\n\n\u8fd9\u91cc\u5305\u542b4\u4e2a\u76ee\u5f55\u548c\u4e00\u4e2a\u6587\u4ef6\uff1a\n\nMainActivity.java Flux\u6846\u67b6\u4e2d\u7684Controller-View\u90e8\u5206\uff0c\u5728Android\u4e2d\u53ef\u4ee5\u662fActivity\u6216\u8005Fragment\nactions Flux\u6846\u67b6\u4e2d\u7684Action\u90e8\u5206\uff0c\u5b58\u653e\u4e0d\u540c\u7c7b\u578b\u7684XXXAction.java\u548cActionsCreator.java\u6587\u4ef6\ndispatcher Flux\u6846\u67b6\u4e2d\u7684Dispatcher\u90e8\u5206\uff0c\u5b58\u653e Dispatcher.java \u6587\u4ef6\uff0c\u4e00\u4e2a\u5e94\u7528\u4e2d\u53ea\u9700\u8981\u4e00\u4e2aDispatcher\nmodel \u5b58\u653e\u5404\u79cd\u4e1a\u52a1\u903b\u8f91\u76f8\u5173\u7684Model\u6587\u4ef6\nstores Flux\u6846\u67b6\u4e2d\u7684Stores\u90e8\u5206\uff0c\u5b58\u5728\u5404\u79cd\u7c7b\u578b\u7684 XXXStore.java \u6587\u4ef6\n\n\u521b\u5efa\u4e00\u4e2aDispatcher\n\u5728AndroidFlux\u4e2dDispatcher\u662f\u5c31\u662f\u4e00\u4e2a\u53d1\u5e03-\u8ba2\u9605\u6a21\u5f0f\u3002Store\u4f1a\u5728\u8fd9\u91cc\u6ce8\u518c\u81ea\u5df1\u7684\u56de\u8c03\u63a5\u53e3\uff0cDispatcher\u4f1a\u628aAction\u5206\u53d1\u5230\u6ce8\u518c\u7684Store\uff0c\u6240\u4ee5\u5b83\u4f1a\u63d0\u4f9b\u4e00\u4e9b\u516c\u6709\u65b9\u6cd5\u6765\u6ce8\u518c\u76d1\u542c\u548c\u5206\u53d1\u6d88\u606f\u3002\n\/**\n * Flux\u7684Dispatcher\u6a21\u5757\n * Created by ntop on 18\/12\/15.\n *\/\npublic class Dispatcher {\n    private static Dispatcher instance;\n    private final List<Store> stores = new ArrayList<>();\n\n    public static Dispatcher get() {\n        if (instance == null) {\n            instance = new Dispatcher();\n        }\n        return instance;\n    }\n\n    Dispatcher() {}\n\n    public void register(final Store store) {\n        stores.add(store);\n    }\n\n    public void unregister(final Store store) {\n        stores.remove(store);\n    }\n\n    public void dispatch(Action action) {\n        post(action);\n    }\n\n    private void post(final Action action) {\n        for (Store store : stores) {\n            store.onAction(action);\n        }\n    }\n}\n\nDispatcher\u5bf9\u5916\u4ec5\u66b4\u97323\u4e2a\u516c\u6709\u65b9\u6cd5\uff1a\n\nregister(final Store store) \u7528\u6765\u6ce8\u518c\u6bcf\u4e2aStore\u7684\u56de\u8c03\u63a5\u53e3\nunregister(final Store store) \u7528\u6765\u63a5\u89e6Store\u7684\u56de\u8c03\u63a5\u53e3\ndispatch(Action action) \u7528\u6765\u89e6\u53d1Store\u6ce8\u518c\u7684\u56de\u8c03\u63a5\u53e3\n\n\u8fd9\u91cc\u4ec5\u4ec5\u7528\u4e00\u4e2aArrayList\u6765\u7ba1\u7406Stores\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u66f4\u590d\u6742\u7684App\u53ef\u80fd\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u6570\u636e\u7ed3\u6784\u6765\u7ba1\u7406Stores\u7ec4\u7ec7\u548c\u76f8\u4e92\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\n\u521b\u5efaStores\n\u8fd9\u91cc\u4f7f\u7528EventBus\u6765\u5b9e\u73b0Store\uff0cEventBus\u7684\u4e3b\u8981\u529f\u80fd\u662f\u7528\u6765\u7ed9Controller-View\u53d1\u9001change\u4e8b\u4ef6\uff1a\n\/**\n * Flux\u7684Store\u6a21\u5757\n * Created by ntop on 18\/12\/15.\n *\/\npublic abstract class Store {\n    private  static final Bus bus = new Bus();\n\n    protected Store() {\n    }\n\n    public void register(final Object view) {\n        this.bus.register(view);\n    }\n\n    public void unregister(final Object view) {\n        this.bus.unregister(view);\n    }\n\n    void emitStoreChange() {\n        this.bus.post(changeEvent());\n    }\n\n    public abstract StoreChangeEvent changeEvent();\n    public abstract void onAction(Action action);\n\n    public class StoreChangeEvent {}\n}\n\n\u62bd\u8c61\u7684Store\u7c7b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e3b\u8981\u7684\u865a\u65b9\u6cd5 void onAction(Action action) \uff0c\u8fd9\u4e2a\u65b9\u6cd5\u662f\u6ce8\u518c\u5728Dispatcher\u91cc\u9762\u7684\u56de\u8c03\u63a5\u53e3\uff0c\u5f53Dispatcher\u6709\u6570\u636e\u6d3e\u53d1\u8fc7\u6765\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u5728\u8fd9\u91cc\u5904\u7406\u3002\n\u4e0b\u9762\u770b\u4e00\u4e0b\u66f4\u5177\u4f53\u7684\u548c\u4e1a\u52a1\u76f8\u5173\u7684MessageStore\u7c7b:\n\/**\n * MessageStore\u7c7b\u4e3b\u8981\u7528\u6765\u7ef4\u62a4MainActivity\u7684UI\u72b6\u6001\n * Created by ntop on 18\/12\/15.\n *\/\npublic class MessageStore extends Store {\n    private static MessageStore singleton;\n    private Message mMessage = new Message();\n\n    public MessageStore() {\n        super();\n    }\n\n    public String getMessage() {\n        return mMessage.getMessage();\n    }\n\n    @Override\n    @Subscribe\n    public void onAction(Action action) {\n        switch (action.getType()) {\n            case MessageAction.ACTION_NEW_MESSAGE:\n                mMessage.setMessage((String) action.getData());\n                break;\n            default:\n        }\n        emitStoreChange();\n    }\n\n\n    @Override\n    public StoreChangeEvent changeEvent() {\n        return new StoreChangeEvent();\n    }\n}\n\n\u5728\u8fd9\u91cc\u5b9e\u73b0\u4e86 onAction(Action action) \u65b9\u6cd5\uff0c\u5e76\u7528\u4e00\u4e2aswitch\u8bed\u53e5\u6765\u8def\u7531\u5404\u79cd\u4e0d\u540c\u7684Action\u7c7b\u578b\u3002\u540c\u65f6\u7ef4\u62a4\u4e86\u4e00\u4e2a\u7ed3\u6784 Message.java \u7c7b\uff0c\u8fd9\u4e2a\u7c7b\u7528\u6765\u8bb0\u5f55\u5f53\u524d\u8981\u663e\u793a\u7684\u6d88\u606f\u3002Store\u7c7b\u53ea\u80fd\u901a\u8fc7Dispatcher\u6765\u66f4\u65b0\uff08\u4e0d\u8981\u63d0\u4f9bsetter\u65b9\u6cd5\uff09\uff0c\u5bf9\u5916\u4ec5\u66b4\u9732\u5404\u79cdgetter\u65b9\u6cd5\u6765\u83b7\u53d6UI\u72b6\u6001\u3002\u8fd9\u91cc\u7528String getMessage()\u65b9\u6cd5\u6765\u83b7\u53d6\u5177\u4f53\u7684\u6d88\u606f\u3002\n\u5728Controller-View\u91cc\u9762\u5904\u7406\u201cchange\u201d\u4e8b\u4ef6\n\u5728Android\u4e2d\uff0cFlux\u7684Controller-View\u5bf9\u5e94\u4e8eActivity\u6216\u8005Fragment\uff0c\u6211\u4eec\u9700\u8981\u5728\u8fd9\u91cc\u6ce8\u518cStrore\u53d1\u751f\u6539\u53d8\u7684\u4e8b\u4ef6\u901a\u77e5\uff0c\u4ee5\u4fbf\u5728Store\u53d8\u5316\u7684\u65f6\u5019\u91cd\u65b0\u7ed8\u5236UI\u3002\n\/**\n * Flux\u7684Controller-View\u6a21\u5757\n * Created by ntop on 18\/12\/15.\n *\/\npublic class MainActivity extends AppCompatActivity implements View.OnClickListener {\n    private EditText vMessageEditor;\n    private Button vMessageButton;\n    private TextView vMessageView;\n\n    private Dispatcher dispatcher;\n    private ActionsCreator actionsCreator;\n    private MessageStore store;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        initDependencies();\n        setupView();\n    }\n\n    @Override\n    protected void onDestroy() {\n        super.onDestroy();\n        dispatcher.unregister(store);\n    }\n\n    private void initDependencies() {\n        dispatcher = Dispatcher.get();\n        actionsCreator = ActionsCreator.get(dispatcher);\n        store = new MessageStore();\n        dispatcher.register(store);\n    }\n\n    private void setupView() {\n        vMessageEditor = (EditText) findViewById(R.id.message_editor);\n        vMessageView = (TextView) findViewById(R.id.message_view);\n        vMessageButton = (Button) findViewById(R.id.message_button);\n        vMessageButton.setOnClickListener(this);\n    }\n\n    @Override\n    public void onClick(View view) {\n        int id = view.getId();\n        if (id == R.id.message_button) {\n            if (vMessageEditor.getText() != null) {\n                actionsCreator.sendMessage(vMessageEditor.getText().toString());\n                vMessageEditor.setText(null);\n            }\n        }\n    }\n\n    private void render(MessageStore store) {\n        vMessageView.setText(store.getMessage());\n    }\n\n    @Override\n    protected void onResume() {\n        super.onResume();\n        store.register(this);\n    }\n\n    @Override\n    protected void onPause() {\n        super.onPause();\n        store.unregister(this);\n    }\n\n    @Subscribe\n    public void onStoreChange(Store.StoreChangeEvent event) {\n        render(store);\n    }\n}\n\n\u8fd9\u90e8\u5206\u7684\u4ee3\u7801\u6bd4\u8f83\u591a\uff0c\u9996\u5148\u5728 onCreatre(...) \u65b9\u6cd5\u4e2d\u521d\u59cb\u5316\u4e86\u4f9d\u8d56\u548c\u9700\u8981\u7684UI\u7ec4\u4ef6\u3002\u6700\u91cd\u8981\u7684\u662f onStoreChange(...) \u65b9\u6cd5\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u662f\u6ce8\u518c\u5728Store\u4e2d\u56de\u8c03\uff08\u4f7f\u7528EventBus\u7684@Subscribe\u6ce8\u89e3\u6807\u8bc6\uff09\uff0c\u5f53Store\u53d1\u751f\u53d8\u5316\u7684\u65f6\u5019\u4f1a\u89e6\u53d1\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u8c03\u7528render()\u65b9\u6cd5\u91cd\u7ed8\u6574\u4e2a\u754c\u9762\u3002\n\u521b\u5efaAction\nAction\u662f\u7b80\u5355\u7684POJO\u7c7b\u578b\uff0c\u53ea\u63d0\u4f9b\u4e24\u4e2a\u5b57\u6bb5\uff1atype \u548c data, \u5206\u522b\u8bb0\u5f55Action\u7684\u7c7b\u578b\u548c\u6570\u636e\u3002\u6ce8\u610fAction\u4e00\u65e6\u521b\u5efa\u662f\u4e0d\u53ef\u66f4\u6539\u7684\uff0c\n\u6240\u4ee5\u5b83\u7684\u5b57\u6bb5\u7c7b\u578b\u4fee\u9970\u4e3afinal\u7c7b\u578b\u3002\npublic class Action<T> {\n    private final String type;\n    private final T data;\n\n    Action(String type, T data) {\n        this.type = type;\n        this.data = data;\n    }\n\n    public String getType() {\n        return type;\n    }\n\n    public T getData() {\n        return data;\n    }\n}\n\n\u4e0b\u9762\u662f\u4e00\u4e2a\u4e1a\u52a1\u76f8\u5173\u7684Action\u5b9e\u73b0\uff1a\npublic class MessageAction extends Action<String> {\n    public static final String ACTION_NEW_MESSAGE = \"new_message\";\n\n    MessageAction(String type, String data) {\n        super(type, data);\n    }\n}\n\n\u8fd9\u4e2a\u5b9e\u73b0\u975e\u5e38\u7b80\u5355\uff0c\u4ec5\u4ec5\u591a\u5b9a\u4e49\u4e86\u4e00\u4e2aAction\u7c7b\u578b\u5b57\u6bb5\uff1apublic static final String ACTION_NEW_MESSAGE = \"new_message\"\u3002\u5982\u4f60\u6240\u89c1\uff0cAction\u90fd\u662f\u8fd9\u4e48\u7b80\u5355\u7684\uff0c\u4e0d\u5305\u542b\u4efb\u4f55\u4e1a\u52a1\u903b\u8f91\u3002\n\u521b\u5efaActionCreator\nActionCreator \u662fFlux\u67b6\u6784\u4e2d\u7b2c\u201c\u56db\u201d\u4e2a\u6700\u91cd\u8981\u7684\u6a21\u5757\uff08\u524d\u4e09\uff1aDispatcher\u3001Store\u3001View\uff09\uff0c\u8fd9\u91cc\u5b9e\u9645\u4e0a\u5904\u7406\u5f88\u591a\u5de5\u4f5c\uff0c\u63d0\u4f9b\u6709\u4e00\u4e2a\u6709\u8bed\u4e49\u7684API\uff0c\u6784\u5efaAction\uff0c\u5904\u7406\u7f51\u7edc\u8bf7\u6c42\u7b49\u3002\n\/**\n * Flux\u7684ActionCreator\u6a21\u5757\n * Created by ntop on 18\/12\/15.\n *\/\npublic class ActionsCreator {\n\n    private static ActionsCreator instance;\n    final Dispatcher dispatcher;\n\n    ActionsCreator(Dispatcher dispatcher) {\n        this.dispatcher = dispatcher;\n    }\n\n    public static ActionsCreator get(Dispatcher dispatcher) {\n        if (instance == null) {\n            instance = new ActionsCreator(dispatcher);\n        }\n        return instance;\n    }\n\n    public void sendMessage(String message) {\n        dispatcher.dispatch(new MessageAction(MessageAction.ACTION_NEW_MESSAGE, message));\n    }\n}\n\n\u6b64\u5904\u63d0\u4f9b\u4e86\u4e00\u4e2a sendMessage(String message) \uff0c\u5c31\u50cf\u540d\u5b57\u6697\u793a\u7684\u90a3\u6837\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u7528\u6765\u53d1\u9001\u6d88\u606f\uff08\u5230Store\uff09\u3002\u5728\u65b9\u6cd5\u5185\u90e8\uff0c\u4f1a\u521b\u5efa\u4e00\u4e2aMessageAction\u6765\u5c01\u88c5\u6570\u636e\u548cAction\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7Dispatcher\u53d1\u9001\u5230Store\u3002\nModel\n\u65e0\u8bba\u662f\u57fa\u4e8e\u54ea\u79cd\u6846\u67b6\u7684\u5e94\u7528\u90fd\u9700\u8981Model\u6a21\u5757\uff0c\u5728\u8fd9\u4e2a\u7b80\u5355\u7684\u201cHelloWorld\u201d\u5e94\u7528\u4e2d\uff0c\u5176\u5b9e\u7528\u4e00\u4e2aString\u5373\u53ef\u4f20\u9012\u6d88\u606f\uff0c\u4f46\u662f\u4e3a\u4e86\u67b6\u6784\u7684\u5b8c\u6574\u548c\u66f4\u597d\u7684\u8bed\u4e49\u8868\u8fbe\uff0c\u5b9a\u4e49\u4e00\u4e2aMessage\u7c7b\u578b\u5c01\u88c5\u4e00\u4e2aString\u5b57\u6bb5\u4f5c\u4e3aModel\u3002\n\u5e0c\u671b\u901a\u8fc7\u8fd9\u4e2a\u7b80\u5355\u7684HelloWorld\u5e94\u7528\uff0c\u80fd\u591f\u8ba9\u4f60\u4e00\u7aa5Flux\u7684\u9762\u8c8c\u3002\u5982\u679c\u4f60\u60f3\u66f4\u6df1\u5165\u7684\u4e86\u89e3\u5728Android\u5e73\u53f0\u4e0a\u5e94\u7528Flux\u67b6\u6784\uff0c\u53ef\u4ee5\u67e5\u770b\u6211\u4eec\u7684Github\u7ad9\u70b9\u3002\n","268":"IceFig   \nJava elegant supplement\nJava 8 delivered lambda expressions, but without the enhancement of basic libraries like List, Map, String, which makes\nlambda expression still not delightful.\nInspired by other popular languages like Ruby and Scala, IceFig intends to supply the missing.\nQuick Scan\nElegant alternative to List: Seq\nSeq<Integer> seq = Seqs.newSeq(1,2,3);\nseq.shuffle(); \/\/ copy to a new seq and shuffle it\nseq.forEach((value, idx) -> { \/\/ with index\n    \/\/ (1, 0)  (2, 1)  (3, 2)\n});\nseq.forEachCons(2, (values)->{\n    \/\/ [1,2]  [2, 3]\n});\n\nseq.join(\"-\"); \/\/\"1-2-3\"\n\nseq.map(a -> a+ 1).distinct().reverse().join()\nElegant alternative to Map: Hash\nHash<Integer, Integer> hash = Hashes.<Integer, Integer>newHash().put(1, 2).put(2, 3).put(3, 3);\nhash.containsAny((k, v) -> k+v == 5 ); \/\/true\nhash.keysOf(3); \/\/ [2, 3]\nElegant alternative to String: CharSeq\nCharSeq str = CharSeq.of(\"a b c d e f g\");\nstr.split(\" \").join(\"-\").capitalize(); \/\/\"A-b-c-d-e-f-g\"\nstr.partition(\"d e\").map(CharSeq::trim);  \/\/[\"a b c\", \"d e\", \"f g\"]\nFull Javadoc\nInclude it\n<dependency>\n    <groupId>com.worksap<\/groupId>\n    <artifactId>icefig<\/artifactId>\n    <version>[latest version]<\/version>\n<\/dependency>\nConcept\nNot stream\nIceFig is different from Stream, and implemented without Stream. While, it is simpler concept -- supplement methods on basic libraries.\nStream has several characteristics:\n\nTrends to process each element independently\nInfinite that we can not get the size\nDesigned for large data flow performance\n\nThus, Stream may not be able to support operations related with the size of it, nor operations involving multiple or even random elements.\nBasically, the vast majority operations on List, Map, String don't need a Stream. Stream brings great merits on big data processing, but when we're not facing performance problem (operating a list of about 10x elements), it is an over kill.\nYet Stream brings the 2 additional steps \"Stream()\" and \"collect()\", which is sometimes annoying to write.\nIceFig targets on \"small data\" operations within application logic, to provide simple & beautiful code writing about String, List, Map operation & transformation.\nNo utilities\nIn traditional Java way, we use a lot of utilities (StringUtils, FileUtils) for the missing methods in standard library. While in IceFig, we make an object oriented and functional way to free you from tedious codes.\nZero runtime dependency\nIceFig has no external runtime dependency except JDK 8.\nMutable & default interfaces\nIceFig firstly aggregates all operations which do not change the state into a default interface(e.x. Seq, Hash).\nOn the other hand, there are interfaces named \"mutableXXX\" extending the default ones with additional in-place operations, which are commonly named xxxInPlace.\nIf you don't want the ability to change the object, you can use the default interface to let compiler check it for you. And it is the recommended way.\nNote that the default interface doesn't mean immutability of the object it is on, it only ensures \"if outside only uses this interface on the object, the object will not be changed\".\nConventions\nIceFig uses conventions on method names. If there is a pair of methods name, nameInPlace, method ends with InPlace means calling this method will change the object itself, while calling the other won't.\nLicense\nApache License 2.0\nContribution\nFeel free to submit issues & PRs\n","269":"IceFig   \nJava elegant supplement\nJava 8 delivered lambda expressions, but without the enhancement of basic libraries like List, Map, String, which makes\nlambda expression still not delightful.\nInspired by other popular languages like Ruby and Scala, IceFig intends to supply the missing.\nQuick Scan\nElegant alternative to List: Seq\nSeq<Integer> seq = Seqs.newSeq(1,2,3);\nseq.shuffle(); \/\/ copy to a new seq and shuffle it\nseq.forEach((value, idx) -> { \/\/ with index\n    \/\/ (1, 0)  (2, 1)  (3, 2)\n});\nseq.forEachCons(2, (values)->{\n    \/\/ [1,2]  [2, 3]\n});\n\nseq.join(\"-\"); \/\/\"1-2-3\"\n\nseq.map(a -> a+ 1).distinct().reverse().join()\nElegant alternative to Map: Hash\nHash<Integer, Integer> hash = Hashes.<Integer, Integer>newHash().put(1, 2).put(2, 3).put(3, 3);\nhash.containsAny((k, v) -> k+v == 5 ); \/\/true\nhash.keysOf(3); \/\/ [2, 3]\nElegant alternative to String: CharSeq\nCharSeq str = CharSeq.of(\"a b c d e f g\");\nstr.split(\" \").join(\"-\").capitalize(); \/\/\"A-b-c-d-e-f-g\"\nstr.partition(\"d e\").map(CharSeq::trim);  \/\/[\"a b c\", \"d e\", \"f g\"]\nFull Javadoc\nInclude it\n<dependency>\n    <groupId>com.worksap<\/groupId>\n    <artifactId>icefig<\/artifactId>\n    <version>[latest version]<\/version>\n<\/dependency>\nConcept\nNot stream\nIceFig is different from Stream, and implemented without Stream. While, it is simpler concept -- supplement methods on basic libraries.\nStream has several characteristics:\n\nTrends to process each element independently\nInfinite that we can not get the size\nDesigned for large data flow performance\n\nThus, Stream may not be able to support operations related with the size of it, nor operations involving multiple or even random elements.\nBasically, the vast majority operations on List, Map, String don't need a Stream. Stream brings great merits on big data processing, but when we're not facing performance problem (operating a list of about 10x elements), it is an over kill.\nYet Stream brings the 2 additional steps \"Stream()\" and \"collect()\", which is sometimes annoying to write.\nIceFig targets on \"small data\" operations within application logic, to provide simple & beautiful code writing about String, List, Map operation & transformation.\nNo utilities\nIn traditional Java way, we use a lot of utilities (StringUtils, FileUtils) for the missing methods in standard library. While in IceFig, we make an object oriented and functional way to free you from tedious codes.\nZero runtime dependency\nIceFig has no external runtime dependency except JDK 8.\nMutable & default interfaces\nIceFig firstly aggregates all operations which do not change the state into a default interface(e.x. Seq, Hash).\nOn the other hand, there are interfaces named \"mutableXXX\" extending the default ones with additional in-place operations, which are commonly named xxxInPlace.\nIf you don't want the ability to change the object, you can use the default interface to let compiler check it for you. And it is the recommended way.\nNote that the default interface doesn't mean immutability of the object it is on, it only ensures \"if outside only uses this interface on the object, the object will not be changed\".\nConventions\nIceFig uses conventions on method names. If there is a pair of methods name, nameInPlace, method ends with InPlace means calling this method will change the object itself, while calling the other won't.\nLicense\nApache License 2.0\nContribution\nFeel free to submit issues & PRs\n","270":"Model-based Deep Hand Pose Estimation\nThis repository is the released code of our IJCAI 2016 paper for estimating hand pose from depth image.\nContact: zhouxy13@fudan.edu.cn\nRequirements\n\nCaffe\nPython with opencv\n\nInstallation\n\nDownload caffe\nCopy .\/libs\/include to caffe_root\/include and .\/libs\/src to caffe_root\/src\nCompile caffe\nCopy path.config.example to path.config and set the pycaffe path\n\nTest\n\nRun demo.py in .\/testing\nOur prediction on NYU dataset here\nOur prediction on ICVL dataset here\n\nTrain\n\ndownload NYU dataset\nset NYU_path in path.config\nRun GetH5DataNYU.py in .\/training\nTrain with solver.prototxt\n\nCitation\nPlease cite DeepModel in your publication if it helps your research:\n@inproceedings{zhou2016model,\n    author = {Xingyi Zhou and Qingfu Wan and Wei Zhang and Xiangyang Xue and Yichen Wei},\n    booktitle = {IJCAI},\n    title = {Model-based Deep Hand Pose Estimation},\n    year = {2016}\n}\n\n","271":"Model-based Deep Hand Pose Estimation\nThis repository is the released code of our IJCAI 2016 paper for estimating hand pose from depth image.\nContact: zhouxy13@fudan.edu.cn\nRequirements\n\nCaffe\nPython with opencv\n\nInstallation\n\nDownload caffe\nCopy .\/libs\/include to caffe_root\/include and .\/libs\/src to caffe_root\/src\nCompile caffe\nCopy path.config.example to path.config and set the pycaffe path\n\nTest\n\nRun demo.py in .\/testing\nOur prediction on NYU dataset here\nOur prediction on ICVL dataset here\n\nTrain\n\ndownload NYU dataset\nset NYU_path in path.config\nRun GetH5DataNYU.py in .\/training\nTrain with solver.prototxt\n\nCitation\nPlease cite DeepModel in your publication if it helps your research:\n@inproceedings{zhou2016model,\n    author = {Xingyi Zhou and Qingfu Wan and Wei Zhang and Xiangyang Xue and Yichen Wei},\n    booktitle = {IJCAI},\n    title = {Model-based Deep Hand Pose Estimation},\n    year = {2016}\n}\n\n","272":"node-libdtrace\nOverview\nnode-libdtrace is a Node.js addon that interfaces to libdtrace, allowing\nnode programs to control DTrace enablings.\nStatus\nThe primary objective is not to create a dtrace(1M) alternative in node, but\nrather to allow node programs to create and control programmatically useful\nDTrace enablings.  That is, the goal is software-software interaction, and as\nsuch, DTrace actions related to controlling output (e.g., printf(),\nprinta()) are not supported.  Error handling is, for the moment, weak.\nPlatforms\nThis should work on any platform that supports DTrace, and is known to work on\nMac OS X (tested on 10.7.5) and illumos (tested on\nSmartOS).\nInstallation\nAs an addon, node-libdtrace is installed in the usual way:\n  % npm install libdtrace\n\nAPI\nnew libdtrace.Consumer()\nCreate a new libdtrace consumer, which will correspond to a new libdtrace\nstate.  If DTrace cannot be initalized for any reason, this will throw an\nexception with the message member set to the more detailed reason from\nlibdtrace.  Note that one particularly common failure mode is attempting to\ninitialize DTrace without the necessary level of privilege; in this case, for\nexample, the message member will be:\n  DTrace requires additional privileges\n\n(The specifics of this particular message should obviously not be\nprogrammatically depended upon.)  If encountering this error, you will\nneed to be a user that has DTrace privileges.\nconsumer.strcompile(str)\nCompile the specified str as a D program.  This is required before\nany call to consumer.go().\nconsumer.go()\nInstruments the system using the specified enabling.  Before consumer.go()\nis called, the specified D program has been compiled but not executed; once\nconsumer.go() is called, no further D compilation is possible.\nconsumer.setopt(option, value)\nSets the specified option (a string) to value (an integer, boolean,\nstring, or string representation of an integer or boolean, as denoted by\nthe option being set).\nconsumer.consume(function func (probe, rec) {})\nConsume any DTrace data traced to the principal buffer since the last call to\nconsumer.consume() (or the call to consumer.go() if consumer.consume()\nhas not been called).  For each trace record, func will be called and\npassed two arguments:\n\n\nprobe is an object that specifies the probe that corresponds to the\ntrace record in terms of the probe tuple: provider, module, function\nand name.\n\n\nrec is an object that has a single member, data, that corresponds to\nthe datum within the trace record.  If the trace record has been entirely\nconsumed, rec will be undefined.\n\n\nIn terms of implementation, a call to consumer.consume() will result in a\ncall to dtrace_status() and a principal buffer switch.  Note that if the\nrate of consumption exceeds the specified switchrate (set via either\n#pragma D option switchrate or consumer.setopt()), this will result in no\nnew data processing.\nconsumer.aggwalk(function func (varid, key, value) {})\nSnapshot and iterate over all aggregation data accumulated since the\nlast call to consumer.aggwalk() (or the call to consumer.go() if\nconsumer.aggwalk() has not been called).  For each aggregate record,\nfunc will be called and passed three arguments:\n\n\nvarid is the identifier of the aggregation variable.  These IDs are\nassigned in program order, starting with 1.\n\n\nkey is an array of keys that, taken with the variable identifier,\nuniquely specifies the aggregation record.\n\n\nvalue is the value of the aggregation record, the meaning of which\ndepends on the aggregating action:\n\n\nFor count(), sum(), max() and min(), the value is the\ninteger value of the aggregation action\n\n\nFor avg(), the value is the numeric value of the aggregating action\n\n\nFor quantize() and lquantize(), the value is an array of 2-tuples\ndenoting ranges and value:  each element consists of a two element array\ndenoting the range (minimum followed by maximum, inclusive) and the\nvalue for that range.\n\n\n\n\nUpon return from consumer.aggwalk(), the aggregation data for the specified\nvariable and key(s) is removed.\nNote that the rate of consumer.aggwalk() actually consumes the aggregation\nbuffer is clamed by the aggrate option; if consumer.aggwalk() is called\nmore frequently than the specified rate, consumer.aggwalk() will not\ninduce any additional data processing.\nconsumer.aggwalk() does not iterate over aggregation data in any guaranteed\norder, and may interleave aggregation variables and\/or keys.\nconsumer.version()\nReturns the version string, as returned from dtrace -V.\nExamples\nHello world\nThe obligatory \"hello world\":\n  var sys = require('sys');\n  var libdtrace = require('libdtrace');\n  var dtp = new libdtrace.Consumer();\n    \n  var prog = 'BEGIN { trace(\"hello world\"); }';\n    \n  dtp.strcompile(prog);\n  dtp.go();\n    \n  dtp.consume(function (probe, rec) {\n          if (rec)\n                  sys.puts(rec.data);\n  });\n\nUsing aggregations\nA slightly more sophisticated example showing system calls aggregated and\nsorted by executable name:\n  var sys = require('sys');\n  var libdtrace = require('libdtrace');\n  var dtp = new libdtrace.Consumer();\n  \n  var prog = 'syscall:::entry { @[execname] = count(); }'\n  \n  dtp.strcompile(prog);\n  dtp.go();\n  \n  var syscalls = {};\n  var keys = [];\n  \n  var pad = function (val, len)\n  {\n          var rval = '', i, str = val + '';\n  \n          for (i = 0; i < Math.abs(len) - str.length; i++)\n                  rval += ' ';\n  \n          rval = len < 0 ? str + rval : rval + str;\n  \n          return (rval);\n  };\n  \n  setInterval(function () {\n          var i;\n  \n          sys.puts(pad('EXECNAME', -40) + pad('COUNT', -10));\n  \n          dtp.aggwalk(function (id, key, val) {\n                  if (!syscalls.hasOwnProperty(key[0]))\n                  \tkeys.push(key[0]);\n  \n                  syscalls[key[0]] = val;\n          });\n  \n          keys.sort();\n  \n          for (i = 0; i < keys.length; i++) {\n                  sys.puts(pad(keys[i], -40) + pad(syscalls[keys[i]], -10));\n                  syscalls[keys[i]] = 0;\n          }\n  \n          sys.puts('');\n  }, 1000);\n\n","273":"<iframe class=\"roundPhoto\" src=\"http:\/\/player.vimeo.com\/video\/20904879?autoplay=1\" width=\"575\" height=\"359\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen allowFullScreen><\/iframe>\nKinectCoreVision is a refurnished version of the well know track application CommunityCoreVision to use a KinectSensor.\nThis version of CCV was original designed for a light and portable setup of Communitas multiTouch Table for IEATA\u00b4s 9th International Conference (more info and videos about it). But it was lot of potencial.\nBinaries\nMacOSX\n\nSnow Leopard\nLion\n\nMacports is required to be installed. Please check http:\/\/www.macports.org\/ for its own dependencies and installation procedure. It requires XCode and would need to be installed on your Mac. Then:\nsudo port install libtool\nsudo port install libusb-devel +universal\nLinux\n\nUbuntu 64bit\n\nsudo apt-get install libusb-1.0-0-dev\nWindows\n\nWindows XP\n\n","274":"<iframe class=\"roundPhoto\" src=\"http:\/\/player.vimeo.com\/video\/20904879?autoplay=1\" width=\"575\" height=\"359\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen allowFullScreen><\/iframe>\nKinectCoreVision is a refurnished version of the well know track application CommunityCoreVision to use a KinectSensor.\nThis version of CCV was original designed for a light and portable setup of Communitas multiTouch Table for IEATA\u00b4s 9th International Conference (more info and videos about it). But it was lot of potencial.\nBinaries\nMacOSX\n\nSnow Leopard\nLion\n\nMacports is required to be installed. Please check http:\/\/www.macports.org\/ for its own dependencies and installation procedure. It requires XCode and would need to be installed on your Mac. Then:\nsudo port install libtool\nsudo port install libusb-devel +universal\nLinux\n\nUbuntu 64bit\n\nsudo apt-get install libusb-1.0-0-dev\nWindows\n\nWindows XP\n\n","275":"OpenDetection\nOpenDetection is a standalone open source project for object detection and recognition in images and 3D point clouds.\nWebsite - http:\/\/opendetection.com or http:\/\/krips89.github.io\/opendetection_docs\nGit - https:\/\/github.com\/krips89\/opendetection\nGetting started - http:\/\/opendetection.com\/getting_started.html\nUser guide - http:\/\/opendetection.com\/introduction_general.html\nAPI Documentation - http:\/\/opendetection.com\/annotated.html\nShort demo video - https:\/\/www.youtube.com\/watch?v=sp8W0NspY54\n","276":"ofxLearn\nofxLearn is a general-purpose machine learning library for OpenFrameworks, built on top of dlib.\nAbout\nofxLearn supports classification, regression, and unsupervised clustering. The goal is to be a high-level wrapper for dlib's machine learning routines, taking care of the ugly stuff, i.e. determining a model, kernel, and parameter selection).\nThe library contains a basic example for each of classification, regression, and clustering. Because training can take a long time, there are also examples for placing each of these tasks into its own separate thread.\nFeatures\nofxLearn supports classification (using kernel ridge regression), regression (using kernel ridge or multilayer perceptron (neural network)), and k-means clustering.\nAlso has an example for doing a principal component analysis via singular value decomposition. See example_pca.\nEach has a separate class for threading (see the _threaded examples).\nUsage\nSee examples for usage of classification, regression, and clustering. Depending on the size and complexity of your data, training can take a long time, and it will freeze the application, unless you use the threaded learners. The examples ending with _threaded run the training in a separate thread and alert you with a callback function when they are done.\n","277":"ofxLearn\nofxLearn is a general-purpose machine learning library for OpenFrameworks, built on top of dlib.\nAbout\nofxLearn supports classification, regression, and unsupervised clustering. The goal is to be a high-level wrapper for dlib's machine learning routines, taking care of the ugly stuff, i.e. determining a model, kernel, and parameter selection).\nThe library contains a basic example for each of classification, regression, and clustering. Because training can take a long time, there are also examples for placing each of these tasks into its own separate thread.\nFeatures\nofxLearn supports classification (using kernel ridge regression), regression (using kernel ridge or multilayer perceptron (neural network)), and k-means clustering.\nAlso has an example for doing a principal component analysis via singular value decomposition. See example_pca.\nEach has a separate class for threading (see the _threaded examples).\nUsage\nSee examples for usage of classification, regression, and clustering. Depending on the size and complexity of your data, training can take a long time, and it will freeze the application, unless you use the threaded learners. The examples ending with _threaded run the training in a separate thread and alert you with a callback function when they are done.\n","278":"mace-makefile-project\nMace\u662f\u5c0f\u7c73\u53d1\u5e03\u7684\u79fb\u52a8\u7aef\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5e93\u3002\u4f46\u662f\u73b0\u5728\u4e0d\u652f\u6301\u79bb\u7ebf\u7f16\u8bd1\u548carm-linux\u4ea4\u53c9\u7f16\u8bd1\uff0c\u4e3a\u4e86\u5feb\u901f\u9a8c\u8bc1Mace\u5728\u5d4c\u5165\u5f0f\u7aef\u7684\u6027\u80fd\uff0c\u6545\u628aMACE\u7684\u6e90\u7801\u548c\u4f9d\u8d56\u63d0\u51fa\u6765\uff0c\u641e\u4e86\u4e00\u4e2a\u53ef\u4ee5\u4f7f\u7528MakeFile\u8fdb\u884c\u4ea4\u53c9\u7f16\u8bd1libmace.a\u7684\u5de5\u7a0b\uff0c\u7701\u53bb\u4e86Bazel\u7f16\u8bd1\u7684\u9ebb\u70e6\u3002 \u672c\u5de5\u7a0b\u4e3b\u8981\u9488\u5bf93559A\uff0c3536\uff0crk3288\uff08\u9884\u88c5ubuntu\u7cfb\u7edf\uff09\uff0crk3399\uff08\u9884\u88c5ubuntu\u7cfb\u7edf\uff09\u8fd9\u7c7b\u5e26GPU\u7684\u5d4c\u5165\u5f0fSOC\u3002\n1.\u5982\u679c\u4f60\u8981\u57283559A\u4e0a\u4f7f\u7528CPU\u6765\u8fd0\u884cmace\u6846\u67b6\uff0c\u90a3\u4e48\u8bf7\u4f9d\u6b21\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4(3559a\u5e73\u53f0\u4e0a\u7684protobuf\u7684\u5e93\u5df2\u7ecf\u7f16\u597d\u4e86)\ncd mace;\nvim Makefike\n\u5c06PLATFORM\u6539\u4e3aCPU\nmake clean;\nmake;\ncp libmace.a ..\/library\/mace_cpu\ncd unit_test_cpu\nmake clean;make\n.\/demo\n\u5907\u6ce8\uff1a\u5982\u4f55\u4f60\u7684\u4ea4\u53c9\u7f16\u8bd1\u94fe\u4e0d\u662faarch64-himix100-linux-,\u8bf7\u5728mace\u6587\u4ef6\u5939\u548cunit_test\u6587\u4ef6\u5939\u7684\u7684MAKEFILE\u6587\u4ef6\u4e2d\u4fee\u6539\u7f16\u8bd1\u94fe\u8def\u5f84\u3002\n2.\u5982\u679c\u4f60\u8981\u57283559A\u4e0a\u4f7f\u7528GPU\u6765\u8fd0\u884cmace\u6846\u67b6\uff0c\u90a3\u4e48\u8bf7\u4f9d\u6b21\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a\ncd mace;\nvim Makefile;\n\u5c06PLATFORM\u6539\u4e3aGPU\nmake clean;make\ncp libmace.a ..\/library\/mace_gpu\ncd unit_test_gpu\nmake clean;make\n.\/demo\n2.\u5982\u679c\u4f60\u8981\u5728\u5176\u4ed6\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u8fd0\u884c\uff0c\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u7f16\u8bd1\n(1).\u4f7f\u7528\u5bf9\u5e94\u7684\u4ea4\u53c9\u7f16\u8bd1\u94fe\u5148\u7f16\u8bd1\u597dprotobuf3.4.0,https:\/\/cnbj1.fds.api.xiaomi.com\/mace\/third-party\/protobuf\/protobuf-3.4.0.zip\n(2).https:\/\/blog.csdn.net\/adrian169\/article\/details\/9051839\n\u53ef\u4ee5\u6309\u7167\u8fd9\u4e2a\u94fe\u63a5\u65b9\u6cd5\u6765\u7f16\u8bd1\n(3).\u628a\u7f16\u8bd1\u597d\u7684libprotoc.a\u548clibprotobuf.a\u548clibproto-lite.a\u62f7\u8d1d\u5230library\/mace\/\u76ee\u5f55\u4e0b\n(4).\u5982\u679c\u662f\u4f7f\u7528CPU\u6765\u8fd0\u884cmace\uff0c\u63a5\u4e0b\u6765\u6309\u71671\u65b9\u6cd5\u8fd0\u884c(\u6ce8\u610f\u5982\u679c\u662farmv7a\u5e73\u53f0\uff0c\u8bf7\u5728Makefile\u4e2d\u52a0\u4e0a\u7f16\u8bd1\u9009\u9879-mfloat-abi=softfp -mfpu=neon\uff0c\u786c\u6d6e\u70b9\u7684\u8bdd-     mfloat-abi=hard)\n(5).\u5982\u679c\u662f\u4f7f\u7528GPU\u6765\u8fd0\u884cmace\uff08RK3399\u9700\u8981\u4fee\u6539mace\/Makefile\u4e2d\u7f16\u8bd1\u9009\u9879-std=c++11\u6539\u4e3a-std=c++14\uff09\uff0c\u53c2\u80032\uff0c\u53ea\u662f\u5728\u8fd0\u884cdemo\u524d\uff0c\u9700\u8981\u5148\u628aopencl\u7684\u5e93libopencl.so\uff0clibmali.so\u653e\u5230opencl_library\u4e0b\u3002\n\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5d4c\u5165\u5f0f\u7aef\u8fd0\u884cGPU\u9700\u8981\u4fdd\u8bc1GPU\u7684\u9a71\u52a8\u52a0\u8f7d\u597d\u4e86\uff0c\u4e0d\u7136\u8fd0\u884cdemo\u7684\u65f6\u5019\u4f1a\u201cKernel module may not have been loaded\u201d\u7684\u9519\u8bef\u3002\n\u5982\u679cGPU\u8fd0\u884c\u65f6\u9664\u4e86libopencl.so\uff0clibmali.so\u4e4b\u5916\u8fd8\u6709\u5176\u4ed6\u52a8\u6001\u5e93\uff0c\u8bf7\u5728mace\/core\/runtime\/opencl\/opencl_wrapper.cc\u7684278\u884c\u52a0\u4e0a\u5e93\u8def\u5f84\u5373\u53ef\u3002\n3.\u901a\u8fc7unit_test.cpp\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u672c\u5de5\u7a0b\u73b0\u5728\u53ea\u63d0\u4f9b\u4e86mobilenetv1\u7684\u6d4b\u8bd5\u3002\u8fd0\u884cmace\u9700\u8981\u7684\u6743\u91cd\u6587\u4ef6\u548c\u7f51\u7edc\u914d\u7f6e\u6587\u4ef6\u5206\u522b\u4e3amobilenetv1_v1.data\u548cmobilenetv1.proto\u3002\u7279\u522b\u9700\u8981\u6ce8\u610f\u7684\u662f\u90e8\u7f72\u5728gpu\u4e0a\u7684model\u548c\u90e8\u7f72\u5230cpu\u4e0a\u7684model\u662f\u4e0d\u80fd\u6df7\u7528\u7684\u3002\u4e3b\u8981\u539f\u56e0\u662f\u56e0\u4e3agpu\u6a21\u578b\u8fd0\u7b97\u662ff16\uff0ccpu\u662ff32\u3002\n4.\u5982\u679c\u4f60\u9700\u8981\u6d4b\u8bd5\u5176\u4ed6\u7f51\u7edc\uff0c\u9700\u8981\u628acaffe\u6216\u8005tensorflow\u6a21\u578b\u8f6c\u6362\u6210*.data\u548c*.pb\u6587\u4ef6\uff0c\u5177\u4f53\u600e\u4e48\u505a\u8bf7\u53c2    \u8003https:\/\/mace.readthedocs.io\/en\/latest\/getting_started\/how_to_build.html\n\u6a21\u578b\u8f6c\u6362ok\u4e4b\u540e\uff0c\u8bf7\u4fee\u6539unit_test.cpp\u4e2d\u5bf9\u5e94\u4ee3\u7801\u3002\n5.https:\/\/github.com\/conansherry\/convert_model\n\u8fd9\u4e2a\u94fe\u63a5\u53ef\u4ee5\u8f6ccaffe\u6a21\u578b\uff0c\u4e0d\u9700\u8981docker\u90a3\u4e00\u5957\u4e1c\u897f\n","279":"nyroModal jQuery Plugin\nTested in all majors browsers:\n\nFirefox\nChrome\nSafari\nInternet Explorer\n\n##License\nnyroModal is licensed under the MIT License.\n##How to use it\nGo to http:\/\/nyromodal.nyrodev.com\/ and download your package.\nYou need to include jQuery on your page.\nThen you can include jquery.nyromodal.custom.js on your page.\nInclude styles\/nyroModal.css. You will also need the images stored in the img folder.\nTo add compabitibilty with IE6, you have to add the following :\n<!--[if IE 6]>\n  <script type=\"text\/javascript\" src=\"js\/jquery.nyroModal-ie6.js\"><\/script>\n<![endif]-->\n\nThen, add the nyroModal functionnality to your links:\n$('YOUR SELECTOR').nyroModal();\n\nThis will work on many kind of elements, depending of what you included as nyroModal filters (see below)\n##Functions available\nnyroModal comes with a lot of functions that tou could use through your programming.\n###$.fn.nyroModal(opts, fullObj);\nThis is obviously the most useful. It enables the nyroModal functionnality on the element when clicked or submitted for a form.\nYou have to call it to add the bind. The most usual place might be on a ready page event like:\n$(function() {\n  $('.nyroModal').nyroModal();\n});\n\nParameters:\n\nopts: the options parameter might be used in order to change the setting for the specified nyroModal objects\nfullObj: a boolean to indicate if the options parameter is actually a fully nyroModal object\n\n###$.fn.nm(opts, fullObj);\nThis is simply a shortcut for the nyroModal function, they both do exactly the same.\nFor more information, see description above.\n###$.fn.nmInit(opts, fullObj);\nThis function is used in order to initliaze the nyroModal object as data of DOM node.\nThis function will result by create a data('nmObj') on the jQuery object representing the DOM node.\nThis function is used internally, it shouldn't be use in most of case.\nUsage example:\n$(function() {\n  $('.nyroModal').nmInit();\n});\n\nParameters:\n\nopts: the options parameter might be used in order to change the setting for the specified nyroModal objects\nfullObj: a boolean to indicate if the options parameter is actually a fully nyroModal object\n\n###$.fn.nmCall();\nThis function is used to open the nyroModal on a DOM element that already have been initalised with the nyroModal or nm functions.\nUsage example:\n$('#myLink').nmCall();\n\nNote: This code has exaclty the same effect than triggering a nyroModal event like:\n$('#myLink').trigger('nyroModal');\n\n###$.nmManual(url, opts);\nThis function is used to manually open a nyroModal by giving only an url.\nTo use it, the Link Filter is required.\nExample usage:\n$.nmManual('page.html');\n\nParameters:\n\nurl: The url to open. it might a DOM selector or anything else\nopts: the options parameter might be used in order to change the setting for the specified nyroModal objects\n\n###$.nmData(data, opts);\nThis function is used to manually open a nyroModal by giving it's content.\nTo use it, the Link Filter and the Data Filter are required.\nExample usage:\n$.nmData('my content<br \/>is so greaaaaaaaaaaaat !');\n\nParameters:\n\ndata: The data to be shown inside the modal\nopts: the options parameter might be used in order to change the setting for the specified nyroModal objects\n\n###$.nmObj(opts);\nThis is used to overwrite the default function or settings of the nyroModal object created when initialising a nyroModal Object.\nThis could be used to change the default animations to use or change the default behavior of the nyroModal.\nExemple usage:\n$(function() {\n  $.nmObj({anim: {def: 'fade'}});\n});\n\n###$.nmInternal(opts);\nThis is used to overwrite internal object of nyroModal used in many places in the code.\nThis could be used to change the default behavior of nyroModal.\nExemple usage in jquery.nyroModal-ie6 in order to calculate the size according to this browser:\n$.nmInternal({\n  _calculateFullSize: function() {\n\tvar scrollHeight = Math.max(\n\t  document.documentElement.scrollHeight,\n\t  document.body.scrollHeight\n\t),\n\toffsetHeight = Math.max(\n\t  document.documentElement.offsetHeight,\n\t  document.body.offsetHeight\n\t),\n\tscrollWidth = Math.max(\n\t  document.documentElement.scrollWidth,\n\t  document.body.scrollWidth\n\t),\n\toffsetWidth = Math.max(\n\t  document.documentElement.offsetWidth,\n\t  document.body.offsetWidth\n\t);\n\tthis.fullSize = {\n\t  wW: $w.width(),\n\t  wH: $w.height()\n\t};\n\tthis.fullSize.h = scrollHeight < offsetHeight ? $w.height() : scrollHeight;\n\tthis.fullSize.w = scrollWidth < offsetWidth ? $w.width() : scrollWidth;\n\tthis.fullSize.viewW = Math.min(this.fullSize.w, this.fullSize.wW);\n\tthis.fullSize.viewH = Math.min(this.fullSize.h, this.fullSize.wH);\n  }\n});\n\n###$.nmAnims(opts);\nThis is used to add or overwrite animations pack.\nSee js\/jquery.nyroModal.anims.fade.js to see how the code looks like.\nSee animations section below to learn how it should be written an used.\n###$.nmFilters(opts);\nThis is used to add or overwrite filters pack.\nThis might be the most useful part of nyroModal as it allow devleopper to create a totally new behavior on it's own, without breaking all others.\nSee filters section below to learn how it should be written and used.\n###$.nmTop();\nAs nyroModal allow multiple nyroModal to be opened at the same time, this function return the nyroModal Object of the last opened modal, ie the modal at the top.\nIf nothing is open, undefined will be returned.\n##Selectors\nnyroModal defines 3 selectors that could be used transparently in your code like any others jQuery selectors.\n###:nyroModal\nThis selector retrieves all elements that was binded using the nyroModal() function.\nFor instance, if you want to open all modal at the same time (not recommended), you can do:\n$(':nyroModal').nmCall();\n\n###:nm\nThis is an alias for the previous selector, :nyroModal.\nthey both do exactly the same.\n###:nmOpen\nThis selector retrieves all modals that are curently opened.\nThis could be useful to close them all at the same time:\n$(':nmOpen').each(function() {\n  $(this).data('nmObj').close();\n}):\n\nnyroModal Object\nDescription of what is, how and when to use it.\nList of all of settings and functions\n##Filters\nFilters are used to provide many basics functionnality like Ajax download, but it alos let you the ability to fully customize the way nyroModal work, by adding as many callbacks as you need.\nA filter is a set function that will be called by nyroModal core at many differents points. These functions call be used to do everything needed to make the modal work as you excpet or to add others behaviors needed for your projet: add some elements inside the modal, updating your page content, add some Ajax call, etc...\nHere is how filters works:\n\n\nWhen $('element').nyroModal(); is called, the nyroModal object is created and attached to the DOM element instancied\n\n\nEvery available filters are tested again the nyroModal object using the is function\n\nIf this function returns true, the filters will be used all the time for this element.\nIf it returns false, the filters will not be used anymore for this element.\n\n\n\nnyroModal core calls every init function of every filters selected for the element\n\nThese init function should bind the event they need, create their own object if needed.\nIf the filters need to be the one used to load the content, the loadFilter setting of the nyroModal ovject should be defined to its name.\n\n\n\nThrougout the living of nyroModal, every function or callback defined in all selected filters will be called (see the list below)\n\n\nAll function or callbacks receive the same nyroModal object as a unique parameter.\nThe list of all function or callback that can be called in a filter:\n\nis: should return true or false to select the filter for the current or element or not. This function is REQUIRED !\ninitFilters: called just after every filters have been determined to use or not, and just before the init of them. Good place to force filters.\ninit: called at the very beggining of the process. This function should bind element or create object needed later\ninitElts: called at the beggining of the open process, juste before the load start. After that, all the needed div are created and attached to the DOM\nload: called only for ONE filter defined in nm.loadFilter attribute. This function should load the function and set it's content using the _setCont function\nfilledContent: called once the content is placed on the hidden div\nerror: called in case of error (URL not founr for example)\nsize: called after the size has been defined\nclose: called at the end of the closing process, regarding only the data (useful for gallery)\nbeforeClose: called at the beggining of the closing process, regarding the animation\nafterClose: called at the end of the closing process, regarding the animation\nkeyHandle: called when a key is hit if the keyHandle is enable and if the modal is on the top\n\nLike the version 1, there is a bunch of others callback that you can define before and after every animation. The version 2 put it in a new way by letting you the ability to define a function before and after every animation function.\n\nbeforeShowBg: called just before the showBg animation\nafterShowBg: called just after the showBg animation\nbeforeHideBg: called just before the hideBg animation\nafterHideBg: called just after the hideBg animation\nbeforeShowLoad: called just before the showLoad animation\nafterShowLoad: called just after the showLoad animation\nbeforeHideLoad: called just before the hideLoad animation\nafterHideLoad: called just after the hideLoad animation\nbeforeShowCont: called just before the showCont animation (also called in case of a transition, before beforeHideTrans)\nafterShowCont: called just after the showCont animation (also called in case of a transition, after afterHideTrans)\nafterReposition: called just after the .nmRepositon have been placed\nafterUnreposition: called just after the .nmRepositon have been replaced on their initial positions\nbeforeHideCont: called just before the hideCont animation\nafterHideCont: called just after the hideCont animation\nbeforeShowTrans: called just before the showTrans animation (transition)\nafterShowTrans: called just after the showTrans animation (transition)\nbeforeHideTrans: called just before the hideTrans animation (transition)\nafterHideTrans: called just after the hideTrans animation (transition)\nbeforeResize: called just before the resize animation\nafterresize: called just after the resize animation\n\nnyroModal automatically add 2 filters for every elements:\n\nbasic: used to provide basic functionnality. This filter shouldn't be modified unless you know exactly what you're doing\ncustom: This filter has absolutely no programming. It's added here juste to provide a quick way to add your filters callback\n\nThere is some some basic filters that you can use on your project that enable the basic usage of this kind of plugin.\nHere is a list of the filters officially provided:\n\nTitle: Show the title for the modal using the title attribute of the opener element\nGallery: Enable arrows and navigation keys trough the element with the same rel attribute defined in the DOM (depends on title)\nLink: Bind an anchor element and load the target using Ajax to retrieve content\nDom: Instead of using Ajax, it get the content within the page using an id. (exemple link : ) (depends on link)\nImage: Show an image, and resize it if needed (depends on link)\nSWF: Show a SWf animation (depends on link)\nForm: Bind a form and load the target by sending it's data trhough Ajax\nForm file: Same as Form but used when form should send file\nIframe: Used to show data from an other domain name or when target=\"_blank\" is present (depends on link)\nIframe form: Used to open a form in an iframe (depends iframe)\nEmbedly: Used to show a bunch of different element using embedly API (Examples) (depends on link) Embedly requires developers to sign up for an API key at embedly Pricing\n\n##Animations\n##Debug\nIn order to enable the debug mode, you should change the debug value of the internal object. You can do it like:\n$.nmInternal({debug: true});\n\nBy default, the debug function show function name in the console. If you want do something else, you should overwrite the _debug function like:\n$.nmInternal({\n  _debug: function(msg) {\n\tif (this.debug && window.console && window.console.log)\n\t  window.console.log(msg);\n  }\n});\n\n##Help\nYou can find some useful tips on the issues labeled with tips\nIf it's still not enought, be free to open an Issue on github.\n","280":"\nOther Options\nMaxArt2501 has started his own Object.observe polyfill, and a look at his commit history and reasoning makes me think it will probably be well supported.  If your looking for an Object.observe solution you should take a look at https:\/\/github.com\/MaxArt2501\/object-observe\nNeeds a maintainer\nI'd like to find someone who is willing to take this library over.  I've had no time to work with and\/or maintain the library.  Honestly I have little need for Object.observe making it hard to justify time spent against it.\nSo, if your using this shim, feel comfortable with the code, and would like to maintain it, let me know.\nIf your curious why I don't have much use for the library, this http:\/\/markdalgleish.github.io\/presentation-a-state-of-change-object-observe\/ pretty well sums it up.  I worked in native development when two way data binding caused all sorts of issues with application development, and I see the same coming out of Object.observe at the end of the day.\nHopefully, someone will want to maintain this work in the future.\nOh, and if you take it over, feel free to relicense it within reason as it seems no one likes unlicense :).  Also feel free to follow up on the polyfill-service integration if you so see fit (https:\/\/github.com\/Financial-Times\/polyfill-service\/pull\/81#issuecomment-66382432).\nObject.observe Polyfill\/shim\nThanks to my new job I have a lot more time to devote to things like this library.  It has gone a REALLY long time without updates and there is a lot that can be done to make it more functional.  I hope to be spending more time on it soon, but for now I've fixed all the bugs that I know of and have been reported.  Thanks to everyone for their reports, and keep them coming if you find one.\nStarted as GIST: https:\/\/gist.github.com\/4173299\nTested against Chromium build with Object.observe and acts EXACTLY the same for the basics, though Chromium build is MUCH faster.\nTrying to stay as close to the spec as possible, this is a work in progress, feel free to comment\/update\nhttp:\/\/wiki.ecmascript.org\/doku.php?id=harmony:observe\nTODO\nThe spec has changed a lot since I origionally wrote this, need to go back and add in a lot of things like custom update types and other fun.  For now though it seems to suffice.\nLimits so far\nBuilt using polling in combination with getter&setters.  This means that if you do something quick enough it won't get caught.\nExample:\nvar myObject = {};\nObject.observe(myObject, console.log);\nmyObject.foo = \"bar\";\ndelete myObject.foo;\nThe observer function would never see the addition of foo since it was deleted so quickly.\nTesting\nTo run the tests first make sure you have Node.js installed.  Then use NPM to install Karam and all dependencies:\nnpm install\n\nFinally run the tests with:\nnpm test\n\nPlanned Updates\nFor FireFox using Proxies will result in better performance.  Will look into this more.\nExamples and Usage\nSee examples folder\nLatest Updates\n* Memory leak fixed with PR #16\n* Tests added to project thanks to mikeb1rd also part of PR #16\n* Added Notifier.notify() with custom types support by klimlee\n* Added accept list support by klimlee\n* Stopped monitoring DOM nodes, Canary can't do it and neither should the shim.\n* Added in support for setImmediate if it is available.\n* Memory leak fix by Moshemal\n* Array length now reports as an update when it changes\n* Added enumerable flag to defineProperty\n\n","281":"TransformJS 1.0 Beta\n2D and 3D transforms as regular CSS properties you can set using .css() and animate using .animate()\nOverview\nCSS Transforms  were first introduced in WebKit in 2007, and have now\nreached mass-adoption by all the major browser vendors. This is great news\nfor web developers, especially in the case of 3D transforms which are\nhardware-accelerated, resulting in extremely smooth animations and\nresponsive applications.\nThe API for applying transforms however, does not scale to complex applications\nwhich require intricate and complex management of transformations. TransformJS\nattempts to identify and address these problems, allowing developers to\nmake use of transforms without having to be encumbered by cross browser\nissues, and low-level APIs.\nHere's a snippet of code that uses TransformJS to apply multiple 3d\ntransformations to the same element, relative to their current value,\nand animate the changes:\n    $('#test').animate({\n      translateY:'+=150',\n      scale:'+=2',\n      rotateY: '+=6.24',\n      rotateX: '+=3.15',\n      rotateZ: '+=3.15'\n    },1500);    \nFor more detailed usage and overview information, please visit the\nproject homepage at http:\/\/transformjs.strobeapp.com\nLicense\n  \n    Copyright (c) 2011 Strobe Inc.\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n\nBack to top\n","282":"DEPRECATED, I recommend you the tool SVGR\n\nreact-svg-inline\n\n\nA react component to clean and display raw SVGs.\n\nInstall\n$ npm install react-svg-inline\nYou might also need to npm install raw-loader if you want to use this with\nwebpack.\nUsage\nHere is an example of a usage in a React stateless component:\nimport React from \"react\"\nimport SVGInline from \"react-svg-inline\"\n\nexport default () => (\n  <div>\n    <SVGInline svg={\"<svg....<\/svg>\"} \/>\n  <\/div>\n)\nWebpack to require() SVGs\nUse the raw-loader to require() raw SVGs files and pass them to\nreact-svg-inline.\nmodule.exports = {\n  loaders: [\n    {\n      test: \/\\.svg$\/,\n      loader: 'raw-loader'\n    }\n  ]\n}\nimport React from \"react\"\nimport SVGInline from \"react-svg-inline\"\nimport iconSVG from \".\/myicon.svg\"\n\nexport default () => (\n  <div>\n    <SVGInline svg={ iconSVG } \/>\n  <\/div>\n)\nOptions (props)\nclassName\nPropTypes.string\nClass name used for the component that will wrap the SVG.\nclassSuffix\nPropTypes.string\nThe class suffix that will be added to the svg className (default: \"-svg\").\ncomponent\nPropTypes.oneOfType([ PropTypes.string, PropTypes.func, ]),\nThe component that will wrap the svg (default: span).\nsvg\nPropTypes.string.isRequired\nfill\nPropTypes.string\nColor to use\ncleanup\nPropTypes.oneOfType([ PropTypes.bool, PropTypes.array, ])\nThis allow you to cleanup (remove) some svg attributes.\nHere are the supported value that can be removed:\n\ntitle\ndesc\ncomment\ndefs\nwidth\nheight\nfill\nsketchMSShapeGroup\nsketchMSPage\nsketchMSLayerGroup\n\nIf cleanup === true, it will remove all the attributes above.\ncleanupExceptions\nPropTypes.array\nThis allow you to whitelist some svg attributes to keep while cleaning some\nothers.\nwidth\nPropTypes.string\nheight\nPropTypes.string\naccessibilityLabel\nPropTypes.string\nThis value is added as an svg <title> element that is accessible to screen readers.\n(Note: when this option is used, an SVG id attribute will be automatically injected).\naccessibilityDesc\nPropTypes.string\nThis value is added as an svg <desc> element that is accessible to screen readers.\n\nCONTRIBUTING\n\n\u21c4 Pull requests and \u2605 Stars are always welcome.\nFor bugs and feature requests, please create an issue.\nPull requests must be accompanied by passing automated tests ($ npm test).\n\nCHANGELOG\nLICENSE\n","283":"Intelligist\na jQuery plugin that makes it easy to share multiple, executable GitHub gists\nDemo\nSee Intelligist in action\nRequirements\njQuery\nChosen (optional, it is auto-downloaded if not already included on the page)\nHow to use\nUsing Intelligist is easy. The general idea is:\n$(selector).intelligist( gists, options );\n\nJust select the container, and pass in an object of gists that you'd like to display.\n$(\"#demo\").intelligist({\n      \"1973984\": \"Welcome to Intelligist\"\n    , \"1973990\": \"Live CSS preview\"\n    , \"1973575\": \"Live JS preview\"\n}, { exec: true });\n\nThe object is made of keys (the gist ID) and values (titles for the drop-down menu).\nNote: In this example, we are setting the \"exec\" option to true. This instructs Intelligist to execute the code after the gist is displayed. It is optional, and disabled by default.\nImportant: If you are using the \"exec\" option, your Gist must be set to the correct language. Intelligist uses the Gist language to determine how it should execute the code.\nOptions\nexec\nShould the gist be executed after it is loaded onto the page? Note: If you are using this option, your Gist must be set to the correct language (e.g. CSS or JavaScript) (default=false)\nbefore\nA function to be called before the gist is loaded onto the page. It is passed two variables, the ID of the previous gist and the ID of the newly selected gist. e.g. function(oldGistId, newGistId) {}\nafter\nA function to be called after the gist is loaded onto the page. It is passed one variable, the ID of the newly selected gist. e.g. function(newGistId) {}\nThank yous\n\nGitHub: for creating the Gists service\nHarvest: for creating Chosen\nMartin Angelov: whose Shuffle Letters plugin is being used in the demo\n\n","284":"AlphaBeta\nAlphaBeta lets you build split tests (A\/B tests) directly into\nyour React app.\nAlphaBeta is...\n\ndeclarative: Like React itself, AlphaBeta benefits from the advantages of declarative programming. The AlphaBeta component is, in fact, just a special type of React component that \"wraps\" the component variants you're testing.\nlightweight: The AlphaBeta package is small and the AlphaBeta component is thin, so AlphaBeta tests won't measurably increase the time it takes for your application to render. Since AlphaBeta is so lightweight, you can be less selective about what you choose to test - it may even make sense to run single-variant tests, wrapping components you may test in the future in an AlphaBeta component today in order to establish a baseline for comparison.\nbackend agnostic: Since split testing requires that you store event data, AlphaBeta needs to communicate with a datastore in order to work. But AlphaBeta will work with whatever datastore you're currently using - just follow the instructions in Backend \/ API Setup to build your endpoint, point AlphaBeta to it, and you're good to go.\nextensible: AlphaBeta is designed to make it easy for developers to integrate basic split tests into their React apps without having to think about the underlying statistics. But it's also possible to build your own custom logic around how confidence intervals are calculated and how user cohorting works within your app.\n\nBuilding your first A\/B test is simple:\nimport { ABComponent } from 'react-alpha-beta';\n\nclass ButtonA extends React.Component {\n  render() {\n    return (<Button onClick={this.props.successAction}\n                    style={{'background-color':'blue'}}>\n              \"Sign Up\"\n            <\/Button>);\n  }\n}\n\nclass ButtonB extends React.Component {\n  render() {\n    return (<Button onClick={this.props.successAction}\n                    style={{'background-color':'orange'}}>\n              \"Sign Up\"\n            <\/Button>);\n  }\n}\n\nclass Page extends React.Component {\n    render() {\n        return (\n            <div>\n                <ABComponent\n                    ComponentA={ButtonA}\n                    ComponentB={ButtonB}\n                    experimentParams={{\n                      id: '1',\n                      testCohortSize: 0.4,\n                    }}\n                \/>\n            <\/div>\n        );\n    }\n}\n\nReactDOM.render(\n    <Page \/>,\n    document.getElementById('app')\n);\nYour experiment results will look something like this:\n{\n  meanDifferenceValue: -0.05023923444976075,\n  marginOfError: 0.04837299277280508,\n  statisticalSignificance: true,\n  details: \"Our best estimate is that the absolute rate of success is 5% lower with variant B, and this result is statistically significant (We are 95% confident the true difference is between -10% and 0%.). Given this information, you should probably stick with variant A.\",\n}\nInstallation\n$ npm install react-alpha-beta --save\nOverview and Basic Usage\nThe AlphaBeta component is a React component that \"wraps\" two other components. These two \"wrapped\" components are passed as ComponentA and ComponentB, and they represent the two variants you're testing. Each user that encounters the AlphaBeta component will see one of the two variants, and the AlphaBeta component will report back to your server (i) which variant was displayed and (ii) if a success event occurred.\nIn addition to your ComponentA and ComponentB variants, you'll also pass experimentParams to each of your AlphaBeta components. experimentParams is an object containing the keys id and testCohortSize.\nid is the unique id of a particular experiment, and is passed by the AlphaBeta component to your Backend \/ API (described below). Each AlphaBeta component that you declare should have a unique id associated with it.\ntestCohortSize is a number between 0.0 and 1.0. Its value tells your AlphaBeta component what proportion of your users will see each experiment variant. A testCohortSize value of 0.01 means that 1% of your users should see the ComponentB variant - the other 99% should see ComponentA. A value of .5 indicates that there should be an even split between the two variants.\nWhen you wrap your variants in an AlphaBeta component, the AlphaBeta component passes the prop successAction to each of them.\nYou get to decide what constitutes \"success\" in the context of your experiment. If you're testing a button variation, \"success\" might be defined as a click (this is the case in the above code sample). If you're testing a landing page variation, \"success\" might be defined as submitting a validated form.\nNote that while you have the ability to define \"success\" however you want, it is also your responsibility to make sure that successAction is fired by each of your variants when \"success\" occurs. Otherwise, AlphaBeta will have no way of giving you guidance about which variant is more likely to produce the desired outcome.\nThe Button example is designed to help you get comfortable using the AlphaBeta component. You'll need to set up your Backend \/ API for the example to work (instructions below), but reading through the example may help you better understand how to use AlphaBeta, even prior to fully setting things up.\nBackend \/ API Setup\nWait, AlphaBeta Needs a Backend?\nIn order for AlphaBeta to be useful, it needs to be able to record data about the experiments you're running. In other words, it needs to be linked to a datastore of some type. This reliance on a datastore isn't unique to AlphaBeta - it is true of split testing in general.\nImagine that you're running an experiment to see if changing a particular button from a transparent background (variant A) to a solid blue background (variant B) leads to more clicks. (If you already looked at the Button example, this should look familiar...)\nTo measure which variant performs better, you need to keep track of each variant's \"impressions\" (how many users have seen each button) and \"conversions\" (how many times each button is clicked).\nWhen we are able to keep track of these values, all it takes is a bit of math to estimate (within a specific range or \"confidence interval\") which button leads to more conversions. AlphaBeta handles this math for you, but you're responsible for logging the events themselves in your datastore.\nSo How Do I Set Up My AlphaBeta Endpoint?\nYou can connect AlphaBeta to a datastore you're already using in two steps.\nStep 1:\nSet up an API endpoint for AlphaBeta to Consume\nAlphaBeta expects to be able to interact with an endpoint at www.yoursite.com\/api\/alphabeta\/{{experimentId}}\/, where {{experimentId}} is the unique id you pass to each AlphaBeta component in experimentParams.\nAlphaBeta will both POST to and GET from this endpoint. When AlphaBeta detects an \"impression\" or a \"conversion\", it will POST to this endpoint, so all users who may encounter an experiment should be able to POST to this endpoint.\nYou can safely restrict GET requests to only allow access to users who should be able to see data about your experiments.\nIt's also a good idea (though not strictly necessary) to set up your endpoint such that GET requests made without an {{experimentId}} return a list of your experiments. This is a good idea if you wish to build a single page where you can view data about all of our experiments.\nEnsure Your Endpoint Accepts POST Requests Correctly\nWhen AlphaBeta POST data to your endpoint, the POST body should look like this:\n{\n    variant: \"a\",             \/\/ this will either be \"a\" or \"b\"\n    success: null,            \/\/ this will either be null or true\n    userCohortValue: .10392,  \/\/ a number between 0 and 1\n    metaId: null,             \/\/ this will be null unless you choose to set it\n}\n\n\nvariant tells your datastore which component variant (A or B) was presented to a particular user.\n\n\nsuccess tells your datastore whether the success event occurred (true) or not (null).\n(Note that the value for this parameter will either be true or null, as opposed to true or false. When success is passed as null, that signals that an impression has occurred. It is passed as null because when the component is loaded we don't know if the user will trigger the success event or not. When success is passed as true, that signals that a success event has occurred.)\n\n\nuserCohortValue is a number between 0.0 and 1.0 that AlphaBeta has associated with the particular user in this experiment. This number is randomly generated the first time a user encounters a particular AlphaBeta experiment, and is core to how AlphaBeta separates users into cohorts.\n\n\nmetaId is a value that you can optionally pass to your AlphaBeta component. It should be used in cases where the component that you're testing occurs more than one time times on your site.\n\n\nHere is an example of when you would set the metaId attribute:\nImagine you instead were testing the copy on a Facebook-style \"like\" button to see if changing \"like\" to \"+1\" led to more engagement. Each piece of content a user views in his\/her news feed should have a \"like\" (or \"+1) button below it. But since each user has multiple items in his\/her feed, a single user could \"like\" more than one piece of content.\nIn this case, you could set a metaId that uniquely identifies the piece of content being \"liked\". If you were to set the metaId, you would be testing which variant leads to more total likes per unit of content seen. If you were to not set the metaId, you would be testing which variant is more likely to lead to a user liking at least one piece of content.\nEnsure Your Endpoint Responds to GET Requests Correctly\nWhen AlphaBeta GETs data from your endpoint, the returned data should look like this\n{\n  variantA: {\n    trialCount:   291,  \/\/ the number of unique impressions for this variant\n    successCount: 59,   \/\/ the number of unique success events for this variant\n  },\n  variantB: {\n    trialCount:   101,\n    successCount: 22,\n  },\n  confidenceInterval: .95 \/\/ the CI you're looking to achieve, expressed as a float\n}\nStep 2:\nEnsure Your Back End Processes POST Requests Correctly\nWhen POST data is received, one of three things is supposed to happen:\n1 - the trialCount for an experiment variant could be incremented by 1.\n2 - the successCount for an experiment could be incremented by 1.\n3 - Nothing at all.\nThe logic for what should happen must be executed by your application's backend. Here's how things should work:\n\n\nif success === null and no previous trial exists where both userCohortValue and metaId are equal to this trial's values, you should increment trialCount by one for the appropriate variant.\n\n\nif success === true and no previous trial exists where both userCohortValue and metaId are equal to this trial's values and success === true, you should increment successCount by one for the appropriate variant.\n\n\nin all other cases, you should not take any action.\n\n\nChecking Your Experiment Results\nimport { getExperimentData } from 'react-alpha-beta';\n\nconsole.log(getExperimentData(experimentId));\nTo view your experiment results, call the getExperimentData function with the experimentId for a particular experiment. It will return a json object with the keys meanDifferenceValue, marginOfError, statisticalSignificance, and details.\n\nmeanDifferenceValue is AlphaBeta's best estimate (or mean estimate) of ComponentB's performance relative to ComponentA. A positive number indicates that ComponentB is leading to more successActions per impression than ComponentA, while a negative number indicates the opposite.\n\nNote that meanDifferenceValue alone doesn't mean much if the experiment hasn't yet reached statistical significance.\n\n\nmarginOfError is the margin of error (or uncertainty) that exists in the experiment.\n\n\nstatisticalSignificance, a boolean, represents whether this experiment has yet reached statistical significance at the level of confidence you defined.\n\n\ndetails is a human readable description of this experiment's current results.\n\n\nSample result from getExperimentData:\n{\n  meanDifferenceValue: -0.05023923444976075,\n  marginOfError: 0.04837299277280508,\n  statisticalSignificance: true,\n  details: \"Our best estimate is that the absolute rate of success is 5% lower with variant B, and this result is statistically significant (We are 95% confident the true difference is between -10% and 0%.). Given this information, you should probably stick with variant A.\",\n}\n\nExample\nNote: in order for this example to work, you must first set up an API endpoint for AlphaBeta to consume. If you haven't done this yet, follow the steps in Backend \/ API Setup\n\nButton example: Set up an experiment to see which of two button variants has a greater click-through rate. This example covers (i) basic experiment setup, (ii) the two ways to pass your variant components to the AlphaBeta component, and (iii) basic usage of the AlphaBeta DevTools.\n\nAlphaBeta DevTools\nAlphaBeta comes with a DevTools component that can be used on any page containing an experiment.\nimport { ABComponent, DevTools } from 'react-alpha-beta';\n\n\/\/ ***\n\/\/ Build your experiment component, which we'll call <Page \/> here.\n\/\/ Make sure that <DevTools \/> is in your <Page \/> component.\n\/\/ ***\n\nReactDOM.render(\n    <Page \/>,\n    document.getElementById('app')\n);\nOne easy way to familiarize yourself with the DevTools component is to load the Button example.\nIf the DevTools component is included on your page and you are not in a production environment (i.e. process.env.NODE_ENV !== 'production'), you should see a DevTools box in the lower right hand corner of your screen. This box lets you control your user cohort value for each of the experiments on the page. Recall that the user cohort value for an experiment, along with the testCohortSize parameter, determine which variant a user sees.\nIf the user cohort value is greater than or equal to testCohortSize, the user will see variant A for this experiment. If the user cohort value is less than testCohortSize, the user will see variant B. When you manipulate the DevTools sliders, you are changing your user cohort value for an experiment. These changes will take place when you refresh the page.\nYou can add the DevTools component to the lower level components that contain your experiments, or to higher level components of your application.\nDiscussion and Support\nJoin our Slack team!\nAdditional Resources\n\nA\/B testing course (Udacity)\nHypothesis testing with one sample (Khan Academy)\n\nLint\n$ npm run lint\nTest\n$ npm run test        # run once\n$ npm run test:watch  # continuous testing as file changes\n$ npm run test:cov    # generate test coverage report\nContribute\nWe are using commitizen to make commit format consistent.\n# Install the command line tool.\n$ npm install -g commitizen\n\n# From then on, whenever you would like to commit:\n$ git add .\n$ git cz\n# ... follow the prompt messages\nLicense\nMIT\nCredits\n\nJack McCloy\nBrian Park\nBen Hall\n\n","285":"Famous Examples\n##DEPRECATED: please find our examples inside of our main repo\nLicense\nAll the code in the src\/examples folder is licensed under the MIT license. This is basically a better MIT license since it removes the ambiguous language from the original MIT.\nThe famous framework source code found in \/src\/famous is a git submodule cloned from the famous\/famous git repo, and is licensed only under the MPL-2.0 license. More information about the licensing of that code can be found in the original repo.\nMIT License (everything in src\/examples)\nCopyright (c) 2014 Famous Industries, Inc.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nMPL-2.0 (everything in src\/famous)\nCopyright (c) 2014 Famous Industries, Inc.\nThis Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http:\/\/mozilla.org\/MPL\/2.0\/.\n","286":"Co Mocha\n\n\n\n\n\nEnable support for generators in Mocha tests using co.\nUse the --harmony-generators flag when running node 0.11.x to access generator functions, or transpile your tests using traceur or regenerator.\nInstallation\nnpm install co-mocha --save-dev\n\nUsage\nJust require the module in your tests and start writing generators in your tests.\nit('should do something', function * () {\n  yield users.load(123)\n})\nNode\nInstall the module using npm install co-mocha --save-dev. Now just require the module to automatically monkey patch any available mocha instances. With mocha, you have multiple ways of requiring the module - add --require co-mocha to your mocha.opts or add require('co-mocha') inside your main test file.\nIf you need to monkey patch a different mocha instance you can use the library directly:\nvar mocha = require('mocha')\nvar coMocha = require('co-mocha')\n\ncoMocha(mocha)\n<script> Tag\n<script src=\"co-mocha.js\"><\/script>\nIncluding the browserified script will automatically patch window.Mocha. Just make sure you include it after mocha.js. If that is not possible the library exposes window.coMocha, which can be used (window.coMocha(window.Mocha)).\nAMD\nSame details as the script, but using AMD requires instead.\nHow It Works\nThe module monkey patches the Runnable.prototype.run method of mocha to enable generators. In contrast to other npm packages, co-mocha extends mocha at runtime - allowing you to use any compatible mocha version.\nLicense\nMIT\n","287":"Co Mocha\n\n\n\n\n\nEnable support for generators in Mocha tests using co.\nUse the --harmony-generators flag when running node 0.11.x to access generator functions, or transpile your tests using traceur or regenerator.\nInstallation\nnpm install co-mocha --save-dev\n\nUsage\nJust require the module in your tests and start writing generators in your tests.\nit('should do something', function * () {\n  yield users.load(123)\n})\nNode\nInstall the module using npm install co-mocha --save-dev. Now just require the module to automatically monkey patch any available mocha instances. With mocha, you have multiple ways of requiring the module - add --require co-mocha to your mocha.opts or add require('co-mocha') inside your main test file.\nIf you need to monkey patch a different mocha instance you can use the library directly:\nvar mocha = require('mocha')\nvar coMocha = require('co-mocha')\n\ncoMocha(mocha)\n<script> Tag\n<script src=\"co-mocha.js\"><\/script>\nIncluding the browserified script will automatically patch window.Mocha. Just make sure you include it after mocha.js. If that is not possible the library exposes window.coMocha, which can be used (window.coMocha(window.Mocha)).\nAMD\nSame details as the script, but using AMD requires instead.\nHow It Works\nThe module monkey patches the Runnable.prototype.run method of mocha to enable generators. In contrast to other npm packages, co-mocha extends mocha at runtime - allowing you to use any compatible mocha version.\nLicense\nMIT\n","288":"Javascript Refactor plugin for Sublime Text 2 and 3\n \n[![Package Control](https:\/\/packagecontrol.herokuapp.com\/downloads\/JavaScript%20Refactor.svg?color=50C32E)](https:\/\/packagecontrol.io\/packages\/JavaScript%20Refactor)\n[![Donate](http:\/\/s-a.github.io\/donate\/donate.svg)](http:\/\/s-a.github.io\/donate\/)\nOverview\n\nGoto definition of a variable or function\nRename variable or function respecting its current scope\nIntroduce variable\nExtract selected source to a new method\n\nPreview\nhttp:\/\/www.youtube.com\/watch?v=P9K7mxWItPw\nInstallation\nUse the Sublime Package Control and search for: \"JavaScript Refactor\"\nor\nClone or download the git repository into your packages folder.\nIn Sublime Text use \"Preferences\/Browse Packages\" menu item to open this folder.\nThe shorter way of doing this is:\nLinux\ngit clone https:\/\/github.com\/s-a\/sublime-text-refactor.git ~\/.config\/sublime-text-2\/Packages\/sublime-text-refactor\nMac\ngit clone https:\/\/github.com\/s-a\/sublime-text-refactor.git ~\/Library\/Application\\ Support\/Sublime\\ Text\\ 2\/Packages\/sublime-text-refactor\nWindows\nIn an elevated command prompt or powershell (as Admin):\ngit clone https:\/\/github.com\/s-a\/sublime-text-refactor.git \"%APPDATA%\\Sublime Text 2\\Packages\\sublime-text-refactor\"\nfor Sublime Text 3:\ngit clone https:\/\/github.com\/s-a\/sublime-text-refactor.git \"%APPDATA%\\Sublime Text 3\\Packages\\sublime-text-refactor\"\nDependencies\n\nThis Plugin makes heavy usage of Node.js. So it needs a local installation of http:\/\/nodejs.org\nmocha (only for testing)\n\nUsage\nGoto Definition:\nSelect a keyword via double click or point the cursor to the keyword and choose \"Goto Definition\" from context menu.\nRename:\nSelect a keyword via double click or point the cursor to the keyword and choose \"Rename\" from context menu. The plugin will select all variables or function calls occurring in the source code including its declaration. After that you rename them all on the fly. The logic respects the variables or functions scope. So it should be safe to rename them all without thinking ;) .\nIntroduce Variable:\nSelect an Expression from source code or point the cursor to the desired position and choose \"Introduce Variable\" from context menu.\nExtract Method:\nSelect the source code you want to extract into a new method and choose \"Refactor \/ Extract methode\" from context menu.\nThis will extract the source code instantly to a new methode aka function. The plugin will manage undeclared variable usages and pass them within a single bundled JSON parameter to the new function.\nIt also generates a sample function call at the bottom of the new methode.\nThe plugin marks all variables occurring in the source code so you can rename them on the fly.\nRun the tests\nGoto Pluginfolder and type\nnpm test\nYou can find current test cases here\nhttps:\/\/github.com\/s-a\/sublime-text-refactor\/blob\/master\/js\/test\/\nTroubleshoot\n\nNode not found\nChoose Preferences: Refactor Settings \u2013 User from context menu and configure the nodePath setup. (Default Value is node)\n\n\t\"nodePath\" : \"node\"\n}```\n\n\nTodo\n========================\n- ***Extract method***  \n- Define exceptions of global scoped variable names like jQuery or $.\n- Do not pass variables available in current Scope (optional).\n- Let the user choose a function name before or after extraction.\n- Let the user choose a custom position to insert extracted methode code and indent it correctly.\n\n\nLicense\n=======\n\n\nMIT and GPL license.\n\nCopyright (c) 2013 Stephan Ahlf <stephan@ahlf-it.de>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n","289":"Amazon EC2\nEver try to query for some instances with boto? It sucks.\n>>> import ec2\n>>> ec2.instances.filter(state='running', name__startswith='production')\n[...]\nInstall\n$ pip install ec2\nUsage\nAWS credentials\nCredentials are defined as a global state, either through an environment variable, or in Python.\nec2.credentials.ACCESS_KEY_ID = 'xxx'\nec2.credentials.SECRET_ACCESS_KEY = 'xxx'\nec2.credentials.REGION_NAME = 'us-west-2'  # (optional) defaults to us-east-1\nCredentials can also be loaded from a CSV file generated by Amazon's IAM.\nNote: REGION_NAME still needs to be specified.\nec2.credentials.from_file('credentials.csv')\nQuerying\nAll instances\nec2.instances.all()\nAll Security Groups\nec2.security_groups.all()\nAll Virtual Private Clouds\nec2.vpcs.all()\nFiltering\nFilter style is based on Django's ORM\nAll filters map directly to instance\/security group properties.\nec2.instances.filter(id='i-xxx')  # Exact instance id\nec2.instances.filter(state='running')  # Exact instance state\nFilters will also dig into tags.\nec2.instances.filter(name='production-web')  # Exact \"Name\" tag\nFilters support many types of comparisons, similar to Django's ORM filters.\nec2.instances.filter(name__exact='production-web-01')  # idential to `name='...'`\nec2.instances.filter(name__iexact='PRODUCTION-WEB-01')  # Case insensitive \"exact\"\nec2.instances.filter(name__like=r'^production-web-\\d+$')  # Match against a regular expression\nec2.instances.filter(name__ilike=r'^production-web-\\d+$')  # Case insensitive \"like\"\nec2.instances.filter(name__contains='web')  # Field contains the search string\nec2.instances.filter(name__icontains='WEB')  # Case insensitive \"contains\"\nec2.instances.filter(name__startswith='production')  # Fields starts with the search string\nec2.instances.filter(name__istartswith='PRODUCTION')  # Case insensitive \"startswith\"\nec2.instances.filter(name__endswith='01')  # Fields ends with the search string\nec2.instances.filter(name__iendswith='01')  # Case insensitive \"endswith\"\nec2.instances.filter(name__isnull=False)  # Match if the field exists\nFilters can also be chained.\nec2.instances.filter(state='running', name__startswith='production')\nFilters can also be used with security groups.\nec2.security_groups.filter(name__iexact='PRODUCTION-WEB')\nFilters can also be used with virtual private clouds.\nec2.vpcs.filter(cidr_blocks__startswith='10.10')\nget() works exactly the same as filter(), except it returns just one instance and raises an exception for anything else.\nec2.instances.get(name='production-web-01')  # Return a single instance\nec2.instances.get(name='i-dont-exist')  # Raises an `ec2.instances.DoesNotExist` exception\nec2.instances.get(name__like=r'^production-web-\\d+$')  # Raises an `ec2.instances.MultipleObjectsReturned` exception if matched more than one instance\nec2.security_groups.get(name__startswith='production')  # Raises an `ec2.security_groups.MultipleObjectsReturned` exception\nec2.vpcs.get(cidr_block='10.10.0.0\/16')\nSearch fields\nInstances\n\nid (Instance id)\nstate (running, terminated, pending, shutting-down, stopping, stopped)\npublic_dns_name\nip_address\nprivate_dns_name\nprivate_ip_address\nroot_device_type (ebs, instance-store)\nkey_name (name of the SSH key used on the instance)\nimage_id (Id of the AMI)\n\nAll fields can be found at: https:\/\/github.com\/boto\/boto\/blob\/d91ed8\/boto\/ec2\/instance.py#L157-204\nSecurity Groups\n\nid (Security Group id)\nname\nvpc_id\n\nVirtual Private Clouds\n\nid (Virtual Private Cloud id)\ncidr_block (CIDR Network Block of the VPC)\nstate (Current state of the VPC, creation is not instant)\nis_default\ninstance_tenancy\ndhcp_options_id (DHCP options id)\n\nExamples\nGet public ip addresses from all running instances who are named production-web-{number}\nimport ec2\nec2.credentials.ACCESS_KEY_ID = 'xxx'\nec2.credentials.SECRET_ACCESS_KEY = 'xxx'\n\nfor instance in ec2.instances.filter(state='running', name__like=r'^production-web-\\d+$'):\n    print instance.ip_address\nAdd a role to a security group\nimport ec2\nec2.credentials.ACCESS_KEY_ID = 'xxx'\nec2.credentials.SECRET_ACCESS_KEY = 'xxx'\n\ntry:\n    group = ec2.security_groups.get(name='production-web')\nexcept ec2.security_groups.DoesNotExist:\n    import sys\n    sys.stderr.write('Not found.')\n    sys.exit(1)\ngroup.authorize('tcp', 80, 80, cidr_ip='0.0.0.0\/0')\n","290":"Detective.io \nDownload \u2022\nFork \u2022\nLicense \u2022\nTest coverage \u2022\nDocumentation \u2022\nVersion 1.12.13 Gorilla\nInstallation\nSee also the full installation guide.\n1. Prerequisite\nsudo apt-get install build-essential git-core python python-pip python-dev libmemcached-dev libpq-dev libxslt1-dev libxml2-dev libxml2 libjpeg8-dev\nsudo pip install virtualenv\n2.  Download the project\ngit clone git@github.com:jplusplus\/detective.io.git\ncd detective.io\n3. Install\nmake install\nRun in development\nmake run\nThen visit http:\/\/127.0.0.1:8000\nTechnical stack\nThis small application uses the following tools and opensource projects:\n\nDjango Framework - Backend Web framework\nNeo4django - Object Graph Mapper for Neo4j\nTastypie - RestAPI for Django\nAngularJS - Javascript Framework\nUI Router - Application states manager\nUnderscore - Utility library\nBootstrap - HTML and CSS framework\nLess - CSS pre-processor\nCoffeeScript\n\n","291":"THIS PROJECT IS UNMAINTAINED\nPlease use micawber -- it has much\nof the same functionality along with many improvements.\n\nGetting Started with OEmbed\n\nInstallation\nFirst, you need to install OEmbed.  It is available at http:\/\/github.com\/worldcompany\/djangoembed\/\ngit clone git:\/\/github.com\/worldcompany\/djangoembed\/\ncd djangoembed\npython setup.py install\n\n\nAdding to your Django Project\nAfter installing, adding OEmbed consumption to your projects is a snap.  First,\nadd it to your projects' INSTALLED_APPs and run 'syncdb':\n# settings.py\nINSTALLED_APPS = [\n    ...\n    'oembed'\n]\n\ndjangoembed uses a registration pattern like the admin's.  In order to be\nsure all apps have been loaded, djangoembed should run autodiscover() in the\nurls.py.  If you like, you can place this code right below your admin.autodiscover()\nbits:\n# urls.py\nimport oembed\noembed.autodiscover()\n\n\nConsuming Resources\nNow you're ready to start consuming OEmbed-able objects.  There are a couple of\noptions depending on what you want to do.  The most straightforward way to get\nup-and-running is to add it to your templates:\n{% load oembed_tags %}\n\n{% oembed %}blog.content{% endoembed %}\n\n{# or use the filter #}\n\n{{ blog.content|oembed }}\n\n{# maybe you're working with some dimensional constraints #}\n\n{% oembed \"600x600\" %}blog.content{% endoembed %}\n\n{{ blog.content|oembed:\"600x600\" }}\n\nYou can consume oembed objects in python as well:\nimport oembed\noembed.autodiscover()\n\n# just get the metadata\nresource = oembed.site.embed('http:\/\/www.youtube.com\/watch?v=nda_OSWeyn8')\nresource.get_data()\n\n{u'author_name': u'botmib',\n u'author_url': u'http:\/\/www.youtube.com\/user\/botmib',\n u'height': 313,\n u'html': u'<object width=\"384\" height=\"313\"><param name=\"movie\" value=\"http:\/\/www.youtube.com\/v\/nda_OSWeyn8&fs=1\"><\/param><param name=\"allowFullScreen\" value=\"true\"><\/param><param name=\"allowscriptaccess\" value=\"always\"><\/param><embed src=\"http:\/\/www.youtube.com\/v\/nda_OSWeyn8&fs=1\" type=\"application\/x-shockwave-flash\" width=\"384\" height=\"313\" allowscriptaccess=\"always\" allowfullscreen=\"true\"><\/embed><\/object>',\n u'provider_name': u'YouTube',\n u'provider_url': u'http:\/\/www.youtube.com\/',\n u'title': u'Leprechaun in Mobile, Alabama',\n u'type': u'video',\n u'version': u'1.0',\n u'width': 384}\n\n# get the metadata and run it through a template for pretty presentation\nfrom oembed.consumer import OEmbedConsumer\nclient = OEmbedConsumer()\nembedded = client.parse_text(\"http:\/\/www.youtube.com\/watch?v=nda_OSWeyn8\")\n\n<div class=\"oembed oembed-video provider-youtube\">\n  <object width=\"384\" height=\"313\">\n    <param name=\"movie\" value=\"http:\/\/www.youtube.com\/v\/nda_OSWeyn8&fs=1\"><\/param>\n    <param name=\"allowFullScreen\" value=\"true\"><\/param>\n    <param name=\"allowscriptaccess\" value=\"always\"><\/param>\n    <embed src=\"http:\/\/www.youtube.com\/v\/nda_OSWeyn8&fs=1\"\n           type=\"application\/x-shockwave-flash\"\n           width=\"384\"\n           height=\"313\"\n           allowscriptaccess=\"always\"\n           allowfullscreen=\"true\">\n    <\/embed>\n  <\/object>\n  <p class=\"credit\">\n    <a href=\"http:\/\/www.youtube.com\/watch?v=nda_OSWeyn8\">Leprechaun in Mobile, Alabama<\/a>\n    by\n    <a href=\"http:\/\/www.youtube.com\/user\/botmib\">botmib<\/a>\n  <\/p>\n<\/div>'\n\n\nTroubleshooting\nProblem: You try the youtube embed example, but all you get is a link to the youtube video.\nSolution: Djangoembed uses fixtures to load data about oembed providors like Youtube in to the database.  Try fooling around with syncdb (or migrations, if you're running South) until there are objects of type oembed.storedprovider.\nIf you have another problem, consider looking through the more extensive docs in the project's doc subdirectory.\n","292":"ZeroDB server and client-side example of using it\nDocumentation: http:\/\/docs.zerodb.io\/\n","293":"crontabber\nA cron job runner with self-healing and job dependencies.\nLicense: MPL 2\n\n\n\nHow to run tests\nFirst you need to create a dedicated test database. We recommend you\ncall it test_crontabber. Then you need the necessary credentials for\nit.\nBefore running the tests you need to install some extras to be able to\nrun tests at all:\npip install -r test-requirements.txt\n\nNext, in the root directory of the project create a file called\ntest-crontabber.ini and it should look something like this:\n[crontabber]\nuser=myusername\npassword=mypassword\ndbname=test_crontabber\n\nTo start all the tests run:\nPYTHONPATH=. nosetests\n\nIf you want to run a specific test in a specific file in a specific\nclass you can define it per the nosetests standard like this for\nexample:\nPYTHONPATH=. nosetests tests crontabber\/tests\/test_crontabber.py:TestCrontabber.test_basic_run_job\n\nIf you want the tests to stop as soon as the first test fails add -x\nto that same command above.\nAlso, if you want nosetests to not capture stdout add -s\nto that same command as above.\n\nHow to do code coverage analysis\nFirst you need to install the\ncoverage module. Then,\nwith nosetests, you can run this:\nPYTHONPATH=. nosetests --with-coverage --cover-erase --cover-html --cover-package=crontabber\n\nAfter it has run, you can open the file cover\/index.html in browser.\n\nHow to run the exampleapp\nThe example app helps you set up a playground to play around with and\ntest crontabber to gain a better understanding of how it works.\nThe best place to start with is to read the exampleapp\/README.md\nfile and go through its steps. Once you get the basics to work you can\nstart experimenting with adding your job classes.\n\nHow locking works\ncrontabber supports locking. It basically means if you start a second\ninstance of crontabber whilst it's already ongoing in another\nterminal\/server the second one will exist early. This is only applicable\nif there is an actual job ongoing.\nThere are two kinds of locking.\n\nGeneral locking. The first thing crontabber does before it starts\nan app is to ask the state (stored in PostgreSQL) if it's ongoing and\nif it is, it exists with an error code of 3.\nSub-second locking. If the general locking (see point above) says\n\"No, the job is not ongoing\", it's going to proceed to update the\nstate with a row-level locking transaction in\nPostgreSQL.\nThat basically means PostgreSQL only allows one single UPDATE\nfrom the process that gets there first. The second crontabber process\nwill will exit early with an error code of 2 if the first\ncrontabber process managed to run the UPDATE first.\n\nImagine two separate terminals starting crontabber at the almost same\ntime:\n# Terminal 1\n$ python crontabber.py --admin.conf=crontabber.ini\n$ echo $?\n0\n\n# Terminal 2 (started almost simultaneously)\n$ python crontabber.py --admin.conf=crontabber.ini\n$ echo $?\n3\n\nNote! If a job has been ongoing to a maximum period of time, the\nlocking is ignored. This is controlled by the config option\ncrontabber.max_ongoing_age_hours which defaults to 12 hours.\nThis is applicable if crontabber, updates the state that it's starting a\njob, then when it tries to update the state that it finished\n(successfully or not) and that write fails, if for example it's unable\nto make a connection to PostgreSQL. If this happens crontabber will just\nignore the lock and run it anyway.\n","294":"crontabber\nA cron job runner with self-healing and job dependencies.\nLicense: MPL 2\n\n\n\nHow to run tests\nFirst you need to create a dedicated test database. We recommend you\ncall it test_crontabber. Then you need the necessary credentials for\nit.\nBefore running the tests you need to install some extras to be able to\nrun tests at all:\npip install -r test-requirements.txt\n\nNext, in the root directory of the project create a file called\ntest-crontabber.ini and it should look something like this:\n[crontabber]\nuser=myusername\npassword=mypassword\ndbname=test_crontabber\n\nTo start all the tests run:\nPYTHONPATH=. nosetests\n\nIf you want to run a specific test in a specific file in a specific\nclass you can define it per the nosetests standard like this for\nexample:\nPYTHONPATH=. nosetests tests crontabber\/tests\/test_crontabber.py:TestCrontabber.test_basic_run_job\n\nIf you want the tests to stop as soon as the first test fails add -x\nto that same command above.\nAlso, if you want nosetests to not capture stdout add -s\nto that same command as above.\n\nHow to do code coverage analysis\nFirst you need to install the\ncoverage module. Then,\nwith nosetests, you can run this:\nPYTHONPATH=. nosetests --with-coverage --cover-erase --cover-html --cover-package=crontabber\n\nAfter it has run, you can open the file cover\/index.html in browser.\n\nHow to run the exampleapp\nThe example app helps you set up a playground to play around with and\ntest crontabber to gain a better understanding of how it works.\nThe best place to start with is to read the exampleapp\/README.md\nfile and go through its steps. Once you get the basics to work you can\nstart experimenting with adding your job classes.\n\nHow locking works\ncrontabber supports locking. It basically means if you start a second\ninstance of crontabber whilst it's already ongoing in another\nterminal\/server the second one will exist early. This is only applicable\nif there is an actual job ongoing.\nThere are two kinds of locking.\n\nGeneral locking. The first thing crontabber does before it starts\nan app is to ask the state (stored in PostgreSQL) if it's ongoing and\nif it is, it exists with an error code of 3.\nSub-second locking. If the general locking (see point above) says\n\"No, the job is not ongoing\", it's going to proceed to update the\nstate with a row-level locking transaction in\nPostgreSQL.\nThat basically means PostgreSQL only allows one single UPDATE\nfrom the process that gets there first. The second crontabber process\nwill will exit early with an error code of 2 if the first\ncrontabber process managed to run the UPDATE first.\n\nImagine two separate terminals starting crontabber at the almost same\ntime:\n# Terminal 1\n$ python crontabber.py --admin.conf=crontabber.ini\n$ echo $?\n0\n\n# Terminal 2 (started almost simultaneously)\n$ python crontabber.py --admin.conf=crontabber.ini\n$ echo $?\n3\n\nNote! If a job has been ongoing to a maximum period of time, the\nlocking is ignored. This is controlled by the config option\ncrontabber.max_ongoing_age_hours which defaults to 12 hours.\nThis is applicable if crontabber, updates the state that it's starting a\njob, then when it tries to update the state that it finished\n(successfully or not) and that write fails, if for example it's unable\nto make a connection to PostgreSQL. If this happens crontabber will just\nignore the lock and run it anyway.\n","295":"twosheds\n\n\n\ntwosheds is a library, written in Python, for making command language\ninterpreters, or shells.\nWhile shells like bash and zsh are powerful, extending them and customizing them\nis hard; you need to write in inexpressive arcane languages, such as bash script\nor C. twosheds helps you write and customize your own shell, in pure Python:\n>>> import twosheds\n>>> shell = twosheds.Shell()\n>>> shell.serve_forever()\n$ whoami\narthurjackson\n$ ls\nAUTHORS.rst       build             requirements.txt  test_twosheds.py\nLICENSE           dist              scripts           tests\nMakefile          docs              setup.cfg         twosheds\nREADME.rst        env               setup.py          twosheds.egg-info\nGet started now.\n\nFeatures\n\nSubstitution\nHistory\nTab completion\nHighly extensible\n\n\nInstallation\nTo install twosheds, simply:\n$ pip install twosheds\n\nDocumentation\nDocumentation is available at http:\/\/twosheds.readthedocs.org\/en\/latest\/.\n\nContribute\ntwosheds is under active development and contributions are especially welcome.\n\nCheck for open issues or open a fresh issue to start a discussion around a\nfeature idea or a bug.\nFork the repository on GitHub to start making your changes to the\nmaster branch (or branch off it).\nWrite a test which shows that the bug was fixed or that the feature works as\nexpected.\nSend a pull request and bug the maintainer until its get merged and\npublished. Make sure to add yourself to AUTHORS. :)\n\n\nSupport\nIf you have questions or issues about twosheds, there are several options:\n\nSend a Tweet\nIf your question is less than 140 characters, feel free to tweet at the\nmaintainer, @Ceasar_Bautista.\n\nFile an Issue\nIf you notice some unexpected behavior in twosheds, or want to see support for\na new feature, file an issue on GitHub issues.\n\nE-mail\nI'm more than happy to answer any personal or in-depth questions about twosheds.\nFeel free to email cbautista@gmail.com.\n","296":"uvent: A gevent core implemented using libuv\nuvent is a gevent core implementation using the libuv library.\nuvent uses pyuv, a Python interface for libuv. libuv is a high performance asynchronous\nnetworking library used as the platform layer for NodeJS.\nlibuv provides the same core functionality as libev, with some really nice\nadditions:\n\nHigh performance IO on Windows (not select)\nAsyncronous file operations\nBuiltin thread pool\nAsynchronous getaddrinfo\nNicer to use API\nEtc.\n\nSource code for uvent is on GitHub.\n\nMotivation\nThis is an experimental project to test the feasibility of using libuv as a\ncore for gevent.\nMain functionality is working but not all tests are passing and there are some\nimplementation caveats mostly due to the tight integration between gevent and\nlibev. Implementation notes can be found in the NOTES.rst file.\n\nInstallation\nuvent requires pyuv >= 0.10.0\npip install -U pyuv\n\nNote: uvent only works with gevent >= 1.0rc1, earlier versions are not supported.\n\nUsing it\nIn order to use uvent add the following lines at the beginning\nof your project, before importing anything from Gevent:\nimport uvent\nuvent.install()\n\nAnother way of doing this without modifying your code is by exporting environment variables before\nrunning your program:\nexport GEVENT_LOOP=uvent.loop.UVLoop\nexport GEVENT_RESOLVER=gevent.resolver_thread.Resolver\n\n\nAuthor\nSa\u00fal Ibarra Corretg\u00e9 <saghul@gmail.com>\n\nLicense\nuvent uses the MIT license, check LICENSE file.\n","297":"Sea Cucumber 1.5.1\n\n\nInfo:A Django email backend for Amazon Simple Email Service, backed by django-celery\n\nAuthor:\nDUO Interactive, LLC\nInspired by:Harry Marr's django-ses.\n\nStatus:\nUnmaintained. Let us know if you'd like to step in!\n\n\n\nA bird's eye view\nSea Cucumber is a mail backend for Django. Instead of sending emails\nthrough a traditional SMTP mail server, Sea Cucumber routes email through\nAmazon Web Services' excellent Simple Email Service (SES) via django-celery.\n\nWhy Sea Cucumber\/SES instead of SMTP?\nConfiguring, maintaining, and dealing with some complicated edge cases can be\ntime-consuming. Sending emails with Sea Cucumber might be attractive to you if:\n\nYou don't want to maintain mail servers.\nYour mail server is slow or unreliable, blocking your views from rendering.\nYou need to send a high volume of email.\nYou don't want to have to worry about PTR records, Reverse DNS, email\nwhitelist\/blacklist services.\nYou are already deployed on EC2 (In-bound traffic to SES is free from EC2\ninstances). This is not a big deal either way, but is an additional perk if\nyou happen to be on AWS.\n\n\nInstallation\nAssuming you've got Django and django-celery installed, you'll need\nBoto 2.0b4 or higher. boto is a Python library that wraps the AWS API.\nYou can do the following to install boto 2.0b4 (we're using --upgrade here to\nmake sure you get 2.0b4):\npip install --upgrade boto\n\nInstall Sea Cucumber:\npip install seacucumber\n\nAdd the following to your settings.py:\nEMAIL_BACKEND = 'seacucumber.backend.SESBackend'\n\n# These are optional -- if they're set as environment variables they won't\n# need to be set here as well\nAWS_SES_REGION_NAME = 'YOUR-REGION'  # Default is us-east-1\nAWS_ACCESS_KEY_ID = 'YOUR-ACCESS-KEY-ID'\nAWS_SECRET_ACCESS_KEY = 'YOUR-SECRET-ACCESS-KEY'\n\n# Make sure to do this if you want the ``ses_address`` management command.\nINSTALLED_APPS = (\n    ...\n    'seacucumber'\n)\n\nThe region name is optional but keep in mind addresses are linked to a specific region, therefore if you add\/verify them in one region they will only be available in that region.\n\nEmail Address Verification\nBefore you can send email 'from' an email address through SES, you must first\nverify your ownership of it:\n.\/manage.py ses_address verify batman@gotham.gov\n\nAfter you've run the verification above you will need to check the email\naccount's inbox (from your mail client or provider's web interface) and click\nthe authorization link in the email Amazon sends you. After that, your address\nis ready to go.\nTo confirm the verified email is ready to go:\n.\/manage.py ses_address list\n\nTo remove a previously verified address:\n.\/manage.py ses_address delete batman@gotham.gov\n\nNow, when you use django.core.mail.send_mail from a verified email address,\nSea Cucumber will handle message delivery.\n\nRate Limiting\nIf you are a new SES user, your default quota will be 1,000 emails per 24\nhour period at a maximum rate of one email per second. Sea Cucumber defaults\nto enforcing the one email per second at the celery level, but you must not\nhave disabled celery rate limiting.\nIf you have this:\nCELERY_DISABLE_RATE_LIMITS = True\n\nChange it to this:\nCELERY_DISABLE_RATE_LIMITS = False\n\nThen check your SES max rate by running:\n.\/manage.py ses_usage\n\nIf your rate limit is more than 1.0\/sec, you'll need to set that numeric\nvalue in your CUCUMBER_RATE_LIMIT setting like so:\n# Rate limit to three outgoing SES emails a second.\nCUCUMBER_RATE_LIMIT = 3\n\nFailure to follow the rate limits may result in BotoServerError exceptions\nbeing raised, which makes celery unhappy.\nAs a general note, your quota and max send rate will increase with usage, so\ncheck the ses_usage management command again at a later date after you've\nsent some emails. You'll need to manually bump up your rate settings in\nsettings.py.\n\nRouting Tasks\nIf you want to route Sea Cucumber task to different queues.\nAdd this to setting:\nCUCUMBER_ROUTE_QUEUE = 'YOUR-ROUTE-QUEUE'\n\nThen update the celery configuration for routes. Example celeryconfig.py:\nCELERY_ROUTES = {\n    'seacucumber.tasks.#': {'queue': 'YOUR-ROUTE-QUEUE'},\n}\n\n\nDKIM\nUsing DomainKeys is entirely optional, however it is recommended by Amazon for\nauthenticating your email address and improving delivery success rate.  See\nhttp:\/\/docs.amazonwebservices.com\/ses\/latest\/DeveloperGuide\/DKIM.html.\nBesides authentication, you might also want to consider using DKIM in order to\nremove the via email-bounces.amazonses.com message shown to gmail users -\nsee http:\/\/support.google.com\/mail\/bin\/answer.py?hl=en&answer=1311182.\nTo enable DKIM signing you should install the pydkim package and specify values\nfor the DKIM_PRIVATE_KEY and DKIM_DOMAIN settings.  You can generate a\nprivate key with a command such as openssl genrsa 1024 and get the public key\nportion with openssl rsa -pubout <private.key.  The public key should be\npublished to ses._domainkey.example.com if your domain is example.com.  You\ncan use a different name instead of ses by changing the DKIM_SELECTOR\nsetting.\nThe SES relay will modify email headers such as Date and Message-Id so by\ndefault only the From, To, Cc, Subject headers are signed, not the full\nset of headers.  This is sufficient for most DKIM validators but can be overridden\nwith the DKIM_HEADERS setting.\nExample settings.py:\nDKIM_DOMAIN = 'example.com'\nDKIM_PRIVATE_KEY = '''\n-----BEGIN RSA PRIVATE KEY-----\nxxxxxxxxxxx\n-----END RSA PRIVATE KEY-----\n'''\n\nExample DNS record published to Route53 with boto:\n\nroute53 add_record ZONEID ses._domainkey.example.com. TXT '\"v=DKIM1; p=xxx\"' 86400\n\nDjango Builtin-in Error Emails\nIf you'd like Django's Builtin Email Error Reporting to function properly\n(actually send working emails), you'll have to explicitly set the\nSERVER_EMAIL setting to one of your SES-verified addresses. Otherwise, your\nerror emails will all fail and you'll be blissfully unaware of a problem.\nNote: You can use the included ses_address management command to handle\naddress verification.\n\nGetting Help\nIf you have any questions, feel free to either post them to our\nissue tracker.\n","298":"\nNOTICE: Deprecated\nThis project is deprecated and no longer actively maintained by Disqus. However there is a fork being maintained by YPlan at github.com\/YPlan\/django-modeldict and a similar project by Disqus at github.com\/disqus\/durabledict.\n\ndjango-modeldict\nModelDict is a very efficient way to store things like settings in your database. The entire model is transformed into a dictionary (lazily) as well as stored in your cache. It's invalidated only when it needs to be (both in process and based on CACHE_BACKEND).\nQuick example usage. More docs to come (maybe?):\nclass Setting(models.Model):\n    key = models.CharField(max_length=32)\n    value = models.CharField(max_length=200)\nsettings = ModelDict(Setting, key='key', value='value', instances=False)\n\n# access missing value\nsettings['foo']\n>>> KeyError\n\n# set the value\nsettings['foo'] = 'hello'\n\n# fetch the current value using either method\nSetting.objects.get(key='foo').value\n>>> 'hello'\n\nsettings['foo']\n>>> 'hello'\n\n","299":"PagerIndicator\n  \nPager (especially for ViewPager) indicator in two styles: circle & fraction.\nDemo\n\n\n\ncircle\nfraction\n\n\n\n\n\n\n\n\n\nDependency\nimplementation 'me.liangfei:pagerindicator:0.0.2'\nUsage\nTwo attributes are provided:\n\napp:indicator_type = [fraction | circle].\napp:indicator_spacing works only for the circle type indicator.\n\nStep 1: add PageIndicator to the bottom of the ViewPager.\n<merge xmlns:android=\"http:\/\/schemas.android.com\/apk\/res\/android\"\n    xmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\">\n\n    <androidx.viewpager.widget.ViewPager\n        android:id=\"@+id\/pager\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" \/>\n\n    <me.liangfei.indicator.PagerIndicator\n        android:id=\"@+id\/indicator\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:layout_marginBottom=\"20dp\"\n        android:layout_marginLeft=\"20dp\"\n        android:layout_marginStart=\"20dp\"\n        android:gravity=\"bottom|center_horizontal\"\n        app:indicator_spacing=\"5dp\"\n        app:indicator_type=\"fraction\" \/>\n<\/merge>\nStep 2: pass the ViewPager instance to the PagerIndicator instance.\nval pageIndicator = findViewById(R.id.indicator);\npageIndicator.setViewPager(pager);\nYou can just take PageIndicator as a normal view to make your layout, because it extends LinearLayout.\nCheck the app module for more details.\ndependencies\n\nFresco\n\n\u6b22\u8fce\u5173\u6ce8\u6211\u7684\u5fae\u4fe1\u516c\u4f17\u53f7\uff08Chinese only\uff09\n\n","300":"ExpandableView\n\nExpandableView is a View showing only a content and when clicked on it, it displays more content in a fashion way. You can add views or viewgroups but remember that it will only display the content in a LinearLayout with vertical orientation.\nYou can choose by default a \"chevron\" icon animation or a \"plus\" icon animation. The \"always visible row\" has a left icon, a text and finally a right icon which will be the animated one.\n\nInstructions - Maven Central\n\nAdd this library in your build.gradle:\n\ndependencies {\n    compile 'com.github.nicolasjafelle:expandableview:1.0'\n}\nInstructions\n\nClone the git repo\nImport the \"ExpandableView\" module into your Android-gradle project.\nAdd \"ExpandableView\" module in your settings.gradle\nDONE\n\nHow to Use it\nAs any view in Android you can add it by code or by layout xml file but remember that if you want to change the default height of the visible content you need to use:\ntopExpandableView.setVisibleLayoutHeight(300);\ntopExpandableView.setVisibleLayoutHeight(getResources().getDimensionPixelSize(R.dimen.new_height));\nRemember also to fill the information inside the visible content by using:\nexpandableView.fillData(R.drawable.ic_android, R.string.android_names, true);\n\/\/or\nexpandableView.fillData(R.drawable.ic_android, getString(R.string.android_names), true);\n\/\/or\nexpandableView.fillData(R.drawable.ic_android, R.string.android_names, true);\n\/\/or\nexpandableView.fillData(R.drawable.ic_android, getString(R.string.android_names));\n\/\/or\nexpandableView.fillData(R.drawable.ic_android, R.string.android_names);\n\n\/\/or\nexpandableView.fillData(0, R.string.android_names); \/\/ No drawable left by passing 0.\nIf you want to add content into the discoverable LinearLayout simple use:\nexpandableView.addContentView(itemView); \/\/ itemView could be a simple TextView or more complex custom views\nThe most relevant part of this ExpandableView is when you want to include an ExpandableView into another ExpandableView, you need to pass the parent's View hierarchy, like this:\nexpandableViewLevel1.setOutsideContentLayout(topExpandableView.getContentLayout()); \/\/ 1 Level\nexpandableViewLevel2.setOutsideContentLayout(topExpandableView.getContentLayout(), expandableViewLevel1.getContentLayout()); \/\/ 2 Levels\nexpandableViewLevel3.setOutsideContentLayout(topExpandableView.getContentLayout(), expandableViewLevel1.getContentLayout(), expandableViewLevel2.getContentLayout()); \/\/ 3 Levels\nAlso remember to use this package in your layout files:\n<com.expandable.view.ExpandableView\n\tandroid:id=\"@+id\/activity_main_top_expandable_view\"\n\tandroid:layout_width=\"match_parent\"\n\tandroid:layout_height=\"wrap_content\"\/>\n\nYou can also customize the TextView inside the visible Content Layout in this way:\n<!-- Use this style name to override the default style -->\n<style name=\"ExpandableView_TextView\">\n\t<item name=\"android:textSize\">18sp<\/item>\n\t<item name=\"android:textStyle\">bold<\/item>\n\t<item name=\"android:textColor\">@android:color\/black<\/item>\n<\/style>\n\nDeveloped By\n\nNicolas Jafelle - nicolasjafelle@gmail.com\n\nLicense\nCopyright 2015 Nicolas Jafelle\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","301":"StoreBox\nAndroid library for streamlining SharedPreferences.\n\n\n \n\nContents\n\nOverview\nAdding to a project\nInterface and creation\nGet and set methods\nDefaults for get methods\nCustom types\nPreference types\nAdvanced\nRemove and clear methods\nChange listeners\nChaining calls\nForwarding calls\nSave modes\nVersioning\nBuilder & Defaults\nProGuard\nContributing\nLicense\n\nOverview\nStoreBox is an annotation-based library for interacting with Android's SharedPreferences, with the aim take out the the how and where parts of retrieving\/storing values and instead focus on the more important what part.\nNormally when retrieving or storing values we need to know two pieces of information during each call: the key and the type.\nString username = preferences.getString(\"key_username\", null);\npreferences.edit().putString(\"key_date_of_birth\", \"30\/09\/2004\").apply(); \/\/ should this be a String or a long?\nWith StoreBox the operations above can be changed into pre-defined methods with improved semantics.\n\/\/ definition\npublic interface MyPreferences {\n    \n    @KeyByString(\"key_username\")\n    String getUsername();\n    \n    @KeyByString(\"key_date_of_birth\")\n    void setDateOfBirth(String value);\n}\n\n\/\/ usage\nMyPreferences preferences = StoreBox.create(context, MyPreferences.class);\nString username = preferences.getUsername();\npreferences.setDateOfBirth(\"30\/09\/2004\");\nThe caller now doesn't need to worry about the key, neither about what type the values are stored under. The only important part that needs to be taken into consideration is what is done with the values, whether that is storing them for later, showing them to the user in the UI, or just changing application behaviour.\nRead on to find out more details about how StoreBox can be used and how it can be added to an Android project.\nAdding to a project\nStoreBox can be used in Android projects using minimum SDK version 10 and newer (Android 2.3+).\nJAR\nv1.4.0 JAR\nv1.4.0 JavaDoc JAR\nGradle\ncompile 'net.orange-box.storebox:storebox-lib:1.4.0'\n\nMaven\n<dependency>\n  <groupId>net.orange-box.storebox<\/groupId>\n  <artifactId>storebox-lib<\/artifactId>\n  <version>1.4.0<\/version>\n<\/dependency>\n\nDefining an interface and bringing it to life\nSimply create a new interface class in your IDE or a text editor, give it an access modifier which suits its use, and name it as appropriate.\npublic interface MyPreferences {\n    \n}\nNow you're ready to use StoreBox.create() to obtain an instance.\nMyPreferences instance = StoreBox.create(context, MyPreferences.class);\nAdding get and set methods\nIf you would like to add a getter just add a method to the interface which returns a value and make sure to annotate it using @KeyByString or @KeyByResource.\n@KeyByString(\"key_nickname\")\nString getNickname();\n\n@KeyByResource(R.string.key_notifications)\nboolean shouldShowNotifications();\nAdding a setter is just as easy. The same annotations will need to be used as for getter methods, but now our method will return nothing and will have to provide a parameter for supplying the value that should be saved.\n@KeyByString(\"key_nickname\")\nvoid setNickname(String value)\n\n@KeyByResource(R.string.key_notifications)\nvoid setNotifications(boolean value)\nSpecifying defaults for get methods\nThis can be achieved in two ways, through an argument or by using an annotation.\nFor the first option the following will work.\n@KeyByString(\"key_phone_number\")\nString getPhoneNumber(String defValue);\nAnd using an annotation referencing a default found in the XML resources.\n@KeyByString(\"key_phone_number\")\n@DefaultValue(R.string.default_phone_number)\nString getPhoneNumber();\nFor some types, such as long, which cannot be added to the resources an integer resource may be used instead.\n@KeyByString(\"key_refresh_interval\")\n@DefaultValue(R.integer.default_refresh_interval)\nlong getRefreshInterval();\nStoring and retrieving custom types\nSaving custom types, which are not understood by Android's SharedPreferences, can be supported through the use of type adapters. A type adapter implementation can be provided by extending from one of the following classes:\n\nBaseBooleanTypeAdapter for storing as a Boolean\nBaseFloatTypeAdapter for storing as a Float and so on...\nBaseIntegerTypeAdapter\nBaseLongTypeAdapter\nBaseStringTypeAdapter\nBaseStringSetTypeAdapter (only supported on API11 and newer)\n\nTelling StoreBox which type adapter should be used can be done by adding the @TypeAdapter annotation to the get and set methods.\n@KeyByString(\"key_region\")\n@TypeAdapter(RegionTypeAdapter.class)\nRegion getRegion();\n\n@KeyByString(\"key_region\")\n@TypeAdapter(RegionTypeAdapter.class)\nvoid setRegion(Region value);\nWhich type adapter needs to be extended depends on the use case. Take a look at the DateTypeAdapter, UriTypeAdapter, and CustomClassListTypeAdapter for some examples. It is worth noting that in the last example Gson is being used for serialising the type, as opposed to writing a custom implementation. Gson is not used internally by StoreBox, as such if you wish to use Gson for a type adapter you will need to add it to your project as a dependency.\nThe following types will work out of the box, so type adapters don't need to be provided for them:\n\nDate\nDouble\nEnum\nUri\n\nDisclaimer: APIs around type adapters may change in the future, as I will keep looking for a less verbose way of achieving the same goal without requiring the use of Gson.\nOpening different types of preferences\nIn all of the examples so far details about what preferences are opened and how have been omitted.\nWithout any annotation the default shared preferences will be used, but the @DefaultSharedPreferences annotation can be added to the interface definition for explicitness. Likewise, @ActivityPreferences or @FilePreferences can be used to respectively open preferences private to an activity or to open preferences using a file name.\nThe mode with which the preferences should be opened can also be specified, although this option is not supported by all the types.\n@ActivityPreferences(mode = PreferencesMode.MULTI_PROCESS)\npublic interface WelcomeActivityPreferences {\n    \n    \/\/ method definitions here...\n}\nAdvanced\nRemove and clear methods\nIn order to remove a value stored in the preferences under a key a method to perform the removal can be annotated with the @RemoveMethod annotation. The key can be supplied in two ways;\nThe key can be provided thorough an argument in the method, using either a String or an int in the case of the key being specified in an XML resource.\npublic interface RemoveMethodExample {\n    \n    @RemoveMethod\n    void remove(String key);\n    \n    @RemoveMethod\n    void remove(int keyRes);\n}\n\n\/\/ usage\npreferences.remove(\"key_username\");\npreferences.remove(R.string.key_password);\nOr a value-specific remove method can be defined with the help of the @KeyByString or @KeyByResource annotations.\npublic interface RemoveMethodExample {\n    \n    @KeyByString(\"key_username\")\n    @RemoveMethod\n    void removeUsername();\n    \n    @KeyByResource(R.string.key_password)\n    @RemoveMethod\n    void removePassword();\n}\n\n\/\/ usage\npreferences.removeUsername();\npreferences.removePassword();\nClearing all values stored in the preferences can be done by annotating a method with the @ClearMethod annotation.\npublic interface ClearMethodExample {\n    \n    @ClearMethod\n    void clear();\n}\n\n\/\/ usage\npreferences.clear();\nChange listeners\nCallbacks can be received when a preference value changes through the use of the OnPreferenceValueChangedListener interface. The listeners need to be parametrised with the type which is used for the value whose changes we would like to listen for. For example, if we would like to listen to changes to the password (from previous examples) then we could define the listener as\nOnPreferenceValueChangedListener<String> listener = new OnPreferenceValueChangedListener<String>() {\n    @Override\n    public void onChanged(String newValue) {\n        \/\/ do something with newValue\n    }\n}\nTo register this listener a method for registering the listener would need to be defined using the @RegisterChangeListenerMethod annotation in the interface which gets passed to StoreBox.create(). Likewise, for unregistering a listener the method needs to be annotated with @UnregisterChangeListenerMethod instead. The @KeyByString or @KeyByResource annotation also needs to be used to specify which value we are interested in.\npublic interface ChangeListenerExample {\n    \n    @KeyByString(\"key_password\")\n    @RegisterChangeListenerMethod\n    void registerPasswordListener(OnPreferenceValueChangedListener<String> listener);\n    \n    @KeyByString(\"key_password\")\n    @UnregisterChangeListenerMethod\n    void unregisterPasswordListener(OnPreferenceValueChangedListener<String> listener);\n}\nIf you would like to listen for changes to a custom type then the @TypeAdapter annotation will need to be added to the method in order to tell StoreBox how the value should be adapted when retrieving it from the preferences.\nMore than one listener can be registered and unregistered at a time by changing the method definitions in the interface to use variable arguments.\n\/\/ annotations omitted\nvoid registerPasswordListeners(OnPreferenceValueChangedListener<String>... listeners);\nCaution: StoreBox does not store strong references to the listeners. A strong reference must be kept to the listener for as long as the listener will be required, otherwise it will be susceptible to garbage collection.\nChaining calls\nWith Android's SharedPreferences.Editor class it is possible to keep chaining put methods as each returns back the SharedPreferences.Editor instance. StoreBox allows the same functionality. All that needs to be done is to change the set\/remove method definitions to either return interface type itself or SharedPreferences.Editor.\npublic interface ChainingExample {\n    \n    @KeyByString(\"key_username\")\n    ChainingExample setUsername(String value);\n    \n    @KeyByString(\"key_password\")\n    ChainingExample setPassword(String value);\n    \n    @KeyByString(\"key_country\")\n    ChainingExample removeCountry();\n}\nAnd calls can be chained as\npreferences.setUsername(\"Joe\").setPassword(\"jOe\").removeCountry();\nForwarding calls\nIf you would like to access methods from the SharedPreferences or SharedPreferences.Editor, you can do that by extending your interface from either of the above (or even both).\npublic interface ForwardingExample extends SharedPreferences, SharedPreferences.Editor {\n    \n    \/\/ method definitions here\n}\nAnd the methods from either of the extended interfaces will be callable.\nString username = preferences.getString(\"key_username\", \"\");\npreferences.putString(\"key_username\", \"Joe\").apply();\nSave modes\nChanges to preferences can normally be saved on Android either through apply() or commit(). Which method gets used can be customised in StoreBox through the use of the @SaveOption annotation.\nUnlike any of the previous annotations @SaveOption can be used to annotate both the interface as well as individual set\/remove methods, however an annotation at method-level will take precedence over an interface annotation.\n@SaveOption(SaveMode.APPLY)\npublic interface SaveModeExample {\n    \/\/ key annotations omitted\n    \n    void setUsername(String value); \/\/ will save using apply()\n    \n    @SaveOption(SaveMode.COMMIT)\n    void setPassword(String value); \/\/ will save using commit()\n    \n    @SaveOption(SaveMode.COMMIT)\n    void removeUsername(); \/\/ will persist using commit()\n}\nVersioning\nStoreBox supports versioning of preferences through the use of the @PreferencesVersion interface-level annotation, in a similar fashion to Android's SQLiteOpenHelper. This functionality may be required in the case when the schema of the preferences needs to be changed, such as when a key or type of a preference changes, an enum constant is added\/renamed\/removed, or a class which is being stored in the preferences changes internally. The @PreferencesVersion annotation needs to be added to the interface which will be used with StoreBox.create().\nBy default, without the @PreferencesVersion annotation, the version used is assumed to be 0. The first time a change is required the version for the annotation should be set to 1, with the value being incremented for any subsequent changes. To provide the logic for handling version upgrades a handler class extending from PreferencesVersionHandler needs to be specified.\n@PreferencesVersion(version = 1, handler = MyPreferencesVersionHandler.class)\npublic interface MyPreferences {\n    \n    \/\/ method definitions here\n}\n\npublic class MyPreferencesVersionHandler extends PreferencesVersionHandler {\n    \n    @Override\n    protected void onUpgrade(\n            SharedPreferences prefs,\n            SharedPreferences.Editor editor,\n            int oldVersion,\n            int newVersion) {\n        \n        \/\/ logic for handling upgrades\n    }\n}\nFor an initial upgrade from 0 to 1 the onUpgrade method will be called with oldVersion = 0 and newVersion = 1. If the version of the preferences would be updated to 2, then the handler would be called with oldVersion = 1 and newVersion = 2. If however an application update using version 1 was skipped, then onUpgrade would be called with oldVersion = 0 and newVersion = 2, which means that handling the intermediate upgrade between versions 1 and 2 would be required. Calling apply() or commit() on the editor is not required after changes are made, as StoreBox will take care of this when saving the new version value into the preferences. Take a look here for an example of how upgrades could be handled.\nThe versions are also independent of each other, and apply only to specific preference files. For example, you could have a shared preferences with version X, activity A preferences with version Y, and activity B preferences with version Z. Or none at all, if versioning is not needed.\nObtaining a more customised instance at run-time\nAs previously described you can build an instance of your interface using StoreBox.create(), however if you'd like to override at run-time any annotations you can use StoreBox.Builder and apply different options.\nMyPreferences preferences =\n        new StoreBox.Builder(context, MyPreferences.class)\n        .preferencesMode(PreferencesMode.MULTI_PROCESS)\n        .build()\nDefaults\nGiven the minimum amount of details provided to the interface and method definitions through the use of StoreBox's annotations, the following defaults will get used:\n\nPreferences type: Default shared preferences\nPreferences mode: Private\nSave mode: Apply\nDefault value mode: Empty\n\nProguard\nIf you are using ProGuard add the following lines to your configuration.\n-dontwarn net.jodah.typetools.TypeResolver\n-keep class net.orange_box.storebox.** { *; }\n-keepattributes *Annotation*,Exceptions,InnerClasses,Signature\n\n\nContributing\nAny contributions thorough pull requests as well as raised issues (bugs are rated just as highly as features!) will be welcome and highly appreciated.\nIf you would like to submit a pull request please make sure to do so against the develop branch, and please follow a similar code style to the one used in the existing code base. Running the tests before and after any changes is highly recommended, just as is adding new test cases.\nContributors\n\ncr5315\n\nLicense\nCopyright 2015 Martin Bella\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","302":"StoreBox\nAndroid library for streamlining SharedPreferences.\n\n\n \n\nContents\n\nOverview\nAdding to a project\nInterface and creation\nGet and set methods\nDefaults for get methods\nCustom types\nPreference types\nAdvanced\nRemove and clear methods\nChange listeners\nChaining calls\nForwarding calls\nSave modes\nVersioning\nBuilder & Defaults\nProGuard\nContributing\nLicense\n\nOverview\nStoreBox is an annotation-based library for interacting with Android's SharedPreferences, with the aim take out the the how and where parts of retrieving\/storing values and instead focus on the more important what part.\nNormally when retrieving or storing values we need to know two pieces of information during each call: the key and the type.\nString username = preferences.getString(\"key_username\", null);\npreferences.edit().putString(\"key_date_of_birth\", \"30\/09\/2004\").apply(); \/\/ should this be a String or a long?\nWith StoreBox the operations above can be changed into pre-defined methods with improved semantics.\n\/\/ definition\npublic interface MyPreferences {\n    \n    @KeyByString(\"key_username\")\n    String getUsername();\n    \n    @KeyByString(\"key_date_of_birth\")\n    void setDateOfBirth(String value);\n}\n\n\/\/ usage\nMyPreferences preferences = StoreBox.create(context, MyPreferences.class);\nString username = preferences.getUsername();\npreferences.setDateOfBirth(\"30\/09\/2004\");\nThe caller now doesn't need to worry about the key, neither about what type the values are stored under. The only important part that needs to be taken into consideration is what is done with the values, whether that is storing them for later, showing them to the user in the UI, or just changing application behaviour.\nRead on to find out more details about how StoreBox can be used and how it can be added to an Android project.\nAdding to a project\nStoreBox can be used in Android projects using minimum SDK version 10 and newer (Android 2.3+).\nJAR\nv1.4.0 JAR\nv1.4.0 JavaDoc JAR\nGradle\ncompile 'net.orange-box.storebox:storebox-lib:1.4.0'\n\nMaven\n<dependency>\n  <groupId>net.orange-box.storebox<\/groupId>\n  <artifactId>storebox-lib<\/artifactId>\n  <version>1.4.0<\/version>\n<\/dependency>\n\nDefining an interface and bringing it to life\nSimply create a new interface class in your IDE or a text editor, give it an access modifier which suits its use, and name it as appropriate.\npublic interface MyPreferences {\n    \n}\nNow you're ready to use StoreBox.create() to obtain an instance.\nMyPreferences instance = StoreBox.create(context, MyPreferences.class);\nAdding get and set methods\nIf you would like to add a getter just add a method to the interface which returns a value and make sure to annotate it using @KeyByString or @KeyByResource.\n@KeyByString(\"key_nickname\")\nString getNickname();\n\n@KeyByResource(R.string.key_notifications)\nboolean shouldShowNotifications();\nAdding a setter is just as easy. The same annotations will need to be used as for getter methods, but now our method will return nothing and will have to provide a parameter for supplying the value that should be saved.\n@KeyByString(\"key_nickname\")\nvoid setNickname(String value)\n\n@KeyByResource(R.string.key_notifications)\nvoid setNotifications(boolean value)\nSpecifying defaults for get methods\nThis can be achieved in two ways, through an argument or by using an annotation.\nFor the first option the following will work.\n@KeyByString(\"key_phone_number\")\nString getPhoneNumber(String defValue);\nAnd using an annotation referencing a default found in the XML resources.\n@KeyByString(\"key_phone_number\")\n@DefaultValue(R.string.default_phone_number)\nString getPhoneNumber();\nFor some types, such as long, which cannot be added to the resources an integer resource may be used instead.\n@KeyByString(\"key_refresh_interval\")\n@DefaultValue(R.integer.default_refresh_interval)\nlong getRefreshInterval();\nStoring and retrieving custom types\nSaving custom types, which are not understood by Android's SharedPreferences, can be supported through the use of type adapters. A type adapter implementation can be provided by extending from one of the following classes:\n\nBaseBooleanTypeAdapter for storing as a Boolean\nBaseFloatTypeAdapter for storing as a Float and so on...\nBaseIntegerTypeAdapter\nBaseLongTypeAdapter\nBaseStringTypeAdapter\nBaseStringSetTypeAdapter (only supported on API11 and newer)\n\nTelling StoreBox which type adapter should be used can be done by adding the @TypeAdapter annotation to the get and set methods.\n@KeyByString(\"key_region\")\n@TypeAdapter(RegionTypeAdapter.class)\nRegion getRegion();\n\n@KeyByString(\"key_region\")\n@TypeAdapter(RegionTypeAdapter.class)\nvoid setRegion(Region value);\nWhich type adapter needs to be extended depends on the use case. Take a look at the DateTypeAdapter, UriTypeAdapter, and CustomClassListTypeAdapter for some examples. It is worth noting that in the last example Gson is being used for serialising the type, as opposed to writing a custom implementation. Gson is not used internally by StoreBox, as such if you wish to use Gson for a type adapter you will need to add it to your project as a dependency.\nThe following types will work out of the box, so type adapters don't need to be provided for them:\n\nDate\nDouble\nEnum\nUri\n\nDisclaimer: APIs around type adapters may change in the future, as I will keep looking for a less verbose way of achieving the same goal without requiring the use of Gson.\nOpening different types of preferences\nIn all of the examples so far details about what preferences are opened and how have been omitted.\nWithout any annotation the default shared preferences will be used, but the @DefaultSharedPreferences annotation can be added to the interface definition for explicitness. Likewise, @ActivityPreferences or @FilePreferences can be used to respectively open preferences private to an activity or to open preferences using a file name.\nThe mode with which the preferences should be opened can also be specified, although this option is not supported by all the types.\n@ActivityPreferences(mode = PreferencesMode.MULTI_PROCESS)\npublic interface WelcomeActivityPreferences {\n    \n    \/\/ method definitions here...\n}\nAdvanced\nRemove and clear methods\nIn order to remove a value stored in the preferences under a key a method to perform the removal can be annotated with the @RemoveMethod annotation. The key can be supplied in two ways;\nThe key can be provided thorough an argument in the method, using either a String or an int in the case of the key being specified in an XML resource.\npublic interface RemoveMethodExample {\n    \n    @RemoveMethod\n    void remove(String key);\n    \n    @RemoveMethod\n    void remove(int keyRes);\n}\n\n\/\/ usage\npreferences.remove(\"key_username\");\npreferences.remove(R.string.key_password);\nOr a value-specific remove method can be defined with the help of the @KeyByString or @KeyByResource annotations.\npublic interface RemoveMethodExample {\n    \n    @KeyByString(\"key_username\")\n    @RemoveMethod\n    void removeUsername();\n    \n    @KeyByResource(R.string.key_password)\n    @RemoveMethod\n    void removePassword();\n}\n\n\/\/ usage\npreferences.removeUsername();\npreferences.removePassword();\nClearing all values stored in the preferences can be done by annotating a method with the @ClearMethod annotation.\npublic interface ClearMethodExample {\n    \n    @ClearMethod\n    void clear();\n}\n\n\/\/ usage\npreferences.clear();\nChange listeners\nCallbacks can be received when a preference value changes through the use of the OnPreferenceValueChangedListener interface. The listeners need to be parametrised with the type which is used for the value whose changes we would like to listen for. For example, if we would like to listen to changes to the password (from previous examples) then we could define the listener as\nOnPreferenceValueChangedListener<String> listener = new OnPreferenceValueChangedListener<String>() {\n    @Override\n    public void onChanged(String newValue) {\n        \/\/ do something with newValue\n    }\n}\nTo register this listener a method for registering the listener would need to be defined using the @RegisterChangeListenerMethod annotation in the interface which gets passed to StoreBox.create(). Likewise, for unregistering a listener the method needs to be annotated with @UnregisterChangeListenerMethod instead. The @KeyByString or @KeyByResource annotation also needs to be used to specify which value we are interested in.\npublic interface ChangeListenerExample {\n    \n    @KeyByString(\"key_password\")\n    @RegisterChangeListenerMethod\n    void registerPasswordListener(OnPreferenceValueChangedListener<String> listener);\n    \n    @KeyByString(\"key_password\")\n    @UnregisterChangeListenerMethod\n    void unregisterPasswordListener(OnPreferenceValueChangedListener<String> listener);\n}\nIf you would like to listen for changes to a custom type then the @TypeAdapter annotation will need to be added to the method in order to tell StoreBox how the value should be adapted when retrieving it from the preferences.\nMore than one listener can be registered and unregistered at a time by changing the method definitions in the interface to use variable arguments.\n\/\/ annotations omitted\nvoid registerPasswordListeners(OnPreferenceValueChangedListener<String>... listeners);\nCaution: StoreBox does not store strong references to the listeners. A strong reference must be kept to the listener for as long as the listener will be required, otherwise it will be susceptible to garbage collection.\nChaining calls\nWith Android's SharedPreferences.Editor class it is possible to keep chaining put methods as each returns back the SharedPreferences.Editor instance. StoreBox allows the same functionality. All that needs to be done is to change the set\/remove method definitions to either return interface type itself or SharedPreferences.Editor.\npublic interface ChainingExample {\n    \n    @KeyByString(\"key_username\")\n    ChainingExample setUsername(String value);\n    \n    @KeyByString(\"key_password\")\n    ChainingExample setPassword(String value);\n    \n    @KeyByString(\"key_country\")\n    ChainingExample removeCountry();\n}\nAnd calls can be chained as\npreferences.setUsername(\"Joe\").setPassword(\"jOe\").removeCountry();\nForwarding calls\nIf you would like to access methods from the SharedPreferences or SharedPreferences.Editor, you can do that by extending your interface from either of the above (or even both).\npublic interface ForwardingExample extends SharedPreferences, SharedPreferences.Editor {\n    \n    \/\/ method definitions here\n}\nAnd the methods from either of the extended interfaces will be callable.\nString username = preferences.getString(\"key_username\", \"\");\npreferences.putString(\"key_username\", \"Joe\").apply();\nSave modes\nChanges to preferences can normally be saved on Android either through apply() or commit(). Which method gets used can be customised in StoreBox through the use of the @SaveOption annotation.\nUnlike any of the previous annotations @SaveOption can be used to annotate both the interface as well as individual set\/remove methods, however an annotation at method-level will take precedence over an interface annotation.\n@SaveOption(SaveMode.APPLY)\npublic interface SaveModeExample {\n    \/\/ key annotations omitted\n    \n    void setUsername(String value); \/\/ will save using apply()\n    \n    @SaveOption(SaveMode.COMMIT)\n    void setPassword(String value); \/\/ will save using commit()\n    \n    @SaveOption(SaveMode.COMMIT)\n    void removeUsername(); \/\/ will persist using commit()\n}\nVersioning\nStoreBox supports versioning of preferences through the use of the @PreferencesVersion interface-level annotation, in a similar fashion to Android's SQLiteOpenHelper. This functionality may be required in the case when the schema of the preferences needs to be changed, such as when a key or type of a preference changes, an enum constant is added\/renamed\/removed, or a class which is being stored in the preferences changes internally. The @PreferencesVersion annotation needs to be added to the interface which will be used with StoreBox.create().\nBy default, without the @PreferencesVersion annotation, the version used is assumed to be 0. The first time a change is required the version for the annotation should be set to 1, with the value being incremented for any subsequent changes. To provide the logic for handling version upgrades a handler class extending from PreferencesVersionHandler needs to be specified.\n@PreferencesVersion(version = 1, handler = MyPreferencesVersionHandler.class)\npublic interface MyPreferences {\n    \n    \/\/ method definitions here\n}\n\npublic class MyPreferencesVersionHandler extends PreferencesVersionHandler {\n    \n    @Override\n    protected void onUpgrade(\n            SharedPreferences prefs,\n            SharedPreferences.Editor editor,\n            int oldVersion,\n            int newVersion) {\n        \n        \/\/ logic for handling upgrades\n    }\n}\nFor an initial upgrade from 0 to 1 the onUpgrade method will be called with oldVersion = 0 and newVersion = 1. If the version of the preferences would be updated to 2, then the handler would be called with oldVersion = 1 and newVersion = 2. If however an application update using version 1 was skipped, then onUpgrade would be called with oldVersion = 0 and newVersion = 2, which means that handling the intermediate upgrade between versions 1 and 2 would be required. Calling apply() or commit() on the editor is not required after changes are made, as StoreBox will take care of this when saving the new version value into the preferences. Take a look here for an example of how upgrades could be handled.\nThe versions are also independent of each other, and apply only to specific preference files. For example, you could have a shared preferences with version X, activity A preferences with version Y, and activity B preferences with version Z. Or none at all, if versioning is not needed.\nObtaining a more customised instance at run-time\nAs previously described you can build an instance of your interface using StoreBox.create(), however if you'd like to override at run-time any annotations you can use StoreBox.Builder and apply different options.\nMyPreferences preferences =\n        new StoreBox.Builder(context, MyPreferences.class)\n        .preferencesMode(PreferencesMode.MULTI_PROCESS)\n        .build()\nDefaults\nGiven the minimum amount of details provided to the interface and method definitions through the use of StoreBox's annotations, the following defaults will get used:\n\nPreferences type: Default shared preferences\nPreferences mode: Private\nSave mode: Apply\nDefault value mode: Empty\n\nProguard\nIf you are using ProGuard add the following lines to your configuration.\n-dontwarn net.jodah.typetools.TypeResolver\n-keep class net.orange_box.storebox.** { *; }\n-keepattributes *Annotation*,Exceptions,InnerClasses,Signature\n\n\nContributing\nAny contributions thorough pull requests as well as raised issues (bugs are rated just as highly as features!) will be welcome and highly appreciated.\nIf you would like to submit a pull request please make sure to do so against the develop branch, and please follow a similar code style to the one used in the existing code base. Running the tests before and after any changes is highly recommended, just as is adding new test cases.\nContributors\n\ncr5315\n\nLicense\nCopyright 2015 Martin Bella\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","303":"AndFixDemo\n\u5173\u4e8e\u4e3a\u4ec0\u4e48\u8981\u9009\u62e9AndFix,\u53ef\u4ee5\u53bb\u770b\u770b\u6211\u4e4b\u524d\u5199\u7684\u6587\u7ae0AndFix\u7684\u6289\u62e9\nAndroidStudio\uff0cAndFix\u7684Demo\uff08\u5b98\u65b9\u7684Demo\u662fEclipse\u7684\uff09\nAPK\u6211\u5df2\u7ecf\u751f\u6210\u597d\u4e86\uff0c\u53e6\u52a0\u57fa\u672c\u4f7f\u7528\u6559\u7a0b\uff08\u5b98\u65b9\u7684\u6587\u6863\u5199\u7684\u4e0d\u662f\u5f88\u6e05\u6670\uff09\n\u53cd\u6b63\u6211\u4ec0\u4e48\u90fd\u51c6\u5907\u597d\u4e86\uff0c\u60f3\u4f53\u9a8c\u4e00\u4e0b\u7684\u53ef\u4ee5\u770b\u4e00\u4e0b\uff01\uff01\n\u76f4\u63a5\u4f53\u9a8c\n\u6211\u5df2\u7ecf\u51c6\u5907\u597d\u4e86\u4e00\u5207\uff0c\u662f\u4e0d\u662f\u8d85\u8d34\u5fc3\uff01\n\u5b89\u88c5 1.apk\ncd apkpatch-1.0.3\nadb install 1.apk\n\nRun \u8d77\u6765\uff0c\u67e5\u770b log\uff0cTag\u4e3a MainActivity\uff0c\u53ef\u4ee5\u770b\u5230\u65e5\u5fd7\uff1a\nMainActivity: a\nMainActivity: 0\nMainActivity: 10\n\npush \u8865\u4e01\u5230SD\u5361\n\u628aapatch push\u5230SD\u5361 sdcard \u662f\u6211\u7684\u8def\u5f84 \u6839\u636e\u81ea\u5df1\u7684\u8def\u5f84\u6765\nadb push patch\/out.apatch \/sdcard\/\n\n\u91cd\u65b0\u5b89\u88c51.apk \u5e76run\nadb install -r 1.apk\n\nRun \u8d77\u6765\uff0cTag \u4e3aAPP\u6709\u5982\u4e0b\u65e5\u5fd7\u8f93\u51fa\uff1a\nApp: inited.\nApp: apatch loaded.\nApp: apatch:\/storage\/emulated\/0\/out.apatch added.\n\nMainActivity \u65e5\u5fd7 :\nMainActivity: b \uff08\u4e0d\u518d\u662fa\u800c\u662fb\u4e86\uff01\uff09\nMainActivity: 0\nMainActivity: 10\n\n\u540c\u65f6\uff0cTAG\u4e3aeuler \u4e5f\u6709fix success\u7684\u4fe1\u606f\u8f93\u51fa\n\u8868\u793a\u6210\u529f\u5566\uff01~\n\u751f\u6210apatch\n\u5b98\u65b9\u6ca1\u6709\u7ed9\u5177\u4f53\u7684\u5b9e\u4f8b\uff0c\u8fd9\u91cc\u6211\u63d0\u4f9b\u4e00\u4e2a\u5b9e\u4f8b\uff1a\n\u6ce8\u610f\u6211\u7684\u662fMAC,\u6240\u4ee5\u662f.sh\u6587\u4ef6\uff0cdebugkeystore\u662fAS\u81ea\u5e26\u7684\uff0c2.apk\u662f\u65b0\u7684apk\uff0c1\u662f\u65e7\u7684\n.\/apkpatch.sh -f 2.apk -t 1.apk -k debug.keystore -p android -a androiddebugkey -e android -o patch\n\n\u6267\u884c\u5b8c\u4f1a\u6709\u5982\u4e0b\u65e5\u5fd7\u8f93\u51fa\uff0c\u5e76\u4e14\u5728patch\u76ee\u5f55\u4e0b\u751f\u6210\u8865\u4e01\u6587\u4ef6\nadd modified Method:Ljava\/lang\/String;  a(Ljava\/lang\/String;)  in Class:Lme\/yifeiyuan\/andfixdemo\/A;\nadd new Method:Ljava\/lang\/String;  c()  in Class:Lme\/yifeiyuan\/andfixdemo\/A;\nadd modified Method:I  b(Ljava\/lang\/String;Ljava\/lang\/String;)  in Class:Lme\/yifeiyuan\/andfixdemo\/A;\n\n\n","304":"What are Dexx Collections?\nDexx Collections are a port of Scala's immutable, persistent collection classes to pure Java.\nPersistent in the context of functional data structures means the data structure preserves the previous version of itself when modified. This means any reference to a collection is effectively immutable. However, modifications can be made by returning a new version of the data structure, leaving the original structure unchanged.\nHere's an example using Dexx's Sets (examples are in Kotlin for conciseness, but the collections are pure Java):\nval set1 = Sets.of(1, 2, 3)\nval set2 = set1.add(4)\nval set3 = set1.remove(1)\nprintln(set1) \/\/ Prints Set(1, 2, 3)\nprintln(set2) \/\/ Prints Set(1, 2, 3, 4)\nprintln(set3) \/\/ Prints Set(2, 3)\nFrom the above example we can see that although we've made modifications to set1 to create set2 and set3, the contents of set1 remain unchanged.\nNote: There's now first class support for Kotlin - see the kollection module README for more information.\nWhy port?\nScala's collections can be directly used from Java, but the resulting code is far from idiomatic. Scala's standard library is also large and binary incompatible between versions.\nSecondly, a pure Java implementation of functional persistent collections is usable from not only Java, but other JVM languages that interoperate with Java such as Kotlin, Ceylon or GWT. In fact, the collections have been specifically designed for use with Kotlin.\nOverview\nThe diagram below shows Dexx's class hierarchy (interfaces are in blue and concrete implementations are in green).\n\nNote that the interfaces such as Map, Set and List are not related to the java.util equivalents as persistent collections require all modification methods such as add and remove to return a new collection instance.\nStatus\n\nAll collections have been implemented\nHashSet, TreeSet, HashMap, TreeMap and Vector are ports from Scala\nConsList and ArrayList have been written from scratch.\nHelper classes for construction and adapters to java.util collections are available\nTest coverage is fairly comprehensive: 95% line and 90% branch at present\n\nDependencies\n\nThere are no runtime dependencies\nJetBrain's annotations (@NotNull and @Nullable) are used in the source to support Kotlin's nullable types, but they are not required at runtime.\nThe tests are written in Kotlin, but again this is not a runtime dependency\n\nRoadmap\n\nExplore annotating methods that return a new collection with @CheckReturnValue\nto allow static verification of collection usage.\nActive development is essentially complete. Further work is expected to be bug fixes and refinements.\n\nRelease Notes\n\n0.7:\n\nFixes #11 - a balancing error in red black trees\n\n\n0.6:\n\nAdded OSGI metadata (thanks ajs6f)\nMake internal fields final (thanks mkull)\nPerformance improvement to first and last of TreeMap & TreeSet (thanks mkull)\n\n\n0.5:\n\nUpdated to 1.0.0\nAdded toImmutableMap() conversions from existing Maps\n\n\n0.4:\n\nUpdated to 1.0.0-rc-1036\nRemoved accidental assertJ compile dependency in kollection (thanks @brianegan)\n\n\n0.3.1:\n\nAdded a native Kotlin api in the kollection module\nConverted the build to gradle from maven\nRenamed dexx-collections artifact to collection\n\n\n0.2:\n\nAdd LinkedLists support with ConsList as the default implementation\nAdd ArrayList as an alternative IndexedList implementation\nFormalise the Builder contract and enforce at runtime\n\n\n0.1:\n\nIncludes ports of Scala's HashSet, TreeSet, HashMap, TreeMap and Vector\n\n\n\nLicense\nThis project is licensed under a MIT license. Portions ported from Scala are Scala's 3-clause BSD license.\nUsage\nAdding to your project\nVersion 0.7 has been released and is available in Maven Central here. You can use it via the following gradle dependency:\n'com.github.andrewoma.dexx:collection:0.7' \/\/ For Java\n'com.github.andrewoma.dexx:kollection:0.7' \/\/ For Kotlin\nConstructing collections\nEach of the leaf interfaces (Set, SortedSet, Map, SortedMap, IndexedList and LinkedList) have\nassociated companion classes with static methods for construction.\nThe companion class uses the plural form of the interface. e.g. Set has a companion class of Sets.\nTo build a collection from a fixed number of elements, use the overloaded of() methods. e.g.\nval set = Sets.of(1, 2, 3)\nTo build a collection from a java.util collection, use the copyOf() methods. e.g.\nval set = Sets.copyOf(javaCollection)\nBuilders should be used when incrementally constructing a collection. This allows for more efficient structures\nto be used internally during construction. In the case of LinkedList, using a builder is important as LinkedList does not support appending without copying the entire collection.\nval builder = Sets.builder<Int>()\nfor (i in 1..100) {\n    builder.add(i)\n}\nval set = builder.build()\nViewing as java.util collections\nUnfortunately, the java.util collection interfaces are not compatible with persistent collections as\nmodifications such as add() must return a new collection instance, leaving the original untouched.\nHowever, all collections can be viewed as an immutable form of their java.util equivalent by using the\nthe as...() methods.\nval javaSet = Sets.of(1, 2, 3).asSet() \/\/ Now a java.util.Set\nWhere are filter(), map() and friends?\nSuch transformations are deliberately not supported:\n\n\nIn JDK versions < 1.8, using a functional style is ugly and not recommended.\nSee Excessive use of Guava's functional programming idioms can lead to verbose, confusing, unreadable, and inefficient code.\n\n\nIn Kotlin and JDK 1.8, the platform provides transformations that can be used on the collections for free.\nAdding another set of transformations directly to the collections (with subtly different semantics) seems harmful.\n\n\nHere's an example of using lazy evaluation in a functional style with Kotlin:\nval set = SortedSets.of(1, 2, 3, 4, 5, 6).asSequence()\n        .filter { it % 2 == 0 }\n        .map { \"$it is even\" }\n        .take(2)\n        .toImmutableSet()\n\nassertEquals(SortedSets.of(\"2 is even\", \"4 is even\"), set)\nThe example above uses Kotlins in-built extension function that converts any Iterable into a Sequence.\nIt also uses the following extension functions to add Sequence<T>.toImmutableSet() to cleanly convert the sequence\nback into a Dexx Collection.\nfun <T, R> Sequence<T>.build(builder: Builder<T, R>): R {\n    this.forEach { builder.add(it) }\n    return builder.build()\n}\n\nfun <T> Sequence<T>.toImmutableSet(): SortedSet<T> = build(SortedSets.builder<T>())\nPerformance\nBenchmarking is still a work in progress (all the warnings about JVM benchmarks apply). The results so far\nrunning on Mac OS X 10.11.1 x86_64 with JDK 1.8.0_65 (Oracle Corporation 25.65-b01) are here.\nMy conclusions so far are that the collections perform adequately to be used as a drop-in replacement\nfor the majority of use cases. While slower, slow is generally referring to millions of operations per second.\nIn general, mutating methods incur a overhead of 2-5 times that of java.util equivalents and\nreading operations are 1-1.5 time slower.\n@ptitjes Has done some more rigorous benchmarks here: https:\/\/github.com\/ptitjes\/benchmark-immutables\/blob\/master\/results\/2016-10-02-23:56:36.pdf\nDevelopment\n\nDexx is built with gradle. Use .\/gradlew install to build and install into your local repository.\nTo run the benchmarks, use .\/gradlew check -PdexxTestMode=BENCHMARK --info | grep '^    BENCHMARK:'.\nTo generate coverage reports, use:\n\n.\/gradlew :collection:clean :collection:check :collection:jacocoTestReport\n open collection\/build\/jacocoHtml\/index.html\n\n\nBy default, a quick version of tests are run. Getting better test coverage of Vectors requires large\ncollections. To run tests with complete coverage use: .\/gradlew -PdexxTestMode=COMPLETE :collection:clean :collection:check :collection:jacocoTestReport\n\nMethod counts\nFor android developers, here are method counts:\n\n'com.github.andrewoma.dexx:collection:0.6' = 1036 methods\n'com.github.andrewoma.dexx:kollection:0.6' = 1213 methods\n\n\n","305":" Gank\n\u5e72\u8d27\u96c6\u4e2d\u8425Android\u5ba2\u6237\u7aef\uff01\n\n\u6570\u636e\u6765\u6e90\uff1a\n\u672c\u5e94\u7528\u6240\u6709\u6570\u636e\u5747\u6765\u81ea\u5e72\u8d27\u96c6\u4e2d\u8425\u7684\u5f00\u653eAPI\u3002\n\u5e94\u7528\u7b80\u4ecb\uff1a\n\u5e72\u8d27\u96c6\u4e2d\u8425\u5b89\u5353\u5ba2\u6237\u7aef\uff0c\u968f\u65f6\u968f\u5730\u4e86\u89e3\u6280\u672f\u6700\u65b0\u52a8\u6001(MeiZi)\u3002\u9664\u5468\u672b\u90e8\u5206\u8282\u5047\u65e5\u5916\uff0c\u6bcf\u65e5\u66f4\u65b0\u4e00\u4e2a\u7f8e\u56fe\uff0c\u4e00\u4e2a\u5c0f\u89c6\u9891\uff0c\u4ee5\u53ca\u5404\u7c7b\u6280\u672f\u6700\u65b0\u6587\u7ae0\u3002\n\u754c\u9762\u9884\u89c8\uff1a\n\n\u4e0b\u8f7d\u5e94\u7528\uff1a\n\n\u3010\u626b\u63cf\u4e8c\u7ef4\u7801\u6216\u70b9\u51fb\u8fd9\u91cc\u83b7\u53d6\u5e94\u7528\u3011\n\u8bbe\u8ba1\u7406\u5ff5\uff1a\n\u672c\u5e94\u7528\u79c9\u627f\u7b80\u6d01\u800c\u4e0d\u7b80\u5355\u7684\u7406\u5ff5\u8fdb\u884c\u8bbe\u8ba1\uff0c\u6ca1\u6709\u4efb\u4f55\u591a\u4f59\u7684\u529f\u80fd\uff1a\n\u4f8b\u5982\uff1a\u6ca1\u6709\u5e38\u89c1\u7684\u5185\u7f6e\u6d4f\u89c8\u5668\uff0c\u6ca1\u6709\u6536\u85cf\uff0c\u867d\u7136\u5185\u7f6e\u6d4f\u89c8\u5668\u975e\u5e38\u5e38\u89c1\uff0c\u7136\u800c\u5728\u6b64\u5904\u662f\u5f88\u7cdf\u7cd5\u7684\u4e00\u79cd\u4f53\u9a8c\uff0c\u56e0\u4e3a\u5927\u90e8\u5206\u7684\u6280\u672f\u76f8\u5173\u6587\u7ae0\u5e76\u4e0d\u9002\u5408\u5728\u624b\u673a\u7aef\u9605\u8bfb\uff0c\u770b\u5230\u6bd4\u8f83\u597d\u7684\u6587\u7ae0\u901a\u5e38\u9700\u8981\u6536\u85cf\u4e0b\u6765\u7559\u5f85\u5728\u7535\u8111\u4e0a\u8be6\u7ec6\u9605\u89c8\uff0c\u7136\u800c\u56e0\u4e3a\u9119\u4eba\u6ca1\u6709\u5f00\u53d1\u7535\u8111\u5ba2\u6237\u7aef\u7684\u80fd\u529b\uff0c\u6240\u4ee5\u5373\u4f7f\u5728\u624b\u673a\u7aef \u6536\u85cf\u4e86\u4e5f\u4e0d\u80fd\u63d0\u4f9b\u66f4\u826f\u597d\u7684\u4f53\u9a8c\u6548\u679c\uff0c\u56e0\u6b64\u91c7\u7528\u4e86\u5916\u7f6e\u6d4f\u89c8\u5668\uff0c\u76ee\u524d\u6765\u8bf4\u5f88\u591a\u6d4f\u89c8\u5668\u90fd\u5177\u6709\u8d26\u53f7\u767b\u9646\u529f\u80fd\uff0c\u4f8b\u5982360\u6d4f\u89c8\u5668\uff0cChrome\u7b49\uff0c\u8d26\u53f7\u767b\u9646\u540e\u53ef\u65b9\u4fbf\u7684\u5c06\u624b\u673a\u6536\u85cf\u7684\u7f51\u9875\u540c\u6b65\u5230\u7535\u8111\u6d4f\u89c8\u5668\u4e0a\uff0c\u53cd\u800c\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u4f53\u9a8c\u6548\u679c\u3002\n\u4ee5\u65b9\u4fbf\u4e3a\u4e3b\uff0c\u6ce8\u91cd\u7ec6\u8282\uff1a\n\u5bf9\u5185\u5bb9\u4e5f\u91c7\u7528\u4e86\u78c1\u76d8\u7f13\u5b58\uff0c\u5728\u6ca1\u7f51\u72b6\u6001\u4e0b\u4f9d\u65e7\u53ef\u4ee5\u770b\u5230\u5185\u5bb9(\u6ca1\u7f51\u53ea\u80fd\u770b\u5230\u7f13\u5b58\u7684\u59b9\u5b50)\uff0c\u56fe\u7247\u90e8\u5206\u5219\u91c7\u7528\u53cc\u7f13\u5b58\uff0c\u8282\u7701\u6d41\u91cf\uff0c\u63d0\u9ad8\u52a0\u8f7d\u901f\u5ea6\uff0c\u5e76\u4e14\u56fe\u7247\u90e8\u5206\u4e5f\u6709\u7528\u6d4f\u89c8\u5668\u6253\u5f00\u9009\u9879\uff0c\u914d\u5408Chrome\u548cGoogle\uff0c\u53ef\u4ee5\u5728\u6d4f\u89c8\u5668\u4e2d\u641c\u7d22\u56fe\u7247\u51fa\u5904\uff0c\u9ad8\u6e05\u5927\u56fe\uff0c\u76f8\u4f3c\u7684\u59b9\u5b50\u56fe\u7b49.......\u8ba9\u4e00\u5f20\u59b9\u5b50\u53d8\u6210\u4e00\u7cfb\u5217\u59b9\u5b50\ud83d\udd1e\n\u5efa\u8bae\u914d\u5408Chrome\u4f7f\u7528:\n\nChrome\n\u79d1\u5b66\u4e0a\u7f51\n\n\u5f00\u53d1\u8ba1\u5212\uff1a\n\u57fa\u4e8e\u7528\u6237\u7684\u53cd\u9988\uff0c\u4fee\u590d\u5b8c\u5584\u6216\u8005\u6dfb\u52a0\u4ee5\u4e0b\u529f\u80fd(\u6b22\u8fce\u5728Issues\u53cd\u9988)\uff1a\n\n \u89e3\u51b3\u90e8\u5206\u673a\u578b\u95ea\u9000\u6216\u8005\u5d29\u6e83\u95ee\u9898\n \u5b8c\u5584\u59b9\u5b50\u9875\u9762\u56fe\u7247\u52a0\u8f7d\u7f13\u5b58\u95ee\u9898\n \u6dfb\u52a0\u5de6\u53f3\u6ed1\u52a8\u5207\u6362\u59b9\u5b50\u56fe\u7247\u529f\u80fd\n \u7f13\u5b58WebView\uff0c\u63d0\u4f9b\u79bb\u7ebf\u9605\u8bfb\n\n\u7279\u522b\u9e23\u8c22\uff1a\n\n@\u4ee3\u7801\u5bb6\n@Mr.Simple\n#Retrofit\n#ASimpleCache\n#Android-Universal-Image-Loader\n\u4f17\u591a\u7684\u5f00\u6e90\u515a\n\n\nAbout Me\n\u4f5c\u8005\u5fae\u535a: @GcsSloop\n  \nLicense\nCopyright (c) 2016 GcsSloop\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","306":"Metaball-Menu\nA menu consisting of icons (ImageViews) and metaball bouncing selection to give a blob effect. Inspired by Material design\n\nScreenShot\n\n\nUsage\nThe usage is fairly easy.\n<com.metaballmenu.widgets.MetaballMenu\n    xmlns:android=\"http:\/\/schemas.android.com\/apk\/res\/android\"\n    xmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n    xmlns:tools=\"http:\/\/schemas.android.com\/tools\"\n    android:id=\"@+id\/metaball_menu\"\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:gravity=\"center\"\n    android:orientation=\"horizontal\"\n    android:padding=\"10dp\"\n    android:layout_gravity=\"center\"\n    app:backgroundColor=\"@android:color\/holo_purple\"\n    app:metaballColor=\"@android:color\/white\"\n    app:drawablePadding=\"10dp\"\n    app:backgroundShapeRadius=\"30dp\"\n    app:needsElevation=\"true\">\n \n    <ImageView\n        android:id=\"@+id\/menuitem1\"\n        android:layout_width=\"0dp\"\n        android:layout_height=\"wrap_content\"\n        android:layout_weight=\"1\"\n        android:layout_margin=\"5dp\"\n        android:padding=\"5dp\"\n        android:src=\"@mipmap\/card\"\n        \/>\n<\/com.metaballmenu.widgets.MetaballMenu>\nI have used an Imageview. But any view can be used to obtain the effect.\nCheck out the uploaded project for usage.\nThe code is based on the following references:\n\nMetaball Loading by Dodola - Thanks for the Path draw functions on Android\nPaperJS Metaball Example\nCalvin Metcalf\n\n\nLICENSE\nThe MIT License (MIT)\nCopyright (c) 2015 Melvin Lobo\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n","307":"trackr backend\nWhat is it?\ntrackr is an application to track petty much everything that is going on in your company.\nKeep track of vacations, sick days, invoices and many more.\ntrackr comes with a Java-based backend and a frontend written in AngularJS. This project is the Java\/Spring based\nbackend, a stateless REST API with either OAuth2 or basic authentication.\nYou can read all about trackr in our developer blog:\n\nArchitecture and Backend\nTesting\nMail Approvals with Spring Integration\nFrontend\nFile Downloads with AngularJS\nProcesses and Tools\n\nFor the API documentation just go here.\nThere is also a Vagrant project building the whole application over here.\nHow to start\nIf you just want to mess around with the API a bit the default configuration is very sensible and has no external dependencies (well, except Java).\nIf you have gradle, just run\ngradle run\n\nIf you don't have gradle and want to use the wrapper run\n.\/gradlew run\n# or\ngradlew.bat run\n\nIf you want to start from your IDE, i.e. for debugging open the class Trackr and start the main method.\nTo verify it works you can use curl. The users don't have a password in this configuration, so just press enter when curl asks for one. If you don't like the usernames\nchange them in import.sql.\ncurl --user moritz.schulze@techdev.de localhost:8080\n\nThe default config uses port 8080, if that is used on your system you can add\nserver:\n    port: $port\n\nto the top of the application.yaml and choose a port that you want for $port.\nProfiles\ntrackr has a lot of Spring profiles to add\/switch features.\n\n\n\nprofile\ndescription\nnotes\n\n\n\n\nin-memory-database\nuses a H2 database, creates the schema with hibernate\nexcluse with real-database\n\n\nreal-database\nuses a configurable database, executes flyway\nexclusive with in-memory-database\n\n\nhttp-basic\nprotects the API with HTTP basic authentication\nexclusive with oauth\n\n\noauth\nprotects the API as a OAuth2 resource server\nexclusive with http-basic. Database for OAuth2 tokens needed.\n\n\ngranular-security\nroles and per endpoint security\n\n\n\ngmail\nsends mail with Gmail and enables mail receiving\nwhen off, does not receive mails and uses a logging mail sender.\n\n\ndev\ninitialize the database with data.sql\n\n\n\nprod\nJust some different settings for our production env\n\n\n\n\nTake a look in the application.yaml to see what properties these profiles need.\nThe default profiles are in-memory-database,dev,granular-security,http-basic. If you want to use other profiles, there are several possible ways.\n\nYou can change the spring.profiles.active value in application.yaml\nIf you use gradle run you can prepend (example) SPRING_PROFILES_ACTIVE=dev,gmail,real-database. You can also use this to overwrite e.g. the port with SERVER_PORT=8000.\nIf you run from your IDE, you can add --spring.profiles.active=dev,gmail,real-database as program arguments to the run configuration.\n\nPlease refer to the Spring Boot Reference for more information.\nThe oauth profile\nThe oauth profile marks the trackr backend as a OAuth2 resource server, that means access is only possible with a valid access token issued by an authorization server. We use a\nJDBC token store, so valid tokens need to be put there. Please take a look at our (soon to be open sourced) techdev portal to see how we do this.\nThe granular-security profile\nWhen this is not selected, to access the API the user needs to be authenticated. With granular security the access to some endpoints depend on the role of the user or even the\nid of the user. In trackr, the id of a user is the email address of the belonging employee.\nWhen the oauth profile is switched off, all users have the role ROLE_ADMIN. When oauth is on, the roles must be stored in the access token.\nTake a look at the @PreAuthorize and @PostAuthorize annotations in the code to see what this will activate.\nHow to build\nJust run\ngradle build\n\n(or use the wrapper if you don't have gradle installed). The JAR file will be in build\/libs and can just be run with java -jar. The application.yaml file has to be in the\nworking directory where the java command was issued.\n","308":"RetroDagger\nAndroid project using MVP + Dagger2 + Retrofit.\nThis code was created to support this article: https:\/\/medium.com\/@franpulido\/retrofit-dagger-mvp-305ac2cf646c\n","309":"Mephisto\n\nDependencies\nAll Platforms\nReSwitched unicorn fork:\ngit clone git@github.com:reswitched\/unicorn.git\ncd unicorn\nUNICORN_ARCHS=\"aarch64\" .\/make.sh\nsudo .\/make.sh install\n\nPython packages:\npip install -r requirements.txt\n\nUbuntu\nInstall Clang 5 from the LLVM PPA: http:\/\/apt.llvm.org\/\nYou may need to update libc++ as well, if you get tuple errors.\nOSX\nInstall llvm (will take a while)\nbrew install llvm --HEAD\n\nPatch Makefile\ndiff --git a\/Makefile b\/Makefile\nindex e4c921b..4d53420 100644\n--- a\/Makefile\n+++ b\/Makefile\n@@ -1,4 +1,4 @@\n-CC := clang++-4.0\n+CC := clang++\nRunning\nMuch like the original CageTheUnicorn, the default use of Mephisto is via the load files.  Create a directory, e.g. ns23, and then copy in the NSO file(s).  Create a file inside this, called load.meph with the following format:\n(load-nso \"main\" 0x7100000000)\n(run-from 0x7100000000)\n\nRunning it is then as simple as:\n.\/ctu ns23\n\nAlternatively, you can pass a single NSO file on the command line:\n.\/ctu --load-nso ns23\/main\n\nSee help for other info, e.g. enabling GDB support.\nRun through Docker\nFirst build the docker image, this may take some time\ndocker build -t reswitched\/mephisto .\nTo run Mephisto it needs access to your NSO\/NRO files, make sure to bind mount the location into the container.\nExample:\ndocker run -ti --rm -p 24689:24689 -v $HOME:$HOME -u $UID reswitched\/mephisto --load-nro $HOME\/Coding\/libtransistor\/build\/test\/test_helloworld.nro\nYou can also create a bash alias.\nalias ctu='docker run -ti --rm -p 24689:24689 -v $HOME:$HOME -u $UID reswitched\/mephisto'\n\nNow you can simply run ctu with your desired arguments.\nExample:\nctu --load-nro $HOME\/Coding\/libtransistor\/build\/test\/test_helloworld.nro\n","310":"Warning!\nThis library has been discarded. A new library named WeeESP8266 is recommended, which is more easy-to-use for users.\nWeeESP8266 can be downloaded at https:\/\/github.com\/itead\/ITEADLIB_Arduino_WeeESP8266.\nESP8266 library\nWhen you use with UNO board, uncomment the follow line in uartWIFI.h.\n#define UNO\n\nWhen you use with MEGA board, uncomment the follow line in uartWIFI.h.\n#define MEGA\n\nConnection:\nWhen you use it with UNO board, the connection should be like these:\nESP8266_TX->D0\nESP8266_RX->D1\nESP8266_CHPD->3.3V\nESP8266_VCC->3.3V\nESP8266_GND->GND\nFTDI_RX->D3\t\t\t\/\/The baud rate of software serial can't be higher that 19200, so we use software serial as a debug port\nFTDI_TX->D2\nWhen you use it with MEGA board, the connection should be like these:\nESP8266_TX->RX1(D19)\nESP8266_RX->TX1(D18)\nESP8266_CH_PD->3.3V\nESP8266_VCC->3.3V\nESP8266_GND->GND\nWhen you want to output the debug information, please use DebugSerial. For example,\nDebugSerial.println(\"hello\");\nAttention\nNote1:\tThe size of message from ESP8266 is too big for arduino sometimes, so the library can't receive the whole buffer because\nthe size of the hardware serial buffer which is defined in HardwareSerial.h is too small.\nOpen the file from \\arduino\\hardware\\arduino\\avr\\cores\\arduino\\HardwareSerial.h.\nSee the follow line in the HardwareSerial.h file.\n#define SERIAL_BUFFER_SIZE 64\n\nThe default size of the buffer is 64. Change it into a bigger number, like 256 or more.\nThe SRAM size of mega is bigger than UNO's, so it is better to use MEGA board to communicate with ESP8266.\nBUG: When you use this library and receive the http package, it might miss some characters because the library can't process so much data in the same time.\nCreated by Stan Lee(Lizq@iteadstudio.com)\n2014\/10\/8\nModified version\nV1.0\treleased the first version of ESP8266 library\n","311":"This repository and the associated neural2d.net website are no longer supported and will be going away soon. Neural2d started as a fun weekend project, then gained some momentum from excellent code contributions from a number of programmers. But I\u2019m at a place in my life where I need to turn my attention elsewhere. I would refer users and educators to the many excellent neural net resources that are more pedagogically effective and more technologically current than this project.\nThe most recent neural2d user manual is shown below for historical reference.\nA sincere thank-you to everyone who particpiated in neural2d, and best wishes in your neural-netting.\n\u2014 Dave\nNeural2d - Neural Net Simulator User Manual\nFeatures\n\nOptimized for 2D input data\nNeuron layers can be abstracted as 1D or 2D arrangements of neurons\nInput data can binary or text\nNetwork topology is defined in a text file\nNeurons in layers can be fully or sparsely connected\nSelectable transfer function per layer\nAdjustable or automatic training rate (eta)\nOptional momentum (alpha) and regularization (lambda)\nConvolution filtering and convolution networking\nStandalone console program\nSimple, heavily-commented code, suitable for prototyping, learning, and experimentation\nOptional web-browser-based GUI controller\nGraphic visualizations of hidden-layer data\nNo dependencies! Just C++11 (and POSIX networking for the optional webserver interface)\n\nDocument Contents\nOverview\nRequirements\nCompiling the source\nHow to run the digits demo\nHow to run the XOR example\nGUI interface\nHow to use your own data\nThe 2D in neural2d\nConvolution filtering\nConvolution networking and pooling\nLayer depth\nTopology config file format\nTopology config file examples\nHow-do-I X?\n\nHow do I run the command-line program?\nHow do I run the GUI interface?\nHow do I disable the GUI interface?\nHow do I use my own data instead of the digits images?\nHow do I use a trained net on new data?\nHow do I train on the MNIST handwritten digits data set?\nHow do I change the learning rate parameter?\nAre the output neurons binary or floating point?\nHow do I use a different transfer function?\nHow do I define a convolution filter?\nHow do I define convolution networking and pooling?\nHow do the color image pixels get converted to floating point for the input layer?\nHow can I use .jpg and .png images as inputs to the net?\nHow do I find my way through the source code?\nWhy does the net error rate stay high? Why doesn't my net learn?\nWhat other parameters do I need to know about?\n\nOverview\nNeural2d is a standalone console program with optional HTTP web\ninterface written in C++. It's a backpropagation neural net simulator,\nwith features that make it easy to think of your input data as either\none-dimensional or two-dimensional. You specify a network topology in\na text file (topology.txt). The input data to the neural is specified\nin a text file (inputData.txt). The inputData.txt file can contain the\nactual input values for all the input samples, or it can contain a list\nof .bmp or .dat files that contain the input data in binary form.\nNeural2d is for educational purposes. It's not production-quality code,\nand it's not burdened with a lot of bling. It's just heavily-commented\nneural net code that you can take apart and see how it works. Or modify\nit and experiment with new ideas. Or extract the functions you need and\nembed them into your own project.\nIf you're not using the optional GUI interface, then neural2d has no\ndependencies other than a conforming C++11 compiler. If using the GUI\ninterface, you'll need to link with a standard POSIX sockets networking\nlibrary.\nRequirements\n\nC++-11 compiler\n\ne.g., g++ 4.7 on Linux or Cygwin, Visual Studio 2013 on Windows\n\n\nPOSIX sockets (only needed if compiling the optional GUI)\n\ne.g., Cygwin on Windows\n\n\nCMake 2.8.12 or later\nCompiles and runs on Linux, Windows, and probably Mac\n\nCompiling the source\nWe use CMake to configure the build system. First get the source code\nfrom the Gitub repository. If using the command line, the command is:\n git clone https:\/\/github.com\/davidrmiller\/neural2d\n\nThat will put the source code tree into a directory named neural2d.\nCompiling with CMake graphical interface\nIf you are using the CMake graphical interface, run it and set the\n\"source\" directory to the neural2d top directory, and set the binary\noutput directory to a build directory under that (you must create the\nbuild directory), then click Configure and Generate. Uncheck WEBSERVER\nif you don't want to compile the optional GUI. Here is what it looks like:\n\nCompiling with CMake command line interface\nIf you are using CMake from the command line, cd to the neural2d top\nlevel directory, make a build directory, then run cmake from there:\ngit clone https:\/\/github.com\/davidrmiller\/neural2d\ncd neural2d\nmkdir build\ncd build\ncmake ..\nmake\n\nThere is no \"install\" step. After the neural2d program is compiled,\nyou can execute it or open the project file from the build directory.\nOn Windows, by default CMake generates a Microsoft Visual Studio project\nfile in the build directory. On Linux and Cygwin, CMake generates\na Makefile that you can use to compile neural2d. You can specify a\ndifferent CMake generator with the -G option, for example:\n cmake -G \"Visual Studio 14 2015\" ..\n\nTo get a list of available CMake generators:\n cmake --help\n\nIf you get errors when compiling the integrated webserver, you can\nbuild neural2d without webserver support by running CMake with the\n-DWEBSERVER=OFF option, like this:\n cmake -DWEBSERVER=OFF ..\n\nHow to run the digits demo\nOn systems using Makefiles, in the build directory, execute:\nmake test\n\nThis will do several things: it will compile the\nneural2d program if necessary; it will expand the\narchive named image\/digits\/digits.zip into 5000 individual\nimages;\nand it will then train the neural net to classify those digit images.\nThe input data, or \"training set,\" consists of images of numeric\ndigits. The first 50 look like these:\n\nThe images are 32x32 pixels each, stored in .bmp format. In this demo,\nthe neural net is configured to have 32x32 input neurons, and 10 output\nneurons. The net is trained to classify the digits in the images and\nto indicate the answer by driving the corresponding output neuron to a\nhigh level.\nOnce the net is sufficiently trained, all the connection weights are\nsaved in a file named \"weights.txt\".\nIf you are not using Makefiles, you will need to expand the archive in\nimages\/digits, then invoke the neural2d program like this:\n neural2d ..\/images\/digits\/topology.txt ..\/images\/digits\/inputData.txt weights.txt\n\nHow to run the XOR example\nOn systems using Makefiles, in the build directory, execute:\n make test-xor\n\nFor more information about the XOR example, see\nthis wiki page.\nGUI interface (optional)\nFirst, launch the neural2d console program in a command window with the -p option:\n .\/neural2d topology.txt inputData.txt weights.txt -p\n\nThe -p option causes the neural2d program to wait for a command before\nstarting the training. The screen will look something like this:\n\nAt this point, the neural2d console program is paused and waiting for\na command to continue. Using any web browser, open:\n http:\/\/localhost:24080\n\nA GUI interface will appear that looks like:\n\nPress Resume to start the neural net training. It will automatically\npause when the average error rate falls below a certain threshold (or\nwhen you press Pause). You now have a trained net. You can press Save\nWeights to save the weights for later use.\nSee the neural2d wiki for\ndesign notes on the web interface.\nVisualizations\nAt the bottom of the GUI window, a drop-down box shows the visualization\noptions that are available for your network topology, as shown\nbelow. There will be options to display the activation (the outputs)\nof any 2D layer of neurons 3x3 or larger, and convolution kernels of\nsize 3x3 or larger. Visualization images appear at the bottom of the\nGUI. You can mouse-over the images to zoom in.\n\nHow to use your own data\nInput data to the neural net can be specified in text or binary format. If\nyou want to specify input values in text format, just list the input\nvalues in the inputData.txt file. If you're inputting from .bmp or .dat\nbinary files, then list those filenames in inputData.txt. Details are\nexplained below.\nText input format\nTo specify input data as text, prepare a text file, typically called\ninputData.txt, with one input sample per line. The input values go inside\ncurly braces. If the data is for training, the target output values must\nbe specified on the same line after the input values. In each sample, the\ninput values are given in a linear list, regardless whether the neural\nnet input layer is one-dimensional or two-dimensional. For example, if\nyour neural net has 9 input neurons of any arrangement and two output\nneurons, then the inputData.txt file for training will look like this\nformat:\n{ 0.32 0.98 0.12 0.44 0.98 0.22 0.34 0.72 0.84 } -1  1   \n{ 1.00 0.43 0.19 0.83 0.97 0.87 0.75 0.47 0.92 }  1  1   \n{ 0.87 0.75 0.47 0.92 1.00 0.43 0.19 0.83 0.97 } -1 -1   \n{ 0.34 0.83 0.97 0.87 0.75 0.43 0.19 0.47 0.92 } -1  1   \netc.. . .  \n\nBinary input formats\nThere are two binary options -- .bmp image files, and .dat data\nfiles. First, prepare your .bmp or .dat files, one sample per file. If\nusing .bmp files, the number of pixels in each image should equal the\nnumber of input neurons in your neural net. If using .dat files, each\nfile must contain a linear list of input values, with the same number\nof values as the number of input neurons in your neural net.\nNext, prepare an input data configuration file, called inputData.txt\ncontaining a list of the .bmp or .dat filenames, one per line. If the\ndata is for training, then also list the target output values for each\ninput sample after the filename, like this:\nimages\/thumbnails\/test-918.bmp -1 1 -1 -1 -1 -1 -1 -1 -1 -1\nimages\/thumbnails\/test-919.bmp -1 -1 -1 -1 -1 -1 -1 -1 1 -1\nimages\/thumbnails\/test-920.bmp -1 -1 -1 -1 -1 -1 1 -1 -1 -1\nimages\/thumbnails\/test-921.bmp -1 -1 -1 -1 -1 1 -1 -1 -1 -1\netc. . .\n\nThe path and filename cannot contain any spaces.\nThe path_prefix directive can be used to specify a string to be attached\nto the front of all subsequent filenames, or until the next path_prefix\ndirective. For example, the previous example could be written:\n path_prefix = images\/thumbnails\/\n test-918.bmp\n test-919.bmp\n test-920.bmp\n test-921.bmp\n etc. . .\n\nFor more information on the .bmp file format, see this Wikipedia\narticle..\nFor more information on the neural2d .dat binary format, see this wiki\npage.\nand the design\nnotes.\nTopology file\nIn addition to the input data config file (inputData.txt), you'll also\nneed a topology config file (typically named topology.txt by default)\nto define your neural net topology (the number and arrangement\nof neurons and connections). Its format is described in a later\nsection.  A typical one looks like this example:\ninput size 32x32  \nlayer1 size 32x32 from input radius 8x8  \nlayer2 size 16x16 from layer1  \noutput size 1x10 from layer2  \n\nThen run neuron2d (optionally with the web browser interface) and\nexperiment with the parameters until the net is adequately trained,\nthen save the weights in a file for later use.\nIf you run the web interface, you can change the global parameters\nfrom the GUI while the neural2d program is running. If you run the\nneural2d console program without the GUI interface, there is no way\nto interact with it while running. Instead, you'll need to examine and\nmodify the parameters in the code at the top of the files neural2d.cpp\nand neural2d-core.cpp.\nThe 2D in neural2d\nIn a simple traditional neural net model, the neurons are arranged in\na column in each layer:\n\nIn neural2d, you can specify a rectangular arrangement of neurons in\neach layer, such as:\n\nThe neurons can be sparsely connected to mimic how retinal neurons\nare connected in biological brains. For example, if a radius of \"1x1\"\nis specified in the topology config file, each neuron on the right\n(destination) layer will connect to a circular patch of neurons in the\nleft (source) layer as shown here (only a single neuron on the right\nside is shown connected in this picture so you can see what's going on,\nbut imagine all of them connected in the same pattern):\n\nThe pattern that is projected onto the source layer is elliptical. (Layers\nconfigured as convolution filters work slightly differently; see the\nlater section about convolution filtering.)\nHere are some projected connection patterns for various radii:\nradius 0x0\n\nradius 1x1\n\nradius 2x2\n\nradius 3x1\n\nConvolution filtering\nAny layer other than the input layer can be configured as a convolution\nfilter layer by specifying a convolve-matrix specification for the\nlayer in the topology config file.  The neurons are still called neurons,\nbut their operation differs in the following ways:\n\n\nThe connection pattern to the source layer is defined by the convolution\nmatrix (kernel) dimensions (not by a radius parameter)\n\n\nThe connection weights are initialized from the convolution matrix,\nand are constant throughout the life of the net.\n\n\nThe transfer function is automatically set to the identity function\n(not by a tf parameter).\n\n\nFor example, the following line in the topology config file defines a\n3x3 convolution matrix for shrarpening the source layer:\n layerConv1 size 64x64 from input convolve {{0,-1,0},{-1,5,-1},{0,-1,0}}\n\nWhen a convolution matrix is specified for a layer, you cannot also\nspecify a radius parameter for that layer, as the convolution matrix\nsize determines the size and shape of the rectangle of neurons in the\nsource layer. You also cannot also specify a tf parameter, because the\ntransfer function on a convolution layer is automatically set to be the\nidentity function.\nThe elements of the convolution matrix are stored as connection weights\nto the source neurons. Connection weights on convolution layers are not\nupdated by the back propagation algorithm, so they remain constant for\nthe life of the net.\nFor illustrations of various convolution kernels, see\nthis Wikipedia article\nIn the following illustration, the topology config file defines a\nconvolution filter with a 2x2 kernel that is applied to the input layer,\nthen the results are combined with a reduced-resolution fully-connected\npathway. The blue connections in the picture are the convolution\nconnections; the green connections are regular neural connections:\ninput size 8x8\nlayerConvolve size 8x8 from input convolve {{-1,2},{-1,2}}\nlayerReducedRes size 4x4 from input\noutput size 2 from layerConvolve\noutput size 2 from layerReducedRes\n\n\nConvolution networking and pooling\nA convolution network\nlayer is\nlike a convolution filter layer, except that the kernel participates in\nbackprop training, and everything inside the layer is replicated N times\nto train N separate kernels. A convolution network layer is said to have\ndepth N. A convolution network layer has depth * X * Y neurons.\nAny layer other than the input or output layer can be configured as\na convolution networking layer by specifying a layer depth > 1, and\nspecifying the kernel size with a convolve parameter. For example,\nto train 40 kernels of size 7x7 on an input image of 64x64 pixels:\n  input size 64x64\n  layerConv size 40*64x64 from input convolve 7x7\n  . . .\n\nA pooling layer\ndown-samples the previous layer by finding the average or maximum in\npatches of source neurons.  A pooling layer is defined in the topology\nconfig file by specifying a pool parameter on a layer.\nIn the topology config syntax, the pool parameter requires the argument\n\"avg\" or \"max\" followed by the operator size, For example, in a\nconvolution network pipeline of depth 20, you might have these layers:\n  input size 64x64\n  layerConv size 20*64x64 from input convolve 5x5\n  layerPool size 20*16x16 from layerConv pool max 4x4\n  . . .\n\nLayer depth\nAll layers have a depth, whether explicit or implicit. Layer depth is\nspecified in the topology config file in the layer size parameter by an\ninteger and asterisk before the layer size. If the depth is not specified,\nit defaults to one. For example:\n\n\nsize 10*64x64 means 64x64 neurons, depth 10\n\n\nsize 64x64 means 64x64 neurons, depth 1\n\n\nsize 1*64x64 also means 64x64 neurons, depth 1\n\n\nsize 10*64 means 64x1 neurons, depth 10 (the Y dimension defaults to 1)\n\n\nThe primary purpose of layer depth is to allow convolution network\nlayers to train multiple kernels.  However, the concept of layer depth\nis generalized in neural2d, allowing any layer to have any depth and\nconnect to any other layer of any kind with any depth.\nThe way two layers are connected depends on the relationship of the\nsource and destination layer depths as shown below:\n\n\n\nRelationship\nHow connected\n\n\n\n\nsrc depth == dst depth\nconnect only to the corresponding depth in source\n\n\nsrc depth != dst depth\nfully connect across all depths\n\n\n\nTopology config file format\nHere is the grammar of the topology config file:\n\nlayer-name parameters := parameter [ parameters ]\n\n\nparameters := parameter [ parameters ]\n\n\nparameter :=\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0input | output | layername\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0size dxy-spec\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0from layer-name\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0channel channel-spec\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0radius xy-spec\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tf transfer-function-spec\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0convolve filter-spec\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0convolve xy-spec\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pool { max | avg } xy-spec\n\n\ndxy-spec := [ integer * ] integer [ x integer ]\n\n\nxy-spec := integer [ x integer ]\n\n\nchannel-spec := R | G | B | BW\n\n\ntransfer-function-spec := tanh | logistic | linear | ramp | gaussian | relu\n\n\nfilter-spec := same {{,},{,}} syntax used for array initialization in C, C#, VB, Java, etc.\n\nRules:\n\n\nComment lines that begin with \"#\" and blank lines are ignored.\n\n\nThe first layer defined must be named \"input\".\n\n\nThe last layer defined must be named \"output\".\n\n\nThe hidden layers can be named anything beginning with \"layer\".\n\n\nThe argument for \"from\" must be a layer already defined.\n\n\nThe color channel parameter can be specified only on the input layer.\n\n\nIf a size parameter is omitted, the size is copied from the layer\nspecified in the from parameter.\n\n\nA radius parameter cannot be used on the same line with a convolve\nor pool parameter.\n\n\nThe same layer name can be defined multiple times with different\n\"from\" parameters.  This allows source neurons from more than one layer\nto be combined in one destination layer. The source layers can be any\nsize, but the repeated (the destination) layer must have the same size\nin each specification. For example, in the following, layerCombined is\nsize 16x16 and takes inputs from two source layers of different sizes:\n\n\n     input size 128x128  \n     layerVertical size 32x32 from input radius 1x8  \n     layerHorizontal size 16x16 from input radius 8x1  \n     layerCombined from layerHorizontal          <= assumes size 16x16 from the source layer  \n     layerCombined size 16x16 from layerVertical <= repeated destination, must match 16x16  \n     output size 1 from layerCombined  \n\n\nIn the xy-spec  and in the X,Y part of the dxy-spec, you may\nspecify one or two dimensions.  Spaces are not allowed in the size\nspec. If only one dimension is given, the other is assumed to be 1.\nFor example:\n\n\n\"8x8\" means 64 neurons in an 8 x 8 arrangement.\n\"8x1\" means a row of 8 neurons\n\"1x8\" means a column of 8 neurons.\n\"8\" means the same as \"8x1\"\n\nTopology config file examples\nHere are a few complete topology config files and the nets they specify.\ninput size 4x4\nlayer1 size 3x3 from input\nlayer2 size 2x2 from layer1\noutput size 1 from layer2\n\n\ninput size 4x4\nlayer1 size 1x4 from input\nlayer2 size 3x1 from layer1\noutput size 1 from layer2\n\n\ninput size 4x4\noutput size 4x1 from input radius 0x2\n\n\ninput size 16x16\nlayer1 size 4x4 from input radius 1x1\noutput size 7x1 from layer1\n\n\n# In the picture that follows, layerVertical is the set of 4 neurons\n# in the upper part of the picture, and layerHorizontal is the lower\n# set of 4 neurons.\n\ninput size 6x6\nlayerHorizontal size 2x2 from input radius 2x0\nlayerVertical size 2x2 from input radius 0x2\noutput size 1 from layerHorizontal\noutput size 1 from layerVertical\n\n\n# This example shows how vertical and horizontal image features can be\n# extracted through separate paths and combined in a subsequent layer.\n\ninput size 4x4\n\nlayerH1 size 1x4 from input radius 4x0\nlayerH2 size 1x4 from layerH1\nlayerH3 size 1x4 from layerH2\n\nlayerV1 size 4x1 from input radius 0x4\nlayerV2 size 4x1 from layerV1\nlayerV3 size 4x1 from layerV2\n\noutput size 2 from layerV3\noutput size 2 from layerH3\n\n\nHow-do-I X?\nHow do I run the command-line program?\nRun neural2d with three arguments specifying the topology configuration,\ninput data configuration, and where to store the weights if training\nsucceeds:\n .\/neural2d topology.txt inputData.txt weights.txt\n\nHow do I run the GUI interface?\nFirst launch the neural2d program with the -p option:\n .\/neural2d topology.txt inputData.txt weights.txt -p\n\nThen open a web browser and point it at\nhttp:\/\/localhost:24080 .\nIf your firewall complains, you may need to allow access to TCP port\n24080.\nHow do I disable the GUI interface?\nRun CMake with the -DWEBSERVER=OFF option. Or if you are using\nyour own home-grown Makefiles, you can define the preprocessor\nmacro DISABLE_WEBSERVER. For example, with gnu compilers, add\n-DDISABLE_WEBSERVER to the g++ command line. Alternatively, you can\nundefine the macro ENABLE_WEBSERVER in neural2d.h.\nWhen the web server is disabled, there is no remaining dependency on\nPOSIX sockets.\nAlso see the illustrations above.\nHow do I use my own data instead of the digits images?\nFor binary input data, create your own directory of BMP image\nfiles or .dat binary files, and an input data config file\n(inputData.txt) that follows the same format as shown in the\nexamples elsewhere. Then define a topology config\nfile with the appropriate number of network inputs and\noutputs, then run the neural2d program.\nOr if you don't want to use .bmp image files or .dat binary files for\ninput, make an input config file containing all the literal input values\nand the target output values.\nSee above for more information on the input formats.\nHow do I use a trained net on new data?\nIt's all about the weights file. After the net has been successfully\ntrained, save the internal connection weights in a weights file. That's\ntypically done in neural2d.cpp by calling the member function\nsaveWeights(filename).\nThe weights you saved can be loaded back into a neural net of the same\ntopology using the member function loadWeights(filename). Once the net\nhas been loaded with weights, it can be used applied to new data by\ncalling feedForward(). Prior to calling feedForward(), you'll want to\nset a couple of parameters:\n myNet.repeatInputSamples = false;\n myNet.reportEveryNth = 1;\n\nThis is normally done in neural2d.cpp.\nYou'll need to prepare a new input data config file (default name\ninputData.txt) that contains a list of only those new input data images\nthat you want the net to process.\nHow do I train on the MNIST handwritten digits data set?\nSee the instructions in the wiki.\nHow do I change the learning rate parameter?\nIn the command-line program, you can set the eta parameter or change it\nby directly setting the eta member of the Net object, like this:\n myNet.eta = 0.1;\n\nWhen using the web interface, you can change the eta parameter (and\nother parameters) in the GUI at any time, even while the network is busy\nprocessing input data.\nAlso see the Parameter\nList\nin the wiki.\nAre the output neurons binary or floating point?\nThey are interpreted in whatever manner you train them to be, but you\ncan only train the outputs to take values in the range that the transfer\nfunction is capable of producing.\nIf you're training a net to output binary values, it's best if you use\nthe maxima of the transfer function to represent the two binary values.\nFor example, when using the default tanh() transfer function, train\nthe outputs to be -1 and +1 for false and true. When using the logistic\ntransfer function, train the outputs to be 0 and 1.\nHow do I use a different transfer function?\nYou can add a \"tf\" parameter to any layer definition line in\nthe topology config file.  The argument to tf can be \"tanh\",\n\"logistic\", \"linear\", \"ramp\", \"gaussian\", or \"relu\".  The\ntransfer function you specify will be used by all the neurons\nin that layer.  Here are the graphs of the built-in transfer\nfunctions.\nIn the topology config file, the tf parameter is specified as in this\nexample:\n layerHidden1 size 64x64 from input radius 3x3 tf linear\n\nYou can add new transfer functions by following the examples in\nneural2d-core.cpp.  There are two places to change: first find where\ntransferFunctionTanh() is defined and add your new transfer function and\nits derivative there. Next, locate the constructor for class Neuron and\nadd a new else-if clause there, following the examples.\nHow do I define a convolution filter?\nIn the topology config file, any layer defined with a convolve\nparameter and a list of constant weights will operate as a convolution\nfilter applied to the source layer.  The syntax is of the form:\n layer2 size 64x64 from input convolve {{1,0,-1},{0,0,0},{-1,0,1}}\n\nAlso see above for more information.\nSee this article for the difference between\nconvolution filter and convolution networking.\nHow do I define convolution networking and pooling?\nIn the topology config file, define a layer with an X,Y size and a depth\n(number of kernels to train), and add a convolve parameter to specify\nthe kernel size. For example, to train 40 kernels of size 7x7 on an\ninput image of 64x64 pixels:\n input size 64x64  \n layerConv size 40*64x64 from input convolve 7x7\n . . .\n\nTo define a pooling layer, add a pool parameter, followed by the argument\n\"avg\" or \"max,\" followed by the operator size, e.g.:\n layerConv size 10*32x32 ...\n layerPool size 10*8x8 from layerConv pool max 4x4\n . . .\n\nAlso see above for more information.\nHow do the color image pixels get converted to floating point for the input layer?\nThat's in the ImageReaderBMP class in neural2d-core.cpp. The default\nversion provided converts each RGB pixel to a single floating point\nvalue in the range -1.0 to 1.0.\nBy default, the three color channels are converted to monochrome and\nnormalized to the range -1.0 to 1.0. That can be changed at runtime by\nsetting the colorChannel member of the Net object to R, G, B, or BW prior\nto calling feedForward(). E.g., to use only the green color channel of\nthe images, use:\nmyNet.colorChannel = NNet::G;\n\nThe color conversion can also be specified in the topology config file on\nthe line that defines the input layer by setting the \"channel\" parameter\nto R, G, B, or BW, e.g.:\ninput size 64x64 channel G\n\nThere is no conversion when inputting floating point data directly from a\n.dat file, or from literal values embedded in the input data config file.\nHow can I use .jpg and .png images as inputs to the net?\nCurrently only .bmp images files are supported. This is because the\nuncompressed BMP format is so simple that we can use simple, standard\nC\/C++ to read the image data without any dependencies on third-party\nimage libraries.\nTo support a new input file format, derive a new subclass\nfrom class ImageReader and implement its getData() member\nfollowing the examples of the existing image readers. For\nmore information on the input data readers, see the design\nnotes.\nHow can I find my way through the source code?\nHere is a little map of the important files:\n\nAlso see the class relationship diagram in the project root directory.\nWhy does the net error rate stay high? Why doesn't my net learn?\nNeural nets are finicky. Try different network topologies. Try starting\nwith a larger eta values and reduce it incrementally. It could also\nbe due to redundancy in the input data, or mislabeled target output\nvalues. Or you may need more training samples.\nWhat other parameters do I need to know about?\nCheck out the list of parameters in the\nwiki.\nLicenses\nThe neural2d program and its documentation are\ncopyrighted and licensed under the terms of the MIT\nlicense. See the LICENSE file for\nmore information.\nThe set of digits images in the images\/digits\/ subdirectory is released\nto the public domain.\n","312":"HttpClient for Spark Core | Arduino\nThis is a work in progress Http Client Library for the Spark Core. It is not ready for use other than for people who have very basic needs, or are willing to help with the development. Because of this, it is currently very verbose and makes heavy use of the serial connection so you can see what's going on. That said, if you are reasonably familiar with Arduino or embedded development you might find it useful. I am publishing it in this early stage mostly because I am myself just starting out with C++ and could use all the help I can get. If you find errors or bad code just let me know and I'll work on fixing it!\nThere are a couple other options that are probably better suited if you are using a vanilla Arduino and not a Spark Core. First is Arduino HTTP library from Adrian McEwen. It depends on the Arduino Ethernet API Library though, which may or may not make sense in your implementation. Second there is HTTPClient from Interactive Matter but it also depends on the same Arduino Ethernet Library. Both of these libraries are orders of magnitude more mature than this one. In the future, it might very well make more sense to reuse a lot of code from these other libraries but to get rid of the dependencies rather than reimplementing things again.\nExample usage\n#include \"application.h\"\n\n\/**\n* Declaring the variables.\n*\/\nunsigned int nextTime = 0;    \/\/ Next time to contact the server\nHttpClient http;\n\n\/\/ Headers currently need to be set at init, useful for API keys etc.\nhttp_header_t headers[] = {\n    \/\/  { \"Content-Type\", \"application\/json\" },\n    \/\/  { \"Accept\" , \"application\/json\" },\n    { \"Accept\" , \"*\/*\"},\n    { NULL, NULL } \/\/ NOTE: Always terminate headers will NULL\n};\n\nhttp_request_t request;\nhttp_response_t response;\n\nvoid setup() {\n    Serial.begin(9600);\n}\n\nvoid loop() {\n    if (nextTime > millis()) {\n        return;\n    }\n\n    Serial.println();\n    Serial.println(\"Application>\\tStart of Loop.\");\n    \/\/ Request path and body can be set at runtime or at setup.\n    request.hostname = \"www.timeapi.org\";\n    request.port = 80;\n    request.path = \"\/utc\/now\";\n\n    \/\/ The library also supports sending a body with your request:\n    \/\/request.body = \"{\\\"key\\\":\\\"value\\\"}\";\n\n    \/\/ Get request\n    http.get(request, response, headers);\n    Serial.print(\"Application>\\tResponse status: \");\n    Serial.println(response.status);\n\n    Serial.print(\"Application>\\tHTTP Response Body: \");\n    Serial.println(response.body);\n\n    nextTime = millis() + 10000;\n}\n\n\n","313":"Vinduino\nThe Vinduino project started from the necessity to manage irrigation in my small Southern California vineyard, by monitoring soil moisture at different depths and at several vineyard locations.\nSoil moisture monitoring systems have been around for decades, but they cost hundreds to thousands of dollars and -because these systems are proprietary- there will be ongoing cost to customize and maintain these systems. As a small vineyard owner, I needed something low cost and flexible.\nThe open source Arduino platform, together with low cost gypsum soil moisture and salinity sensors, provides all that. While I first envisioned the Vinduino project (Vineyard + Arduino) for my personal interest and needs, but now the scope has broadened to providing easy to use, open source, low cost solutions for agricultural irrigation management.\nSaving water is more important now than ever. The four year drought in California made everybody realize the importance of reducing water use. But it is not just California that is plagued by prolonged drought periods. Large agricultural areas in South America, India, China, and Africa suffer from continuing water shortage as well.\nThe Vinduino project provided the following results, published on the Vinduino blog, Hackaday, and Github:\n\nDIY calibrated gypsum soil moisture sensors (Watermark SS200 is also supported)\nHand held sensor reader (soil moisture, soil\/water salinity, water pressure)\nSolar powered remote sensor platform (Vinduino R3), available on Tindie.com\nOptions include:\n4 electrically separated inputs for soil moisture sensors\nWifi (ESP8266), Globalsat LM-210 private LoRa network module, Globalsat LM-513 module for LoRaWAN (The Things Network)\nIrrigation valve control, optional pressure sensor for valve operation feedback\nseveral options for temperature\/humidity sensors\nBuilt in solar battery charger\nBuilt in real time clock for precise irrigation timing\nGateway to connect multiple LoRa end nodes to the Internet via Wifi (Vinduino Gateway)\n\nAll publicly released programming and documentation is available for free under the GNU General Public License 3.0.\nFor non-DIY we offer commercial products at www.vinduino.com and https:\/\/www.amazon.com\/Vinduino-Agricultural-Irrigation-Monitoring-Starter\/dp\/B0828JCK49\nSupport web site: https:\/\/vinduino.freshdesk.com\/support\/home\n","314":"Vinduino\nThe Vinduino project started from the necessity to manage irrigation in my small Southern California vineyard, by monitoring soil moisture at different depths and at several vineyard locations.\nSoil moisture monitoring systems have been around for decades, but they cost hundreds to thousands of dollars and -because these systems are proprietary- there will be ongoing cost to customize and maintain these systems. As a small vineyard owner, I needed something low cost and flexible.\nThe open source Arduino platform, together with low cost gypsum soil moisture and salinity sensors, provides all that. While I first envisioned the Vinduino project (Vineyard + Arduino) for my personal interest and needs, but now the scope has broadened to providing easy to use, open source, low cost solutions for agricultural irrigation management.\nSaving water is more important now than ever. The four year drought in California made everybody realize the importance of reducing water use. But it is not just California that is plagued by prolonged drought periods. Large agricultural areas in South America, India, China, and Africa suffer from continuing water shortage as well.\nThe Vinduino project provided the following results, published on the Vinduino blog, Hackaday, and Github:\n\nDIY calibrated gypsum soil moisture sensors (Watermark SS200 is also supported)\nHand held sensor reader (soil moisture, soil\/water salinity, water pressure)\nSolar powered remote sensor platform (Vinduino R3), available on Tindie.com\nOptions include:\n4 electrically separated inputs for soil moisture sensors\nWifi (ESP8266), Globalsat LM-210 private LoRa network module, Globalsat LM-513 module for LoRaWAN (The Things Network)\nIrrigation valve control, optional pressure sensor for valve operation feedback\nseveral options for temperature\/humidity sensors\nBuilt in solar battery charger\nBuilt in real time clock for precise irrigation timing\nGateway to connect multiple LoRa end nodes to the Internet via Wifi (Vinduino Gateway)\n\nAll publicly released programming and documentation is available for free under the GNU General Public License 3.0.\nFor non-DIY we offer commercial products at www.vinduino.com and https:\/\/www.amazon.com\/Vinduino-Agricultural-Irrigation-Monitoring-Starter\/dp\/B0828JCK49\nSupport web site: https:\/\/vinduino.freshdesk.com\/support\/home\n","315":"HMIYC\n\n\n\nHunt Me If You Can is an UnrealEngine4 Battle Lan Game\nIt's a total 3d engraved version of AssassinWar\n\n\n\nThank you UE4, I made it!\nTo begin\nDownload ue4.10.4 and Visual Studio 2015\nOpen HuntMeIfYouCan.uproject and refresh Visual Studio Project\nHow To Play\nWASD walk\nleft mouse use skill\n1 or 2 switch skill\nget 5 kills you win\n\nDownload Windows-32bit\nFor some reasons.Content of character is only packaged in release version.\nHere is a new game engine I'm currectly working on Qin.js\n","316":"cocos2d-x-extensions\nHere is my repository for cocos2d-x extensions.\nPlease check Wiki for more information and documentation.\n","317":"Note: This is not actively maintained, please make an issue if you are interested in helping maintain this project.\ngrunt-nodemon\n\nRun nodemon as a grunt task for easy configuration and integration with the rest of your workflow\n\n  \nGetting Started\nIf you haven't used grunt before, be sure to check out the Getting Started guide, as it explains how to create a gruntfile as well as install and use grunt plugins. Once you're familiar with that process, install this plugin with this command:\nnpm install grunt-nodemon --save-dev\nThen add this line to your project's Gruntfile.js gruntfile:\ngrunt.loadNpmTasks('grunt-nodemon');\nDocumentation\nMinimal Usage\nThe minimal usage of grunt-nodemon runs with a script specified:\nnodemon: {\n  dev: {\n    script: 'index.js'\n  }\n}\nUsage with all available options set\nnodemon: {\n  dev: {\n    script: 'index.js',\n    options: {\n      args: ['dev'],\n      nodeArgs: ['--debug'],\n      callback: function (nodemon) {\n        nodemon.on('log', function (event) {\n          console.log(event.colour);\n        });\n      },\n      env: {\n        PORT: '8181'\n      },\n      cwd: __dirname,\n      ignore: ['node_modules\/**'],\n      ext: 'js,coffee',\n      watch: ['server'],\n      delay: 1000,\n      legacyWatch: true\n    }\n  },\n  exec: {\n    options: {\n      exec: 'less'\n    }\n  }\n}\nAdvanced Usage\nA common use case is to run nodemon with other tasks concurrently. It is also common to open a browser tab when starting a server, and reload that tab when the server code changes. These workflows can be achieved with the following config, which uses a custom options.callback function, and grunt-concurrent to run nodemon, node-inspector, and watch in a single terminal tab:\nconcurrent: {\n  dev: {\n    tasks: ['nodemon', 'node-inspector', 'watch'],\n    options: {\n      logConcurrentOutput: true\n    }\n  }\n},\nnodemon: {\n  dev: {\n    script: 'index.js',\n    options: {\n      nodeArgs: ['--debug'],\n      env: {\n        PORT: '5455'\n      },\n      \/\/ omit this property if you aren't serving HTML files and \n      \/\/ don't want to open a browser tab on start\n      callback: function (nodemon) {\n        nodemon.on('log', function (event) {\n          console.log(event.colour);\n        });\n        \n        \/\/ opens browser on initial server start\n        nodemon.on('config:update', function () {\n          \/\/ Delay before server listens on port\n          setTimeout(function() {\n            require('open')('http:\/\/localhost:5455');\n          }, 1000);\n        });\n\n        \/\/ refreshes browser when server reboots\n        nodemon.on('restart', function () {\n          \/\/ Delay before server listens on port\n          setTimeout(function() {\n            require('fs').writeFileSync('.rebooted', 'rebooted');\n          }, 1000);\n        });\n      }\n    }\n  }\n},\nwatch: {\n  server: {\n    files: ['.rebooted'],\n    options: {\n      livereload: true\n    }\n  } \n}\nNote that using the callback config above assumes you have open installed and are injecting a LiveReload script into your HTML file(s). You can use grunt-inject to inject the LiveReload script.\nRequired property\nscript\nType: String\nScript that nodemon runs and restarts when changes are detected.\nOptions\nargs\nType: Array of Strings\nList of arguments to be passed to your script.\nnodeArgs\nType: Array of Strings\nList of arguments to be passed to node. The most common argument is --debug or --debug-brk to start a debugging server.\ncallback\nType:  Function\nDefault:\nfunction(nodemon) {\n  \/\/ By default the nodemon output is logged\n  nodemon.on('log', function(event) {\n    console.log(event.colour);\n  });\n};\nCallback which receives the nodemon object. This can be used to respond to changes in a running app, and then do cool things like LiveReload a web browser when the app restarts. See the nodemon docs for the full list of events you can tap into.\nignore\nType: Array of String globs Default: ['node_modules\/**']\nList of ignored files specified by a glob pattern relative to the watched folder. Here is an explanation of how to use the patterns to ignore files.\next\nType: String Default: 'js'\nString with comma separated file extensions to watch. By default, nodemon watches .js files.\nwatch\nType: Array of Strings Default: ['.']\nList of folders to watch for changes. By default nodemon will traverse sub-directories, so there's no need in explicitly including sub-directories.\ndelay\nType: Number Default: 1000\nDelay the restart of nodemon by a number of milliseconds when compiling a large amount of files so that the app doesn't needlessly restart after each file is changed.\nlegacyWatch\nType: Boolean Default: false\nIf you wish to force nodemon to start with the legacy watch method. See https:\/\/github.com\/remy\/nodemon\/blob\/master\/faq.md#help-my-changes-arent-being-detected for more details.\ncwd\nType: String\nThe current working directory to run your script from.\nenv\nType: Object\nHash of environment variables to pass to your script.\nexec\nType: String\nYou can use nodemon to execute a command outside of node. Use this option to specify a command as a string with the argument being the script parameter above. You can read more on exec here.\nChangelog\n0.3.0 - Updated to nodemon 1.2.0.\n0.2.1 - Updated README on npmjs.org with correct options.\n0.2.0 - Updated to nodemon 1.0, added new callback option.\nBreaking changes:\n\noptions.file is now script and is a required property. Some properties were changed to match nodemon: ignoredFiles -> ignore, watchedFolders -> watch, delayTime -> delay, watchedExtensions -> ext(now a string) to match nodemon.\n\n0.1.2 - nodemon can now be listed as a dependency in the package.json and grunt-nodemon will resolve the nodemon.js file's location correctly.\n0.1.1 - Added legacyWatch option thanks to @jonursenbach.\n0.1.0 - Removed debug and debugBrk options as they are encapsulated by the nodeArgs option.\nBreaking changes:\n\nConfigs with the debug or debugBrk options will no longer work as expected. They simply need to be added to nodeArgs.\n\n0.0.10 - Added nodeArgs option thanks to @eugeneiiim.\n0.0.9 - Fixed bug when using cwd with ignoredFiles.\n0.0.8 - Added error logging for incorrectly installed nodemon.\n0.0.7 - Added debugBreak option thanks to @bchu.\n0.0.6 - Added env option.\n0.0.5 - Added cwd option.\n0.0.4 - Added nodemon as a proper dependency.\n0.0.3 - Uses local version of nodemon for convenience and versioning.\n0.0.2 - Removes .nodemonignore if it was previously generated and then the ignoredFiles option is removed.\n0.0.1 - Added warning if nodemon isn't installed as a global module.\n0.0.0 - Initial release\n","318":"D3xter\n\nAbout\n\nSimple and powerful syntax to make common charts with minimal code.\nHighly flexible plotting for deep customization.\nSensible defaults but easy to configure when desired.\nEasily extendable via familiar D3.js syntax.\n\nInstall\nbower install d3xter\nDocumentation\nFor full documentation complete with examples, visit this page.\nTesting and Contribution\nRun unit tests by opening test\/test.html in the browser.\nPull requests welcome!\nLicense\nThe MIT License (MIT)\n\nCopyright (c) 2014 Nathan Epstein\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n","319":"\n\n\n\n\n\n\nA git changelog based on ANGULAR JS commit standards (but adaptable to your needs). NPM page\n\nWorks as a CLI option or grunt plugin\nExample output\n\n\n\nBreaking changes and updates\n\nv2.0.0\nV1.0.0\n\nv1.1.0\n\n\n\n\n.changelogrc specification\n\nOptions | Defaults\n\n\nThe \"git_changelog\" task\n\nGrunt Task\n\nGetting Started\n\n\nCommand Line\n\n\nGit Commit Guidelines - Source : \"Angular JS\"\n\nCommit Message Format\nExample types\nScope\nSubject\n\n\nTagging your project\nRelease History\n\nv2.0.0\nv1.0.0\n\n\nContributors\n\nBreaking changes and updates\nv2.0.0\nIncluded commit_template option, allows to customize commit outputs.\nYou can use the following commit properties:\n\ncommit.subject\ncommit.body\ncommit.link\ncommit.hash\ncommit.breaks\ncommit.type\ncommit.component\ncloses: String containing the issues closed\nlink: Link to the commit\ncommit.author\n\nV1.0.0\nSince version 1.0.0 git-changelog has included the .changelogrc specification and has discontinued the next options:\n\ngrep_commits option has been removed in favour of the .changelogrc options\nrepo_url fixed as parameter\nbranch_name changed to branch\n\nv1.1.0\n\nversion_name instead of version\n\n.changelogrc specification\nThe .changelogrc file contains the \"standard commit guideliness\" that you and your team are following.\nThis specification is used to grep the commits on your log, it contains a valid JSON that will tell git-changelog which sections to include on the changelog.\n{\n    \"app_name\": \"Git Changelog\",\n    \"logo\": \"https:\/\/github.com\/rafinskipg\/git-changelog\/raw\/master\/images\/git-changelog-logo.png\",\n    \"intro\": \"Git changelog is a utility tool for generating changelogs. It is free and opensource. :)\",\n    \"branch\" : \"\",\n    \"repo_url\": \"\",\n    \"version_name\" : \"v1.0.0\",\n    \"file\": \"CHANGELOG.md\",\n    \"template\": \"myCustomTemplate.md\",\n    \"commit_template\": \"myCommitTemplate.md\"\n    \"sections\": [\n        {\n            \"title\": \"Bug Fixes\",\n            \"grep\": \"^fix\"\n        },\n        {\n            \"title\": \"Features\",\n            \"grep\": \"^feat\"\n        },\n        {\n            \"title\": \"Documentation\",\n            \"grep\": \"^docs\"\n        },\n        {\n            \"title\": \"Breaking changes\",\n            \"grep\": \"BREAKING\"\n        },\n        {\n            \"title\": \"Refactor\",\n            \"grep\": \"^refactor\"\n        },\n        {\n            \"title\": \"Style\",\n            \"grep\": \"^style\"\n        },\n        {\n            \"title\": \"Test\",\n            \"grep\": \"^test\"\n        },\n        {\n            \"title\": \"Chore\",\n            \"grep\": \"^chore\"\n        },\n        {\n            \"title\": \"Branchs merged\",\n            \"grep\": \"^Merge branch\"\n        },\n        {\n            \"title\" : \"Pull requests merged\",\n            \"grep\": \"^Merge pull request\"\n        }\n    ]\n}\nOptions | Defaults\n\nbranch : The name of the branch. Defaults to  \nrepo_url : The url of the project. For issues and commits links. Defaults to git config --get remote.origin.url\nprovider : Optional field, the provider is calculated from the repo_url, but can also be passed as config parameter. Values available: gitlab, github, bitbucket.\nversion_name: The version name of the project.\nfile: The name of the file that will be generated. Defaults to CHANGELOG.md, leave empty for console stream\ntemplate: The template for generating the changelog. It defaults to the one inside this project (\/templates\/template.md)\ncommit_template: The template for printing each of the commits of the project. It defaults to the one inside this project (\/templates\/commit_template.md)\napp_name : The name of the project. Defaults to My App - Changelog\nintro : The introduction text on the header of the changelog. Defaults to null\nlogo : A logo URL to be included in the header of the changelog. Defaults to null\nchangelogrc : Relative path indicating the location of the .changelogrc file, defaults to current dir.\ntag: You can select from which tag to generate the log, it defaults to the last one. Set it to false for log since the beginning of the project\ndebug: Debug mode, false by default\nsections: Group the commit by sections. The sections included by default are the ones that are on the previous example of .changelogrc file.\n\nThe \"git_changelog\" task\nGrunt Task\nGetting Started\nThis plugin requires Grunt 1.0.0\nIf you haven't used Grunt before, be sure to check out the Getting Started guide, as it explains how to create a Gruntfile as well as install and use Grunt plugins. Once you're familiar with that process, you may install this plugin with this command:\nnpm install git-changelog --save-dev\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\ngrunt.loadNpmTasks('git-changelog');\nIn your project's Gruntfile, add a section named git_changelog to the data object passed into grunt.initConfig().\ngrunt.initConfig({\n  git_changelog: {\n    minimal: {\n      options: {\n        file: 'MyChangelog.md',\n        app_name : 'Git changelog',\n        changelogrc : '\/files\/.changelogrc',\n        logo : 'https:\/\/github.com\/rafinskipg\/git-changelog\/raw\/master\/images\/git-changelog-logo.png',\n        intro : 'Git changelog is a utility tool for generating changelogs. It is free and opensource. :)'\n      }\n    },\n    extended: {\n      options: {\n        app_name : 'Git changelog extended',\n        file : 'EXTENDEDCHANGELOG.md',\n        version_name : 'squeezy potatoe',\n        sections : [\n          {\n            \"title\": \"Test commits\",\n            \"grep\": \"^test\"\n          },\n          {\n            \"title\": \"New Awesome Features!\",\n            \"grep\": \"^feat\"\n          }\n        ],\n        debug: true,\n        tag : false \/\/False for commits since the beggining\n      }\n    },\n    fromCertainTag: {\n      options: {\n        repo_url: 'https:\/\/github.com\/rafinskipg\/git-changelog',\n        app_name : 'My project name',\n        file : 'tags\/certainTag.md',\n        tag : 'v0.0.1'\n      }\n    },\n    customTemplate: {\n      options: {\n        app_name : 'Custom Template',\n        intro: 'This changelog is generated with a custom template',\n        file: 'output\/customTemplate.md',\n        template: 'templates\/template_two.md',\n        logo : 'https:\/\/github.com\/rafinskipg\/git-changelog\/raw\/master\/images\/git-changelog-logo.png',\n        version_name : 'squeezy potatoe',\n        tag: 'v0.0.1',\n        debug: true\n      }\n    }\n  }\n})\nCommand Line\nInstall it globally\nnpm install -g git-changelog\n\nSee commands\ngit-changelog -h\n\nUse it directly with the common options\n Usage: git-changelog [options]\n\n  Options:\n    -V, --version                               output the version number\n    -e, --extended                              Extended log\n    -n, --version_name [version_name]           Name of the version\n    -a, --app_name [app_name]                   Name [app_name]\n    -b, --branch [branch]                       Branch name [branch]\n    -f, --file [file]                           File [file]\n    -tpl, --template [template]                 Template [template]\n    -ctpl, --commit_template [commit_template]  Commit Template [commit_template]\n    -r, --repo_url [repo_url]                   Repo url [repo_url]\n    -l, --logo [logo]                           Logo path [logo]\n    -i, --intro [intro]                         intro text [intro]\n    -t, --tag [tag]                             Since tag [tag]\n    -rc, --changelogrc [changelogrc]            .changelogrc relative path [changelogrc]\n    -g, --grep [grep]                           Grep commits for [grep]\n    -d, --debug                                 Debugger\n    -p, --provider [provider]                   Provider: gitlab, github, bitbucket (Optional)\n    -h, --help                                  output usage information\n\n\n\nFor example:\ngit-changelog -t false -a \"My nice application\"\n\nGit Commit Guidelines - Source : \"Angular JS\"\nWe have very precise rules over how our git commit messages can be formatted.  This leads to more\nreadable messages that are easy to follow when looking through the project history.  But also,\nwe use the git commit messages to generate the AngularJS change log.\nCommit Message Format\nEach commit message consists of a header, a body and a footer.  The header has a special\nformat that includes a type, a scope and a subject:\n<type>(<scope>): <subject>\n<BLANK LINE>\n<body>\n<BLANK LINE>\n<footer>\n\nAny line of the commit message cannot be longer 100 characters! This allows the message to be easier\nto read on github as well as in various git tools.\nExample commit messages\ngit commit -m \"docs(readme): Add documentation for explaining the commit message\"\ngit commit -m \"refactor: Change other things\"\n\nClosing issues :\ngit commit -m \"fix(git_changelog_generate): pass tag if it exists to gitReadLog\nPreviously if a tag was found the script would try to find commits\nbetween undefined..HEAD. By passing the tag, it now finds tags between\ntag..HEAD.\n\nCloses #5.\"\n\nExample types\nYou may define your own types refering to the .changelogrc specification\nMust be one of the following:\n\nfeat: A new feature\nfix: A bug fix\ndocs: Documentation only changes\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\nrefactor: A code change that neither fixes a bug or adds a feature\ntest: Adding missing tests\nchore: Changes to the build process or auxiliary tools and libraries such as documentation generation\n\nScope\nThe scope could be anything specifying place of the commit change. For example $location,\n$browser, $compile, $rootScope, ngHref, ngClick, ngView, etc...\nSubject\nThe subject contains succinct description of the change:\n\nuse the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\ndon't capitalize first letter\nno dot (.) at the end\n\n###Body\nJust as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\"\nThe body should include the motivation for the change and contrast this with previous behavior.\n###Footer\nThe footer should contain any information about Breaking Changes and is also the place to\nreference GitHub issues that this commit Closes.\nA detailed explanation can be found in this [document][commit-message-format].\n[commit-message-format]: https:\/\/docs.google.com\/document\/d\/1QrDFcIiPjSLDn3EL15IJygNPiHORgU1_OOAqWjiDU5Y\/edit#\nTagging your project\nIn order to have you project versions correctly displayed on your changelog, try to use this commit message format:\nchore(release): v1.4.0 codename(jaracimrman-existence)\n\nIn order to do that, you can use git annotated tags:\ngit tag -a v1.4.0 -m 'chore(release): v1.4.0 codename(jaracimrman-existence)'\n\nIf you are publishing NPM modules you can let NPM do that for you:\nnpm version patch -m \"chore(release): %s codename(furious-stallman)\"\n\nRelease History\nv2.0.0\n\nIntroduced Commit Template\n\nv1.0.0\n\nSupport for .changelogrc\n\nContributors\nAdd your name here by contributing to this project\n\nrafinskipg\nJohnnyEstilles\ncolegleason\njodybrewster\nGlebcha\n\n","320":" Vienna IKS Editables\nVIE is a utility library for implementing decoupled Content Management systems. VIE is developed as part of the EU-funded IKS project.\n\n\nIn French, vie means life, showcasing how VIE makes your website come alive\nIn English, vie means striving for victory or superiority\n\nVIE development is now targeting a 2.0 release. Read this blog post to find out the changes from VIE 1.0. There is also a good introductory post on VIE on the IKS blog.\nVIE integration in nutshell\nAdding VIE to your system is as easy as:\n\nMark up your pages with RDFa annotations\nInclude vie.js into the pages\nImplement Backbone.sync\n\nChanges\nPlease refer to the CHANGES.md document.\nCommon representation of content on HTML level\nA web editing tool has to understand the contents of the page. It has to understand what parts of the page should be editable, and how they connect together. If there is a list of news for instance, the tool needs to understand it enough to enable users to add new news items. The easy way of accomplishing this is to add some semantic annotations to the HTML pages. These annotations could be handled via Microformats, HTML5 microdata, but the most power lies with RDFa.\nRDFa is a way to describe the meaning of particular HTML elements using simple attributes. For example:\n<div id=\"myarticle\" typeof=\"http:\/\/rdfs.org\/sioc\/ns#Post\" about=\"http:\/\/example.net\/blog\/news_item\">\n    <h1 property=\"dcterms:title\">News item title<\/h1>\n    <div property=\"sioc:content\">News item contents<\/div>\n<\/div>\n\nHere we get all the necessary information for making a blog entry editable:\n\ntypeof tells us the type of the editable object. On typical CMSs this would map to a content model or a database table\nabout gives us the identifier of a particular object. On typical CMSs this would be the object identifier or database row primary key\nproperty ties a particular HTML element to a property of the content object. On a CMS this could be a database column\n\nAs a side effect, this also makes pages more understandable to search engines and other semantic tools. So the annotations are not just needed for UI, but also for SEO.\nCommon representation of content on JavaScript level\nHaving contents of a page described via RDFa makes it very easy to extract the content model into JavaScript. We can have a common utility library for doing this, but we also should have a common way of keeping track of these content objects. Enter Backbone.js:\n\nBackbone supplies structure to JavaScript-heavy applications by providing models with key-value binding and custom events, collections with a rich API of enumerable functions, views with declarative event handling, and connects it all to your existing application over a RESTful JSON interface.\n\nWith Backbone, the content extracted from the RDFa-annotated HTML page is easily manageable via JavaScript. Consider for example:\nv = new VIE();\nv.use(new v.RdfaService());\nv.load({element: jQuery('#myarticle')}).from('rdfa').execute().success(function(entities) {\n    _.forEach(entities, function(entity) {\n        entity.set({'dcterms:title': 'Hello, world'});\n        entity.save(null, {\n            success: function(savedModel, response) {\n                alert(\"Your article '\" + savedModel.get('dcterms:title') + \"' was saved to server\");\n            }\n        });\n    })\n    console.log(\"We got \" + entities.length + \" editable objects from the page\");\n});\n\nThe classic VIE API will also work:\nvar v = new VIE({classic: true});\nvar objectInstance = v.RDFaEntities.getInstance(jQuery('#myarticle'));\nobjectInstance.set({'dcterms:title': 'Hello, world'});\nobjectInstance.save(null, {\n    success: function(savedModel, response) {\n        alert(\"Your article '\" + savedModel.get('dcterms:title') + \"' was saved to server\");\n    }\n});\n\nThis JS would work across all the different CMS implementations. Backbone.js provides a quite nice RESTful implementation of communicating with the server with JSON, but it can be easily overridden with CMS-specific implementation by just implementing a new Backbone.sync method.\nExample\nThere is a full static HTML example of VIE at work. Saving outputs the edited contents as JSON into the JavaScript console:\n\nExample with Hallo\n\nBe sure to read the annotated VIE source code for API documentation.\nI\/O operations\nAll Input\/Output operations of VIE are based on the jQuery Deferred object, which means that you can attach callbacks to them either before they run, or also after they've been run.\nThe operations may either succeed, in which case the then callbacks will fire, or be rejected, in which case the fail callbacks will fire. Any then callbacks will fire in either case.\nDependencies\nVIE uses the following JavaScript libraries:\n\njQuery for DOM manipulation and Deferreds\nBackbone.js for entities (models) and collections\nUnderscore.js for various JavaScript utilities\n\nSome functionality in VIE additionally uses:\n\nRdfQuery as a triplestore and for reasoning over rules\n\nIntegrations\n\nCreate\nGoogle Web Toolkit\nSymfony2\nPalsu\n\nUsing VIE on Node.js\nVIE is a CommonJS library that works on both browser and the server. On Node.js you can install it with:\nnpm install vie\n\nHere is a simple Node.js script that uses VIE for parsing RDFa:\nvar jQuery = require('jquery');\nvar vie = require('vie');\n\n\/\/ Instantiate VIE\nvar VIE = new vie.VIE();\n\n\/\/ Enable the RDFa service\nVIE.use(new VIE.RdfaService());\n\nvar html = jQuery('<p xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" about=\"http:\/\/www.example.com\/books\/wikinomics\">In his latest book <cite property=\"dc:title\">Wikinomics<\/cite>, <span property=\"dc:creator\">Don Tapscott<\/span> explains deep changes in technology, demographics and business.<\/p>');\n\n\/\/ \nVIE.load({element: html}).from('rdfa').execute().done(function() {\n\n  var objectInstance = VIE.entities.get('http:\/\/www.example.com\/books\/wikinomics');\n\n  console.log(objectInstance.get('dc:title'));\n\n});\n\nDevelopment\nVIE development is coordinated using Git at bergie\/VIE.\nFeel free to report issues or send pull requests if you have ideas for pushing VIE forward. Contributions that include their own unit tests appreciated!\nDevelopment discussions happen on the VIE mailing list and the #iks channel on Freenode. See also VIE on Ohloh.\nCode organization\nVIE source code is inside the src directory. Each separate unit of functionality should be handled in its own file, with the src\/VIE.js being the entry point to the system.\n\nBuilding VIE\nThe VIE library consists of many individual pieces that we merge together in the build process. You'll need Grunt. Then just run:\n$ grunt build\n\nThe built VIE library will appear in the dist folder.\nCore-only distribution\nIn addition to the regular full build, there is also a slimmer build of VIE available that only includes the core parts of the library and no external service. To build that instead, run:\n$ grunt build:core\n\nRunning Unit Tests\nDirect your browser to the test\/index.html file to run VIE's QUnit tests.\nUnit tests on Node.js\nThe Grunt testing setup includes multiple parts. With it, you can test the library on both Node.js and a headless browser. Run:\n$ grunt test\n\nor:\n$ npm test\n\nAutomatic unit tests\nYou can also run the Grunt setup in watch mode, where any change in VIE sources or tests will trigger a rebuild and test run:\n$ grunt watch\n\nContinuous integration\nVIE uses Travis for continuous integration. Simply add your fork there and every time you push you'll get the tests run.\n\n\n","321":"jQuery Image Preload Plugin\nThe jQuery.imgpreload plugin lets you preload images before and after the DOM has loaded.\nTested: IE6, IE7, IE8, IE9+, FF, Chrome, Safari\nUsage\nCallbacks\nThe following are available callbacks, you may change them globally or override the defaults by passing the settings object to the imgpreload method.\n$.fn.imgpreload.defaults =\n{\n    each: null \/\/ callback invoked when each image is loaded\n    , all: null \/\/ callback invoked when all images have loaded\n};\n\nAfter DOM loaded\nThe following illustrates using the plugin to preload images after the dom has loaded (e.g. using jQuery selectors):\n$('#content img').imgpreload(function()\n{\n    \/\/ callback invoked when all images have loaded\n    \/\/ this = array of dom image objects\n    \/\/ check for success with: $(this[i]).data('loaded')\n});\n\n$('img.logos').imgpreload\n({\n    each: function()\n    {\n        \/\/ callback invoked when each image is loaded\n        \/\/ this = dom image object\n        \/\/ check for success with: $(this).data('loaded')\n    },\n    all: function()\n    {\n        \/\/ callback invoked when all images have loaded\n        \/\/ this = array of dom image objects\n        \/\/ check for success with: $(this[i]).data('loaded')\n    }\n});\n\nBefore DOM loaded\nTo preload images before the dom has loaded, for instance in the head of the document, you would have to use specific image paths.\n$.imgpreload('\/images\/a.gif',function()\n{\n    \/\/ callback invoked when all images have loaded\n    \/\/ this = array of dom image objects\n    \/\/ check for success with: $(this[i]).data('loaded')\n});\n\nYou can pass a single image path (as above) or an array of image paths.\n$.imgpreload(['\/images\/a.gif','\/images\/b.gif'],function()\n{\n    \/\/ this = array of dom image objects\n    \/\/ check for success with: $(this[i]).data('loaded')\n    \/\/ callback executes when all images are loaded\n});\n\neach and all callbacks are available to use.\n$.imgpreload(['\/images\/a.gif','\/images\/b.gif'],\n{\n    each: function()\n    {\n        \/\/ callback invoked when each image is loaded\n        \/\/ this = dom image object\n        \/\/ check for success with: $(this).data('loaded')\n    },\n    all: function()\n    {\n        \/\/ callback invoked when all images have loaded\n        \/\/ this = array of dom image objects\n        \/\/ check for success with: $(this[i]).data('loaded')\n    }\n});\n\n","322":"\u00a0\n\n\n\n\u00a0\nRecon\nCode Intelligence for React Applications.\n\nWhat?\nRecon provides a new level of insight into your codebase allowing you to understand\nhow your components are being used, easily grep their complexity, draw dependency graphs,\nand isolate points for optimisation.\nOn top of this the raw information Recon provides gives a strong base for creating tools\n(such as living styleguides) which may need to plug in to component meta data.\nHow?\nThe core of Recon revolves around recon-engine. This engine parses your application pulling out\nany data which may be useful (eg. Props, component dependencies, enhancements etc.). Then a\ngraphql query interface is exposed allowing you to explore your applications in an incredibly\nintuitive manner!\nChecking out our test fixtures is a\ngreat place to an example of this.\nOnce this data is consolidated the possibility of tools to be built on top are endless!\nGetting Started\nPrerequisites: Node >v6. Also ensure using import\/export syntax, using JSX (see roadmap for full list)\nInstall and configuration\nThe quickest way to get going with Recon for your project is to use our CLI application.\nFirstly install with\n$ npm install -g recon-cli\n\nNow, within your application working directory, simply run\n$ recon\n\nYou are now inside Recon! :O\nFrom this point forwards the entire power of Recon should be just a help command away.\nrecon$ help\n\nThe first thing you're going to want to do from here is create a new config file .reconrc in the working directory\nof your project.\nYou can do this interactively by running\nrecon$ init\n\nTo view all configuration possibilities you can view the docs here.\nShow me the power!\nWhy not start off by trying stats. This will analyse your application and dump out a bunch of objective\nstatements and statistics.\nrecon$ stats\n\nThen if you're feeling extra adventurous give server a go. This will spawn a new graphql server which will\nallow you to query your application meta data freely.\nrecon$ server\n\nFor everything else see what is available via the help command!\nI want to integrate Recon into x\nDocumentation is going to be a little skimpy here for a while since we are planning on getting\nthe internals of recon-engine to be as powerful as possible and stabilising the api as much as\npossible.\nMost likely you'll want to look at using recon-engine and recon-server (their tests are a decent\nplace to start looking).\n\nDisclaimer: This is a very early preview of Recon and you should expect breaking changes within the <v1 range of releases.\n\nContributing\n\nBugs? Please submit a Pull Request with your minimal source code and a test which breaks.\nWant to fix something or add a new feature? Get started with our Dev Guide!\n\nFor more details on all contributions see CONTRIBUTING.md\nHigh-level Roadmap\n\n Move CLI away from vorpal to a non-interactive version\n\n The engine should be spawned in the background. Ie. similar to flow server\n\n\n Identify initial parsing issues across many codebases\n\n Improve test .toMatch to not care about ordering (ie. add toMatchUnordered)\n Provide better DX for dumping debug data and reporting issues\n\n\n Pull prop type definitions from components\n Broader React usage support\n\n React.createClass, React.createElement, hyperscript\n Support decorator syntax as enhancements\n Support require('module')\n\n\n Better prop usage information\n\n Pull out static values (eg. prop=\"stringValue\")\n Resolve basic flows to determine possible values (upto prop types)\n Resolve rest\/spread props\n\n\n More trivial meta data! (eg. docblocks)\n Stabilise graphql interface\n Support long-running persisted engine (ie. watch file changes)\n Work on documentation and integrations for developer tools\n Explore plugin api\n Reassess and tidy implementation\n\n Performance optimisations\n\n\n Identify application critical paths\n Explore using Flow for internal type resolution\n\nLicense\nApache 2.0\n","323":"numtel:pg \nReactive PostgreSQL for Meteor\nProvides Meteor integration of the pg-live-select NPM module, bringing reactive SELECT statement result sets from PostgreSQL >= 9.3.\n\nIf you do not have PostgreSQL server already installed, you may use the numtel:pg-server Meteor Package to bundle the PostgreSQL server directly to your Meteor application.\n\n\nHow to publish joined queries that update efficiently\nLeaderboard example modified to use PostgreSQL\nReactive MySQL for Meteor\n\nServer Implements\nThis package provides the LivePg class as defined in the pg-live-select NPM package.\nAlso exposed on the server is the pg object as defined in the node-postgres NPM package (useful for other operations like INSERT and UPDATE).\nLivePg.prototype.select()\nIn this Meteor package, the SelectHandle object returned by the select() method is modified to act as a cursor that can be published.\nvar liveDb = new LivePg(CONNECTION_STRING, CHANNEL);\n\nMeteor.publish('allPlayers', function(){\n  return liveDb.select('SELECT * FROM players ORDER BY score DESC');\n});\nClient\/Server Implements\nPgSubscription([connection,] name, [args...])\nConstructor for subscribing to a published select statement. No extra call to Meteor.subscribe() is required. Specify the name of the subscription along with any arguments.\nThe first argument, connection, is optional. If connecting to a different Meteor server, pass the DDP connection object in this first argument. If not specified, the first argument becomes the name of the subscription (string) and the default Meteor server connection will be used.\nThe prototype inherits from Array and is extended with the following methods:\n\n\n\nName\nDescription\n\n\n\n\nchange([args...])\nChange the subscription's arguments. Publication name and connection are preserved.\n\n\naddEventListener(eventName, listener)\nBind a listener function to this subscription\n\n\nremoveEventListener(eventName)\nRemove listener functions from an event queue\n\n\ndispatchEvent(eventName, [args...])\nCall the listeners for a given event, returns boolean\n\n\ndepend()\nCall from inside of a Template helper function to ensure reactive updates\n\n\nreactive()\nSame as depend() except returns self\n\n\nchanged()\nSignal new data in the subscription\n\n\nready()\nReturn boolean value corresponding to subscription fully loaded\n\n\nstop()\nStop updates for this subscription\n\n\n\nNotes:\n\nchanged() is automatically called when the query updates and is most likely to only be called manually from a method stub on the client.\nEvent listener methods are similar to native methods. For example, if an event listener returns false exactly, it will halt listeners of the same event that have been added previously. A few differences do exist though to make usage easier in this context:\n\nThe event name may also contain an identifier suffix using dot namespacing (e.g. update.myEvent) to allow removing\/dispatching only a subset of listeners.\nremoveEventListener() and dispatchEvent() both refer to listeners by name only. Regular expessions allowed.\nuseCapture argument is not available.\n\n\n\nEvent Types\n\n\n\nName\nListener Arguments\nDescription\n\n\n\n\nupdate\ndiff, data\nNew change, before data update\n\n\nupdated\ndiff, data\nNew change, after data update\n\n\nreset\nmsg\nSubscription reset (most likely due to code-push), before update\n\n\n\nClosing connections between hot code-pushes\nWith Meteor's hot code-push feature, new triggers and functions on database server are created with each restart. In order to remove old items, a handler to your application process's SIGTERM signal event must be added that calls the cleanup() method on each LivePg instance in your application. Also, a handler for SIGINT can be used to close connections on exit.\nOn the server-side of your application, add event handlers like this:\nvar liveDb = new LivePg(CONNECTION_STRING, CHANNEL);\n\nvar closeAndExit = function() {\n  \/\/ Call process.exit() as callback\n  liveDb.cleanup(process.exit);\n};\n\n\/\/ Close connections on hot code push\nprocess.on('SIGTERM', closeAndExit);\n\/\/ Close connections on exit (ctrl + c)\nprocess.on('SIGINT', closeAndExit);\nTests \/ Benchmarks\nThe test suite does not require a separate Postgres server installation as it uses the numtel:pg-server package to run the tests.\nThe database connection settings must be configured in test\/settings\/local.json.\nThe database specified should be an empty database with no tables because the tests will create and delete tables as needed.\n# Install Meteor\n$ curl -L https:\/\/install.meteor.com\/ | \/bin\/sh\n\n# Clone Repository\n$ git clone https:\/\/github.com\/numtel\/meteor-pg.git\n$ cd meteor-pg\n\n# Optionally, configure port and data dir in test\/settings\/test.pg.json.\n# If changing port, keep port value updated in test\/index.es6 as well.\n\n# Test database will be created in dbtest directory.\n\n# Run test server\n$ meteor test-packages .\/\n\nLicense\nMIT\n","324":"angular-datepicker\nWarning: This project is no longer maintained. Use at your own risk!\nThe mobile-friendly, responsive, and lightweight Angular.js date & time input picker. Perfect for Ionic apps!\n, \nThis is basically a pickadate.js fork that completely removes the jQuery dependency and adds Angular.js directives.\nBower\nbower install angular-native-picker\nUsage\nInclude build\/angular-datepicker.js in your application:\n<script src=\"angular-datepicker.js\"><\/script>\nInclude CSS files in your application:\n<link rel=\"stylesheet\" href=\"build\/themes\/default.css\"\/>\n<link rel=\"stylesheet\" href=\"build\/themes\/default.date.css\"\/>\n<link rel=\"stylesheet\" href=\"build\/themes\/default.time.css\"\/>\nNote: for usage within a modal the default (not classic) CSS files are recommended.\nAdd the module angular-datepicker as a dependency to your app module:\nvar myapp = angular.module('myapp', ['angular-datepicker']);\nTo create a date or time picker, add the pick-a-date or pick-a-time directive to your input tag:\n<input type=\"text\" pick-a-date=\"date\" placeholder=\"Select Date\" \/> {{ date }}\n<input type=\"text\" pick-a-time=\"time\" placeholder=\"Select Time\" \/> {{ time }}\nYou can also provide an options object to the directive which will be passed\ninto the pickadate or pickatime constructor.\n\/\/ somewhere in your controller\n$scope.options = {\n  format: 'yyyy-mm-dd', \/\/ ISO formatted date\n  onClose: function(e) {\n    \/\/ do something when the picker closes   \n  }\n}\n<input type=\"text\" pick-a-date=\"date\" pick-a-date-options=\"options\" \/> {{ date }}\n<input type=\"text\" pick-a-time=\"time\" pick-a-time-options=\"options\" \/> {{ time }}\nIf you don't need to provide any callbacks (like onClose) you can specify the\noptions object as an angular expression:\n<input type=\"text\" pick-a-date=\"date\" pick-a-date-options=\"{ format: 'yyyy-mm-dd' }\" \/>\nFor a full list of available options please refer to the pickadate documentation\nfor datepicker options and\ntimepicker options.\n","325":"v2er\nA simple v2ex client app, use React Native.\n\u4f7f\u7528 V2EX \u7684 api \u8fdb\u884c\u7684\u4e00\u4e2a\u5173\u4e8e React-native \u7684\u8bd5\u9a8c\u3002\n\u65e8\u5728\u8bd5\u9a8c\u5404\u4e2a\u529f\u80fd\u662f\u5426\u90fd\u53ef\u4ee5\u7528 React-native \u6765\u5b9e\u73b0\u3002\n\nUPDATE 2017-11-1\n\n\u66f4\u65b0 react-native \u5230\u6700\u65b0\u7248\u672c\n\n  \"react\": \"16.0.0-beta.5\",\n  \"react-native\": \"^0.49.5\",\n\n\n\u589e\u52a0 react-native-vector-icons \uff0c\u5728 npm install \u540e\u9700\u8981\u8fd0\u884c react-native link\n\u4f7f\u7528\u65b0\u7684\u8bed\u6cd5\u8fdb\u884c\u529f\u80fd\u7684\u5236\u4f5c\n\u589e\u52a0 Store \u90e8\u5206\u7684\u5904\u7406\n\u4f18\u5316\u4ee3\u7801\u7ed3\u6784\uff0c\u53bb\u9664\u4e0d\u5fc5\u8981\u7684\u90e8\u5206\nTODO\n\n\u65b0\u7684\u8282\u70b9\u663e\u793a\n\u8282\u70b9\u76f8\u5173\u7684\u64cd\u4f5c\n\n\n\n\n2015-04-01\n\n\u4fee\u6b63\u4e86\u4e00\u4e9b\u6837\u5f0f\u4e0a\u7684\u95ee\u9898\n\u5b8c\u5584\u4e86\u8282\u70b9\u7684\u663e\u793a\n\u589e\u52a0\u4e86\u5bf9\u4e8e\u7f51\u7edc\u72b6\u6001\u7684\u5224\u65ad\uff0c\u5982\u679c\u4e3a cell \u7684\u65f6\u5019\u4e0d\u8f7d\u5165\u5217\u8868\u7684\u56fe\u7247\n\n\n2015-03-31\n\n\u589e\u52a0\u4e86 Tabbar\n\u589e\u52a0\u4e86\u6240\u6709\u8282\u70b9\u7684\u663e\u793a\n\n\n2015-03-29\n\n\u83b7\u53d6\u6700\u8fd1\u4e3b\u9898\u5217\u8868\u3002\n\u83b7\u53d6\u4e3b\u9898\u7684\u5185\u5bb9\u4ee5\u53ca\u76f8\u5e94\u7684\u8bc4\u8bba\u3002\n\n","326":"Skitter - Slideshow for anytime!\nSkitter has 38 different animations, two types of navigations and many options to customize!\nLicense: Dual licensed under the MIT or GPL Version 2\nNPM\nnpm install skitter-slider\nBower\nbower install skitter --save\nManually\nDownload zip https:\/\/github.com\/thiagosf\/skitter\/archive\/master.zip and move files in dist directory for you application. Attention: you need to download the dependencies manually.\nHow to install\nAdd the CSS and JS files inside \n<link type=\"text\/css\" href=\"dist\/skitter.css\" media=\"all\" rel=\"stylesheet\" \/>\n<script type=\"text\/javascript\" src=\"https:\/\/code.jquery.com\/jquery-2.1.1.min.js\"><\/script>\n<script type=\"text\/javascript\" src=\"js\/jquery.easing.1.3.js\"><\/script>\n<script type=\"text\/javascript\" src=\"dist\/jquery.skitter.min.js\"><\/script>\nInit the Skitter\n$(document).ready(function() {\n  $(\".skitter-large\").skitter();\n});\nAdd the images through the unordered list\n<div class=\"skitter skitter-large\">\n  <ul>\n    <li>\n      <a href=\"#cut\"><img src=\"images\/001.jpg\" class=\"cut\" \/><\/a>\n      <div class=\"label_text\"><p>cut<\/p><\/div>\n    <\/li>\n    <li>\n      <a href=\"#swapBlocks\"><img src=\"images\/002.jpg\" class=\"swapBlocks\" \/><\/a>\n      <div class=\"label_text\"><p>swapBlocks<\/p><\/div>\n    <\/li>\n    <li>\n      <a href=\"#swapBarsBack\"><img src=\"images\/003.jpg\" class=\"swapBarsBack\" \/><\/a>\n      <div class=\"label_text\"><p>swapBarsBack<\/p><\/div>\n    <\/li>\n  <\/ul>\n<\/div>\nTodo\n\n Update WP-Plugin\n Update CakePHP Plugin\n Refactor variable names applying a pattern.\n Separate into small pieces the source code.\n Create unit tests\n Exchange images for background divs (for animations)\n\n","327":"SecPub\nPublished security vulnerabilities, research, write-ups, and associated\ninformation which I have worked on. Vulnerabilities have been broken down\nper vendor - where applicable.\nExternal\nResearch\n\nMessing with SWD - Part I\nPivoting from blind SSRF to RCE with HashiCorp Consul\nRemote Code Execution (RCE) on Microsoft's 'signout.live.com'\n\nCTF\n\nBKP CTF - Wackusensor Write-Up\nBKP CTF - Good Morning (Wonderland)\nBKP CTF - Bug Bounty (Suffolk Downs)\n9447 CTF - Super Turbo Atomic GIF Converter\n\nVulnerabilities\n\nJenkins Swarm Plugin - XXE (XML External Entities) via UDP broadcast\nJenkins GitLab Plugin - Information disclosure vulnerability\nJenkins Artifactory Plugin - Information disclosure vulnerabilities\nJenkins Ansible Tower Plugin - Information disclosure vulnerability\nNetGear WNDR Authentication Bypass \/ Information Disclosure\nD-Link and TRENDnet 'ncc2' service - multiple vulnerabilities\nD-Link DSP-W110 - multiple vulnerabilities\nCisco Nexus OS (NX-OS) - Command \"injection\" \/ sanitization issues.\n\n","328":"hermes\nHermes is an extensible XMPP-based chatroom server written in Python.\nEasily setup and manage chatrooms for friends or colleagues.\nHow it Works\nSupply your own XMPP-based accounts (Google accounts work great!) to serve as chatroom hosts, add a bit of configuration, and that's it.\nAll chatroom members are invited to chat with the chatroom host which will in turn broadcast their messages to all other members.\nUsage\nThe \"Hello World\" usage of Hermes looks like this. Put the following in chatserver.py, update the user and chatroom info, and run it:\nfrom hermes.api import run_server\n\nbrain = { 'JID': 'brain@wb.com', 'NICK': 'brain', 'ADMIN': True }\npinky = { 'JID': 'pinky.suavo@wb.com', 'NICK': 'pinky' }\n\nchatrooms = {\n    'world-domination-planning': {\n        'JID': 'world.domination.planning@wb.com',\n        'PASSWORD': 'thesamethingwedoeverynight',\n        'SERVER': ('talk.google.com', 5223,),\n        'MEMBERS': [pinky, brain],\n    },\n}\n\nrun_server(chatrooms)\n\nInstallation\nAvailable from PyPI: http:\/\/pypi.python.org\/pypi\/hermes\/. pip is the recommended installation method:\npip install hermes\n\nCommands\nHermes interprets some messages as commands:\n\n\n\/mute - Mute the chatroom. Messages are queued up for whenever you unmute the chatroom.\n\n\n\/unmute - Unmute the chatroom. Receive all messages that were queued while the chatroom was muted.\n\n\n\/invite <handle> - Invite members to the chatroom. Available to admins only.\n\n\n\/kick <handle> - Kick members from the chatroom. Available to admins only.\n\n\n\/marco - Not sure if you're still connected to the chatroom? Chatroom replies to you (and only you) with \"polo\".\n\n\nExtensibility\nYou can extend the base chatroom class hermes.Chatroom to modify or add extra functionality.\nAdding a command_patterns static property to your class should be particularly useful for extensions.\nIt's a list of regular expression\/method name pairs. Each incoming message is tested against the regexes until a match is found.\nOn a match, the named instance method is invoked to handle the message instead of the default message-handling pipeline.\nSpecify the path to your creation as a string or the Class itself as the CLASS of your chatroom:\nfrom hermes.api import run_server, Chatroom\n\nclass BillyMaysChatroom(Chatroom):\n\tcommand_patterns = ((r'.*', 'shout'),)\n\n\tdef shout(self, sender, body, match):\n        body = body.upper() #SHOUT IT\n        self.broadcast(body)\n\nchatrooms = {\n    'world-domination-planning': {\n        'CLASS': 'BillyMaysChatroom',\n        'JID': 'world.domination.planning@wb.com',\n        'PASSWORD': 'thesamethingwedoeverynight',\n        'SERVER': ('talk.google.com', 5223,),\n        'MEMBERS': [pinky, brain],\n    },\n}\n\nrun_server(chatrooms)\n\nUpcoming Features\n\nPersistence: Allow chatroom configuration\/state\/history to be persisted\n\nIs it any good?\nYes.\nElaborate, you say? Hermes has been successfully used \"in production\" to run several chatrooms for the Crocodoc team since the first commit. It's good to have guinea pigs.\nLicense\nHermes is an ISC licensed library. See LICENSE for more details. If you insist on compensating me, I'd let you buy me a beer. Or just donate money to a good cause...that'd probably be best.\nCan I Contribute?\nYes, please do. Pull requests are great. I'll totally add a CONTRIBUTORS.txt when Hermes gets its first contributor.\n","329":"git-goggles Readme\ngit-goggles is a git management utilities that allows you to manage your source code as\nit evolves through its development lifecycle.\n\nOverview\nThis project accomplishes two things:\n\nManage the code review state of your branches\nGives a snapshot of the where your local branches are vs origin in terms of being ahead \/ behind on commits\n\nThere is a nice blog post describing the features along with screenshots at http:\/\/bit.ly\/git-goggles\n\nField Reference\nIn the table outputted by git-goggles, each row corresponds to a branch, with the following fields:\n\nStatus: the current status of your branch\n\n\nnew: this is a branch that has never been through the review process\nreview: this branch has code that needs to be reviewed\nmerge: everything has been reviewed, but needs to be merged into parent (same as done for being ahead)\ndone: reviewed and merged (doens't matter if you're behind but you can't be ahead)\n\n\n\nBranch: the branch name\n\nReview: how many commits have taken place since the last review\n\nAhead: how many commits are in your local branch that are not in origin\n\nBehind: how many commits are in origin that are not in your local branch\n\nPull & Push: whether your branches need to be pushed or pulled to track origin\n\n\ngreen checkbox: you don't need to pull\nred cross: you need to pull\nquestion mark: you either don't have a checked out copy of this branch or you need to prune your local tree\n\n\n\nModified: the last time that HEAD was modified (NOT the last time the review happened)\n\n\n\nInstallation\nTo install from PyPi you should run one of the following commands. (If you use pip for your package installation, you should take a look!)\npip install git-goggles\n\nor\neasy_install git-goggles\n\nCheckout the project from github http:\/\/github.com\/nowells\/git-goggles\ngit clone git:\/\/github.com\/nowells\/git-goggles.git\n\nRun setup.py as root\ncd git-goggles\nsudo python setup.py install\n\nDocumentation:\nWith Sphinx docs deployment: in the docs\/ directory, type:\nmake html\n\nThen open docs\/_build\/index.html\n\nUsage\nViewing the status of your branches:\ngit goggles\n\nStarting your review process (shows an origin diff):\ngit goggles codereview\n\nComplete your review process (automatically pushes up):\ngit goggles codereview complete\n\n\nConfiguration\nYou can set a few configuration variables to alter to way git-goggles works out of the box.\nDisable automatic fetching from all remote servers.\ngit config --global gitgoggles.fetch false\n\nDisable colorized output\ngit config --global gitgoggles.colors false\n\nAlter the symbols used to display success, failure, unknown states\ngit config --global gitgoggles.icons.success \"OK\"\ngit config --global gitgoggles.icons.failure \"FAIL\"\ngit config --global gitgoggles.icons.unknown \"N\/A\"\n\nAlter the colors of branch states. The available colors are [grey, red, green, yellow, blue, magenta, cyan, white]\ngit config --global gitgoggles.colors.local cyan\ngit config --global gitgoggles.colors.new red\ngit config --global gitgoggles.colors.review red\ngit config --global gitgoggles.colors.merge yellow\ngit config --global gitgoggles.colors.done green\n\nAlter the width of branch column to turn on wordwrap.\ngit config --global gitgoggles.table.branch-width 15\n\nAlter the table cell padding (defaults to 0)\ngit config --global gitgoggles.table.left-padding 1\ngit config --global gitgoggles.table.right-padding 1\n\nAlter the display of horizontal rule between rows of table (default false)\ngit config --global gitgoggles.table.horizontal-rule true\n\n\nInternals\ngit-goggles works by creating and managing special tags called\n'codereview-<branch_name>' and tracking them against HEAD.\nThe first time a codereview is completed, the tag is created. Subsequent\nreviews delete and re-create the tag so that it awlays accurately tracks HEAD.\n","330":"Exelixi\nExelixi is a distributed framework based on Apache Mesos,\nmostly implemented in Python using gevent for high-performance concurrency\nIt is intended to run cluster computing jobs (partitioned batch jobs, which include some messaging) in pure Python.\nBy default, it runs genetic algorithms at scale.\nHowever, it can handle a broad range of other problem domains by\nusing --uow command line option to override the UnitOfWorkFactory class definition.\nPlease see the project wiki for more details,\nincluding a tutorial\non how to build Mesos-based frameworks.\nQuick Start\nTo check out the GA on a laptop (with Python 2.7 installed), simply run:\n.\/src\/ga.py\n\nOtherwise, to run at scale, the following steps will help you get Exelixi running on Apache Mesos.\nFor help in general with command line options:\n.\/src\/exelixi.py -h\n\nThe following instructions are based on using the Elastic Mesos service,\nwhich uses Ubuntu Linux servers running on Amazon AWS.\nEven so, the basic outline of steps shown here apply in general.\nFirst, launch an Apache Mesos cluster.\nOnce you have confirmation that your cluster is running\n(e.g., Elastic Mesos sends you an email messages with a list of masters and slaves)\nthen use ssh to login on any of the masters:\nssh -A -l ubuntu <master-public-ip>\n\nYou must install the Python bindings for Apache Mesos.\nThe default version of Mesos changes in this code as there are updates to Elastic Mesos,\nsince the tutorials are based on that service.\nYou can check http:\/\/mesosphere.io\/downloads\/ for the latest.\nIf you run Mesos in different environment,\nsimply make a one-line change to the EGG environment variable in the bin\/local_install.sh script.\nAlso, you need to install the Exelixi source.\nOn the Mesos master, download the master branch of the Exelixi code repo on GitHub and install the required libraries:\nwget https:\/\/github.com\/ceteri\/exelixi\/archive\/master.zip ; \\\nunzip master.zip ; \\\ncd exelixi-master ; \\\n.\/bin\/local_install.sh\n\nIf you've customized the code by forking your own GitHub code repo, then substitute that download URL instead.\nAlternatively, if you've customized by subclassing the uow.UnitOfWorkFactory default GA,\nthen place that Python source file into the src\/ subdirectory.\nNext, run the installation command on the master, to set up each of the slaves:\n.\/src\/exelixi.py -n localhost:5050 | .\/bin\/install.sh\n\nNow launch the Framework, which in turn launches the worker services remotely on slave nodes.\nIn the following case, it runs workers on two slave nodes:\n.\/src\/exelixi.py -m localhost:5050 -w 2\n\nOnce everything has been set up successfully, the log file in exelixi.log will show a line:\nall worker services launched and init tasks completed\n\nFrom there, the GA runs.\nSee a GitHub gist for an example of a successful run.\nBlame List\nPaco Nathan\n","331":"notice\nUnfortunatly I can no longer support this project. We moved to AppFollow it supports App Store, Google Play and Windows Phone Store.\nInterstellar\nSmall ruby app to get reviews for you Google Play Store-released application to the Slack channel.\n\nWhy do we need it\nMonitoring Google Play reviews is a must for a responsible Android developer.\nGoing every morning to the webpage, finding what's the last you've already answered does not sound like a 2015-thing.\nUsers treat Google Play reviews as a way to seek for support.\nRemember, you have only one hit. Only your first reply is being emailed to the user. Consecutive edits of your reply won\u2019t be emailed to the user as the first one.\nReplies help a lot in troubleshooting, especially given the range of different devices and OS versions on the market.\nPlus, you want a good rating in the Google Play Store, right?\nHow it works\nGoogle Play exports all your app reviews once a day to the Google Cloud Storage bucket.\nInterstellar downloads reviews via google-provided gsutil tool and triggers Slack incoming webhook for all new or updated reviews.\nIt is intended to be fired once a day via cron.\nConfiguration\n\ncreate a file secrets\/secrets.yml. There is an example in secrets\/secrets.yml.example.\n\nYou will need to provide:\n\nreport bucket id. Found in the Reviews page of Google Play Developer console, e.g. pubsite_prod_rev_01234567890987654321\npackage name, eg com.example.reader, found in the Google Play Developer console\nslack incoming webhook url, create new one via direct link once you've logged in to the slack\n\n\n\nconfigure gsutil. It\u2019s a python app from Google, instructions provided below.\n\n\ngem install rest-client\n\n\nGsutil configuration\n\nRun gsutil\/gsutil config and follow the steps.\nOnce done, there will be a .boto file in your home dir.\nCopy this file to the .\/secrets folder and you are good to go.\n\nYou can always get the latest gsutil(https:\/\/cloud.google.com\/storage\/docs\/gsutil_install) and change this line\nsystem \u201cBOTO_PATH=.\/secrets\/.boto gsutil\/gsutil cp -r gs:\/\/#{CONFIG[\u201capp_repo\u201d]}\/reviews\/#{csv_file_name} .\u201d\nto point it to whatever place you like. Keep in mind though, that the sender.rb script expects that the csv file is in the same folder.\nUsage\nOnce configured - run ruby sender.rb\nLicense\nThis piece of software is distributed under 2-clause BSD license.\nWell, actually, you code it yourself during the coffee-break.\n","332":"#Sticky\n\n###D3 + IPython widgets\nSticky is a framework for easily integrating IPython widgets with D3 libraries.\nWhy\nBecause the IPython widgets and comms give us a very powerful communication layer between Python and D3. This allows for dynamic data updating and the potential for other fun things, such as linked chart brushing where brush events trigger Python data filtering.\nStatus\nThe most alpha of alpha. There are no tests yet. d3plus charts have strange updating behavior.\nExperimentation invited, production highly discouraged.\nSo what can I do with this thing?\nGo run the example notebook. Make some charts with the integrated libraries. Read the code. Add new libraries!\n","333":"###\u8c46\u74e3\u5c0f\u7ec4\u56fe\u7247\u91c7\u96c6\u7a0b\u5e8f\n#####By \u80be\u865a\u516c\u5b50\n\u5b98\u7f51: http:\/\/Douban.miaowu.asia\n\u4e3b\u7a0b\u5e8f:dou2.py\nMac\/Liunx \u8fd0\u884c\u7a0b\u5e8f(python dou2.py)\u5373\u53ef\u3002\nWindows\u7528\u6237\u8bf7\u4e0b\u8f7d\u538b\u7f29\u5305: \u4e0b\u8f7d\n\u7a0b\u5e8f\u57fa\u672c\u529f\u80fd\n\n\u7528\u6237\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u7528\u6237\u7ec4\u4e0b\u8f7d\u56fe\u7247\u652f\u6301\u8c46\u74e3\u6240\u6709\u5c0f\u7ec4\n\u672c\u7a0b\u5e8f\u81ea\u52a8\u91c7\u96c6\u4ee3\u7406\n\u91c7\u96c6\u4ee3\u7406\u540e\u968f\u673a\u9009\u62e9\n\u81ea\u52a8\u4e0b\u8f7d\u56fe\u7247\u5e76\u4fdd\u5b58\n\n\u83b7\u53d6\u7528\u6237\u7ec4ID\u65b9\u6cd5\nhttp:\/\/www.douban.com\/group\/264964\/ #\u53ea\u9700\u8981\u8f93\u5165\u4ebagroup\/\u540e\u9762\u7684\u5b57\u7b26 \u4e0d\u5305\u62ec\u659c\u6760\n2015-6-3 \u66f4\u65b0\n\n\u65e0\u9700\u7528\u6237\u624b\u52a8\u521b\u5efa\u6587\u4ef6\u5939\n\u66f4\u65b0\u53ef\u7528\u6027\u53bb\u6389\u4ee3\u7406\u91c7\u96c6\n\u4f18\u5316Windows\u5ba2\u6237\u7aef\u5927\u5c0f\u4ee5\u53ca\u6587\u4ef6\u6570\u91cf\n\n2014-7-17 \u66f4\u65b0\n\n\u4fee\u6539\u7a0b\u5e8fBug\n\u589e\u52a0\u9519\u8bef\u8f93\u51fa\n\u52a0\u5feb\u7a0b\u5e8f\u6267\u884c\u6548\u7387\n\u5c1d\u8bd5\u542f\u7528\u591a\u7ebf\u7a0b\u5931\u8d25!\n\n2014-7-16 \u66f4\u65b0\n\n\u7528\u6237\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u7528\u6237\u7ec4\u4e0b\u8f7d\u56fe\u7247\u652f\u6301\u8c46\u74e3\u6240\u6709\u5c0f\u7ec4\n\u589e\u52a0\u9ed8\u8ba4\u529f\u80fd\n\u6a21\u62dfUA\n\u66f4\u6362\u4ee3\u7406\u6e90\nWin\u7248\u652f\u6301\n\u4f18\u5316\u7a0b\u5e8f\n\u7f8e\u5316\u7a0b\u5e8f\n\n","334":"###\u8c46\u74e3\u5c0f\u7ec4\u56fe\u7247\u91c7\u96c6\u7a0b\u5e8f\n#####By \u80be\u865a\u516c\u5b50\n\u5b98\u7f51: http:\/\/Douban.miaowu.asia\n\u4e3b\u7a0b\u5e8f:dou2.py\nMac\/Liunx \u8fd0\u884c\u7a0b\u5e8f(python dou2.py)\u5373\u53ef\u3002\nWindows\u7528\u6237\u8bf7\u4e0b\u8f7d\u538b\u7f29\u5305: \u4e0b\u8f7d\n\u7a0b\u5e8f\u57fa\u672c\u529f\u80fd\n\n\u7528\u6237\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u7528\u6237\u7ec4\u4e0b\u8f7d\u56fe\u7247\u652f\u6301\u8c46\u74e3\u6240\u6709\u5c0f\u7ec4\n\u672c\u7a0b\u5e8f\u81ea\u52a8\u91c7\u96c6\u4ee3\u7406\n\u91c7\u96c6\u4ee3\u7406\u540e\u968f\u673a\u9009\u62e9\n\u81ea\u52a8\u4e0b\u8f7d\u56fe\u7247\u5e76\u4fdd\u5b58\n\n\u83b7\u53d6\u7528\u6237\u7ec4ID\u65b9\u6cd5\nhttp:\/\/www.douban.com\/group\/264964\/ #\u53ea\u9700\u8981\u8f93\u5165\u4ebagroup\/\u540e\u9762\u7684\u5b57\u7b26 \u4e0d\u5305\u62ec\u659c\u6760\n2015-6-3 \u66f4\u65b0\n\n\u65e0\u9700\u7528\u6237\u624b\u52a8\u521b\u5efa\u6587\u4ef6\u5939\n\u66f4\u65b0\u53ef\u7528\u6027\u53bb\u6389\u4ee3\u7406\u91c7\u96c6\n\u4f18\u5316Windows\u5ba2\u6237\u7aef\u5927\u5c0f\u4ee5\u53ca\u6587\u4ef6\u6570\u91cf\n\n2014-7-17 \u66f4\u65b0\n\n\u4fee\u6539\u7a0b\u5e8fBug\n\u589e\u52a0\u9519\u8bef\u8f93\u51fa\n\u52a0\u5feb\u7a0b\u5e8f\u6267\u884c\u6548\u7387\n\u5c1d\u8bd5\u542f\u7528\u591a\u7ebf\u7a0b\u5931\u8d25!\n\n2014-7-16 \u66f4\u65b0\n\n\u7528\u6237\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u7528\u6237\u7ec4\u4e0b\u8f7d\u56fe\u7247\u652f\u6301\u8c46\u74e3\u6240\u6709\u5c0f\u7ec4\n\u589e\u52a0\u9ed8\u8ba4\u529f\u80fd\n\u6a21\u62dfUA\n\u66f4\u6362\u4ee3\u7406\u6e90\nWin\u7248\u652f\u6301\n\u4f18\u5316\u7a0b\u5e8f\n\u7f8e\u5316\u7a0b\u5e8f\n\n","335":"###\u8c46\u74e3\u5c0f\u7ec4\u56fe\u7247\u91c7\u96c6\u7a0b\u5e8f\n#####By \u80be\u865a\u516c\u5b50\n\u5b98\u7f51: http:\/\/Douban.miaowu.asia\n\u4e3b\u7a0b\u5e8f:dou2.py\nMac\/Liunx \u8fd0\u884c\u7a0b\u5e8f(python dou2.py)\u5373\u53ef\u3002\nWindows\u7528\u6237\u8bf7\u4e0b\u8f7d\u538b\u7f29\u5305: \u4e0b\u8f7d\n\u7a0b\u5e8f\u57fa\u672c\u529f\u80fd\n\n\u7528\u6237\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u7528\u6237\u7ec4\u4e0b\u8f7d\u56fe\u7247\u652f\u6301\u8c46\u74e3\u6240\u6709\u5c0f\u7ec4\n\u672c\u7a0b\u5e8f\u81ea\u52a8\u91c7\u96c6\u4ee3\u7406\n\u91c7\u96c6\u4ee3\u7406\u540e\u968f\u673a\u9009\u62e9\n\u81ea\u52a8\u4e0b\u8f7d\u56fe\u7247\u5e76\u4fdd\u5b58\n\n\u83b7\u53d6\u7528\u6237\u7ec4ID\u65b9\u6cd5\nhttp:\/\/www.douban.com\/group\/264964\/ #\u53ea\u9700\u8981\u8f93\u5165\u4ebagroup\/\u540e\u9762\u7684\u5b57\u7b26 \u4e0d\u5305\u62ec\u659c\u6760\n2015-6-3 \u66f4\u65b0\n\n\u65e0\u9700\u7528\u6237\u624b\u52a8\u521b\u5efa\u6587\u4ef6\u5939\n\u66f4\u65b0\u53ef\u7528\u6027\u53bb\u6389\u4ee3\u7406\u91c7\u96c6\n\u4f18\u5316Windows\u5ba2\u6237\u7aef\u5927\u5c0f\u4ee5\u53ca\u6587\u4ef6\u6570\u91cf\n\n2014-7-17 \u66f4\u65b0\n\n\u4fee\u6539\u7a0b\u5e8fBug\n\u589e\u52a0\u9519\u8bef\u8f93\u51fa\n\u52a0\u5feb\u7a0b\u5e8f\u6267\u884c\u6548\u7387\n\u5c1d\u8bd5\u542f\u7528\u591a\u7ebf\u7a0b\u5931\u8d25!\n\n2014-7-16 \u66f4\u65b0\n\n\u7528\u6237\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u7528\u6237\u7ec4\u4e0b\u8f7d\u56fe\u7247\u652f\u6301\u8c46\u74e3\u6240\u6709\u5c0f\u7ec4\n\u589e\u52a0\u9ed8\u8ba4\u529f\u80fd\n\u6a21\u62dfUA\n\u66f4\u6362\u4ee3\u7406\u6e90\nWin\u7248\u652f\u6301\n\u4f18\u5316\u7a0b\u5e8f\n\u7f8e\u5316\u7a0b\u5e8f\n\n","336":"Morphos\n\n\n\n\n They say a pic is worth a 1000 words. Is it true to admit a .gif is worth a 1000 pics? \n  \nInclude in your project\n In your root\/build.gradle\nallprojects {\n  repositories {\n  ...\n  maven { url 'https:\/\/jitpack.io' }\n  }\n}  \n In your app\/build.gradle\ndependencies {\n  compile 'com.github.rjsvieira:morphos:1.0.0'\n}\nIntroducing Morphos : an animation wrapper.\nMorphos will take care of your views' animations without you having to write all that boring boilerplate code.\nInitialization\nMorphos are easy to interact with. Go ahead and create the following simple Morpho :\nView viewToAnimate = ... ;\nMorpho morph = new Morpho(viewToAnimate)\n  .translate(50,50,0,1500) \/\/ will translate the view (x=50,y=50,z=0) in 1500 milliseconds, \n  .rotationXY(45,45,2000); \/\/ will rotate the view by 45 degrees on both the X-axis and Y-axis in 2000 milliseconds\nYou can then animate it by doing\nmorph.animate(); \nWhich will then animate the desired Morphos using the default animation type (SEQUENTIAL).\nWhat if I want to reverse the animation? Sure, just do :\nmorph.reverse();\n Configuration \nCreate a Morpho\nMorpho morphoOne = new Morphos(view);\nConfigure the Morphos' animations\nAs of the first release, Morphos supports the 7 most basic and common animations.\nSince Morphos has plenty of combinations for interpolation, animation type, duration, etc, the user is allowed to configure them according to his needs.\nEvery animation configuration method returns the Morphos object itself, thus allowing the user to chain his preferred animations.\nNote : If the user does not specify the duration and\/or interpolator, the animation will assume a 0 second duration and the default interpolator.\nalpha(double value)\nalpha(double value, int duration)\nalpha(double value, int duration, Interpolator interpolator)\n\nscale(double valueX, double valueY)\nscale(double valueX, double valueY, int duration)\nscale(double valueX, double valueY, int duration, Interpolator interpolator)\n\ntranslationX(AnimationTarget target, float valueX)\ntranslationX(AnimationTarget target, float valueX, int duration)\ntranslationX(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\n\ntranslationY(AnimationTarget target, float valueX)\ntranslationY(AnimationTarget target, float valueX, int duration)\ntranslationY(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\n\ntranslationZ(AnimationTarget target, float valueX)\ntranslationZ(AnimationTarget target, float valueX, int duration)\ntranslationZ(AnimationTarget target, float valueX, int duration, Interpolator interpolator)\n\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ)\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ, int duration)\ntranslation(AnimationTarget target, float valueX, float valueY, float valueZ, int duration, Interpolator interpolator)\n\ndimensions(float width, float height)\ndimensions(float width, float height, int duration)\ndimensions(float width, float height, int duration, Interpolator interpolator)\n\nrotationXY(AnimationTarget target, double degreesX, double degreesY)\nrotationXY(AnimationTarget target, double degreesX, double degreesY, int duration)\nrotationXY(AnimationTarget target, double degreesX, double degreesY, int duration, Interpolator interpolator)\n\nrotation(AnimationTarget target, double degrees)\nrotation(AnimationTarget target, double degrees, int duration)\nrotation(AnimationTarget target, double degrees, int duration, Interpolator interpolator)\nSet a Listener to track the Morphos' animation process\nmorphoOne.setListener(new Animator.AnimatorListener() {\n  @Override\n  public void onAnimationStart(Animator animator) {\n      System.out.println(\"Start\");\n  }\n\n  @Override\n  public void onAnimationEnd(Animator animator) {\n      System.out.println(\"End\");\n  }\n\n  @Override\n  public void onAnimationCancel(Animator animator) {\n\n  }\n\n  @Override\n  public void onAnimationRepeat(Animator animator) {\n\n  }\n})  \n Animate() the Morpho\nThis can be done through one of the several methods created just to make the invocation easier.\nThe animate() method's default values are : \n\nAnimationType : SEQUENTIAL - All animations are executed one sequentially\nDuration : -1 - Since no duration was specified, -1 is assumed thus executing every animation in its given duration. For example : morphoOne.translate(100,0,0,3000).scale(2,2,2000).animate() will execute the translation animation in 3 seconds, followed by an upscale animation of 2 seconds\nInterpolator - The overall interpolator (LinearInterpolator)\nanimate()\nanimate(AnimationType type, int duration)\nanimate(AnimationType type, int duration, Interpolator interpolator)\nReverse\nAfter executing animate(), the user can rollback the animation by invoking the reverse() method.\nThe reverse method works just like the animate() method, having the same combinations and parameters.\nreverse()\nreverse(AnimationType type, int duration)\nreverse(AnimationType type, int duration, Interpolator interpolator)\nCancel\nIf the user wishes to cancel the on-going animation by any reason, he can do so by invoking the cancel method :\nmorphoOne.cancel();\nupdateView\nLike the method explicitly indicates, the user can update the view associated with the Morpho. This will clear every animation already configured for the given object.\nupdateView(View v) also invokes reset();\nmorphoOne.updateView(newView);\nReset\nIf by any chance the user wants to reset he Morpho and re-build it from scratch, he can do so by invoking the reset() method\nmorphoOne.reset()\nDispose\nLast but not least, if they user wishes to discard the Morphos object, he can invoke the dispose() method, thus clearing and preparing the Morphos' inner variables for garbage collection.\nmorphoOne.dispose();\n","337":"This is library project with a custom view that implements concept of Submit Button (https:\/\/dribbble.com\/shots\/1426764-Submit-Button?list=likes&offset=3) made by Colin Garven.\n###Demo###\n\n###Usage###\n <com.tuesda.submit.SubmitView\n        android:layout_centerInParent=\"true\"\n        android:id=\"@+id\/submit\"\n        android:layout_width=\"200dp\"\n        android:layout_height=\"200dp\" \/>\nmSubmit.setOnProgressStart(new SubmitView.OnProgressStart() {\n            @Override\n            public void progressStart() {\n                \/\/ do something when progress start\n            }\n        });\n        \n        mSubmit.setOnProgressDone(new SubmitView.OnProgressDone() {\n            @Override\n            public void progressDone() {\n                \/\/ do something when progress is done\n            }\n        });\n###public interface###\n\n\n\n\u51fd\u6570\u540d\n\u4f5c\u7528\n\n\n\n\nsetBackColor(int color)\n\u8bbe\u7f6e\u56fe\u6807\u80cc\u666f\u8272\uff0c\u9ed8\u8ba4\u662f\u7eff\u8272(0xff00cd97)\uff0c\u4e0a\u56feDemo\u8bbe\u7f6e\u4e3a\u84dd\u8272(0xff0097cd)\n\n\nsetText(String str)\n\u8bbe\u7f6e\u6309\u94ae\u540d\u5b57\uff0c\u9ed8\u8ba4\u662fSubmit\n\n\nreset()\n\u5c06\u6309\u94ae\u91cd\u7f6e\u5230\u521d\u59cb\u72b6\u6001\n\n\nsetProgress(float progress)\n\u8bbe\u7f6e\u6b63\u5728\u6267\u884c\u5de5\u4f5c\u7684\u6267\u884c\u8fdb\u7a0b\n\n\nisProgressDone()\n\u6b63\u5728\u6267\u884c\u5de5\u4f5c\u662f\u5426\u5b8c\u6210\n\n\nsetOnProgressStart(OnProgressStart listener)\n\u8bbe\u7f6eprogress\u5f00\u59cb\u56de\u8c03\n\n\nsetOnProgressDone(OnProgressDone listener)\n\u8bbe\u7f6eprogress\u5b8c\u6210\u56de\u8c03\n\n\n\n","338":"SugaredListAnimations\nSugaredListAnimations is a library that pretends to add animations to your listview with minor changes to your already existing code.\nCurrently the available animations are:\n\nGoogle Plus alike\nGoogle Now alike\n\nVersion\n0.9 - Yeah, it's still beta\nDemo\nSee demo here\nLicense\nMIT\nFree Software, Fuck Yeah!\n","339":"SugaredListAnimations\nSugaredListAnimations is a library that pretends to add animations to your listview with minor changes to your already existing code.\nCurrently the available animations are:\n\nGoogle Plus alike\nGoogle Now alike\n\nVersion\n0.9 - Yeah, it's still beta\nDemo\nSee demo here\nLicense\nMIT\nFree Software, Fuck Yeah!\n","340":"amap-running-app\n\nUse weex-amap to build a running app\nHow to use\n1.\u9996\u5148\u514b\u9686\u8fd9\u4e2a\u9879\u76ee(\u540e\u9762\u4f1a\u5199\u5982\u4f55\u81ea\u5df1\u521b\u5efa\u8fd9\u6837\u7684\u9879\u76ee). \u786e\u4fdd\u4f60\u81ea\u5df1\u73af\u5883\u5b89\u88c5\u4e86weex-toolkit\ngit clone https:\/\/github.com\/weex-plugins\/amap-running-app\n2.\u8fdb\u5165\u514b\u9686\u7684\u9879\u76ee\u76ee\u5f55\uff0c\u7136\u540e\u6267\u884c npm install\n3.\u6d4b\u8bd5\u4f60\u7684\u9700\u8981\u8fd0\u884c\u7684\u5e73\u53f0\uff0c\u6bd4\u5982android \u6216\u8005 ios\nweex platform add android\n4.\u6dfb\u52a0\u63d2\u4ef6 weex-amap\nweex plugin add weex-amap\n\u8fd9\u4e2a\u65f6\u5019\u4f60\u5c31\u53ef\u4ee5\u8fd0\u884c\u547d\u4ee4\u770b\u5177\u4f53\u8fd0\u884c\u7684\u6548\u679c\u4e86\uff1a\nweex run android\n\u5982\u679c\u4f60\u81ea\u5df1\u4f7f\u7528weex-toolkit\u521b\u5efa\u9879\u76ee\uff0c\u4f60\u53ea\u9700\u8981\u8fd9\u6837\u505a\uff1a\nweex create runningapp\ncd runningapp && npm install\nweex platform add android\nweex plugin add weex-amap\nweexpack run android\n\n\u8fd0\u884cdemo\u622a\u56fe\niOS \u7248\n\nAndroid \u7248\n\n\u622a\u56fe\u6570\u636e\u4ec5\u4f9b\u6f14\u793a\n","341":"AndroidMVVM\n\nNote: issue the following in command line - gradlew build --refresh-dependencies in Windows or .\/gradlew build --refresh-dependencies in Linux or Mac to force update to the latest RoboBinding snapshot when required.\n\n\nA minimal Android app with MVVM pattern based on RoboBinding.\n\n\nThe project can be directly imported into Android Studio.\n","342":"Quarks has been renamed to Apache Edgent (incubating)\nApache Edgent is an open source programming model and runtime for edge devices that enables you to analyze data and events at the device.\nGo to http:\/\/edgent.incubator.apache.org\/ for information about Apache Edgent.\nYou will find the new code repository where you can create pull requests at https:\/\/github.com\/apache\/incubator-edgent.\nPlease joins us by subscribing to the developer mailing list dev at edgent.incubator.apache.org.\nTo subscribe send an email to dev-subscribe at edgent.incubator.apache.org.\n","343":"Carat has moved! Please find the latest code at: https:\/\/github.com\/carat-project\/carat\n","344":"Simple RxJava library for observing and requesting Android runtime permissions introduced in Android 6.0.\nObserving permissions\nSometimes it makes sense to ask user for absolutely required permissions in a separate onboarding step. Simple code down below shows either main or onboarding fragment depending on permissions granted, without asking user for those permissions. This is pure observe case.\npublic class MainActivity extends AppCompatActivity {\n\n    @Override public void onStart() {\n        super.onStart();\n        \n        mSubscription = RxPermissions.get(this)\n              .observe(Manifest.permission.WRITE_EXTERNAL_STORAGE,\n                       Manifest.permission.READ_EXTERNAL_STORAGE)\n              .subscribe(granted -> {\n                  if (granted) {\n                      \/\/ you can pass to main fragment\n                  } else {\n                      \/\/ you can open onboarding fragment \n                  }\n              });\n    }\n    \n    @Override public void onStop() {\n        mSubsrciption.unsubsribe();\n        mSubsrciption = null;\n        super.onStop();\n    }\n}\n\nNote: Returned observable does never complete, so be sure to unsubscribe properly.\nRequesting permissions\nRequesting permissions requires additional binding to component (an activity or a fragment) which can ask user for permissions and receives result. Then you can request permissions. Here is how you do it.\npublic class OnboardingFragment extends Fragment {\n    private static final int REQ_PERMISSIONS = 101;\n\n    private final PermissionsRequester mPermissionsRequester = new PermissionsRequester() {\n        @Override public void performRequestPermissions(String[] permissions) {\n            \/\/ forward request to the system by calling fragment's method\n            requestPermissions(permissions, REQ_PERMISSIONS);\n        }\n    };\n    \n    @Override\n    public void onRequestPermissionsResult(int requestCode, String[] permissions, int[] grantResults) {\n        if (requestCode == REQ_PERMISSIONS) {\n            \/\/ forward response back to requester class for further processing\n            mPermissionsRequester.onRequestPermissionsResult(permissions, grantResults);\n        }\n    }\n    \n    @Override public void onViewCreated(View view, Bundle savedInstanceState) {\n        super.onViewCreated(view, savedInstanceState);\n        \n        mSubscription = RxPermissions.get(getActivity())\n            .request(mPermissionsRequester, \n                        Manifest.permission.WRITE_EXTERNAL_STORAGE,\n                        Manifest.permission.READ_EXTERNAL_STORAGE)\n            .subscribe(granted -> {\n                if (granted) {\n                    \/\/ permissions were granted, observable completes\n                } else {\n                    \/\/ user denied request\n                }\n            });\n    }\n    \n    @Override public void onDestroyView() {\n        mSubsrciption.unsubsribe();\n        mSubsrciption = null;\n        super.onDestroyView();\n    }\n    \n    public void onButtonClicked(View view) {\n        mPermissionsRequester.request();\n    }\n}\nObservable returned by RxPermissions.request() method does only complete when all requested permissions are granted. Until then it emitts false every time user denies a permission request. You can show user a better explanation of why the app needs these permissions and ask for permissions again. Use PermissionsRequester.request() method for triggering permissions request dialog once again.\nTests & stability\nAlthough this library has unit tests covering whole functionality described above, there is still no productive apps using it yet.\nCredits\nThis library was inspired by https:\/\/github.com\/tbruyelle\/RxPermissions, but it uses a bit different design allowing pure observation of permissions and follow up permissions request without necessity to create new observable.\nLicense\nCopyright (c) 2015, 2016 Sergej Shafarenka, halfbit.de\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","345":"JFreeChart - Future State Edition (FSE)\nThis is a development version of JFreeChart that has been branched from the\nJFreeChart 1.0.x series.  Here are some important notes:\n\n\nthe API has been changed and will continue to change until I (and the other\ncontributors) feel it is ready to be frozen - that may be in 6 months, or\nit may be in 6 years;\n\n\nMaven is used to build this edition of JFreeChart, and the default build\nrequires JDK 1.8 (because we include JavaFX support in the build);\n\n\nthere is an Ant script that will build the code without the JavaFX support\nclasses.  This requires AT LEAST JDK 1.6.0 to compile and, depending\non how I (and the other contributors) feel, in the future we may\nrequire JDK 1.7 or even a later version;\n\n\nJCommon is no longer a dependency (the subset of classes from JCommon that\nare still used have been incorporated directly in the source tree);\n\n\nThe aim is to modernise the JFreeChart API and code and have some fun with new\nstuff and NOT to be constrained by backwards compatibility.\nDavid Gilbert (david.gilbert@object-refinery.com)\nJFreeChart Project Leader\n\n","346":"About in_constexpr\nAn approach of detecting if inside a constexpr context in a constexpr function.\nBy being able to detect if we're within a constexpr context we can choose to implement\na runtime specific algorithm while having a different algorithm for doing something at compile time.\nThe approach is discussed in this post\nFeatures\n\nif (in_constexpr()\/in_runtime()) - Being able to detect if in runtime or compile time\nsmart_assert                     - constexpr friendly assert\n\nConstraints\n\nOnly tested on GCC 5+ and Clang 3.8+\nx86 only so far\nRuntime approach only works on linux (replacing code in the binary should work under windows for now)\nSee detailed descriptions about caveats here\nThe library must be statically linked\nThe initialization logic relies on being able to modify the .text segment of your program\n\nUsing the library\nBuilding and installing\nYou can compile and install this library using cmake. You will need to link this library in as a static library.\nYou can customize and install the library using the following:\ncmake <code directory> -DCMAKE_INSTALL_PREFIX=<install directory> && make -j && make install\n\nThen just link in as you would a normal library. You can try the examples in examples\/ to see how to use the library.\nUsing the API and an example\nTo test if this library is working, you can call in_constexpr::is_setup() to see if it is working. The library should automatically call the setup by means of the constructor\nattribute and so you shouldn't need to explicitly call in_constexpr::initialize() but if it's not happening automatically, you can call that function.\nThe library provides an  in_constexpr() and in_runtime() macro\/method that returns if a constexpr function is within which context. This can be used to provide different code paths in each case.\nNote, you can't do something like if (!in_constexpr())  due to those methods being a syntatic sugar. Use either if (in_constexpr()) or if (in_runtime()).\nThe library also provides a smart_assert which you can use regular assert but stil work within constexpr functions.\nYou can find more examples under the examples folder.\nconst int N_MAX            = 30;\nint factorial_cache[N_MAX] = {0};\n\nconstexpr int factorial(int n) {\n  smart_assert(n >= 0 && n < N_MAX, \"N >= 0 && N <= N_MAX\");\n\n  if (n == 0)\n    return 1;\n  else {\n    if (in_constexpr()) {\n      return n * factorial(n - 1);\n    } else {\n      std::cout << \"Calling factorial \" << n << std::endl;\n      \/\/ Since we're in runtime, we can cache results.\n      if (factorial_cache[n] == 0) {\n        factorial_cache[n] = n * factorial(n - 1);\n      }\n      return factorial_cache[n];\n    }\n  }\n}\n\nint main() {\n  volatile int a = 5;\n  volatile int b = 6;\n  std::cout << factorial(a) << std::endl;\n  std::cout << factorial(b) << std::endl;\n\n  constexpr int c = factorial(3);\n  \/\/ Compiler error!\n  \/\/ constexpr int d = factorial(-5);\n\n  std::cout << c << std::endl;\n  \/\/ std::cout << factorial(d) << std::endl;\n  return 0;\n}\nWill produce the following outputs\nCalling factorial 5\nCalling factorial 4\nCalling factorial 3\nCalling factorial 2\nCalling factorial 1\n120\nCalling factorial 6\nCalling factorial 5\n720\n6\n\n","347":"This code is obsolete and the activity moved to https:\/\/github.com\/idni\/tau under a new design\n","348":"This code is obsolete and the activity moved to https:\/\/github.com\/idni\/tau under a new design\n","349":"text_extraction\nThis code is the implementation of the method proposed in the paper \u201cMulti-script text extraction from natural scenes\u201d (Gomez & Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\nIncludes the following third party code:\n\nfast_clustering.cpp Copyright (c) 2011 Daniel M\u00fcllner, under the BSD license. http:\/\/math.stanford.edu\/~muellner\/fastcluster.html\nmser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http:\/\/www.idiap.ch\/~cdubout\/\nbinomial coefficient approximations are due to Rafael Grompone von Gioi. http:\/\/www.ipol.im\/pub\/art\/2012\/gjmr-lsd\/\n\n","350":"text_extraction\nThis code is the implementation of the method proposed in the paper \u201cMulti-script text extraction from natural scenes\u201d (Gomez & Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\nIncludes the following third party code:\n\nfast_clustering.cpp Copyright (c) 2011 Daniel M\u00fcllner, under the BSD license. http:\/\/math.stanford.edu\/~muellner\/fastcluster.html\nmser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http:\/\/www.idiap.ch\/~cdubout\/\nbinomial coefficient approximations are due to Rafael Grompone von Gioi. http:\/\/www.ipol.im\/pub\/art\/2012\/gjmr-lsd\/\n\n","351":"FaceDetect\n\n\n\n\n\n\n\n\nFace detection for iPhone using OpenCV\nAbout\nThis project is an example of using OpenCV on the iPhone.\nIt provides a simple application that let you take a picture (or select one from your photo library), and that detects the face(s) on the picture.\nBuilding\nThe application does not compile (yet) for the iPhone simulator.\nYou need to build it for your iOS device in order to test it.\nOpenCV port\nFaceDetect uses the OpenCV port provided by Macmade (myself), and which is also available on GitHub.\nProject Status\nThis project is no longer maintained.\nIt might not build on latest iOS versions, and might not be compatible with latest OpenCV versions.\nLicense\nFaceDetect is released under the terms of the Boost Software License - Version 1.0.\nRepository Infos\nOwner:\t\t\tJean-David Gadina - XS-Labs\nWeb:\t\t\twww.xs-labs.com\nBlog:\t\t\twww.noxeos.com\nTwitter:\t\t@macmade\nGitHub:\t\t\tgithub.com\/macmade\nLinkedIn:\t\tch.linkedin.com\/in\/macmade\/\nStackOverflow:\tstackoverflow.com\/users\/182676\/macmade\n\n","352":" _______  _______  _______  _______  _______  _______  _______  _______  _______ \n|       ||   _   ||       ||       ||       ||   _   ||       ||       ||       |\n|    _  ||  |_|  ||  _____||_     _||    _  ||  |_|  ||    ___||    ___||  _____|\n|   |_| ||       || |_____   |   |  |   |_| ||       ||   | __ |   |___ | |_____ \n|    ___||       ||_____  |  |   |  |    ___||       ||   ||  ||    ___||_____  |\n|   |    |   _   | _____| |  |   |  |   |    |   _   ||   |_| ||   |___  _____| |\n|___|    |__| |__||_______|  |___|  |___|    |__| |__||_______||_______||_______|\nThe news homepage archive. At pastpages.org.\nHow it works\nPastPages is built using Python, Django and PostgreSQL. Screenshots are taken by PhantomJS via a Celery task queue. The files collected are stored in the Rackspace cloud. Deployment is managed with Chef and you can review how the environment is configured in its configuration files.","353":" _______  _______  _______  _______  _______  _______  _______  _______  _______ \n|       ||   _   ||       ||       ||       ||   _   ||       ||       ||       |\n|    _  ||  |_|  ||  _____||_     _||    _  ||  |_|  ||    ___||    ___||  _____|\n|   |_| ||       || |_____   |   |  |   |_| ||       ||   | __ |   |___ | |_____ \n|    ___||       ||_____  |  |   |  |    ___||       ||   ||  ||    ___||_____  |\n|   |    |   _   | _____| |  |   |  |   |    |   _   ||   |_| ||   |___  _____| |\n|___|    |__| |__||_______|  |___|  |___|    |__| |__||_______||_______||_______|\nThe news homepage archive. At pastpages.org.\nHow it works\nPastPages is built using Python, Django and PostgreSQL. Screenshots are taken by PhantomJS via a Celery task queue. The files collected are stored in the Rackspace cloud. Deployment is managed with Chef and you can review how the environment is configured in its configuration files.","354":"The AndroidTransporterPlayer is a media player for the Raspberry Pi\nDemo video\n\nTake a look at the Android Transporter blog post for more information.\nSetup\nsudo apt-get install git-core\ncd \/opt\/vc\/src\/hello_pi\/libs\/ilclient\nmake\ncd \/home\/pi\nmkdir Projects\ncd Projects\ngit clone https:\/\/github.com\/esrlabs\/AndroidTransporterPlayer.git\ngit clone -b android-transporter https:\/\/github.com\/esrlabs\/Mindroid.cpp.git Mindroid\ngit clone https:\/\/github.com\/esrlabs\/fdk-aac.git\ncd Mindroid\nmake -f Makefile.RPi\nsudo cp libmindroid.so \/usr\/lib\/\ncd ..\ncd fdk-aac\nmake -f Makefile.RPi\nsudo cp libaac.so \/usr\/lib\/\ncd ..\ncd AndroidTransporterPlayer\nmake -f Makefile.RPi\n.\/AndroidTransporterPlayer rtsp:\/\/<IP-Address>:9000\/test.sdp\n\nUsage\n<IP-Address> is always the IP address of the VLC streaming server.\nRaspberry Pi media player\n\/home\/pi\/AndroidTransporterPlayer\/AndroidTransporterPlayer rtsp:\/\/<IP-Address>:9000\/test.sdp\nVLC streaming server\nvlc <video>.mp4 --sout '#rtp{sdp=rtsp:\/\/<IP-Address>:9000\/test.sdp}' --rtsp-timeout=-1\n","355":"APRIL-ANN\n Travis CI (master branch)\n Travis CI (devel branch)\nUser documentation\nFollow wiki for user documentation.\nTutorials\nTutorials are available at april-ann-tutorials\nrepository.\nContributions\nContributions are wellcome. Only pull requests to devel branch will be\naccepted, so avoid to create pull requests to master. A Travis CI instance\nwill check that your request passes all tests. For Lua unit testing use the\npackage basics\/utest, and for C++ unit testing the package basics\/gtest.\nAt the end of this document there are information about Doxygen documentation\nwhich can be useful for C\/C++ developing. For Lua developing use the wiki.\nIt is possible to make a pull request directly to master branch for bug or\nmemory leak solving.\nDependencies and basic setup\nClone the repository and enter to it:\n$ git clone https:\/\/github.com\/april-org\/april-ann.git\n$ cd april-ann\n\nThe first time you want to run APRIL-ANN, you need to install\ndependencies in Linux (via apt-get) and in MacOS X (via MacPorts\/Homebrew).\nThere is an automatic shellscript which covers majorly Ubuntu (other Debian\nbased distributions can be supported) and MacOS X systems. You just need\nto run:\n$ .\/DEPENDENCIES-INSTALLER.sh\nRequirements, installed by previous script\nRequires the following libraries. Versions are only orientative, it could work\nwith older and newer versions whenver the API was compatible.\n\nGNU C++ compiler (g++): v 4.7.2\nOnly in Linux systems: Lua 5.2 headers to tell APRIL-ANN the default system\npath for Lua modules (lua5.2-deb-multiarch.h header).\nBLAS implementation: ATLAS (v. 3), Intel MKL (v. 10.3.6), MacOS Accelerate Framework\nThreads posix (pthread)\nReadline (libreadline)\nOpenMP\nLAPACK library, offered by liblapack_atlas, mkl_lapack, or MacOS Accelerate Framework\nLAPACKE library when compiling with ATLAS\n\nThe following libreries are recommended, but optional, you will need to remove\nits package from the path profile_build_scripts\/package_list.lua:\n\n[OPTIONAL] libpng: if you want to open PNG images, package libpng.\n[OPTIONAL] libtiff: if you want to open TIFF images, package libtiff.\n[OPTIONAL] libz: support for open of GZIPPED files, package gzio.\n[OPTIONAL] libzip: support for open ZIP packages, package zip.\n\nFor perform computation on GPU, this optional library, which has an specific\nmake target:\n\n[OPTIONAL] CUDA and CUBLAS: release 4.2.6.\n\nCompilation\nFirst, it is mandatory to configure the repo PATH and other stuff.\nThis step is needed every time you start a session, and before compilation:\n$ . configure.sh\nSecond, you can compile the APRIL-ANN version which you need.\nWe have developed compiling files for using different libraries.\nThe most simple compilation way is\n$ make\nand it would detect automatically your platform (Darwin or Linux) and your\ntarget SUFIX (mkl, atlas, macports or homebrew). The automatic targets are:\n\nrelease automatic platform and sufix detection target (the same as make\nwithout any argument).\ndebug automatic platform and sufix detection target for debug version.\ntest automatic platform and sufix detection target for test-debug version.\n\nThere are available more targets, just depending in your particular system\ninstallation you can execute them by using $ make TARGET:\n\nrelease-mkl needs of MKL library installed at \/opt\/MKL as prefix.\nrelease-atlas needs of OMP and ATLAS library.\nrelease-no-omp needs ATLAS library.\nrelease-cuda-mkl needs CUDA and MKL installed at \/opt\/MKL as prefix.\nrelease-macports needs Mac OS X with MacPorts and Accelerate Framework.\nrelease-homebrew needs Mac OS X with Homebrew and Accelerate Framework.\n\nBesides this targets, it is possible to compile for debug replacing release\nstring with debug string, and for testing replacing release by\ntest-debug.\nThe makefile has the following variables which can be forced by the user:\n\nPREFIX indicates the prefix for libraries and binaries. In Linux it is\n\/usr, in Darwin it depends on MacPorts (\/opt\/local) or Homebrew\n(\/usr\/local).\nLIB indicates shared library install directory, by default it is\n$(PREFIX)\/lib\nINCLUDE indicates header sources install directory, by default it is\n$(PREFIX)\/include\/april-ann\nLUALIB indicates where Lua library modules are installed, by default it is\n$(PREFIX)\/lib\/lua\/5.2\nLUAMOD indicates where Lua code modules are installed, by default it is\n$(PREFIX)\/share\/lua\/5.2 and there will be created april_tools folder with\na copy of the content of repository's tools folder.\nBIN indicates where you want to install binary files, by default it is\n$(PREFIX)\/bin\n\nEach of this targets will need a little configuration depending on your library\ninstallation. For example, in order to compile with MKL, the file\nprofile_build_scripts\/build_mkl_release.lua contains the following sections\n(among others):\n  global_flags = {\n    debug=\"no\",\n    use_lstrip = \"no\",\n    use_readline=\"yes\",\n    optimization = \"yes\",\n    platform = \"unix\",\n    extra_flags={\n      -- For Intel MKL :)\n      \"-DUSE_MKL\",\n      \"-I\/opt\/MKL\/include\",\n      --------------------\n      \"-march=native\",\n      \"-msse\",\n      \"-DNDEBUG\",\n    },\n    extra_libs={\n      \"-lpthread\",\n      -- For Intel MKL :)\n      \"-L\/opt\/MKL\/lib\",\n      \"-lmkl_intel_lp64\",\n      \"-Wl,--start-group\",\n      \"-lmkl_intel_thread\",\n      \"-lmkl_core\",\n      \"-Wl,--end-group\",\n      \"-liomp5\"\n    },\n  },\n\nYou need to especify the -I option to the compiler, and all the extra_libs\nstuff related with MKL.  Exists one build file for each possible target:\nbuild_release.lua, build_debug.lua, build_mkl_release.lua,\nbuild_mkl_debug.lua, ... and so on.\nThe binary will be generated at bin\/april-ann, which incorporates the Lua 5.2\ninterpreter and works without any dependency in Lua. Besides, a shared library\nand a Lua module will be generated at lib\/libapril-ann.so and\nlib\/aprilann.so, so it is possible to use require from Lua to load APRIL-ANN\nin a standard Lua 5.2 interpreter. In order to require aprilann module it is\nrequired the installation of both libraries in their corresponding place in your\nsystem. Normally this can be done executing $ sudo make install.\nNOTE that loading april-ann as a Lua 5.2 module, you need to have the\n.so library in the package.cpath or LUA_CPATH. It is possible to install it\nin your system defaults following next section.\nENJOY!\nInstallation\nThe installation is done executing:\n$ sudo make install\n\nThis procedure copies the binary to system location in \/usr (or in\n\/opt\/local for Mac OS X via MacPorts). The shared library is copied\nto Lua default directory, in order to load it by using require function.\nIf you are using a non default installation (a custom one), please\ncopy the .so files manually to your package.cpath or LUA_CPATH.\nUse\n\nYou can execute the standalone binary:\n\n$ april-ann\nAPRIL-ANN v0.2.1-beta COMMIT 920  Copyright (C) 2012-2013 DSIC-UPV, CEU-UCH\nThis program comes with ABSOLUTELY NO WARRANTY; for details see LICENSE.txt.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; see LICENSE.txt for details.\nLua 5.2.2  Copyright (C) 1994-2013 Lua.org, PUC-Rio\n> print \"Hello World!\"\nHello World!\n\n\nIt is possible to use APRIL-ANN as a Lua module, loading only the packages\nwhich you need (i.e. require(\"aprilann.matrix\")), or loading the full\nlibrary (require(\"aprilann\")). Be careful, the APRIL-ANN modules doesn't\nfollow Lua guidelines and have lateral effects because of the declaration of\ntables, functions, and other values at the GLOBALs Lua table. Before using\nAPRIL-ANN as a Lua module you need to install it into your system (currently\nonly available for Linux systems) by executing $ sudo make install.\n\n$ lua\nLua 5.2.2  Copyright (C) 1994-2013 Lua.org, PUC-Rio\n> require \"aprilann.matrix\"\n> require \"aprilann\"\nAPRIL-ANN v0.2.1-beta COMMIT 920  Copyright (C) 2012-2013 DSIC-UPV, CEU-UCH\nThis program comes with ABSOLUTELY NO WARRANTY; for details see LICENSE.txt.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; see LICENSE.txt for details.\n> print \"Hello World!\"\nHello World!\n\nCitation\nIf you are interested in use this software, please cite correctly the source. In academic publications\nyou can use this bibitem:\n@misc{aprilann,\n  Author = {Francisco Zamora-Mart\\'inez and Salvador Espa{\\~n}a-Boquera and\n\t        Jorge Gorbe-Moya and Joan Pastor-Pellicer and Adri\\'an Palacios-Corella},\n  Note = {{https:\/\/github.com\/april-org\/april-ann}},\n  Title = {{APRIL-ANN toolkit, A Pattern Recognizer In Lua with Artificial Neural Networks}},\n  Year = {2013}}\nPublications\nList of research papers which uses this tool:\n\n\nJoan Pastor-Pellicer, Salvador Espa\u00f1a-Boquera, Francisco Zamora-Mart\u00ednez,\nM. Zeshan Afzal, M.J. Castro-Bleda.\nInsights on the Use of Convolutional Neural Networks for Document Image Binarization,\nIWANN, Advances in Computational Intelligence, pages 115-126, 2015.\n\n\nJoan Pastor-Pellicer, Salvador Espa\u00f1a-Boquera, Francisco Zamora-Mart\u00ednez,\nMar\u00eda Jos\u00e9 Castro-Bleda.\nHandwriting Normalization by Zone Estimation using HMM\/ANNs,\nICFHR, pages 633-638, 2014.\n\n\nFrancisco Zamora-Mart\u00ednez, Pablo Romeu, Paloma Botella-Rocamora, and Juan Pardo.\nOn-line learning of indoor temperature forecasting models towards energy efficiency,\nEnergy and Buildings, 83:162-172, 2014.\n\n\nFrancisco Zamora-Mart\u00ednez, Pablo Romeu, Paloma Botella-Rocamora, and Juan\nPardo. Towards Energy Efficiency: Forecasting Indoor Temperature via Multivariate Analysis.\nEnergies, 6(9):4639-4659, 2013.\n\n\nPablo Romeu, Francisco Zamora-Martinez, Paloma Botella, and Juan Pardo.\nTime-Series Forecasting of Indoor Temperature Using Pre-trained Deep Neural Networks.\nIn ICANN, pages 451-458. 2013.\n\n\nJoan Pastor-Pellicer, Francisco Zamora-Martinez, Salvador Espa\u00f1a-Boquera, and M.J. Castro-Bleda.\nF-Measure as the error function to train Neural Networks.\nIn Advances in Computational Intelligence, IWANN, part I, LNCS, pages 376-384. Springer, 2013.\n\n\nF. Zamora-Mart\u00ednez, Pablo Romeu, Juan Pardo, and Daniel Tormo.\nSome empirical evaluations of a temperature forecasting module based on Artificial Neural Networks for a domotic home environment.\nIn IC3K - KDIR, pages 206-211, 2012.\n\n\nOur ancient ANN implementation in the former APRIL tookit was published here:\n\nS. Espa\u00f1a-Boquera, F. Zamora-Martinez, M.J. Castro-Bleda, J. Gorbe-Moya.\nEfficient BP algorithms for general feedforward neural networks.\nIn IWINAC, pages 327-336, 2007.\n\nOther projects using it\nCompetition participations and other projects where APRIL-ANN has been used:\n\nKaggle American Epilepsy Society Seizure Prediction Challenge,\nsystem ESAI-CEU-UCH positioned as 4th in the leaderboard.\nDownload from GitHub the code\nto run this system.\n\nPackages\nAPRIL-ANN is compiled following a package system. In the directory packages you could find a\ntree of directory entries. Leaves in the tree are directories which contain file \"package.lua\".\nThe \"package.lua\" defines requirements, dependencies, libraries, and other stuff needed by the\ncorresponding package.\nEach package could contain this directories:\n\nc_src: source files (.h, .cc, .c, .cpp, .cu, and others).\nbinding: binding files (.lua.cc), a kind of templatized file which generates the glue code between C\/C++ and Lua.\nlua_src: lua source files which define functions, tables, and pseudo-classes in Lua.\ndoc: doxygen documentation additional files.\ntest: examples and files for testing.\n\nAt root directory exists a file named \"package_list.lua\". It is a Lua table with the name of packages that\nyou want to compile. If you don't want or don't have libpng, or libtiff, or other library, you could\nerase the package name from this list to avoid its compilation.\nIncludes these sources\n\nLua virtual machine 5.2.2: http:\/\/www.lua.org\/\nLuiz's lstrip for Lua 5.1, adapted to compile with Lua 5.2.2: http:\/\/www.tecgraf.puc-rio.br\/~lhf\/ftp\/lua\/5.1\/lstrip.tar.gz\nMersenneTwister: http:\/\/www.math.sci.hiroshima-u.ac.jp\/~m-mat\/MT\/emt.html\nMedian filter from Simon Perreault: http:\/\/nomis80.org\/ctmf.html\nRuningStat class for efficient and stable computation of mean and variance: http:\/\/www.johndcook.com\/standard_deviation.html\nMike Pall's advanced readline patch for Lua: http:\/\/smbolton.com\/lua.html#readline\nGoogle C++ Testing Framework: https:\/\/code.google.com\/p\/googletest\/\n(Included, but not used) Lua autocompletion rlcompleter release 2, by rthomas:\nhttps:\/\/github.com\/rrthomas\/lua-rlcompleter\nLua base64 encoding developed by\nErnie Ewert.\n\nWiki documentation\n\nPDF version\nHTML one-page\n\nDoxygen documentation\nThe documentation of the devel branch will be mantained as updated as possible\nin the following links:\n\nC\/C++ developer manual\nC\/C++ binding manual\n\nHowever, you can produce the Doxygen documentation of the branch where\nyou are working by using the makefile's document target. Please, note that\nyou need to have installed Doxygen and\nGraphviz.\n$ make document\n$ open doxygen_doc\/developer\/html\/index.html\n\nThe last command can be substituted by you opening the indicated\nlocation in your prefered web browser ;)\nLINUX dependencies installation\nExecute: $ .\/DEPENDENCIES-INSTALLER.sh\nIf your distribution is not supported (currently only Ubuntu has support), then\ninstall g++, libatlas-dev, libreadline-dev, libpng-dev, libtiff-dev, libz-dev,\nlibopenmp-dev, libzip-dev, liblua5.2-dev.\nMAC OS X dependencies installation\nVia MacPorts:\n\nInstall MacPorts\nExecute $ .\/DEPENDENCIES-INSTALLER.sh\n\nOr via HomeBrew:\n\nInstall Homebrew\nExecute $ .\/DEPENDENCIES-INSTALLER.sh\n\nBuilding new modules out of APRIL-ANN repository\nFind the code base necessary for compilation of new modules at\nAPRIL-ANN module example.\nCurrently this option has been tested for Linux systems, despite it can be done\nin MacOS X. So, for Linux systems, you need to install APRIL-ANN using the\nfollowing commands (after you have downloaded or cloned the main repository):\n$ .\/DEPENDENCIES-INSTALLER.sh\n$ make\n$ sudo make install\n\nAfter that, you need to link your software using pkg-config to configure your\ncompiler:\n$ g++ -fPIC -shared -o YOUR_MODULE_NAME.so *.o $(pkg-config --cflags --libs april-ann)\n\nDon't forget to require APRIL-ANN in your C++ code using the following\ninstruction:\nluaL_requiref(L, \"aprilann\", luaopen_aprilann, 1);\nOnce you have done this steps, you can load your module into APRIL-ANN using Lua\ninterpreter:\n$ lua\nLua 5.2.2  Copyright (C) 1994-2013 Lua.org, PUC-Rio\n> your_module = require \"YOUR_MODULE_NAME\"\nAPRIL-ANN v0.4.0  Copyright (C) 2012-2015 DSIC-UPV, CEU-UCH\nCompiled at Sat Jul 18 13:45:52 2015, timestamp 1437219952\nThis program comes with ABSOLUTELY NO WARRANTY; for details see LICENSE.txt.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; see LICENSE.txt for details.\n\nThe next C++ code is an example of file which can be loaded as external module\nfor APRIL-ANN:\n\/\/ includes all APRIL-ANN dependencies and declares luaopen_aprilann header\n#include \"april-ann.h\"\nusing AprilMath::MatrixExt::Initializers::matFill;\nusing AprilUtils::LuaTable;\nusing AprilUtils::SharedPtr;\nusing Basics::MatrixFloat;\n\/\/ exported function example\nint get(lua_State *L) {\n  SharedPtr<MatrixFloat> m = new MatrixFloat(2, 10, 20);\n  matFill(m.get(), 20.0f);\n  \/\/ using LuaTable you can push APRIL-ANN objects in Lua stack (be careful,\n  \/\/ not all objects can be pushed)\n  LuaTable::pushInto(L, m.get());\n  return 1;\n}\n\/\/ declaration of module opening function\nextern \"C\" {\n  int luaopen_example(lua_State *L) {\n    static const luaL_Reg funcs[] = {\n      {\"get\", get},\n      {NULL, NULL}\n    };\n    luaL_requiref(L, \"aprilann\", luaopen_aprilann, 1);\n    lua_pop(L, 1);\n    luaL_newlib(L, funcs);\n    return 1;\n  }\n}\nThe module can be loaded using a Lua 5.2 interpreter (for instance, the one\ndeployed with APRIL-ANN, but not the april-ann executable command), as\nindicated above.\n","356":"Si Ping Pong\nWe hacked our Ping Pong table! Read the blog post.\nTechnology\n\nnode.js\nsocket.io\nReact\ngulp.js\nBrowserify\n\nBuilding\nGulp is used to build the client. From the project root, run npm install and gulp to build.\nThe default Gulp task will build and then start watching.\nSounds\nAudiosprite is used to build the sound sprite.\nYou'll first need to install Audiosprite:\nnpm install -g audiosprite\nbrew install ffmpeg --with-theora --with-libogg --with-libvorbis\n\nThen run gulp sounds from the project root to rebuild the sprite. This will:\n\nFetch audio announcements for all players from Google's unofficial TTS API\nFetch point announcements for scores 0\u201340 from Google's unofficial TTS API\nInclude any .mp3 or .wav files in the ui\/public\/sounds directory\nRebuild the JSON file that contains the audio data required to play the individual sounds\n\ngulp sounds depends on a DB connection in order to get the player list. You may need to specify the environment to use, for example:\nNODE_ENV=development gulp sounds\nRemember to rebuild the frontend after regenerating the sounds in order to include the updated sprite JSON in the Browserify build.\nTodo\n\nGeneral restructuring and refactoring (v1 \u2013 new architecture, tidy events, move game logic client-side)\nRather than mashing up audio, use full clips for each phrase for each player for more natural sounds (this should be automated)\nAdd an easy method for plugging in events from custom hardware (adding players, recording points)\nRemove dependency on global vars (game, for example)\nAim for strict mode compliance\nWhen an RFID tag that does not have an associated player is scanned, the ID should be emailed, posted to HipChat, etc. so that it can be easily added ... or just add an inactive player to the database?\n\nUI\n\nOn first load, the leaderboard does not fade in like the other stats\n\nCSS\n\nAdd Autoprefixer and strip vendor prefixes\nMake variables for commonly used values\n\n","357":"Google Analytics\nUNMAINTAINED: In case it wasn't clear from the lack of activity, I don't have the time to work on this project anymore.\nI'd be happy to transfer ownership to someone else or add someone as a contributor to the project. Please reach out to me\nand let me know!\n\n\nThis project doesn't work and hasn't worked for some time due to google removing the use based configuration.\n\tSee more at https:\/\/github.com\/ncb000gt\/node-googleanalytics\/issues\/36#issuecomment-383822453\n\nPull data from Google Analytics for use in projects.\nThe library maintains tracking of the token so that you don't have to and will push the token around with your requests.\nShould you require a different token, just create a new GA instance. However, this is asynchronous through eventing so if you do want the token you can latch onto the event.\n\nUpdated for NodeJS 0.6.x *\n\nUsage\nWith a user and password:\nvar GA = require('googleanalytics'),\n    util = require('util'),\n    config = {\n        \"user\": \"myusername\",\n        \"password\": \"mypassword\"\n    },\n    ga = new GA.GA(config);\n\nga.login(function(err, token) {\n    var options = {\n        'ids': 'ga:<profileid>',\n        'start-date': '2010-09-01',\n        'end-date': '2010-09-30',\n        'dimensions': 'ga:pagePath',\n        'metrics': 'ga:pageviews',\n        'sort': '-ga:pagePath'\n    };\n\n    ga.get(options, function(err, entries) {\n       util.debug(JSON.stringify(entries));\n    });\n});\n\nIf you have already gotten permission from a user, you can simply use the oAuth access token you have:\nvar GA = require('googleanalytics'),\n    util = require('util'),\n    config = {\n        \"token\": \"XXXXXXXXXXXX\"\n    },\n    ga = new GA.GA(config);\n\nvar options = {\n    'ids': 'ga:<profileid>',\n    'start-date': '2010-09-01',\n    'end-date': '2010-09-30',\n    'dimensions': 'ga:pagePath',\n    'metrics': 'ga:pageviews',\n    'sort': '-ga:pagePath'\n};\n\nga.get(options, function(err, entries) {\n    util.debug(JSON.stringify(entries));\n});\n\nYou can specify the type of token by setting 'tokenType', default is 'Bearer'.\nSee node-gapitoken for easy service account Server to Server authorization flow.\nAPI\n\nlogin([callback]) - The callback is optional. However, if it is given, it is added to the token event.\nget(options, callback)\n\nEvent API\n\ntoken(err, token)\nentries(err, entries)\n\nEntry API\n\nmetrics[]\ndimensions[]\n\nEach array contains objects. These objects contain the following:\n\nname - The name of the metric or dimension requested\nvalue - The value associated. If the value is a Number, it is parsed for you. Otherwise, it will be a string.\n\nContributors\n\nBrian Zeligson - Updates for a more recent version of node. Also makes use of better selectors.\nMike Schierberl\nGal Ben-Haim - Bug fixes for access token flow.\nPatrick Nolan - Bug fixes for parsed_data not containing a rows field.\nRyan Smith - Fixed GH-28.\n\nLicense\nsee license file\n","358":"#Phonegap desktop\nThe aim of this project is to allow developers to use PhoneGap in a desktop browser.\nHave a look at the online demo (Best viewed in Chrome)\nThis is a copy of the phonegap kitchen sink demo using my desktop library.\nAll PhoneGap API calls are simulated and the data returned is determined by json files.\nSee the Wiki for some notes on getting started.\n###Current Status:\nAdded section headings and links to API docs\nAdded Cordova File API methods\nMoved Connection API for v2.2 and up (previous can still be used)\n###Next Steps:\nUpdate features for recent API changes\nLook at using new getUserMedia method for camera capture (latest release of Chrome, Opera)\nLook at emulation (not simulation) of other features\nInvestigate overriding Firefox geolocation\n##License\nPhoneGap Desktop is licensed under Apache v2.\nA copy of the license is included in the project or you can view it at http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n","359":"toe.js\ntoe.js is a tiny library based on jQuery to enable sophisticated gestures on touch devices.\nWhy toe.js\nThe goal of toe.js is\n\nSmooth integration into jQuery's event handling\nFast responding events to give the user a better experience\nExtensible, hooks allow new touch events to be part of toe and use existing functionality\nCustomizable through grunt. The build process allows you to remove not needed gestures\nTiny overhead (1649 bytes gzipped by version 1.0)\n\nAvailable events (v. 2.0)\n\ntap\ntaphold\nswipe (all directions)\ntransformstart, transform, transformend (scale and rotation)\n\nComing soon: fancy drag and drop\nUsage\nUse the default jQuery event binding to bind a toe.js event\n$('div.myElem').on('tap', $.noop);\n\nMost of the events support multiple fingers. So if you want to find out the amount of fingers used by a gesture, just look into the TouchList of the original event.\n$('div.myElem').on('tap', function (event) {\n\tvar original = event.originalEvent,\n\t\ttouches = original.touches.length > 0 ? original.touches : original.changedTouches;\n\t\n\tif (touches.length === 2) {\n\t\t\/\/ do something if the user tapped with two fingers\n\t}\n});\n\nDefault eventing behavior will not be influenced by the default toe.js events. So in case you want to catch to a swipe event in a scrollable direction then you have to stop the the default behavior on your own.\n$('div.myElem').on('touchstart touchmove touchend', function (event) {\n\tevent.preventDefault();\n});\n\n$('div.myElem').on('swipe', function (event) {\n\t\n});\n\nHow to extend toe.js? I'll provide a simple template as soon as possible!\nEvents\ntap, doubletap, taphold\nThe \"tap\" event is somehow similar to a click event with a pointer device so there is nothing more to say about it.\n\"taphold\" is triggered if the user starts touching the target and keeps his finger on it for a certain amount of time (default: 500ms)\nswipe\nThe swipe event can be occur in any direction on the element.\n$('div.myElem').on('swipe', function (event) {\n\tif(event.direction === 'up') { \/\/ or right, down, left\n\t\n\t}\n});\n\ntransformstart, transform, transformend\nThis event is also known as pinch event. It allows the user to use two fingers moving away from or towards each other. The user will be able to signalize a scale or rotation of an object. All three events will deliver the center of the pinch, the rotation and the scale.\n$('div.myElem').on('transform', function (event) {\n\tvar center = event.center, \/\/center.pageX and center.pageY\n\t\tscale = event.scale,\n\t\trotation = event.rotation; \/\/in deegres\n\t\n\t\/\/ do sth\n\t\n});\n\nCustom build\nToe.js is a modular library. In order you do not want to use all events just clone this repo and remove the unwanted gestures under src\/gestures. The grunt script does the rest for you.\nMore information\n(blog entry)[http:\/\/damien.antipa.at\/2013\/03\/24\/toe-js-version-2-was-released\/]\nThanks\nto the developers of all related libraries which inspired me: jQuery mobile, Hammer.JS, jGestures and TouchSwipe.\n","360":"Check out the demo of Bootstrap file input at http:\/\/gregpike.net\/demos\/bootstrap-file-input\/demo.html\n","361":"Hotdot - Realtime webapp using Django + Orbited + Twisted\n\nWhat is Hotdot?\nA very complete example of how to a create a\nrealtime web application using Django + Orbited + Twisted.\nRead more about the motivation behind Hotdot:\nhttp:\/\/clemesha.org\/blog\/2009\/dec\/17\/realtime-web-apps-python-django-orbited-twisted\nCurrently the example is:\nRealtime Voting Both: Collaborative Realtime Voting, Chatting, and Editing Polls.\n\nIs Hotdot a 'Realtime Web Framework'?\nNot currently. Maybe it should turn into one? UPDATE [12\/27\/2009]: I've started\nwork on making Hotdot more \"framework\" like.  That is, I'm making it easier to \"plug in\"\nyour own application logic, etc, by following a small set of conventions.  The work-in-progress\ncode is in this branch: http:\/\/github.com\/clemesha\/hotdot\/tree\/framework\nFor comparison, consider Tornado (http:\/\/www.tornadoweb.org),\nwhich is the 'realtime web framework' used to build FriendFeed.\nTornado is minimal and clean, and does include some very nice\nfeatures beyond just being a non-blocking webserver.\nMy proposition with Hotdot is that the combination of\nDjango, Orbited, and Twisted results in a more\n'full-featured realtime web framework', with a greater\ncommunity and total features, than can be found in Tornado.\n\nHow and Why?\nThe combination of Django + Orbited + Twisted is everything\nyou need to make a 'real-world' realtime web application with Python.\n\nTheir roles:\n\nDjango: Excellent web framework for creating the backbone of a great web application.\nOrbited: Realtime web (Comet) library to build the realtime components with.\nTwisted: Scalable asynchronous network lib, for serving Orbited (and Django too, with WSGI!)\n\n\nOther reasons for Hotdot:\n\nIncorporate core bits into http:\/\/codenode.org (http:\/\/github.com\/codenode\/codenode) to make it realtime.\nMy personal education on this awesome topic.\n\n\n\n\nInstall\n\nRecommended: Use a virtualenv and install with pip, to get them type:\neasy_install virtualenv pip\n\n\nCreate a fresh virtualenv:\nvirtualenv --no-site-packages hotdot_env\n\n\nInstall dependencies into your virtualenv:\n#You must have Django 1.0+ and Twisted 9.0+\n\npip -E hotdot_env install -U django orbited twisted simplejson\n\n\nMove into your virtualenv and activate it:\n$ cd hotdot_env\n$ source bin\/activate\n\n\nGet a copy of Hotdot from here:\ncurl http:\/\/github.com\/clemesha\/hotdot\/tarball\/master\n\n#Or clone a copy:\n\n$ git clone git:\/\/github.com\/clemesha\/hotdot.git\n\n\n\n\nUsage\n\nIn the directory hotdot\/djangoweb, type:\ndjango-admin.py syncdb --pythonpath='.' --settings='settings'\n\n\nIn the toplevel directory hotdot, type:\ntwistd -ny server.py\n\n\n\n\nNow open browser to http:\/\/localhost:8000\/\nAlso see settings in server.py and djangoweb\/settings.py\nTo change host interface, see server.py->INTERFACE and djangoweb\/settings.py->INTERFACE.\n\n\nTests\n\nIn the directory hotdot\/djangoweb, type:\ndjango-admin.py test --pythonpath='.' --settings='settings'\n\n\n\n\nDetails of how Hotdot works\n(WORK IN PROGRESS)\n\nOrbited as a Twisted Service\nDjango running from twisted.web.wsgi\nAuthentication using Twisted Cred+Django models\nFiltering + modification + logging of in-transit Orbited messages\nSTOMP as the default, example protocol.\n\n\nWhy the name Hotdot?\n'Hot' as in the latest goodness.\n'Dot' as in _D_jango + _O_rbited + _T_wisted.\n\nLicense, Questions, Contact\nHotdot is licensed under the BSD.\nPlease fork a copy here!: http:\/\/github.com\/clemesha\/hotdot\nContact: Alex Clemesha <alex@clemesha.org> | http:\/\/twitter.com\/clemesha\n","362":"--> Demos, Examples, Playground, Docu\n--> d3-react-squared-c3-loader\n--> New live example\n--> New blog post, based on live example\nNotes\n--> v0.6.0 and later require d3 v4!\n--> v0.3.0 and later require React 0.14!\nc3\nDocumentation is still missing, sorry!\nv0.3.6 and newer\nStarting in 0.3.6, c3 charts are loaded using d3-react-squared-c3-loader\nv0.2.7 through v0.3.5:\nPlease note that this is still 'beta'. So far, there is no docu on the docu page.\n--> please check out the c3example.js in the source (.\/examples).\nd3-react-squared\n\nFeedback, ideas, PRs, etc. very welcome!\nWhy yet another d3-react component?\nThere are already some great solutions out there, combining React and D3, e.g.:\nA gist with some links here\nMost of these articles\/code aims to combine\/add d3 into the lifecycle methods to\ngenerate charts that way. Have a look at them, great ideas there.\nSee docu page for some details about my approach. I don't want to bore you with details here -\njust contact us (contacts on docu page). I am very happy to discuss ideas\/concepts!\nSome keywords:\n\nUse D3 charts 'directly', maybe very limited adjustments needed (just think examples!)\nProvide viewboxes etc. to get responsive graphs\nMake chart modular (a.k.a. reusable)\nProvide a clean API to create and update charts (from ANY component!).\nParametrize charts\nBe lightweight\nProvide a way to share events between charts (and using a wrapper: any component!)\nProvide access to a charts library (we currently offer c3js, as of v0.2.7)\nProvide a limited set of examples in this repo and make it easy to the users to add their own custom charts\n\nWe believe that especially the last bullet is helpful to teams separate concerns and have maintainable solutions.\nWhy? The chart generating code is in its own module and the interaction designer doesn't really have to care about React (maybe he should, but that's another story...).\nDetails?\nSee also here\n(click on DR2 in top right navigation!)\nDocumentation\n--> See here\n(click on DR2 in top right navigation!)\nThe documentation is still somewhat basic. Definitely check out the examples in the repo!\nBut hey, writing docu is sooooo time consuming...\nStand-alone example\nThis repo now includes a stand-alone example. Simply:\nnpm install\n\nand then\nnpm run dev\n\nand it should be running on localhost:8080.\nRequirements\nAs far as I know, you shouldn't need anything fancy.\nWe run it in a babel\/webpack\/react setup, plain vanilla, so to speak (plenty of setup guides out there),\nand it works.\nAlso: we have bootstrap, no other css\/sass\/... (actually: we love react-bootstrap)\n(Note: you could, if you wanted, to use SASS to style your graphs, must require the files where and when needed; you know how.).\nThanks\nHuge thanks to all the people involved in providing awesome tools such as:\n\nReactJS\nD3\nwebpack\nBabelJS\nReflux (no longer using it, thanks anyway!)\nredux (replaces Reflux)\nc3.js\n\nand many others...\nSome screenshots\nNew Docu-Page\n\nPlayground (--> See here) to learn about parameters:\n\n","363":"Terminus\nTerminus is an experimental\nCapybara driver for real browsers. It\nlets you control your application in any browser on any device (including\nPhantomJS), without needing browser plugins. This\nallows several types of testing to be automated:\n\nCross-browser testing\nHeadless testing\nMulti-browser interaction e.g. messaging apps\nTesting on remote machines, phones, iPads etc\n\nSince it is experimental, this project is sporadically maintained. Usage is\nentirely at your own risk.\nInstallation\n$ gem install terminus\n\nRunning the example\nInstall the dependencies and boot the Terminus server, then open\nhttp:\/\/localhost:70004\/ in your browser.\n$ bundle install\n$ bundle exec bin\/terminus\n\nWith your browser open, start an IRB session and begin controlling the app:\n$ irb -r .\/example\/app\n>> extend Capybara::DSL\n>> visit '\/'\n>> click_link 'Sign up!'\n>> fill_in 'Username', :with => 'jcoglan'\n>> fill_in 'Password', :with => 'hello'\n>> choose 'Web scale'\n>> click_button 'Go!'\n\nLicense\n(The MIT License)\nCopyright (c) 2010-2013 James Coglan\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the 'Software'), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n","364":"Really Simple Color Picker\nThis is a very minimal, yet robust Color Picker based on jQuery.\nFor more details check the introductory blog post - http:\/\/laktek.com\/2008\/10\/27\/really-simple-color-picker-in-jquery\/\nUsage\nYou can either clone this repo or download the latest build as a zip from here - http:\/\/github.com\/laktek\/really-simple-color-picker\/zipball\/master\nColor Picker requires jQuery 1.2.6 or higher. Make sure to load it before Color Picker (there's no other dependencies!).\nFor default styles of the color picker load the CSS file that comes with the plugin.\n  <script language=\"javascript\" type=\"text\/javascript\" src=jquery.min.js\"><\/script>\n  <script language=\"javascript\" type=\"text\/javascript\" src=\"jquery.colorPicker.min.js\"\/><\/script>\n\n  <link rel=\"stylesheet\" href=\"colorPicker.css\" type=\"text\/css\" \/>\nAdd a text field to take the color input.\n  <div><label for=\"color1\">Color 1<\/label> <input id=\"color1\" type=\"text\" name=\"color1\" value=\"#333399\" \/><\/div>\nThen call 'colorPicker' method on the text field when document loads.\n  <script language=\"javascript\">\n    jQuery(document).ready(function($) {\n      $('#color1').colorPicker();\n    }\n  <\/script>\nOptions\nThere are several options you can set at the time of binding.\nSelected color\nColor Picker will use the value of the input field, which the picker is attached to as the selected color. If not, it will use the color passed with pickerDefault property.\n  $('#color1').colorPicker({pickerDefault: \"ffffff\"});\nColor Palette\nOverrides the default color palette by passing an array of color values.\n  $('#color1').colorPicker({colors: [\"333333\", \"111111\"]});\nTransparency\nEnable transparency value as an option.\n  $('#color1').colorPicker({transparency: true});\nColor Change Callback\nRegisters a callback that can be used to notify the calling code of a color change.\n  $('#color1').colorPicker( { onColorChange : function(id, newValue) { console.log(\"ID: \" + id + \" has been changed to \" + newValue); } } );\nIf you want to set an option gloablly (to apply for all color pickers), use:\n  $.fn.colorPicker.defaults.colors = ['151337', '111111']\nDefault text on picker field\nYou can set some text to show on the picker field. For example, you could show a user's initials.\n  <input id=\"color4\" type=\"text\" name=\"color4\" value=\"#FF0000\" data-text=\"AG\" \/>\n  $('#color4').colorPicker();\nDemo\nDemo can be found at http:\/\/laktek.github.com\/really-simple-color-picker\/demo.html\nReal-world Examples\n\nCurdBee\nReadability\n\nLet us know how you are using Really Simple Color Picker...\nContributors\n\nLakshan Perera - http:\/\/laktek.com\nDaniel Lacy  - http:\/\/daniellacy.com\n\nIssues & Suggestions\nPlease report any bugs or feature requests here:\nhttps:\/\/github.com\/laktek\/really-simple-color-picker\/issues\n","365":"Installation and usage\nVia bower:\nbower install jquery.serialScroll\nVia npm:\nnpm install jquery.serialscroll\nUsing a public CDN\nCDN provided by jsdelivr\n<script src=\"\/\/cdn.jsdelivr.net\/npm\/jquery.serialscroll@1.3.0\/jquery.serialScroll.min.js\"><\/script>\nDownloading Manually\nIf you want the latest stable version, get the latest release from the releases page.\njQuery.scrollTo\nThis plugin requires jQuery.scrollTo.\nIn order to use jQuery.scrollTo 2.0 you need to update jQuery.localScroll to 1.3.0 and above.\nNotes\n\n\nThe hash of settings is passed in to jQuery.scrollTo, so, in addition to jQuery.localScroll's settings, you can use any of jQuery.scrollTo's. Check that plugin's documentation for further information.\n\n\nMost of this plugin's defaults, belong to jQuery.scrollTo, check it's demo for an example of each option.\n\n\n","366":"\nDjango API Playground\nA django app that creates api explorer for RESTful APIs.\nWorks with any RESTful API. For example, you can create api explorer for your tastypie based API with this app.\nDemo: http:\/\/api-playground-demo.hipo.biz\n\n\nInstructions\nTo get this application up and running, please follow the steps below:\nInstall from pip:\npip install django-api-playground\n\nOr from source:\npip install git+git:\/\/github.com\/Hipo\/Django-API-Playground.git\n\nAdd to installed apps:\nINSTALLED_APPS =(\n    # ...\n\n    'apiplayground',\n)\n\nCreate database tables:\n.\/manage.py syncdb\n\nInstallation is completed. You can define the API schema now.\nFirst step, Create an url:\n# urls.py\n\nfrom api.playgrounds import ExampleAPIPlayground\n\nurlpatterns = patterns('',\n    (r'api-explorer\/', include(ExampleAPIPlayground().urls)),\n)\n\nSecond step, Define a subclass for your API:\n# api\/playgrounds.py\n\nfrom apiplayground import APIPlayground\n\nclass ExampleAPIPlayground(APIPlayground):\n\n    schema = {\n        \"title\": \"API Playground\",\n        \"base_url\": \"http:\/\/localhost\/api\/\",\n        \"resources\": [\n            {\n                \"name\": \"\/feedbacks\",\n                \"description\": \"This resource allows you to manage feedbacks.\",\n                \"endpoints\": [\n                    {\n                        \"method\": \"GET\",\n                        \"url\": \"\/api\/feedbacks\/{feedback-id}\",\n                        \"description\": \"Returns a specific feedback item\",\n                        \"parameters\": [{\n                            \"name\": \"order_by\",\n                            \"type\": \"select\",\n                            \"choices\": [[\"\", \"None\"], [\"id\", \"id\"], [\"-id\", \"-id\"]],\n                            \"default\": \"id\"\n                        }]\n                    },\n                    {\n                        \"method\": \"POST\",\n                        \"url\": \"\/api\/feedbacks\/\",\n                        \"description\": \"Creates new feedback item\",\n                        \"parameters\": [{\n                            \"name\": \"title\",\n                            \"type\": \"string\"\n                        },\n                        {\n                            \"name\": \"resource\",\n                            \"type\": \"string\"\n                        },\n                        {\n                           \"name\": \"description\",\n                           \"type\": \"string\"\n                        }]\n                    }\n                ]\n            },\n        ]\n    }\n\nThat's all. More detailed documentation will be coming soon.\n\nSpecial Thanks\n\n\nBerker Peksag (for such a beautiful project name suggestion)\n\n\n","367":"face-search\nFace search engine\nDownload demo dataset (Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\nLabeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.\nUniversity of Massachusetts, Amherst, Technical Report 07-49, October, 2007.) or\/and add your own images\n.\/download_images.sh\n\nGenerate db:\ncd code\n.\/create_db.sh\n\nSearch by face:\n# csv output\n.\/run.sh URL 0\n# image output\n.\/run.sh URL 1\n\nQuery:\n\nResults:\n\nrequirements:\nsudo apt-get install imagemagick\nsudo pip install awscli\nsudo pip install click\n\n","368":"face-search\nFace search engine\nDownload demo dataset (Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\nLabeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.\nUniversity of Massachusetts, Amherst, Technical Report 07-49, October, 2007.) or\/and add your own images\n.\/download_images.sh\n\nGenerate db:\ncd code\n.\/create_db.sh\n\nSearch by face:\n# csv output\n.\/run.sh URL 0\n# image output\n.\/run.sh URL 1\n\nQuery:\n\nResults:\n\nrequirements:\nsudo apt-get install imagemagick\nsudo pip install awscli\nsudo pip install click\n\n","369":"Overmind\nThis project aims to provider a complete server provisioning and configuration management application.\nThe first version is a unified front-end to public and private clouds, custom server providers and dedicated hardware.\nFeatures\n\nEC2 and Rackspace server provisioning. All clouds supported by libcloud will be supported given enough testing\nProvider Plugins: Any provider can be integrated by writing either a libcloud driver or an Overmind provisioning plugin\nImport any server into Overmind witht the \"Dedicated Hardware\" plugin\nComplete REST API for provider and nodes\nAuthentication with three user roles\n\nSee the wiki for architectural info.\nInstallation\nRequirements\n\nPython 2.6+\nDjango 1.3\napache-libcloud\ndjango-celery\nRabbitMQ (or alternative message queue supported by Celery)\n\nAll python dependencies can be installed using the requirements file:\n$ pip install -r requirements.txt\n\nInstall Overmind\n\n\nDownload the last stable release from\nhttp:\/\/github.com\/tobami\/overmind\/downloads\nand unpack it\n\n\nCreate the DB by changing to the overmind\/ directory and running:\n  python manage.py syncdb\n\n\n\nFor testing purposes start the celery server on a console\n  python manage.py celeryd -l info\n\nand the django development server\n  python manage.py runserver\n\n\n\nNow you can visit the Overmind overview page on localhost:8000\/overview\n","370":"CNN-RelationExtraction\nConvolution neural network for relation classification between two given entities\nThe CNN architecture implemented is inspired be Nguyen et al. 2015 in which for each:\n\nReferences:\n\n\nNguyen, Thien Huu, and Ralph Grishman. \"Relation Extraction: Perspective from Convolutional Neural Networks.\"\n\n\nKim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\n\n\nZeng, D., Liu, K., Lai, S., Zhou, G. and Zhao, J., 2014, August. Relation classification via convolutional deep neural network. In Proceedings of COLING (pp. 2335-2344).\nVancouver\n\n\n","371":"CNN-RelationExtraction\nConvolution neural network for relation classification between two given entities\nThe CNN architecture implemented is inspired be Nguyen et al. 2015 in which for each:\n\nReferences:\n\n\nNguyen, Thien Huu, and Ralph Grishman. \"Relation Extraction: Perspective from Convolutional Neural Networks.\"\n\n\nKim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\n\n\nZeng, D., Liu, K., Lai, S., Zhou, G. and Zhao, J., 2014, August. Relation classification via convolutional deep neural network. In Proceedings of COLING (pp. 2335-2344).\nVancouver\n\n\n","372":"Annotator Store\nThis is a backend store for Annotator.\nThe functionality can roughly be separated in two parts:\n\nAn abstraction layer wrapping Elasticsearch, to easily manage annotation\nstorage. It features authorization to filter search results according to\ntheir permission settings.\nA Flask blueprint for a web server that exposes an HTTP API to the annotation\nstorage. To use this functionality, build this package with the [flask]\noption.\n\n\nGetting going\nYou'll need a recent version of Python (Python 2 >=2.6\nor Python 3 >=3.3) and ElasticSearch (>=1.0.0)\ninstalled.\nThe quickest way to get going requires the pip and virtualenv\ntools (easy_install virtualenv will get them both). Run the\nfollowing in the repository root:\nvirtualenv pyenv\nsource pyenv\/bin\/activate\npip install -e .[flask]\ncp annotator.cfg.example annotator.cfg\npython run.py\n\nYou should see something like:\n* Running on http:\/\/127.0.0.1:5000\/\n* Restarting with reloader...\n\nIf you wish to customize the configuration of the Annotator Store, make\nyour changes to annotator.cfg or dive into run.py.\nAdditionally, the HOST and PORT environment variables override\nthe default socket binding of address 127.0.0.1 and port 5000.\n\nStore API\nThe Store API is designed to be compatible with the\nAnnotator. The annotation store, a\nJSON-speaking REST API, will be mounted at \/api by default. See the\nAnnotator\ndocumentation for\ndetails.\n\nRunning tests\nWe use nosetests to run tests. You can just\npip install -e .[testing], ensure ElasticSearch is running, and\nthen:\n$ nosetests\n......................................................................................\n----------------------------------------------------------------------\nRan 86 tests in 19.171s\n\nOK\n\nAlternatively (and preferably), you should install\nTox, and then run tox. This will run\nthe tests against multiple versions of Python (if you have them\ninstalled).\nPlease open an issue\nif you find that the tests don't all pass on your machine, making sure to include\nthe output of pip freeze.\n","373":"chainer-char-rnn\nkarpathy's char-rnn implementation by Chainer\nRequirement\n\nChainer\n\n$ pip install chainer\n\nTrain\nStart training the model using train.py, for example\n$ python train.py\n\nThe --data_dir flag specifies the dataset to use. By default it is set to data\/tinyshakespeare which consists of a subset of works of Shakespeare.\nYour own data: If you'd like to use your own data create a single file input.txt and place it into a folder in data\/. For example, data\/some_folder\/input.txt.\nSampling\nGiven a checkpoint file (such as those written to cv) we can generate new text. For example:\n$ python sample.py \\\n--vocabulary data\/tinyshakespeare\/vocab.bin \\\n--model cv\/some_checkpoint.chainermodel \\\n--primetext some_text --gpu -1\n\nReferences\n\nOriginal implementation: https:\/\/github.com\/karpathy\/char-rnn\nBlog post: http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/\n\n","374":"django-websocket (ABANDONED, do not use)\nTHIS PROJECT IS ABANDONED! Please use django-channels to implement\nwebsockets with Django on the server.\nFor legacy reasons, you can access the old documentation in the README of\nversion 0.3.0.\n","375":" PixlUI\nProvide few methods for visual elements.\n\n\n\n\n\n\n\nCheckBox:\n\nCustom font\nText All caps (works on all API version)\n\nButton:\n\nCustom font\nText All caps (works on all API version)\n\nEditText:\n\nCustom font\nCopy\/Cut\/Paste (enable\/disable) (works on all API version)\nCancel clipboard content (works on all API version)\nText All caps (works on all API version) - (in progress)\nFocus listener\nBatch listener (replace TextWatcher, wich that you can intercept DEL touch on all API)\n\nAutoCompleteEditText:\n\nCustom font\nCopy\/Cut\/Paste (enable\/disable) (works on all API version)\nCancel clipboard content (works on all API version)\nText All caps (works on all API version) - (in progress)\nFocus listener\nBatch listener (replace TextWatcher, wich that you can intercept DEL touch on all API)\n\nImage View:\n\nAlpha  (works on all API version)\n\nRelativeLayout:\n\nAlpha  (works on all API version)\n\nTextView:\n\nContains a fix to do proper ellipsizing\nCustom font\nText All caps (works on all API version)\n\nScreenshot\n\nGradle Setup\nCompile with one line easy code!\nrepositories {\n    maven {\n        url \"https:\/\/jitpack.io\"\n    }\n}\n\nCompile in the build.gradle file. for X.X.X please refer to the change log.\ndependencies{\n  compile 'com.github.neopixl:PixlUI:vX.X.X.'\n}\nMaven Setup\nAdd Repository\n<repository>\n\t    <id>jitpack.io<\/id>\n\t    <url>https:\/\/jitpack.io<\/url>\n\t<\/repository>\n\nAdd Dependency:\n\t<dependency>\n\t    <groupId>com.github.neopixl<\/groupId>\n\t    <artifactId>PixlUI<\/artifactId>\n\t    <version>v1.0.5<\/version>\n\t<\/dependency>\nHow use it ?\n\n\nAdd your custom fonts in \/assets\/fonts\/\n\n\nDefine your fonts in styles.xml\n\n\n    <style name=\"AppTheme.TextGearedSlab\">\n        <item name=\"typeface\">GearedSlab.ttf<\/item>\n    <\/style>\n\n\n    <style name=\"AppTheme.TextGearedSlab.t1\">\n        <item name=\"android:textSize\">12sp<\/item>\n    <\/style>\n\n\n    <style name=\"AppTheme.TextGearedSlab.t2\">\n        <item name=\"android:textSize\">14sp<\/item>\n    <\/style>\n\nUse it in XML:\n\n<RelativeLayout xmlns:android=\"http:\/\/schemas.android.com\/apk\/res\/android\"\n    xmlns:pixlui=\"http:\/\/schemas.android.com\/apk\/com.neopixl.pixlui\"\n    xmlns:tools=\"http:\/\/schemas.android.com\/tools\" >\n\n    <np.TextView\n        style=\"@style\/AppTheme.TextGearedSlab.t1\"\n        android:id=\"@+id\/textView1\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"@string\/hello_world\"\n        pixlui:copyandpaste=\"false\"\n        pixlui:clearclipboardcontent=\"true\"\/>\n<\/RelativeLayout>\n ChangeLog\n1.1.2\n\ncloses #31 (Memory Leak: Activity Leak on PixlUIfaceManager.INSTANCE)\nUpdate buildTools (25.0.3) and compileSdkVersion \/ targetSdkVersion = 25.\n\n1.1\n\nAndroid studio 1.5.1 Support\nAndroid Layout Preview support (on API >=23 preview)\nCheckedTextView added\nChronometer added\nExtractEditText added\nSwitch added\nRefactor\nFixed some bugs\nFind easy accessor to our components:\n- Before: <com.neopixl.pixlui.components.textview.TextView\n- Now: <np.TextView\n\n1.0.6\n\nUpgraded to last build tools\nEllipsizingTextView is now removed from com.neopixl.pixlui.components.textview\n\n1.0.5a (Prerelease)\n\nTemp release to fix jitpack.io build\ninclude a gradle'ized versin of PixlUI (now a library project)\n\n1.0.5\n\nAdded custom RadioButton (Custom font, Text all caps)\n\n1.0.4\n\nAdded RelativeLayoutAnimator (VISIBLE\/GONE\/INVISIBLE transition animation)\nAdded LinearLayoutAnimator (VISIBLE\/GONE\/INVISIBLE transition animation)\nAdded custom AutoCompleteEditText\nAdded custom AutoResizeTextView (in progress)\n\n1.0.3\n\nAdded custom CheckBox (Custom font, Text all caps)\n\n1.0.2\n\nAdded method in custom EditText (Autofocus Listener, Hide\/Show Keyboard)\n\n1.0.1\n\nAdded method in custom TextView (Text all caps) - for old api version\nAdded method in custom EditText (Text all caps) - for old api version\nAdded method in custom Button (Text all caps) - for old api version\nFix NPE in Batch Listener\nAdded custom RelativeLayout (Alpha) - for old api version\n\n1.0.0\n\nAdded custom TextView (Custom font)\nAdded custom EditText (Custom font, Focus Listener, Batch Listener)\nAdded custom Button (Custom font)\nFix many crash\n\n Application using PixlUI\n\n \n\n\n\n\n\n[Flow] (https:\/\/play.google.com\/store\/apps\/details?id=com.metalab.flow)\t\t-\nW-Zup  \t-       FLASHiZ      -       MeeTincS      -       Wort.lu     -\nHypebeast     -       iBeezi-       RootCoinExplorer\nDonation\n\nCopyright\nCopyright 2014-2016 Neopixl - Olivier Demolliens\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this\n\nfile except in compliance with the License. You may obtain a copy of the License at\n\nhttp:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under\n\nthe License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF \n\nANY KIND, either express or implied. See the License for the specific language governing\n\npermissions and limitations under the License.\n\n","376":"FlickableView\n\nFlickable ImageView for Android. It's like a view of twitter's detail image.\nIt's possible that other views animate with FlickableView.\nFeature\n\nMove (Up, Down)\nZoom\nFlick (Up, Down)\n\nHow to use\n final FlickableImageView flickableImageView = (FlickableImageView) findViewById(R.id.fiv);\n \/\/ Resource\n flickableImageView.setImageResource(R.drawable.travel);\n\n \/\/ Http Request\n \/\/ String url = \"...\";\n \/\/ Picasso.with(context).load(url).into(flickableImageView);\n \n \n \/\/ Listeners\n \n \/\/ Flick Listener\n flickableImageView.setOnFlickListener(new FlickableImageView.OnFlickableImageViewFlickListener() {\n     @Override\n     public void onStartFlick() {\n     }\n     \n     @Override\n     public void onFinishFlick() {\n     }\n });\n \n \/\/ Drag Listener\n flickableImageView.setOnDraggingListener(new FlickableImageView.OnFlickableImageViewDraggingListener() {\n     @Override\n     public void onStartDrag() {\n     }\n     \n     @Override\n     public void onCancelDrag() {\n     }\n });\n \n \/\/ SingleTap Listener\n flickableImageView.setOnSingleTapListener(new FlickableImageView.OnFlickableImageViewSingleTapListener() {\n     @Override\n     public void onSingleTapConfirmed() {\n     }\n });\n \n \/\/ DoubleTap Listener\n flickableImageView.setOnDoubleTapListener(new FlickableImageView.OnFlickableImageViewDoubleTapListener() {\n     @Override\n     public void onDoubleTap() {\n     }\n });\n \n \/\/ Zoom Listener\n flickableImageView.setOnZoomListener(new FlickableImageView.OnFlickableImageViewZoomListener() {\n     @Override\n     public void onStartZoom() {\n     }\n     \n     @Override\n     public void onBackFromMinScale() {\n     }\n });\n \nCheck this sample code.\nGradle\nrepositories {\n    jcenter()\n}\n\ndependencies {\n    compile 'com.github.goka.flickableview:flickableview:1.0.0'\n}\nRelease\n1.0.0\n\u3000First release.\nReference\nImageViewZoom\n","377":"Android Lifecycle [Deprecated]\n\nA 'compatibility' version of the ActivityLifecycleCallbacks APIs (http:\/\/developer.android.com\/reference\/android\/app\/Application.ActivityLifecycleCallbacks.html)\nthat were introduced in Android 4 (API Level 14) and adding similar mechanism for Fragment.\nWhy & When you need this\nThe reason google introduced ActivityLifecycleCallbacks APIs in Android 4 is try to simplify and modularize the code which need to \"inject\" to Activity's lifecycle.\nFor instance the Google Analytics service requires call a specific method in onStart and onStop of all activities, another good example is ViewServer enable developer inspect UI hierarchy in an un-rooted device.\nFurther more, you might expect those code could be plug-in or out in your building, and this library can make it easier for you.\nMore info about ActivityLifecycleCallbacks APIs, please check android documents.\nSince fragment play more and more important role in android UI development(One Activity + fragments), you would need similar tools to simplify your fragment development.\nHow to use\n\nYou can grab the jar from Maven Central Repository and put it to your libs dictionary\nMaven\n\n<dependency>\n  <groupId>com.cocosw<\/groupId>\n  <artifactId>lifecycle<\/artifactId>\n  <version>0.1<\/version>\n<\/dependency>\n\nGradle\n\ncompile 'com.cocosw:lifecycle:0.1'\nAPI\n\n\nHave all your activities extend one of the base activities in the com.cocosw.lifecycle.app package.\n\n\nCreate your activity\/fragment lifecycle callbacks class extend from ActivityLifecycleCallbacksCompat and FragmentLifecycleCallbacks\n\n\nCall LifecycleDispatcher.registerActivityLifecycleCallbacks(this, callback) and\/or LifecycleDispatcher.registerFragmentLifecycleCallbacks(this, callback).\n\n\nFor Android 4.0\n\nThis library will use build-in activity lifecycle mechanism for API >14 target platform.\nIf you decide to drop 2.x support and move to android activity lifecycle API, you could done that by modify few lines, because the API is basically the same as the official one.\n\nActionBarSherlock or AppCompact\nIf you use a library that already requires your activities to extend a base class, you can simply create your own base activity.\nLicence\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\nhttp:\/\/www.apache.org\/licenses\/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n","378":"Android Design Support Sample\nA sample that use Android Design Support Library.\nNew Widgets\n\nNavigationView\nTabLayout\nCoordinatorLayout\nAppBarLayout\nCollapsingToolbarLayout\nFloatingActionButton\nSnackbar\nNestedScrollView\nTextInputLayout\n\nRequired Dependencies\n\nappcompat-v7:22.2.0\nrecyclerview-v7:22.2.0\n\nSample Download\n\nScreenshots\n\n\n\n\n\nLicense\nCopyright 2015 Eric Liu\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","379":"##About\nA classic password visualization concept, ported to Android\n\n\nChroma-Hash is a concept for visualizing secure text input using ambient color bars\nPassword entry can be frustrating, especially with long or difficult passwords. Secure fields obscure your input with \u2022's, so others can't read it. Unfortunately, neither can you\u2014you can't tell if you got your password right until you tap \"Log In\".\nChroma-Hash displays a series of colored bars at the end of field inputs so you can instantly see if your password is right. Chroma-Hash takes an MD5 hash of your input and uses that to compute the colors in the visualization. The resulting color pattern is non-reversible, so no one could know what your password just from the colors.\nSee the original web version for a live demonstration, and a bit more explanation.\n\n##Usage\nChromaHashView is a drop in replacement for an EditText for password input.\nRight now you need to import the library to your project, but it will be uploaded to Maven Central shortly.\n##License\nCopyright 2014 Michael Evans\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","380":"\u26a0\ufe0f\nTHIS REPO IS DEPRECATED SINCE THE DRAWER LAYOUT WAS INTEGRATED INTO THE TITANIUM CORE. CONSIDER USING: https:\/\/docs.appcelerator.com\/platform\/latest\/#!\/api\/Titanium.UI.Android.DrawerLayout\n\nDEPRECATED (Ti.DrawerLayout)  \nNative Android Navigation Drawer for Titanium\n\nOverview\nInstallation\nUsage\nDocs\nDemo\nLicense\n\nThis is a fork of Tripvi\/Ti.DrawerLayout\nOverview\n\nThis module adds support for using the DrawerLayout in Titanium Apps.\nThe Drawer Layout is a view that can be pulled from the edge of a window. This can answer various purposes. The most common use case is the Navigation Drawer as seen in the above screenshot. The Navigation Drawer displays navigation options in a drawer which slides in from the left edge.\nTo expand the drawer the user can either touch the app icon or swipe from the left edge. The navigation drawer overlays the content but not the action bar.\nInstallation\n\nGrab the latest package from the dist folder\nInstall it following this guide\nwith gittio: $ gittio install com.tripvi.drawerlayout\n\nUsage\nHere's an example of how to use the module.\n\nPlease note: This module requires a Theme without ActionBar such as Theme.AppCompat.Light.NoActionBar since it adds a Toolbar to its own layout. If you do not want the Toolbar, just pass the hideToolbar property at creation-time.\n\n\/\/ Load module\nvar TiDrawerLayout = require('com.tripvi.drawerlayout');\n\n\/\/ define left and center view\nvar leftView = Ti.UI.createView({backgroundColor:'gray'});\nvar centerView = Ti.UI.createView({backgroundColor:'white'});\n\n\/\/ create the Drawer\nvar drawer = TiDrawerLayout.createDrawer({\n    leftView: leftView,\n    centerView: centerView\n});\n\n\/\/ create a window\nvar win = Ti.UI.createWindow();\n\n\/\/ add the drawer to the window\nwin.add(drawer);\n\n\/\/ listen for the open event...\nwin.addEventListener('open', function(){\n    \n    \/\/ ...to access activity and action bar\n    var activity = win.getActivity();\n    var actionbar = activity.getActionBar();\n    \n    if (actionbar){\n    \n        \/\/ this makes the drawer indicator visible in the action bar\n        actionbar.displayHomeAsUp = true;\n        \n        \/\/ open and close with the app icon\n        actionbar.onHomeIconItemSelected = function() {\n            drawer.toggleLeftWindow();\n        };\n    }\n});\n\n\/\/ open the window\nwin.open();\nAPI Documentation\nDemo App\nDevelopment\nContributions are very welcome. If you are able to fix bugs or want to add features to the module, please refer to the official module development guide. You can build the module with appc as described there. You can also use grunt build here. For distribution a new version, please update the version number in package.json and run grunt. It will update the version number in manifest and readme. Then it builds the new version and installs it globally with gittio.\nLicense\nMIT license, see LICENSE\nCopyright (c) 2013 - 2014 by Tripvi Inc., 2015 - 2016 by Manuel Lehner\n","381":"\nIOTA C++ Library\n\n\n\n\n\n\n\n\nThis is not (yet) an official C++ client library for the IOTA Reference Implementation (IRI).\nIt implements both the official API, as well as newly proposed features.\nTable of Contents\n\nDisclaimer\nDocumentation\n\nInstallation\nGetting Started\nDoxygen\nWiki\nExamples\n\n\nTechnologies, tools & dependencies\n\nTechnologies\nDependencies\n\n\nCommunity\n\nResources\nContributing\nDonating\n\n\n\nDisclaimer\n\nThis is an early beta release, expect unexpected results.\nThere may be performance and stability issues.\nThis library is still in early development and there may be breaking changes in the near future.\nYou may lose all your money.\n\nDocumentation\nInstallation\ngit clone --recursive git@github.com:thibault-martinez\/iota.lib.cpp.git\ncd iota.lib.cpp\nmkdir build\ncd build\ncmake ..\nmake\n\nGetting Started\nIOTA::API::Core api(\"node.iotawallet.info\", 14265);\nauto            res = api.getTransactionsToApprove(27);\n\nstd::cout << res.getTrunkTransaction() << std::endl;\nstd::cout << res.getBranchTransaction() << std::endl;\nDoxygen\nA Doxygen documentation is available and provides full API documentation for the library.\nWiki\nA Wiki is available and provides full documentation for the library as well as installation explanations.\nExamples\nThere's an extensive list of test cases on the test folder that can be used as reference when developing apps with IOTA.\nTechnologies, tools & dependencies\nTechnologies\nThis library currently uses C++11.\nTools\n\nxsltproc (needed in Keccak toolchain)\n\nDependencies\n\nC++ Requests\nJSON For Modern C++\nKeccak Code Package\n\nCommunity\nResources\nIf you want to get involved in the community, need help with getting setup, have any issues related with the library or just want to discuss Blockchain, Distributed Ledgers and IoT with other people, feel free to visit one of our resources :\n\nWebsite\nDiscord\nForum\nStack Exchange\n\nContributing\nPlease report any issues using the Issue Tracker.\nDonating\nOZCTDHTFCB9PTAZWGYCGOA9XKDKPSHWVSZDJKZCOINNQTPNNEANGPBBDLSNGKDGCAAKBDVBOVCPTRLHTANMIRGFFGD\n","382":"ICU Character Set Detection for Node.js\n\nCharacter set detection is the process of determining the character set, or encoding, of character data in an unknown format.\n\nA simple binding of ICU character set detection (http:\/\/userguide.icu-project.org\/conversion\/detection) for Node.js.\nInstallation\nAt first, install libicu into your system (See this instruction for details).\nAfter that, install node-icu-charset-detector from npm.\nnpm install node-icu-charset-detector\n\nInstalling ICU\nLinux\n\n\nDebian (Ubuntu)\napt-get install libicu-dev\n\n\nGentoo\nemerge icu\n\n\nFedora\/CentOS\nyum install libicu-devel\n\n\nOSX\n\n\nMacPorts\nport install icu +devel\n\n\nHomebrew\n\n\nbrew install icu4c\nbrew link icu4c --force\nIf experiencing issues with 'homebrew' installing version 50.1 of icu4c, try the following:\nbrew search icu4c\nbrew tap homebrew\/versions\nbrew versions icu4c\ncd $(brew --prefix) && git pull --rebase\ngit checkout c25fd2f $(brew --prefix)\/Library\/Formula\/icu4c.rb\nbrew install icu4c\n\nFrom source\n\ncurl -O http:\/\/download.icu-project.org\/files\/icu4c\/52.1\/icu4c-52_1-src.tgz\ntar xzvf icu4c-4_4_2-src.tgz\ncd icu\/source\nchmod +x runConfigureICU configure install-sh\n.\/runConfigureICU MacOSX\nmake\nsudo make install\nxcode-select --install\nUsage\nSimple usage\nnode-icu-charset-detector provides a function detectCharset(buffer), where buffer is an instance of Buffer whose charset should be detected.\nvar charsetDetector = require(\"node-icu-charset-detector\");\n\nvar buffer = fs.readFileSync(\"\/path\/to\/the\/file\");\nvar charset = charsetDetector.detectCharset(buffer);\n\nconsole.log(\"charset name: \" + charset.toString());\nconsole.log(\"language: \" + charset.language);\nconsole.log(\"detection confidence: \" + charset.confidence);\ndetectCharset(buffer) returns the detected charset name for buffer, and the returned charset name has two extra properties language and confidence:\n\ncharset.language\n\nlanguage name for the detected character set.\n\n\ncharset.confidence\n\nconfidence of the charset detection for charset.\n\n\n\nLeveraging node-iconv\nSince ICU itself does not have a feature to convert character sets, you may need to use node-iconv (https:\/\/github.com\/bnoordhuis\/node-iconv), which has a powerful character sets converting feature.\nHere is a simple example to leverage node-iconv to convert character sets not supported by Node itself.\nfunction bufferToString(buffer) {\n  var charsetDetector = require(\"node-icu-charset-detector\");\n  var charset = charsetDetector.detectCharset(buffer).toString();\n\n  try {\n    return buffer.toString(charset);\n  } catch (x) {\n    var Iconv = require(\"iconv\").Iconv;\n    var charsetConverter = new Iconv(charset, \"utf8\");\n    return charsetConverter.convert(buffer).toString();\n  }\n}\n\nvar buffer = fs.readFileSync(\"\/path\/to\/the\/file\");\nvar bufferString = bufferToString(buffer);\n","383":"OpenCVBlobsLib is a library written in C++ on the base of cvblobslib. It allows for labelling, filling, filtering, gathering information when dealing with \"zones\" with homogeneous features in an image. It uses OpenCV and PThread in order to boost the performance. The used algorithm is very efficient with big images and\/or many blobs and can become even faster exploiting the multi-core architecture of modern CPUs.\nA list of its features:\n\nBinary image 8-connected component labelling\/blob extraction.\nBlob filtering (based on size or other user-defined features).\nBlob properties computation, e.g.:\nMean and standard deviation of the pixel values in the covered region.\nArea and perimeter.\nBounding box.\nContaining ellipse.\nMoments computation.\nColor-fill of the blob region.\n\nOpenCVBlobsLib added Features:\n\nMulti core support for the extraction stage.\nOpenCV 2.0 compliant interface.\nBlob joining, allowing for distinct regions to be grouped as one.\nGeneric bug fixing.\n\n","384":"STATUS: DEPRECATED, See ofxOMXCamera for future development\/releases\nMaster may be unstable, features untested. See Releases for tested versions\nDESCRIPTION:\nopenFrameworks addon to control the Raspberry Pi Camera Module. This does not provide still camera functionality.\nREQUIREMENTS:\nopenFrameworks .9 or higher Setup Guide\nDeveloped with GPU memory set at 256, overclock to medium but 128\/default should work as well\nDesktop Mode (X11 enabled) may work but untested\nUSAGE:\nClone into your openFrameworks\/addons folder\nEither copy one of the examples into \/myApps or add ofxRPiCameraVideoGrabber to the addons.make file in your project\nLED Toggling requires gpio program provided via wiringPi\n$sudo apt-get install wiringpi\nThe addon works in a few different modes:\nTEXTURE MODE:\nAllows the use of:\n\nShaders\nPixel access\nOverlays, etc\n\nNON-TEXTURE MODE (or direct-to screen)\nIn non-texture mode the camera is rendered directly to the screen. It typically looks a bit faster\/cleaner but no other drawing operations can happen.\nRECORDING:\nRecording is available in both texture and non-texture modes\nEXAMPLES:\nexample-demo-mode\nShows different settings available to tweak the camera exposure, metering, cropping, zooming, filters, mirroring, white balance\nexample-direct-mode\nCamera turns on and is rendered full screen via OMX acceleration\nPress the \"e\" key to toggle through built in filters\nexample-direct-mode-transform\nDemos cropping, alpha, mirroring of direct display (not camera)\nexample-texture-mode\nCamera turns on and renders to a texture that is drawn at full screen and a scaled version\nPress the \"e\" key to toggle through built in filters\nexample-shaders\nBasic shader usage with texture-mode\nPress the \"e\" key to toggle through built in filters\nPress the \"s\" key to toggle shader\nexample-saved-settings:\nAlternative way to load a camera configuration through a text file\nexample-recording:\nRecording of video in texture or direct mode\nexample-wrapper:\nDrop-in replacement for ofVideoGrabber (texture-mode only)\nTHANKS:\nThanks to @tjormola for sharing his demos and exploration - especially in regards to recording\nhttps:\/\/github.com\/tjormola\/rpi-openmax-demos\nand thanks to @linuxstb for helping get started with the camera and OpenMax\nhttps:\/\/github.com\/linuxstb\/pidvbip\n","385":"kinectable_pipe\nkinectable_pipe is a command-line utility that dumps user skeleton data from a Microsoft Kinect device to a standard Unix pipe.\nWhy?\nTo bring Unix-y goodness to the world of Microsoft Kinect programming!\nNo, really, why?\nBecause Kinect programming is a pain in the neck, and by trivializing the device's output into a simple text format, it becomes infinitely easier to digest in the scripting language of your choice.\nThis seems simple to the point of being almost useless\nYes, that's the point. Do One Thing and Do It Well. There's an accompanying rubygem that will add all the smart stuff like advanced gesture recognition, events, etc.\nUSAGE\n% kinectable_pipe | ruby gesture_recognizer.rb | python play_light_show.py\n\nINSTALLATION (OS X \/ homebrew)\n# must have universal binary for libusb\nbrew uninstall libusb\nbrew install libusb --universal\n\nbrew tap marshally\/alt\n\nbrew install kinectable_pipe\n\n# now plug in your kinect\n# and run this command in the terminal\n\nkinectable_pipe\n\n# step back from the sensor, wave your arms like a lunatic\n# until it recognizes you and starts pumping out skeleton data\n# to STDOUT\n\nOPTIONS\n-r 15 # restrict output to 15fps, Kinect max is 30fps\n\nTODO\n\nLinux install instructions\nRecognize all available xn::GestureGenerator\nFix output to work with non-interactive terminal sessions.\nImprove Ruby sample app.\nAdd sample apps for other scripting languages.\nAdd CLI argument for alternate output encodings (XML, msgpack, BERT, whatever).\n\n","386":"\n\nSingle file embedded C++ web server\n===\nHow to use in own project\n#include <iostream>\n#include <map>\n\n#include \"web++.hpp\"\n\nusing namespace WPP;\n\nvoid web(Request* req, Response* res) {\n    std::cout << req->method << \" \" << req->path << std::endl;\n\n    std::cout << \"Headers:\" << std::endl;\n\n    std::map<std::string, std::string>::iterator iter;\n    for (iter = req->headers.begin(); iter != req->headers.end(); ++iter) {\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\n    }\n\n    std::cout << \"Query:\" << std::endl;\n\n    for (iter = req->query.begin(); iter != req->query.end(); ++iter) {\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\n    }\n\n    std::cout << \"Cookies: \" << req->cookies.size() << std::endl;\n\n    for (iter = req->cookies.begin(); iter != req->cookies.end(); ++iter) {\n        std::cout << iter->first << \" = \" << iter->second << std::endl;\n    }\n\n    res->body << \"HELLO\";\n}\n\nint main(int argc, const char* argv[]) {\n    try {\n        std::cout << \"Listening on port 5000\" << std::endl;\n\n        WPP::Server server;\n        server.get(\"\/\", &web);\n        server.all(\"\/dir\", \".\/\");\n        server.start(5000);\n    } catch(WPP::Exception e) {\n        std::cerr << \"WebServer: \" << e.what() << std::endl;\n    }\n\n    return EXIT_SUCCESS;\n}\nHow to compile\ng++ demo.cpp -o demo\n\nSpecial requirements\nNop\nTested on\n\nMac OS X\nLinux\n\nThe MIT License\nCopyright (c) Alex Movsisyan\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n","387":"WebKit\nWebKit is an open source web browser engine. QtWebKit is the Qt Port of WebKit.\nThis code is based on the source found in webkit.org but it contains modifications introduced\nby the isis-project.\nFor more information see:\nhttp:\/\/www.webkit.org\nhttp:\/\/developer.qt.nokia.com\/wiki\/QtWebKit\nhttp:\/\/en.wikipedia.org\/wiki\/Webkit\nhttp:\/\/isis-project.org\/\nLicense\nSome parts of WebKit are available under the GNU Lesser General Public License and others under\na BSD-style license.\n","388":"\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a Marunouchi.mongo\n\u4e3b\u306b\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a\u306e\u8cc7\u6599\u3092\u7ba1\u7406\u3057\u3066\u3044\u307e\u3059\u3002\nThe guide of meetup MongoDB in Marunouchi Tokyo.\n\n\u958b\u50ac\u4e88\u5b9a\n2015\/7\/16\uff08\u6728\uff0919:00 - 20:30 \u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #22 \u300c\u521d\u5fc3\u8005\u5411\u3051\uff1aMongoDB\u306e\u57fa\u790e\u3068\u30cf\u30f3\u30ba\u30aa\u30f3(ver3\u5bfe\u5fdc)\u300d\u3068\u300cMongoDB World 2015\u30ec\u30dd\u30fc\u30c8\u300d 2015\/7\/16DoorKeeper\n\u304a\u77e5\u3089\u305b\ngihyo.jp\u3067\u300cMongoDB\u3067\u3086\u308b\u3075\u308fDB\u4f53\u9a13\u300d\u9023\u8f09\u7d42\u4e86\uff01\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff01\n\u3054\u767a\u8868\u3044\u305f\u3060\u3044\u305f\u8cc7\u6599\u306fOpenStandia\u306eWeb\u30b5\u30a4\u30c8\u304b\u3089\u30ea\u30f3\u30af\u3092\u8cbc\u3089\u305b\u3066\u3044\u305f\u3060\u304f\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u3054\u4e86\u627f\u304f\u3060\u3055\u3044\u3002\n\nMongoDB\u6700\u521d\u306e1\u6b69\n@fetarodc\u306e\u521d\u5fc3\u8005\u5411\u3051MongoDB\u306e\u30ad\u30db\u30f3\uff01\u3092\u4e00\u901a\u308a\u8aad\u3080\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u3068\u308a\u3042\u3048\u305a\u624b\u3092\u52d5\u304b\u3057\u305f\u3044\u4eba\u306f\u3001\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #1\u306estep01\u3092\u3084\u3063\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\n\nWiki\n\u52c9\u5f37\u4f1a\u3067\u51fa\u305f\u8cea\u554f\u3092\u4e2d\u5fc3\u306b\u30ce\u30a6\u30cf\u30a6\u3001Tips\u3092\u307e\u3068\u3081\u3066\u3044\u304d\u307e\u3059\u3002\nwiki\n\n\u304a\u5f79\u7acb\u3061\u30ea\u30f3\u30af\u96c6\nmongodb\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9(rpm\u3082\u3042\u308b\u3088) - github\n\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8(\u65e5\u672c\u8a9e) \u30de\u30cb\u30e5\u30a2\u30eb\n\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8(\u65e5\u672c\u8a9e) \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\n\u65b0\u30fb\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8(\u65e5\u672c\u8a9e) \u30de\u30cb\u30e5\u30a2\u30eb\nMongoDB\u306e\u57fa\u790e(\u52d5\u753b) - dotinstall\n@doryokujin\u3055\u3093\u306e\u30b9\u30e9\u30a4\u30c9\n@fetarodc\u306e\u521d\u5fc3\u8005\u5411\u3051MongoDB\u306e\u30ad\u30db\u30f3\uff01\nMongoDB\u3067\u3086\u308b\u3075\u308fDB\u4f53\u9a13\n\n\u958b\u50ac\u8a18\u9332\n\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #1 SQL\u3068\u6bd4\u8f03\u3057\u306a\u304c\u3089\u30af\u30a8\u30ea\u3092\u5b66\u3076&Ruby\u304b\u3089MongoDB\u3092\u89e6\u3063\u3066\u307f\u308b 2012\/07\/30 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #2 \u307f\u3093\u306a\u3067\u30b7\u30e3\u30fc\u30c7\u30a3\u30f3\u30b0 2012\/08\/28 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #3 2.2\u306e\u65b0\u6a5f\u80fd&\u30ec\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30cf\u30f3\u30ba\u30aa\u30f3 2012\/09\/26 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #4 \u300cMongoDB\u3067Web\u30a2\u30d7\u30ea\u3092\u4f5c\u3063\u3066\u307f\u3088\u3046\u300d\u3068\u300cConfig\u30d1\u30e9\u30e1\u30fc\u30bf\u89e3\u8aac\u300d 2012\/11\/06 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #5 \u300c\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\u5165\u9580\u300d\u3068\u300cMongoDB\u3092\u30ab\u30b9\u30bf\u30e0\u30d3\u30eb\u30c9\u3057\u3066\u307f\u3088\u3046\u300d\u3068\u300c\u904b\u7528\u306b\u3064\u3044\u3066\u300d 2012\/12\/18 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #6 \u300cGridFS\u30cf\u30f3\u30ba\u30aa\u30f3\u300d\u3068\u300cSharding\u306e\u30b3\u30cd\u30af\u30b7\u30e7\u30f3\u6570\u5468\u308a\u306e\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306b\u3064\u3044\u3066\u300d 2012\/01\/23 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #7 \u300cMongoDB 2.4 \u65b0\u6a5f\u80fd\u7d39\u4ecb\u300d\u3068\u300cMongoDB\u3067XML\u30c7\u30fc\u30bf\u3092\u6271\u3046\u30b7\u30b9\u30c6\u30e0\u958b\u767a\u300d 2013\/02\/19 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #8 in CookPad \u300cghostsync and slaveDelay\u300d\u3001\u300c\u6a29\u9650\u306b\u3088\u308bACL\u306e\u30cf\u30f3\u30ba\u30aa\u30f3\u300d\u4ed6 2013\/03\/27 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #9 in \u697d\u5929 \u300cMongoDB 2.4\u306e\u6ce8\u76ee\u306e\u65b0\u6a5f\u80fd\u30cf\u30f3\u30ba\u30aa\u30f3\u300d\u3068\u300cMongoDB\u306e\u691c\u8a3c\u74b0\u5883\u3092\u4f5c\u308d\u3046\u300d 2013\/04\/17 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #10 \u300c\u521d\u5fc3\u8005\u5411\u3051MongoDB\u5165\u9580\u300d\u3068\u300cMongoDB\u306e\u904b\u7528\u300d 2013\/05\/22 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #11 \u300c\u521d\u5fc3\u8005\u5411\u3051\u30ec\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30cf\u30f3\u30ba\u30aa\u30f3\u300d\u300cMongoDB\u3067\u30b2\u30fc\u30e0\u3092\u4f5c\u308b\u300d 2013\/06\/26 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #12 in \u7d0d\u6dbc\u3082\u3093\u3054\u796d\u308a \u300cNode.js+Mongoose+MongoDB \u3067\u4f5c\u308bWeb\u30a2\u30d7\u30ea\u300d\u300c\u30db\u30b9\u30c6\u30a3\u30f3\u30b0\u30b5\u30fc\u30d3\u30b9\u3067\u59cb\u3081\u308bMongoDB\u300d 2013\/07\/28 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #13 \u300c\u521d\u5fc3\u8005\u5411\u3051\uff1a\u307f\u3093\u306a\u3067\u30b7\u30e3\u30fc\u30c7\u30a3\u30f3\u30b0\u300d 2013\/09\/25 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #14 \u300c\u521d\u5fc3\u8005\u5411\u3051\uff1a\u30ec\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\uff0b\u30b7\u30e3\u30fc\u30c7\u30a3\u30f3\u30b0\u300d\u300c\u3082\u3093\u3054\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\uff01\u7b2c\u4e00\u5f3e\u300d 2013\/10\/31 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #15 \u300c\u521d\u5fc3\u8005\u5411\u3051\uff1a\u30ec\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\uff0b\u30b7\u30e3\u30fc\u30c7\u30a3\u30f3\u30b0\uff08\u7b2c14\u56de\u306e\u30ea\u30d9\u30f3\u30b8\uff09\u300d\u300cMongoDB\u3068\u79c1\uff08\u4e8b\u4f8b\u7d39\u4ecb\uff09\u300d 2013\/12\/19 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #16 \u300c\u304c\u3063\u3064\u308a\u4e8b\u4f8b\u7d39\u4ecb\u300d 2014\/3\/19 ATND\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #17 \u300c\u521d\u5fc3\u8005\u5411\u3051MongoDB\u306e\u30ad\u30db\u30f3\uff01\u300d\u300c\u521d\u5fc3\u8005\u5411\u3051MongoDB\u5165\u9580\u300d\u300c\u6708\u959310\u5104PV\u3092\u652f\u3048\u308bMongoDB\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u300d 2014\/5\/14 DoorKeeper\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #18 \u300c\u521d\u5fc3\u8005\u5411\u3051\uff1a\u30ec\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\uff0b\u30b7\u30e3\u30fc\u30c7\u30a3\u30f3\u30b0\u300d\u300cMongoDB World@NewYork \u5831\u544a\u4f1a\u300d\u300cLT\uff1aMongoDB World 2014 \u30d7\u30c1\u5831\u544a\u300d 2014\/7\/17 DoorKeeper\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #19 in \u3082\u3093\u3054\u796d\u308a\u300c\u521d\u5fc3\u8005\u5411\u3051\uff1aSQL\u3068\u6bd4\u8f03\u3057\u306a\u304c\u3089\u30af\u30a8\u30ea\u3092\u5b66\u3076\u300d\u300cMongoDB\u306eindex\u306b\u3064\u3044\u3066\u300d 2014\/10\/11 DoorKeeper\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #20 \u300c\u521d\u5fc3\u8005\u5411\u3051\uff1aSQL\u3068\u6bd4\u8f03\u3057\u306a\u304c\u3089\u30af\u30a8\u30ea\u3092\u5b66\u3076\u30cf\u30f3\u30ba\u30aa\u30f3\u300d\u300cMMS\u3001\u7279\u306b\u65b0\u6a5f\u80fd\u300eAutomation\u300f\u306b\u3064\u3044\u3066\u300d\u300cLT:MongoDB 2.8RC0\u306e\u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3\u306b\u3064\u3044\u3066\u300d 2014\/11\/26 DoorKeeper\n\u4e38\u306e\u5185MongoDB\u52c9\u5f37\u4f1a #21\u300cCouchbase Server\u306e\u7d39\u4ecb\u300d\u300cMongoDB \u306e\u7d39\u4ecb\u3068Couchbase\u3068\u306e\u9055\u3044\u300d 2015\/03\/05 DoorKeeper\n\n","389":"DEPRECATED: See https:\/\/www.mysensors.org\/build\/raspberry\n#Wiring the NRF\t24L01+ radio\n\n\n\nNRF24l01+\nRpi Header Pin\n\n\n\n\nGND\n25\n\n\nVCC\n17\n\n\nCE\n22\n\n\nCSN\n24\n\n\nSCK\n23\n\n\nMOSI\n19\n\n\nMISO\n21\n\n\nIRQ\n--\n\n\n\n#Building & Installing\n##RF24 library\n\nDownload the library from https:\/\/github.com\/TMRh20\/RF24\nEither an official release(tested with 1.1.3) or clone the master branch.\nDecompress(if needed) and change to the library directory\nRun make all followed by sudo make install\n\n##Serial Gateway\nThe standard configuration will build the Serial Gateway with a tty name of\n'\/dev\/ttyMySensorsGateway' and PTS group ownership of 'tty' the PTS will be group read\nand write. The default install location will be \/usr\/local\/sbin. If you want to change\nthat edit the variables in the head of the Makefile.\n###Build the Gateway\n\nClone this repository\nChange to the Raspberry directory\nRun make all followed by sudo make install\n(if you want to start daemon at boot) sudo make enable-gwserial\n\nFor some controllers a more recognisable name needs to be used: e.g. \/dev\/ttyUSB020 (check if this is free).\nsudo ln -s \/dev\/ttyMySensorsGateway \/dev\/ttyUSB20\nTo automatically create the link on startup, add ln -s \/dev\/ttyMySensorsGateway \/dev\/ttyUSB20 just before exit0 in \/etc\/rc.local\n#Uninstalling\n\nChange to Raspberry directory\nRun sudo make uninstall\n\nSupport: http:\/\/forum.mysensors.org\n","390":"Welcome to python-nlpir\nThis open source project is a python wrapper for NLPIR.\nNLPIR is a powerful tool for Chinese segmentation. This program supported Windows and Linux, both 32bit and 64bit platform.\nCurrent Version: v3.0\nChange Log\nVersion 3.0\n\nUprgrade NLPIR to NPLIR2015(v20141230).\nFix bug in installation script.\n\nVersion 2.0\n\nUprgrade NLPIR core library from NPLIR to NPLIR2014.\nInstall SWIG automatically during PyNLPIR installation.\nRefactor installation scripts.\n\nVersion 1.1\n\nAdd Windows 64bit and Linux 64bit supported.\nCode constructure adjustment.\nUpgrade NLPIR2013 core library.\nAdd installation scripts for each platform.\n\nInstallation\n\nWindows: python install.py\nLinux: sudo python install.py\n\nOthers\nIf you want to know more details, you can access my blog.\nLinks\n\nNLPIR Home: http:\/\/www.nlpir.org\/\n\n","391":"Ioniclub App\n\n\nhttp:\/\/ionichina.com Ionichina\u793e\u533a\u5ba2\u6237\u7aef\uff0c\u91c7\u7528Ionic Framework\u5f00\u53d1\n\n\n\u5feb\u901f\u5f00\u59cb\n1. \u9996\u5148\u5b89\u88c5ionic\n$ sudo npm install -g cordova ionic\n\n2. \u9879\u76eeClone\u5230\u672c\u5730\n$ git clone https:\/\/github.com\/IonicChina\/ioniclub.git\n\n3. \u6dfb\u52a0 android \u6216 ios \u5e73\u53f0\n\u6ce8\uff1a \u771f\u673a\u8c03\u8bd5\uff0c\u6d4f\u89c8\u5668\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\uff1b\nios \u5f00\u53d1\u53ea\u80fd\u5728 mac \u4e0b\u8fdb\u884c\u3002\n$ cd ioniclub\n$ ionic platform add ios\n$ ionic platform add android\n\n4. \u6dfb\u52a0\u6240\u6709\u7528\u5230\u7684\u63d2\u4ef6\nbower install moment --save\nbower install ngCordova\nbower install angular-moment --save\nbower install angular-resource --save\ncordova plugin add https:\/\/github.com\/danwilson\/google-analytics-plugin.git\ncordova plugin add https:\/\/github.com\/EddyVerbruggen\/SocialSharing-PhoneGap-Plugin.git\ncordova plugin add cordova-plugin-inappbrowser\ncordova plugin add https:\/\/github.com\/katzer\/cordova-plugin-email-composer.git\ncordova plugin add https:\/\/github.com\/whiteoctober\/cordova-plugin-app-version.git\ncordova plugin add cordova-plugin-network-information\ncordova plugin add https:\/\/github.com\/wildabeast\/BarcodeScanner.git\ncordova plugin add https:\/\/github.com\/VersoSolutions\/CordovaClipboard.git\n\njpush \u63d2\u4ef6\u5b89\u88c5\u53c2\u8003\uff1a[https:\/\/github.com\/jpush\/jpush-phonegap-plugin](https:\/\/github.com\/jpush\/jpush-phonegap-plugin\n\u6ce8\uff1a\u5728\u6dfb\u52a0\u4e86BarcodeScanner\u63d2\u4ef6\u540eandroid \u6253\u5305\u4f1a\u9047\u5230\u4e2a\u86cb\u75bcerror\uff0c\u89e3\u51b3\u65b9\u6848\u662f\u5728platforms\/android\/build.gradle\u6587\u4ef6\u91cc android {} \u7ed3\u70b9\uff08243\u884c\uff09\u6dfb\u52a0\n  abortOnError false\n}```\n[#\u8be6\u60c5](http:\/\/forum.ionicframework.com\/t\/error-when-running-cordova-build-release-android\/25136\/4)\n### 5. \u8fd0\u884c\n#### \u6d4f\u89c8\u5668\n  $  ionic serve\n#### ios\n  $  ionic build ios\n  $  ionic run ios\n#### android\n  $  ionic build android\n  $  ionic run android\n\n\n## \u8d21\u732e\n[\u8d21\u732e\u8005\u5217\u8868](https:\/\/github.com\/IonicChina\/ioniclub\/graphs\/contributors)\n\n\u6709\u4efb\u4f55\u610f\u89c1\u6216\u5efa\u8bae\u90fd\u6b22\u8fce\u63d0 issue\uff0c\u6216\u8005\u76f4\u63a5\u5728\u793e\u533a [http:\/\/ionichina.com](http:\/\/ionichina.com) \u63d0\u7ed9 [@DongHongfei](http:\/\/ionichina.com\/user\/DongHongfei)\n\n## License\n[MIT](LICENSE)\n\n","392":"Framer Presentation Templates\n\nFeatures\n\nWhen viewed on a desktop browser, your prototype will appear inside a device \"frame\", with an optional background behind it. The content of your prototype will be scaled to match the size of your browser window.\nWhen viewed on a mobile device, the frame disappears, so your prototype can feel like a real app. As a bonus, Mobile Safari's bouncy scrolling is also prevented automatically.\nSelf-contained. All you need to do is link to a script. No need to bother with additional assets or write code.\nOption to prompt users to add the prototype as an app on their homescreen before using it.\nSwitch between presentation and development mode by pressing Alt + P.\nSwitch between zoom levels with Alt + 1 (100%), Alt + 2 (75%), Alt + 3 (50%), Alt + 4 (25%), Alt + 0 (Fit to Screen). You can also use Alt + - to zoom out and Alt + = to zoom in.\n\nUsage\n\nDownload a template file from templates\/ that matches your needs and place it in your prototype's main directory. Here's the list of all currently available templates:\n\n\niPhone 5s (White, Portrait)\niPhone 5s (White, Landscape)\niPhone 5s (Black, Portrait)\niPhone 5s (Black, Landscape)\niPhone 5s (Gold, Portrait)\niPhone 5s (Gold, Landscape)\niPhone 5c (White, Portrait)\niPhone 5c (White, Landscape)\niPhone 5c (Blue, Portrait)\niPhone 5c (Blue, Landscape)\niPhone 5c (Green, Portrait)\niPhone 5c (Green, Landscape)\niPhone 5c (Red, Portrait)\niPhone 5c (Red, Landscape)\niPhone 5c (Yellow, Portrait)\niPhone 5c (Yellow, Landscape)\niPad Mini (White, Portrait)\niPad Mini (White, Landscape)\niPad Mini (Black, Portrait)\niPad Mini (Black, Landscape)\nNexus 5 (Portrait)\nNexus 5 (Landscape)\nAndroid Wear (Circle)\nAndroid Wear (Square)\n\n\nOpen your index.html in a text editor and link to the template using a <script> tag. The template should be included after the framer.js script:\n\n<script src=\"framer\/framer.js\"><\/script>\n<script src=\"framer\/framerps.js\"><\/script>\n<script src=\"app.js\"><\/script>\n\n<!-- Link to the template script: -->\n<script src=\"iphone-5s-white.js\"><\/script>\n\nLoad index.html in a browser. You should see your prototype appear inside the presentation template.\n\nUsage with Framer Studio\n\n\nDownload one of the templates and place it in the folder of your project, under framer\/.\n\n\nPut this at the top of your script:\n\n\nUtils.domLoadScriptSync('framer\/iphone-5c-blue-landscape.js')\n\nChoose the Fullscreen option in the Preview pane, so you don't get the \"phone in a phone\" effect.\n\nInstalling with Bower\nYou can use bower to install and keep the templates up to date:\nbower install framer-templates\n\nAll of the available templates will be installed to bower_components\/framer-templates\/templates.\nA Note on Compatibility\nThe presentation templates are fully compatible with Framer 3. You can still download the old, Framer 2 compatible version of the templates from here: Download Framer 2 Templates.\nIf you're upgrading from a Framer 2 template, everything should work as expected by just replacing the template file. If you've used the config.template = { ... } syntax to customize the appearance of your template, you should change it to Framer.Config.template = { ... }.\nSetting the Background Image\nThe default background image is a plain off-white color, but if that's not to your tastes, you can use your own by specifying it as a template option before you load the template:\n<script>FramerTemplateConfig = { backgroundImage: '[url to your background image]' };<\/script>\n<script src=\"iphone-5s-white.js\"><\/script>\n\"Add to Homescreen\" Prompt\n\nThe templates have built-in capability for reminding users that view your prototype inside a mobile browser to add it to their homescreen for a more app-like experience. This feature is off by default, but you can turn it on like this:\n<script>FramerTemplateConfig = { shouldShowAddToHomescreenPrompt: true };<\/script>\n<script src=\"iphone-5s-white.js\"><\/script>\nTips\n\nAppend #dev to the URL of your prototype to load it directly in developer mode (no device frame, scaled to 100%). E.g. http:\/\/localhost\/prototype\/index.html#dev\nAppend #z75 to the URL of your prototype to load it at 75% scale. Putting any number after the z will work.  E.g. http:\/\/localhost\/prototype\/index.html#z75\n\nBuilding Your Own Template\nAll templates use the same basic code, but vary in configuration, depending on which device you want to use to present your prototype.\nHere's what the configuration file for the white iPhone template looks like:\n{\n  \"backgroundImage\": \"\",\n  \"shouldShowAddToHomescreenPrompt\": false,\n  \"deviceWidth\": 385,\n  \"deviceHeight\": 805,\n  \"screenWidth\": 320,\n  \"screenHeight\": 568,\n  \"contentWidth\": 640,\n  \"contentHeight\": 1136,\n  \"cursorWidth\": 32,\n  \"promptAnchorTop\": 704,\n  \"promptAnchorLeft\": 320,\n  \"addToHomescreenPromptImage\": \"images\/addtohomescreen-prompt-iphone.png\",\n  \"deviceImage\": \"images\/iphone-5s-white.png\",\n  \"cursorImage1x\": \"\",\n  \"cursorImage2x\": \"\"\n}\nLet's go over the different attributes:\n\n\nbackgroundImage (data URI, URL, or image path) Default: empty\nAn image that will be placed behind the device image.\n\n\npreventBounce (true or false) Default: true\nPrevents the entire page from bouncing when scrolled up\/down (will also prevent scrolling beyond the content boundaries).\n\n\nshouldShowAddToHomescreenPrompt (true or false) Default: false\nSpecifies whether an \"add to homescreen\" prompt should be shown when the prototype is viewed on an iOS device, but not in homescreen standalone app mode.\n\n\ndeviceWidth and deviceHeight (number)\nThe width and height of the device image you're using. Note that the device image is automatically scaled by 50% in order to look crisp and clear on a retina screen, so you'll need to specify the size \/ 2 (e.g. if your image is 2050px, put in 1025 for device width).\n\n\nscreenWidth and screenHeight (number)\nThe size of the actual screen in your device image. For the iPhone background, for example, the size of the screen is 320x568.\n\n\ncontentWidth and contentHeight (number)\nThe size of your actual prototype. This is usually the screen size multiplied by 2. By default, Framer's template assumes that your content is 640px wide (iPhone sized).\n\n\nsidePadding (number) Default: 50\nThe minimum amount of padding (in px) you want to leave between the edges of the browser window and the device image.\n\n\nzoomFactor (number) Default: not set\nA specific zoom factor to use, instead of automatically re-scaling your prototype to fit inside your browser window. A zoom factor of 0.75 would mean 75%, a factor of 2 would mean 200%.\n\n\ncursorWidth (number)\nThe width of the custom cursor image you supplied. This is used to figure out the location of anchor point of the cursor (for circular cursors, for example, this would be the middle of the circle).\n\n\npromptAnchorTop and promptAnchorLeft (number)\nThis is the point in the \"add to homescreen\" prompt image that the arrow points to. You don't have to specify this if you're not using the prompt in the first place.\n\n\naddToHomescreenPromptImage (data URI, URL, or image path)\nThis image will be displayed if the prototype is viewed on a mobile device and not in standalone (homescreen app) mode.\n\n\ndeviceImage (data URI, URL, or image path)\nThis is the device image that will be used to \"hold\" your prototype. Make sure that the actual screen portion of the device is centered perfectly in the image (hint: use one of the existing images for a template).\n\n\ncursorImage1x and cursorImage2x (data URI, URL, or image path) Default: a 66x66 bobble cursor.\nCustom cursor images in normal and retina resolutions.\n\n\ncursorPressedImage1x and cursorPressedImage2x (data URI, URL, or image path) Default: a 66x66 bobble cursor.\nCustom pressed state cursor images in normal and retina resolutions.\n\n\nNote: All image paths are automatically converted to data URIs, so that the template is self-contained.\nTo create your own template, copy one of the config-*.json files, edit the configuration values in there and then run:\nnpm install\ncake build\n\nYou should see something like this:\nclean Cleaning out templates directory...\nbuild Using src\/config-ipad-mini-black.json to generate templates\/ipad-mini-black.js\nbuild Using src\/config-ipad-mini-white.json to generate templates\/ipad-mini-white.js\nbuild Using src\/config-iphone-5c-blue.json to generate templates\/iphone-5c-blue.js\nbuild Using src\/config-iphone-5c-green.json to generate templates\/iphone-5c-green.js\nbuild Using src\/config-iphone-5c-red.json to generate templates\/iphone-5c-red.js\nbuild Using src\/config-iphone-5c-white.json to generate templates\/iphone-5c-white.js\nbuild Using src\/config-iphone-5c-yellow.json to generate templates\/iphone-5c-yellow.js\nbuild Using src\/config-iphone-5s-black.json to generate templates\/iphone-5s-black.js\nbuild Using src\/config-iphone-5s-gold.json to generate templates\/iphone-5s-gold.js\nbuild Using src\/config-iphone-5s-white.json to generate templates\/iphone-5s-white.js\nbuild Using src\/config-nexus-5.json to generate templates\/nexus-5.js\n\nAll done. Have a nice day!\n\nThe newly generated template will appear under the templates\/ directory.\nThanks & Acknowledgements\n\niPhone 5s & 5c PSD by Louie Mantia\nNexus 5 PSD by Victor Stuber\nBobble Cursor Image from Josh Puckett's FramerWebView\nAndroid Wear templates by Patrick Keenan\n\n","393":"\u0411\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 \u0431\u043b\u043e\u043a\u043e\u0432 bem-bl\n\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0438 \u043f\u0440\u0438\u043c\u0435\u0440\u044b.\n\n","394":"NodeSource Docker Images\nThe NodeSource docker images deliver NodeSource's deb and rpm packages across all of our supported platforms! We offer version pinning, allowing your project to track major, minor, or patch versions of Node or iojs.\nUsage\nUse any one of our images as a base for your image. We suggest caching your package.json and npm install in layers to reduce build time:\nFROM nodesource\/jessie:0.12.7\n\n# cache package.json and node_modules to speed up builds\nADD package.json package.json\nRUN npm install\n\n# Add your source files\nADD . .\nCMD [\"npm\",\"start\"]\nNotes\n\nNODE_ENV is set to production on these images. If you are using these images for development work, add the line: ENV NODE_ENV dev to your Dockerfile.\n\nImages\nWe offer tags for tracking major and minor releases. These tags enable you to receive either security patches or new features, while avoiding breaking changes.\nNote: some images may not have all tags, if a major\/minor version is not supported on a particular release, these tags will not work.\nFor example, Ubuntu Precise does not have any iojs tags, and Ubuntu Vivid does not have any tags before iojs-1.8.1.\n\nlatest points to the latest release of Node\nLTS points to the latest LTS release of Node\nargon points to the named LTS release of Node\nX points to the latest release of Node X\nX.Y points to the latest release of Node X.Y\niojs points to the latest release of iojs\niojs-X points to the latest minor release of iojs-X\niojs-X.Y points to the latest patch release of iojs-X.Y\n\nDebian-based images\n\nDebian jessie - docker pull nodesource\/jessie\n\nNode 0.10.30 - docker pull nodesource\/jessie:0.10.30\nNode 0.10.31 - docker pull nodesource\/jessie:0.10.31\nNode 0.10.32 - docker pull nodesource\/jessie:0.10.32\nNode 0.10.33 - docker pull nodesource\/jessie:0.10.33\nNode 0.10.34 - docker pull nodesource\/jessie:0.10.34\nNode 0.10.35 - docker pull nodesource\/jessie:0.10.35\nNode 0.10.36 - docker pull nodesource\/jessie:0.10.36\nNode 0.10.37 - docker pull nodesource\/jessie:0.10.37\nNode 0.10.38 - docker pull nodesource\/jessie:0.10.38\nNode 0.10.39 - docker pull nodesource\/jessie:0.10.39\nNode 0.10.40 - docker pull nodesource\/jessie:0.10.40\nNode 0.10.41 - docker pull nodesource\/jessie:0.10.41\nNode 0.10.42 - docker pull nodesource\/jessie:0.10.42\nNode 0.10.43 - docker pull nodesource\/jessie:0.10.43\nNode 0.10.44 - docker pull nodesource\/jessie:0.10.44\nNode 0.10.45 - docker pull nodesource\/jessie:0.10.45\nNode 0.10.46 - docker pull nodesource\/jessie:0.10.46\nNode 0.12.0 - docker pull nodesource\/jessie:0.12.0\nNode 0.12.10 - docker pull nodesource\/jessie:0.12.10\nNode 0.12.11 - docker pull nodesource\/jessie:0.12.11\nNode 0.12.12 - docker pull nodesource\/jessie:0.12.12\nNode 0.12.13 - docker pull nodesource\/jessie:0.12.13\nNode 0.12.14 - docker pull nodesource\/jessie:0.12.14\nNode 0.12.15 - docker pull nodesource\/jessie:0.12.15\nNode 0.12.2 - docker pull nodesource\/jessie:0.12.2\nNode 0.12.3 - docker pull nodesource\/jessie:0.12.3\nNode 0.12.4 - docker pull nodesource\/jessie:0.12.4\nNode 0.12.5 - docker pull nodesource\/jessie:0.12.5\nNode 0.12.6 - docker pull nodesource\/jessie:0.12.6\nNode 0.12.7 - docker pull nodesource\/jessie:0.12.7\nNode 0.12.8 - docker pull nodesource\/jessie:0.12.8\nNode 0.12.9 - docker pull nodesource\/jessie:0.12.9\nNode 4.0.0 - docker pull nodesource\/jessie:4.0.0\nNode 4.1.0 - docker pull nodesource\/jessie:4.1.0\nNode 4.1.1 - docker pull nodesource\/jessie:4.1.1\nNode 4.1.2 - docker pull nodesource\/jessie:4.1.2\nNode 4.2.0 - docker pull nodesource\/jessie:4.2.0\nNode 4.2.1 - docker pull nodesource\/jessie:4.2.1\nNode 4.2.2 - docker pull nodesource\/jessie:4.2.2\nNode 4.2.3 - docker pull nodesource\/jessie:4.2.3\nNode 4.2.4 - docker pull nodesource\/jessie:4.2.4\nNode 4.2.5 - docker pull nodesource\/jessie:4.2.5\nNode 4.2.6 - docker pull nodesource\/jessie:4.2.6\nNode 4.3.0 - docker pull nodesource\/jessie:4.3.0\nNode 4.3.1 - docker pull nodesource\/jessie:4.3.1\nNode 4.3.2 - docker pull nodesource\/jessie:4.3.2\nNode 4.4.0 - docker pull nodesource\/jessie:4.4.0\nNode 4.4.1 - docker pull nodesource\/jessie:4.4.1\nNode 4.4.2 - docker pull nodesource\/jessie:4.4.2\nNode 4.4.3 - docker pull nodesource\/jessie:4.4.3\nNode 4.4.4 - docker pull nodesource\/jessie:4.4.4\nNode 4.4.5 - docker pull nodesource\/jessie:4.4.5\nNode 4.4.6 - docker pull nodesource\/jessie:4.4.6\nNode 4.4.7 - docker pull nodesource\/jessie:4.4.7\nNode 6.0.0 - docker pull nodesource\/jessie:6.0.0\nNode 6.1.0 - docker pull nodesource\/jessie:6.1.0\nNode 6.2.0 - docker pull nodesource\/jessie:6.2.0\nNode 6.2.1 - docker pull nodesource\/jessie:6.2.1\nNode 6.2.2 - docker pull nodesource\/jessie:6.2.2\nNode 6.3.0 - docker pull nodesource\/jessie:6.3.0\nNode 6.3.1 - docker pull nodesource\/jessie:6.3.1\n\n\nDebian sid - docker pull nodesource\/sid\n\nNode 0.10.30 - docker pull nodesource\/sid:0.10.30\nNode 0.10.31 - docker pull nodesource\/sid:0.10.31\nNode 0.10.32 - docker pull nodesource\/sid:0.10.32\nNode 0.10.33 - docker pull nodesource\/sid:0.10.33\nNode 0.10.34 - docker pull nodesource\/sid:0.10.34\nNode 0.10.35 - docker pull nodesource\/sid:0.10.35\nNode 0.10.36 - docker pull nodesource\/sid:0.10.36\nNode 0.10.37 - docker pull nodesource\/sid:0.10.37\nNode 0.10.38 - docker pull nodesource\/sid:0.10.38\nNode 0.10.39 - docker pull nodesource\/sid:0.10.39\nNode 0.10.40 - docker pull nodesource\/sid:0.10.40\nNode 0.10.41 - docker pull nodesource\/sid:0.10.41\nNode 0.10.42 - docker pull nodesource\/sid:0.10.42\nNode 0.10.43 - docker pull nodesource\/sid:0.10.43\nNode 0.10.44 - docker pull nodesource\/sid:0.10.44\nNode 0.10.45 - docker pull nodesource\/sid:0.10.45\nNode 0.10.46 - docker pull nodesource\/sid:0.10.46\nNode 0.12.0 - docker pull nodesource\/sid:0.12.0\nNode 0.12.10 - docker pull nodesource\/sid:0.12.10\nNode 0.12.11 - docker pull nodesource\/sid:0.12.11\nNode 0.12.12 - docker pull nodesource\/sid:0.12.12\nNode 0.12.13 - docker pull nodesource\/sid:0.12.13\nNode 0.12.14 - docker pull nodesource\/sid:0.12.14\nNode 0.12.15 - docker pull nodesource\/sid:0.12.15\nNode 0.12.2 - docker pull nodesource\/sid:0.12.2\nNode 0.12.3 - docker pull nodesource\/sid:0.12.3\nNode 0.12.4 - docker pull nodesource\/sid:0.12.4\nNode 0.12.5 - docker pull nodesource\/sid:0.12.5\nNode 0.12.6 - docker pull nodesource\/sid:0.12.6\nNode 0.12.7 - docker pull nodesource\/sid:0.12.7\nNode 0.12.8 - docker pull nodesource\/sid:0.12.8\nNode 0.12.9 - docker pull nodesource\/sid:0.12.9\nNode 4.0.0 - docker pull nodesource\/sid:4.0.0\nNode 4.1.0 - docker pull nodesource\/sid:4.1.0\nNode 4.1.1 - docker pull nodesource\/sid:4.1.1\nNode 4.1.2 - docker pull nodesource\/sid:4.1.2\nNode 4.2.0 - docker pull nodesource\/sid:4.2.0\nNode 4.2.1 - docker pull nodesource\/sid:4.2.1\nNode 4.2.2 - docker pull nodesource\/sid:4.2.2\nNode 4.2.3 - docker pull nodesource\/sid:4.2.3\nNode 4.2.4 - docker pull nodesource\/sid:4.2.4\nNode 4.2.5 - docker pull nodesource\/sid:4.2.5\nNode 4.2.6 - docker pull nodesource\/sid:4.2.6\nNode 4.3.0 - docker pull nodesource\/sid:4.3.0\nNode 4.3.1 - docker pull nodesource\/sid:4.3.1\nNode 4.3.2 - docker pull nodesource\/sid:4.3.2\nNode 4.4.0 - docker pull nodesource\/sid:4.4.0\nNode 4.4.1 - docker pull nodesource\/sid:4.4.1\nNode 4.4.2 - docker pull nodesource\/sid:4.4.2\nNode 4.4.3 - docker pull nodesource\/sid:4.4.3\nNode 4.4.4 - docker pull nodesource\/sid:4.4.4\nNode 4.4.5 - docker pull nodesource\/sid:4.4.5\nNode 4.4.6 - docker pull nodesource\/sid:4.4.6\nNode 4.4.7 - docker pull nodesource\/sid:4.4.7\nNode 6.0.0 - docker pull nodesource\/sid:6.0.0\nNode 6.1.0 - docker pull nodesource\/sid:6.1.0\nNode 6.2.0 - docker pull nodesource\/sid:6.2.0\nNode 6.2.1 - docker pull nodesource\/sid:6.2.1\nNode 6.2.2 - docker pull nodesource\/sid:6.2.2\nNode 6.3.0 - docker pull nodesource\/sid:6.3.0\nNode 6.3.1 - docker pull nodesource\/sid:6.3.1\n\n\nDebian wheezy - docker pull nodesource\/wheezy\n\nNode 0.10.30 - docker pull nodesource\/wheezy:0.10.30\nNode 0.10.31 - docker pull nodesource\/wheezy:0.10.31\nNode 0.10.32 - docker pull nodesource\/wheezy:0.10.32\nNode 0.10.33 - docker pull nodesource\/wheezy:0.10.33\nNode 0.10.34 - docker pull nodesource\/wheezy:0.10.34\nNode 0.10.35 - docker pull nodesource\/wheezy:0.10.35\nNode 0.10.36 - docker pull nodesource\/wheezy:0.10.36\nNode 0.10.37 - docker pull nodesource\/wheezy:0.10.37\nNode 0.10.38 - docker pull nodesource\/wheezy:0.10.38\nNode 0.10.39 - docker pull nodesource\/wheezy:0.10.39\nNode 0.10.40 - docker pull nodesource\/wheezy:0.10.40\nNode 0.10.41 - docker pull nodesource\/wheezy:0.10.41\nNode 0.10.42 - docker pull nodesource\/wheezy:0.10.42\nNode 0.10.43 - docker pull nodesource\/wheezy:0.10.43\nNode 0.10.44 - docker pull nodesource\/wheezy:0.10.44\nNode 0.10.45 - docker pull nodesource\/wheezy:0.10.45\nNode 0.10.46 - docker pull nodesource\/wheezy:0.10.46\nNode 0.12.0 - docker pull nodesource\/wheezy:0.12.0\nNode 0.12.10 - docker pull nodesource\/wheezy:0.12.10\nNode 0.12.11 - docker pull nodesource\/wheezy:0.12.11\nNode 0.12.12 - docker pull nodesource\/wheezy:0.12.12\nNode 0.12.13 - docker pull nodesource\/wheezy:0.12.13\nNode 0.12.14 - docker pull nodesource\/wheezy:0.12.14\nNode 0.12.15 - docker pull nodesource\/wheezy:0.12.15\nNode 0.12.2 - docker pull nodesource\/wheezy:0.12.2\nNode 0.12.3 - docker pull nodesource\/wheezy:0.12.3\nNode 0.12.4 - docker pull nodesource\/wheezy:0.12.4\nNode 0.12.5 - docker pull nodesource\/wheezy:0.12.5\nNode 0.12.6 - docker pull nodesource\/wheezy:0.12.6\nNode 0.12.7 - docker pull nodesource\/wheezy:0.12.7\nNode 0.12.8 - docker pull nodesource\/wheezy:0.12.8\nNode 0.12.9 - docker pull nodesource\/wheezy:0.12.9\nNode 4.1.1 - docker pull nodesource\/wheezy:4.1.1\nNode 4.1.2 - docker pull nodesource\/wheezy:4.1.2\nNode 4.2.0 - docker pull nodesource\/wheezy:4.2.0\nNode 4.2.1 - docker pull nodesource\/wheezy:4.2.1\nNode 4.2.2 - docker pull nodesource\/wheezy:4.2.2\nNode 4.2.3 - docker pull nodesource\/wheezy:4.2.3\nNode 4.2.4 - docker pull nodesource\/wheezy:4.2.4\nNode 4.2.5 - docker pull nodesource\/wheezy:4.2.5\nNode 4.2.6 - docker pull nodesource\/wheezy:4.2.6\nNode 4.3.0 - docker pull nodesource\/wheezy:4.3.0\nNode 4.3.1 - docker pull nodesource\/wheezy:4.3.1\nNode 4.3.2 - docker pull nodesource\/wheezy:4.3.2\nNode 4.4.0 - docker pull nodesource\/wheezy:4.4.0\nNode 4.4.1 - docker pull nodesource\/wheezy:4.4.1\nNode 4.4.2 - docker pull nodesource\/wheezy:4.4.2\nNode 4.4.3 - docker pull nodesource\/wheezy:4.4.3\nNode 4.4.4 - docker pull nodesource\/wheezy:4.4.4\nNode 4.4.5 - docker pull nodesource\/wheezy:4.4.5\nNode 4.4.6 - docker pull nodesource\/wheezy:4.4.6\nNode 4.4.7 - docker pull nodesource\/wheezy:4.4.7\nNode 6.0.0 - docker pull nodesource\/wheezy:6.0.0\nNode 6.1.0 - docker pull nodesource\/wheezy:6.1.0\nNode 6.2.0 - docker pull nodesource\/wheezy:6.2.0\nNode 6.2.1 - docker pull nodesource\/wheezy:6.2.1\nNode 6.2.2 - docker pull nodesource\/wheezy:6.2.2\nNode 6.3.0 - docker pull nodesource\/wheezy:6.3.0\nNode 6.3.1 - docker pull nodesource\/wheezy:6.3.1\n\n\n\nUbuntu-based images\n\nUbuntu precise - docker pull nodesource\/precise\n\nNode 0.10.30 - docker pull nodesource\/precise:0.10.30\nNode 0.10.31 - docker pull nodesource\/precise:0.10.31\nNode 0.10.32 - docker pull nodesource\/precise:0.10.32\nNode 0.10.33 - docker pull nodesource\/precise:0.10.33\nNode 0.10.34 - docker pull nodesource\/precise:0.10.34\nNode 0.10.35 - docker pull nodesource\/precise:0.10.35\nNode 0.10.36 - docker pull nodesource\/precise:0.10.36\nNode 0.10.37 - docker pull nodesource\/precise:0.10.37\nNode 0.10.38 - docker pull nodesource\/precise:0.10.38\nNode 0.10.39 - docker pull nodesource\/precise:0.10.39\nNode 0.10.40 - docker pull nodesource\/precise:0.10.40\nNode 0.10.41 - docker pull nodesource\/precise:0.10.41\nNode 0.10.42 - docker pull nodesource\/precise:0.10.42\nNode 0.10.43 - docker pull nodesource\/precise:0.10.43\nNode 0.10.44 - docker pull nodesource\/precise:0.10.44\nNode 0.10.45 - docker pull nodesource\/precise:0.10.45\nNode 0.10.46 - docker pull nodesource\/precise:0.10.46\nNode 0.12.0 - docker pull nodesource\/precise:0.12.0\nNode 0.12.10 - docker pull nodesource\/precise:0.12.10\nNode 0.12.11 - docker pull nodesource\/precise:0.12.11\nNode 0.12.12 - docker pull nodesource\/precise:0.12.12\nNode 0.12.13 - docker pull nodesource\/precise:0.12.13\nNode 0.12.14 - docker pull nodesource\/precise:0.12.14\nNode 0.12.15 - docker pull nodesource\/precise:0.12.15\nNode 0.12.2 - docker pull nodesource\/precise:0.12.2\nNode 0.12.3 - docker pull nodesource\/precise:0.12.3\nNode 0.12.4 - docker pull nodesource\/precise:0.12.4\nNode 0.12.5 - docker pull nodesource\/precise:0.12.5\nNode 0.12.6 - docker pull nodesource\/precise:0.12.6\nNode 0.12.7 - docker pull nodesource\/precise:0.12.7\nNode 0.12.8 - docker pull nodesource\/precise:0.12.8\nNode 0.12.9 - docker pull nodesource\/precise:0.12.9\nNode 4.1.1 - docker pull nodesource\/precise:4.1.1\nNode 4.1.2 - docker pull nodesource\/precise:4.1.2\nNode 4.2.0 - docker pull nodesource\/precise:4.2.0\nNode 4.2.1 - docker pull nodesource\/precise:4.2.1\nNode 4.2.2 - docker pull nodesource\/precise:4.2.2\nNode 4.2.3 - docker pull nodesource\/precise:4.2.3\nNode 4.2.4 - docker pull nodesource\/precise:4.2.4\nNode 4.2.5 - docker pull nodesource\/precise:4.2.5\nNode 4.2.6 - docker pull nodesource\/precise:4.2.6\nNode 4.3.0 - docker pull nodesource\/precise:4.3.0\nNode 4.3.1 - docker pull nodesource\/precise:4.3.1\nNode 4.3.2 - docker pull nodesource\/precise:4.3.2\nNode 4.4.0 - docker pull nodesource\/precise:4.4.0\nNode 4.4.1 - docker pull nodesource\/precise:4.4.1\nNode 4.4.2 - docker pull nodesource\/precise:4.4.2\nNode 4.4.3 - docker pull nodesource\/precise:4.4.3\nNode 4.4.4 - docker pull nodesource\/precise:4.4.4\nNode 4.4.5 - docker pull nodesource\/precise:4.4.5\nNode 4.4.6 - docker pull nodesource\/precise:4.4.6\nNode 4.4.7 - docker pull nodesource\/precise:4.4.7\nNode 6.0.0 - docker pull nodesource\/precise:6.0.0\nNode 6.1.0 - docker pull nodesource\/precise:6.1.0\nNode 6.2.0 - docker pull nodesource\/precise:6.2.0\nNode 6.2.1 - docker pull nodesource\/precise:6.2.1\nNode 6.2.2 - docker pull nodesource\/precise:6.2.2\nNode 6.3.0 - docker pull nodesource\/precise:6.3.0\nNode 6.3.1 - docker pull nodesource\/precise:6.3.1\n\n\nUbuntu trusty - docker pull nodesource\/trusty\n\nNode 0.10.30 - docker pull nodesource\/trusty:0.10.30\nNode 0.10.31 - docker pull nodesource\/trusty:0.10.31\nNode 0.10.32 - docker pull nodesource\/trusty:0.10.32\nNode 0.10.33 - docker pull nodesource\/trusty:0.10.33\nNode 0.10.34 - docker pull nodesource\/trusty:0.10.34\nNode 0.10.35 - docker pull nodesource\/trusty:0.10.35\nNode 0.10.36 - docker pull nodesource\/trusty:0.10.36\nNode 0.10.37 - docker pull nodesource\/trusty:0.10.37\nNode 0.10.38 - docker pull nodesource\/trusty:0.10.38\nNode 0.10.39 - docker pull nodesource\/trusty:0.10.39\nNode 0.10.40 - docker pull nodesource\/trusty:0.10.40\nNode 0.10.41 - docker pull nodesource\/trusty:0.10.41\nNode 0.10.42 - docker pull nodesource\/trusty:0.10.42\nNode 0.10.43 - docker pull nodesource\/trusty:0.10.43\nNode 0.10.44 - docker pull nodesource\/trusty:0.10.44\nNode 0.10.45 - docker pull nodesource\/trusty:0.10.45\nNode 0.10.46 - docker pull nodesource\/trusty:0.10.46\nNode 0.12.0 - docker pull nodesource\/trusty:0.12.0\nNode 0.12.10 - docker pull nodesource\/trusty:0.12.10\nNode 0.12.11 - docker pull nodesource\/trusty:0.12.11\nNode 0.12.12 - docker pull nodesource\/trusty:0.12.12\nNode 0.12.13 - docker pull nodesource\/trusty:0.12.13\nNode 0.12.14 - docker pull nodesource\/trusty:0.12.14\nNode 0.12.15 - docker pull nodesource\/trusty:0.12.15\nNode 0.12.2 - docker pull nodesource\/trusty:0.12.2\nNode 0.12.3 - docker pull nodesource\/trusty:0.12.3\nNode 0.12.4 - docker pull nodesource\/trusty:0.12.4\nNode 0.12.5 - docker pull nodesource\/trusty:0.12.5\nNode 0.12.6 - docker pull nodesource\/trusty:0.12.6\nNode 0.12.7 - docker pull nodesource\/trusty:0.12.7\nNode 0.12.8 - docker pull nodesource\/trusty:0.12.8\nNode 0.12.9 - docker pull nodesource\/trusty:0.12.9\nNode 4.0.0 - docker pull nodesource\/trusty:4.0.0\nNode 4.1.0 - docker pull nodesource\/trusty:4.1.0\nNode 4.1.1 - docker pull nodesource\/trusty:4.1.1\nNode 4.1.2 - docker pull nodesource\/trusty:4.1.2\nNode 4.2.0 - docker pull nodesource\/trusty:4.2.0\nNode 4.2.1 - docker pull nodesource\/trusty:4.2.1\nNode 4.2.2 - docker pull nodesource\/trusty:4.2.2\nNode 4.2.3 - docker pull nodesource\/trusty:4.2.3\nNode 4.2.4 - docker pull nodesource\/trusty:4.2.4\nNode 4.2.5 - docker pull nodesource\/trusty:4.2.5\nNode 4.2.6 - docker pull nodesource\/trusty:4.2.6\nNode 4.3.0 - docker pull nodesource\/trusty:4.3.0\nNode 4.3.1 - docker pull nodesource\/trusty:4.3.1\nNode 4.3.2 - docker pull nodesource\/trusty:4.3.2\nNode 4.4.0 - docker pull nodesource\/trusty:4.4.0\nNode 4.4.1 - docker pull nodesource\/trusty:4.4.1\nNode 4.4.2 - docker pull nodesource\/trusty:4.4.2\nNode 4.4.3 - docker pull nodesource\/trusty:4.4.3\nNode 4.4.4 - docker pull nodesource\/trusty:4.4.4\nNode 4.4.5 - docker pull nodesource\/trusty:4.4.5\nNode 4.4.6 - docker pull nodesource\/trusty:4.4.6\nNode 4.4.7 - docker pull nodesource\/trusty:4.4.7\nNode 6.0.0 - docker pull nodesource\/trusty:6.0.0\nNode 6.1.0 - docker pull nodesource\/trusty:6.1.0\nNode 6.2.0 - docker pull nodesource\/trusty:6.2.0\nNode 6.2.1 - docker pull nodesource\/trusty:6.2.1\nNode 6.2.2 - docker pull nodesource\/trusty:6.2.2\nNode 6.3.0 - docker pull nodesource\/trusty:6.3.0\nNode 6.3.1 - docker pull nodesource\/trusty:6.3.1\n\n\nUbuntu vivid - docker pull nodesource\/vivid\n\nNode 0.10.38 - docker pull nodesource\/vivid:0.10.38\nNode 0.10.39 - docker pull nodesource\/vivid:0.10.39\nNode 0.10.40 - docker pull nodesource\/vivid:0.10.40\nNode 0.10.41 - docker pull nodesource\/vivid:0.10.41\nNode 0.10.42 - docker pull nodesource\/vivid:0.10.42\nNode 0.10.43 - docker pull nodesource\/vivid:0.10.43\nNode 0.10.44 - docker pull nodesource\/vivid:0.10.44\nNode 0.12.10 - docker pull nodesource\/vivid:0.12.10\nNode 0.12.11 - docker pull nodesource\/vivid:0.12.11\nNode 0.12.12 - docker pull nodesource\/vivid:0.12.12\nNode 0.12.13 - docker pull nodesource\/vivid:0.12.13\nNode 0.12.2 - docker pull nodesource\/vivid:0.12.2\nNode 0.12.3 - docker pull nodesource\/vivid:0.12.3\nNode 0.12.4 - docker pull nodesource\/vivid:0.12.4\nNode 0.12.5 - docker pull nodesource\/vivid:0.12.5\nNode 0.12.6 - docker pull nodesource\/vivid:0.12.6\nNode 0.12.7 - docker pull nodesource\/vivid:0.12.7\nNode 0.12.8 - docker pull nodesource\/vivid:0.12.8\nNode 0.12.9 - docker pull nodesource\/vivid:0.12.9\nNode 4.0.0 - docker pull nodesource\/vivid:4.0.0\nNode 4.1.0 - docker pull nodesource\/vivid:4.1.0\nNode 4.1.1 - docker pull nodesource\/vivid:4.1.1\nNode 4.1.2 - docker pull nodesource\/vivid:4.1.2\nNode 4.2.0 - docker pull nodesource\/vivid:4.2.0\nNode 4.2.1 - docker pull nodesource\/vivid:4.2.1\nNode 4.2.2 - docker pull nodesource\/vivid:4.2.2\nNode 4.2.3 - docker pull nodesource\/vivid:4.2.3\nNode 4.2.4 - docker pull nodesource\/vivid:4.2.4\nNode 4.2.5 - docker pull nodesource\/vivid:4.2.5\nNode 4.2.6 - docker pull nodesource\/vivid:4.2.6\nNode 4.3.0 - docker pull nodesource\/vivid:4.3.0\nNode 4.3.1 - docker pull nodesource\/vivid:4.3.1\nNode 4.3.2 - docker pull nodesource\/vivid:4.3.2\nNode 4.4.0 - docker pull nodesource\/vivid:4.4.0\nNode 4.4.1 - docker pull nodesource\/vivid:4.4.1\nNode 4.4.2 - docker pull nodesource\/vivid:4.4.2\n\n\nUbuntu wily - docker pull nodesource\/wily\n\nNode 0.10.44 - docker pull nodesource\/wily:0.10.44\nNode 0.10.45 - docker pull nodesource\/wily:0.10.45\nNode 0.10.46 - docker pull nodesource\/wily:0.10.46\nNode 0.12.10 - docker pull nodesource\/wily:0.12.10\nNode 0.12.11 - docker pull nodesource\/wily:0.12.11\nNode 0.12.12 - docker pull nodesource\/wily:0.12.12\nNode 0.12.13 - docker pull nodesource\/wily:0.12.13\nNode 0.12.14 - docker pull nodesource\/wily:0.12.14\nNode 0.12.15 - docker pull nodesource\/wily:0.12.15\nNode 0.12.8 - docker pull nodesource\/wily:0.12.8\nNode 0.12.9 - docker pull nodesource\/wily:0.12.9\nNode 4.2.1 - docker pull nodesource\/wily:4.2.1\nNode 4.2.2 - docker pull nodesource\/wily:4.2.2\nNode 4.2.3 - docker pull nodesource\/wily:4.2.3\nNode 4.2.4 - docker pull nodesource\/wily:4.2.4\nNode 4.2.5 - docker pull nodesource\/wily:4.2.5\nNode 4.2.6 - docker pull nodesource\/wily:4.2.6\nNode 4.3.0 - docker pull nodesource\/wily:4.3.0\nNode 4.3.1 - docker pull nodesource\/wily:4.3.1\nNode 4.3.2 - docker pull nodesource\/wily:4.3.2\nNode 4.4.0 - docker pull nodesource\/wily:4.4.0\nNode 4.4.1 - docker pull nodesource\/wily:4.4.1\nNode 4.4.2 - docker pull nodesource\/wily:4.4.2\nNode 4.4.3 - docker pull nodesource\/wily:4.4.3\nNode 4.4.4 - docker pull nodesource\/wily:4.4.4\nNode 4.4.5 - docker pull nodesource\/wily:4.4.5\nNode 4.4.6 - docker pull nodesource\/wily:4.4.6\nNode 4.4.7 - docker pull nodesource\/wily:4.4.7\nNode 6.0.0 - docker pull nodesource\/wily:6.0.0\nNode 6.1.0 - docker pull nodesource\/wily:6.1.0\nNode 6.2.0 - docker pull nodesource\/wily:6.2.0\nNode 6.2.1 - docker pull nodesource\/wily:6.2.1\nNode 6.2.2 - docker pull nodesource\/wily:6.2.2\nNode 6.3.0 - docker pull nodesource\/wily:6.3.0\nNode 6.3.1 - docker pull nodesource\/wily:6.3.1\n\n\nUbuntu xenial - docker pull nodesource\/xenial\n\nNode 0.10.44 - docker pull nodesource\/xenial:0.10.44\nNode 0.10.45 - docker pull nodesource\/xenial:0.10.45\nNode 0.10.46 - docker pull nodesource\/xenial:0.10.46\nNode 0.12.13 - docker pull nodesource\/xenial:0.12.13\nNode 0.12.14 - docker pull nodesource\/xenial:0.12.14\nNode 0.12.15 - docker pull nodesource\/xenial:0.12.15\nNode 4.4.2 - docker pull nodesource\/xenial:4.4.2\nNode 4.4.3 - docker pull nodesource\/xenial:4.4.3\nNode 4.4.4 - docker pull nodesource\/xenial:4.4.4\nNode 4.4.5 - docker pull nodesource\/xenial:4.4.5\nNode 4.4.6 - docker pull nodesource\/xenial:4.4.6\nNode 4.4.7 - docker pull nodesource\/xenial:4.4.7\nNode 6.0.0 - docker pull nodesource\/xenial:6.0.0\nNode 6.1.0 - docker pull nodesource\/xenial:6.1.0\nNode 6.2.0 - docker pull nodesource\/xenial:6.2.0\nNode 6.2.1 - docker pull nodesource\/xenial:6.2.1\nNode 6.2.2 - docker pull nodesource\/xenial:6.2.2\nNode 6.3.0 - docker pull nodesource\/xenial:6.3.0\nNode 6.3.1 - docker pull nodesource\/xenial:6.3.1\n\n\n\nFedora-based images\n\nFedora 20 - docker pull nodesource\/fedora20\n\nNode 0.10.31 - docker pull nodesource\/fedora20:0.10.31\nNode 0.10.32 - docker pull nodesource\/fedora20:0.10.32\nNode 0.10.33 - docker pull nodesource\/fedora20:0.10.33\nNode 0.10.34 - docker pull nodesource\/fedora20:0.10.34\nNode 0.10.35 - docker pull nodesource\/fedora20:0.10.35\nNode 0.10.36 - docker pull nodesource\/fedora20:0.10.36\nNode 0.10.38 - docker pull nodesource\/fedora20:0.10.38\nNode 0.10.39 - docker pull nodesource\/fedora20:0.10.39\nNode 0.10.40 - docker pull nodesource\/fedora20:0.10.40\nNode 0.12.1 - docker pull nodesource\/fedora20:0.12.1\nNode 0.12.2 - docker pull nodesource\/fedora20:0.12.2\nNode 0.12.3 - docker pull nodesource\/fedora20:0.12.3\nNode 0.12.5 - docker pull nodesource\/fedora20:0.12.5\nNode 0.12.6 - docker pull nodesource\/fedora20:0.12.6\nNode 0.12.7 - docker pull nodesource\/fedora20:0.12.7\nNode 4.0.0 - docker pull nodesource\/fedora20:4.0.0\nNode 4.1.0 - docker pull nodesource\/fedora20:4.1.0\nNode 4.1.1 - docker pull nodesource\/fedora20:4.1.1\nNode 4.1.2 - docker pull nodesource\/fedora20:4.1.2\nNode 4.2.0 - docker pull nodesource\/fedora20:4.2.0\nNode 4.2.1 - docker pull nodesource\/fedora20:4.2.1\n\n\nFedora 21 - docker pull nodesource\/fedora21\n\nNode 0.10.35 - docker pull nodesource\/fedora21:0.10.35\nNode 0.10.36 - docker pull nodesource\/fedora21:0.10.36\nNode 0.10.38 - docker pull nodesource\/fedora21:0.10.38\nNode 0.10.39 - docker pull nodesource\/fedora21:0.10.39\nNode 0.10.40 - docker pull nodesource\/fedora21:0.10.40\nNode 0.10.41 - docker pull nodesource\/fedora21:0.10.41\nNode 0.12.1 - docker pull nodesource\/fedora21:0.12.1\nNode 0.12.2 - docker pull nodesource\/fedora21:0.12.2\nNode 0.12.3 - docker pull nodesource\/fedora21:0.12.3\nNode 0.12.5 - docker pull nodesource\/fedora21:0.12.5\nNode 0.12.6 - docker pull nodesource\/fedora21:0.12.6\nNode 0.12.7 - docker pull nodesource\/fedora21:0.12.7\nNode 0.12.8 - docker pull nodesource\/fedora21:0.12.8\nNode 0.12.9 - docker pull nodesource\/fedora21:0.12.9\nNode 4.0.0 - docker pull nodesource\/fedora21:4.0.0\nNode 4.1.0 - docker pull nodesource\/fedora21:4.1.0\nNode 4.1.1 - docker pull nodesource\/fedora21:4.1.1\nNode 4.1.2 - docker pull nodesource\/fedora21:4.1.2\nNode 4.2.0 - docker pull nodesource\/fedora21:4.2.0\nNode 4.2.1 - docker pull nodesource\/fedora21:4.2.1\nNode 4.2.2 - docker pull nodesource\/fedora21:4.2.2\nNode 4.2.3 - docker pull nodesource\/fedora21:4.2.3\nNode 4.2.4 - docker pull nodesource\/fedora21:4.2.4\nNode 4.2.5 - docker pull nodesource\/fedora21:4.2.5\nNode 4.2.6 - docker pull nodesource\/fedora21:4.2.6\nNode 5.0.0 - docker pull nodesource\/fedora21:5.0.0\nNode 5.1.0 - docker pull nodesource\/fedora21:5.1.0\nNode 5.1.1 - docker pull nodesource\/fedora21:5.1.1\nNode 5.2.0 - docker pull nodesource\/fedora21:5.2.0\nNode 5.3.0 - docker pull nodesource\/fedora21:5.3.0\nNode 5.4.0 - docker pull nodesource\/fedora21:5.4.0\nNode 5.4.1 - docker pull nodesource\/fedora21:5.4.1\nNode 5.5.0 - docker pull nodesource\/fedora21:5.5.0\n\n\nFedora 22 - docker pull nodesource\/fedora22\n\nNode 0.10.40 - docker pull nodesource\/fedora22:0.10.40\nNode 0.10.41 - docker pull nodesource\/fedora22:0.10.41\nNode 0.10.42 - docker pull nodesource\/fedora22:0.10.42\nNode 0.10.43 - docker pull nodesource\/fedora22:0.10.43\nNode 0.10.44 - docker pull nodesource\/fedora22:0.10.44\nNode 0.10.45 - docker pull nodesource\/fedora22:0.10.45\nNode 0.10.46 - docker pull nodesource\/fedora22:0.10.46\nNode 0.12.10 - docker pull nodesource\/fedora22:0.12.10\nNode 0.12.11 - docker pull nodesource\/fedora22:0.12.11\nNode 0.12.12 - docker pull nodesource\/fedora22:0.12.12\nNode 0.12.13 - docker pull nodesource\/fedora22:0.12.13\nNode 0.12.14 - docker pull nodesource\/fedora22:0.12.14\nNode 0.12.15 - docker pull nodesource\/fedora22:0.12.15\nNode 0.12.7 - docker pull nodesource\/fedora22:0.12.7\nNode 0.12.8 - docker pull nodesource\/fedora22:0.12.8\nNode 0.12.9 - docker pull nodesource\/fedora22:0.12.9\nNode 4.0.0 - docker pull nodesource\/fedora22:4.0.0\nNode 4.1.0 - docker pull nodesource\/fedora22:4.1.0\nNode 4.1.1 - docker pull nodesource\/fedora22:4.1.1\nNode 4.1.2 - docker pull nodesource\/fedora22:4.1.2\nNode 4.2.0 - docker pull nodesource\/fedora22:4.2.0\nNode 4.2.1 - docker pull nodesource\/fedora22:4.2.1\nNode 4.2.2 - docker pull nodesource\/fedora22:4.2.2\nNode 4.2.3 - docker pull nodesource\/fedora22:4.2.3\nNode 4.2.4 - docker pull nodesource\/fedora22:4.2.4\nNode 4.2.5 - docker pull nodesource\/fedora22:4.2.5\nNode 4.2.6 - docker pull nodesource\/fedora22:4.2.6\nNode 4.3.0 - docker pull nodesource\/fedora22:4.3.0\nNode 4.3.1 - docker pull nodesource\/fedora22:4.3.1\nNode 4.3.2 - docker pull nodesource\/fedora22:4.3.2\nNode 4.4.0 - docker pull nodesource\/fedora22:4.4.0\nNode 4.4.1 - docker pull nodesource\/fedora22:4.4.1\nNode 4.4.2 - docker pull nodesource\/fedora22:4.4.2\nNode 4.4.3 - docker pull nodesource\/fedora22:4.4.3\nNode 4.4.4 - docker pull nodesource\/fedora22:4.4.4\nNode 4.4.5 - docker pull nodesource\/fedora22:4.4.5\nNode 4.4.6 - docker pull nodesource\/fedora22:4.4.6\nNode 4.4.7 - docker pull nodesource\/fedora22:4.4.7\nNode 5.0.0 - docker pull nodesource\/fedora22:5.0.0\nNode 5.1.0 - docker pull nodesource\/fedora22:5.1.0\nNode 5.1.1 - docker pull nodesource\/fedora22:5.1.1\nNode 5.10.0 - docker pull nodesource\/fedora22:5.10.0\nNode 5.10.1 - docker pull nodesource\/fedora22:5.10.1\nNode 5.11.0 - docker pull nodesource\/fedora22:5.11.0\nNode 5.11.1 - docker pull nodesource\/fedora22:5.11.1\nNode 5.12.0 - docker pull nodesource\/fedora22:5.12.0\nNode 5.2.0 - docker pull nodesource\/fedora22:5.2.0\nNode 5.3.0 - docker pull nodesource\/fedora22:5.3.0\nNode 5.4.0 - docker pull nodesource\/fedora22:5.4.0\nNode 5.4.1 - docker pull nodesource\/fedora22:5.4.1\nNode 5.5.0 - docker pull nodesource\/fedora22:5.5.0\nNode 5.6.0 - docker pull nodesource\/fedora22:5.6.0\nNode 5.7.0 - docker pull nodesource\/fedora22:5.7.0\nNode 5.7.1 - docker pull nodesource\/fedora22:5.7.1\nNode 5.8.0 - docker pull nodesource\/fedora22:5.8.0\nNode 5.9.0 - docker pull nodesource\/fedora22:5.9.0\nNode 5.9.1 - docker pull nodesource\/fedora22:5.9.1\nNode 6.0.0 - docker pull nodesource\/fedora22:6.0.0\nNode 6.1.0 - docker pull nodesource\/fedora22:6.1.0\nNode 6.2.0 - docker pull nodesource\/fedora22:6.2.0\nNode 6.2.1 - docker pull nodesource\/fedora22:6.2.1\nNode 6.2.2 - docker pull nodesource\/fedora22:6.2.2\nNode 6.3.0 - docker pull nodesource\/fedora22:6.3.0\n\n\nFedora 23 - docker pull nodesource\/fedora23\n\nNode 0.10.44 - docker pull nodesource\/fedora23:0.10.44\nNode 0.10.45 - docker pull nodesource\/fedora23:0.10.45\nNode 0.10.46 - docker pull nodesource\/fedora23:0.10.46\nNode 0.10.43 - docker pull nodesource\/fedora23:0.10.43\nNode 0.12.11 - docker pull nodesource\/fedora23:0.12.11\nNode 0.12.12 - docker pull nodesource\/fedora23:0.12.12\nNode 0.12.13 - docker pull nodesource\/fedora23:0.12.13\nNode 0.12.14 - docker pull nodesource\/fedora23:0.12.14\nNode 0.12.15 - docker pull nodesource\/fedora23:0.12.15\nNode 4.2.1 - docker pull nodesource\/fedora23:4.2.1\nNode 4.2.2 - docker pull nodesource\/fedora23:4.2.2\nNode 4.2.3 - docker pull nodesource\/fedora23:4.2.3\nNode 4.2.4 - docker pull nodesource\/fedora23:4.2.4\nNode 4.2.5 - docker pull nodesource\/fedora23:4.2.5\nNode 4.2.6 - docker pull nodesource\/fedora23:4.2.6\nNode 4.3.0 - docker pull nodesource\/fedora23:4.3.0\nNode 4.3.1 - docker pull nodesource\/fedora23:4.3.1\nNode 4.3.2 - docker pull nodesource\/fedora23:4.3.2\nNode 4.4.0 - docker pull nodesource\/fedora23:4.4.0\nNode 4.4.1 - docker pull nodesource\/fedora23:4.4.1\nNode 4.4.2 - docker pull nodesource\/fedora23:4.4.2\nNode 4.4.3 - docker pull nodesource\/fedora23:4.4.3\nNode 4.4.4 - docker pull nodesource\/fedora23:4.4.4\nNode 4.4.5 - docker pull nodesource\/fedora23:4.4.5\nNode 4.4.6 - docker pull nodesource\/fedora23:4.4.6\nNode 4.4.7 - docker pull nodesource\/fedora23:4.4.7\nNode 5.0.0 - docker pull nodesource\/fedora23:5.0.0\nNode 5.1.0 - docker pull nodesource\/fedora23:5.1.0\nNode 5.1.1 - docker pull nodesource\/fedora23:5.1.1\nNode 5.10.0 - docker pull nodesource\/fedora23:5.10.0\nNode 5.10.1 - docker pull nodesource\/fedora23:5.10.1\nNode 5.11.0 - docker pull nodesource\/fedora23:5.11.0\nNode 5.11.1 - docker pull nodesource\/fedora23:5.11.1\nNode 5.12.0 - docker pull nodesource\/fedora23:5.12.0\nNode 5.2.0 - docker pull nodesource\/fedora23:5.2.0\nNode 5.3.0 - docker pull nodesource\/fedora23:5.3.0\nNode 5.4.0 - docker pull nodesource\/fedora23:5.4.0\nNode 5.4.1 - docker pull nodesource\/fedora23:5.4.1\nNode 5.5.0 - docker pull nodesource\/fedora23:5.5.0\nNode 5.6.0 - docker pull nodesource\/fedora23:5.6.0\nNode 5.7.0 - docker pull nodesource\/fedora23:5.7.0\nNode 5.7.1 - docker pull nodesource\/fedora23:5.7.1\nNode 5.8.0 - docker pull nodesource\/fedora23:5.8.0\nNode 5.9.0 - docker pull nodesource\/fedora23:5.9.0\nNode 5.9.1 - docker pull nodesource\/fedora23:5.9.1\nNode 6.0.0 - docker pull nodesource\/fedora23:6.0.0\nNode 6.1.0 - docker pull nodesource\/fedora23:6.1.0\nNode 6.2.0 - docker pull nodesource\/fedora23:6.2.0\nNode 6.2.1 - docker pull nodesource\/fedora23:6.2.1\nNode 6.2.2 - docker pull nodesource\/fedora23:6.2.2\nNode 6.3.0 - docker pull nodesource\/fedora23:6.3.0\nNode 6.3.1 - docker pull nodesource\/fedora23:6.3.1\n\n\nFedora 24 - docker pull nodesource\/fedora24\n\nNode 0.12.15 - docker pull nodesource\/fedora24:0.12.15\nNode 4.4.6 - docker pull nodesource\/fedora24:4.4.6\nNode 4.4.7 - docker pull nodesource\/fedora24:4.4.7\nNode 5.12.0 - docker pull nodesource\/fedora24:5.12.0\nNode 6.2.2 - docker pull nodesource\/fedora24:6.2.2\nNode 6.3.0 - docker pull nodesource\/fedora24:6.3.0\nNode 6.3.1 - docker pull nodesource\/fedora24:6.3.1\n\n\n\nCentos-based images\n\nCentos 5 - docker pull nodesource\/centos5\n\nNode 0.10.31 - docker pull nodesource\/centos5:0.10.31\nNode 0.10.32 - docker pull nodesource\/centos5:0.10.32\nNode 0.10.33 - docker pull nodesource\/centos5:0.10.33\nNode 0.10.34 - docker pull nodesource\/centos5:0.10.34\nNode 0.10.35 - docker pull nodesource\/centos5:0.10.35\nNode 0.10.36 - docker pull nodesource\/centos5:0.10.36\nNode 0.10.38 - docker pull nodesource\/centos5:0.10.38\nNode 0.10.39 - docker pull nodesource\/centos5:0.10.39\nNode 0.10.40 - docker pull nodesource\/centos5:0.10.40\nNode 0.10.41 - docker pull nodesource\/centos5:0.10.41\nNode 0.10.42 - docker pull nodesource\/centos5:0.10.42\nNode 0.10.43 - docker pull nodesource\/centos5:0.10.43\nNode 0.10.44 - docker pull nodesource\/centos5:0.10.44\nNode 0.10.45 - docker pull nodesource\/centos5:0.10.45\nNode 0.10.46 - docker pull nodesource\/centos5:0.10.46\n\n\nCentos 6 - docker pull nodesource\/centos6\n\nNode 0.10.31 - docker pull nodesource\/centos6:0.10.31\nNode 0.10.32 - docker pull nodesource\/centos6:0.10.32\nNode 0.10.33 - docker pull nodesource\/centos6:0.10.33\nNode 0.10.34 - docker pull nodesource\/centos6:0.10.34\nNode 0.10.35 - docker pull nodesource\/centos6:0.10.35\nNode 0.10.36 - docker pull nodesource\/centos6:0.10.36\nNode 0.10.38 - docker pull nodesource\/centos6:0.10.38\nNode 0.10.39 - docker pull nodesource\/centos6:0.10.39\nNode 0.10.40 - docker pull nodesource\/centos6:0.10.40\nNode 0.10.41 - docker pull nodesource\/centos6:0.10.41\nNode 0.10.42 - docker pull nodesource\/centos6:0.10.42\nNode 0.10.43 - docker pull nodesource\/centos6:0.10.43\nNode 0.10.44 - docker pull nodesource\/centos6:0.10.44\nNode 0.10.45 - docker pull nodesource\/centos6:0.10.45\nNode 0.10.46 - docker pull nodesource\/centos6:0.10.46\nNode 0.12.1 - docker pull nodesource\/centos6:0.12.1\nNode 0.12.10 - docker pull nodesource\/centos6:0.12.10\nNode 0.12.11 - docker pull nodesource\/centos6:0.12.11\nNode 0.12.12 - docker pull nodesource\/centos6:0.12.12\nNode 0.12.13 - docker pull nodesource\/centos6:0.12.13\nNode 0.12.14 - docker pull nodesource\/centos6:0.12.14\nNode 0.12.15 - docker pull nodesource\/centos6:0.12.15\nNode 0.12.2 - docker pull nodesource\/centos6:0.12.2\nNode 0.12.3 - docker pull nodesource\/centos6:0.12.3\nNode 0.12.5 - docker pull nodesource\/centos6:0.12.5\nNode 0.12.6 - docker pull nodesource\/centos6:0.12.6\nNode 0.12.7 - docker pull nodesource\/centos6:0.12.7\nNode 0.12.8 - docker pull nodesource\/centos6:0.12.8\nNode 0.12.9 - docker pull nodesource\/centos6:0.12.9\nNode 4.2.6 - docker pull nodesource\/centos6:4.2.6\nNode 4.3.0 - docker pull nodesource\/centos6:4.3.0\nNode 4.3.1 - docker pull nodesource\/centos6:4.3.1\nNode 4.3.2 - docker pull nodesource\/centos6:4.3.2\nNode 4.4.0 - docker pull nodesource\/centos6:4.4.0\nNode 4.4.1 - docker pull nodesource\/centos6:4.4.1\nNode 4.4.2 - docker pull nodesource\/centos6:4.4.2\nNode 4.4.3 - docker pull nodesource\/centos6:4.4.3\nNode 4.4.4 - docker pull nodesource\/centos6:4.4.4\nNode 4.4.5 - docker pull nodesource\/centos6:4.4.5\nNode 4.4.6 - docker pull nodesource\/centos6:4.4.6\nNode 4.4.7 - docker pull nodesource\/centos6:4.4.7\nNode 5.10.0 - docker pull nodesource\/centos6:5.10.0\nNode 5.10.1 - docker pull nodesource\/centos6:5.10.1\nNode 5.11.0 - docker pull nodesource\/centos6:5.11.0\nNode 5.11.1 - docker pull nodesource\/centos6:5.11.1\nNode 5.12.0 - docker pull nodesource\/centos6:5.12.0\nNode 5.5.0 - docker pull nodesource\/centos6:5.5.0\nNode 5.6.0 - docker pull nodesource\/centos6:5.6.0\nNode 5.7.0 - docker pull nodesource\/centos6:5.7.0\nNode 5.7.1 - docker pull nodesource\/centos6:5.7.1\nNode 5.8.0 - docker pull nodesource\/centos6:5.8.0\nNode 5.9.0 - docker pull nodesource\/centos6:5.9.0\nNode 5.9.1 - docker pull nodesource\/centos6:5.9.1\nNode 6.0.0 - docker pull nodesource\/centos6:6.0.0\nNode 6.1.0 - docker pull nodesource\/centos6:6.1.0\nNode 6.2.0 - docker pull nodesource\/centos6:6.2.0\nNode 6.2.1 - docker pull nodesource\/centos6:6.2.1\nNode 6.3.0 - docker pull nodesource\/centos6:6.3.0\nNode 6.3.1 - docker pull nodesource\/centos6:6.3.1\n\n\nCentos 7 - docker pull nodesource\/centos7\n\nNode 0.10.31 - docker pull nodesource\/centos7:0.10.31\nNode 0.10.32 - docker pull nodesource\/centos7:0.10.32\nNode 0.10.33 - docker pull nodesource\/centos7:0.10.33\nNode 0.10.34 - docker pull nodesource\/centos7:0.10.34\nNode 0.10.35 - docker pull nodesource\/centos7:0.10.35\nNode 0.10.36 - docker pull nodesource\/centos7:0.10.36\nNode 0.10.38 - docker pull nodesource\/centos7:0.10.38\nNode 0.10.39 - docker pull nodesource\/centos7:0.10.39\nNode 0.10.40 - docker pull nodesource\/centos7:0.10.40\nNode 0.10.41 - docker pull nodesource\/centos7:0.10.41\nNode 0.10.42 - docker pull nodesource\/centos7:0.10.42\nNode 0.10.43 - docker pull nodesource\/centos7:0.10.43\nNode 0.10.44 - docker pull nodesource\/centos7:0.10.44\nNode 0.10.45 - docker pull nodesource\/centos7:0.10.45\nNode 0.10.46 - docker pull nodesource\/centos7:0.10.46\nNode 0.12.1 - docker pull nodesource\/centos7:0.12.1\nNode 0.12.10 - docker pull nodesource\/centos7:0.12.10\nNode 0.12.11 - docker pull nodesource\/centos7:0.12.11\nNode 0.12.12 - docker pull nodesource\/centos7:0.12.12\nNode 0.12.13 - docker pull nodesource\/centos7:0.12.13\nNode 0.12.14 - docker pull nodesource\/centos7:0.12.14\nNode 0.12.15 - docker pull nodesource\/centos7:0.12.15\nNode 0.12.2 - docker pull nodesource\/centos7:0.12.2\nNode 0.12.3 - docker pull nodesource\/centos7:0.12.3\nNode 0.12.5 - docker pull nodesource\/centos7:0.12.5\nNode 0.12.6 - docker pull nodesource\/centos7:0.12.6\nNode 0.12.7 - docker pull nodesource\/centos7:0.12.7\nNode 0.12.8 - docker pull nodesource\/centos7:0.12.8\nNode 0.12.9 - docker pull nodesource\/centos7:0.12.9\nNode 4.0.0 - docker pull nodesource\/centos7:4.0.0\nNode 4.1.0 - docker pull nodesource\/centos7:4.1.0\nNode 4.1.1 - docker pull nodesource\/centos7:4.1.1\nNode 4.1.2 - docker pull nodesource\/centos7:4.1.2\nNode 4.2.0 - docker pull nodesource\/centos7:4.2.0\nNode 4.2.1 - docker pull nodesource\/centos7:4.2.1\nNode 4.2.2 - docker pull nodesource\/centos7:4.2.2\nNode 4.2.3 - docker pull nodesource\/centos7:4.2.3\nNode 4.2.4 - docker pull nodesource\/centos7:4.2.4\nNode 4.2.5 - docker pull nodesource\/centos7:4.2.5\nNode 4.2.6 - docker pull nodesource\/centos7:4.2.6\nNode 4.3.0 - docker pull nodesource\/centos7:4.3.0\nNode 4.3.1 - docker pull nodesource\/centos7:4.3.1\nNode 4.3.2 - docker pull nodesource\/centos7:4.3.2\nNode 4.4.0 - docker pull nodesource\/centos7:4.4.0\nNode 4.4.1 - docker pull nodesource\/centos7:4.4.1\nNode 4.4.2 - docker pull nodesource\/centos7:4.4.2\nNode 4.4.3 - docker pull nodesource\/centos7:4.4.3\nNode 4.4.4 - docker pull nodesource\/centos7:4.4.4\nNode 4.4.5 - docker pull nodesource\/centos7:4.4.5\nNode 4.4.6 - docker pull nodesource\/centos7:4.4.6\nNode 4.4.7 - docker pull nodesource\/centos7:4.4.7\nNode 5.0.0 - docker pull nodesource\/centos7:5.0.0\nNode 5.1.0 - docker pull nodesource\/centos7:5.1.0\nNode 5.1.1 - docker pull nodesource\/centos7:5.1.1\nNode 5.10.0 - docker pull nodesource\/centos7:5.10.0\nNode 5.10.1 - docker pull nodesource\/centos7:5.10.1\nNode 5.11.0 - docker pull nodesource\/centos7:5.11.0\nNode 5.11.1 - docker pull nodesource\/centos7:5.11.1\nNode 5.12.0 - docker pull nodesource\/centos7:5.12.0\nNode 5.2.0 - docker pull nodesource\/centos7:5.2.0\nNode 5.3.0 - docker pull nodesource\/centos7:5.3.0\nNode 5.4.0 - docker pull nodesource\/centos7:5.4.0\nNode 5.4.1 - docker pull nodesource\/centos7:5.4.1\nNode 5.5.0 - docker pull nodesource\/centos7:5.5.0\nNode 5.6.0 - docker pull nodesource\/centos7:5.6.0\nNode 5.7.0 - docker pull nodesource\/centos7:5.7.0\nNode 5.7.1 - docker pull nodesource\/centos7:5.7.1\nNode 5.8.0 - docker pull nodesource\/centos7:5.8.0\nNode 5.9.0 - docker pull nodesource\/centos7:5.9.0\nNode 5.9.1 - docker pull nodesource\/centos7:5.9.1\nNode 6.0.0 - docker pull nodesource\/centos7:6.0.0\nNode 6.1.0 - docker pull nodesource\/centos7:6.1.0\nNode 6.2.0 - docker pull nodesource\/centos7:6.2.0\nNode 6.2.1 - docker pull nodesource\/centos7:6.2.1\nNode 6.2.2 - docker pull nodesource\/centos7:6.2.2\nNode 6.3.0 - docker pull nodesource\/centos7:6.3.0\nNode 6.3.1 - docker pull nodesource\/centos7:6.3.1\n\n\n\n","395":"jquery-timing\nA jQuery plugin that provides easy-to-use timing methods to write less and do more.\nExamples and complete reference can be found at http:\/\/creativecouple.github.com\/jquery-timing\nAuthor\nCreativeCouple - Peter Liske\nLicense\nMIT License\nThe MIT License (MIT)\nCopyright (c) 2012 CreativeCouple, Peter Liske\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","396":"DEPRECATION NOTICE!\nThis project is deprecated in favor of chjj\/marked. I never created the parser myself, the module was created in the early days of node as a wrapper for an existing browser based parser Showdown so if you are using this module and have problems with the parsing logic, I can't help you much as I'm not familiar with the inner details.\nPull requests are still welcomed - if you find a bug and fix it, then I'll pull the change in but I won't be fixing the bugs myself. Sorry for that.\nnode-markdown\nnode-markdown is based on Showdown parser and is meant to parse Markdown syntax into HTML code.\nInstallation\nUse npm package manager\nnpm install node-markdown\n\nUsage\nInclude Markdown parser\nvar md = require(\"node-markdown\").Markdown;\n\nParse Markdown syntax into HTML\nvar html = md(\"**markdown** string\");\n\nAllow only default set of HTML tags to be used\nvar html = md(\"**markdown** string\", true);\n\nAllow only specified HTML tags to be used (default set of allowed attributes is used)\nvar html = md(\"**markdown** string\", true, \"p|strong|span\");\n\nAllow specified HTML tags and specified attributes\nvar html = md(\"**markdown** string\", true, \"p|strong|span\", {\n    \"a\":\"href\",        \/\/ 'href' for links\n    \"*\":\"title|style\"  \/\/ 'title' and 'style' for all\n});\n\nComplete example\nvar md_text = \"**bold** *italic* [link](http:\/\/www.neti.ee) `code block`\",\n    md_parser = require(\"node-markdown\").Markdown;\n\n\/\/ simple\nconsole.log(md_parser(md_text));\n\n\/\/ limit HTML tags and attributes\nconsole.log(md_parser(md_text, true, 'h1|p|span'));\n\n\/\/ limit HTML tags and keep attributes for allowed tags\nvar allowedTags = 'a|img';\n    allowedAttributes = {\n        'a':'href|style',\n        'img': 'src',\n        '*': 'title'\n    }\nconsole.log(md_parser(md_text, true, allowedTags, allowedAttributes));\n\n","397":"jCarouselLite - Original Version\nThis is the original version of jCarouselLite.\nCurrent stable version is 1.1.\nFor detailed documentation visit the Project Page\n\nProject Page\nInstallation Instruction\nDefault & Custom Styling\nDemos\nDocumentation\nChange Log\n\n","398":"Parse for AngularJS\nThis is pre-alpha\/actively developed. There are no guarantees of\nstability, but you are welcome to play around and submit issues\nangular-parse is an AngularJS module for\ninteracting with the Parse REST\nAPI. It does not utlize the Parse\nJavaScript API but instead is built\nfrom (mostly) scratch. The reason is the existing Parse JavaScript API\nis not ideal for AngularJS applications.\nWhy Angular-Parse\nThere are a few things that are not ideal about the existing Parse\nJavaScript API in AngularJS. The existing API is modeled after Backbone\nModels and the main problem is setters\nare used instead of object properties. instance.set('property', 'value')\ndoesn't really fit well with things like ng-model\nInstead, angular-parse is based loosely on Spine\nModels where properties directly\ndefined on the object are used. To facilitate this, when defining a\nmodel, it is \"configured\" by supplying the class name (as defined in\nParse) as well as which properties are part of that class.\nAngular-parse also uses promises for any methods making network calls.\nGetting started\nInclude the JavaScript file\n<!-- Include AngularJS -->\n<script src=\"path\/to\/angular-parse.js\"><\/script>\nMake sure to add \"Parse\" as a dependency of your main module\nvar app = angular.module(\"YourApp\", [\"Parse\"])\nAngular-parse also requires you provide the value \"ParseConfig\" as an\nobject with the following format\napp.config(function (ParseProvider) {\n  ParseProvider.initialize(\"PARSE_APPLICATION_ID\", \"PARSE_REST_API_KEY\");\n});\nDefining Models\nYou can define models by extending Parse.Model. You must call configure\non the class and pass it the Parse class name, and the name of any\nattributes of that class\nUsing CoffeeScript:\napp.factory 'Car', (Parse) ->\n  class Car extends Parse.model\n    @configure \"Car\", \"make\", \"model\", \"year\"\n\n    @customClassMethod: (arg) ->\n      # add custom class methods like this\n\n    customInstanceMethod: (arg) ->\n      # add custom instance methods like this\nUsing JavaScript:\n\/\/ Not implemented yet, sorry\nUsing Models\nA model acts much the same as a normal JavaScript object with a\nconstructor\nCreating a new instance\nYou can create a new instance by using new. Any attributes passed in\nwill be set on the instance. This does not save it to parse, that must\nbe done with .save(). The save method returns a promise, which is\nfulfilled with the instance itself.\nvar car = new Car({\n  make: \"Scion\",\n  model: \"xB\",\n  year: 2008\n});\n\ncar.isNew() === true;\ncar.objectId == null;\n\ncar.save().then(function (_car) {\n  _car === car;\n  car.isNew() === false;\n  car.objectId === \"...aParseId\";\n  car.createdAt === \"...aDateString\";\n  car.updatedAt === \"...aDateString\";\n}\nIf the object has an objectId, it will be updated properly, and will not\ncreate a new instance. save() can be used either for new or existing\nrecords.\nGetting an instance By Id\nThe find method on your model class takes an objectId, and returns a\npromise that will be fulfilled with your instance if it exists.\nCar.find(\"someObjectId\").then(function (car) {\n  car.objectId === \"someObjectId\";\n})\nDestroying an instance\nThe destroy method on an instance will destroy it set destroyed to true\nand set the item's objectId to null\nCar.find(\"someObjectId\").then(function (car) {\n  car.objectId === \"someObjectId\";\n\n  car.destroy().then(function (_car) {\n    car === _car;\n    car.destroyed === true;\n    car.isNew() === true;\n    car.objectId === null;\n  })\n})\nDefining a custom user class\nA simple User class is provided to you. However, you can subclass it:\nangular.module('Parse').factory 'ParseCustomUser', (ParseDefaultUser) ->\n      class CustomUser extends ParseDefaultUser\n        @configure 'users', 'username', 'password', 'property'\nIn this manner, all User instances returned by the Parse methods\nwill be of your custom class.\nContributing\nPull requests and issues are welcome.\n","399":"Screw.Unit is a Behavior-Driven Testing Framework for Javascript. It features nested describes. Its goals are to provide:\n\na DSL for elegant, readable, organized specs;\nan interactive runner that can execute focused specs and describes;\nand brief, extensible source-code.\n\nWhat it is\n\nThe testing language is closure-based. Consider,\ndescribe(\"Matchers\", function() {\n  it(\"invokes the provided matcher on a call to expect\", function() {\n    expect(true).to(equal, true);\n    expect(true).to_not(equal, false);\n  });\n});\n\nA key feature of Screw.Unit are nested describes and the cascading before (and after) behavior that entails:\ndescribe(\"a nested describe\", function() {\n  var invocations = [];\n  \n  before(function() {\n    invocations.push(\"before\");\n  });\n\n  describe(\"a doubly nested describe\", function() {\n    before(function() {\n      invocations.push('inner before');\n    });\n\n    it(\"runs befores in all ancestors prior to an it\", function() {\n      expect(invocations).to(equal, [\"before\", \"inner before\"]);\n    });\n  });\n});\n\nThe Runner\nThe Screw.Unit runner is pretty fancy, supporting focused describes and focused its:\n\nClick on a describe or it to run just those tests.\nGlobal Befores and Afters\nA global before is a before block run before all tests in a test suite, regardless of their nesting. This is often useful to reset global variables, or blank-out DOM nodes before each test is run. Put this at the top of the your suite file or in your spec helper.\nScrew.Unit(function(c) { with(c) {\n  before(function() { ... });\n}});\n\nNote that you can have any number of Screw.Unit(...) blocks in one file. Thus, you can have multiple global befores and afters.\nCustom Matchers\nA custom matcher is a custom assertion specifically tailored to your application. These are helpful in increasing the readability and declarativity of your tests. To create a custom matcher, fill in the blanks for this code:\nScrew.Matchers[\"be_even\"] = {\n  match: function(expected, actual) {\n    return actual % 2 == 0;\n  },\n  failure_message: function(expected, actual, not) {\n    return 'expected ' + $.print(actual) + (not ? ' not' : '') + ' to be even';\n  }\n}\n\nYou can invoke this matcher as follows: expect(2).to(be_even).\nThe Anatomy of Test Infrastructure\nTypical test infrastructure spans multiple files:\n\nA suite.html file that has the necessary html, script tags, and link tags, to include your source code as well as the test infrastructure.\nA spec_helper.js file with global before and after blocks.\nA set of custom matchers.\nYour individual tests.\n\nThe file structure will typically look like:\nspec\/\n  suite.html\n  spec_helper.js\n  matchers\/\n    a_matcher.js\n    another_matcher.js\n  models\/\n    a_spec.js\n    another_spec.js\n  views\/\n    yet_another_spec.js\n\nThe models and views directories are here only for comparison. As a general rule, mirror the file structure of your source code in your spec directory. For example, if you have an MVC application and you organize your source code into models, views, and controllers directories, have parallel directories in your spec\/ directory, with tests for your models, views, and controllers in their respective directories.\nWriting Good Tests\nA great test maximizes these features:\n\nit provides documentation, explaining the intended functioning of the system as well as how the source code works;\nit supports ongoing development, as you bit-by-bit write a failing test and make it pass;\nit supports later refactoring and prevents regression as you add other features;\nand it requires little modification as the implementation of the system changes, especially changes to unrelated code.\n\nThis section focuses principally on tests as documentation. To provide documentation, as well as support future modification, a test should be readable and well organized. Here are some recommendations on how to do just that.\nUse Nested Describes to Express Context\nOften, when you test a system (a function, an object), it behaves differently in different contexts. Use nested describes liberally to express the context under which you make an assertion.\ndescribe(\"Caller#prioritize\", function() {\n  describe(\"when there are two callers in the queue\", function() {\n    describe(\"and one caller has been waiting longer than another\", function() {\n      ...\n    });\n  });\n});\n\nIn addition to using nested describes to express context, use them to organize tests by the structural properties of your source code and programming language. In Javascript this is typically prototype and function. A parent describe for a prototype contains nested describes for each of its methods. If you have cross-cutting concerns (e.g., related behavior that spans across methods or prototypes), use a describe to group them conceptually.\ndescribe(\"Car\", function() {\n  describe(\"#start\", function() {\n  });\n  \n  describe(\"#stop\", function() {\n  });\n  \n  describe(\"callbacks\", function() {\n    describe(\"after_purchase\", function() {\n    });\n  });\n  \n  describe(\"logging\", function() {\n  });\n});\n\nIn this example, one parent describe is used for all Car behavior. There is a describe for each method. Finally, cross-cutting concerns like callbacks and logging are grouped because of their conceptual affinity.\nTest Size\nIndividual tests should be short and sweet. It is sometimes recommended to make only one assertion per test:\nit(\"chooses the caller who has been waiting the longest\", function() {\n  expect(Caller.prioritize()).to(equal, caller_waiting_the_longest);\n});\n\nAccording to some, the ideal test is one line of code. In practice, it may be excessive to divide your tests to be this small. At ten lines of code (or more), a test is difficult to read quickly. Be pragmatic, bearing in mind the aims of testing.\nAlthough one assertion per test is a good rule of thumb, feel free to violate the rule if equal clarity and better terseness is achievable:\nit(\"returns the string representation of the boolean\", function() {\n  expect($.print(true)).to(equal, 'true');\n  expect($.print(false)).to(equal, 'false');\n});\n\nTwo tests would be overkill in this example.\nVariable Naming\nName variables descriptively, especially ones that will become expected values in assertions. caller_waiting_the_longest is better than c1.\nDividing code between tests and befores\nIf there is only one line of setup and it is used in only one test, it may be better to include the setup in the test itself:\nit(\"decrements the man's luck by 5\", function() {\n  var man = new Man({luck: 5});\n  \n  cat.cross_path(man);\n  expect(man.luck()).to(equal, 0);\n});\n\nBut in general, it's nice to keep setup code in before blocks, especially if the setup can be shared across tests.\ndescribe('Man', function() {\n  var man;\n  before(function() {\n    man = new Man({luck: 5});\n  });\n\n  describe('#decrement_luck', function() {\n    it(\"decrements the luck field by the given amount\", function() {\n      man.decrement_luck(3);\n      expect(man.luck()).to(equal, 2)\n    });\n  });\n  ...\n});\n\nPreconditions\nIt is ideal, if there is any chance that your preconditions are non-obvious, to make precondition asserts in your test. The last example, were it more complicated, might be better written:\nit(\"decrements the luck field by the given amount\", function() {\n  expect(man.luck()).to(equal, 5);\n\n  man.decrement_luck(3);\n  expect(man.luck()).to(equal, 2)\n});\n\nWhitespace, as seen here, can be helpful in distinguishing setup and preconditions from the system under test (SUT) and its assertions. It is nice to be consistent in your use of whitespace (e.g., \"always follow a group of preconditions by a newline\"). But it is better to use whitespace as makes the most sense given the context. As with everything in life, do it consciously and deliberately, but change your mind frequently.\nBehavioral Testing\nBehavioral testing, that is, asserting that certain functions are called rather than certain values returned, is best done with closures. The dynamic nature of JavaScript makes mocking frameworks mostly unnecessary.\nit(\"invokes #decrement_luck\", function() {\n  var decrement_luck_was_called = false;\n  man.decrement_luck = function(amount) {\n    decrement_luck_was_called = true;\n  });\n  \n  cat.cross_path(man);\n  expect(decrement_luck_was_called).to(equal, true);\n});\n\nHow to Test the DOM\nThe simplest way to test the DOM is to have a special DOM node in your suite.html file. Have all tests insert nodes into this node; have a global before reset the node between tests.\nIn suite.html:\n<div id=\"dom_test\"><\/div>\n\nIn spec_helper.js:\nScrew.Unit(function() {\n  before(function() {\n    document.getElementById('dom_test').innerHTML = ''; \/\/ but use your favorite JS library here.\n  });\n});\n\nIn some_spec.js:\ndescribe(\"something that manipulates the DOM\", function() {\n  it(\"is effortless to test!\", function() {\n    var dom_test = document.getElementById('dom_test');\n    dom_test.innerHTML = 'awesome';\n    expect(dom_test.innerHTML).to(equal, 'awesome');\n  });\n});\n\nA Javascript library like jQuery, Prototype, or YUI is a essential for testing events.\nImplementation Details\nScrew.Unit is implemented using some fancy metaprogramming learned from the formidable Yehuda Katz. This allows the describe and it functions to not pollute the global namespace. Essentially, we take the source code of your test and wrap it in a with block which provides a new scope:\nvar contents = fn.toString().match(\/^[^\\{]*{((.*\\n*)*)}\/m)[1];\nvar fn = new Function(\"matchers\", \"specifications\",\n  \"with (specifications) { with (matchers) { \" + contents + \" } }\"\n);\n\nfn.call(this, Screw.Matchers, Screw.Specifications);\n\nFurthermore, Screw.Unit is implemented using the Concrete Javascript style, which is made possible by the Effen plugin and jQuery. Concrete Javascript is an alternative to MVC. In Concrete Javascript, DOM objects serve as the model and view simultaneously. The DOM is constructed using semantic (and visual) markup, and behaviors are attached directly to DOM elements. For example,\n$('.describe').fn({\n  parent: function() {\n    return $(this).parent('.describes').parent('.describe');\n  },\n  run: function() {\n    $(this).children('.its').children('.it').fn('run');\n    $(this).children('.describes').children('.describe').fn('run');\n  },\n});\n\nHere two methods (#parent and #run) are attached directly to DOM elements that have class describe. To invoke one of these methods, simply:\n$('.describe').fn('run');\n\nBind behaviors by passing a hash (see the previous example). Using CSS3 selectors and cascading to attach behaviors provides interesting kind of multiple inheritance and polymorphism:\n$('.describe, .it').fn({...}); \/\/ applies to both describe and its\n$('.describe .describe').fn({...}); \/\/ applies to nested describes only\n\nExtensibility\nScrew.Unit is designed from the ground-up to be extensible. For example, to add custom logging, simply subscribe to certain events:\n$('.it')\n  .bind('enqueued', function() {...})\n  .bind('running', function() {...})\n  .bind('passed', function() {...})\n  .bind('failed', function(e, reason) {...})\n\nThere are also events for the loading and loaded test code code, as well as just before and just after all tests are run:\n$(Screw)\n  .bind('loading', function() {...})\n  .bind('loaded', function() {...})\n  .bind('before', function() {...})\n  .bind('after', function() {...})\n\nDownload\nYou can download the source from Github. There is are plenty of examples in the distribution.\nThanks to\n\nNathan Sobo\nYehuda Katz\nBrian Takita\nAman Gupta\nTim Connor\n\n","400":"\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\nThis repository is no longer maintained, please use Shipit instead\n\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f\ngrunt-shipit\n\n\n\n\nGrunt plugin for Shipit, an automation engine and a deployment tool written for node \/ iojs.\nIf you prefer using Shipit without grunt, please go to Shipit repository.\nGetting Started\nThis plugin requires Grunt ~0.4.0\nIf you haven't used Grunt before, be sure to check out the Getting Started guide, as it explains how to create a Gruntfile as well as install and use Grunt plugins. Once you're familiar with that process, you may install this plugin with this command:\nnpm install grunt-shipit --save-dev\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\ngrunt.loadNpmTasks('grunt-shipit');\nUsage\nExample Gruntfile.js\nmodule.exports = function (grunt) {\n  grunt.initConfig({\n    shipit: {\n      options: {\n        workspace: '\/tmp\/github-monitor',\n        deployTo: '\/tmp\/deploy_to',\n        repositoryUrl: 'https:\/\/github.com\/user\/repo.git',\n        ignores: ['.git', 'node_modules'],\n        keepReleases: 2,\n        key: '\/path\/to\/key',\n        shallowClone: true\n      },\n      staging: {\n        servers: ['user@myserver.com', 'user2@myserver2.com']\n      }\n    }\n  });\n\n  grunt.loadNpmTasks('grunt-shipit');\n  grunt.loadNpmTasks('shipit-deploy');\n\n  grunt.registerTask('pwd', function () {\n    grunt.shipit.remote('pwd', this.async());\n  });\n};\nLaunch command\ngrunt shipit:<environment> <tasks ...>\nFor more documentation about Shipit commands please refer to Shipit repository.\nFor more documentation about Shipit deploy task, please refer to Shipit deploy repository.\nUpgrading from v0.5.x\nMethods\nNow all methods returns promises, you can still use callback but the result has changed.\nBefore:\nshipit.remote('echo \"hello\"', function (err, stdout, stderr) {\n  console.log(stdout, stderr);\n});\nNow:\nshipit.remote('echo \"hello\"', function (err, res) {\n  console.log(res.stdout, res.stderr);\n});\nDeployment task\nThe deployment task is now separated from Shipit. You must install it and load it separately:\nnpm install shipit-deploy\n\ngrunt.loadNpmTasks('shipit-deploy');\nAPI change\nThe exposed property grunt.shipit.stage is now grunt.shipit.environment.\nLicense\nMIT\n","401":"Armstrong\nArmstrong is an open-source publishing system designed for news organizations\nthat gives your team the technology edge it needs to report in a media-rich\nenvironment.\nThis package is a meta package that loads all of the various components of\nArmstrong.  Installing this package is the easiest way to get the full\ndistribution of Armstrong, but is not required to use the various components of\nArmstrong.\n\nGetting Started\n\nInstallation\nFor the latest released version of Armstrong, use pip to install it from\nPyPI like this:\n$ pip install armstrong\n\nThe latest release is 12.03.1.  This is beta software, so please\nkeep that in mind while developing on it.  While we are making every effort to\nmaintain backwards compatibility between releases while in beta, things may\nchange in ways that break your code.\n\nNote on virtualenv\nWe recommend that you use virtualenv to isolate Armstrong.  We highly\nrecommend that you use the --distribute flag when creating a virtual\nenvironment, as that's what we use for testing.  Your results with traditional\nsetuptools may vary.\n\nDevelopment Releases\nYou can track the latest development of Armstrong by installing the development\nversion from Git.  Obtain the latest version by visiting our GitHub page and\neither cloning or downloading a tarball.\nOnce obtained, switch into the directory of the repository (or snapshot if a\ntarball was downloaded) and tell pip to install it:\n$ git clone git:\/\/github.com\/armstrong\/armstrong.git\n... a few lines of output from Git ...\n$ cd armstrong\n$ pip install .\n\n\nCreating an Armstrong project\nTo help get started, the armstrong.cli component can create a basic project\nstructure for you.  Create a new project like this:\n$ armstrong init mysite\narmstrong initialized!\n\nYou can initialize a project using the --template=demo parameter to\ninitialize with a demo SQLite3 database already set up.  This provides a\nworking example of how you can use Armstrong.\n\nArmstrong Project Structure\nThe following files are created in the mysite directory:\n|~fixtures\/\n| |-initial_data.json\n|~requirements\/\n| |-development.txt\n| `-project.txt\n|~settings\/\n| |-__init__.py\n| |-defaults.py\n| |-development.py\n| `-production.py\n|~templates\/\n| `-index.html\n|~urls\/\n| |-__init__.py\n| |-defaults.py\n| |-development.py\n| `-production.py\n|-wsgi.py\n\nThe settings directory contains your Django settings.  The\nsettings.defaults module contains all of the base settings that are common\nto your environment.  settings.development has settings specific to your\ndevelopment environment, while settings.production contains all of your\nproduction settings.\nYou need to edit the settings.development and settings.production to\nconfigure the database engine you want to use.\nYou can also use the settings.local_development and\nsettings.local_production modules to store values that are specific to a\nparticular box.  You shouldn't include these files in your\nrepository---anything that should be shared should go in the appropriate\nsettings module.\nsettings.development and settings.production configure you\nROOT_URLCONF as either urls.development or urls.production,\nrespectively.  Like their settings.* counterparts, you can use these for\nenvironment-specific settings while storing all of your default values in\nurls.defaults.\nAll of your requirements are specified inside the two text files in the\nrequirements directory: development.txt and project.txt.  You can\nuse pip to install the dependencies of your project by providing either file\nas an argument to pip install -r.  development.txt should contain all\nof requirements for your development environment and include project.txt.\nThe project.txt file should contain all of requirements that you have\nto have for your project.\nThe templates directory is configured as the base for your project's\ntemplates.  It contains a simple index.html that is loaded on a request to\n\/ so you can verify that everything is setup correctly.\nThe wsgi.py file provides a basic WSGI module for running your project.  It\nis configured to run using the settings.development settings, so you must\nadjust it prior to running in production.\nNote: You do not have to use the Armstrong project layout.  You can utilize\nall of Armstrong's components inside an existing Django project.  These are\nhere simply to help get you started.\n\nNext Steps\nOnce you have the project created and configured (remember, you need to setup\nyour database just like any other Django project), you've got two final steps.\nFirst, you need to install the requirements file as there are packages that\nArmstrong relies on that need to be installed from GitHub.\n$ cd mysite\n$ pip install -r requirements\/project.txt\n\nAfter you've configured the database engine and installed the base\nrequirements, you're last step is to create the database .  You run armstrong\nsyncdb which initial the database based on the apps listed in your\nINSTALLED_APPS setting.  After this runs, you will have a database created\nby Django (for more information on syncdb, see the Django docs).\nFinally, now that you have all of the dependencies installed and have a\ndatabase, you can test everything out by running armstrong runserver from\ninside your project.  By default, it listens to the localhost on port\n8000.  Loading that up should either give you the Welcome to Armstrong!\npage or the demo site, depending on whether you used the --template=demo\nflag when called armstrong init.\nCongrats, you're now setup and ready to start developing on Armstrong.\n\nVersions\nArmstrong uses date-based versions for this main armstrong package.  The\ncurrent release is 11.09.0.alpha.1.  For more information about how\nversions are handled in Armstrong, see the Versions page on the wiki.\n\nChangelog\n\n12.03.0\nThis updates the various packages to their current stable releases.\n\nDjango 1.4 Support\nArmstrong now supports Django 1.4 and has maintained backwards\ncompatibility with Django 1.3.1.\nArmstrong Wells\nWells now support allow empty wells (you must explicitly opt-in to the\nnew styles), provides abstract models for creating custom well models\nfrom and allows duplication in the admin.\nArmstrong Sections\nSections have undergone numerous small enhancements.  They now have a\nbetter admin, are more signal friendly, and have support for only\nshowing published items.\nArmstrong Layouts\nThe utils.render_model function now boasts configurable backends so\nyou can customize how models are rendered.\nRelated Content\nBackwards Incompatible Changes: The internal representation of\nfields have been changed to better reflect what they should.  A full\nexplanation of all changes is available in the\narmstrong.apps.related_content README.  No database migrations are\nrequired for this new code.\n\n\n11.12.0\nThis updates the various packages to their current release.\n\nArmstrong Hatband\nWe've updated the wells interface inside Hatband to make it more\naccessible.\nArmstrong Images\nWe now include an ImageSet for dealing with, as you might have\nguessed it, sets of Image models.  Thanks for @pizzapanther at\nMouth Watering Media for the contribution.\nImproved Related Content\nWe've added better handling of Related Content, a new admin, and new\nhelper fields for dealing with both sides of a related content\nrelationship.\nArmstrong CLI\nWe've removed the --demo flag in favor of --template=demo\nwhich provides more flexbility going forward.\n\n\n11.09.0\nThis updates the various packages to their current release.  In addition,\nit adds armstrong.hatband and armstrong.core.arm_layout to the\nmix.\n\nArmstrong Hatband\nEvery good hat needs a hatband.  Armstrong's Hatband app is the\nfoundation for our enhancements to Django's built-in admin interface.\nWe've got lots planned for it, but there are a couple of things worth\ncalling out specifically.\n\nIntegration with VisualSearch\nWells now have a much better UI thanks VisualSearch.  This new\nUI allows you to quickly search through all of your models when\nattaching a new Node to a Well.\nRich Text Editor\nWe've added a new RichTextWidget that allows you to easily\nconfigure the rich-text editor of your choice and have all of the\nadmin fields across Armstrong switch to using it.  We're shipping\nwith CKEditor support built-in.\n\n\nNew Demo Data\nNow you can include the --demo parameter to armstrong init to\nuse our demo database.  This includes lorem ipsum articles and some\ndefault sections.\nNew Layout Code\narmstrong.core.arm_layout introduces the {% render_model %}\ntemplate tag which handles switching the template used for rendering\nmodels.\nBackwards Incompatible Changes\n\narmstrong.core.arm_wells had all of its display logic moved to\nthe new armstrong.core.arm_layout app.\nWe've removed primary_section from ContentBase\n\n\n\n\n11.06.0\nThe first generally available release of Armstrong.  It is an unstable,\ndeveloper preview.\n\n\nComponents\nArmstrong is broken down into multiple components.  The main armstrong\npackage installs these individually with each being pinned to a specific\npoint release.\nIncluded in the 11.09 release are the following components:\n\narmstrong.cli\nA command line tool for creating and working with an Armstrong environment.\nYou can use this inside an Armstrong environment as a replacement for the\ntraditional manage.py in Django.\nSee the armstrong.cli repository for more information.\n\narmstrong.core.arm_content\nContains the basic elements for Armstrong-style content.  This does not\nprovide any concrete implementations of models, instead it includes lower\nlevel functionality: fields, mixins, and a base ContentBase for\ncreating a shared content model.\nSee the armstrong.core.arm_content repository for more information.\n\narmstrong.core.arm_layout\nContains helpers for managing the display of data in the context of its\ncurrent layout.\nSee the armstrong.core.arm_layout repository for more information.\n\narmstrong.core.arm_sections\nProvides a system for structuring models into \"sections\" to be used on the\nsite for organizational purposes.\nSee the armstrong.core.arm_sections repository for more information.\n\narmstrong.core.arm_wells\nFunctionality related to \"pinning\" content to a particular area.  Wells\ngive you the ability to specify any collection of models and their order to\ndisplay in various places throughout the site.\nSee the armstrong.core.arm_wells repository for more information.\n\narmstrong.apps.articles\nSimple application for handling basic articles.  This provides a thin layer\non top of the article-specific features found in the arm_content\ncomponent, but will meet the needs of many newsrooms with simple\nrequirements.\nSee the armstrong.apps.articles repository for more information.\n\narmstrong.apps.content\nSimple application for providing a concrete Content model that other\nDjango apps can build off of.\nSee the armstrong.apps.content repository for more information.\n\narmstrong.hatband\nArmstrong's enhanced version of Django's built-in django.contrib.admin\napplication.\nSee the armstrong.hatband repository for more information.\n\n\n\nContributing\nStart by finding the component of Armstrong that you would like to change.  It\nis rare that you will need to start by modifying the main Armstrong repository\nto start.\n\nCreate something awesome -- make the code better, add some functionality,\nwhatever (this is the hardest part).\nFork it\nCreate a topic branch to house your changes\nGet all of your commits in the new topic branch\nSubmit a pull request\n\n\nState of Project\nArmstrong is an open-source news platform that is freely available to any\norganization.  It is the result of a collaboration between the The Texas Tribune\nand The Bay Citizen, and a grant from the John S. and James L. Knight\nFoundation.\nTo follow development, be sure to join the Google Group.\n","402":"\n===========\nMafan - Toolkit for working with Chinese in Python\nMafan is a collection of Python tools for making your life working with Chinese so much less \u9ebb\u70e6 (mafan, i.e. troublesome).\nContained in here is an ever-growing collection of loosely-related tools, broken down into several files. These are:\ninstallation\nInstall through pip:\npip install mafan\n\nencodings\nencodings contains functions for converting files from any number of \u9ebb\u70e6 character encodings to something more sane (utf-8, by default). For example:\nfrom mafan import encoding\n\nfilename = 'ugly_big5.txt' # name or path of file as string\nencoding.convert(filename) # creates a file with name 'ugly_big5_utf-8.txt' in glorious utf-8 encoding\ntext\ntext contains some functions for working with strings. Things like detecting english in a string, whether a string has Chinese punctuation, etc. Check out text.py for all the latest goodness. It also contains a handy wrapper for the jianfan package for converting between simplified and traditional:\n>>> from mafan import simplify, tradify\n>>> string = u'\u8fd9\u662f\u9ebb\u70e6\u5566'\n>>> print tradify(string) # convert string to traditional\n\u9019\u662f\u9ebb\u7169\u5566\n>>> print simplify(tradify(string)) # convert back to simplified\n\u8fd9\u662f\u9ebb\u70e6\u5566\nThe has_punctuation and contains_latin functions are useful for knowing whether you are really dealing with Chinese, or Chinese characters:\n>>> from mafan import text\n>>> text.has_punctuation(u'\u8fd9\u662f\u9ebb\u70e6\u5566') # check for any Chinese punctuation (full-stops, commas, quotation marks, etc)\nFalse\n>>> text.has_punctuation(u'\u8fd9\u662f\u9ebb\u70e6\u5566.')\nFalse\n>>> text.has_punctuation(u'\u8fd9\u662f\u9ebb\u70e6\u5566\u3002')\nTrue\n>>> text.contains_latin(u'\u8fd9\u662f\u9ebb\u70e6\u5566\u3002')\nFalse\n>>> text.contains_latin(u'You are\u9ebb\u70e6\u5566\u3002')\nTrue\nYou can also test whether sentences or documents use simplified characters, traditional characters, both or neither:\n>>> import mafan\n>>> from mafan import text\n>>> text.is_simplified(u'\u8fd9\u662f\u9ebb\u70e6\u5566')\nTrue\n>>> text.is_traditional(u'Hello,\u9019\u662f\u9ebb\u7169\u5566') # ignores non-chinese characters\nTrue\n\n# Or done another way:\n>>> text.identify(u'\u8fd9\u662f\u9ebb\u70e6\u5566') is mafan.SIMPLIFIED\nTrue\n>>> text.identify(u'\u9019\u662f\u9ebb\u7169\u5566') is mafan.TRADITIONAL\nTrue\n>>> text.identify(u'\u8fd9\u662f\u9ebb\u70e6\u5566! \u9019\u662f\u9ebb\u7169\u5566') is mafan.BOTH\nTrue\n>>> text.identify(u'This is so mafan.') is mafan.NEITHER # or None\nTrue\nThe identification functionality is introduced as a very thin wrapper to Thomas Roten's hanzidentifier, which is included as part of mafan.\nAnother function that comes pre-built into Mafan is split_text, which tokenizes Chinese sentences into words:\n>>> from mafan import split_text\n>>> split_text(u\"\u9019\u662f\u9ebb\u7169\u5566\")\n[u'\\u9019', u'\\u662f', u'\\u9ebb\\u7169', u'\\u5566']\n>>> print ' '.join(split_text(u\"\u9019\u662f\u9ebb\u7169\u5566\"))\n\u9019 \u662f \u9ebb\u7169 \u5566\nYou can also optionally pass the boolean include_part_of_speech parameter to get tagged words back:\n>>> split_text(u\"\u9019\u662f\u9ebb\u7169\u5566\", include_part_of_speech=True)\n[(u'\\u9019', 'r'), (u'\\u662f', 'v'), (u'\\u9ebb\\u7169', 'x'), (u'\\u5566', 'y')]\npinyin\npinyin contains functions for working with or converting between pinyin. At the moment, the only function in there is one to convert numbered pinyin to the pinyin with correct tone marks. For example:\n>>> from mafan import pinyin\n>>> print pinyin.decode(\"ni3hao3\")\nn\u01d0h\u01ceo\ntraditional characters\nIf you want to be able to use split_text on traditional characters, you can make use of one of two options:\n\nEither set an environment variable, MAFAN_DICTIONARY_PATH, to the absolute path to a local copy of this dictionary file,\nor install the mafan_traditional convenience package: pip install mafan_traditional. If this package is installed and available, mafan will default to use this extended dictionary file.\n\nContributors:\n\nHerman Schaaf (IronZebra.com) (Author)\nThomas Roten (Github)\nJOEWONGLVFS\nCasper CY Chiang (Github)\n\nAny contributions are very welcome!\nSites using this:\n\nChineseLevel.com\n\n","403":"Detective.io \nDownload \u2022\nFork \u2022\nLicense \u2022\nTest coverage \u2022\nDocumentation \u2022\nVersion 1.12.13 Gorilla\nInstallation\nSee also the full installation guide.\n1. Prerequisite\nsudo apt-get install build-essential git-core python python-pip python-dev libmemcached-dev libpq-dev libxslt1-dev libxml2-dev libxml2 libjpeg8-dev\nsudo pip install virtualenv\n2.  Download the project\ngit clone git@github.com:jplusplus\/detective.io.git\ncd detective.io\n3. Install\nmake install\nRun in development\nmake run\nThen visit http:\/\/127.0.0.1:8000\nTechnical stack\nThis small application uses the following tools and opensource projects:\n\nDjango Framework - Backend Web framework\nNeo4django - Object Graph Mapper for Neo4j\nTastypie - RestAPI for Django\nAngularJS - Javascript Framework\nUI Router - Application states manager\nUnderscore - Utility library\nBootstrap - HTML and CSS framework\nLess - CSS pre-processor\nCoffeeScript\n\n","404":"##Hello, this is FileRock Client\nThis is the client of FileRock,\na backup and synchronization service that provides confidentiality and\nchecks the integrity of your data.\nFor instructions about how to run FileRock Client from the source code, look here.\nFor the list of required dependencies, look here.\nRelease notes for FileRock Client are available here.\nIn order to use FileRock Client, you will need a FileRock account.\nYou can get one here.\nIf you don't have an invitation code,\nyou can leave your email address on FileRock landing page,\nand you will receive one as soon as possible. First arrived, first served ;-)\n ______ _ _      _____            _       _____ _ _            _\n|  ____(_) |    |  __ \\          | |     \/ ____| (_)          | |\n| |__   _| | ___| |__) |___   ___| | __ | |    | |_  ___ _ __ | |_\n|  __| | | |\/ _ \\  _  \/\/ _ \\ \/ __| |\/ \/ | |    | | |\/ _ \\ '_ \\| __|\n| |    | | |  __\/ | \\ \\ (_) | (__|   <  | |____| | |  __\/ | | | |_\n|_|    |_|_|\\___|_|  \\_\\___\/ \\___|_|\\_\\  \\_____|_|_|\\___|_| |_|\\__|\n\nCopyright (C) 2012 Heyware s.r.l.\n\nThis file is part of FileRock Client.\n\nFileRock Client is free software: you can redistribute it and\/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nFileRock Client is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with FileRock Client. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n--\n###How to run FileRock Client\nIn order to run FileRock client from the source code, follow these instructions:\n\nMake sure that your system has all the required dependencies installed.\nClone this repository: git clone https:\/\/github.com\/filerock\/FileRock-Client.git\nRun the client from its main python script:\n\non Mac OS X, make sure tu use the 32-bit version of python 2.7.x: e.g., python2.7-32 FileRock.py\non Linux or Windows, if your default python version is python 2.7.x, just run python FileRock.py\n\n\nPlease not that FileRock Client used from the source code does not update itself automatically or notify the user when updates are available. If you run FileRock Client from the source code, you will need to periodically pull from this repository to get the latest version.\n\nPackaged version of FileRock Client are availble here.\nWe will do our best to keep the code in this repository and the packaged versions synchronized.\n--\n###Required dependencies\n\nReference python version is 2.7.2\n\nMost of the following dependencies can be easily installed via pip.\nIf you don't have pip yet, installation instructions are available here.\n\npbkdf2 1.3 - Can be installed through pip or from the tarball\npycrypto 2.5 - Can be ckecked out from git or from the tarball\nwxPython 2.8.12.1 - Can be downloaded from wxpython.org. Binaries for MS Windows are available [here](http:\/\/downloads.sourceforge.net\/wxpython\/wxPython2.8-win32-uni\ncode-2.8.12.1-py27.exe) and for Mac Os X here\nPIL (Python Image Library) - Can be installed through pip.\n\nInstallation through pip requires a C compiler\nBinaries for MS Windows are available here\n\n\napscheduler 2.0.3 - Can be installed through pip.\nPySocks 1.04 - Can be ckecked out from git.\nportalocker 0.3 - Can be installed through pip.\n\nThe following are required only on Linux and Mac OS X:\n\nsetproctitle 1.1.6 - Can be installed through pip.\n\nThe following are required only on Mac OS X:\n\nxattr 0.6.4 - Can be installed through pip.\n\nThe following are required only on MS Windows:\n\npywin32 217 - Can be checked out from sourceforge project page\nprotobuf 2.4.1 - Can be installed through pip.\n\nThe following are optional dependencies, needed only for the i18n support (that is, translation to languages different from English).\n\ndistutils-extra 2.37 - Can be checked out from Launchpad.\nintltool 0.50.2 - Can be checked out from Launchpad.\n\nRequires to install a perl interpreter as a dependency.\n\n\n\nThe following are required only for developers, in order to run the automated tests:\n\nnose 1.1.2 - Can be installed through pip.\nmock 1.0.1 - Can be installed through pip.\n\n","405":"django-discover-runner\n\nNote\nThis runner has been added to Django 1.6 as the default test runner.\nIf you use Django 1.6 or above you don't need this app.\n\nAn alternative Django TEST_RUNNER which uses the unittest2 test discovery\nfrom a base path specified in the settings, or any other module or package\nspecified to the test management command -- including app tests.\nIf you just run .\/manage.py test, it'll discover and run all tests\nunderneath the current working directory. E.g. if you run\n.\/manage.py test full.dotted.path.to.test_module, it'll run the tests in\nthat module (you can also pass multiple modules). If you give it a single\ndotted path to a package (like a Django app) like .\/manage.py test myapp\nand that package does not itself directly contain any tests, it'll do\ntest discovery in all submodules of that package.\n\nNote\nThis code uses the default unittest2 test discovery behavior, which\nonly searches for tests in files named test*.py. To override this\nsee the TEST_DISCOVER_PATTERN setting or use the --pattern\noption.\n\n\nWhy?\nDjango's own test discovery is very much tied to the directory structure\nof Django apps, partly due to historic reasons (the unittest library\ndidn't have its own discovery for a long time) and prevents Django app\nauthors from being good Python citizens. django-discover-runner uses the\nofficial test discovery feature of the new unittest2 library which is\nincluded in Django.\nBy default there is no way to put project specific tests in a separate\nfolder outside the Python package of the Django project, which is a great\nway to organize your code, separating the tests and non-test code.\ndjango-discover-runner helps you clean up your project tests.\nThere is also no way to specify fully dotted import paths to test\nmodules, functions, class or methods to the test management command\nbut only Django's odd standard <appname>.<TestClassName>.\ndjango-discover-runner allows you to specify any type of label to Django's\ntest management command.\nBy default Django's test runner will execute the tests of Django's own\ncontrib apps, which doesn't make sense if you just want to run your\nown app's or project's tests. django-discover-runner fixes this by allowing\nyou to specify which tests to run and organize your test code outside the\nreach of the Django test runner.\nMore reasons can be found in Carl Meyer's excellent talk about\nTesting and Django (slides).\n\nInstallation\nInstall it with your favorite installer, e.g.:\npip install -U django-discover-runner\n\ndjango-discover-runner requires at least Django 1.4 and also works on 1.5.x.\nStarting in Django 1.6 the discover runner is a built-in.\n\nSetup\n\nTEST_RUNNER (required) needs to point to the DiscoverRunner class\nto enable it:\nTEST_RUNNER = 'discover_runner.DiscoverRunner'\n\n\nAdd 'discover_runner' to your INSTALLED_APPS setting to enable the\nability to override the discovery settings below when using the test\nmanagement command.\n\nTEST_DISCOVER_TOP_LEVEL (optional) should be the directory containing\nyour top-level package(s); in other words, the directory that should be on\nsys.path for your code to import. This is for example the directory\ncontaining manage.py in the new Django 1.4 project layout.\nThe management command option is called --top-level.\n\nTEST_DISCOVER_PATTERN (optional) is the pattern to use when discovering\ntests and defaults to the unittest2 standard test*.py. The management\ncommand option is called --pattern.\n\n\n\nExamples\n\nDjango app\nTo test a reusable Django app it's recommended to add a test_settings.py\nfile to your app package to easily run the app tests with the test\nmanagement command. Simply set the TEST_RUNNER setting to\n'discover_runner.DiscoverRunner', configure the other settings necessary\nto run your tests and call the test management command with the name of\nthe app package, e.g.:\ndjango-admin.py test --settings=myapp.test_settings myapp\n\n\nDjango project\nIf you want to test a project and want to store the project's tests outside\nthe project main package (recommended), you can simply follow the app\ninstructions above, applying it to the \"project\" package, but set a few\nadditional settings to tell the test runner to find the tests:\nfrom os import path\nTEST_DISCOVER_TOP_LEVEL = path.dirname(path.dirname(__file__))\n\nThis would find all the tests within a top-level \"tests\" package. Running the\ntests is as easy as calling:\ndjango-admin.py test --settings=mysite.test_settings tests\n\nAlternatively you can specify the --top-level-directory management\ncommand option.\n\nMultiple Django versions\nIn case you want to test your app on older Django versions as well as\nDjango >= 1.6 you can simply conditionally configure the test runner in your\ntest settings, e.g.:\nimport django\n\nif django.VERSION[:2] < (1, 6):\n  TEST_RUNNER = 'discover_runner.DiscoverRunner'\n\n\nChangelog\n\n1.0 06\/15\/2013\n\nGOOD NEWS! This runner was added to Django 1.6 as the new default!\nThis version backports that runner for Django 1.4.x and 1.5.x.\nRemoved TEST_DISCOVER_ROOT setting in favor of unittest2's own way to\nfigure out the root.\nDropped support for Django 1.3.x.\n\n\n0.4 04\/12\/2013\n\nAdded ability to override the discover settings with a custom test management\ncommand.\n\n\n0.3 01\/28\/2013\n\nFixed setup.py to work on Python 3. This should make this app compatible\nto Python 3.\n\n\n0.2.2 09\/04\/2012\n\nStopped setting the top level variable in the case of using a module path\nas the test label as it made the wrong assumption that the parent directory\nis the top level.\n\n\n0.2.1 08\/20\/2012\n\nFixed a rather esoteric bug with testing test case class methods\nthat was caused by a wrong import and the way Django wraps itself\naround the unittest2 module (if availale) or unittest on Python >= 2.7.\n\n\n0.2 05\/26\/2012\n\nAdded ability to use an optionally installed unittest2 library\nfor Django projects using Django < 1.3 (which added unittest2 to the\ndjango.utils.unittest package).\n\n\n0.1.1 05\/23\/2012\n\nFixed a bug that prevented the project based feature to work correctly.\n\n\n0.1 05\/20\/2012\n\nInitial release with support for Django >= 1.3.\n\n\nThanks\nThis test runner is a humble rip-off of Carl Meyer's DiscoveryRunner\nwhich he published as a gist a while ago. All praise should be directed at\nhim. Thanks, Carl!\nThis was also very much related to ticket #17365 which eventually led\nto the replacement of the default test runner in Django. Thanks again,\nCarl!\n","406":"Jetstream\n\nJetstream is a real-time stream processing system for analyzing live event streams to enable business to react to signals much earlier than is possible with batch oriented systems like Hadoop.\nIt is targeted for security, risk, machine learning, fault monitoring, predictive analytics and many more use cases where there is a need to track, detect and react to user or machine behavior patterns over windows of time. Examples of these could be monitoring DOS attacks, predicting failures by watching metrics, correlating events etc.\nFollow wiki for more details.\n","407":"\u6548\u679c\u5c55\u793a\n\n\n\u8be6\u89e3\nhttp:\/\/yueban.github.io\/2015\/04\/28\/MaterialDesign\u6587\u5b57\u7f29\u653e\u5e76\u5165Toolbar\u6548\u679c\u7684\u4e00\u79cd\u5b9e\u73b0\/\n","408":"TimeSinceTextView\nThis is a subclass of android.widget.TextView that exposes a method setDate() which accepts a long Unix timestamp or java.util.Date. The view converts the date into a String which describes the date in terms of time since that timestamp. For example, if the current timestamp is Unix 1453503166 and we call timeSinceTextView.setDate(1453503116), \"50 seconds ago\" is displayed.\nChangelog\nJavadoc\nComparison to DateUtils.getRelativeTimeSpanString\nI actually wrote this library before I knew about DateUtils.getRelativeTimeSpanString, but the output is actually quite a bit different. The DateUtils implementation should return localized text and allows for customizable flags. See here for a comparison of the output of different time stamps.\nUsage\nSimply declare a TimeSinceTextView in XML or create one in code.\n<com.ddiehl.timesincetextview.TimeSinceTextView\n  android:id=\"@+id\/timestamp\"\n  android:layout_width=\"wrap_content\"\n  android:layout_height=\"wrap_content\" \/>\nThen call setDate(Date) or setDate(long) with a Unix timestamp, and the text will be automatically generated and set to the view.\n((TimeSinceTextView) findViewById(R.id.timestamp)).setDate(1452827942);\nTo get an abbreviated form of the converted text, add app:abbreviated=\"true\" to your XML layout.\n<com.ddiehl.timesincetextview.TimeSinceTextView\n  xmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n  android:layout_width=\"wrap_content\"\n  android:layout_height=\"wrap_content\"\n  app:tstv_abbreviated=\"true\" \/>\nThe class TimeSince also contains static methods which can be used to retrieve a relative timestamp string without an instance of TimeSinceTextView.\nAdd to your project\n\nrepositories {\n    maven { url \"https:\/\/jitpack.io\" }\n}\ndependencies {\n  compile 'com.github.damien5314:TimeSinceTextView:1.+'\n}\n\nContributions\nPull requests are welcome, in particular in would be nice to have strings.xml translated into as many languages as possible.\n","409":"WanAndroid\nAn Android app for www.wanandroid.com\n\n\n\nWanAndroid App aims to help people access the latest android articles, which is designed with Material Design style, built on MVP(Model-View-Presenter) architecture with RxJava2, Retrofit2, Realm database, Glide.\nThe source code in the repository reflects the app which supports mobile devices running Android 6.0+.\nAbout This Repository And App\nThis app is inspired by Espresso which is developed by TonnyL, Awesome-WanAndroid and has a lot of similar elements in design.\nAnd this app is using the WanAndroid API(doc) designed by HongYang.\nFeatures\n\nCollect many articles of Android.\nUpdate the articles everyday.\nSupport collect the articles ,which will synchronize the user data so you can visit website to browse the list of articles collected.\nSupport mark the articles so you can read it later.\nDay mode and night mode are supported.\nSupport send feedback on using experience from your devices.\n\nScreenshots\n\n\nI hope the source code for this app is useful for you as a reference or starting point for creating your own app. Here is some instructions to help you better build and run the code in Android Studio.\nClone the Repository:\ngit clone https:\/\/github.com\/CoderLengary\/WanAndroid\n\nCheck out the master branch:\ngit checkout master\n\nNotice: If you want to review a different branch, replace the master with the name you want to checkout (if it does exist). Finally open the WanAndroid\/ directory in Android Studio.\nSuggestion: It is better for you to update your Android Studio to version 3.0 when you open this project.\nLibraries Used in This App\n\n\n\nName\nIntroduction\n\n\n\n\nAndroid Support Libraries\nThe Android Support Library offers a number of features that are not built into the framework. These libraries offer backward-compatible versions of new features, provide useful UI elements that are not included in the framework, and provide a range of utilities that apps can draw on.\n\n\nCircleImageView\nA circular ImageView for Android\n\n\nRealm\nRealm is a mobile database: a replacement for SQLite & ORMs.\n\n\nRetrofit\nType-safe HTTP client for Android and Java by Square, Inc.\n\n\nRxAndroid\nRxJava bindings for Android.\n\n\nRxJava\nRxJava \u2013 Reactive Extensions for the JVM \u2013 a library for composing asynchronous and event-based programs using observable sequences for the Java VM.\n\n\nGlide\nAn image loading and caching library for Android focused on smooth scrolling.\n\n\nAgentWeb\nAgentWeb is an Android WebView based, extremely easy to use and powerful library.\n\n\nFlowLayout\nA very convenient and powerful flow layout.\n\n\n\nThanks to\n\nTonnyL Really appreciate his help!\nWanAndroid\nAwesome-WanAndroid\n\nLicense\nCopyright 2018 CoderLengary\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","410":"MaterialDesignLearn\n\u4e3b\u8981\u662f\u5b66\u4e60design lib\u91cc\u9762\u7684\u63a7\u4ef6\u4f7f\u7528\n##NavigationView\n\n##RecyclerView\u5b9e\u73b0\u7011\u5e03\u6d41\n\n##CoordinatorLayout\u7ed3\u5408ToolBar\u663e\u793a\u9690\u85cf \u7c7b\u4f3c\u6dd8\u5b9d\u5546\u54c1\u8be6\u60c5\n###\u91cc\u9762\u8fd8\u53ef\u4ee5\u770b\u5230TabLayout\u4f7f\u7528\uff0c\u7ed3\u5408\u4e86ViewPager\n\n##FloatActionButton\u70b9\u51fb\u5b9e\u73b0\u6dd8\u5b9d\u52a0\u5165\u8d2d\u7269\u8f66\u52a8\u753b\n\n##\u81ea\u5b9a\u4e49Behavior\u5b9e\u73b0\u77e5\u4e4e \u7b80\u4e66\u6548\u679c\n####\u77e5\u4e4e\u6548\u679c\n\n####\u7b80\u4e66\u6548\u679c\n\n","411":"Vert.x 2.x is deprecated - use instead http:\/\/vertx.io\/docs\/vertx-rx\/java\/\nmod-rxvertx\nVert.x module which uses RxJava to add support for Reactive Extensions (RX) using the RxJava library. This allows VertX developers to use the RxJava type-safe composable API to build VertX verticles.\nDependencies\n\nThe module wraps the VertX core objects to add Observable support so it is tightly bound to the VertX release.\nThis module also contains the Netflix RxJava library.\n\nStatus\nCurrently Observable wrappers are provided for\n\nEventBus\nHttpServer\nHttpClient\nNetServer\nNetClient\nTimer\n\nThere are also base Observable adapters that map Handler and AsyncResultHandler to Observable that can be used to call other Handler based APIs.\nSupport coming soon for\n\nFileSystem\nSockJSServer\n\nUsage\nThis is a non-runnable module, which means you add it to your module via the \"includes\" attribute of mod.json.\nAll standard API methods of the form\nvoid method(args...,Handler<T> handler)\nare typically available in the form\nObservable<T> method(args...)\nwhere the operation is executed immediately or\nObservable<T> observeMethod(args...)\nwhere the operation is executed on subscribe. This latter form is the more 'pure' Rx method and should be used where possible (required to maintain semantics of concat eg)\nEventBus\nRxEventBus rxEventBus = new RxEventBus(vertx.eventBus());\nrxEventBus.<String>registerHandler(\"foo\").subscribe(new Action1<RxMessage<String>>() {\n  public void call(RxMessage<String> message) {\n    \/\/ Send a single reply\n    message.reply(\"pong!\");\n  }\n});\n\nObservable<RxMessage<String>> obs = rxEventBus.send(\"foo\", \"ping!\");\n\nobs.subscribe(\n  new Action1<RxMessage<String>>() {\n    public void call(RxMessage<String> message) {\n      \/\/ Handle response \n    }\n  },\n  new Action1<Throwable>() {\n    public void call(Throwable err) {\n     \/\/ Handle error\n    }\n  }\n);\n\nScheduler\nThe standard RxJava schedulers are not compatible with VertX. In order to preserve the Vert.x Threading Model all callbacks to a Verticle must be made in the context of that Verticle instance.\nRxVertx provides a custom Scheduler implementation that uses the Verticle context to scheduler timers and ensure callbacks run on the correct context.\nIn the following example the scheduler is used to run a Timer and then buffer the output.\nNote: The RxVertx scheduler must always be used to observe results inside the Verticle. It is possible to use the other Schedulers (eg for blocking calls) as long as you always use observeOn to route the callbacks onto the Verticle EventLoop. For timers it is more efficient to just use the Vert.x scheduler\nRxVertx rx = new RxVertx(vertx);\nObservable o = (some observable source)\n\nObservable\n      .timer(10, 10, TimeUnit.MILLISECONDS, rx.contextScheduler())\n      .buffer(100,TimeUnit.MILLISECONDS,rx.contextScheduler())\n      .take(10)\n      .subscribe(...)\nTimer\nThe timer functions are provided via the RxVertx wrapper. The timer is set on-subscribe. To cancel a timer that has not first, or a periodic timer, just unsubscribe.\nRxVertx rx = new RxVertx(vertx);\nrx.setTimer(100).subscribe(new Action1<Long>() {\n  public void call(Long t) {\n    \/\/ Timer fired\n  }\n});\nThe new Scheduler means you can use the native RxJava Timer methods - this Timer may be deprecated in future\nHelper\nThe support class RxSupport provides several helper methods for some standard tasks\nStreams\nThere are two primary wrappers\nObservable RxSupport.toObservable(ReadStream)\nConvert a ReadStream into an Observable<Buffer>\nRxSupport.stream(Observable,WriteStream)\nStream the output of an Observable to a WriteStream.\nplease note that this method does not handle writeQueueFull so cannot be used as a pump\n","412":"Android Autowire\nUsing Java Annotations and Reflection, this library will allow you to replace some of annoying boilerplate setup from your Activities, Fragments, and Views with an annotation based approach.\nThis repository is referenced in the blog post: http:\/\/www.cardinalsolutions.com\/cardinal\/blog\/mobile\/2014\/01\/dealing_with_android.html\nFeatures\n\nSupports Inheritance of Activities. You can inherit views from parent Activities, and every view will be picked up and wired in\nAs it uses reflection, it will work with private variables\nComes with several out of the box ways of specifying IDs allowing for flexibility in naming IDs and implementing the annotations\nProvides an optional required field in the annotation, so if an ID is not found, the variable will be skipped without an Exception being thrown\nSupport Annotations for Layout as well as Views\nSupport an Annotation based approach for saving instance state.  This also allows for inheritance.\nCan be adapted to work with Fragments as well as Activities\nCan be adapted to work with CustomViews\n\nThe Android Way\nHere are some Examples of Android Boilerplate code that we can make more clear, readable, and easier to use with Annotations.\nfindViewById()\nOne particularly jarring example of Android boilerplate code is the findViewById() method.  Every time you want to access an Android view defined in your XML, you need to use this method, often with a typecast.  For large Activities with many views, this can add a lot of code that does nothing but pull variables out of the xml.\npublic class MainActivity extends BaseActivity{\n\n\tprivate ImageView logo;\n\n\t@Override\n    public void onCreate(Bundle savedInstanceState){\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.main);\n\n    \tlogo = (ImageView) findViewById(R.id.logo);\n\t}\n}\nsetContentView()\nIn the code example above, we have the setContentView(R.layout.main) line.  You need something like this in every Activity class, with the sole purpose of inflating your layout.  It's not a big deal, but it is one extra step you have to go through when creating your Activity classes because it has to be put in exactly the right spot.  It needs to be in onCreate() before any findViewById() call.\nSaving Instance State\nA quirk of how the Android operating systems works, Activities can be destroyed at almost anytime to make room for other OS processes.  They are also destroyed and re-created on rotation.  The developer is in charge of saving the Activity's state, making sure the Activity comes back exactly the same way before it was destroyed.\nIn the Android way, instance variables that you have to manually store are put into a Bundle in the onSaveInstanceState method.  Then they must be pulled out again in the onCreate() method.\npublic class MainActivity extends BaseActivity{\n\n    private static final String SOME_STATE_KEY = \"some_state_key\";\n\tprivate int someState;\n\n\t@Override\n    public void onCreate(Bundle savedInstanceState){\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.main);\n\n    \tif(savedInstanceState != null){\n              someState = savedInstanceState.getInt(SOME_STATE_KEY);\n        }\n\t}\n\n    @Override\n    protected void onSaveInstanceState(Bundle outState){\n\t\tsuper.onSaveInstanceState(outState);\n        outState.putInt(SOME_STATE_KEY, someState);    \n    }\n}\nWith AndroidAutowire\nThis library will help streamline this process into a more readable format using annotations and reflection.\nfindViewById()\nBy annotating a class variable for the View with the @AndroidView custom annotation, you enable the reflection code to pull the view out of the xml.  The variable name will be the view id, or alternatively, the view id can be specified in the annotation.  The annotation processing occurs in an overridden method of setContentView(int layoutResID) in the Activity\u2019s base class.\nMainActivity Class\npublic class MainActivity extends BaseActivity{\n\n\t@AndroidView\n\tprivate ImageView logo;\n\n\t@Override\n    public void onCreate(Bundle savedInstanceState){\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.main);\n\t}\n}\nBaseActivity class\npublic class BaseActivity extends Activity {\n\n\t@Override\n    public void setContentView(int layoutResID) {\n    \tsuper.setContentView(layoutResID);\n    \tAndroidAutowire.autowire(this, BaseActivity.class);\n    }\n}\nsetContentView()\nSpecifying the layout resource in the onCreate is not difficult, but it can create problems if you forget add the method call, or if you do it out of order.  Instead, use an annotation:\nMainActivity Class\n@AndroidLayout(R.layout.main)\npublic class MainActivity extends BaseActivity{\n\n\t@Override\n    public void onCreate(Bundle savedInstanceState){\n        super.onCreate(savedInstanceState);\n\t}\n}\nBaseActivity class\npublic class BaseActivity extends Activity {\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState){\n        super.onCreate(savedInstanceState);\n        int layoutId = AndroidAutowire.getLayoutResourceByAnnotation(this, this, BaseActivity.class);\n\t\t\/\/If this activity is not annotated with AndroidLayout, do nothing\n\t\tif(layoutId == 0){\n\t\t\treturn;\n\t\t}\n\t\tsetContentView(layoutId);\n    }\n\n\t@Override\n    public void setContentView(int layoutResID) {\n    \tsuper.setContentView(layoutResID);\n    \tAndroidAutowire.autowire(this, BaseActivity.class);\n    }\n}\nSaving Instance State\nAll of the reading\/writing with the Bundle can be done with reflection.  Simply annotate the instance variable you want to save\/load, and the AndroidAutowire library will do the work for you.\nMainActivity Class\n@AndroidLayout(R.layout.main)\npublic class MainActivity extends BaseActivity{\n    @SaveInstance\n    private int someState;\n\n\t@Override\n    public void onCreate(Bundle savedInstanceState){\n        super.onCreate(savedInstanceState);\n\t}\n}\nBaseActivity Class\npublic class BaseActivity extends Activity {\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState){\n        super.onCreate(savedInstanceState);\n        AndroidAutowire.loadFieldsFromBundle(savedInstanceState, this, BaseActivity.class);\n    }\n\n    @Override\n\tprotected void onSaveInstanceState(Bundle outState){\n\t\tsuper.onSaveInstanceState(outState);\n\t\tAndroidAutowire.saveFieldsToBundle(outState, this, BaseActivity.class);\n\t}\n}\nConfiguration\nSimply include the jar in your classpath.  The process for including the AndroidAutowire library will be IDE specific, but once the library is included in the project, the methods will all be there for you to use.\nYou can create your own BaseActivity using the process above, or you can use a provided BaseActivity called BaseAutowireActivity.  That will provide support for all features given above, as well as including a new abstract method that acts as a callback once the autowiring is complete. If you use features like BaseAutowireActivity and @AndroidLayout it may not even be necessary to override onCreate in your Activity class.\nFragments\nMuch like Activities, Fragments have layouts, state to be saved, and views to be autowired. But the process for setting up a Fragment is different than an Activity.  None the less, AndroidAutowire provides the ability to do all of this using Annotations as well by providing a new method: AndroidAutowire.autowireFragment().\nHere is an Example base class for Fragments:\npublic abstract class BaseFragment extends Fragment {\n\n\tprotected View contentView;\n\t\n\t@Override\n    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {\n\t\t\/\/Load any annotated fields from the bundle\n\t\tAndroidAutowire.loadFieldsFromBundle(savedInstanceState, this, BaseFragment.class);\n\t\t\n\t\t\/\/Load the content view using the AndroidLayout annotation\n        contentView = super.onCreateView(inflater, container, savedInstanceState);\n        if (contentView == null) {\n        \tint layoutResource = AndroidAutowire.getLayoutResourceByAnnotation(this, getActivity(), BaseFragment.class);\n        \tif(layoutResource == 0){\n            \treturn null;\n            }\n        \tcontentView = inflater.inflate(layoutResource, container, false);\n        }\n        \/\/If we have the content view, autowire the Fragment's views\n        autowireViews(contentView);\n        \/\/Callback for when autowiring is complete\n        afterAutowire(savedInstanceState);\n        return contentView;\n    }\n\t\n\tprotected void autowireViews(View contentView){\n\t\tAndroidAutowire.autowireFragment(this, BaseFragment.class, contentView, getActivity());\n\t}\n\t\n\t@Override\n\tpublic void onSaveInstanceState(Bundle outState){\n\t\tsuper.onSaveInstanceState(outState);\n\t\tAndroidAutowire.saveFieldsToBundle(outState, this, BaseFragment.class);\n\t}\n\t\n\tprotected abstract void afterAutowire(Bundle savedInstanceState);\n}\nUnfortunately, do to fragmentation between the Android Core API and the Support Library, this class is not included with the Jar (whereas BaseAutowireActivity is included).\nCustom Views\nIf you are writing a non-trivial Android App, chances are you will need to make your own custom Views at some point.  These views may have subviews.  Again, rather than being forced to use findViewById(), we can use AndroidAutowire and Annotations with the AndroidAutowire.autowireView() method.\npublic class CustomView extends RelativeLayout {\n\n\t@AndroidView(R.id.title)\n\tprivate TextView title;\n\t\n\t@AndroidView(R.id.icon)\n\tprivate ImageView icon;\n\n    public CustomView(Context context, AttributeSet attrs, int defStyle) {\n\t\tsuper(context, attrs, defStyle);\n\t\tLayoutInflater inflater = LayoutInflater.from(context);\n\t\tinflater.inflate(R.layout.custome_view, this);\n\t\tAndroidAutowire.autowireView(this, CustomView.class, context);\n\t}\n}\nComparison to Other Libraries\nThere are some other open source libraries that accomplish something similar to what Android Autowire hopes to provide\nRoboGuice is a dependency injection library that can inject views in much the same way.  However, you must extend the Robo* classes, and there may be performance issues. (https:\/\/github.com\/roboguice\/roboguice\/wiki)\nAndroid Annotations can wire in views by annotation, but the approach they take is quite different.  Android Annotations requires you to use an extra compile step, creating generated Activity classes that must be referenced in the AndroidManifest.xml.  As this approach will create subclasses of your Activity, you cannot use this on private variables.  Additionally, there is much more configuration and initial setup. (https:\/\/github.com\/excilys\/androidannotations\/wiki)\nButter Knife does the same compile time annotation approach as Android Annotations, but instead of generating a new Activity, they generate a class to pass your activity into. This way, you don't have to deal with generated sub classes, but you still get some of the heavy hitting features like onClick Listeners. (http:\/\/jakewharton.github.io\/butterknife\/)\nThe real advantage to this \"Android Autowire\" library is ease of use.  There is minimal configuration in just about every IDE, and little overhead, allowing you to quickly start using these annotations in your new or existing project.  Instead of providing a full feature set, this library concentrates only on limited number of features, such as views, layouts and Bundle resources, allowing it to fill the gap while still being lightweight.\nPerformance\nThe more you use the library, the more you want to keep an eye out for performance hits. Most of this reflection code is going to be done on the main thread, and that is always a risk. However, I have been using all of the features, from loading Serializable objects from the Bundle to finding views inside of Fragments, and I have not noticed any type of performance decrease. In fact, even some very complex Activities have made full use of this reflection code without any issue. My biggest concern would be older devices that I have not tested on, devices that may be slow to begin with.\nTo illustrate this, I did some benchmarks on an HTC Nexus One running 2.3.4 Gingerbread. The application I used is a fairly complex production Android App. The time is the total time for the reflection to complete, not including the time it takes for the system to start the Activity\/Fragment and not including any time to inflate XML layouts.\n\nActivity wiht 1 Autowired View, 0 Save Instance variables, and layout: 0.7ms\nActivity with 15 Autowired Views, 2 Save Instance variables, and layout: 4.9ms\nFragment with 1 Autowired View, 0 Save Instance variables, and layout: 2.0ms\nFragment with 3 Autowired Views, 4 Save Instance variables, and layout: 6.5ms\nFragment with 18 Autowired Views, 6 Save Instance variables, layout, and inheritance: 44.6ms\n\nThis is hardly a scientific endeavour, but it should give some pretty clear direction as to what the performance impact of using this library would be. Using this library with API level 10 and up seems to be fairly safe, as the most complicated bit of reflection using a Fragment with many views and instance state was still completed in less than 50 milliseconds.\nAuthor \/ License\nCopyright Cardinal Solutions 2015. Licensed under the MIT license.\n\n","413":"PoiPhoto\nA simple Photo Selecter\nWhat is PoiPhoto\nPoiPhoto is a simple lib to select photos for Android.\n\n\n##How to use\n###Gradle\ncompile 'com.flying.xiaopo:poiphoto:0.4.2'\n###AndroidManifest.xml\n<uses-permission\n    android:name=\"android.permission.READ_EXTERNAL_STORAGE\"\n    android:maxSdkVersion=\"23\"\/>\n<uses-permission\n    android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"\n    android:maxSdkVersion=\"23\"\/>\n###Java\nPhotoPicker.newInstance()\n          .setAlbumTitle(\"Album\")\n          .setPhotoTitle(\"Photo\")\n          .setToolbarColor(Color.BLACK)\n          .setToolbarTitleColor(Color.WHITE)\n          .setMaxNotice(\"can not select more\") \/\/the message when user selected photos too more\n          .setStatusBarColor(Color.BLACK)   \/\/when sdk >21 ,it will work\n          .setMaxCount(6)             \/\/max count of selected count\n          .pick(MainActivity.this);   \/\/context\nalso\nPhotoPicker.newInstance()\n                .inflate(RecyclerView, RecyclerView.LayoutManager);\nor just get data\nPhotoManager photoManager = new PhotoManager(context);\n###To Get Data\n@Override\nprotected void onActivityResult(int requestCode, int resultCode, Intent data) {\n    super.onActivityResult(requestCode, resultCode, data);\n    if (resultCode == RESULT_OK && requestCode == Define.DEFAULT_REQUEST_CODE) {\n        \/\/to get path of the selected photos\n        List<String> paths = data.getStringArrayListExtra(Define.PATHS);\n        \/\/to get datatype of photo of the selected photos\n        List<Photo> photos = data.getParcelableArrayListExtra(Define.PHOTOS);\n    }\n}\n","414":"###\u793a\u4f8b\u4ee3\u7801\n\n\n\n\u5305\n\u4f5c\u7528\n\n\n\n\naidl\naidl\u793a\u4f8b\u4ee3\u7801\n\n\nbrzier\n\u8d1d\u585e\u5c14\u66f2\u7ebf\u793a\u4f8b\n\n\njni\njni\u793a\u4f8b\n\n\nmvp\nmvp\u7ed1\u5b9a\u793a\u4f8b\n\n\nrecycler\nrecycler\u7684\u70b9\u51fb\u62d6\u62fd\u793a\u4f8b\n\n\nretrofit\nretrofit\u5de7\u5999\u7684\u5c01\u88c5\u793a\u4f8b\n\n\nsensor\n\u4f20\u611f\u5668\u793a\u4f8b\n\n\nview\nLightingColorFilter\u4f7f\u7528\u793a\u4f8b\n\n\nzxing\n\u4e00\u4e2azxing\u7684\u7b2c\u4e09\u65b9\u5c01\u88c5\u5e93\u4f7f\u7528\u793a\u4f8b\n\n\n\n","415":"Amazon Kinesis Storm Spout\nThe Amazon Kinesis Storm spout helps Java developers integrate Amazon Kinesis with Storm.\nRequirements\n\nAWS SDK for Java\nJava 1.7 (Java SE 7) or later\nApache Commons Lang 3.0 or later\nGoogle Guava 13.0 or later\nAnd, of course, Amazon Kinesis and Storm\n\nOverview\nThe Amazon Kinesis Storm spout fetches data records from Amazon Kinesis and emits them as tuples. The spout stores checkpoint state in ZooKeeper to track the current position in the stream.\nThe Amazon Kinesis Storm spout can be configured to retry failed records. By default, it retries a failed record 3 times. If a record fails and the retry limit has been reached, the spout will log an error and skip over the record. The spout buffers pending records in memory, so it can re-emit a failed record without having to re-fetch the record from Amazon Kinesis. The spout sets the checkpoint to the highest sequence number that has been ack'ed (or exhausted retry attempts).\nTo use the spout, you'll need to add it to your Storm topology.\n\nKinesisSpout: Constructs an instance of the spout, using your AWS credentials and the configuration specified in KinesisSpoutConfig (as well as com.amazonaws.ClientConfiguration, via the AWS SDK). Each task executed by the spout operates on a distinct set of Amazon Kinesis shards. Shard states are periodically committed to ZooKeeper. When the spout is deactivated, it will disconnect from ZooKeeper, but the spout will continue monitoring its local state so you can activate it again later.\nKinesisSpoutConfig: Configures the spout, including the Storm topology name, the Amazon Kinesis stream name, the endpoint for connecting to ZooKeeper, and the prefix for the ZooKeeper paths where the spout state is stored. See the samples folder for configuration examples.\nDefaultKinesisRecordScheme: This default scheme, used by the sample topology, emits a tuple of (partitionKey, record). If you want to emit more structured data, you can provide your own implementation of IKinesisRecordScheme.\n\nThe samples folder includes a sample topology and sample bolt, using the number of Amazon Kinesis shards as the parallelism hint for the spout. For more information about Storm topologies and bolts, see the Storm documentation.\nUsing the Sample\n\nEdit the *.properties file to configure your Storm topology, Amazon Kinesis stream, and ZooKeeper details. For your AWS Credentials, we recommend using IAM roles on Amazon EC2 when possible. You can also specify your credentials using system properties, environment variables, or AwsCredentials.properties.\nPackage the spout and the sample (including all dependencies but excluding Storm itself) into one JAR file.\nDeploy the package to Storm via the JAR file, e.g., storm jar my-spout-sample.jar SampleTopology sample.properties RemoteMode\n\nRelease Notes\nRelease 1.1.1 (June 1, 2015)\n\nWhen a Kinesis Stream is resharded, \"storm rebalance\" can be invoked to refresh the shard list and distribute the latest shards across the Spout tasks.\n\nRelease 1.1.0 (October 21, 2014)\n\nAdded support for retrying failed records\nAdded region name support\n\nFuture Work\n\nAutomatically handle closed, split, and merged shards.\n\nRelated Resources\nAmazon Kinesis Developer Guide\nAmazon Kinesis API Reference\nAmazon Kinesis Client Library\nAmazon Kinesis Connector Library\n","416":"Amazon Kinesis Storm Spout\nThe Amazon Kinesis Storm spout helps Java developers integrate Amazon Kinesis with Storm.\nRequirements\n\nAWS SDK for Java\nJava 1.7 (Java SE 7) or later\nApache Commons Lang 3.0 or later\nGoogle Guava 13.0 or later\nAnd, of course, Amazon Kinesis and Storm\n\nOverview\nThe Amazon Kinesis Storm spout fetches data records from Amazon Kinesis and emits them as tuples. The spout stores checkpoint state in ZooKeeper to track the current position in the stream.\nThe Amazon Kinesis Storm spout can be configured to retry failed records. By default, it retries a failed record 3 times. If a record fails and the retry limit has been reached, the spout will log an error and skip over the record. The spout buffers pending records in memory, so it can re-emit a failed record without having to re-fetch the record from Amazon Kinesis. The spout sets the checkpoint to the highest sequence number that has been ack'ed (or exhausted retry attempts).\nTo use the spout, you'll need to add it to your Storm topology.\n\nKinesisSpout: Constructs an instance of the spout, using your AWS credentials and the configuration specified in KinesisSpoutConfig (as well as com.amazonaws.ClientConfiguration, via the AWS SDK). Each task executed by the spout operates on a distinct set of Amazon Kinesis shards. Shard states are periodically committed to ZooKeeper. When the spout is deactivated, it will disconnect from ZooKeeper, but the spout will continue monitoring its local state so you can activate it again later.\nKinesisSpoutConfig: Configures the spout, including the Storm topology name, the Amazon Kinesis stream name, the endpoint for connecting to ZooKeeper, and the prefix for the ZooKeeper paths where the spout state is stored. See the samples folder for configuration examples.\nDefaultKinesisRecordScheme: This default scheme, used by the sample topology, emits a tuple of (partitionKey, record). If you want to emit more structured data, you can provide your own implementation of IKinesisRecordScheme.\n\nThe samples folder includes a sample topology and sample bolt, using the number of Amazon Kinesis shards as the parallelism hint for the spout. For more information about Storm topologies and bolts, see the Storm documentation.\nUsing the Sample\n\nEdit the *.properties file to configure your Storm topology, Amazon Kinesis stream, and ZooKeeper details. For your AWS Credentials, we recommend using IAM roles on Amazon EC2 when possible. You can also specify your credentials using system properties, environment variables, or AwsCredentials.properties.\nPackage the spout and the sample (including all dependencies but excluding Storm itself) into one JAR file.\nDeploy the package to Storm via the JAR file, e.g., storm jar my-spout-sample.jar SampleTopology sample.properties RemoteMode\n\nRelease Notes\nRelease 1.1.1 (June 1, 2015)\n\nWhen a Kinesis Stream is resharded, \"storm rebalance\" can be invoked to refresh the shard list and distribute the latest shards across the Spout tasks.\n\nRelease 1.1.0 (October 21, 2014)\n\nAdded support for retrying failed records\nAdded region name support\n\nFuture Work\n\nAutomatically handle closed, split, and merged shards.\n\nRelated Resources\nAmazon Kinesis Developer Guide\nAmazon Kinesis API Reference\nAmazon Kinesis Client Library\nAmazon Kinesis Connector Library\n","417":"text_extraction\nThis code is the implementation of the method proposed in the paper \u201cMulti-script text extraction from natural scenes\u201d (Gomez & Karatzas), International Conference on Document Analysis and Recognition, ICDAR2013.\nThis code should reproduce the same quantitative results published on the paper for the KAIST dataset (for the task of text segmentation at pixel level). If you plan to compare this method with your's in other datasets please drop us a line ({lgomez,dimos}@cvc.uab.es). Thanks!\nIncludes the following third party code:\n\nfast_clustering.cpp Copyright (c) 2011 Daniel M\u00fcllner, under the BSD license. http:\/\/math.stanford.edu\/~muellner\/fastcluster.html\nmser.cpp Copyright (c) 2011 Idiap Research Institute, under the GPL license. http:\/\/www.idiap.ch\/~cdubout\/\nbinomial coefficient approximations are due to Rafael Grompone von Gioi. http:\/\/www.ipol.im\/pub\/art\/2012\/gjmr-lsd\/\n\n","418":"410 Obsolete Repository\nThese are not the codes you are looking for\nWe've made a ton of progress on Couchbase Mobile technologies since this repository was current. You can find the latest code in these Github projects:\n\nCouchbase Mobile documentation repository. Look here for an architecture overview, tutorials, and links to API docs.\nCouchbase Lite iOS sync client. This is the latest version of TouchDB. We've changed the name because it is lighter, and built by Couchbase. TouchDB will continue to get bug fixes but new development is happening on Couchbase Lite.\nCouchbase Lite Android is coming soon. If you need something for Android today, look at TouchDB for Android\nLiteGap iOS PhoneGap \/ Cordova container for Couchbase Lite, makes it easy to build HTML5 mobile sync apps. If you are creating a PhoneGap container on Android, it should be as simple as adding TouchDB Android to the generic PhoneGap Android app.\n\nTo discuss Couchbase Mobile and find answers about the latest state of the art, please join our Couchbase Mobile Google Group\nIf you really wanted the legacy code for this repository, it's available on the master branch. You are currently looking at the redirect branch. Check out the repo and switch to the master branch, or use the branch selector in the Github UI.\n","419":"GraphLite version 0.20\n","420":"Spherical Harmonic Tools\nCopyright \u00a9 2012\u20132013 \u2014 Robert Kooima\nThis module provides a straightforward implementation of the real spherical harmonic transform and its inverse. It uses the orthonormalized associated Legendre functions, generated by an m-varying recurrence. The implementation is expressed in terms of the Cooley-Tukey fast Fourier transform, so the running time of both synthesis and analysis is O(n3) and n must be a power of two. It is parallelized using OpenMP and will use all available processor cores.\n\nsh.hpp\nshtrans.cpp\nshimage.cpp\nsherror.cpp\nMakefile\n\nCommand Line Tools\nA few command line tools are provided to directly apply the transform to real image data. These tools are compiled with this image handling module, which supports input and output of TIFF, PNG, JPEG, and OpenEXR files of any supported depth. For reliable resynthesis, it is highly recommended that frequency-domain images be stored using 32-bit floating point samples in TIFF format, which are the defaults.\nSpherical Harmonic Analysis\nshtrans    [-FDL] [-o output] [-b bytes] input\nGiven a 2n \u00d7 2n spatial-domain input image, perform a spherical harmonic analysis of degree n and produce an n \u00d7 n frequency-domain output image.\n\n\n-o output\nOutput file name. Default is \"out.tif\".\n\n\n-b bytes\nOutput depth in bytes per sample. 1 requests 8-bit unsigned integer. 2 requests 16-bit unsigned integer. 4 selects selects 32-bit float.\n\n\n-h width\n-l width\n-g width\nAfter analysis, apply a Hanning -h, Lanczos -l, or Gauss -g filter window with the given width. A width of n is used if zero is given. Filter selection is documented below.\n\n\n-d\nAfter analysis, apply the diffuse convolution.\n\n\n-F\n-D\n-L\nPerform the computation using float, double, or long double values. Default is long double.\n\n\nSpherical Harmonic Synthesis\nshtrans -i [-FDL] [-o output] [-b bytes] input\nGiven an n \u00d7 n frequency-domain input image, perform a spherical harmonic synthesis of degree n and produce a 2n \u00d7 2n spatial-domain output image.\n\n\n-o output\nOutput file name. Default is \"out.tif\".\n\n\n-b bytes\nOutput depth in bytes per sample. 1 requests 8-bit unsigned integer. 2 requests 16-bit unsigned integer. 4 selects selects 32-bit float.\n\n\n-F\n-D\n-L\nPerform the computation using float, double, or long double values. Default is long double.\n\n\nStorage of Spherical Harmonics\nSpatial domain images are stored in a normal 2n \u00d7 2n raster with the top of the image corresponding to the north pole of the sphere. Frequency domain images are stored as an n \u00d7 n raster with the following layout. Degree zero is placed at the upper left and the degree increases moving toward the lower right. Negative orders are stored in the rows and positive orders in the column, putting the \"zonal\" harmonics of order zero along the diagonal. This table shows the degree and order (l, m) corresponding to each pixel of an 8 \u00d7 8 image.\n\n0,        01,       +12,       +23,       +34,       +45,       +56,       +67, 7\n1, \u201311,        02,       +13,       +24,       +35,       +46,       +57, 6\n2, \u201322, \u201312,        03,       +14,       +25,       +36,       +47, 5\n3, \u201333, \u201323, \u201313,        04,       +15,       +26,       +37, 4\n4, \u201344, \u201334, \u201324, \u201314,        05,       +16,       +27, 3\n5, \u201355, \u201345, \u201335, \u201325, \u201315,        06,       +17, 2\n6, \u201366, \u201356, \u201346, \u201336, \u201326, \u201316,        07, 1\n7, \u201377, \u201367, \u201357, \u201347, \u201337, \u201327, \u201317, 0\n\nThis is a useful layout as it casts an otherwise triangular structure into a square and distinguishes low-frequency harmonics from high-frequency. Together, these properties allow frequency domain images to be edited with common image editing software, enabling interactive spherical harmonic filtering.\nVisualization of Spherical Harmonics\nshimage [-o output] [-b bytes] [-c channels] [-l l] [-m m] [-n n]\nThis tool synthesizes an example image of a single spherical harmonic function of degree l and order m. Positive values are rendered in green and negative values in red. The resulting n \u00d7 n image is helpful in understanding the appearance and behavior of the spherical harmonics.\n\n\n-o output\nOutput file name. Default is \"out.tif\".\n\n\n-l l\nHarmonic degree. Default is 0.\n\n\n-m m\nHarmonic order. Default is 0.\n\n\n-n n\nSynthesis degree, which determines output image size.\n\n\n-b bytes\nOutput depth in bytes per sample. 1 requests 8-bit unsigned integer. 2 requests 16-bit unsigned integer. 4 selects selects 32-bit float.\n\n\n-c channels\nOutput channel count. 3 requests unsigned RGB where green implies positive values and red implies negative. 1 selects signed grayscale. In particular, b=1 c=3 generates a reasonable visualization of a single spherical harmonic, while b=4 c=1 generates the real value of that harmonic.\n\n\nHere we see the first eight degrees and orders synthesized at degree 64. They are laid out in an 8 \u00d7 8 grid as described by the table above. The zonal harmonics are clearly visible along the diagonal and the increasing frequency is obvious toward the right and down.\n\nValidation of Spherical Harmonics\nsherror [-FDL] [-n degree]\n\n\n-n degree\nAnalysis degree. Default is 1.\n\n\n-F\n-D\n-L\nPerform the computation using float, double, or long double values. Default is long double.\n\n\nThis tool quantifies the performance and precision of the implementation. It begins with white noise, defined as frequency coefficients of one for all l and m up to n. These coefficients are synthesized at a resolution of 2n \u00d7 2n, and re-analyzed up to degree n. The round trip time is measured, and the output frequency coefficients are compared with one, giving root-mean-square error and maximum error. Upon completion, a table of results in printed to stdout including\n\nthe degree n,\nthe run time in seconds,\nthe RMS error,\nthe log2 RMS error,\nthe maximum absolute error, and\nthe log2 maximum absolute error.\n\nThe log2 results indicate the number of bits to which the output agrees with the input, thus quantifying the numerical stability of synthesis together with analysis. Several runs of this tool are graphed below.\nAPI\nEach of these tools uses a spherical harmonic transformation template library given in the file sh.hpp. Templating allows the transform to be calculated using float, double, or long double built-in types. User-defined types are even supported if they overload the basic arithmetic operators and include a few of the functions of the math library.\n\n\nsht<real>::sht(int n, int c)\nConstruct a spherical harmonic transform object. All internal computation will be performed using the real type. The frequency domain representation has order n the spatial domain representation has order 2n \u00d7 2n. c gives the number of channels of both.\n\n\nThis object has public attributes for input and output. S is a 2n \u00d7 2n spatial domain image with c channels of type real. F is an n \u00d7 n frequency domain image with c channels of type real. These images overload the function operator allowing direct access to their contents.\n\n\nreal& Flm<real>::operator()(int l, int m, int k)\nGive a reference to the frequency sample at degree l, order m, and channel k suitable for reading or writing.\n\n\nreal& Sij<real>::operator()(int i, int j, int k)\nGive a reference to the spatial sample at row i, column j, and channel k suitable for reading or writing.\n\n\nWith either the spatial domain or frequency domain image set, analysis or synthesis may be performed.\n\n\nvoid sht<real>::ana()\nPerform a spherical harmonic analysis of S giving F.\n\n\nvoid sht<real>::syn()\nPerform a spherical harmonic synthesis of F giving S.\n\n\nWhen performing image IO, both S and F support bulk transfer functions that automatically cast to and from the internal floating point type to 32-bit floating point.\n\n\nvoid Flm<real>::set(const float *data, int N)\nvoid Flm<real>::get(float *data, int N)\nTransfer 32-bit floating point data into or out of the frequency domain image. The data argument must accomodate N \u00d7 N \u00d7 c 32-bit floats. N need not be a power of two, and when N does not equal n then a truncated set of coefficients is accepted or provided.\n\n\nvoid Sij<real>::set(const float *src)\nvoid Sij<real>::get(float *dst)\nTransfer 32-bit floating point data into or out of the spatial domain image. The data buffer must accommodate 2n \u00d7 2n \u00d7 c 32-bit floats.\n\n\nIn both cases, passing a null pointer to the set function initializes the contents of the buffer to zero.\nExamples\nThis segment of code shows the basic usage of the API for analysis. A square source image with power-of-two size is read from a file and a floating point destination buffer is allocated. A double precision SHT object is instanced with the desired degree and image parameters supplied to the constructor. The spatial domain input is set and the analysis is performed. Finally, the frequency domain output is acquired and written to a file. Synthesis is similar.\nsrc = image_read_float(\"input.tif\", &w, &h, &c, &b);\n\nn = w \/ 2;\n\ndst = (float *) calloc(n * n * c, sizeof (float);\n\nsht<double> T(n, c);\n\nT.S.set(src);\nT.ana();\nT.F.get(dst);\n\nimage_write_float(\"output.tif\", n, n, c, b, dst);\n\nEnvironment Mapping\nThe following examples demonstrate the application of the spherical harmonic tools to real-time environment mapping. We begin with this 32-bit floating-point 512 \u00d7 512 spherical panorama of a St. Peter's Basilica, one of several de facto standard light probes provided by Paul Debevec, unwrapped and resampled using envtools.\n\nIt's a common practice to map such an image onto a model to generate the appearance of a reflective mirror finish...\n\nor a refractive glass material with magnifying effects and more subtle reflections. Here, let me hold that up to the light for you.\n\nApplying the spherical harmonic transform to the cathedral image allows us to generalize the appearance of such materials through manipulation in the frequency domain. To begin, the image is analyzed at n=512 giving a 32-bit 256 \u00d7 256 floating point frequency domain image.\nshtrans -o st-peters-sht.tif st-peters.tif\n\nThe output looks like this.\n\nWe can achieve some interesting material effects by removing some of the high-frequency harmonics from it. However, this must be done carefully. To simply crop the frequency domain image at the upper left would corrupt the synthesis with ringing artifacts. Instead, use the -g option to apply a Gaussian filter window with a width of 64.\nshtrans -g64 -o st-peters-sht-g64.tif st-peters.tif\n\nHere's the result.\n\nSynthesize this result...\nshtrans -i -o st-peters-g64.tif st-peters-sht-g64.tif\n\ngiving this image.\n\nIt's a blurry cathedral. Critically, however, this is not a simple 2D blur like that produced by a Photoshop filter. Instead, it behaves as though the blur radius were uniform at every point on the sphere, including the poles. In contrast, a 2D blur of a spherical image would behave as if the blur became increasingly thin toward the poles, producing unsightly artifacts there.\nApplying the blurred image to the model shows an imperfect reflection and a frosted refraction, giving a much more natural and realistic material. In addition, all of these images use the width-64 blurred image as a backdrop, giving the appearance of shallow focus regardless of view direction.\n\n\nWe can take this a step further. Here we apply a Gaussian filter with a width of 32.\nshtrans -g32 -o st-peters-sht-g32.tif st-peters.tif\n\nThere's a lot less information in the frequency domain.\n\nTherefore there's much more blur in the spatial domain. With only 32 degrees of spherical harmonics, only the very low frequencies remain.\nshtrans -i -o st-peters-g32.tif st-peters-sht-g64.tif\n\n\nThe resulting reflection no longer resembles chrome, and instead takes on the character of pewter or brushed aluminum. In the pinkish glow of St. Peter's Basilica, it almost looks like copper. In comparison, the perfect reflection provided by the original unfiltered environment map looks downright fake.\n\nThe refractive effect is that of diffuse or frosted glass.\n\nDiffuse Convolution\nThese are all specular illumination effects, but the spherical harmonic transform provides a means to extend environment mapping into diffuse illumination, as described by Ramamoorthi and Hanrahan in their 2001 SIGGRAPH paper An Efficient Representation for Irradiance Environment Maps. In this work, Ramamoorthi and Hanrahan note that diffuse illumination is essentially convolution with a cosine-weighted hemisphere, and demonstrate that the resulting irradiance need be represented using only three degrees of spherical harmonics.\nWe can apply the diffuse convolution using the -d option.\nshtrans -d -o st-peters-sht-dif.tif sh-peters.tif\n\nIndeed, only the top-left 3 \u00d7 3 block of pixels contains much data. There are a few dim pixels elsewhere, but the literature demonstrates that eliminating these will result in an average error of at most 3%.\n\nThere's so little information here that we could validly crop that image down to 4 \u00d7 4 (a power of two), resulting in an 8 \u00d7 8 synthesis with no significant loss of information. But for the sake of consistency with the other examples...\nshtrans -i -o st-peters-dif.tif sh-peters-sht-dif.tif\n\nThe resulting irradiance environment map is not just very heavily blurred, it actually gives a weighted sum of every input pixel visible at every possible orientation. The sphere map represents the total light falling upon every point on a sphere from every direction. In this example, St. Peter's is well lit from above, and diffuse reflection of the marble gives a pink ambient glow.\n\nWhen rendering, a single texture reference is made along the object normal. This returns a value equivalent to the usual diffuse lighting calculation, but done for every light source in the room simultaneously. The resulting material is a perfectly matte white, like unglazed ceramic.\n\nCombining this with one of the reflection maps above gives a specular effect, like glazed ceramic, the material of the original Utah teapot.\n\nOf course, color and gloss texture maps and normal maps can be combined with these blurred and convolved environment maps, giving very rich and extremely realistic materials.\nHere's a summary image giving all of the examples together. Click to enlarge.\n\nFilter Selection\nThe choice of filter is an important one. Ringing artifacts will always arise from a spherical harmonic synthesis with an equirectangular projection. This is because the equirectangular projection badly matches the true distribution of information on the sphere. In particular, the spatial resolution near the poles is far higher than near the equator, so any reasonable set of spherical harmonic coefficients will be band-limited at high latitude. The available filters will handle different types of images with varying levels of success. The following examples are intended to help clarify this, but ultimately, experience gained through trial and error are most valuable.\nThe three filter windows are graphed here for n=256. The Gauss window, in red, has the familiar shape of the bell curve, dropping off rapidly and approaching zero smoothly. The Hann (a.k.a. Hanning) window, in green, is a cosine wave with equal balance. The Lanczos window (in blue) is the first lobe of the sinc function, dropping off slowly and meeting zero abruptly.\n\nWhen the filter width is less than n, the Gauss and Hann windows still clamp out at zero. However, the Lanczos window will oscillate about zero as one would expect from the sync function.\nThe following example uses a 512 \u00d7 512 image of a 16 \u00d7 16 black-white checker pattern. With its high contrast, it represents a worst-case scenario for 8-bit images, and will demonstrate the best choice for round-trip frequency-domain operations upon common spherical images.\nThe checker input is analyzed and re-synthesized four times. Image A shows the center of the input, which covers the equator where the spherical harmonics match the information density in the image well. Image B shows a synthesis of an unfiltered set of coefficients. The ringing is obvious even in this easy area. Images C, D, and E shows the Gauss, Hann, and Lanczos filters, with each sharper than the last.\n\nHere is the same set of images, showing instead a portion of the checker pattern in the troublesome region near the north pole. Image B, with no filtering, is dominated by ringing artifacts. The Gauss, Hann, and Lanczos filters are, again, increasingly sharp. Ringing is generally under control, though slightly apparent in the Lanczos image when magnified.\n\nWe can infer from this that some form of filtering is necessary. The Lanczos filter preserves the sharpness in the input best, and is probably the go-to choice when doing frequency-domain manipulations of spherical data sets. Blurring is expected and usually even desirable near the poles, especially if the filtered output is to be mapped onto a sphere for real-time texture mapping. Under such circumstances the Gauss filter might produce the most visually appealing results.\nThings change when we shift to high dynamic range inputs, such as the light probe of St. Peter's Basilica used in the teapot renderings, above. The following images show the results of each filter applied with a width of 32, cropped at top of the image.\n\nThe Gauss filtering in image A gives a nice, smooth blur. The Hann window, image B, does not handle extremely bright light sources well at all, and produces a severe overshoot. The Lanczos window, image C, produces a surprising kind of bokeh effect with high-frequency ringing produced by the many lobes of the tail of the sinc. It's interesting, but not what we're looking for. For high dynamic range light probes, stick with Gauss.\nIn general, there will always be a trade-off between sharpness and ringing. The type of the data and its intended usage will determine how this trade-off is best made.\nTests\nConformance\nAs a basic eye-ball level test of conformance, the shtrans tool was used to synthesize the EGM 2008 data set, the Earth Gravitational Model consisting of 2190 degrees of spherical harmonic coefficients. The 8192 \u00d7 8192 grayscale output was gradient-mapped using GIGO and scaled down using Photoshop. It matches.\n\nPrecision\nThe sherror tool tests the numerical precision of the spherical harmonic transform implementation by synthesizing and re-analyzing white noise. The task is performed at degrees from n= 21 to 212 using single, double, and long double precision floating point values. The output is compared with the input and the degree to which the two match is quantified.\nThe software was compiled for OSX 10.8 using g++ 4.7.2. The test hardware used 2 \u00d7 2.0 GHz eight-core Intel Xeon E5 processors. Data shown here was collected using 16 OpenMP threads.\nHere are the round trip times, with time in seconds plotted on a log-10 scale. Single precision floating point performance is shown in red, double in green, and long double in blue. G++'s long double is in fact an Intel-native 80-bit float, which is the same type that implements single and double precision calculation, so performance parity is not a big surprise. Their stored size varies though, so these results would indicate that the process is not cache-bound on this hardware. The computation is effectively instantaneous when n is smaller than 6 and the resolution of the timer was not sufficient to demonstrate any distinction.\n\nHere we see the agreement between the input and the round trip output. Specifically, this graph shows the average number of bits to which each pair of values coincides, as given by the base-2 log of their root-mean-square difference. Single precision floating point (in red) gives at most 24 bits of useful precision and sees a precipitous drop in stability past n=24, with a total failure past n=27. Double precision (in green) gives 50 bits and sees a similar limit at n=27. Long double (in blue) continues to n=211. This places an upper bound on the input size that this utility can usefully handle when instanced using native data types: 2048.\n\nHere is the base-2 log maximum difference. Again, it shows the number of binary digits to which the input and output values agree. The results are largely the same as the RMS, and this test is done merely to demonstrate that the RMS does not average away any outliers.\n\nThese numbers show that the current implementation is useful in a number of circumstances, but not as powerful as we would like. The use of extended-range arithmetic (Lozier & Smith) will take us beyond the inevitable failure of long double precision, and a GPU-cluster implementation will extend the performance envelope by sheer brute force.\nOn the bright side, the spherical harmonic transform is a trivially parallelizable process that fully benefits from all floating point capacity placed at its disposal. He we see the speed-up of the round trip test run at n=29 in double precision with from one to 32 threads. Speed-up is linear to 16 threads, with 90% efficiency, as this is the number of real processor cores in the test system. They're hyperthreaded cores, and modest gains can be achieved by overcommiting them, though this is not reliable.\n\n","421":"Node-mysql-libmysqlclient \nAsynchronous MySQL binding for Node.js using libmysqlclient.\nThis module has been tested with Node.js versions 0.8.26, v0.10.25 and 0.11.10\nOverview\nThese bindings provides all general connection\/querying functions from the MySQL C API,\nand partial support for prepared statements. Connect, query and fetchAll are asynchronous.\nThis module also includes support for asynchronous querySend from internals of libmysqlclient.\nI started this project in 2010 when Node.js was growing. Ryan had plans to write this binding as part of GSoC.\nIt is now used by many projects and has more than 10 contributors,\nwho are listed in the AUTHORS file.\nI also maintain the Node.js MySQL bindings benchmark which shows how mysql-libmysqlclient performs.\nNode-mysql-libmysqlclient's source code is available in the Github repo and you can report issues at the project tracker.\nVisit the module site for API docs and examples. You can also read some extra information in wiki.\nDependencies\nTo build this module you must install the libmysqlclient library and the development files for it.\nmysql_config is used to determine the paths to the library and header files.\nTo install these dependencies, execute the commands below for the OS you're running.\nFor CentOS:\n#> yum install mysql-devel\n\nFor openSUSE:\n#> zypper install libmysqlclient-devel\n\nFor Debian-based systems\/Ubuntu:\n#> apt-get install libmysqlclient-dev\n\nAlternatively, you can use aptitude for Debian-based systems.\nPlease refer to your system's documentation for more information and feel free to send me a patch for this readme.\nInstallation\nYou can install this module via NPM:\n$> npm install mysql-libmysqlclient\n\nYou can also build latest source code from repository.\nPlease refer to the developers documentation for more information.\nContributing\nThis module is written in collaboration with many peoples listed on GitHub contributors page.\nList of authors ordered by first contribution also available.\nIf you are interested in wide MySQL usage in Node.JS applications,\nleave your comments to the code.\nTo contribute any patches, simply fork this repository using GitHub\nand send a pull request to me. Thanks!\nAll information about development use and contribution is placed in the DEVELOPMENT file.\nUsers and related projects\nThis module is used by Taobao guys\nfor their distributed MySQL proxy Myfox-query module.\nThere is long time developed Node.js ORM library called noblerecord.\nIt is inspired by Rails and widely used by Noblesamurai.\nIf you are looking for lightweight Node.js ORM on top of this module,\ntry mapper by Mario Gutierrez.\nLicense\nNode-mysql-libmysqlclient itself is published under MIT license.\nSee license text in LICENSE file.\n","422":"grunt-env \nSpecify an ENV configuration as a task, e.g.\ngrunt.registerTask('dev', ['env:dev', 'lint', 'server', 'watch']);\ngrunt.registerTask('build', ['env:build', 'lint', 'other:build:tasks']);\n\nGetting Started\nInstall this grunt plugin next to your project's grunt.js gruntfile with: npm install grunt-env\nThen add this line to your project's grunt.js gruntfile:\ngrunt.loadNpmTasks('grunt-env');\nConfiguration\n  env : {\n    options : {\n \t\/\/Shared Options Hash\n    },\n    dev : {\n      NODE_ENV : 'development',\n      DEST     : 'temp'\n    },\n    build : {\n      NODE_ENV : 'production',\n      DEST     : 'dist',\n      concat   : {\n        PATH     : {\n          'value': 'node_modules\/.bin',\n          'delimiter': ':'\n        }\n      }\n    },\n    functions: {\n      BY_FUNCTION: function() {\n        var value = '123';\n        grunt.log.writeln('setting BY_FUNCTION to ' + value);\n        return value;\n      }\n    }\n  }\nUsing external files\nYou can specify environment values in INI, JSON or YAML style and load them via the src option.\n  env : {\n    dev : {\n      src : \"dev.json\"\n    },\n    prod: {\n      src: \"settings.yaml\"\n    }\n    heroku : {\n      src : \".env\"\n    }\n  }\nUsing envdir\nYou can specify files to read environment variables from, similar to the daemontools envdir utility.\n  env : {\n    dev : {\n      src : [\"envdir\/*\"],\n      options: {\n        envdir: true\n      }\n    }\n  }\nDynamic ENV configuration\nThe following directives can be specified in the options to alter the environment in more specific ways\n\nadd\n\nThis will add the variables only if they don't already exist\n\n\nreplace\n\nWill replace the variable with the value specified\n\n\nunshift\n\nWill prepend the value to the variable specified, optionally specifying a 'delimiter'\n\n\npush\n\nSame as unshift, but at the end of the value.\n\n\nconcat\n\nFunctionally same as push, added for readability\n\n\n\nyourtask : {\n  USER : 'you',\n  PATH : '\/bin:\/usr\/bin'\n\n  options : {\n    add : {\n      VERBOSE : '1' \/\/ will only be added if VERBOSE isn't already set\n    },\n    replace : {\n      USER : 'me'\n    },\n    push : {\n      PATH : {\n        value : '~\/bin',\n        delimiter : ':'\n      }\n    },\n    unshift : {\n      PATH : '\/sbin:'\n    }\n  }\n}\n\nEnvironment-specific configuration\nIn order to configure your tasks based on the environment, you need to define a task and use templates:\n\ngrunt.initConfig({\n  env: {\n    dev: {\n      MY_CONST: 'a'\n    },\n    prod: {\n      MY_CONST: 'b'\n    }\n  },\n  myTask: {\n    options: {\n      myOpt: <%= MY_CONST %>\n    }\n  }\n});\n\ngrunt.registerTask('loadconst', 'Load constants', function() {\n    grunt.config('MY_CONST', process.env.MY_CONST);\n});\n\ngrunt.registerTask('default', [\n    'env:dev',\n    'loadconst',\n    'myTask'\n]);\n\n\nImportant note on data types\nEnvironment variables are strings only. If you attempt to assign complex objects, they will be converted to strings.\nContributing\nIn lieu of a formal styleguide, take care to maintain the existing coding style. Add unit tests for any new or changed functionality. Lint and test your code using grunt.\nRelease History\n\n0.4.0 Removed automatic parse, added ability to add ini or json style src files\n0.3.0 Automatically parses .env files now\n0.2.1 fixed npm install\n0.2.0 grunt 0.4.0 support, simplified\n0.1.0 Initial release\n\nLicense\nLicensed under the Apache 2.0 license.\nAuthor\nJarrod Overson\n","423":"Chart\nAscii bar chart for nodejs.\n\nInstallation\n$ npm install jstrace\/chart\n\nExample\nWhen data exceeds the available width the data will \"roll\" to the tail-end\nof the array. This may become an option in the future, but that's the default\nbehaviour for now ;)\nvar chart = require('chart');\nvar clear = require('clear');\n\nvar data = [1, 2, ...];\n\nclear();\nconsole.log(chart(data, {\n  width: 130,\n  height: 30,\n  pointChar: '\u2588',\n  negativePointChar: '\u2591'\n}));\nLicense\nMIT\n","424":"webpack ES6 demo\nA small demo project that shows how to use webpack for client-side development in ECMAScript 6.\nInstallation\n\nInstall  node\nrun npm install\n\nUsage\n\nnpm run watch to start Webpack in watch mode - will recompile when you change a file.\nopen index.html in a browser.\nChange or add files in es6 folder. main.js is the entry point.\nReload the browser when you have made a change.\n\n","425":"Analytics\n\nComplete Google Analytics, Mixpanel, KISSmetrics (and more) integration for Meteor\n\nNOTE: This pkg is no longer being actively maintained, if you'd like to maintain this pkg please express interest by opening an issue.\nOK GROW! analytics uses a combination of the browser History API, Meteor's accounts package and Segment.io's analytics.js to automatically record and send user identity and page view event data from your Meteor app to your analytics platforms.\nTable of Contents\n\nBackground\n\nAnalytics 3.0+\nAnalytics 2.1.0+\nPre Meteor 1.3.1\n\n\nInstall\nUsage\n\nCurrently Supported Analytic Services\nPage views\nRouters\n\nReact Router\nFlow Router\nIron Router\n\n\nDisabling automatic page views\nLog signin\/signout\nEvent tracking\nTrack visitor scrolling\nBrowser Policy\n\nExample browser policy\n\n\n\n\nDebugging\n\nURL Whitelisting on Android Devices\nAd-blocker\n\n\nExample React, Flow and Iron Router Apps\nMaintainers\nContributing\nLicense\n\nBackground\nAnalytics 3.0+\nIn version 3.X of this package, the automatic page view tracking is handled by our new router-agnostic @okgrow\/auto-analytics NPM package, which can be used by any JavaScript application whether using Meteor or not. This package adds automatic user identification by using hooks in the Meteor accounts package and building on Segment.io's analytics package through the @okgrow\/auto-analytics package.\nAnalytics 2.1.0+\nOur Analytics package has been rewritten to be router agnostic. You should be able to use this package with any router that you use with Meteor app. We have tested and used our Analytics package with iron-router, flow-router, and react-router. You can view and test this out in our iron-router, flow-router and react-router example apps located in the examples folder.\nNOTE: A fundamental change that may affect some applications is that we no longer look for or use the router's route name when logging page views. Instead we use document.title. This may affect applications that do not change or set their document.title for each screen or page of their application. The simplest solution is to simply set document title like this document.title = \"My new title\"; for each screen or page in your application. If you are using flow router or iron router you can remain at okgrow:analytics@2.0.1 to keep using the name of the route for your analytic events.\nPre Meteor 1.3.1\nFor Meteor Apps older than v1.3.1, please use v1.0.9 of this package. Going forward this package will officially only be supporting Meteor Apps >= v1.3.1\nInstall\nIn your Meteor project folder, run:\nmeteor add okgrow:analytics\nUsage\nThis package will automatically configure the underlying @okgrow\/auto-analytics package using Meteor.settings.public.analyticsSettings. In Meteor you typically specify your settings using a settings.json file:\n{\n  \"public\": {\n    \"analyticsSettings\": {\n      \/\/ Add your analytics tracking ids here (remove this line before running)\n      \"Google Analytics\" : {\"trackingId\": \"Your tracking ID\"},\n      \"Amplitude\"        : {\"apiKey\": \"...\"},\n      \"Chartbeat\"        : {\"uid\": \"...\"},\n      \"comScore\"         : {\"c2\": \"...\"},\n      \"HubSpot\"          : {\"portalId\": \"...\"},\n      \"Intercom\"         : {\"appId\": \"...\"},\n      \"Keen IO\"          : {\"projectId\": \"...\", \"writeKey\": \"...\"},\n      \"KISSmetrics\"      : {\"apiKey\": \"...\"},\n      \"Mixpanel\"         : {\"token\":  \"...\", \"people\": true},\n      \"Quantcast\"        : {\"pCode\": \"...\"},\n      \"Segment.io\"       : {\"apiKey\": \"...\"}\n    }\n  }\n}\nAnd run your app with that settings file as follows:\nmeteor --settings settings.json\nSee the @okgrow\/auto-analytics package for more details on configuration.\nCurrently Supported Analytic Services\nSee the @okgrow\/auto-analytics package for up-to-date details of supported analytics services.\nPage views\nSee the @okgrow\/auto-analytics package for details on page view tracking. In short, that package uses the browser History API to automatically track page views.\nSince the History API is used to automatically track page views, document.title is used instead of the router's route name as the default page name.\nIf you rely on your router's route name for the page name in page view events, you can easily set document.title programming using the router's route name. Here are examples of how to do this with React Router, Flow Router and Iron Router:\nRouters\nThis package is router agnostic. It will work with any router, and by default it uses the document.title as the page name for reporting to your analytics service.\nReact Router\nIn your router setup, add a name property to your routes:\n<Router history={ browserHistory }>\n  <Route path=\"\/\" name=\"Home\" component={ App } \/>\n  <Route path=\"\/one\" name=\"One\" component={ App } \/>\n  <Route path=\"\/two\" name=\"Two\" component={ App } \/>\n  <Route path=\"\/three\" name=\"Three\" component={ App } \/>\n<\/Router>\nNOTE The current route is passed in as a property named route to your component.\nThen, in the render() function of your main layout component, using a package like react-document-title:\nrender() {\n  return (\n    <DocumentTitle title={this.props.route.name}>\n      ...\n    <\/DocumentTitle>\n  );\n}\nFlow Router\nTemplate.mainLayout.onRendered(function() {\n  Tracker.autorun(() => {\n    document.title = FlowRouter.getRouteName();\n  });\n});\nIron Router\nTemplate.mainLayout.onRendered(function() {\n  Tracker.autorun(() => {\n    document.title = Router.current().route.getName();\n  });\n});\nDisabling automatic page views\nTo disable automatic page view tracking change Meteor.settings as shown below then manually log a page view by calling analytics.page('page name'):\n{\n  \"public\": {\n    \"analyticsSettings\": {\n      \/\/ Disable autorun if you do not want analytics running on every route (remove this line before running)\n      \"autorun\"  : false\n    }\n  }\n}\nLog signin\/signout\nIf you have the accounts package installed, this package will automatically track when a user logs in and logs out. Logging in will call identify on the user and associate their Meteor.userId to their previous anonymous activities.\nEvent tracking\nSee the @okgrow\/auto-analytics package for details on event tracking. In short, track any event by calling the analytics.track() function:\nanalytics.track(\"Bought Ticket\", {\n  eventName: \"Wine Tasting\",\n  couponValue: 50,\n});\nTrack visitor scrolling\nJosh Owens' article, Google Analytics events, goals, and Meteor.js, goes over a great way to capture how far a visitor has scrolled down a page.\nBrowser Policy\nIf your project uses the Browser Policy package, we've included the Google Analytics and MixPanel domains in our browser policy configuration. Any additional services you add will need to be added to your browser policy config as well.\nExample browser policy\nBrowserPolicy.content.allowOriginForAll(\"www.google-analytics.com\");\nBrowserPolicy.content.allowOriginForAll(\"cdn.mxpnl.com\");\nIf your project doesn't use the Browser Policy package, don't worry, it won't affect your usage.\nDebugging\nTo log package activity to the console for debugging purposes, turn on debugging in the console:\n> analytics.debug()\nTurn debug logging off with:\n> analytics.debug(false)\nURL Whitelisting on Android Devices\nIf your app is running on Android devices you will probably have to add the cordova-plugin-whitelist package and set access rules in your mobile-config.js for all URLs of the platforms that you are using.\nExample for Intercom:\nApp.accessRule('https:\/\/js.intercomcdn.com\/*');\nApp.accessRule('https:\/\/static.intercomcdn.com\/*');\nApp.accessRule('https:\/\/api-iam.intercom.io\/*');\nApp.accessRule('https:\/\/widget.intercom.io\/*');\nApp.accessRule('https:\/\/nexus-websocket-a.intercom.io\/*');\nApp.accessRule('https:\/\/nexus-websocket-b.intercom.io\/*');\n\nTo find all the necessary URLs for your project, build your production app and install it on your Android device. Then connect it via USB and open the Android Studio Device Monitor (Tools >> Android Device Monitor >> LogCat). Perform a relevant action and then search for \"whitelist\". It should a show message for each URL that was blocked.\nAd-blocker\nWhen running your Meteor app in \"development mode\" ad-blocking web-browser extensions may block the okgrow:analytics package due to the word \"analytics\" in the package name. This only occurs when running Meteor in \"development mode\" because files are not bundled together and minified. To work around this issue you can disable your ad-blocker when running in development mode.\nTo test that application with an ad-blocker, run your Meteor app in production mode with this command:\nmeteor run --production --settings settings.json\nNOTE If an ad-blocker is enabled the expected behavior is that analytic events will not be received. You'll see an error message in your console reporting the events being blocked.\nExample React, Flow and Iron Router Apps\nWhile page view event tracking is router agnostic, the examples directory contains example apps using the three most common routers used in Meteor apps: React Router, Flow Router and Iron Router. These apps can be run from within their respective directories with:\nmeteor npm start\nMaintainers\nThis is an open source package. We hope to deal with contributions in a timely manner, but that's not always the case. The main maintainers are:\n@okgrow\nFeel free to ping if there are open issues or pull requests which are taking a while to be dealt with!\nAdditional Notes\nThere has been at least one report of Google Analytics taking over a day in between GA account creation and any data showing up on the actual GA dashboard. See this issue for details. You may just need to wait if nothing's showing up.\nContributing\nIssues and Pull Requests are always welcome.\nPlease read our contribution guidelines.\nIf you are interested in becoming a maintainer, get in touch with us by sending an email or opening an issue. You should already have code merged into the project. Active contributors are encouraged to get in touch.\nPlease note that all interactions in @okgrow's repos should follow our Code of Conduct.\nLicense\nReleased under the MIT license \u00a9 2015-2017 OK GROW!.\n","426":"taiga-ncurses\n\n\n\n\nA NCurses client for Taiga.\n\nProject status\nCurrently on design phases: This project was a proof of concept to try to create a curses client\nfor Taiga in the 6th PiWeek. It isn't finished yet and currently it isn't\nfeature complete. You can see some screenshots at https:\/\/github.com\/taigaio\/taiga-ncurses\/issues\/4#issuecomment-57717386\n\nSetup development environment\nJust execute these commands in your virtualenv(wrapper):\n$ pip install -r dev-requirements.txt\n$ python setup.py develop\n$ py.test               # to run the tests\n$ taiga-ncurses         # to run the app\n\nObviously you need the taiga backend and, if you don't fancy living in darkness,\nyou can use the taiga web client, sometimes. :P\nNote: taiga-ncurses only runs with python 3.3+.\n\nCommunity\nTaiga has a mailing list. Feel free to join it and ask any questions you may have.\nTo subscribe for announcements of releases, important changes and so on, please follow\n@taigaio on Twitter.\n","427":"Welcome to Brownie!\n\nHave you ever started a new project and implemented this little function\nfoo or this datastructure bar you already implemented for another project?\nEver wondered why a specific feature is not in the standard library\nalready?\nWanted to use that new datastructure but you are still stuck with this\nancient Python version or are just not willing or able to switch to\nPython 3.x, yet?\nAnd most importantly were too lazy to implement this datastructure which\nwould be more appropriate to use?\n\nBrownie wants to solve these problems by providing all these small things\nwell documented, well tested and most importantly right now when you need\nit.\nTake a look at the documentation for further information, visit Github\nfor development or issue tracking or fetch the latest development version.\n\nInstallation\nBrownie runs on Python 2.5-2.7 (tested on CPython and PyPy). In order to\ninstall it simply run pip install brownie or easy_install brownie,\nshould you prefer the latter, to get the latest stable version.\nIf you really need to, you can also install the development version of\nBrownie with pip install brownie==dev. However I do not recommend it.\n\nDevelopment\nShould you want to participate in the development, fork the repository on\nGithub and take a moment to read :ref:`contributing`, for a couple of\ngeneral and not so general guidelines you should follow.\n","428":"YUNOMI: Y U NO MEASURE IT\n\n\nAs in:\n\nIt has performance implications, Y U NO MEASURE IT!?\nYunomi provides insights to the internal behavior of an application, providing useful statistics and metrics on selected portions of your code.\nIt\u2019s a Python port of the core portion of a Java Metrics library by Coda Hale.\nStop guessing, start measuring:\n$ pip install yunomi\n\nCore Features\n\nCounter\nSimple interface to increment and decrement a value.\nFor example, this can be used to measure the total number of jobs sent to the queue, as well as the pending (not yet complete) number of jobs in the queue.\nSimply increment the counter when an operation starts and decrement it when it completes.\n\nMeter\nMeasures the rate of events over time.\nUseful to track how often a certain portion of your application gets requests so you can set resources accordingly.\nTracks the mean rate (the overall rate since the meter was reset) and the rate statistically significant regarding only events that have happened in the last 1, 5, and 15 minutes (Exponentially weighted moving average).\n\nHistogram\nMeasures the statistical distribution of values in a data stream.\nKeeps track of minimum, maximum, mean, standard deviation, etc.\nIt also measures median, 75th, 90th, 95th, 98th, 99th, and 99.9th percentiles.\nAn example use case would be for looking at the number of daily logins for 99 percent of your days, ignoring outliers.\n\nTimer\nA useful combination of the Meter and the Histogram letting you measure the rate that a portion of code is called and a distribution of the duration of an operation.\nYou can see, for example, how often your code hits the database and how long those operations tend to take.\n\nExamples\n\nDecorators\nThe simplest and easiest way to use the yunomi library.\n\nCounter\nYou can use the count_calls decorator to count the number of times a function is called.\n>>> from yunomi import counter, count_calls\n>>> @count_calls\n... def test():\n...     pass\n...\n>>> for i in xrange(10):\n...     test()\n...\n>>> print counter(\"test_calls\").get_count()\n10\n\nTimer\nYou can use the time_calls decorator to time the execution of a function and get distributtion data from it.\n>>> import time\n>>> from yunomi import timer, time_calls\n>>> @time_calls\n... def test():\n...     time.sleep(0.1)\n...\n>>> for i in xrange(10):\n...     test()\n...\n>>> print timer(\"test_calls\").get_mean()\n0.100820207596\n\nRequirements\nYunomi has no external dependencies and runs on PyPy and Python 2.6, 2.7, and 3.3.\n","429":"About\nA simple and efficient paginator.\n\nJinja2\nJinja2 is supported via Coffin:\n{% with paginate(request, my_queryset) as results %}\n  {{ results.paging }}\n  {% for result in results.objects %}\n    {{ result }}\n  {% endfor %}\n  {{ results.paging }}\n{% endwith %}\n\n\nDjango\nDjango templatetags require django-templatetag-sugar:\n{% load paging_extras %}\n\n{% paginate my_queryset from request as results %}\n{{ results.paging }}\n{% for result in results.objects %}\n  {{ result }}\n{% endfor %}\n{{ results.paging }}\n\n","430":"GoogleClosureCompiler\nMakes integrating the Google JavaScript Compiler with your Rails deployment process dead simple. Read why compressing your JavaScript is important\nBoth the Google Closure Compiler API and Application are supported. Sensible defaults are provided.\nInstalling the plugin\nscript\/plugin install git:\/\/github.com\/mkelly12\/google_closure_compiler.git\n\nRequirements\nAny version of Rails 2.x; including Rails 2.3.4 and 2.1.2.\nSo how does it work?\nRead how this integrates with your workflow\nThe plugin uses the Google Closure Compiler to optimize JavaScript files cached by Rails.\nAnytime you use the javascript_include_tag with the :cache => true or :cache => 'bundle_name' the resulting JavaScript file will be compiled. Read more about Rails asset caching\nYou will also need this in your production.rb (and in your development.rb only when testing):\nconfig.action_controller.perform_caching = true\n\nKeep in mind that cached files are saved to your public directory and only generated when needed. If you forget to delete them in the development environment they can lead to some serious headaches. It's a good practice to use a naming scheme like 'cache\/bundle_name' so you can easily remove the cached files and add ignore rules to your version control.\nHow does it work with the Google Closure Compiler?\nThere are three ways to integrate with the Google Closure Compiler which are attempted in the following order:\n\nIf you have the Closure Compiler Application properly installed (yes we check this) then that is always used.\nIf the Application is not detected then the API is used with your JavaScript embeded in the POST data.\nIf your JavaScript file is larger then POST data will allow then a link to your JavaScript is sent to the API. If your host is not specified or not reachable by the Google service then no compilation is performed.\n\nApplication\nThe preferred method is to use the compile.jar file which is included in this plugin.\nYou will need the Java Runtime Environment version 6.\nIf you are on OS X with the latest updates you will need to specify the path to the 1.6 JRE since \/usr\/bin\/java still point to 1.5.\nAdd the following to \/config\/google_closure_compiler.yml\ndevelopment:\n\tjava_path: '\/System\/Library\/Frameworks\/JavaVM.framework\/Versions\/1.6\/Commands\/java'\n\nAPI with code in request\nYou don't need anything besides an outgoing internet connection. However you JavaScript needs to fit in the POST data.\nAPI with code urls\nThis works well if your server is accessible to the world and you specified your host name using the following:\nconfig.action_mailer.default_url_options = { :host => HOST_NAME }\n\nThe only limitation is that cached JavaScript files larger then 500k cannot be processed with this method.\nUsing the Google Closure Library\nIf you are using the Google Closure Library use the following view helper to include your Closure JavaScript:\nclosure_include_tag \"wooly_zurbian.js\"\n\nIn this example wooly_zurbian.js is a JavaScript file in \/public\/javascripts\/.\nThis helper inserts the Google Closure Library base.js and events.js scripts when perform_caching is false. When perform_caching is true these files are not required since they are added using the calcdeps.py script.\nThe Google Closure Library is required and is assumed to be in public\/javascripts\/closure. If you put it somewhere else specify the relative path from \/public\/javascripts\/ using closure_library_path in google_closure_compiler.yml.\nThe wooly_zurbian.js script is inserted and cached in \/public\/javascripts\/cache\/closure\/wooly_zurbian.js.\nUsing the Dependency Calculation Script\nIf any of your cached JavaScript files contain a call to goog.require() then that cached file will be expanded using the calcdeps.py script. This requires Python 2.4 or greater and the Google Closure Library (see Using the Google Closure Library).\ncalcdeps.py takes each goog.require() call and replaces it with the required libraries. Read more about calcdeps.py and why it is important when using the Google Closure Library.\nADVANCED_OPTIMIZATIONS is used for the compilation of cached file that are expanded with calcdeps.py regardless of what is specified in the \/config\/google_closure_compiler.yml file.\nFAQ\nCan I change the compilation level?\nYes, you can specify it in the config\/google_closure_compiler.yml file.\ndevelopment:\n\tcompilation_level: 'ADVANCED_OPTIMIZATIONS'\n\nThe default is SIMPLE_OPTIMIZATIONS. Other options are WHITESPACE_ONLY and ADVANCED_OPTIMIZATIONS.\nMake sure you read the documentation on Advanced Optimizations before enabling them.\nWhat happens if there is an error or the API is down?\nIf all compilation methods fail then the original JavaScripts are used in the bundles.\nDoes this play nice with Smurf?\nIt sure does. If you have Smurf installed then CSS minification works as expected and JavaScript files are processed by both Smurf and the Google Closure Compiler.\nCopyright (c) 2009 Matt Kelly - ZURB, released under the MIT license\n","431":"Japont\nDynamic Subsetting System for CJK fonts.\n\u30d5\u30a9\u30f3\u30c8\u3092\u8efd\u91cf\u5316\u3057\u3066\u914d\u4fe1\u3059\u308b\u305f\u3081\u306b\uff0c\u52d5\u7684\u306b\u5fc5\u8981\u6587\u5b57\u3092\u62bd\u51fa\u3057\u305fWeb\u30d5\u30a9\u30f3\u30c8\u3092\u751f\u6210\u3059\u308b\u30b7\u30b9\u30c6\u30e0\n\u6bd4\u8f03\u7684\u7c21\u5358\u306b\u65e5\u672c\u8a9eWeb\u30d5\u30a9\u30f3\u30c8\u3092\u5c0e\u5165\u3067\u304d\u307e\u3059\n\u26a0\ufe0f Notice\nThis branch is under developing.\nIf you use Japont, please access with-fonttools branch.\nDEMO\nDEMO\nInstallation\nWIP\nUsage\nWIP\nEnvironment variables\n\n\n\nENVS\nDefault\nNote\n\n\n\n\nX_ROBOTS_TAG\nnoindex, nofollow\nX-Robots-Tag Header\n\n\nSERVER_OWNER\nAnonymous\nServer owner's name\n\n\nFONTS_DIR_PATH\n.\/fonts\nFolder path where fonts are\n\n\nZIP_COMPRESSION_TYPE\nZIP_STORED\n\n\n\nBIND_IP\n0.0.0.0\n\n\n\nPORT\n8000\n\n\n\n\nContribution\n\nFork it ( http:\/\/github.com\/Japont\/Japont-core\/fork )\nCreate your feature branch ( git checkout -b my-new-feature )\nCommit your changes ( git commit -am 'Add some feature' )\nPush to the branch ( git push origin my-new-feature )\nCreate new Pull Request\n\nLICENSE\nApply the Apache License version 2.0.\nApache License version 2.0 \u3092\u9069\u7528\u3057\u307e\u3059\uff0e\nCopyright 2015- 3846masa\nAuthor\n 3846masa\n","432":"What about running nose with a smarter interactive debugger?\nUse this and never risk yourself forgetting import ipdb; ipdb.set_trace() in your code again!\nThis plugin is 99.99% based on nose's builtin debug plugin.\nIf you have any ideas about how to improve it, come and fork the code at http:\/\/github.com\/flavioamieiro\/nose-ipdb\nInstall\npip install ipdbplugin\n\nUsage\nTo drop into ipdb on errors:\nnosetests --ipdb\n\nTo drop into ipdb on failures:\nnosetests --ipdb-failures\n\nLicense\nGNU Lesser General Public License\nAuthors\n\nBernardo Fontes (falecomigo@bernardofontes.net)\nFl\u00e1vio Amieiro (amieiro.flavio@gmail.com)\nHenrique Bastos (henrique@bastos.net)\n\n","433":"django-compat\n\n\n\n\nForward and backwards compatibility layer for Django 1.4, 1.7, 1.8, 1.9, 1.10 and 1.11\nConsider django-compat as an experiment based on the discussion on reddit. Let's see where it goes.\nWhat started as an experiment based on this discussion on reddit has proven to be true in real life.\ndjango-compat is under active development. To learn about other features, bug fixes, and changes, please refer to the changelog.\nWho uses django-compat\nTwo popular examples of open source reusable app that uses django-compat are django-hijack and django-background-tasks.\nWant to have yours listed here? Send us a PR.\nWhy use django-compat\n\nBe able to use the LTS versions of Django and support newer versions in your app\nUse features from newer Django versions in an old one\nManage and master the gap between different framework versions\n\nHow to use django-compat\nInstall compat from the PyPI or download and install manually. All relevant  releases are listed here under releases.\nUsing one of the compatible objects is easy. For example\nfrom compat import patterns, url\n\nurlpatterns = patterns('ABC.views',\n\t\turl(r'^abc\/$', 'abc', name='abc-link'),\n...\n\nSee a full example here.\n\n\n\ndjango-compat is free software. If you find it useful and would like to give back, please consider to make a donation using Bitcoin or PayPal. Thank you!\n\n\n\nCompatible objects\n\n\n\nCompatible object\nSpecifically tested\n1.8\n1.9\n1.10\n1.11\nNotes\n\n\n\n\nBytesIO\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nDjangoJSONEncoder\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nEmailValidator\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nGenericForeignKey\n\u2716\ufe0f\n\u2714\ufe0f\n\u274c\n\u274c\n\u274c\n\n\n\nmodels.GenericForeignKey\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nHttpResponseBase\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nJsonResponse\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nLocaleRegexProvider\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nLocaleRegexURLResolver\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nNoReverseMatch\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nRegexURLPattern\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nRegexURLResolver\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nResolver404\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nResolverMatch\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nSortedDict\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nStringIO\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nURLValidator\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nVariableNode\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nView\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nadd_to_builtins\n\u2716\ufe0f\n\u2714\ufe0f\n\u274c\n\u274c\n\u274c\n\n\n\nadmin_utils\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\natomic\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nclean_manytomany_helptext\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nclear_url_caches\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nclose_connection\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\ncommit\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\ncommit_on_success\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n`\u2714\ufe0f\ncommit_on_success replaced by atomic in Django >= 1.8\n\n\nforce_text\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nformat_html\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_callable\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_current_site\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_ident\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_mod_func\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_model\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_model_name\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_ns_resolver\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_resolver\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_script_prefix\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_template_loaders\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_urlconf\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_user_model\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_username_field\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nhandler404\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nhandler500\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nimport_module\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nimport_string\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\ninclude\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nis_valid_path\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nparse_qs\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\npatterns\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u274c\n\u274c\n\n\n\npython_2_unicode_compatible\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nrender_to_string\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\nThe new function signature (https:\/\/docs.djangoproject.com\/en\/1.9\/releases\/1.8\/#dictionary-and-context-instance-arguments-of-rendering-functions) is backported to pre-1.8.\n\n\nresolve\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nresolve_url\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u26a0\ufe0f\n\u26a0\ufe0f\n1.10: Reversing by dotted path has been removed\n\n\nreverse\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nreverse_lazy\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nrollback\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\nTransaction savepoint (sid) is required for Django < 1.8\n\n\nset_script_prefix\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nset_urlconf\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nsimplejson\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nslugify\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nsmart_text\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nunquote_plus\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nurl\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2716\ufe0f\n\u2716\ufe0f\nFunction used in urlpatterns\n\n\ntempat.url\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n`uravy_multiplication_x:\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nurlparse\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nurlresolvers\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nurlunparse\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nuser_model_label\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\ntemplatetags.compat.verbatim\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\nTemplatetag; import with {% load verbatim from compat %}. 1.4: Does not allow specific closing tags, e.g. {% endverbatim myblock %}, and does not preserve whitespace inside tags.\n\n\n\nResources and references\nResources\n\nhttps:\/\/github.com\/ubernostrum\/django-compat-lint\nhttps:\/\/docs.djangoproject.com\/en\/dev\/misc\/api-stability\/\nhttps:\/\/docs.djangoproject.com\/en\/dev\/topics\/python3\/\nhttp:\/\/andrewsforge.com\/presentation\/upgrading-django-to-17\/\n\ncompat.py\nBits and bites of the following projects were re-used to build django-compat.\n\n https:\/\/github.com\/lukaszb\/django-guardian\/blob\/devel\/guardian\/compat.py\n https:\/\/github.com\/evonove\/django-oauth-toolkit\/blob\/master\/oauth2_provider\/compat.py\n https:\/\/github.com\/toastdriven\/django-tastypie\/blob\/master\/tastypie\/compat.py\n https:\/\/github.com\/tomchristie\/django-rest-framework\/blob\/master\/rest_framework\/compat.py\n\n TODO: MinValueValidator, MaxValueValidator et al. (other relevant bits are included) Django 1.8\n\n\n https:\/\/gist.github.com\/theskumar\/ff8de60ff6a33bdacaa8\n https:\/\/github.com\/evonove\/django-oauth-toolkit\/blob\/master\/oauth2_provider\/templatetags\/compat.py\n https:\/\/github.com\/kennethreitz\/requests\/blob\/master\/requests\/compat.py\n https:\/\/github.com\/mitsuhiko\/jinja2\/blob\/master\/jinja2\/_compat.py\n https:\/\/github.com\/jaraco\/setuptools\/blob\/master\/setuptools\/compat.py\n https:\/\/github.com\/mariocesar\/sorl-thumbnail\/blob\/master\/sorl\/thumbnail\/compat.py\n\nChangelog\n2017\/04\/07\n\nUpdate existing patches for Django 1.10\n\n2016\/08\/02\n\nUpdate existing patches for Django 1.10\n\n2016\/06\/01\n\nAdd get_current_site and admin_utils\n\n2016\/05\/11\n\nFix error when installing package under python 3.4\n\n###\u00a02015\/11\/12\n\nBackport new render_to_string function signature to Django < 1.8\nBackport verbatim tag to Django 1.4\nAdd get_template_loaders\nAdd close_connection\nImprove JsonResponse backport to Django 1.4\nAdd tests for import_module, get_model and add_to_builtins\nAnticipate renaming of django.core.urlresolvers to django.urls in 1.10\nAvoid warnings in setup.py\n\n2015\/11\/11\n\n1.9 compatibility for existing objects with the following changes:\n\nadd_to_builtins was removed for Django >= 1.9\nGenericForeignKey` was moved to compat.models`` for Django >= 1.9\n\n\n\n2015\/07\/15\n\nadd_to_builtins was added\n\n2015\/07\/08\n\nget_query_set\/get_queryset support was dropped again (see #29)\n\n","434":"django-compat\n\n\n\n\nForward and backwards compatibility layer for Django 1.4, 1.7, 1.8, 1.9, 1.10 and 1.11\nConsider django-compat as an experiment based on the discussion on reddit. Let's see where it goes.\nWhat started as an experiment based on this discussion on reddit has proven to be true in real life.\ndjango-compat is under active development. To learn about other features, bug fixes, and changes, please refer to the changelog.\nWho uses django-compat\nTwo popular examples of open source reusable app that uses django-compat are django-hijack and django-background-tasks.\nWant to have yours listed here? Send us a PR.\nWhy use django-compat\n\nBe able to use the LTS versions of Django and support newer versions in your app\nUse features from newer Django versions in an old one\nManage and master the gap between different framework versions\n\nHow to use django-compat\nInstall compat from the PyPI or download and install manually. All relevant  releases are listed here under releases.\nUsing one of the compatible objects is easy. For example\nfrom compat import patterns, url\n\nurlpatterns = patterns('ABC.views',\n\t\turl(r'^abc\/$', 'abc', name='abc-link'),\n...\n\nSee a full example here.\n\n\n\ndjango-compat is free software. If you find it useful and would like to give back, please consider to make a donation using Bitcoin or PayPal. Thank you!\n\n\n\nCompatible objects\n\n\n\nCompatible object\nSpecifically tested\n1.8\n1.9\n1.10\n1.11\nNotes\n\n\n\n\nBytesIO\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nDjangoJSONEncoder\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nEmailValidator\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nGenericForeignKey\n\u2716\ufe0f\n\u2714\ufe0f\n\u274c\n\u274c\n\u274c\n\n\n\nmodels.GenericForeignKey\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nHttpResponseBase\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nJsonResponse\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nLocaleRegexProvider\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nLocaleRegexURLResolver\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nNoReverseMatch\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nRegexURLPattern\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nRegexURLResolver\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nResolver404\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nResolverMatch\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nSortedDict\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nStringIO\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nURLValidator\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nVariableNode\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nView\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nadd_to_builtins\n\u2716\ufe0f\n\u2714\ufe0f\n\u274c\n\u274c\n\u274c\n\n\n\nadmin_utils\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\natomic\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nclean_manytomany_helptext\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nclear_url_caches\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nclose_connection\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\ncommit\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\ncommit_on_success\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n`\u2714\ufe0f\ncommit_on_success replaced by atomic in Django >= 1.8\n\n\nforce_text\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nformat_html\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_callable\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_current_site\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_ident\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_mod_func\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_model\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_model_name\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_ns_resolver\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_resolver\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_script_prefix\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_template_loaders\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_urlconf\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_user_model\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nget_username_field\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nhandler404\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nhandler500\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nimport_module\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nimport_string\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\ninclude\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nis_valid_path\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nparse_qs\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\npatterns\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u274c\n\u274c\n\n\n\npython_2_unicode_compatible\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nrender_to_string\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\nThe new function signature (https:\/\/docs.djangoproject.com\/en\/1.9\/releases\/1.8\/#dictionary-and-context-instance-arguments-of-rendering-functions) is backported to pre-1.8.\n\n\nresolve\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nresolve_url\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u26a0\ufe0f\n\u26a0\ufe0f\n1.10: Reversing by dotted path has been removed\n\n\nreverse\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nreverse_lazy\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nrollback\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\nTransaction savepoint (sid) is required for Django < 1.8\n\n\nset_script_prefix\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nset_urlconf\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nsimplejson\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nslugify\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nsmart_text\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nunquote_plus\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nurl\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2716\ufe0f\n\u2716\ufe0f\nFunction used in urlpatterns\n\n\ntempat.url\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n`uravy_multiplication_x:\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nurlparse\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nurlresolvers\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nurlunparse\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\nuser_model_label\n\u2716\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\n\n\ntemplatetags.compat.verbatim\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\n\u2714\ufe0f\nTemplatetag; import with {% load verbatim from compat %}. 1.4: Does not allow specific closing tags, e.g. {% endverbatim myblock %}, and does not preserve whitespace inside tags.\n\n\n\nResources and references\nResources\n\nhttps:\/\/github.com\/ubernostrum\/django-compat-lint\nhttps:\/\/docs.djangoproject.com\/en\/dev\/misc\/api-stability\/\nhttps:\/\/docs.djangoproject.com\/en\/dev\/topics\/python3\/\nhttp:\/\/andrewsforge.com\/presentation\/upgrading-django-to-17\/\n\ncompat.py\nBits and bites of the following projects were re-used to build django-compat.\n\n https:\/\/github.com\/lukaszb\/django-guardian\/blob\/devel\/guardian\/compat.py\n https:\/\/github.com\/evonove\/django-oauth-toolkit\/blob\/master\/oauth2_provider\/compat.py\n https:\/\/github.com\/toastdriven\/django-tastypie\/blob\/master\/tastypie\/compat.py\n https:\/\/github.com\/tomchristie\/django-rest-framework\/blob\/master\/rest_framework\/compat.py\n\n TODO: MinValueValidator, MaxValueValidator et al. (other relevant bits are included) Django 1.8\n\n\n https:\/\/gist.github.com\/theskumar\/ff8de60ff6a33bdacaa8\n https:\/\/github.com\/evonove\/django-oauth-toolkit\/blob\/master\/oauth2_provider\/templatetags\/compat.py\n https:\/\/github.com\/kennethreitz\/requests\/blob\/master\/requests\/compat.py\n https:\/\/github.com\/mitsuhiko\/jinja2\/blob\/master\/jinja2\/_compat.py\n https:\/\/github.com\/jaraco\/setuptools\/blob\/master\/setuptools\/compat.py\n https:\/\/github.com\/mariocesar\/sorl-thumbnail\/blob\/master\/sorl\/thumbnail\/compat.py\n\nChangelog\n2017\/04\/07\n\nUpdate existing patches for Django 1.10\n\n2016\/08\/02\n\nUpdate existing patches for Django 1.10\n\n2016\/06\/01\n\nAdd get_current_site and admin_utils\n\n2016\/05\/11\n\nFix error when installing package under python 3.4\n\n###\u00a02015\/11\/12\n\nBackport new render_to_string function signature to Django < 1.8\nBackport verbatim tag to Django 1.4\nAdd get_template_loaders\nAdd close_connection\nImprove JsonResponse backport to Django 1.4\nAdd tests for import_module, get_model and add_to_builtins\nAnticipate renaming of django.core.urlresolvers to django.urls in 1.10\nAvoid warnings in setup.py\n\n2015\/11\/11\n\n1.9 compatibility for existing objects with the following changes:\n\nadd_to_builtins was removed for Django >= 1.9\nGenericForeignKey` was moved to compat.models`` for Django >= 1.9\n\n\n\n2015\/07\/15\n\nadd_to_builtins was added\n\n2015\/07\/08\n\nget_query_set\/get_queryset support was dropped again (see #29)\n\n","435":"Zippopotamus Cloud\n\nAPI Moved\nOur full crowd-source zip-db can be found\nhere\nTrea has taken over Zippopotamus - you can find it at the ekotechnology fork\nThanks\nJeff & Samir\n\nThis is a repository for  Zippopotamus the global postal code API\nZippopotamus is hosted by dotCloud. This repo is used to build and maintain the site.\nIf you want to contribute to the improving the site, back-end, front-end etc. Just fork away and submit pull requests.\nSample Implementations\nCheckout the static\/ folder to see some of the sample implementations of Zippopotamus for inspiration and examples for how to implement Zippopotamus API for use in your website etc.\nIf you want to share an implementation, we would love to post example cases of Zippopotamus on our homepage.\nResponse Format\nOn May 1st Zippopotamus changed their JSON response format to work better with international postal codes.  Now we support a one-to-many format service. That is that one zip code may map to many regions, this is common in countries like Spain and France (but not in the US and Germany).\nPostal Code Information\nFor information our postal codes and countries supported, you should check out the zippopotamus crowd-sourcing project.  Here you can download the entire database dump, or fork and add changes that we will incorporate into our DB.\nTechnical Information\nWhat is Zippopotamus built on\nAt the moment the zippopotamus is built on Python, MongoDB and bottle.py framework.\nLocal Testing?\nThe site is configured to run on dotCloud, if you want to test out the web interfaceyou can change the wsgi.py file to include the last commented line, which is used to run the site on your local host.\nSuggestions and Comments?\nHate it? Love it? Open an issue if you have a problem or contact\nJeff Crowell or Samir Ahmed\nAlso, we aren't bottle or python or mongo experts. So if you see a way that we can improve, please let us know. Additionally, if you have examples (translation corrects etc) of using Zippopotamus that you want to share let us know and we can feature your site \/ blog on the homepage.\n","436":"mongo-spark\nExample application on how to use mongo-hadoop connector with Apache Spark.\nRead more details at http:\/\/codeforhire.com\/2014\/02\/18\/using-spark-with-mongodb\/\nPrerequisites\n\nMongoDB installed and running on localhost\nScala 2.10 and SBT installed\n\nRunning\nImport data into the database, run either JavaWordCount or ScalaWordCount and print the results.\nmongoimport -d beowulf -c input beowulf.json\nsbt 'run-main JavaWordCount'\nsbt 'run-main ScalaWordCount'\nmongo beowulf --eval 'printjson(db.output.find().toArray())' | less\n\nLicense\nThe code itself is released to the public domain according to the Creative Commons CC0.\nThe example files are based on Beowulf from Project Gutenberg and is under its corresponding license.\n","437":"CollapseLayout\nCollapseLayout can collapse\/expand layout with smooth animation.\nHere is a gif showing the effect:\n\nHow to use\n<com.example.collapselayout.CollapseLayout\n\tandroid:id=\"@+id\/el\"\n\tandroid:layout_width=\"match_parent\"\n\tandroid:layout_height=\"wrap_content\" >\n\t\n\t<LinearLayout\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:orientation=\"vertical\" >\n        \n        .........\n        \n\t<\/LinearLayout>\n\t\n<\/com.example.collapselayout.CollapseLayout>\npublic void showEffect(View v) {\n\tint modeId = modeGroup.getCheckedRadioButtonId();\n\tswitch (modeId) {\n\tcase R.id.fixedTop:\n\t\tel.setCollapseMode(Mode.FIXED_START);\n\t\tbreak;\n\tcase R.id.fixedBottom:\n\t\tel.setCollapseMode(Mode.FIXED_END);\n\t\tbreak;\n\t}\n\t\n\tint interpolatorId = interpolatorGroup.getCheckedRadioButtonId();\n\tswitch (interpolatorId) {\n\tcase R.id.linear:\n\t\tel.setInterpolator(linearInterpolator);\n\t\tbreak;\n\tcase R.id.bounce:\n\t\tel.setInterpolator(bounceInterpolator);\n\t\tbreak;\n\tcase R.id.accelerate:\n\t\tel.setInterpolator(accelerateDecelerateInterpolator);\n\t\tbreak;\n\t}\n\t\n\tint orientationId = orientationGroup.getCheckedRadioButtonId();\n\tswitch (orientationId) {\n\tcase R.id.vertical:\n\t\tel.setCollapseOrientation(Orientation.VERTICAL);\n\t\tbreak;\n\tcase R.id.horizontal:\n\t\tel.setCollapseOrientation(Orientation.HORIZONTAL);\n\t\tbreak;\n\t}\n\t\n\tint state = el.getState();\n\tswitch (state) {\n\tcase CollapseLayout.STATE_OPEN:\n\t\tel.close();\n\t\tbreak;\n\tcase CollapseLayout.STATE_CLOSE:\n\t\tel.open();\n\t\tbreak;\n\t}\n}\n<declare-styleable name=\"CollapseLayout\">\n\t<attr name=\"collapseOrientation\">\n\t    <enum name=\"vertical\" value=\"0\" \/>\n\t    <enum name=\"horizontal\" value=\"1\" \/>\n\t<\/attr>\n\t<attr name=\"collapseMode\">\n\t    <enum name=\"fixed_end\" value=\"0\"\/>\n\t    <enum name=\"fixed_start\" value=\"1\"\/>\n\t<\/attr>\n\t<attr name=\"initialCollapseState\">\n\t    <enum name=\"open\" value=\"0\"\/>\n\t    <enum name=\"close\" value=\"1\"\/>\n\t<\/attr>\n\t<attr name=\"collapseDuration\" format=\"integer\"\/>\n<\/declare-styleable>\n","438":"Kafka Graphite Metrics Reporter\n\nThis is a simple reporter for kafka using the\nGraphiteReporter. It works with\nkafka 0.8.x and 0.9.x versions.\nBig thanks to Maxime Brugidou from Criteo who did the initial commit of the Ganglia version,\navailable here https:\/\/github.com\/criteo\/kafka-ganglia\nInstall On Broker\n\nBuild the kafka-graphite-1.0.*.jar jar using mvn package or download it from the releases.\nHint: The jar will include the metrics-graphite dependency\nwhich is not brought by Kafka.\nAdd kafka-graphite-1.0.*.jar to the libs\/ directory of your kafka broker installation\nConfigure the broker (see the configuration section below)\nRestart the broker\n\nConfiguration\nEdit the server.properties file of your installation, activate the reporter by setting:\nkafka.metrics.reporters=com.criteo.kafka.KafkaGraphiteMetricsReporter\nkafka.graphite.metrics.reporter.enabled=true\n\nYou may also specify multiple comma-separated reporter classes for the kafka.metrics.reporters property:\nkafka.metrics.reporters=com.criteo.kafka.KafkaGraphiteMetricsReporter,kafka.metrics.KafkaCSVMetricsReporter[,....]\n\nHere is a list of default properties used:\nkafka.graphite.metrics.host=localhost\nkafka.graphite.metrics.port=2003\n# The group value is going to be part of the metrics name to distinguish between different brokers\nkafka.graphite.metrics.group=kafka\n# This can be use to exclude some metrics from graphite \n# since kafka has quite a lot of metrics, it is useful\n# if you have many topics\/partitions. For example :\nkafka.graphite.metrics.exclude.regex=(kafka.network.*|kafka.*.topic.*)\n\n# Each metric provides multiple dimensions: min, max, meanRate, etc\n# This might be too much data.\n# It is possible to disable some metric dimensions with the following properties:\n# By default all dimensions are enabled. Only the unwanted dimensions have to be configured\nkafka.graphite.dimension.enabled.count=false\nkafka.graphite.dimension.enabled.meanRate=false\nkafka.graphite.dimension.enabled.rate1m=false\nkafka.graphite.dimension.enabled.rate5m=false\nkafka.graphite.dimension.enabled.rate15m=false\nkafka.graphite.dimension.enabled.min=false\nkafka.graphite.dimension.enabled.max=false\nkafka.graphite.dimension.enabled.mean=false\nkafka.graphite.dimension.enabled.sum=false\nkafka.graphite.dimension.enabled.stddev=false\nkafka.graphite.dimension.enabled.median=false\nkafka.graphite.dimension.enabled.p75=false\nkafka.graphite.dimension.enabled.p95=false\nkafka.graphite.dimension.enabled.p98=false\nkafka.graphite.dimension.enabled.p99=false\nkafka.graphite.dimension.enabled.p999=false\n\nKnown Issues\nWith Kafka  <= 0.8.2.2 there is an issue if topics get deleted or partions are moved between brokers.\nThe metrics are not get deleted in this case and because they are implemented as a Gauge, a NoSuchElementException\nis thrown when the metrics are reported.\nThere is already a fix for this, see KAFKA-1866 but it did not make\nit into an 0.8.x release. Because of this we implemented a workaround for this within the FilterMetricsPredicate.\n","439":"Big O\nA big collection of data structures and algorithms puzzles.\nAs on May 2013, the repository has roughly 130 problems with solutions.\nSolutions are primarily in Java (under java\/). Some solutions are in Ruby (under ruby\/).\nProblem statements are specified as Javadoc at the top of every solution. I have tried to be as descriptive as I can, but if something is not clear then please do let me know!\nProblems are broadly classified into the following categories which are self explanatory.\nPlease feel free to contribute!\n\narrays\/\nbinarytrees\/\nbitsandbytes\/\ncache\/\ncollections\/\nconcurrency\/\ndp\/\ngeneral\/\ngraphs\/\nlinkedlists\/\nsorting\/\nstrings\/\ntrees\/\n\n","440":"\u6b64\u9879\u76ee\u5df2\u5e9f\u5f03\n","441":"HipChat plugin for Jenkins\nStarted with a fork of the Campfire plugin:\nhttps:\/\/github.com\/jgp\/hudson_campfire_plugin\n","442":"Carma-Public\/swagger-jaxrs-doclet has been doing more recent development on a fork of this project. Please consider checking that fork out first.\nSwagger Doclet \nA JavaDoc Doclet that can be used to generate a Swagger resource listing suitable for feeding to\nswagger-ui.\nUsage\nTo use the Swagger Doclet in your Maven project, add the following to your POM file.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http:\/\/maven.apache.org\/POM\/4.0.0\"\n         xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\"\n         xsi:schemaLocation=\"http:\/\/maven.apache.org\/POM\/4.0.0 http:\/\/maven.apache.org\/xsd\/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0<\/modelVersion>\n\n    <groupId>\u2026<\/groupId>\n    <artifactId>\u2026<\/artifactId>\n    <version>\u2026<\/version>\n    \n    <dependencies>\n        \u2026\n    <\/dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins<\/groupId>\n                <artifactId>maven-javadoc-plugin<\/artifactId>\n                <version>2.9.1<\/version>\n                <executions>\n                    <execution>\n                        <id>generate-service-docs<\/id>\n                        <phase>generate-resources<\/phase>\n                        <configuration>\n                            <doclet>com.hypnoticocelot.jaxrs.doclet.ServiceDoclet<\/doclet>\n                            <docletArtifact>\n                                <groupId>com.hypnoticocelot<\/groupId>\n                                <artifactId>jaxrs-doclet<\/artifactId>\n                                <version>0.0.4-SNAPSHOT<\/version>\n                            <\/docletArtifact>\n                            <reportOutputDirectory>${project.build.outputDirectory}<\/reportOutputDirectory>\n                            <useStandardDocletOptions>false<\/useStandardDocletOptions>\n                            <additionalparam>-apiVersion 1 -docBasePath \/apidocs -apiBasePath \/<\/additionalparam>\n                        <\/configuration>\n                        <goals>\n                            <goal>javadoc<\/goal>\n                        <\/goals>\n                    <\/execution>\n                <\/executions>\n            <\/plugin>\n        <\/plugins>\n    <\/build>\n<\/xml>\nExample\nAn example project using Dropwizard is included in jaxrs-doclet-sample-dropwizard. To get it running, run the following commands.\n$ cd jaxrs-doclet-sample-dropwizard\n$ mvn package\n$ java -jar target\/jaxrs-doclet-sample-dropwizard-0.0.4-SNAPSHOT.jar server sample.yml\n\nThe example server should be running on port 8080:\n$ curl localhost:8080\/apidocs\/service.json\n{\n  \"apiVersion\" : \"1\",\n  \"basePath\" : \"\/apidocs\/\",\n  \"apis\" : [ {\n    \"path\" : \"\/Auth.{format}\",\n    \"description\" : \"\"\n  }, {\n    \"path\" : \"\/HttpServletRequest.{format}\",\n    \"description\" : \"\"\n  }, {\n    \"path\" : \"\/ModelResource_modelid.{format}\",\n    \"description\" : \"\"\n  }, {\n    \"path\" : \"\/Recursive.{format}\",\n    \"description\" : \"\"\n  }, {\n    \"path\" : \"\/Response.{format}\",\n    \"description\" : \"\"\n  }, {\n    \"path\" : \"\/greetings_name.{format}\",\n    \"description\" : \"\"\n  } ],\n  \"swaggerVersion\" : \"1.1\"\n}\n$\n\nOverride Swagger UI\nTo override the swagger ui included with the doclet, create your own swagger-ui.zip file and add a swaggerUiZipPath to the additionalparam attribute in the pom file.\n<additionalparam>-apiVersion 1 -docBasePath \/apidocs -apiBasePath \/ -swaggerUiZipPath ..\/..\/..\/src\/main\/resources\/swagger-ui.zip<\/additionalparam>\n\n","443":"Expandable-RecyclerView\nA RecyclerView that behaves like an ExpandableListView, also includes a RecyclerView with header\/footer support.\nDownload\nDownload the latest AAR via Maven:\n<dependency>\n  <groupId>com.levelupstudio<\/groupId>\n  <artifactId>expandable-recyclerview<\/artifactId>\n  <version>1.0.1<\/version>\n<\/dependency>\nor Gradle:\ncompile 'com.levelupstudio:expandable-recyclerview:1.0.1'\nLicense\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","444":"MultiTypeDemo\n\u91c7\u7528\u771f\u5b9e\u7684\u7f51\u7edc\u8bf7\u6c42\u6570\u636e\u6f14\u793a MultiType \u6846\u67b6\u7684\u7528\u6cd5\uff0c\u7b80\u5316 recyclerView \u7684\u590d\u6742\u5217\u8868\u89c6\u56fe\n1.\u4ee3\u7801\u57fa\u672c\u5b8c\u5168\u6a21\u4eff MaterialHome \u7684\u5f00\u6e90\u9879\u76ee\uff0c\u56e0\u4e3a\u6211\u5728\u6d4f\u89c8\u6b64\u9879\u76ee\u7684\u65f6\u5019\u53d1\u73b0\u8fd9\u4e2a\u5e03\u5c40\u6837\u5f0f\u548c\u8bf7\u6c42\u6570\u636e\u80fd\u5f88\u597d\u7684\u6f14\u793a  MultiType  \u7684\u57fa\u672c\u7528\u6cd5\uff0c\u6240\u4ee5\u62a0\u51fa\u4e86\u5176\u4e2d\u7684\u4e00\u5757\u529f\u80fd\u7528\u6765\u4e13\u95e8\u6f14\u793a  MultiType \u7684\u7528\u6cd5\uff0c\u8fd9\u6837\u4e5f\u4fbf\u4e8e\u5927\u5bb6\u53bb\u5bf9\u6bd4\u4f7f\u7528 MultiType \u4e4b\u540e\u6709\u591a\u723d.\n2.\u91c7\u7528MVP\uff0crxJava\uff0cretrofit\u3002\u975e\u5e38\u6709\u5b9e\u7528\u6027\u7684\u5217\u5b50\u6765\u63cf\u8ff0 MultiType \u7684\u7528\u6cd5\u3002\u5f53\u7136\uff0c\u5982\u679c\u4f60\u4e0d\u4e86\u89e3MVP\uff0crxJava\uff0cretrofit\uff0c\u521a\u597d\uff0c\u4f60\u53ea\u9700\u8981\u770brecyclerView\u548cMultiType\u7684\u642d\u914d\u4f7f\u7528\uff0c\u5979\u663e\u5f97\u66f4\u7b80\u5355\n3.\u4ee5\u4e0b\u6574\u4e2a\u754c\u9762\u91c7\u7528\u7684\u5c31\u662f\u4e00\u4e2arecyclerView + MultiType \u5b8c\u6210\u7684\n\n","445":"ZeroNights2017 (c) 2017 James Forshaw\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\nWindows access tokens.\nConsists of two parts.\n\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\n\n","446":"ZeroNights2017 (c) 2017 James Forshaw\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\nWindows access tokens.\nConsists of two parts.\n\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\n\n","447":"ZeroNights2017 (c) 2017 James Forshaw\nSome sample code from my Zero Nights 2017 presentation on UAC bypasses using\nWindows access tokens.\nConsists of two parts.\n\nbypass_uac - This bypasses UAC when in admin approval mode by using token impersonation (including Win10)\nots_auth - Abusing OTS elevation and enterprise domain auth to access a resource.\n\n","448":"ssh\nCreate ssh servers in node.js!\nstatus\nThis module has all kinds of problems since the node.js thread pool does not\nplay nicely with libssh.\nexample\nsimple echo shell\nvar ssh = require('ssh');\n\nssh.createServer(function (session) {\n    session.on('password', function (user, pass, cb) {\n        cb(user === 'foo' && pass === 'bar');\n    });\n    \n    session.on('shell', function (sh) {\n        sh.pipe(sh); \/\/ echo on\n    });\n}).listen(2222);\n\ninstallation\nYou'll need a version of libssh with my patches:\ngit clone git:\/\/github.com\/substack\/libssh.git master\ncd libssh && mkdir build && cd build\ncmake -DCMAKE_INSTALL_PREFIX=$PREFIX -DCMAKE_BUILD_TYPE=Debug ..\nmake && make install\n\nThat should install libssh.pc, which is used by pkg-config during the\nwscript installation. Make sure libssh.pc got installed into your\n$PKG_CONFIG_PATH someplace.\nYou can then install with npm:\nnpm install ssh\n\n","449":"SPDY daemon\nThis is a wrapper around the original Google's SPDY Framer.\nIt includes a standalone server (spdyd) which can act as a SPDY-HTTP proxy (or use yet another HTTP proxy)\nas well as a Rack adapter.\nThe server is built around Eventmachine, and should be pretty fast.\nInstallation:\nGem\n\ngem build spdy.gemspec\nsudo gem install .\/spdy-0.1.gem\n\nManual\n\ngem install em-http-request -v 0.3.0\nOptional, for daemonization: gem install daemons\ncd ext; ruby extconf.rb; make\n\nRunning standalone server:\nRunning it standalone is as simple as:\nbin\/spdyd\n\nCheck bin\/spdyd -h for options.\nRack:\nYou can also run it as a rack server:\nrackup -s Spdy examples\/local.ru\n\nor for Rails application:\nrackup -s Spdy config.ru\n\nTODO:\n\nIntegrate with npn-enabled openssl which can be built using these steps:\nhttps:\/\/gist.github.com\/944386\n\nCopyright 2010 (c) Roman Shterenzon, released under the AGPLv3 license.\n","450":"SPDY daemon\nThis is a wrapper around the original Google's SPDY Framer.\nIt includes a standalone server (spdyd) which can act as a SPDY-HTTP proxy (or use yet another HTTP proxy)\nas well as a Rack adapter.\nThe server is built around Eventmachine, and should be pretty fast.\nInstallation:\nGem\n\ngem build spdy.gemspec\nsudo gem install .\/spdy-0.1.gem\n\nManual\n\ngem install em-http-request -v 0.3.0\nOptional, for daemonization: gem install daemons\ncd ext; ruby extconf.rb; make\n\nRunning standalone server:\nRunning it standalone is as simple as:\nbin\/spdyd\n\nCheck bin\/spdyd -h for options.\nRack:\nYou can also run it as a rack server:\nrackup -s Spdy examples\/local.ru\n\nor for Rails application:\nrackup -s Spdy config.ru\n\nTODO:\n\nIntegrate with npn-enabled openssl which can be built using these steps:\nhttps:\/\/gist.github.com\/944386\n\nCopyright 2010 (c) Roman Shterenzon, released under the AGPLv3 license.\n","451":"SPDY daemon\nThis is a wrapper around the original Google's SPDY Framer.\nIt includes a standalone server (spdyd) which can act as a SPDY-HTTP proxy (or use yet another HTTP proxy)\nas well as a Rack adapter.\nThe server is built around Eventmachine, and should be pretty fast.\nInstallation:\nGem\n\ngem build spdy.gemspec\nsudo gem install .\/spdy-0.1.gem\n\nManual\n\ngem install em-http-request -v 0.3.0\nOptional, for daemonization: gem install daemons\ncd ext; ruby extconf.rb; make\n\nRunning standalone server:\nRunning it standalone is as simple as:\nbin\/spdyd\n\nCheck bin\/spdyd -h for options.\nRack:\nYou can also run it as a rack server:\nrackup -s Spdy examples\/local.ru\n\nor for Rails application:\nrackup -s Spdy config.ru\n\nTODO:\n\nIntegrate with npn-enabled openssl which can be built using these steps:\nhttps:\/\/gist.github.com\/944386\n\nCopyright 2010 (c) Roman Shterenzon, released under the AGPLv3 license.\n","452":"Cryptose\nCryptose is an addictive cryptogram game with three gritty occupations to choose from: Hacker, Detective, or Spy. It's available in the iOS App Store: http:\/\/itunes.apple.com\/us\/app\/cryptose\/id368874791?mt=8 and the Android Market: market:\/\/details?id=com.insurgentgames.cryptose\nYou can see video, screenshots, and more details at: http:\/\/www.insurgentgames.com\/cryptose\/\nCryptose was programmed in C++ using the Airplay SDK, which has since been renamed to the Marmalade SDK: http:\/\/www.madewithmarmalade.com\/\nIt was made using Airplay SDK 4.2, and it probably won't compile out-of-the box in the latest version of Marmalade. If anyone wants to get it to run using the newest Marmalade, patches are welcome.\nGame Description\nCryptose is an addictive cryptogram puzzle game where you use logic and reasoning to decode secret messages. Choose between three occupations to play as a hacker, a detective, or a spy.\nEach game you get a random short encrypted phrase and it's your job to decrypt it. The phrase is encrypted with a simple substitution cipher, meaning that each letter in the alphabet is replaced with another letter (for example, all A's might be replaced with Q's). You start out with the ciphertext, and you decode it letter-by-letter until you reveal the plaintext message.\nFeatures:\n\nThousands of phrases to decrypt\nThree gritty graphical themes to choose from\nUnlimited hints to curb frustration\nLearn interesting facts and quotes about encryption, cypherpunks, and code-breaking\n\nLicense\nThis game is licensed under the GNU General Public License (see gpl.txt). As the copyright owner, I hereby give anyone permission to re-license my GPL code under a non-GPL license for the purpose of distributing it in the iOS App Store or the Android Market.\nInsurgent Games\nInsurgent Games was founded in 2009 by Micah Lee and Crystal Mayer out of their San Francisco studio apartment. For a couple of years they happily made iPhone and Android games. They quickly realized that unless you\u2019re incredibly lucky, it\u2019s hard to make enough money developing indie mobile games to pay San Francisco rent. So Micah got a full time job and Crystal moved on to other things.\nNow, several years later, Micah works for the Electronic Frontier Foundation defending internet users from evil (https:\/\/www.eff.org\/files\/xkcd_comic.png), and Crystal is a freelance web designer (http:\/\/moonsprocket.com\/). But Insurgent Games is dormant.\nSince they're not working on the games anymore, they decided to release them to the community. They hope their games will thrive and be reborn as bigger and better things. All of their games are licensed under the GNU General Public License.\n\nFollow Micah on Twitter: https:\/\/twitter.com\/#!\/micahflee\nFollow Insurgent Games on Twitter: https:\/\/twitter.com\/#!\/insurgentgames\n\n","453":"Jedi-Academy\nActivision and Raven released this code for people to learn from and play with.\nThis code is copyright Activision 2003. This source was released under GNU GPLv2.\n","454":"ebloom\nOverview\nTravis-CI :: \nebloom is a NIF wrapper around a basic bloom filter.\nQuick Start\nYou must have Erlang\/OTP R13B04 or later and a GNU-style build\n  system to compile and run ebloom.\ngit clone git:\/\/github.com\/basho\/ebloom.git\ncd ebloom\nmake\nStart up an Erlang shell with the path to ebloom included.\nerl -pa path\/to\/ebloom\/ebin\nCreate a new bloom filter, insert elements, and test for an\n  elements presence.\n1> PredictedElementCount=5.\n5\n2> FalsePositiveProbability=0.01.\n\n3> RandomSeed=123.\n123\n4> {ok, Ref} = ebloom:new(PredictedElementCount, FalsePositiveProbability, RandomSeed).\n{ok,<<>>}\n5> ebloom:insert(Ref, <<\"abcdef\">>).\nok\n6> true = ebloom:contains(Ref, <<\"abcdef\">>).\ntrue\n7> false = ebloom:contains(Ref, <<\"zzzzzz\">>).\nfalse\nContributing\nWe encourage contributions to ebloom from the community.\n\nFork the ebloom repository on Github.\nClone your fork or add the remote if you already have a clone of\n    the repository.\n\ngit clone git@github.com:yourusername\/ebloom.git\n# or\ngit remote add mine git@github.com:yourusername\/ebloom.git\n\nCreate a topic branch for your change.\n\ngit checkout -b some-topic-branch\n\nMake your change and commit. Use a clear and descriptive commit\n    message, spanning multiple lines if detailed explanation is\n    needed.\nPush to your fork of the repository and then send a pull-request\n    through Github.\n\ngit push mine some-topic-branch\n\nA Basho engineer or community maintainer will review your patch\n    and merge it into the main repository or send you feedback.\n\n","455":"angular-modal \nA modal factory service for AngularJS that makes it easy to add modals to your app.\nInstall\nnpm install angular-modal\nUsage\n\nInclude the modal.js script provided by this component into your app.\nOptional: Include the modal.css style provided by this component into your html.\nAdd btford.modal as a module dependency to your app.\n\nExamples\nPlunker demo\nTypical Use\n\napp.js\n\nangular.module('myApp', ['btford.modal']).\n\n\/\/ let's make a modal called `myModal`\nfactory('myModal', function (btfModal) {\n  return btfModal({\n    controller: 'MyModalCtrl',\n    controllerAs: 'modal',\n    templateUrl: 'my-modal.html'\n  });\n}).\n\n\/\/ typically you'll inject the modal service into its own\n\/\/ controller so that the modal can close itself\ncontroller('MyModalCtrl', function (myModal) {\n  this.closeMe = myModal.deactivate;\n}).\n\ncontroller('MyCtrl', function (myModal) {\n  this.showModal = myModal.activate;\n});\n\nmy-modal.html\n\n<div class=\"btf-modal\">\n  <h3>Hello {{name}}<\/h3>\n  <p><a href ng-click=\"modal.closeMe()\">Close Me<\/a><\/p>\n<\/div>\n\nindex.html\n\n<div ng-app=\"myApp\" ng-controller=\"MyCtrl as ctrl\">\n  <a href ng-click=\"ctrl.showModal()\">Show the modal<\/a>\n<\/div>\nCleaning up\nIf you add any listeners within the modal's controller that are outside the modal's scope,\nyou should remove them with $scope.$on('$destroy', fn () { ... }) to avoid creating a memory leak.\nBuilding on the example above:\n\napp.js\n\n\/\/ ...\ncontroller('MyModalCtrl', function (myModal, $timeout) {\n\n  var ctrl = this,\n      timeoutId;\n\n  ctrl.tickCount = 5;\n\n  ctrl.closeMe = function () {\n    cancelTick();\n    myModal.deactivate();\n  };\n\n  function tick() {\n    timeoutId = $timeout(function() {\n      ctrl.tickCount -= 1;\n      if (ctrl.tickCount <= 0) {\n        ctrl.closeMe();\n      } else {\n        tick();\n      }\n    }, 1000);\n  }\n\n  function cancelTick() {\n    $timeout.cancel(timeoutId);\n  }\n\n  $scope.$on('$destroy', cancelTick);\n\n  tick();\n}).\n\/\/ ...\nInline Options\nNote: The best practice is to use a separate file for the template and a separate declaration for\nthe controller, but inlining these options might be more pragmatic for cases where the template or\ncontroller is just a couple lines.\nangular.module('myApp', []).\n\n\/\/ let's make a modal called myModal\nfactory('myModal', function (btfModal) {\n  return btfModal({\n    controller: function () {\n      this.name = 'World';\n    },\n    controllerAs: 'ctrl',\n    template: '<div class=\"btf-modal\">Hello {{ctrl.name}}<\/div>'\n  });\n}).\n\ncontroller('MyCtrl', function (myModal) {\n  this.showModal = myModal.activate;\n});\n<div ng-app=\"myApp\" ng-controller=\"MyCtrl\">\n  <a href ng-click=\"ctrl.showModal()\">Show the modal<\/a>\n<\/div>\nAPI\nbtfModal\nThe modal factory. Takes a configuration object as a parameter:\nvar modalService = btfModal({\n  \/* options *\/\n})\nAnd returns a modalService object that you can use to show\/hide the modal (described below).\nThe config object must either have a template or a templateUrl option.\nThese options work just like the route configuration in Angular's\n$routeProvider.\nconfig.template\nstring: HTML string of the template to be used for this modal.\nUnless the template is very simple, you should probably use config.templateUrl instead.\nconfig.templateUrl\nstring (recommended): URL to the HTML template to be used for this modal.\nconfig.controller\nstring|function (optional): The name of a controller or a controller function.\nconfig.controllerAs\nstring (optional, recommended): Makes the controller available on the scope of the modal as the given name.\nconfig.container\nDOM Node (optional): DOM node to prepend . Defaults to document.body.\nmodalService\nA modalService has just two methods: activate and deactivate.\nmodalService.activate\nTakes a hash of objects to add to the scope of the modal as locals.\nAdds the modal to the DOM by prepending it to the <body>.\nReturns a promise that resolves once the modal is active.\nmodalService.deactivate\nRemoves the modal (DOM and scope) from the DOM.\nReturns a promise that resolves once the modal is removed.\nmodalService.active\nReturns whether or not the modal is currently activated.\nTests\nYou can run the tests with karma:\nkarma start karma.conf.js\nLicense\nMIT\n","456":"datmusic\nSearch and Download free music from VK.\nDemo\ndatmusic\n\nUsage wiki\nAPI\nVK disabled their public Audio API, so I had to write \"parser\" for their website to simulate api.\nAPI repo\nAndroid Version repo\nAndroid Version repo\nTelegram Bot repo\nTelegram Bot repo\nLicense\nMIT - Alashov Berkeli\n","457":"Deprecated\nThis project is no longer maintained and is not representative of the current best practices in Marionette.\n\nBBCloneMail: A Backbone.Marionette Reference Application\nNOTE: This code is currently in an experimental state. I don't recommend\nusing it as guidance for how to build Marionette apps at the moment. If you\nwould like to see a better reference code base, check out the version of\nBBCloneMail that Foxandxss is building.\nSee It In Action\nThis is a sample application, demonstrating how to use my\nBackbone.Marionette\nplugin for Backbone.js. You can see it in action at:\nhttp:\/\/bbclonemail.heroku.com\nRunning BBCloneMail On Your Computer\nBBCloneMail is a NodeJS app built on Express.js. To run it on your\ncomputer you'll want to clone this repository to your machine somewhere,\nand then follow these steps:\n\nInstall the latest http:\/\/nodejs.org if you don't have it already\nOpen a command prompt \/ terminal window in the BBCloneMail project folder\nRun npm install to install all of the needed components\nRun npm start to start the server\nOpen http:\/\/localhost:3000 in your browser\n\nNote that step 1 through 3 only have to be done once. After you have\ndone that, you just need to run step 4 and 5 any time you want to\nsee the app running on your computer.\nA Work In Progress\nKeep in mind that this is always a work in progress. While the source code\nand functionality do demonstrate all of the core features and capabilities\nof Backbone.Marionette, the application itself is very limited in it's\nfunctionality.\nAlso note that I haven't optimized the JavaScript downloads in any way. There\nis no minification, and no asset packaging to create a single download for the\nentire application at this point. As a result, the app takes a moment or two\nto download all of the JavaScript files and start up.\nAs I continue working on functionality, I'll also put in some optimizations for\nthe JavaScript, so that it starts up faster.\nLegal Mumbo Jumbo (MIT License)\nCopyright (c) 2012 Derick Bailey, Muted Solutions, LLC\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","458":"Twitty Twister\nTwitty twister allows you to access twitter-compatible APIs from pure twisted\napps. It supports standard and OAuth authentication.\nSee the example\/ directory\nfor example commandline tools that exercise various parts of the API.\nWhat are People Saying About It?\nNot much, but I consider this an excellent review:\n\nA winning project name if ever I've read one.\n\n\u2014 Alex Payne\n","459":"Uploadr.py\nUploadr.py is a simple Python script for uploading your photos to Flickr. Unlike\nmany GUI applications out there, it lends itself to automation; and because it's\nfree and open source, you can just change it if you don't like it.\n\nAuthentication\nTo use this application, you need to obtain your own Flickr API key and secret\nkey. You can apply for keys on the Flickr website.\nWhen you have got those keys, you need to set environment variables so that they\ncan be used by this application. For example, if you use Bash, add the following\nlines to your $HOME\/.bash_profile:\nexport FLICKR_UPLOADR_PY_API_KEY=0123456789abcdef0123456789abcdef\nexport FLICKR_UPLOADR_PY_SECRET=0123456789abcdef\n\n\nLicense\nUploadr.py consists of code by Cameron Mallory, Martin Kleppmann, Aaron Swartz and\nothers. See COPYRIGHT for details.\n","460":"Spans\n\n   \n \n \n\nSpans is a pure Python implementation of PostgreSQL's\nrange types.\nRange types are convenient when working with intervals of any kind. Every time\nyou've found yourself working with date_start and date_end, an interval may have\nbeen what you were actually looking for.\nSpans has successfully been used in production since its first release\n30th August, 2013.\n\nInstallation\nSpans exists on PyPI.\n$ pip install Spans\nDocumentation is hosted on Read the\nDocs.\n\nExample\nImagine you are building a calendar and want to display all weeks that overlaps\nthe current month. Normally you have to do some date trickery to achieve this,\nsince the month's bounds may be any day of the week. With Spans' set-like\noperations and shortcuts the problem becomes a breeze.\nWe start by importing date and daterange\n>>> from datetime import date\n>>> from spans import daterange\nUsing daterange.from_month we can get range representing January in the year\n2000\n>>> month = daterange.from_month(2000, 1)\n>>> month\ndaterange(datetime.date(2000, 1, 1), datetime.date(2000, 2, 1))\nNow we can calculate the ranges for the weeks where the first and last day of\nmonth are\n>>> start_week = daterange.from_date(month.lower, period=\"week\")\n>>> end_week = daterange.from_date(month.last, period=\"week\")\n>>> start_week\ndaterange(datetime.date(1999, 12, 27), datetime.date(2000, 1, 3))\n>>> end_week\ndaterange(datetime.date(2000, 1, 31), datetime.date(2000, 2, 7))\nUsing a union we can express the calendar view.\n>>> start_week.union(month).union(end_week)\ndaterange(datetime.date(1999, 12, 27), datetime.date(2000, 2, 7))\nDo you want to know more? Head over to the\ndocumentation.\n\nUse with Psycopg2\nTo use these range types with Psycopg2 the\nPsycoSpans.\n\nMotivation\nFor a project of mine I started using PostgreSQL's tsrange type and needed\nan equivalent in Python. These range types attempt to mimick PostgreSQL's\nbehavior in every way. Deviating from it is considered as a bug and should be\nreported.\n\nContribute\nI appreciate all the help I can get! Some things to think about:\n\nIf it's a simple fix, such as documentation or trivial bug fix, please file\nan issue or submit a pull request. Make sure to only touch lines relevant to\nthe issue. I don't accept pull requests that simply reformat the code to be\nPEP8-compliant. To me the history of the repository is more important.\nIf it's a feature request or a non-trivial bug, always open an issue first to\ndiscuss the matter. It would be a shame if good work went to waste because a\npull request doesn't fit the scope of this project.\n\nPull requests are credited in the change log which is displayed on PyPI and the\ndocumentaion on Read the Docs.\n","461":"Instadrop\nAutomatically sync your Instagram photos to Dropbox\nA demo application of the Instagram real-time API.\nIt's live! http:\/\/instadrop.appspot.com\nInstallation on Google App Engine\n\nSign up for an App Engine account.\nDownload the App Engine SDK.\nFork and modify our code! Need help? Read the Google App Engine Getting Started Guide\n\nFollow @instagramapi on Twitter\nYou should follow @instagramapi on Twitter for announcements,\nupdates, and news about the Instagram API.\nJoin the mailing list!\nhttps:\/\/groups.google.com\/group\/instagram-api-developers\nDid you fork this app to create something cool?\nAdd it to the apps wiki!\nContributing\nIn the spirit of free software, everyone is encouraged to help improve this project.\nHere are some ways you can contribute:\n\nby using alpha, beta, and prerelease versions\nby reporting bugs\nby suggesting new features\nby writing or editing documentation\nby writing specifications\nby writing code (no patch is too small: fix typos, add comments, clean up inconsistent whitespace)\nby refactoring code\nby closing issues\nby reviewing patches\n\nSubmitting an Issue\nWe use the GitHub issue tracker to track bugs and\nfeatures. Before submitting a bug report or feature request, check to make sure it hasn't already\nbeen submitted. You can indicate support for an existing issuse by voting it up. When submitting a\nbug report, please include a Gist that includes a stack trace and any\ndetails that may be necessary to reproduce the bug, including your Python version and\noperating system. Ideally, a bug report should include a pull request with failing specs.\nSubmitting a Pull Request\n\nFork the project.\nCreate a topic branch.\nImplement your feature or bug fix.\nAdd documentation for your feature or bug fix.\nCommit and push your changes.\nSubmit a pull request.\n\nCopyright\nCopyright (c) 2011 Instagram (Burbn, Inc).\nSee LICENSE for details.\n","462":"SimpleRecyclerView\n\n\u4e2d\u6587 README\nAn enhancement to RecyclerView and SwipeRefreshLayout. Integrated with timehop\/sticky-headers-recyclerview for sticky headers.\nhttps:\/\/github.com\/timehop\/sticky-headers-recyclerview\n\nMain Characters:\n1. Pull-To-Refresh\nAn enhancement to SwipeRefreshLayout in 2 aspects:\n\n\nIn some versions of android.support library, SwipeRefreshLayout has\nsliding conflict with AppbarLayout. When you pull down RecyclerView, SwipeRefreshLayout will appear instantly, preventing you from pulling down the list.\n\n\nNow you can invoke setRefreshing(true) to show loading progress in onCreate() while the official's can not.\n\n\n2.Load more\n\n\nAutomatically load more data when there are still ${THRESHOLD} items to the bottom (THRESHOLD customizable)\n\n\nIf user slides to the bottom and the loading process is still incomplete, the loading progress animation will be displayed.\n\n\nLayoutManager irrelevant\n\n\n2 indicator styles available: ProgressBar and SwipeRefreshLayout (The SwipeRefreshLayout style is independent of the Android version, with indicator color and background color Customizable. The ProgressBar style is dependent on the Android version. Its Material Design style is only available in API 21+.)\n\n\n3.Loading View \/ Empty View \/ Error View\n4.OnItemClickListener \/ OnItemLongClickListener\n5.Sticky headers\n\nSupports any number of fixed header types\n\n6.Item divider support\n\n\nCustomizable divider width and color\n\n\nYou can customize the length of the blank area on the left \/ top \/ right \/ bottom of the divider (divider will not draw in blank area)\n\n\nSupports horizontal \/ vertical LinearLayoutManager\n\n\n7.Item animation\n\nMaterial Design animation when initing RecyclerView, adding \/ modifying \/ deleting items\n\n8.Support for group display\n9.Get scrolled distance and distance to end\nSimpleRecyclerView added 2 methods to get these distances;\nMeanwhile, these two distances are passed as parameters in SimpleOnScrollListener.onScrolled.\n\/\/Scrolled distance (px)\nint SimpleRecyclerView.getScrolledDistance();\n\n\/\/Distance to end (px)\nint SimpleRecyclerView.getDistanceToEnd();\n\nabstract class SimpleOnScrollListener extends RecyclerView.OnScrollListener {\n  abstract void onScrollStateChanged(int scrolledDistance, int distanceToEnd, int newState);\n  \/**\n  * @param scrolledDistance Scrolled distance (px)\n  * @param distanceToEnd Distance to end (px)\n  * @param velocity Current scroll velocity (positive or negative indicates the direction)\n  *\/\n  abstract void onScrolled(int scrolledDistance, int distanceToEnd, int velocity);\n}\n\nImport\n1.Add binary\nIn build.gradle, add\ncompile 'com.xdandroid:simplerecyclerview:+'\n\n2.Basic usage is the same as official RecyclerView and SwipeRefreshLayout\n\n\nSimple* classes inherit from the official widgets\n\n\nPlease refer to the demo for usage\n\n\nIf you have multiple viewType (Adapter inherits com.xdandroid.simplerecyclerview.Adapter), The ways of setting data to Adapter are the same as RecyclerView.Adapter. You can pass the data set by the constructor, or you can create a method called setList or so, setting data set to the Adapter and refresh UI using notifyDataSetChanged().\n\n\nIf there is only one viewType (Adapter inherits SingleViewTypeAdapter<${JavaBean}>), the method of setting data list to Adapter is:\n\n\n.\nrecyclerView.setAdapter(adapter);\nvoid Adapter.setList(List<${JavaBean}> list);\n\n3.Layout XML\n<com.xdandroid.simplerecyclerview.SimpleSwipeRefreshLayout\n    tools:context=\"com.xdandroid.sample.MainActivity\"\n    xmlns:android=\"http:\/\/schemas.android.com\/apk\/res\/android\"\n    xmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n    xmlns:tools=\"http:\/\/schemas.android.com\/tools\"\n    app:layout_behavior=\"@string\/appbar_scrolling_view_behavior\"\n    tools:showIn=\"@layout\/activity_main\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:id=\"@+id\/swipe_container\">\n\n    <com.xdandroid.simplerecyclerview.SimpleRecyclerView\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:id=\"@+id\/recycler_view\"\/>\n\n<\/com.xdandroid.simplerecyclerview.SimpleSwipeRefreshLayout>\n\n<!--Empty View-->\n<TextView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:id=\"@+id\/empty_view\"\n    android:visibility=\"gone\"\/>\n\n<!--Error View-->\n<TextView\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:id=\"@+id\/error_view\"\n    android:visibility=\"gone\"\/>\n\n<!--Loading View-->\n<FrameLayout\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:id=\"@+id\/loading_view\">\n\n    <ProgressBar\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_gravity=\"center\"\n        android:layout_marginBottom=\"20dp\"\/>\n\n    <TextView\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_gravity=\"center\"\n        android:text=\"Hello Loading View\"\n        android:layout_marginTop=\"20dp\"\n        android:textSize=\"20sp\"\n        android:textColor=\"@android:color\/black\"\/>\n<\/FrameLayout>\n\n4. Define Adapter abstract class\n4.1 When you only have one viewType, subclass SingleViewTypeAdapter<${JavaBean}>\n\n\nOverride onViewHolderCreate, corresponds to onCreateViewHolder in RecyclerView.Adapter\n\n\nOverride onViewHolderBind, corresponds to onBindViewHolder in RecyclerView.Adapter\n\n\nCreate ViewHolder\n\n\nOverride int getItemSpanSize(int position, int viewType, int spanCount) if you use GridLayoutManager\n\n\nDo not override onLoadMore or hasMoreElements, instead implement them when instantiat Adapter in Activity \/ Fragment.\n\n\n4.2 Multiple viewType, subclass Adapter\n\n\nOverride onViewHolderCreate, corresponds to onCreateViewHolder in RecyclerView.Adapter\n\n\nOverride onViewHolderBind, corresponds to onBindViewHolder in RecyclerView.Adapter\n\n\nOverride getViewType, corresponds to getItemViewType in RecyclerView.Adapter\n\n\nOverride getCount, corresponds to getItemCount in RecyclerView.Adapter\n\n\nCreate ViewHolder classes for each viewType\n\n\nOverride int getItemSpanSize(int position, int viewType, int spanCount) if you use GridLayoutManager\n\n\nDo not override onLoadMore or hasMoreElements, instead implement them when instantiat Adapter in Activity \/ Fragment.\n\n\nPull-To-Refresh\nResolve a sliding conflict with AppbarLayout: Assign an id to AppBarLayout named \"appbar\" (android:id=\"@+id\/appbar\")\nTo automatically leave the height for Toolbar and prevent from being blocked by the Toolbar, add app:layout_behavior=\"@string\/appbar_scrolling_view_behavior\" to SwipeRefreshLayout or the layout under CoordinatorLayout.\nLoad more\nImplement 2 methods when instantiate Adapter:\n1. void onLoadMore(Void please_make_your_adapter_class_as_abstract_class) :\nFirst self-increment pageIndex varible by one, then invoke API to get more data. After new piece of data is parsed:\nFor SingleViewTypeAdapter, Do NOT call List[E].addAll(Collection[? extends E]) on your data set, instead call void SingleViewTypeAdapter.addAll(List[E]) directly. SingleViewTypeAdapter will update the underlying data set automatically.\nFor Adapter, first update your data set using List[E].addAll(Collection[? extends E]), then call void Adapter.onAddedAll(int newDataSize) to notify Adapter that some data has been added to the data set.\n2. boolean hasMoreElements(Void let_activity_or_fragment_implement_these_methods) :\nTell the Adapter whether there is more data to load.\nif needed, you can call void Adapter.setLoadingFalse() to restore the status of not loading more.\nCustomize style:\nadapter.setUseMaterialProgress(true, new int[]{getResources().getColor(R.color.colorAccent)});\n\nWhen the first parameter (boolean useMaterialProgress) is true, SwipeRefreshLayout style is used, otherwise ProgressBar style is used. The second parameter (int[] colors) is available only if useMaterialProgress is true. You can pass an int[] in the method to set the color set of the loading indicator. If there are more than one color in the int[], the indicator will iterate the colors at the frequency of one color per turn.\nadapter.setColorSchemeColors(new int[]{getResources().getColor(R.color.colorAccent)});\n\nCall the method above to change the indicator color at any time.\nprogressView.setProgressBackgroundColor(Color.parseColor(\"#FAFAFA\"));\n\nSet the background color of indicator.\nSet threshold: void Adapter.setThreshold(int threshold);\nFor GridLayoutManager:\nGridLayoutManager gridLayoutManager = new GridLayoutManager(context,SPAN_SIZE);\ngridLayoutManager.setSpanSizeLookup(adapter.getSpanSizeLookup(SPAN_SIZE));\nrecyclerView.setLayoutManager(gridLayoutManager);\n\nPlease refer to GridFragment and GridAdapter in the demo.\nLoading View \/ Empty View \/ Error View\nXML prepare (Please refer to Import - Layout XML)\n\n\nPlace the LoadingView in parallel with the SwipeRefreshLayout element.\n\n\nPlace the ErrorView \/ EmptyView in parallel with the SwipeRefreshLayout element. Set the visibility of ErrorView \/ EmptyView to \"gone\".\n\n\nJava Code\n\/**\n * Call this method to set the LoadingView before calling the setAdapter or notify* methods.\n * LoadingView will automatically hide when the setAdapter or notify* methods is called.\n * @param loadingView LoadingView got by findViewById.\n *\/\nvoid SimpleRecyclerView.setLoadingView(View loadingView);   \/\/Call this method to set the LoadingView before calling the setAdapter or notify* methods.\n\/\/Example:\nrecyclerView.setLoadingView(findViewById(R.id.loading_view));   \/\/Set custom LoadingView layout.\n\nView SimpleRecyclerView.hideLoadingView();                  \/\/Manually hide LoadingView. Generally needn't.\nView SimpleRecyclerView.showLoadingView();                  \/\/Manually show LoadingView. Generally needn't.\n\nrecyclerView.setEmptyView(findViewById(R.id.empty_view)); \t\/\/Set EmptyView\nrecyclerView.showErrorView(findViewById(R.id.error_view));\t\/\/Set and show ErrorView\nrecyclerView.hideErrorView();\t\t\t\t\t\t\t\t\/\/Hide ErrorView\n\nOnItemClickListener\/OnItemLongClickListener\nadapter.setOnItemClickListener(new OnItemClickListener());\nadapter.setOnItemLongClickListener(new OnItemLongClickListener());\n\nTo implement ripple effect on item clicks, add the code below to the root element of the item layout XML:\nandroid:foreground=\"?android:attr\/selectableItemBackground\"(for CardView, SimpleDraweeView)\nandroid:background=\"?android:attr\/selectableItemBackground\"(for general View)\n\nFor CardView, add the code above to the element of CardView, instead of adding to the root element.\nDivider\nInstantiate Divider:\npublic Divider(\n  @Px int width,           \/\/The width of the divider\n  @ColorInt int color,     \/\/The color of the divider\n  boolean isHorizontalList,   \/\/Whether the LinearLayoutManager is horizontal\n  @Px int leftOffset, @Px int topOffset, @Px int rightOffset, @Px int bottomOffset);\n\n\n\nleftOffset is the length of the blank area on the left of the divider (divider will not draw in blank area).\n\n\ntopOffset \/ rightOffset \/ bottomOffset are the same.\n\n\nUsage\uff1a\nmRecyclerView.addItemDecoration(Divider divider);\n\nAnimation when initing RecyclerView, adding \/ modifying \/ deleting items\nThe Adapter \/ SingleViewTypeAdapter encapsulates common methods for manipulating data sets. Using these methods, you will get animation effects and the correct loading state settings.\nAdapter:\n\nvoid onAdded();\nvoid onAdded(int position);\nvoid onAddedAll(int newDataSize);\nvoid onAddedAll(int position, int newDataSize);\nvoid onListSet();\nvoid onRemovedLast(); \/ void onRemoved();\nvoid onRemoved(int position);\nvoid onRemoveAll(int positionStart, int itemCount);\nvoid onSet(int position);\nvoid onSetAll(int positionStart, int itemCount);\n\nWhen you are using Adapter, you should update your data set first, then call the above methods.\nSingleViewTypeAdapter :\n\nvoid setList(List<${JavaBean}> list);\nvoid add(${JavaBean} javaBean);\nvoid add(int position, ${JavaBean} javaBean);\nvoid remove(int position);\nvoid removeLast(); \/ void remove();\nvoid removeAll(int positionStart, int itemCount);\nvoid set(int position,${JavaBean} javaBean);\nvoid setAll(int positionStart, int itemCount, ${JavaBean} javaBean);\nvoid addAll(int position, List<${JavaBean}> newList);\nvoid addAll(List<${JavaBean}> newList);\n\nWhen you are using SingleViewTypeAdapter, you should NOT update your data set, instead call the above methods directly.\nYou only need to call the methods above, SingleViewTypeAdapter will update the underlying data set automatically.\nIf the required method of operation on the data set is not listed above, you can operate on the data set List[${JavaBean}] first, then call adapter.notifyItem* mmethods to refresh UI, finally call setLoadingFalse() to restore the status of not loading more.\nSticky headers\n1.Make your Adapter class implements StickyRecyclerHeadersAdapter<RecyclerView.ViewHolder> interface;\n2.Create your Header ViewHolder;\n3.Implement 3 methods in the interface:\n\nlong getHeaderId(int position);\n\nThis method determines which header the item in ${position} is displayed under. One header corresponds to one headerId, so, For items that want to be displayed under the same header, this method should return the same headerId for the items' positions. The number of headerIds the method possibility returns is the number of fixed headers.\nYou can call this method to get the headerId for the current adapterPosition in onViewHolderBind and onBindHeaderViewHolder.\n\n\nRecyclerView.ViewHolder onCreateHeaderViewHolder(ViewGroup parent);\n\n\nvoid onBindHeaderViewHolder(RecyclerView.ViewHolder holder, int adapterPosition);\n\n\nExample:\n@Override\npublic long getHeaderId(int position) {\n    \/\/This example takes the elements 0-9 as a group, the 10-19 elements as a group, and so on, with every 10 elements belonging to the same group.\n    \/\/For actual use, you can base on position and the data got from List.get(position) to judge the fields inside, to decide which elements belong to which groups.\n    return position \/ 10;\n}\n\n@Override\npublic HeaderVH onCreateHeaderViewHolder(ViewGroup parent) {\n    return new HeaderVH(LayoutInflater.from(parent.getContext()).inflate(R.layout.item_header, parent, false));\n}\n\n@Override\npublic void onBindHeaderViewHolder(HeaderVH holder, int position) {\n    holder.tvHeader.setText(\"Group \" + getHeaderId(position) \/* The group the current header is in. *\/ +\n        \": Adapter Position \" + String.valueOf(position + 1) + \" - \" + String.valueOf(position + 10));\n}\n\nPlease refer to PinnedFragment and PinnedAdapter in the demo.\nGroup display\nType Group[Title, ChildItem] is required to represent a group. Each group contains one title and a number of subitems.\n<Title, ChildItem> Group<Title, ChildItem> {\n    Title title;  \/\/Title of one group\n    List<ChildItem> childItemList;  \/\/Subitem list under one group\n}\n\nTherefore, you need to convert the data to List[Group[Title, ChildItem]], that is, the list of groups.\nYou need to subclass GroupAdapter, and override the following 4 methods:\nViewHolder onTitleVHCreate(ViewGroup parent);\n\nViewHolder onChildItemVHCreate(ViewGroup parent);\n\n\/**\n* @param adapterPos The absolute position of the Title in the Adapter ( = holder.getAdapterPosition()).\n* @param titleOrderInAllTitles The relative position of the Group of the current Title in List[Group].\n*\/\nvoid onTitleVHBind(ViewHolder holder, int adapterPos, Title title, int titleOrderInAllTitles);\n\n\/**\n* @param adapterPos The absolute position of the ChildItem in the Adapter ( = holder.getAdapterPosition()).\n* @param titleOrderInAllTitles The relative position of the Group of the current ChildItem in List[Group].\n* @param childOrderInCurrentGroup The relative position of the current ChildItem in the Group.\n* (The childOrder of the first ChildItem is 0, that is, the childOrder does not include the position of the Title.)\n*\/\nvoid onChildItemVHBind(ViewHolder holder, int adapterPos, Title title,\n    int titleOrderInAllTitles, ChildItem childItem, int childOrderInCurrentGroup);\n\nThe data List[Group] is passed in through the GroupAdapter.setList(List[Group] groupList) method.\nIf you know the absolute position of the Title or ChildItem in the Adapter, and you need to get the Title object, the relative position of the Group of the current Title in List[Group], the ChildItem object and the relative position of the current ChildItem in the Group, You can use 2 methods below on the GroupAdapter:\n\/**\n* According to the absolute position of the Title in the Adapter,\n* get the Title object and the relative position of the Group of the current Title in List[Group].\n* @param positionInRV_viewType_title The absolute position of the Title in the Adapter\uff0c\n* @return TitleChildItemBean {Title title;  int titleOrder;}\n*\/\nTitleChildItemBean<Title, Void> getTitleWithOrder(int positionInRV_viewType_title)\n\n\/**\n* According to the absolute position of the ChildItem in the Adapter,\n* get the Title object, the relative position of the Group of the current Title in List[Group],\n* the ChildItem object and the relative position of the current ChildItem in the Group.\n* @param positionInRV_viewType_childItem The absolute position of the ChildItem in the Adapter.\n* @return TitleChildItemBean {Title title;  int titleOrder;\n    ChildItem childItem;  int childOrder;}\n*\/\nTitleChildItemBean<Title, ChildItem> getTitleAndChildItem(int positionInRV_viewType_childItem)\n\nSet OnGroupItemClickListener and OnGroupItemLongClickListener:\nGroupAdapter.setOnGroupItemClickListener(OnGroupItemClickListener l);\n\nvoid onGroupItemClick(ViewHolder holder, View v, int adapterPos, int viewType,\n   Title title, int titleOrder, ChildItem childItem, int childOrder);\n\nGroupAdapter.setOnGroupItemLongClickListener(OnGroupItemLongClickListener l);\n\nboolean onGroupItemLongClick(ViewHolder holder, View v, int adapterPos, int viewType,\n   Title title, int titleOrder, ChildItem childItem, int childOrder);\n\nAs with the SingleViewTypeAdapter, the GroupAdapter provides a set of methods for easily manipulating data sets held by the Adapter:\n- void setList(List<Group<Title, ChildItem>> groupList);\n- void add(Group<Title, ChildItem> group);\n- void add(int position, Group<Title, ChildItem> group);\n- void remove(int position);\n- void removeLast(); \/ void remove();\n- void removeAll(int positionStart, int itemCount);\n- void set(int position, Group<Title, ChildItem> group);\n- void setAll(int positionStart, int itemCount, Group group);\n- void addAll(int position, List<Group> newGroupList);\n- void addAll(List<Group<Title, ChildItem>> newGroupList);\n\nPlease refer to the codes and comments of GroupAdapter. There is also an example usage in GroupFragment and GroupRVAdapter of the demo.\n","463":"OpenTripPlanner for Android  \nAn Android app for multi-modal trip planning and navigation using any OpenTripPlanner server.\nDownload the app via the Google Play store.\n\nSee more details on the wiki.\nLike hacking things?  See our Developer Guide to get started.\nBuild Setup\nPrerequisites for both Android Studio and Gradle\n\nDownload and install the Android SDK.  Make sure to install the Google APIs for your API level (e.g., 17), the Android SDK Build-tools version for your buildToolsVersion version, and the Android Support Repository and Google Repository.\nSet the \"ANDROID_HOME\" environmental variable to your Android SDK location.\nSet the \"JAVA_HOME\" environmental variables to point to your JDK folder (e.g., \"C:\\Program Files\\Java\\jdk1.6.0_27\")\n\nBuilding in Android Studio\n\nDownload and install the latest version of Android Studio.\nIn Android Studio, choose \"Import Project\" at the welcome screen.\nBrowse to the location of the project, and double-click on the project directory.\nIf prompted with options, check \"Use auto-import\", and select \"Use default gradle wrapper (recommended)\".  Click \"Ok\".\nClick the green play button (or 'Shift->F10') to run the project!\n\nBuilding from the command line using Gradle\n\nTo build and push the app to the device, run gradlew installDebug from the command line at the root of the project\nTo start the app, run adb shell am start -n edu.usf.cutr.opentripplanner.android\/.MyActivity (alternately, you can manually start the app)\n\nRelease builds\nTo build a release build, you need to create a \"gradle.properties\" file that points to a \"secure.properties\" file, and a \"secure.properties\" file that points to your keystore and alias. The gradlew assembleRelease command will prompt for your keystore passphrase.\nThe \"gradle.properties\" file is located in the opentripplanner-android directory and has the contents:\nsecure.properties=<full_path_to_secure_properties_file>\n\nThe \"secure.properties\" file (in the location specified in gradle.properties) has the contents:\nkey.store=<full_path_to_keystore_file>\n\nkey.alias=<key_alias_name>\n\nNote that the paths in these files always use the Unix path separator  \/, even on Windows. If you use the Windows path separator \\ you will get the error No value has been specified for property 'signingConfig.keyAlias'.\nContributing\nWe welcome contributions to the project!  Please see our Contributing Guide for details, including Code Style Guidelines and Template.\nTroubleshooting\nWhen importing to Android Studio, I get an error \"You are using an old, unsupported version of Gradle...\"\nIf you're using Android Studio v0.4.2 or lower, when importing, please be sure to select the \"settings.gradle\" file in the root, NOT the project directory.\nYou will get the above error if you select the project directory \/ name of the project.\nI get build errors for the Android Support libraries or Google APIs\nOpen Android SDK Manager, and under the \"Extras\" category make sure you've installed both the \"Android Support Repository\" (in addition to the \"Android Support library\") as well as the\n\"Google Repository\".  Also, make sure you have the Google API installed for the API level that you're working with in the \"\/build.gradle\" file,\nincluding the \"Android SDK Build-tools\" version (at the top of the \"Tools\" category in the Android SDK Manager) that\nmatches the compileSdkVersion and buildToolsVersion numbers in \/opentripplanner-android\/build.gradle.\nI get the import gradle project error - \u201cCause: unexpected end of block data\u201d\nMake sure you have the Google API installed for the API level that you're working with in the \/build.gradle file,\nincluding the \"Android SDK Build-tools\" version (at the top of the \"Tools\" category in the Android SDK Manager) that\nmatches the compileSdkVersion and buildToolsVersion numbers in \/opentripplanner-android\/build.gradle.\nAndroid Studio or Gradle can't find my Android SDK, or the API Levels that I have installed\nMake sure that you're consistently using the same Android SDK throughout Android Studio and your environmental variables.\nAndroid Studio comes bundled with an Android SDK, and can get confused if you're pointing to this SDK within Android Studio\nbut have your environmental variables pointed elsewhere.  Click \"File->Project Structure\", and then under \"Android SDK\"\nmake sure you \"Android SDK Location\" is the correct location of your Android SDK.\nAlso, make sure you've set the \"ANDROID_HOME\" environmental variable to your Android SDK location and\nthe \"JAVA_HOME\" environmental variables to point to your JDK folder.\nOpenTripPlanner Project\nWant to learn more about the main OpenTripPlanner project? Read up here:\nhttp:\/\/opentripplanner.org\n","464":"ShareSDK for Android\n\nwebsite -- http:\/\/www.mob.com\nwiki -- http:\/\/wiki.mob.com\/Android_%E5%BF%AB%E9%80%9F%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97\nbbs -- http:\/\/bbs.mob.com\/forum-36-1.html\n\nStep One: Download the SDK\nVisit our official website and download the latest version of ShareSDK. After extracting the downloaded file, you will find the following directory structure\uff1a\n\nOpen the ShareSDK for Android directory, you will find MainLibs and OnekeyShare. ShareSDK is stored in the MainLibs directory, and OnekeyShare is a GUI tool for developers to quickly complete the share feature by ShareSDK.\nStep Two: Import ShareSDK to Your Project\nThere are two ways to import ShareSDK into your project: reference to the ShareSDK project or copy the jars and resources into your project. If you select the second way, we provide the following tool to help you quickly finish these operations:\n\nExecute this tool and copy its products into your project.\nShareSDK encourage you integrate ShareSDK by referencing its project, because this will be much simpler. Here are the steps:\n(1) Copy the extracted SDK into your workspace of Eclipse\n(2) Import the SDK projects:\n\nSelect MainLibs and OnekeyShare\n\n(3) Change dependency of your project to OnekeyShare (if you need this GUI tool) or MainLibs\n\nStep Three: Add Applications Information\nThere are three ways to add your applications information into ShareSDK: register on the application console of ShareSDK, configurate the \u201cassets\/ShareSDK.xml\u201d file, or modify by ShareSDK.setPlatformDevInfo(String, HashMap<String, Object>) method at runtime.\nHere is the example of \u201cassets\/ShareSDK.xml\u201d way:\n<ShareSDK\n   AppKey=\"add appkey you got from SahreSDK here\" \/>\n\n<Facebook\n    Id=\"int field, custom value for developer to recognize this platform\"\n    SortId=\"int field, the priority in the registered platforms\"\n    ConsumerKey=\"consumer key you got from Facebook\"\n    ConsumerSecret=\"consumer secret you got from Facebook\"\n    Enable=\"Boolean field, false means to remove the platform from the registered platforms\" \/>\n```\n\nAll applications information is registered in the \u201cassets\/ShareSDK.xml\u201d of ShareSDK Sample project.\n\n# Step Four: Configurate AndroidManifest.xml\n\nAdd the following permissions into your AndroidMenifest.xml:\n\n```` xml\n<uses-permission android:name=\"android.permission.GET_TASKS\" \/>\n<uses-permission android:name=\"android.permission.INTERNET\" \/>\n<uses-permission android:name=\"android.permission.ACCESS_WIFI_STATE\" \/>\n<uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" \/>\n<uses-permission android:name=\"android.permission.CHANGE_WIFI_STATE\" \/>\n<uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" \/>\n<uses-permission android:name=\"android.permission.READ_PHONE_STATE\" \/>\n<uses-permission android:name=\"android.permission.MANAGE_ACCOUNTS\"\/>\n<uses-permission android:name=\"android.permission.GET_ACCOUNTS\"\/>\n```\nYou should add the intent-filter in your  launchActivity if you want to use the KakaoTalk to share msg.\n<!--\n\tIf you share msg in KakaoTalk, your share-params of executeUrl should set the value \"kakaoTalkTest:\/\/starActivity\"\n\tSo it do, when the user to click the share-msg, then startActivity of your app's launch-activity. \n\tWhen you use the lib of onekeyshare, you can use the method of \n    setExecuteUrl(\"kakaoTalkTest:\/\/starActivity\") to set executeUrl.\n-->\n    <intent-filter>\n        <data android:scheme=\"kakaoTalkTest\" android:host=\"starActivity\"\/>\n        <action android:name=\"android.intent.action.VIEW\" \/>\n        <category android:name=\"android.intent.category.BROWSABLE\" \/>\n        <category android:name=\"android.intent.category.DEFAULT\" \/>\n    <\/intent-filter>\n```\n\t\t\nAnd the single Activity for GUIs of ShareSDK:\n\n```` xml\n<activity\n   android:name=\"cn.sharesdk.framework.ShareSDKUIShell\"\n   android:theme=\"@android:style\/Theme.Translucent.NoTitleBar\"\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\n   android:screenOrientation=\"portrait\"\n   android:windowSoftInputMode=\"stateHidden|adjustResize\" \/>\n```\n\nIf you integrate Wechat, add this callback activity:\n\n```` xml\n<activity\n   android:name=\".wxapi.WXEntryActivity\"\n   android:theme=\"@android:style\/Theme.Translucent.NoTitleBar\"\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\n   android:exported=\"true\"\n   android:screenOrientation=\"portrait\" \/>\n```\n\nAnd if you integrate Yixin, add this callback activity:\n\n```` xml\n<activity\n   android:name=\".yxapi.YXEntryActivity\"\n   android:theme=\"@android:style\/Theme.Translucent.NoTitleBar\"\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\n   android:exported=\"true\"\n   android:screenOrientation=\"portrait\" \/>\n```\n\n# Step Five: Add Codes\n\nAdd the following line in the **onCreate** method of **the entrance activity**:\n\n````java\nShareSDK.initSDK(this);\n```\n\nAnd add the following line int the **onDestroy** method of **the last activity**:\n\n````java\nShareSDK.stopSDK(this);\n```\n\n# Screenshots\n![logo grid view of onekeyshare](http:\/\/wiki.sharesdk.cn\/images\/thumb\/a\/ad\/p4.png\/337px-p4.png)\n![edit page of onekeyshare](http:\/\/wiki.sharesdk.cn\/images\/thumb\/b\/b1\/p5.png\/337px-p5.png)\n![image preview](http:\/\/wiki.sharesdk.cn\/images\/thumb\/8\/88\/p6.png\/337px-p6.png)\n![authorizes](http:\/\/wiki.sharesdk.cn\/images\/thumb\/6\/69\/p7.png\/337px-p7.png)\n\n# And the Next\n\nFor more information about how to integrate ShareSDK or how use ShareSDK to get your friends list, following someone, share statuses, etc. please visit our [official wiki](http:\/\/wiki.sharesdk.cn\/Android_%E5%BF%AB%E9%80%9F%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97).\n","465":"ShareSDK for Android\n\nwebsite -- http:\/\/www.mob.com\nwiki -- http:\/\/wiki.mob.com\/Android_%E5%BF%AB%E9%80%9F%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97\nbbs -- http:\/\/bbs.mob.com\/forum-36-1.html\n\nStep One: Download the SDK\nVisit our official website and download the latest version of ShareSDK. After extracting the downloaded file, you will find the following directory structure\uff1a\n\nOpen the ShareSDK for Android directory, you will find MainLibs and OnekeyShare. ShareSDK is stored in the MainLibs directory, and OnekeyShare is a GUI tool for developers to quickly complete the share feature by ShareSDK.\nStep Two: Import ShareSDK to Your Project\nThere are two ways to import ShareSDK into your project: reference to the ShareSDK project or copy the jars and resources into your project. If you select the second way, we provide the following tool to help you quickly finish these operations:\n\nExecute this tool and copy its products into your project.\nShareSDK encourage you integrate ShareSDK by referencing its project, because this will be much simpler. Here are the steps:\n(1) Copy the extracted SDK into your workspace of Eclipse\n(2) Import the SDK projects:\n\nSelect MainLibs and OnekeyShare\n\n(3) Change dependency of your project to OnekeyShare (if you need this GUI tool) or MainLibs\n\nStep Three: Add Applications Information\nThere are three ways to add your applications information into ShareSDK: register on the application console of ShareSDK, configurate the \u201cassets\/ShareSDK.xml\u201d file, or modify by ShareSDK.setPlatformDevInfo(String, HashMap<String, Object>) method at runtime.\nHere is the example of \u201cassets\/ShareSDK.xml\u201d way:\n<ShareSDK\n   AppKey=\"add appkey you got from SahreSDK here\" \/>\n\n<Facebook\n    Id=\"int field, custom value for developer to recognize this platform\"\n    SortId=\"int field, the priority in the registered platforms\"\n    ConsumerKey=\"consumer key you got from Facebook\"\n    ConsumerSecret=\"consumer secret you got from Facebook\"\n    Enable=\"Boolean field, false means to remove the platform from the registered platforms\" \/>\n```\n\nAll applications information is registered in the \u201cassets\/ShareSDK.xml\u201d of ShareSDK Sample project.\n\n# Step Four: Configurate AndroidManifest.xml\n\nAdd the following permissions into your AndroidMenifest.xml:\n\n```` xml\n<uses-permission android:name=\"android.permission.GET_TASKS\" \/>\n<uses-permission android:name=\"android.permission.INTERNET\" \/>\n<uses-permission android:name=\"android.permission.ACCESS_WIFI_STATE\" \/>\n<uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" \/>\n<uses-permission android:name=\"android.permission.CHANGE_WIFI_STATE\" \/>\n<uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" \/>\n<uses-permission android:name=\"android.permission.READ_PHONE_STATE\" \/>\n<uses-permission android:name=\"android.permission.MANAGE_ACCOUNTS\"\/>\n<uses-permission android:name=\"android.permission.GET_ACCOUNTS\"\/>\n```\nYou should add the intent-filter in your  launchActivity if you want to use the KakaoTalk to share msg.\n<!--\n\tIf you share msg in KakaoTalk, your share-params of executeUrl should set the value \"kakaoTalkTest:\/\/starActivity\"\n\tSo it do, when the user to click the share-msg, then startActivity of your app's launch-activity. \n\tWhen you use the lib of onekeyshare, you can use the method of \n    setExecuteUrl(\"kakaoTalkTest:\/\/starActivity\") to set executeUrl.\n-->\n    <intent-filter>\n        <data android:scheme=\"kakaoTalkTest\" android:host=\"starActivity\"\/>\n        <action android:name=\"android.intent.action.VIEW\" \/>\n        <category android:name=\"android.intent.category.BROWSABLE\" \/>\n        <category android:name=\"android.intent.category.DEFAULT\" \/>\n    <\/intent-filter>\n```\n\t\t\nAnd the single Activity for GUIs of ShareSDK:\n\n```` xml\n<activity\n   android:name=\"cn.sharesdk.framework.ShareSDKUIShell\"\n   android:theme=\"@android:style\/Theme.Translucent.NoTitleBar\"\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\n   android:screenOrientation=\"portrait\"\n   android:windowSoftInputMode=\"stateHidden|adjustResize\" \/>\n```\n\nIf you integrate Wechat, add this callback activity:\n\n```` xml\n<activity\n   android:name=\".wxapi.WXEntryActivity\"\n   android:theme=\"@android:style\/Theme.Translucent.NoTitleBar\"\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\n   android:exported=\"true\"\n   android:screenOrientation=\"portrait\" \/>\n```\n\nAnd if you integrate Yixin, add this callback activity:\n\n```` xml\n<activity\n   android:name=\".yxapi.YXEntryActivity\"\n   android:theme=\"@android:style\/Theme.Translucent.NoTitleBar\"\n   android:configChanges=\"keyboardHidden|orientation|screenSize\"\n   android:exported=\"true\"\n   android:screenOrientation=\"portrait\" \/>\n```\n\n# Step Five: Add Codes\n\nAdd the following line in the **onCreate** method of **the entrance activity**:\n\n````java\nShareSDK.initSDK(this);\n```\n\nAnd add the following line int the **onDestroy** method of **the last activity**:\n\n````java\nShareSDK.stopSDK(this);\n```\n\n# Screenshots\n![logo grid view of onekeyshare](http:\/\/wiki.sharesdk.cn\/images\/thumb\/a\/ad\/p4.png\/337px-p4.png)\n![edit page of onekeyshare](http:\/\/wiki.sharesdk.cn\/images\/thumb\/b\/b1\/p5.png\/337px-p5.png)\n![image preview](http:\/\/wiki.sharesdk.cn\/images\/thumb\/8\/88\/p6.png\/337px-p6.png)\n![authorizes](http:\/\/wiki.sharesdk.cn\/images\/thumb\/6\/69\/p7.png\/337px-p7.png)\n\n# And the Next\n\nFor more information about how to integrate ShareSDK or how use ShareSDK to get your friends list, following someone, share statuses, etc. please visit our [official wiki](http:\/\/wiki.sharesdk.cn\/Android_%E5%BF%AB%E9%80%9F%E9%9B%86%E6%88%90%E6%8C%87%E5%8D%97).\n","466":"Compass\nThis sample inserts a live card to the left of the Glass clock that displays a\ncompass. Tapping the live card presents a menu with two options:\n\nRead aloud: read the compass's current heading using text-to-speech\nStop: remove the compass from the timeline\n\nThe compass also contains a small list of landmarks that will appear on the\nscreen when the user is within 10 km of those locations. See the\nres\/raw\/landmarks.json file to add your own.\nGetting started\nCheck out our documentation to learn how to get started on\nhttps:\/\/developers.google.com\/glass\/gdk\/index\nRunning the sample on Glass\nYou can use your IDE to compile and install the sample or use\nadb\non the command line:\n$ adb install -r CompassSample.apk\n\nTo start the sample, say \"ok glass, show a compass\" from the Glass clock\nscreen or use the touch menu.\n","467":"dlib_for_arm\nvery fast face detection for ARM platform.\nThe code is based on dlib with the following enhancement\n\nreduce work load : only use 1 filter(front looking) instead of 5 filters in frontal_face_detector.h\nthread level parallelism : use 3 threads to do face detection\nSIMD : use arm neon to implement dlib\/simd\/\n\n","468":"injeqt\nSimple dependency injection framework for Qt\nDocumentation\nDocumentation available at github.\nExample\nHere is example of what can be done using injeqt:\n#include <injeqt\/injector.h>\n#include <injeqt\/module.h>\n\n#include <QtCore\/QObject>\n#include <iostream>\n#include <memory>\n#include <string>\n\nclass hello_service : public QObject\n{\n\tQ_OBJECT\n\npublic:\n\thello_service() {}\n\tvirtual ~hello_service() {}\n\n\tstd::string say_hello() const\n\t{\n\t\treturn {\"Hello\"};\n\t}\n};\n\nclass world_service : public QObject\n{\n\tQ_OBJECT\n\npublic:\n\tworld_service() {}\n\tvirtual ~world_service() {}\n\n\tstd::string say_world() const\n\t{\n\t\treturn {\"World\"};\n\t}\n};\n\nclass hello_factory : public QObject\n{\n\tQ_OBJECT\n\npublic:\n\tQ_INVOKABLE hello_factory() {}\n\tvirtual ~hello_factory() {}\n\n\tQ_INVOKABLE hello_service * create_service()\n\t{\n\t\treturn new hello_service{};\n\t}\n};\n\nclass hello_client : public QObject\n{\n\tQ_OBJECT\n\npublic:\n\tQ_INVOKABLE hello_client() : _s{nullptr}, _w{nullptr} {}\n\tvirtual ~hello_client() {}\n\n\tstd::string say() const\n\t{\n\t\treturn _s->say_hello() + \" \" + _w->say_world() + \"!\";\n\t}\n\nprivate slots:\n\tINJEQT_INIT void init()\n\t{\n\t\tstd::cerr << \"all services set\" << std::endl;\n\t}\n\n\tINJEQT_DONE void done()\n\t{\n\t\tstd::cerr << \"ready for destruction\" << std::endl;\n\t}\n\n\tINJEQT_SET void set_hello_service(hello_service *s)\n\t{\n\t\t_s = s;\n\t}\n\n\tINJEQT_SET void set_world_service(world_service *w)\n\t{\n\t\t_w = w;\n\t}\n\nprivate:\n\thello_service *_s;\n\tworld_service *_w;\n\n};\n\nclass module : public injeqt::module\n{\npublic:\n\texplicit module()\n\t{\n\t\t_w = std::unique_ptr<world_service>{new world_service{}};\n\n\t\tadd_type<hello_client>();\n\t\tadd_type<hello_factory>();\n\t\tadd_factory<hello_service, hello_factory>();\n\t\tadd_ready_object<world_service>(_w.get());\n\t}\n\n\tvirtual ~module() {}\n\nprivate:\n\tstd::unique_ptr<world_service> _w;\n\n};\n\nint main()\n{\n\tauto modules = std::vector<std::unique_ptr<injeqt::module>>{};\n\tmodules.emplace_back(std::unique_ptr<injeqt::module>{new module{}});\n\n\tauto injector = injeqt::injector{std::move(modules)};\n\tauto client = injector.get<hello_client>();\n\tauto hello = client->say();\n\n\tstd::cout << hello << std::endl;\n}\n\n#include \"hello-world.moc\"\n\nIn that example we can see two main services names hello_service and world_service. There\nis also client of these names hello_client. In module class we configure how we create\nand access these instances.\nhello_client is added using add_type function. It means that injeqt will try to create it\nusing default constructor. We provide that by declaration of Q_INVOKABLE hello_client()\n(Q_INVOKABLE is requires by Qt's meta object system).\nhello_service is added using add_factory function. It means that injeqt will first try to\ncreate a hello_factory object, then it will look for a method of that objet that returns\nhello_service object. It will find Q_INVOKABLE hello_service * create_service() and use it.\nTo be able to create hello_factory injeqt must know about it, so we also add it using add_type\nmethod.\nLast, world_service, is added as a ready object - provided from outside of injeqt scope.\nIn main method list of conifguration modules are passed to newly created injector instance.\nFrom that moment, we can use injector to create and manage our services. Just one line below\nan hello_client instance is required. This is what happens next:\n\ninjeqt looks for dependencies of hello_client and found that it first needs to create hello_factory\ninjeqt creates hello_factory without problems, as it does not have dependencies of its own\ninjeqt adds new instance to object pool\ninjeqt calls hello_factory::create_service() methods and adds its result to object pool\nnow all dependencies of hello_client are available, so new instance of it is created with\ndefault constructor and its added to objec tpool\nall methods of hello_client marked with INJEQT_SET are called with proper objects from pool\nall methods of hello_client marked with INJEQT_INIT are called\nthis instance is returned to caller\nbefore injector is destructed, all methods of hello_client marked with INJEQT_DONE are called\n\n","469":"#\u534e\u8f6f\u7f51\u7edc\u5b89\u5168\u5c0f\u7ec4\u9006\u5411\u5de5\u7a0b\u8bad\u7ec3\u8425\n###SNST Reverse Engineering Traning\n\u5404\u4e2a\u8bad\u7ec3\u7a0b\u5e8f\u89e3\u6790\n\n###1.RE-50 Writeup\n\u628a\u4ee3\u7801\u5bfc\u5165\u5230IDA ,\u7528Hex-ray \u628aMain \u51fd\u6570\u6c47\u7f16\u8f6c\u5230\u4f2aC \u4ee3\u7801,\u7ed3\u679c\u5982\u4e0b\n\n\u7a0b\u5e8f\u4ee3\u7801\u610f\u601d\u662f:\u5148\u7ed9\u672c\u5730\u7684\u6570\u7ec4\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u8d4b\u503c,\u7136\u540e\u63a5\u6536\u6211\u4eec\u8f93\u5165\u7684\u5b57\u7b26\u4e32,\u518d\u901a\u8fc7\u5b57\u7b26\u4e32\u5bf9\u6bd4\u51fd\u6570\u6765\u5bf9\u6bd4\u8f93\u5165\u7684\u5b57\u7b26\u4e32\u662f\u5426\u548c\u672c\u5730\u6821\u9a8c\u7684\u5b57\u7b26\u4e32\u4e00\u6837,\u4e8e\u662f\u5728strcmp \u5904\u4e0b\u65ad\u70b9,\u4f4d\u7f6e\u5982\u4e0b\n\n\u542f\u52a8Ollydbg ,\u968f\u610f\u8f93\u5165\u5b57\u7b26\u4e32\n\nOllydbg \u4f1a\u5361\u57280x40109D \u8fd9\u4e2a\u5730\u65b9(\u4e0b\u65ad\u70b9\u7684\u5feb\u6377\u952e\u662fF2 )\n\n\u67e5\u770b\u5bc4\u5b58\u5668\u7a97\u53e3,\u53ef\u4ee5\u770b\u5230flag\n\n\n###2.RE-100 Writeup\n\u540c\u6837\u662f\u8f6c\u6362\u5230\u4f2aC \u4ee3\u7801\u5206\u6790\n\n\u7a0b\u5e8f\u539f\u7406:\u5224\u65ad\u8f93\u5165\u7684\u5b57\u7b26\u4e32\u957f\u5ea6\u662f\u5426\u4e3a19 ,\u7b26\u5408\u7684\u8bdd\u628a\u8f93\u5165\u7684\u5b57\u7b26\u4e32\u91cc\u9762\u7684\u6bcf\u4e2a\u5b57\u7b26\u7684\u503c\u52a050 \u548cbyte_408030 \u91cc\u9762\u7684\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4,\u4e8e\u662f\u6211\u4eec\u628a\u8fdb\u53bbbyte_408030 ,\u8bbe\u7f6e\u5b83\u7684Array (\u6570\u7ec4)\u957f\u5ea6\n\n\u7136\u540e\u6309\u51e0\u4e0bd \u952e\u628a\u8fd9\u4e2a\u4e32\u8f6c\u6362\u6210char\n\n\u63d0\u53d6\u8fd9\u4e9b\u6570\u636e\u51fa\u6765,\u62ff\u5230python \u91cc\u9762\u505a\u9006\u8fd0\u7b97(\u56e0\u4e3a\u7a0b\u5e8f\u662f\u628a\u6211\u4eec\u8f93\u5165\u7684\u6570\u636e\u52a0\u4e0a50 \u518d\u548c\u8fd9\u4e9b\u6570\u636e\u505a\u6bd4\u8f83\u7684,\u6240\u4ee5\u6211\u4eec\u628a\u8fd9\u4e9b\u6570\u636e\u51cf50 \u5c31\u53ef\u4ee5\u83b7\u53d6\u5230\u539f\u6765\u7684\u5b57\u7b26\u4e32\u4e86)\ncode=[0x9A,0xA4,0x95,0xA4,0x98,0xAD,0x84,0x77,0x63,0x62,0x62,0x91,0x75,0x93,0x9E,0x95,0xA7,0xAF]\n\nfor index in code :\n    print chr(index-50),\n\n\n\n###3.RE-250 Writeup\n\u5728main \u51fd\u6570\u7684\u5165\u53e3\u5904\u548c\u4e4b\u524d\u6240\u89c1\u7684\u6709\u70b9\u4e0d\u540c,\u6211\u4eec\u76f4\u63a5\u5173\u6ce8jmp loc_401113\n\n\u8df3\u5230jmp loc_401113 \u4e2d\u5206\u6790\u4ee3\u7801,\u53ef\u4ee5\u770b\u5230\u8fd9\u91cc\u6709SEH \u5f02\u5e38\u5904\u7406,\u5f02\u5e38\u56de\u8c03\u7684\u5730\u5740\u662f0x401053\n\n\u5f80\u4e0b\u5206\u6790,\u7a0b\u5e8f\u4f1a\u6267\u884c\u52300x40115A ,\u8fd9\u662f\u4e00\u4e2a\u65e0\u6548\u7684\u6307\u4ee4,\u4e8e\u662f\u4f1a\u5370\u53d1SEH \u5f02\u5e38\u5904\u7406,\u7a0b\u5e8f\u8df3\u8f6c\u52300x401053\n\n\u4e8e\u662f\u627e\u52300x401053 ,\u5206\u6790\n\n\u53ef\u4ee5\u770b\u5230\u8fd9\u91cc\u7684\u5bf9\u6bd4\u8fd0\u7b97:\u8f93\u5165\u7684\u5b57\u7b26\u4e32\u548cbyte_40B938 \u91cc\u9762\u7684\u6570\u636e\u505a\u5f02\u6216\u8fd0\u7b97\u4e4b\u540e\u518d\u5bf9\u6bd4\u4e24\u4e2a\u5b57\u7b26\u662f\u5426\u76f8\u540c,\u5f02\u6216\u8fd0\u7b97\u7684key \u662f80 \u52a0\u4e0a\u5f53\u524d\u5bf9\u6bd4\u7684\u504f\u79fb\u4f4d\u7f6e,\u4e8e\u662f\u53ef\u4ee5\u5199\u51fa\u89e3\u5bc6python\ncode=[0x38,0x23,0x31,0x27,0x32,0x2E,0x4,0x32,0x7,0x6B,0x6F,0x6B,0x3,0x25,0x31,0x2D,0x1D]\nxor_number=80\n\nfor index in code :\n    print chr(index^xor_number),\n    xor_number+=1\n\n\n\n###4.RE-500 Writeup\n\u7b2c\u4e00\u6b65\u9996\u5148\u8981\u7ed5\u8fc7IDA \u672c\u8eab\u7684bug ,\u56e0\u4e3aIDA \u9ed8\u8ba4\u662f\u4ece\u7b2c\u4e00\u533a\u5757\u5f00\u59cb\u89e3\u6790\u6570\u636e\u7684,\u4f46\u662f\u5728\u5165\u53e3\u70b9\u4e2d\u7684\u8df3\u8f6c\u5374\u662f\u5728\u7b2c\u4e00\u533a\u5757\u4e4b\u524d\u7684,\u6240\u4ee5IDA \u65e0\u6cd5\u4ece\u8fd9\u4e2a\u4f4d\u7f6e\u4e2d\u83b7\u53d6\u6570\u636e,\u4e8e\u662f\u4f7f\u7528OllyDbg \u8ddf\u8e2a\n\n\u8ddf\u8e2a\u52300x401041 \u5904,\u7ee7\u7eed\u5f80\u4e0b\u5355\u6b65\n\n\u53ef\u4ee5\u770b\u5230\u8fd9\u53c8\u662fSEH \u5f02\u5e38\u5904\u7406,\u76f4\u63a5\u8df3\u52300x404100\n\n\u6765\u52300x404100 ,\u5148\u7ed9\u5b83\u521b\u5efa\u51fd\u6570\n\n\u521b\u5efa\u51fd\u6570\u4e4b\u540e\u7684\u53d8\u5316\n\n\u7136\u540e\u7528Hex-ray\u8f6c\u6362\n\n\u8fdb\u53bb\u52300x401014\n\n\u5207\u6362\u56de\u6c47\u7f16\u7a97\u53e3\u67e5\u770b\u4ee3\u7801\n\n\u7ee7\u7eed\u8df3\u52300x40199E \u53bb\u5206\u6790,\u8fd9\u91cc\u521b\u5efa\u4e00\u4e2a\u7ebf\u7a0b,\u7ebf\u7a0b\u5165\u53e3\u5730\u5740\u662f0x401023 ,\u4e8e\u662f\u6211\u4eec\u8fc7\u53bb\u5206\u6790\u8fd9\u4e2a\u51fd\u6570\u7684\u529f\u80fd\n\n\u53d1\u73b0\u8fd9\u91cc\u4e00\u76f4\u6b7b\u5faa\u73af\u8c03\u7528\u4e24\u4e2a\u51fd\u6570\n\nsub_40102D \u7684\u4ee3\u7801\u5982\u4e0b(\u4e3a\u4ec0\u4e48\u4f1a\u663e\u793asub_401400 \u5462?\u56e0\u4e3a\u4ee3\u7801\u662f\u4ecesub_40102D \u76f4\u63a5jmp \u5230sub_401400 \u7684):\u4ecePEB \u4e2d\u83b7\u53d6\u7a0b\u5e8f\u662f\u5426\u88ab\u8c03\u8bd5\n\nSub_401028 \u7684\u529f\u80fd\u662f\u76f4\u63a5\u8c03\u7528IsDebuggerPresent() \u83b7\u53d6\u662f\u5426\u88ab\u8c03\u8bd5\n\n\u8f6c\u6362\u5230\u6c47\u7f16,\u53ef\u4ee5\u53d1\u73b0,\u6bcf\u6b21\u68c0\u6d4b\u5230\u7a0b\u5e8f\u88ab\u8c03\u8bd5\u7684\u8bdd\u5c31\u4f1a\u96640 ,\u4ea7\u751f\u5f02\u5e38\u9000\u51fa\u7a0b\u5e8f,\u4e8e\u662f\u8fd9\u4e2a\u7ebf\u7a0b\u662f\u7528\u6765\u53cd\u8c03\u8bd5\u7684\n\n\u56de\u53bb\u4e3b\u7ebf\u4ee3\u7801\u7ee7\u7eed\u5206\u6790,\u9047\u5230\u4ee3\u7801\u6df7\u6dc6\n\n\u5148\u52300x4019C8 \u4e2d\u7528c \u952e\u628a\u4ee3\u7801\u8f6c\u6362\u6210\u4e8c\u8fdb\u5236\u6570\u636e\n\n\u518d\u52300x4019C9 \u4e2d\u7528d \u952e\u628a\u4e8c\u8fdb\u6307\u6570\u636e\u8f6c\u6362\u6210\u4ee3\u7801,\u8f6c\u6362\u4e4b\u540e\u7684\u4ee3\u7801\u5982\u4e0b:\n\n\u7ee7\u7eed\u5f80\u4e0b\u5206\u6790,\u8fd9\u91cc\u8ba1\u7b97\u51fd\u6570\u7684\u771f\u5b9e\u5730\u5740,\u5177\u4f53\u4f5c\u7528\u8fd8\u4e0d\u77e5\u9053\n\n\u7ee7\u7eed\u5f80\u4e0b,\u53d1\u73b0\u7a0b\u5e8f\u53c8\u521b\u5efa\u65b0\u7ebf\u7a0b\n\n\u4e8e\u662f\u6211\u4eec\u67e5\u770b\u7ebf\u7a0b\u91cc\u9762\u7684\u4ee3\u7801,\u8fd9\u91cc\u662f\u8981\u8ba9\u6211\u4eec\u8f93\u5165flag\n\n\u518d\u5f80\u4e0b,\u7ebf\u7a0b\u81ea\u5df1\u5c31\u5173\u95ed\u4e86\n\n\u4e3b\u7ebf\u4ee3\u7801\u91cc\u9762\u5728\u521b\u5efa\u7ebf\u7a0b\u6210\u529f\u4e4b\u540e\u5c31\u963b\u585e\u7b49\u5f85\u5b83\u5173\u95ed,\u5982\u679c\u521b\u5efa\u5931\u8d25\u7684\u8bdd\u5c31\u5173\u95ed\u7a0b\u5e8f\n\n\u7ee7\u7eed\u5f80\u4e0b,\u53d1\u73b0\u53c8\u6709\u65b0\u7ebf\u7a0b\u2026\n\n\u5728\u7ebf\u7a0b\u6267\u884c\u7684\u4ee3\u7801\u6700\u540e\u6709\u4e00\u5904\u6df7\u6dc6\n\n\u56e0\u4e3aPush EAX + Retn \u7b49\u4ef7\u4e8eJmp Eax ,eax \u7684\u503c\u662fecx+40 (sub -40 \u7b49\u4e8eadd 40),ecx \u7684\u503c\u662f\u8fd9\u4e2a\u51fd\u6570\u7684\u5165\u53e3\u5730\u5740,\u4e8e\u662f\u53ef\u4ee5\u8ba1\u7b97\u51fa\u63a5\u4e0b\u6765\u7a0b\u5e8f\u8fd0\u884c\u5230\u8fd9\u91cc\u7684\u5c06\u4f1a\u8df3\u8f6c\u5230\u4e0b\u9762\u8fd9\u4e2a\u5730\u5740\n0x401610+0x40=0x401650\n\u8df3\u8f6c\u5230\u6b64\n\nC \u952e\u8f6c\u6362\u4ee3\u7801\n\n\u7136\u540e\u53c8\u662f\u8fc7\u6df7\u6dc6,\u76f8\u4fe1\u5927\u5bb6\u5df2\u7ecf\u660e\u767d\u5566\n\n\u4e0b\u9762\u9047\u5230switch \u8bed\u53e5,\u5148\u6765\u5206\u6790\u4e0b\u6bcf\u4e2acases \u7684\u610f\u4e49\n\nCase 0 \u662f\u68c0\u6d4b\u7a0b\u5e8f\u662f\u5426\u88ab\u8c03\u8bd5\n\nCase 1 \u5f80\u7f13\u51b2\u533a\u4e2d\u5199ERROR\n\nCase 2 \u8f93\u51fa\u521a\u624d\u7684\u7f13\u51b2\u533a\n\nCase 3 \u628aOK \u5199\u5165\u7f13\u51b2\u533a\n\nCase 4 \u662fflag \u5bf9\u6bd4,\u7136\u540e\u6211\u4eec\u628a\u7cbe\u529b\u4e3b\u8981\u96c6\u4e2d\u5728\u6b64\n\n\u8fd9\u91cc\u8c03\u7528\u4e86\u4e24\u4e2a\u672a\u77e5\u7684\u51fd\u6570,\u5148\u5ffd\u7565\u4ed6\u4eec,\u7ee7\u7eed\u5f80\u4e0b\u5206\u6790\n\n\u53d1\u73b0\u4e0b\u9762\u6709\u4e00\u6bb5\u6570\u636e\n\n\u7136\u540e\u628a\u8fd9\u6bb5\u6570\u636e\u548c\u53e6\u4e00\u4e2a\u7f13\u51b2\u533a\u505a\u5bf9\u6bd4\n\n\u5f80\u4e0a\u5bfb\u627eEBP-0x28 ,\u53d1\u73b0\u521a\u624d\u5ffd\u7565\u7684\u51fd\u6570\u6709\u5229\u7528\u5230\u8fd9\u4e24\u4e2a\u7f13\u51b2\u533a\n\n\u5728\u5206\u6790sub_401019 \u4e2d\u53d1\u73b0,\u8fd9\u4e2a\u5e94\u8be5\u662f\u52a0\u5bc6\u51fd\u6570\n\n\u51fd\u6570\u7684\u53c2\u6570\u6709\u4e24\u4e2a,\u4e8e\u662f\u56de\u53bb\u770b\u770b\u5230\u5e95\u7a0b\u5e8f\u4f20\u54ea\u4e24\u4e2a\u53c2\u6570\u8fdb\u53bb\n\n\u53ef\u4ee5\u770b\u5230,\u4e00\u4e2a\u662f0x10 ,\u4e00\u4e2a\u662f\u7f13\u51b2\u533a\u5730\u5740,\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5927\u80c6\u7684\u786e\u5b9a,a1 \u662f\u8981\u52a0\u5bc6\u7684\u6570\u636e\u5730\u5740,a2 \u662f\u6570\u636e\u957f\u5ea6\n\n\u7ed9\u52a0\u5bc6\u51fd\u6570\u4e2d\u7684\u53d8\u91cf\u5199\u597d\u547d\u540d,\u6574\u4f53\u7684\u52a0\u5bc6\u6d41\u7a0b\u4e5f\u5c31\u4e00\u770b\u660e\u4e86\n\n\u4e8e\u662f\u83b7\u53d6\u5230\u8fd9\u6bb5\u52a0\u5bc6\u8fc7\u540e\u7684\u5b57\u7b26\u4e32\n\n\u653e\u5230python \u91cc\u9762\u89e3\u7801\ndef decode(string) :\n    for index in range(0,16,2) :\n        string[index]=string[index]^16\n        string[index+1]=string[index+1]^16\n        bit_1_high=string[index]&0xF\n        bit_2_low=(string[index]&0xF0)>>4\n        bit_2_high=string[index+1]&0xF\n        bit_1_low=(string[index+1]&0xF0)>>4\n        source_bit_1=bit_1_high*16+bit_1_low\n        source_bit_2=bit_2_high*16+bit_2_low\n        print chr(source_bit_1),chr(source_bit_2),\n    \ncode=[0xE7,0x86,0xE7,0x45,0x06,0x26,0xE6,0xF5,0x06,0xC6,0x46,0xA6,0x05,0xE6,0xD6,0xD6]\ndecode(code)\n\n\n\u5408\u8d77\u6765\u5c31\u662fhrctf:{you_can_make_all}\n","470":"PWP3D\nThis is a cmake version of the PWP3D. The original VS version is availabled at:\nhttp:\/\/www.robots.ox.ac.uk\/~victor\/code.html\nFor more detail of the paper, see:\nSee original paper from:\nPWP3D: Real-time Segmentation and Tracking of 3D Objects\nVictor Adrian Prisacariu, Ian Reid\n","471":"#jq-tiles\nSlideshow with many cool css3 effects.\nDemo: http:\/\/elclanrs.github.com\/jq-tiles\/ (Use Google Chrome for best experience)\nSupport: Webkit, Firefox, Opera, IE10, IE9-8*\nLicense: MIT\nOptions:\n{\n  x              : 4, \/\/ # of tiles in x axis, 20 max\n  y              : 4, \/\/ # of tiles in y axis, 20 max\n  effect         : 'default',\n  fade           : false, \/\/ fade images in addition to the tiles effect\n  random         : false, \/\/ animate tiles in random order\n  reverse        : false, \/\/ start animation from opposite direction\n  backReverse    : false, \/\/ reverse the animation when going back in the slideshow (useful for some effects)\n  rewind         : false, \/\/ reverse animation at a certain percentage in time\n  auto           : false, \/\/ Start the slideshow on load\n  loop           : false, \/\/ Start slideshow again when it finishes\n  slideSpeed     : 3500, \/\/ time between slides\n  tileSpeed      : 800, \/\/ time to clear all tiles\n  cssSpeed       : 300, \/\/ css3 transition speed [100,200,300,400,500,600,700,800,900,1000],\n  nav            : true, \/\/ Add navigation\n  navWrap        : null, \/\/ Add the navigation to an existing element\n  bullets        : true, \/\/ Show bullets, if false the show pagination with numbers\n  thumbs         : true, \/\/ Show thumbnails when hovering nav\n  thumbSize      : 25, \/\/ Thumbnail size (percentage of the original image)\n  timer          : true \/\/ show or hide the timer bar\n  beforeChange   : function() {}, \/\/ Runs before changing the image\n  afterChange    : function() {} \/\/ Runs after the tiles have cleared\n  onSlideshowEnd : function() {} \/\/ Runs when the slideshow finishes ( \"loop\" must be set to false )\n}\nMethods:\nstart\n$('.slider').tilesSlider('start')\nstop\n$('.slider').tilesSlider('stop')\nnext\n$('.slider').tilesSlider('next', callback)\nprev\n$('.slider').tilesSlider('prev', callback)\nUsage:\nHTML:\n<div class=\"slider\">\n  <img src=\"img1.jpg\"\/> <!-- No description -->\n  <img src=\"img2.jpg\"\/><p>Description image two<\/p>\n  <img src=\"img3.jpg\"\/><p>Description image three<\/p>\n<\/div>\nCSS:\n.slider { width: 600px; height: 400px; }\njQuery:\n$('.slider').tilesSlider({ random: true })\n","472":"backbone-d3\nWith backbone-d3 we aim to provide a simple interface to visualise with d3\ndynamic data held in backbone collections. The simple visualisations provided\n(pie, bar, line, scatter) are as much for demonstration or testing as for wider\nuse. We've tried to connect two great packages without putting too much between\nthem.\nHopefully this means you can quickly create some basic plots of your data or\nget into more sophisticated visualisations without having to fight with\nasynchronous JavaScript or a lot of wrapper code.\nExamples from the repo are also hosted on http:\/\/drsm79.github.com\/Backbone-d3\/\nso you can see the library in action.\nPlotCollection\nAny backbone collection of any backbone model can be used by the plotting view.\nThe collection containing them can have a caption variable set which will be\nrendered (using Markdown if pagedown is available) under the plot. This is all\nthe backbone.d3.PlotCollection does.\nSome simple collections are provided for use with the canned views. If your\ndata maps onto these models\/collections well you can just reuse them. If you\nalready have model\/collections in use you should be able to reuse them\ntrivially.\nPlotView\nPlotView is where the magic happens. It deals with the captioning of plots and\nmakes sure the appropriate actions are taken when data in the collection\nchanges. The PlotView defines how the data is extracted from the collection\n(through the plotdata() method) and how it is rendered to the browser via the\nplot() method.\nThe PlotView is responsible for both formatting and rendering the plot to allow\none collection to be visualised in multiple ways. The view can also hold a\ncaption for the visualisation, overriding the one set on the data collection.\nStreaming\nAs your collection changes so should your visualisation. The change\/add\/remove\ntriggers are bound to the redraw method of the plot view (reset redraws the\nvisualisation from scratch. This triggers a d3 transformation, updating the\nplot in place with your new data. Tasty!\nThe canned plot views\nEach type of visualisation (should be careful about referring to them as\nplots...) is a Backbone view. There are canned views for some common\nvisualisations, or ones we've needed ourselves, that are available in\nindividual files (to minimise what gets loaded) under the Backbone.d3.Canned\nnamespace.\nIf you want to create more interesting visualisations you'll be subclassing the\nPlotView baseclass (please send pull requests if you make something nice!).\n","473":"CMND\nCommand Line Interface Utility for Node.js\nCMND is a package that lets you easily create CLI tools in Node.js using\nidiomatic ES6 syntax (Node 4+). It's also simple to create associated manual\n(help) pages for each command.\nThis module was initially built for Nodal, but\ncan be used anywhere you'd like.\nUsage\nTo use CMND, first install it in your Node project with npm install cmnd --save.\nNext, modify your project's package.json to include:\n\"bin\": {\n  \"mycli\": \".\/cli.js\"\n}\n\nWhere mycli is the intended name of your command in the CLI.\nNow create a file: .\/cli.js and folder .\/commands:\n#!\/usr\/bin\/env node\n\n'use strict';\n\nconst CommandLineInterface = require('cmnd').CommandLineInterface;\nconst CLI = new CommandLineInterface();\n\nCLI.load(__dirname, '.\/commands');\nCLI.run(process.argv.slice(2));\nFinally, populate your commands directory with your commands, here's an example\nfile: .\/commands\/example.js\nmodule.exports = (() => {\n\n  'use strict';\n\n  const Command = require('cmnd').Command;\n\n  class ExampleCommand extends Command {\n\n    constructor() {\n\n      super('example');\n\n    }\n\n    help() {\n\n      return {\n        description: 'An example command',\n        args: ['example_arg1', 'example_arg2'],\n        flags: {flag: 'An example flag'},\n        vflags: {vflag: 'An example verbose flag'}\n      };\n\n    }\n\n    run(args, flags, vflags, callback) {\n\n      \/\/ Run code here.\n      \/\/ To throw an error, use: callback(new Error(msg))\n      \/\/ To optionally return a result, use: callback(null, result)\n\n      callback(null);\n\n    }\n\n  }\n\n  return ExampleCommand;\n\n})();\nCreating manual pages (help)\nView all the commands available to your CLI with mycli help where mycli is\nthe intended name of your command in the CLI. To modify help information,\nchange the return value of the help() method for each command.\nCreating Subcommands\nTo subclass a command (i.e. mycli command_name:sub_name) simply change the contructor()\nmethod in your command to the following:\nconstructor() {\n\n  super('command_name', 'sub_name');\n\n}\n\nRunning your commands\nEach command has a run() method which takes three arguments: args, flags,\nand vflags.\nargs\nargs is the array of arguments, passed before any flags.\ni.e. mycli command alpha beta would populate args with ['alpha', 'beta']\nflags\nflags is an object containing any flags (prefixed with -), where each entry\nis an array of values passed after the flag\ni.e. mycli command -f my flag would populate vflags with {f: ['my', 'flag']}\nvflags\nvflags works identically to flags, but with \"verbose flags\" (prefixed\nwith --).\nAdditional notes\nAll argument arrays passed to args or any flags or vflags options will\nbe separated by spaces, except in the case of quotation marks. Use\nquotation marks to specify an argument with spaces in it.\ni.e. mycli command -f \"argument one\" argument_two\nAcknowledgements\nThanks for checking out the library! Feel free to submit issues or PRs if you'd\nlike to see more features.\nFollow me on Twitter, @keithwhor.\nFeel free to check out more of my GitHub projects.\n","474":"connect-prerenderer\n\n\nExpress\/connect middleware to pre-render ajax page for non-ajax browsers, especially using angular.js\nHow to use\n$ npm install connect-prerenderer\n\nIn app.js:\nvar express = require('express');\nvar prerenderer = require('connect-prerenderer');\nvar app = express();\napp.use(prerenderer());\n...\n\nOptions\n\ntargetGenerator: a name or a function to generate a new one for HTTP request.\n\n\"default\" --- check \"\/PRERENDER-\" prefix which will be removed, and replace \"HASH-\" to \"#\/\" and all \"-\"s to \"\/\"s to make a target URL. (see the source code for more options.)\n\"googlebot\" --- follow https:\/\/developers.google.com\/webmasters\/ajax-crawling\/docs\/getting-started\na function that returns a target URL for prerendering or null.\n\n\ntimeout: an integer in milliseconds to specify how long it waits to prerender.\ncookieDomain: a domain name to allow passing cookies.\nattachConsole: when truthy, attach global.console to window.console when prerendering files\n(useful for debugging)\nsubprocess: if truthy, do the rendering in a subprocess (it helps to prevent possible memory leaks in jsdom). You can also set the RENDERER_USE_SUBPROCESS environment variable to any value (except an empty string) to achieve the same result. Try it if you experience memory leaks.\n\nCoding conventions (client-side)\n\nIf an html is prerendered, the body is like: <body data-prerendered=\"true\">\nWhen JavaScript code finishes prerendering, it should call: document.onprerendered()\n\nNotes for AngularJS\nThe following snippet would help AngularJS to work:\n<script>\n  angular.element(document).ready(function() {\n    if (document.body.getAttribute('data-prerendered')) {\n      document.addEventListener('click', function() {\n        document.removeEventListener('click', arguments.callee, true);\n        angular.bootstrap(document.documentElement, []);\n        return true;\n      }, true);\n    } else {\n      angular.bootstrap(document.documentElement, []);\n    }\n  });\n<\/script>\n\nTo keep templates for interpolation in a prerendered html,\nuse the modified version of angular.js (v1.2.5)\nlocated in test\/server\/public\/.\nLimitations\n\nstyle attributes are not preserved by jsdom (use class).\n\nAngularJS only limitations:\n\nng-repeat workaround only works with ng-repeat attributes.\nng-repeat-(start|end) is not supported.\n\n","475":"Stormpath is Joining Okta\nWe are incredibly excited to announce that Stormpath is joining forces with Okta. Please visit the Migration FAQs for a detailed look at what this means for Stormpath users.\nWe're available to answer all questions at support@stormpath.com.\nWhat does this mean for developers who are using this library?\n\nIf you have upgraded to the 2.x series from 1.x, you should downgrade to 1.1.1.  Why?  The 2.x series depends on the Stormpath Client API, which will not be migrated to the Okta platform.\nWhen downgrading to 1.1.1 you will need to use one of our backend framework integrations to serve the APIs that the 1.x series depends on.\nThese backend integrations are being patched to work with Okta:\n\n\nJava Spring\nJava Spring Boot\nNode Express\nASP.NET 4.x\nASP.NET Core\n\n\nIf you are using the Express integration, please see the Express-Stormpath Angular Sample Project, it can be used to test your migration to Okta.\n\nREADME\nIf you are actively using this library, you can find the old readme in OLD-README.md. It is not possible to register for new Stormpath tenants at this time, so you must already have a Stormpath tenant if you wish to use this library during the migration period.\n","476":"Employee Directory\nSample Application built with Backbone.js and TopCoat\n\"Backbone Directory\" is a simple Employee Directory application built with Backbone.js and [TopCoat] (http:\/\/topcoat.io).\nRefer to this blog post for more information about the application.\nThe application runs out-of-the-box with an in-memory data store.\n","477":"Synopsis\nCustom command line tab completion for node.js applications.\nExample\n#!\/usr\/bin\/env node\n\nvar complete = require('complete'); \/\/ get the `complete` module.\n\n\/\/\n\/\/ list of items to complete on.\n\/\/\ncomplete.list = ['apple', 'orange', 'pear', 'lemon', 'mango'];\n\ncomplete.callback = function(lastSelection, userInput, reducedList) {\n\n  if (lastSelection === 'apple') {\n\tcomplete.add('sauce');\n  }\n};\n\ncomplete.init();\n\n\/\/\n\/\/ continue with the application...\n\/\/\nconsole.log('program started with the following arguments:', process.argv[2] || 'none provided');\nDistribution and Installation\nYour installment procedure should place your CLI program in a location made accessible by the PATH variable. If users install your program with the NPM -g option, your program will be in the path.\n\/usr\/local\/bin\/myprogram -> \/usr\/local\/lib\/node_modules\/myprogram\/bin\/myprogram\nAPI\nlist\nCreate a list of commands that you want to autocomplete with.\ncomplete.list = ['apple', 'orange', 'pear', 'lemon', 'mango'];\ncallback\nOptionally you can define a callback that will get called when the match when the completion happens.\ncomplete.callback = function(lastSelection, userInput, reducedList) {\n\n  \/\/\n  \/\/ do something if this is an `orange`. Note that anything that\n  \/\/ you `process.stdout.write()` will be added to the auto complete\n  \/\/ list.\n  \/\/\n};\ninit()\nInitialize the auto completion behavior.\ncomplete.init();\nHigher Level Example\nvar complete = require('complete');\n\ncomplete({\n  program: 'my-program',\n  \/\/ Commands\n  commands: {\n    'hello': function(words, prev, cur) {\n      complete.output(cur, ['abc', 'def']);\n    },\n    'world': {\n      'hi': function(words, prev, cur) {\n        complete.echo('next');\n      }\n    }\n  },\n  \/\/ Position-independent options.\n  \/\/ These will attempted to be\n  \/\/ matched if `commands` fails\n  \/\/ to match.\n  options: {\n    '--help': {},\n    '-h': {},\n    '--version': {},\n    '-v': {}\n  }\n});\nThe above results in\n$ my-program he<TAB>\n$ my-program hello\n$ my-program hello a<TAB>\n$ my-program hello abc\nLicense\n(The MIT License)\nCopyright (c) 2010 hij1nx http:\/\/www.twitter.com\/hij1nx\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","478":"New Relic Boxes\n \nNew Relic Boxes is an html5 dashboard for your R&D team. Boxes lets you monitor your applications and servers on New Relic.\nAlarm is played when one of your application \/ servers health status is changed.\n\nInstallation\n\nEnsure node ^0.10.33 is installed and on your path.\nRun npm install bower to install bower\nRun bower install to install libraries\nRun npm install to install dependencies\nRun index.html (the easiest way) or run npm start to start a static server\n\nSetup\npress settings and add the following information:\nYour new relic apikey (mandatory).\nYour company name (optional).\nYour favicon url (optional).\nYour new relic account id (optional) - to open a new tab with your specific box on new relic app.\n","479":"angular-flash\n\n\nA flash service and directive for setting and displaying flash messages in Angular JS.  Specifically, the flash service is a publisher of flash messages and the flash directive is a subscriber to flash messages.  The flash directive leverages the Twitter Bootstrap Alert component.\nInstallation\nDownload angular-flash.min.js or install with bower.\n$ bower install angular-flash --save\nLoad the angular-flash.service and the angular-flash.flash-alert-directive modules in your app.\nangular.module('app', ['angular-flash.service', 'angular-flash.flash-alert-directive']);\nConfigure\nangular.module('app', ['angular-flash.service', 'angular-flash.flash-alert-directive'])\n        .config(function (flashProvider) {\n        \n            \/\/ Support bootstrap 3.0 \"alert-danger\" class with error flash types\n            flashProvider.errorClassnames.push('alert-danger');\n\n            \/**\n             * Also have...\n             *\n             * flashProvider.warnClassnames\n             * flashProvider.infoClassnames\n             * flashProvider.successClassnames\n             *\/\n\n        })\nUsage\nUse the flash service to publish a flash messages...\nvar FooController = function(flash){\n    \/\/ Publish a success flash\n    flash.success = 'Do it live!';\n\n    \/\/ Publish a error flash\n    flash.error = 'Fail!';\n\n    \/\/ Publish an info flash to the `alert-1` subscriber\n    flash.to('alert-1').info = 'Only for alert 1';\n\n    \/\/ The `flash-alert` directive hides itself when if receives falsey flash messages of any type\n    flash.error = '';\n\n};\n\nFooController.$inject = ['flash'];\nUse the flash-alert directive to subscribe to flash messages...\n<!-- Subscribe to success flash messages. -->\n<div flash-alert=\"success\" active-class=\"in\" class=\"alert fade\">\n    <strong class=\"alert-heading\">Congrats!<\/strong>\n    <span class=\"alert-message\">{{flash.message}}<\/span>\n<\/div>\n\n<!-- Subscribe to error flash messages. -->\n<div flash-alert=\"error\" active-class=\"in\" class=\"alert fade\">\n    <strong class=\"alert-heading\">Boo!<\/strong>\n    <span class=\"alert-message\">{{flash.message}}<\/span>\n<\/div>\n\n<!-- Subscribe to all flash messages. -->\n<div flash-alert active-class=\"in\" class=\"alert fade\">\n    <strong class=\"alert-heading\">Boo!<\/strong>\n    <span class=\"alert-message\">{{flash.message}}<\/span>\n<\/div>\n\n<!-- Subscribe to all flash messages sent to `alert-1`. -->\n<div id=\"alert-1\" flash-alert active-class=\"in\" class=\"alert fade\">\n    <strong class=\"alert-heading\">Boo!<\/strong>\n    <span class=\"alert-message\">{{flash.message}}<\/span>\n<\/div>\n\n<!-- Set the display duration in milli-secs.  Default is 5000, 0 disables the fade-away. -->\n<div flash-alert active-class=\"in\" class=\"alert fade\" duration=\"0\">\n    <!-- Manually hide the alert with `hide()` -->\n    <button type=\"button\" class=\"close\" ng-click=\"hide()\">&times;<\/button>\n    <strong class=\"alert-heading\">Boo!<\/strong>\n    <span class=\"alert-message\">{{flash.message}}<\/span>\n<\/div>\nWhen a flash message is published, the flash-alert directive will add a class of the form alert-<type> and also add classes specified in active-class.  Then after 5 seconds it will remove them.\nThe example above leverages Twitter Bootstrap CSS3 transitions: fade and in.\nStyling Considerations\nBootstrap 2 has a few styling quirks with the .alert and .fade classes.\nVisible or not\nSome folks may want hidden alerts to take up visible space others may not.  Fortunately, each case is easy to achieve by declaring .alert as indicated below...\nTakes up no visible space when hidden\n<div flash-alert active-class=\"in alert\" class=\"fade\">\n...\n<\/div>\nTakes up visible space when hidden\n<div flash-alert active-class=\"in\" class=\"fade alert\">\n...\n<\/div>\nCSS Transition Quirks\nThe .fade class only transitions opacity and the base .alert class has a background color and background border used for alert warnings.  Together these styling attributes can make it challenging to achieve smooth transitions.\nFortunately, its easy to replace the .alert class and move the warning colors to .alert-warn as illustrated below...\nStyling\n\/* Remove colors and add transition property *\/\n.alert-flash {\n    padding: 8px 35px 8px 14px;\n    margin-bottom: 20px;\n    text-shadow: 0 1px 0 rgba(255, 255, 255, 0.5);\n    border: 1px solid transparent;\n    -webkit-border-radius: 4px;\n    -moz-border-radius: 4px;\n    border-radius: 4px;\n\n    \/* change transition property to all *\/\n    -webkit-transition-property: all;\n    transition-property: all;\n}\n\n.alert-flash h4 {\n    margin: 0;\n}\n\n.alert-flash .close {\n    position: relative;\n    top: -2px;\n    right: -21px;\n    line-height: 20px;\n}\n\n\/* add warning colors to warn class *\/\n.alert-warn {\n    background-color: #fcf8e3;\n    border: 1px solid #fbeed5;\n}\n\n.alert-warn,\n.alert-warn h4 {\n    color: #c09853;\n}\nTemplate:\n<div flash-alert=\"info\" active-class=\"in\" class=\"alert-flash fade\">\n    <i class=\"icon-info-sign\"><\/i>\n    <strong class=\"alert-heading\">Ahem...<\/strong>\n    <span class=\"alert-message\">{{flash.message}}<\/span>\n<\/div>\nFlashProvider API\nflashProvider.errorClassnames\nflashProvider.warnClassnames\nflashProvider.infoClassnames\nflashProvider.successClassnames\nFlash Service API\nProperties\nSet and get flash messages with the following flash properties...\n\nsuccess\ninfo\nwarn\nerror\n\nMethods\nsubscribe(listener, [type])\nRegister a subscriber callback function to be notified of flash messages.  The subscriber function has two arguments: message and type.\nclean()\nClear all subscribers and flash messages.\nContributing\nPrerequisites\nThe project requires Bower, Grunt, and PhantomJS.  Once you have installed them, you can build, test, and run the project.\nBuild & Test\nTo build and run tests, run either...\n$ make install\nor\n$ npm install\n$ bower install\n$ grunt install\nDemo & Develop\nTo run a live demo or do some hackery, run...\n$ grunt server\n","480":"DEPRECATED\nYou should use Facebook's module -- https:\/\/github.com\/facebook\/jscodeshift\nSYNOPSIS\nBuild a subset or superset of a code base.\nDESCRIPTION\nRather than breaking apart files at design-time for the purpose of targeted\ndistribution, you can break them apart using a filter at build-time.\nEXAMPLES\nAs the Javascript file is traversed and syntax is discovered, you will have the\nopportunity to interject, for example:\nInput\nvar a = 10\nvar b = 11\nvar c = 12\nCode\nvar cs = require('codesurgeon')\n\nvar filter = cs.filter()\n\nfilter.inclusive = true\n\nfilter.syntax.VariableDeclaration = function(name, item) {\n  if (name === 'b') {\n    return true\n  }\n}\n\nfstream\n  .Reader('example.js')\n  .pipe(filter)\n  .pipe(process.stdout)\nOutput\nvar b = 11\nA filter supports a streaming style API, but most operations block processing so\nthis is purely for convenience.\nSyntax Reference\nAssignmentExpression\nArrayExpression\nArrayPattern\nBlockStatement\nBinaryExpression\nBreakStatement\nCallExpression\nCatchClause\nComprehensionBlock\nComprehensionExpression\nConditionalExpression\nContinueStatement\nDirectiveStatement\nDoWhileStatement\nDebuggerStatement\nEmptyStatement\nExpressionStatement\nForStatement\nForInStatement\nFunctionDeclaration\nFunctionExpression\nIdentifier\nIfStatement\nLiteral\nLabeledStatement\nLogicalExpression\nMemberExpression\nNewExpression\nObjectExpression\nObjectPattern\nProgram\nProperty\nReturnStatement\nSequenceExpression\nSwitchStatement\nSwitchCase\nThisExpression\nThrowStatement\nTryStatement\nUnaryExpression\nUpdateExpression\nVariableDeclaration\nVariableDeclarator\nWhileStatement\nWithStatement\nYieldExpression\nLICENSE\n(The MIT License)\nCopyright (c) 2010 hij1nx http:\/\/www.twitter.com\/hij1nx\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","481":"\n\n\nDjango OAuth2 Server\nImplementation of OAuth2 Server for Django. Feel free to fork this repository and contribute.\nWritten for Django 1.9 :)\n\nGrant Types\n\nAuthorization Code\nImplicit\nClient Credentials\nUser Credentials\nRefresh Token\n\n\nScope\nAuthentication\nContributing\n\nInstallation\nConfiguration\nRunning Tests\n\n\n\nGrant Types\nAuthorization Code\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.1\nInsert test data:\n$ python oauth2server\/manage.py loaddata test_credentials\n$ python oauth2server\/manage.py loaddata test_scopes\n\nRun the development web server:\n$ python oauth2server\/manage.py runserver\n\nAnd you can now go to this page in your web browser:\nhttp:\/\/localhost:8000\/web\/authorize\/?response_type=code&client_id=testclient&redirect_uri=https:\/\/www.example.com&state=somestate\n\nYou should see a screen like this:\n\nClick yes, you will be redirected to the redirect_uri and the authorization code will be in the query string. For example:\nhttps:\/\/www.example.com\/?code=cd45169cf6575f76d789f55764cb751b4d08274d&state=somestate\n\nYou can use it to get access token:\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.1.3\n$ curl -u testclient:testpassword localhost:8080\/api\/v1\/tokens\/ -d 'grant_type=authorization_code&code=cd45169cf6575f76d789f55764cb751b4d08274d'\n\nYou should get a response like:\n{\n    \"id\": 1,\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\n    \"expires_in\": 3600,\n    \"token_type\": \"Bearer\",\n    \"scope\": \"foo bar qux\",\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\n}\nImplicit\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.2\nVery similar to the authorization code but the token is returned in URL fragment.\nInsert test data:\n$ python oauth2server\/manage.py loaddata test_credentials\n$ python oauth2server\/manage.py loaddata test_scopes\n\nRun the development web server:\n$ python oauth2server\/manage.py runserver\n\nAnd you can now go to this page in your web browser:\nhttp:\/\/localhost:8080\/web\/authorize\/?response_type=token&client_id=testclient&redirect_uri=https:\/\/www.example.com&state=somestate\n\nYou should see a screen like this:\n\nClick yes, you will be redirected to the redirect_uri and the access token code will be in the URL fragment. For example:\nhttps:\/\/www.example.com#access_token=66b80fb9d6630705bcea1c9be0df2a5f7f7a52bf&expires_in=3600&token_type=Bearer&state=somestate\n\nUser Credentials\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.3\nInsert test data:\n$ python oauth2server\/manage.py loaddata test_credentials\n$ python oauth2server\/manage.py loaddata test_scopes\n\nRun the development web server:\n$ python oauth2server\/manage.py runserver\n\nAnd you can now get a new access token:\n$ curl -u testclient:testpassword localhost:8080\/api\/v1\/tokens\/ -d 'grant_type=password&username=testuser@example.com&password=testpassword'\n\nYou should get a response like:\n{\n    \"id\": 1,\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\n    \"expires_in\": 3600,\n    \"token_type\": \"Bearer\",\n    \"scope\": \"foo bar qux\",\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\n}\nClient Credentials\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.4\nInsert test data:\n$ python oauth2server\/manage.py loaddata test_credentials\n$ python oauth2server\/manage.py loaddata test_scopes\n\nRun the development web server:\n$ python oauth2server\/manage.py runserver\n\nAnd you can now get token either using HTTP Basic Authentication:\n$ curl -u testclient:testpassword localhost:8080\/api\/v1\/tokens\/ -d 'grant_type=client_credentials'\n\nOr using POST body:\n$ curl localhost:8000\/api\/v1\/tokens\/ -d 'grant_type=client_credentials&client_id=testclient&client_secret=testpassword'\n\nYou should get a response like:\n{\n    \"id\": 1,\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\n    \"expires_in\": 3600,\n    \"token_type\": \"Bearer\",\n    \"scope\": \"foo bar qux\",\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\n}\nRefresh Token\nLet's say you have created a new access token using the user credentials grant type. The response included a refresh token which you can use to get a new access token before your current access token expires.\n$ curl -u testclient:testpassword localhost:8080\/api\/v1\/tokens\/ -d 'grant_type=refresh_token&refresh_token=55697efd4b74c980f2c638602556115bc14ca931'\n\nAnd you get a new access token:\n{\n    \"id\": 1,\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\n    \"expires_in\": 3600,\n    \"token_type\": \"Bearer\",\n    \"scope\": \"foo bar qux\",\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\n}\nScope\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-3.3\nScope is quite arbitrary. Basically it is a space delimited case-sensitive string where each part defines a specific access range.\nYou can define your scopes and insert them into tokens_oauthscope table, is_default flag can be used to specify default scope.\nAuthentication\nNow that you have obtained an access token, you can make requests to protected resources.\nIn order to require authentication for a view, wrap it in the authentication_required decorator:\nfrom apps.tokens.decorators import authentication_required\n\n@authentication_required(\"some_scope\")\ndef some_view(request, *args, **kwargs):\n    ...\nContributing\nIn order to contribute to this project, fork it and make a pull request. I will review and accept it.\nAll tests must be passing in order for the pull request to be accepted.\nInstallation\nClone the repository:\n$ git clone https:\/\/github.com\/RichardKnop\/django-oauth2-server.git\n\nCreate a virtual environment and install requirements:\n$ virtualenv venv\n$ source venv\/bin\/activate\n$ pip install -r requirements.txt\n\nCreate a local.py file and insert correct configuration details:\n$ cp oauth2server\/proj\/settings\/local.example.py oauth2server\/proj\/settings\/local.py\n$ nano cp oauth2server\/proj\/settings\/local.py\n\nSync the database:\n$ python oauth2server\/manage.py syncdb\n\nConfiguration\nThese are the current configuration options:\nOAUTH2_SERVER = {\n    'ACCESS_TOKEN_LIFETIME': 3600,\n    'AUTH_CODE_LIFETIME': 3600,\n    'REFRESH_TOKEN_LIFETIME': 1209600,\n    'IGNORE_CLIENT_REQUESTED_SCOPE': False,\n}\n\nACCESS_TOKEN_LIFETIME: lifetime of an access token in seconds\nAUTH_CODE_LIFETIME: lifetime of an authorization code in seconds\nREFRESH_TOKEN_LIFETIME: lifetime of a refresh token in seconds\nIGNORE_CLIENT_REQUESTED_SCOPE: if true, client requested scope will be ignored\n\nRunning Tests\n$ python oauth2server\/manage.py test\n\n","482":"\n\n\nDjango OAuth2 Server\nImplementation of OAuth2 Server for Django. Feel free to fork this repository and contribute.\nWritten for Django 1.9 :)\n\nGrant Types\n\nAuthorization Code\nImplicit\nClient Credentials\nUser Credentials\nRefresh Token\n\n\nScope\nAuthentication\nContributing\n\nInstallation\nConfiguration\nRunning Tests\n\n\n\nGrant Types\nAuthorization Code\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.1\nInsert test data:\n$ python oauth2server\/manage.py loaddata test_credentials\n$ python oauth2server\/manage.py loaddata test_scopes\n\nRun the development web server:\n$ python oauth2server\/manage.py runserver\n\nAnd you can now go to this page in your web browser:\nhttp:\/\/localhost:8000\/web\/authorize\/?response_type=code&client_id=testclient&redirect_uri=https:\/\/www.example.com&state=somestate\n\nYou should see a screen like this:\n\nClick yes, you will be redirected to the redirect_uri and the authorization code will be in the query string. For example:\nhttps:\/\/www.example.com\/?code=cd45169cf6575f76d789f55764cb751b4d08274d&state=somestate\n\nYou can use it to get access token:\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.1.3\n$ curl -u testclient:testpassword localhost:8080\/api\/v1\/tokens\/ -d 'grant_type=authorization_code&code=cd45169cf6575f76d789f55764cb751b4d08274d'\n\nYou should get a response like:\n{\n    \"id\": 1,\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\n    \"expires_in\": 3600,\n    \"token_type\": \"Bearer\",\n    \"scope\": \"foo bar qux\",\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\n}\nImplicit\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.2\nVery similar to the authorization code but the token is returned in URL fragment.\nInsert test data:\n$ python oauth2server\/manage.py loaddata test_credentials\n$ python oauth2server\/manage.py loaddata test_scopes\n\nRun the development web server:\n$ python oauth2server\/manage.py runserver\n\nAnd you can now go to this page in your web browser:\nhttp:\/\/localhost:8080\/web\/authorize\/?response_type=token&client_id=testclient&redirect_uri=https:\/\/www.example.com&state=somestate\n\nYou should see a screen like this:\n\nClick yes, you will be redirected to the redirect_uri and the access token code will be in the URL fragment. For example:\nhttps:\/\/www.example.com#access_token=66b80fb9d6630705bcea1c9be0df2a5f7f7a52bf&expires_in=3600&token_type=Bearer&state=somestate\n\nUser Credentials\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.3\nInsert test data:\n$ python oauth2server\/manage.py loaddata test_credentials\n$ python oauth2server\/manage.py loaddata test_scopes\n\nRun the development web server:\n$ python oauth2server\/manage.py runserver\n\nAnd you can now get a new access token:\n$ curl -u testclient:testpassword localhost:8080\/api\/v1\/tokens\/ -d 'grant_type=password&username=testuser@example.com&password=testpassword'\n\nYou should get a response like:\n{\n    \"id\": 1,\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\n    \"expires_in\": 3600,\n    \"token_type\": \"Bearer\",\n    \"scope\": \"foo bar qux\",\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\n}\nClient Credentials\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-4.4\nInsert test data:\n$ python oauth2server\/manage.py loaddata test_credentials\n$ python oauth2server\/manage.py loaddata test_scopes\n\nRun the development web server:\n$ python oauth2server\/manage.py runserver\n\nAnd you can now get token either using HTTP Basic Authentication:\n$ curl -u testclient:testpassword localhost:8080\/api\/v1\/tokens\/ -d 'grant_type=client_credentials'\n\nOr using POST body:\n$ curl localhost:8000\/api\/v1\/tokens\/ -d 'grant_type=client_credentials&client_id=testclient&client_secret=testpassword'\n\nYou should get a response like:\n{\n    \"id\": 1,\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\n    \"expires_in\": 3600,\n    \"token_type\": \"Bearer\",\n    \"scope\": \"foo bar qux\",\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\n}\nRefresh Token\nLet's say you have created a new access token using the user credentials grant type. The response included a refresh token which you can use to get a new access token before your current access token expires.\n$ curl -u testclient:testpassword localhost:8080\/api\/v1\/tokens\/ -d 'grant_type=refresh_token&refresh_token=55697efd4b74c980f2c638602556115bc14ca931'\n\nAnd you get a new access token:\n{\n    \"id\": 1,\n    \"access_token\": \"00ccd40e-72ca-4e79-a4b6-67c95e2e3f1c\",\n    \"expires_in\": 3600,\n    \"token_type\": \"Bearer\",\n    \"scope\": \"foo bar qux\",\n    \"refresh_token\": \"6fd8d272-375a-4d8a-8d0f-43367dc8b791\"\n}\nScope\nhttp:\/\/tools.ietf.org\/html\/rfc6749#section-3.3\nScope is quite arbitrary. Basically it is a space delimited case-sensitive string where each part defines a specific access range.\nYou can define your scopes and insert them into tokens_oauthscope table, is_default flag can be used to specify default scope.\nAuthentication\nNow that you have obtained an access token, you can make requests to protected resources.\nIn order to require authentication for a view, wrap it in the authentication_required decorator:\nfrom apps.tokens.decorators import authentication_required\n\n@authentication_required(\"some_scope\")\ndef some_view(request, *args, **kwargs):\n    ...\nContributing\nIn order to contribute to this project, fork it and make a pull request. I will review and accept it.\nAll tests must be passing in order for the pull request to be accepted.\nInstallation\nClone the repository:\n$ git clone https:\/\/github.com\/RichardKnop\/django-oauth2-server.git\n\nCreate a virtual environment and install requirements:\n$ virtualenv venv\n$ source venv\/bin\/activate\n$ pip install -r requirements.txt\n\nCreate a local.py file and insert correct configuration details:\n$ cp oauth2server\/proj\/settings\/local.example.py oauth2server\/proj\/settings\/local.py\n$ nano cp oauth2server\/proj\/settings\/local.py\n\nSync the database:\n$ python oauth2server\/manage.py syncdb\n\nConfiguration\nThese are the current configuration options:\nOAUTH2_SERVER = {\n    'ACCESS_TOKEN_LIFETIME': 3600,\n    'AUTH_CODE_LIFETIME': 3600,\n    'REFRESH_TOKEN_LIFETIME': 1209600,\n    'IGNORE_CLIENT_REQUESTED_SCOPE': False,\n}\n\nACCESS_TOKEN_LIFETIME: lifetime of an access token in seconds\nAUTH_CODE_LIFETIME: lifetime of an authorization code in seconds\nREFRESH_TOKEN_LIFETIME: lifetime of a refresh token in seconds\nIGNORE_CLIENT_REQUESTED_SCOPE: if true, client requested scope will be ignored\n\nRunning Tests\n$ python oauth2server\/manage.py test\n\n","483":"\nCreated by Stephen McDonald\n\nIntroduction\nA Django reusable app providing the overextends template tag, a\ndrop-in replacement for Django's extends tag, which allows you to\nuse circular template inheritance.\nThe primary use-case for overextends is to simultaneously override\nand extend templates from other reusable apps, in your own Django project.\n\nExample\nConsider the following settings module and templates, with the apps\napp1 and app2 bundled in the project, for example's sake:\n# settings.py\nINSTALLED_APPS = (\n    \"app1\",\n    \"app2\",\n    \"overextends\",\n)\nTEMPLATE_LOADERS = (\n    \"django.template.loaders.filesystem.Loader\",\n    \"django.template.loaders.app_directories.Loader\",\n)\nPROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))\nTEMPLATE_DIRS = (os.path.join(PROJECT_ROOT, \"templates\"),)\n\n<!-- myproject\/app1\/templates\/pages\/page.html -->\n<h1>Title<\/h1>\n{% block main %}\n<p>A paragraph in app1<\/p>\n{% enblock %}\n<footer>Copyright 2012<\/footer>\n\n<!-- myproject\/app2\/templates\/pages\/page.html -->\n{% overextends \"pages\/page.html\" %}\n{% block main %}\n<p>A paragraph in app2, that wants to be on top of app1's main block<\/p>\n{{ block.super }}\n{% enblock %}\n\n<!-- myproject\/templates\/pages\/page.html -->\n{% overextends \"pages\/page.html\" %}\n{% block main %}\n{{ block.super }}\n<p>A paragraph in the project's template directory, under the other main blocks<\/p>\n{% enblock %}\n\nThe resulting HTML rendered when pages\/page.html was loaded would be:\n<h1>Title<\/h1>\n<p>A paragraph in app2, that wants to be on top of app1's main block<\/p>\n<p>A paragraph in app1<\/p>\n<p>A paragraph in the project's template directory, under the other main blocks<\/p>\n<footer>Copyright 2012<\/footer>\n\nFor a detailed analysis of why you would use this approach, how it works,\nand alternative approaches, read my initial blog post:\nCircular Template Inheritance for Django\n\nInstallation\nThe easiest way to install django-overextends is directly from PyPi\nusing pip by running the following command:\n$ pip install -U django-overextends\n\nOtherwise you can download django-overextends and install it directly\nfrom source:\n$ python setup.py install\n\n\nProject Configuration\nOnce installed you can configure your project to use\ndjango-overextends by adding the overextends app to the\nINSTALLED_APPS in your project's settings module:\nINSTALLED_APPS = (\n    # ... other apps here ...\n    'overextends',\n)\n\nFor Django 1.9+ you must add overextends to the builtins key of your TEMPLATES setting:\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'builtins': ['overextends.templatetags.overextends_tags'],\n        }\n    },\n]\n\nNote that while the overextends tag is provided by the package\noverextends.templatetags.overextends_tags, it is unnecessary to use\n{% load overextends_tags %} in your templates. Like the extends\ntag, overextends must be the first tag in your template, so it is\nautomatically added to Django's built-in template tags, removing the\nneed to load its tag library in each template.\n","484":"RecyclerViewSample\n[Android]\u4f7f\u7528RecyclerView\u66ff\u4ee3ListView\uff08\u4e00\uff09\uff1ahttp:\/\/www.cnblogs.com\/tiantianbyconan\/p\/4232560.html\n[Android]\u4f7f\u7528RecyclerView\u66ff\u4ee3ListView\uff08\u4e8c\uff09\uff1ahttp:\/\/www.cnblogs.com\/tiantianbyconan\/p\/4242541.html\n[Android]\u4f7f\u7528RecyclerView\u66ff\u4ee3ListView\uff08\u4e09\uff09\uff1ahttp:\/\/www.cnblogs.com\/tiantianbyconan\/p\/4268097.html\n","485":"Spock for Android\n\n\n\n\n\n\n\n\n\n\n\nThis library allows for the Spock Framework mocks to\nbe used on Android. As well as add some helpful features to make Android\ntesting easier.\n\n\n\n\nUsage\n\n\nSetup Groovy For Android\n\n\nbuildscript {\n  repositories {\n    jcenter()\n  }\n\n  dependencies {\n     classpath 'com.android.tools.build:gradle:2.3.2'\n     classpath 'org.codehaus.groovy:groovy-android-gradle-plugin:1.2.0'\n  }\n}\n\napply plugin: 'com.android.application'\napply plugin: 'groovyx.android'\n\n\n\nSee groovy-android-gradle-plugin for more\ninformation on setting this plugin up.\n\n\n\nSetup Dependencies\n\n\ndependencies {\n  ...\n  androidTestCompile \"com.andrewreitz:spock-android:2.0\"\n  ...\n}\n\n\n\n\nSetup Android Plugin\n\n\nandroid {\n  ...\n\n  defaultConfig {\n    ...\n    testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\"\n  }\n\n  packagingOptions {\n    exclude 'META-INF\/LICENSE'\n  }\n}\n\n\n\n\nWrite your tests\n\nTests must be placed in the .\/src\/androidTest\/groovy directory.\n\n\nWrite your tests like you would regular spock tests. See the spock-android-sample project and\nSpock Framework for more details.\n\n\n\nMocking\n\nObjenesis and cglib do not work with Android. But that\u2019s okay, using dexmaker we can still create\nmock objects in spock fashion. The only difference is instead of your test classes inheriting from\nSpecification, you need to inherit from AndroidSpecification.\n\n\nNote: You can not use mocked automatic getters and setters. Example mocked.getName() will work\nwhere as mocked.name will not. This is due to limitations of Android not having the java.bean\npackage available.\n\n\n\nAnnotations\n\nUseActivity is an field annotation to get access to your Activity during tests. You can even\nprovide bundle arguments by supplying a BundleCreator.\n\n\nEx.\n\n\n\n@UseActivity(MyActivity) def myActivity\n\n\n\nUseApplication is a field annotation that supplies your Application.\n\n\nEx.\n\n\n\n@UseApplication(MyApplication) def myApplication\n\n\n\nWithContext is a field annotation that supplies you with a context. This is not an implementation of\nyour application.\n\n\nEx.\n\n\n\n@WithContext def context\n\n\n\nAll field annotations will be set during the setup fixture.\n\n\n\nOther Notes\n\n\n\nAndroid Studio can not run tests with spaces in the name. This is due to a limitation\nwith how it tells the Android device how to run them. To get around this you can\nname tests without spaces. EX: \"this_is_my_test\" or just run them in gradle with gradle connectedAndroidTest.\n\n\nSpock includes some references to Java packages that are not included in Android. You may need to use\nlintOptions { disable 'InvalidPackage' } to avoid lint failures.\n\n\n\n\n\n\n\nLicense\n\n\n\n\nCopyright 2017 Andrew Reitz\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\n\nhttp:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n\n\n","486":"SpringStudy\n\u5bf9Spring\u6846\u67b6\u7684\u5b66\u4e60\uff0c\u5305\u62ec\u5b98\u65b9\u6587\u6863\u7684\u7ffb\u8bd1\uff0cSpring\u6846\u67b6\u7684\u5e94\u7528\uff0cSpring\u6e90\u7801\u7684\u5256\u6790\u3002\u3002\u3002\n","487":"fxosstub\nBuilding an Open Web App on Firefox, step by step.\n\nAll explanations are found\nhere\n","488":"Important Notes\nnode-webcl will be soon superseeded by https:\/\/github.com\/mikeseven\/node-opencl. Node-opencl is a reimplementation of node.js bindings to OpenCL:\n\ntrue wrapper of OpenCL, not an object-oriented model as WebCL. This makes it easier to develop higher-level API\nsupports OpenCL 1.1, 1.2, 2.0\nsupport for Nan2 and Node.JS 4\nno dependency on external packages such as node-webgl, which was a huge issue to install for many users\nLots of unit tests\n\nIntroduction\nThis is an implementation of Khronos WebCL specification using NodeJS.\nThis implementation solely relies on OpenCL 1.1 C headers.\nDislaimer\nWhile we are close to the WebCL specification, some features in this code may or may not be available in the final specification. As such, this implementation should be considered for\n\nexperimental development of WebCL\/WebGL content,\nexperimenting with new constructs that may not be available in browsers,\ncoupled with Node.JS awesome capabilities, this software opens the door to non-browser applications (e.g. fast server-side image processing).\n\nThis implementation is not secure nor robust, we will update as the standard progresses in these areas. So beware that hacking your GPU may crash your computer; don't blame us!\nLicense\nnode-webcl is distributed under BSD license\nCopyright (c) 2011-2012, Motorola Mobility, Inc.\nAll rights reserved.\nSee LICENSES in this distribution for all licenses used in samples from other companies.\nDependencies\n\n\nNAN must be installed first to support all versions of v8\n\n\nnode-webgl. This module is used for samples using WebGL interoperability with WebCL.\nIn turns, node-webgl relies on node-glfw that relies on GLFW, GLEW, AntTweakBar, and FreeImage. See node-webgl and node-glfw for instructions on how to install these modules.\n\n\nOpenCL 1.1 must be installed on your machine. Typically, this means your machine has a not too old graphic card (maybe not more than 3 years old) and its latest graphic drivers installed.\n\n\nOn Mac, we recommend using OSX 10.7 \"Lion\" since OSX 10.6 \"Snow Leopard\" only supports OpenCL 1.0 and is buggy.\nOn Windows, use Windows 7. Note that if your machine is 64-bit, you should use node.js 64-bit distribution, not the 32-bit default to avoid mismatch between node libraries and these native dependencies when node-gyp build the modules.\nOn Linux, make sure you use the latest AMD or NVidia drivers. This module has been tested with Ubuntu 10.10, 11.04 and 11.10 64-bit.\nPre-built binaries are available in submodule deps. Don't forget to do:\ngit submodule init\ngit submodule udpate\n\nif you need these binaries.\nInstallation\nMake sure GLEW, GLFW, AntTweakBar, and FreeImage libraries are in your path.\n\n\non Windows, put DLLs in Windows\\System32. Put headers in \\include and static librairies in \\lib for 32-bit libraries (if you use node.js in 32-bit) or \\lib\\x64 (if you use 64-bit node.js).\n\n\non Mac, use homebrew\nbrew install freeimage anttweakbar glfw3 glew\n\n\non Linux use you package manager to install these libraries\n\n\nNow install the usual way:\nnpm install node-webcl\n\nthis will also install https:\/\/github.com\/mikeseven\/node-webgl, https:\/\/github.com\/mikeseven\/node-glfw, https:\/\/github.com\/mikeseven\/node-image, and https:\/\/github.com\/rvagg\/nan.\nIf you want to use the latest code, retrieve each repo (node-webcl, node-webgl, node-glfw, and node-image) from github and simply do\nnode-gyp rebuild\nnpm link\n\nA crash course on WebCL\nWebCL is a JavaScript representation of OpenCL 1.1. It is not a straight mapping of OpenCL C methods but rather an object-oriented representation of OpenCL object model. Knowledge of OpenCL is therefore mandatory to be able to develop WebCL programs. See the Books section and\/or jump directly into Specifications references at the end of this page.\nThere are few steps in creating a WebCL program:\n\nInit WebCL\n\nfind a suitable platform\/device (cl.getPlatform(), cl.getDevices())\ncreate a context (context = cl.createContext())\ncreate a command queue (queue = context.createCommandQueue())\ncompile a program (program = context.createProgram(), program.build())\nset up a kernel and its arguments (kernel = program.createKernel(), kernel.setArg())\nset up commands (queue.enqueueXXX())\n\n\nRun computation\n\nset up kernel arguments (kernel.setArg())\nset up commands (queue.enqueueXXX())\nlaunch the computation (queue.enqueueTask() or queue.enqueueNDRange())\n\n\n\nWhen used with WebGL, WebGL context must be created first because WebCL context is created with sharing the WebGL context. Remember that WebCL allows computations on WebGL buffers. WebCL doesn't do any rendering. By using WebCL and WebGL together, data remains in the device and this avoids expensive data transfer to\/from CPU memory.\nSo the sequence becomes:\n\ninit WebGL\n\ninit buffers\ninit shaders\ninit textures\n\n\ninit WebCL (context = cl.createContext({ sharedObject: gl }))\ninit shared objects (e.g. textures\/array\/pixel\/render buffers)\nlaunch WebGL rendering loop\n\n... execute GL commands\nacquire GL objects (queue.enqueueAcquireGLObjects())\nlaunch CL computation (queue.enqueueNDRange())\nrelease GL objects (queue.enqueueReleaseGLObjects())\n... execute GL commands\n\n\n\nReferences\nSpecifications\n\nKhronos OpenCL specification, http:\/\/www.khronos.org\/registry\/cl\/\nKhronos WebCL working draft, https:\/\/cvs.khronos.org\/svn\/repos\/registry\/trunk\/public\/webcl\/spec\/latest\/index.html\nKhronos WebGL specification, http:\/\/www.khronos.org\/webgl\/\n\nBooks\n\nHeterogeneous Computing with OpenCL, Benedict Gaster, Lee Howes, David R. Kaeli and Perhaad Mistry, Morgan Kaufmann, August 2011\nOpenCL Programming Guide by Aaftab Munshi, Benedict Gaster, Timothy G. Mattson and James Fung, Addison-Wesley Professional, July 2011\nOpenCL in Action: How to Accelerate Graphics and Computations, Matthew Scarpino, Manning Publications, November 2011\nThe OpenCL Programming Book, Ryoji Tsuchiyama, Takashi Nakamura, Takuro Iizuka and Akihiro Asahara, Fixstars Corporation, April 2010, http:\/\/www.fixstars.com\/en\/opencl\/book\/OpenCLProgrammingBook\/contents.html\nOpenCL Programming Guide for Mac OS X, http:\/\/developer.apple.com\/library\/mac\/documentation\/Performance\/Conceptual\/OpenCL_MacProgGuide\/OpenCL_MacProgGuide.pdf\n\nOpenCL SDKs (use latest!)\n\nNVidia graphic cards: NVidia CUDA\/OpenCL SDK, http:\/\/developer.nvidia.com\/opencl\nAMD\/ATI graphics cards: AMD Accelerated Parallel Processing SDK, http:\/\/developer.amd.com\/zones\/openclzone\/Pages\/default.aspx\nIntel CPUs: Intel OpenCL SDK, http:\/\/software.intel.com\/en-us\/articles\/vcsource-tools-opencl-sdk\/\n\n","489":"TextureMovie Plugin for Unreal Engine 4\nAdds the ability to import AVI\/WMV videos and use them as textures in UE4. Currently only for Windows as it uses the Media Foundation as the backend for playing the videos. Currently working on a cross platform solution.\n\nCurrent version: v0.0.1\nInstallation\n\nDownload the source code and place into \/Engine\/Plugins\/Runtime directory (or your own projects plugin directory) and compile.\nRun editor, you should now have the ability to import AVI\/WMV videos.\n\nKnown Issues\n\nOption \"Reset on Last Frame\" currently does nothing.\nThere is no way to stop or play the video during gameplay.\n\nLegal info\nUnreal\u00ae is a trademark or registered trademark of Epic Games, Inc. in the United States of America and elsewhere.\nUnreal\u00ae Engine, Copyright 1998 \u2013 2014, Epic Games, Inc. All rights reserved.\n","490":"TextureMovie Plugin for Unreal Engine 4\nAdds the ability to import AVI\/WMV videos and use them as textures in UE4. Currently only for Windows as it uses the Media Foundation as the backend for playing the videos. Currently working on a cross platform solution.\n\nCurrent version: v0.0.1\nInstallation\n\nDownload the source code and place into \/Engine\/Plugins\/Runtime directory (or your own projects plugin directory) and compile.\nRun editor, you should now have the ability to import AVI\/WMV videos.\n\nKnown Issues\n\nOption \"Reset on Last Frame\" currently does nothing.\nThere is no way to stop or play the video during gameplay.\n\nLegal info\nUnreal\u00ae is a trademark or registered trademark of Epic Games, Inc. in the United States of America and elsewhere.\nUnreal\u00ae Engine, Copyright 1998 \u2013 2014, Epic Games, Inc. All rights reserved.\n","491":"mangos-zero \nmangos-zero is a full featured server, including authentication, client updates,\nand world content serving compatible with the classic World of Warcaft\nexperience for World of Warcraft Client Patch 1.12 - Drums of War.\nmangos-zero is released under the GPL v2.  The file (LICENSE.mdown) must\nbe a part of any redistributable packages made from this software.  No licenses\nshould be removed from this software if you are making redistributable copies.\nDevelopment\nThe develop branch is where the development of mangos-zero server is done.\nAny of the commits submitted here may or may not become part of the next\nrelease.\nIt is recommended to use the master branch for stable systems, and only use\nthe develop branch if you intend to test commits and submit issues and\/or\nreports.\nCompatibility\nmangos-zero is compatible with scriptdev0 revision 464, and the game\ncontent database 2012-07-15, and newer revisions.\nAnd if something goes wrong?\nIf you feel like submitting an issue, please do so only if you are willing\nto provide a detailed report, and are available to verify any solution to the\nissue provided by the developers of this repository.\nCredits\nMaNGOS has originally been written by Team Python and WoW Daemon Team. Many\npeople further contributed to MaNGOS by reporting problems, suggesting various\nimprovements or submitting actual code.\nSpecial thanks should also go out to the WowwoW team. We have gained help from\nthem many times in the creation of this project.\nThanks should also go out to the Ludmilla team, who are also providing the\ncommunity with a great server. We have not gained too much help from them,\nbut we have recieved some.\nWarning\nThe Massive Network Game Object Server (MaNGOS) has been built with education\nas the main purpose and the MaNGOS team would like to keep it that way.\nSince any public and\/or commercial use of this software is considered illegal\nin many countries (please refer to your local law), the MaNGOS team will not\nprovide any help nor support with such usage in any way.  Every user of this\nsoftware is encouraged to make sure no law is being broken on his\/her side.\nBoth the MaNGOS team and MaNGOS foundation will not take any responsibility\nfor any kind of usage of this software by the end users.\n","492":"mangos-zero \nmangos-zero is a full featured server, including authentication, client updates,\nand world content serving compatible with the classic World of Warcaft\nexperience for World of Warcraft Client Patch 1.12 - Drums of War.\nmangos-zero is released under the GPL v2.  The file (LICENSE.mdown) must\nbe a part of any redistributable packages made from this software.  No licenses\nshould be removed from this software if you are making redistributable copies.\nDevelopment\nThe develop branch is where the development of mangos-zero server is done.\nAny of the commits submitted here may or may not become part of the next\nrelease.\nIt is recommended to use the master branch for stable systems, and only use\nthe develop branch if you intend to test commits and submit issues and\/or\nreports.\nCompatibility\nmangos-zero is compatible with scriptdev0 revision 464, and the game\ncontent database 2012-07-15, and newer revisions.\nAnd if something goes wrong?\nIf you feel like submitting an issue, please do so only if you are willing\nto provide a detailed report, and are available to verify any solution to the\nissue provided by the developers of this repository.\nCredits\nMaNGOS has originally been written by Team Python and WoW Daemon Team. Many\npeople further contributed to MaNGOS by reporting problems, suggesting various\nimprovements or submitting actual code.\nSpecial thanks should also go out to the WowwoW team. We have gained help from\nthem many times in the creation of this project.\nThanks should also go out to the Ludmilla team, who are also providing the\ncommunity with a great server. We have not gained too much help from them,\nbut we have recieved some.\nWarning\nThe Massive Network Game Object Server (MaNGOS) has been built with education\nas the main purpose and the MaNGOS team would like to keep it that way.\nSince any public and\/or commercial use of this software is considered illegal\nin many countries (please refer to your local law), the MaNGOS team will not\nprovide any help nor support with such usage in any way.  Every user of this\nsoftware is encouraged to make sure no law is being broken on his\/her side.\nBoth the MaNGOS team and MaNGOS foundation will not take any responsibility\nfor any kind of usage of this software by the end users.\n","493":"mangos-zero \nmangos-zero is a full featured server, including authentication, client updates,\nand world content serving compatible with the classic World of Warcaft\nexperience for World of Warcraft Client Patch 1.12 - Drums of War.\nmangos-zero is released under the GPL v2.  The file (LICENSE.mdown) must\nbe a part of any redistributable packages made from this software.  No licenses\nshould be removed from this software if you are making redistributable copies.\nDevelopment\nThe develop branch is where the development of mangos-zero server is done.\nAny of the commits submitted here may or may not become part of the next\nrelease.\nIt is recommended to use the master branch for stable systems, and only use\nthe develop branch if you intend to test commits and submit issues and\/or\nreports.\nCompatibility\nmangos-zero is compatible with scriptdev0 revision 464, and the game\ncontent database 2012-07-15, and newer revisions.\nAnd if something goes wrong?\nIf you feel like submitting an issue, please do so only if you are willing\nto provide a detailed report, and are available to verify any solution to the\nissue provided by the developers of this repository.\nCredits\nMaNGOS has originally been written by Team Python and WoW Daemon Team. Many\npeople further contributed to MaNGOS by reporting problems, suggesting various\nimprovements or submitting actual code.\nSpecial thanks should also go out to the WowwoW team. We have gained help from\nthem many times in the creation of this project.\nThanks should also go out to the Ludmilla team, who are also providing the\ncommunity with a great server. We have not gained too much help from them,\nbut we have recieved some.\nWarning\nThe Massive Network Game Object Server (MaNGOS) has been built with education\nas the main purpose and the MaNGOS team would like to keep it that way.\nSince any public and\/or commercial use of this software is considered illegal\nin many countries (please refer to your local law), the MaNGOS team will not\nprovide any help nor support with such usage in any way.  Every user of this\nsoftware is encouraged to make sure no law is being broken on his\/her side.\nBoth the MaNGOS team and MaNGOS foundation will not take any responsibility\nfor any kind of usage of this software by the end users.\n","494":"SCC \u2014 Simple C++\n\nSCC is C++  snippets evaluator at shell prompt\n\n\nSee full docs at http:\/\/volnitsky.com\/project\/scc\n","495":"UACElevator\nPassive UAC elevation using dll infection\nThe shellcode binary is in the shellcode[] char array in UACElevator.cpp and the raw code is in shellcode.asm\n","496":"Grape is a stepping stone to building data processing systems on top of Elliptics routing and server-side code execution.\nIts main goal is to provide an active example of elliptics data processing capabilities and also to provide ready-to-use building blocks for such systems.\nGrape, as for now, consist of 2 components:\n\nfault-tolerant persistent queue\nand a connector that allows to direct queue output into user application running on elliptics cluster (see event driver concept in Cocaine docs)\n\nQueue\nQueue is a cocaine application running on elliptics node. Its deployment process follows general process for cocaine applications.\nOnce deployed and started queue accepts data entries pushed into it, stores them among nodes of elliptics cluster its working on, and gives data entries back on consumer request, maintaining entries original order.\nQueue supports fault-tolerance by using data replication and by implementing fault-replay mechanics: consumer must acknowledge processing status of every data entry that it retrieved from the queue - failing to do so will result in entry \"replay\", over and over again up until it'll be confirmed.\n(For further details about how this works internally see TODO: How queue works.)\nAPI\nQueue's API basically consist of three methods: push, peek, ack:\n\npush pushes data entry to the top of the queue\npeek gets data entry from the bottom of the queue\nack confirms that entry has been processed and could be dropped\n\n(Latter two are combined in additional short-circuit method pop.)\nThese methods are implemented in two sets: simple one operates in single queue entries and more complex one operates in multi-entry blocks.\nqueue.push\ndnet_id key;\nsession->exec(&key, \"queue@push\", ioremap::elliptics::data_pointer::from_raw(\"abcd\")).wait();\n\nPushes data entry (\"abcd\") to the queue running under the base name \"queue\" at node responsible for the specified dnet_id.\nThere is no multi-entry variant for this method.\nqueue.peek\ndnet_id key;\nioremap::elliptics::exec_context context = session->exec(\n        &key, \"queue@peek\", ioremap::elliptics::data_pointer()\n        ).get_one().context();\nioremap::elliptics::data_pointer entry_data = context.data();\nioremap::grape::entry_id entry_id = ioremap::grape::entry_id::from_dnet_raw_id(context.src_id());\n\nPeeks data entry from the queue running under the base name \"queue\" at node responsible for the specified dnet_id.\nReturns entry id embedded in src_id field of the response. Also returns queue's supplemental subid in the src_key field (that subid makes possible to acknowledge entry back and thus must be preserved). Both fields are accessible through exec_context.\n(Details of the TODO: request and response fields of the exec command explained separately.)\nqueue.ack\nsession->exec(context, \"queue@ack\", ioremap::elliptics::data_pointer()).wait();\n\nor equivalent:\nsession->exec(context.src_id(), context.src_key(), \"queue@ack\", ioremap::elliptics::data_pointer()).wait();\n\nAcknowledges entry received by a previous peek.\nEntry id must be sent embedded in dnet_id of the request. src_key must be set to that received by a previous peek.\nqueue.peek-multi\ndnet_id key;\nioremap::elliptics::exec_context context = session->exec(\n        &key, \"queue@peek-multi\", ioremap::elliptics::data_pointer(\"100\")\n        ).get_one().context();\nauto array = ioremap::grape::deserialize<ioremap::grape::data_array>(context.data());\nioremap::elliptics::data_pointer d = array.data();\nsize_t offset = 0;\nfor (size_t i = 0; i < array.sizes().size(); ++i) {\n    int bytesize = array.sizes()[i];\n    \/\/ process data: (d.data() + offset, bytesize)\n    offset += bytesize;\n}\n\nPeeks multiple data entries from the queue running under the base name \"queue\" at node responsible for the specified dnet_id.\nPeek-multi has an argument: hint about number of entries, which must be presented in a string form.\nReturns serialized ioremap::grape::data_array structure which holds entries' data packed into byte array and array with entries' byte sizes and array with entries' ids.\nioremap::grape::data_array is declared in a header file include\/grape\/data_array.hpp.\nqueue.ack-multi\nioremap::grape::data_array array = ...;\nsession->exec(context, \"queue@ack-multi\", ioremap::grape::serialize(array.ids())).wait();\n\nAcknowledges entries received by a previous peek (may be several).\nqueue.pop and queue.pop-multi\nShort circuit methods pop and pop-multi has a combined effect of peek and ack called in one go. They are simple to use but also lose acking and replaying properties.\nAdditional methods\nQueue also implements few techical methods (in addition to common TODO: Cocaine and Elliptics app managment capabilities):\n\nping can be used to see if queue is currently active (or activate it for that matter)\nstats shows internal state and statistics queue gathers about itself\n\nConfiguration\nQueue reads its configuration from the file queue.conf. This file must be included in deployment tarball along with an app executable (see following section on Deployment).\nqueue.conf must contain configuration for the elliptics client (used to return replies on inbound events) and can include queue configuration options.\nThere is only one configuration option for now:\n\nchunk-max-size (int) - specifies how many entries will contain single chunk in the queue (default value: 10000)\n\nDeployment\nDeployment process of the queue follows general process for cocaine applications. For launching the queue user needs three files:\n\nqueue application file (which is an executable)\nqueue.conf config file (which is also a manifest file)\nqueue.profile execution profile file\n\nqueue app could be taken from the binary package grape-components or built from the sources. Config and profile files also exist both in source repository and included in the same package.\nHere we presume that user have installation of elliptics running on localhost:1025 in group 2 (how to do it see Elliptics: Server setup tutorial).\nqueue.conf content:\n{\n    \"type\": \"binary\",\n    \"slave\": \"queue\",\n\n    \"remotes\": [\n        \"localhost:1025:2\"\n     ],\n    \"groups\": [2]\n}\n\nqueue.profile content:\n{\n    \"heartbeat-timeout\" : 60,\n    \"pool-limit\" : 1,\n    \"queue-limit\" : 1000,\n    \"grow-threshold\" : 1,\n    \"concurrency\" : 10,\n    \"idle-timeout\": 0\n}\n\nSteps to launch a queue:\n\nCreate tarball with queue executable and config files:\n\ntar cvjf queue.tar.bz2 queue queue.conf\n\n\nUpload tarball, manifest (same as config) and profile\n\ncocaine-tool -n queue -m queue.conf -p queue.tar.bz2 app:upload\ncocaine-tool -n queue -m queue.profile profile:upload\n\n\nDeploy the app (get it ready to run)\n\ndnet_ioclient -r localhost:1025:2 -g 2 -c \"queue@start-multiple-task local\"\n\n(More details about what these commands do exactly see in TODO: Cocaine: application deployment and TODO: Elliptics task management.)\nNow queue is deployed (on every node that this elliptics installation includes, most possible that would be a single node here) and will actually start as soon as it'll receive its first command (or event).\nActivate the queue:\ndnet_ioclient -r localhost:1025:2 -g 2 -c \"queue@ping\"\n\nQueue is up and running if reply would be:\n127.0.0.1:1025: queue@ping \"ok\"\n\n\nLinks:\n\nElliptics: http:\/\/www.reverbrain.com\/elliptics\/\nCocaine: https:\/\/github.com\/cocaine\/cocaine-core\nGoogle group: https:\/\/groups.google.com\/forum\/?fromgroups=#!forum\/reverbrain\n\n","497":"Stp.pediff\n\nA simple set of tools for visually comparing web pages built on top of\nCasperjs and ImageMagick\ncompare tool.\n\nView a sample report here.\nTable of Contents\n\nHow it works\nWhy to use it\nDependencies\nUsage\nVerbose mode\nReports\nCoverage\nMocks\n\nHow it works\nBasically, Pediff executes a set of user defined tasks over two different versions of a website,\ntakes screenshots at desired moments and scans the output for differences.\nThen it generates human-friendly report containing all the inconsistencies ordered by\nrelative number of differences. Optionally, it can check whether all the routes of your web application\nare tested.\nWhy to use it\nPediff enables developers to detect entire class of visual problems invisible to\nclassic unit tests and only occasionally catchable by manual review. For more details on the topic\nsee this great talk by Brett Slatkin at Air Mozilla:\nhttps:\/\/air.mozilla.org\/continuous-delivery-at-google\/\nDependencies\n\nUnix-like operating system with Bash shell\nCasperjs 1.1.0 or newer\nImageMagick compare tool\n\nUsage\n\n\nDownload the project to a place of your convenience\n\n\nRename config.js-dist to config.js and set the environments values so that candidate key\npoints to candidate version of your app and current key points to, well... current version of\nyour app. The options object will be passed to Casperjs instance upon every Pediff run.\n\n\nCreate as many task files as needed. They all must be placed inside the tasks\/ subdirectory.\nExample task file:\nmodule.exports = {\n    config: {\n        path: 'PATH\/TO\/RESOURCE'\n        options: {\n            viewportSize: [{width: 1440, height: 900}]\n        },\n        media: {\n            print: false\n        },\n        package: \"homepage\"\n    },\n    execute: function(){\n        this.save('home');\n    }\n};\nEvery task must contain two sections: config and execute. The config part will be merged\nwith global config file before task execution. It serves the purpose of \"creating a sandbox\" for\nthe task: subpage to open, a set of screen resolutions, user agent, media types,\nresponse mocks - things like that.\n###Some config keys explained:\n\npackage is the label under which all the output for given task will be grouped by in the report.\nmedia is the key that enables you to force PhantomJS to render page as different media (ex.\nprint)\n\nThe execute function will be run after the page has been loaded by Casperjs. Everything from\nCasperjs API is acceptable, however you should use\nsave(filename) instead of built in capture method for taking screenshots to make sure the\nfiles are saved in correct directories for comparison.\n\n\nAfter creating your tasks, type:\n$ .\/pediff.sh\ninto terminal and wait for the tool to finish.\nPediff runs every task in a subshell to speed things up. You can limit number of tasks\nto be run parallelly by providing single positive integer as a parameter:\n$ .\/pediff.sh 4\nThis way, tasks will be run in sets of 4 at a time.\n\n\nAt this point index.html file should be sitting in project's report\/ subdirectory. Open it with your\nbrowser and review results.\n\n\nVerbose mode\nWhile working on tasks it is useful to see what is going on during execution.\nYou can force pediff to output more data by setting verbose option to true in your config.js\nmodule.exports = {\n    options: {\n        verbose: true,\n        ...\nReports\nPediff generates convenient reports by default. Just open report\/index.html file with your browser.\n\nExample: Diff view between consecutive deployments of VGTV (difference in video impressions picked up).\nReport's anatomy\nGeneral\n\nOn the left you can see a list of all the tasks executed on your page along with label marking\nlevel of compatibility between the two versions. The list is sorted by number of differences\n(most different first).\nOptionally, you can toggle visibility of matching tasks (100% compability) with \"Show only\ndiffering\" button below.\nBy default, the tool renders lighter (and uglier) jpg images to reduce download times. You can change\nthis behavior with the \"HQ\" button, which forces pediff to load much heavier png images.\nIn the bottom left corner there's \"Test coverage\" link. More on coverage.\n\nTask view\n\nIn the top right corner you can see a list of different screen resulutions that selected task was\nexecuted on.\nIn the central area of report the actual screenshots are displayed. You can switch between\ndiff, current and candidate versions using both arrow keys and mouse clicks (left click forwards,\nright click backwards).\n\nMocks\nSometimes you may want to alter browser state by ensuring a request ends with a certain response.\nYou can do that by providing mocks object to your task's config section:\n\/\/ ...\nconfig: {\n    options: {\n        viewportSize: [{width: 1100, height: 2500}]\n    },\n    mocks: {'modernizer.js': 'modernizr-notouch.js'}\n}\n\/\/ ...\nwhere you map request to mock. Mocks must be located in mocks\/ subdirectory.\nCoverage\nGeneral\nThe tests coverage feature, allows one to easily check, whether all possible subpages of a project are being tested.\nPediff does this by comparing a given set of routes, placed in routes.json, with test paths defined in individual tasks.\nYou can disable the tests coverage feature by setting coverage property of pediff config file to false.\n\/\/ ...\ncoverage: {\n    routes: 'routes.json',\n    skipOptional: true,\n    exceptions: [\n        {\n            route: '',\n            mapTo: ''\n        }\n    ]\n}\n\/\/ ...\nCoverage configuration allows you to set a different path to the routes file (e.g. when this file is generated\nautomatically), and define exceptions by explicitly mapping a path from routes definition to a path in a task file.\nYou may also choose to force checking of routes' optional fragments by setting the skipOptional flag to false.\nReport\nResults of the test coverage check are presented in the pediff report, with an indication of tested viewport sizes,\nmedia and files in which specific tasks are defined. Routes not tested with any tasks will be marked with red color,\nand be included in the total coverage percentage.\nExtending\nCurrently coverage check is possible only for routes defined according to\nBackbone.js routes specification. Support for custom routing setups can be\nadded by extending coverage Route prototype (located in coverage\/route.js), and specifically by overriding the\nfollowing methods: buildVariants, isFragmentOptional, isFragmentVariable. More details can be found in comments\nfor these methods.\n","498":"    \nInline Manifest Webpack Plugin\nThis is a webpack plugin that inline your manifest.js with a script tag to save http request. Cause webpack's runtime always change between every build, it's better to split the runtime code out for long-term caching.\nInstallation\nInstall the plugin with npm:\n$ npm i inline-manifest-webpack-plugin -D\nBasic Usage\nThis plugin need to work with webpack v4 (for webpack v3 and below, pls use version 3) and HtmlWebpackPlugin v3 :\nStep1: split out the runtime code\n\/\/ the default name is \"runtime\"\noptimization: {\n    runtimeChunk: 'single'\n }\n\n\/\/ or specify another name\noptimization: {\n    runtimeChunk: {\n        name: 'another name'\n    }\n }\nStep2: add plugins:\n\/\/ this plugin need to put after HtmlWebpackPlugin\n[\n    new HtmlWebpackPlugin(),\n    new InlineManifestWebpackPlugin()\n]\n\nor\n\n[\n    new HtmlWebpackPlugin(),\n    \/\/ if you changed the runtimeChunk's name, you need to sync it here\n    new InlineManifestWebpackPlugin('another name')\n]\nDone! This will replace the external script with inline code.\nOne more thing\nif you use inject: false in your HtmlWebpackPlugin, you can access the runtime code like this:\n<%= htmlWebpackPlugin.files.runtime %>\n\n<% for (var chunk in htmlWebpackPlugin.files.chunks) { %>\n<script src=\"<%= htmlWebpackPlugin.files.chunks[chunk].entry %>\"><\/script>\n<% } %>\n","499":"Dockunit\nContainerized unit testing across any platform and programming language.\nPurpose\nWe all want to test our applications on as many relevant platforms as possible. Sometimes this is easy.\nSometimes it's not. Dockunit let's you define a set of Docker containers to run your tests against. You can run your\ntest framework of choice in your language of choice on any type of environment. In the past many developers, myself\nincluded, have relied on Travis CI to run tests in environments that aren't setup locally (i.e. PHP 5.2). With\nDockunit you don't need to do this anymore.\nRequirements\n\nOSX or a Linux Distribution (Windows not yet tested or officially supported)\nNode.js\nnpm\nDocker\n\nInstallation\n\nMake sure you have Node.js, Docker, and npm install\nInstall via npm:\n\nnpm install -g dockunit\nUsage\nDockunit relies on Dockunit.json files. Each of your projects should have their own Dockunit.json file.\nDockunit.json defines what test commands should be run on what type of containers for any given project. Here is an\nexample Dockunit.json:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"PHP 5.2 on Ubuntu\",\n      \"image\": \"user\/my-php-image\",\n      \"beforeScripts\": [],\n      \"testCommand\": \"phpunit\"\n    },\n    {\n      \"prettyName\": \"PHP 5.6 FPM on Ubuntu\",\n      \"image\": \"user\/my-php-image2\",\n      \"beforeScripts\": [],\n      \"testCommand\": \"phpunit\"\n    }\n  ]\n}\ncontainers contains an array of container objects. Each container object can contain the following properties:\n\nprettyName (required) - This is used in output to help you identify your container.\nimage (required) - This is a valid Docker container image located in the Docker registry. We have a number of handy prebuilt Docker images for use in your Dockunit.json files.\nbeforeScripts (optional) - This is a string array of bash scripts to be run in order.\ntestCommand (required) - This is the actual test command to be run on each container i.e. phpunit or qunit.\n\nThe Dockunit command is:\ndockunit <path-to-project-directory> [--du-verbose] [--du-container] [--help] [--version] ...\nNote: sudo is usually required when run within a Linux distribution since Dockunit runs Docker commands which require special permissions.\n\n<path-to-project-directory> (optional) - If you run dockunit in a folder with a Dockunit.json folder, it will detect it\nautomatically.\n[--du-verbose] (optional) - This will print out light verbose Dockunit output. [--du-verbose=2] will output even more verbose Dockunit output.\n[--du-container] (optional) - Run only one container in your Dockunit.json file by specifying the index of that container in the containers array .i.e --du-container=1.\n[--help] (optional) - This will display usage information for the dockunit command.\n[--version] (optional) - This will display the current installed version of Dockunit.\n... - Any additional arguments and options passed to the command will be passed to your test command. For example,\nif you wanted to pass a few extra options to PHPUnit, you could append them to the end of your dockunit command.\n\nYou can simply run dockunit in any folder with a Dockunit.json to run Dockunit.\nDockunit.json Examples\nEach of your projects should have a Dockunit.json file in the project root. You should define your containers to fit\nyour application's unique needs. Here's a few example Dockunit.json files for a variety of different programming languages and\nenvironments. Feel free to use any of our prebuilt Docker images in your Dockunit.json files or create your own.\nPHP and WordPress\nDockunit and WordPress work well together. WordPress is backwards compatible with PHP 5.2. It's very difficult to test\napplications on PHP 5.2 without some sort of containerized workflow. Here is an example Dockunit.json file that you\ncan use to test your WordPress plugins in PHP 5.2, 5.6, and PHP 7.0 RC 1 (make sure to replace PLUGIN-FILE.php with your plugins main file):\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"PHP-FPM 5.2 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-5.2-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp-install latest\"\n      ],\n      \"testCommand\": \"wp-activate-plugin PLUGIN-FILE.php\"\n    },\n    {\n      \"prettyName\": \"PHP-FPM 5.6 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-5.6-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp core download --path=\/temp\/wp --allow-root\",\n        \"wp core config --path=\/temp\/wp --dbname=test --dbuser=root --allow-root\",\n        \"wp core install --url=http:\/\/localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=\/temp\/wp --allow-root\",\n        \"mkdir \/temp\/wp\/wp-content\/plugins\/test\",\n        \"cp -r . \/temp\/wp\/wp-content\/plugins\/test\"\n      ],\n      \"testCommand\": \"wp plugin activate test --allow-root --path=\/temp\/wp\"\n    },\n    {\n      \"prettyName\": \"PHP-FPM 7.0 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-7.0-rc-1-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp core download --path=\/temp\/wp --allow-root\",\n        \"wp core config --path=\/temp\/wp --dbname=test --dbuser=root --allow-root\",\n        \"wp core install --url=http:\/\/localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=\/temp\/wp --allow-root\",\n        \"mkdir \/temp\/wp\/wp-content\/plugins\/test\",\n        \"cp -r . \/temp\/wp\/wp-content\/plugins\/test\"\n      ],\n      \"testCommand\": \"wp plugin activate test --allow-root --path=\/temp\/wp\"\n    }\n  ]\n}\nHere is an example Dockunit.json file that you can use to test your WordPress themes in PHP 5.2, 5.6, and PHP 7.0 RC 1:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"PHP-FPM 5.2 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-5.2-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp-install latest\"\n      ],\n      \"testCommand\": \"wp-activate-theme test\"\n    },\n    {\n      \"prettyName\": \"PHP-FPM 5.6 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-5.6-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp core download --path=\/temp\/wp --allow-root\",\n        \"wp core config --path=\/temp\/wp --dbname=test --dbuser=root --allow-root\",\n        \"wp core install --url=http:\/\/localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=\/temp\/wp --allow-root\",\n        \"mkdir \/temp\/wp\/wp-content\/themes\/test\",\n        \"cp -r . \/temp\/wp\/wp-content\/themes\/test\"\n      ],\n      \"testCommand\": \"wp theme activate test --allow-root --path=\/temp\/wp\"\n    },\n    {\n      \"prettyName\": \"PHP-FPM 7.0 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-7.0-rc-1-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp core download --path=\/temp\/wp --allow-root\",\n        \"wp core config --path=\/temp\/wp --dbname=test --dbuser=root --allow-root\",\n        \"wp core install --url=http:\/\/localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=\/temp\/wp --allow-root\",\n        \"mkdir \/temp\/wp\/wp-content\/themes\/test\",\n        \"cp -r . \/temp\/wp\/wp-content\/themes\/test\"\n      ],\n      \"testCommand\": \"wp theme activate test --allow-root --path=\/temp\/wp\"\n    }\n  ]\n}\nPHP and WordPress Unit Tests\nHere are some more advanced WordPress examples. That assume you have unit tests setup via WP-CLI.\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"PHP 5.2 FPM WordPress 4.1\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-5.2-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"bash bin\/install-wp-tests.sh wordpress_test root '' localhost 4.1\"\n      ],\n      \"testCommand\": \"phpunit\"\n    },\n    {\n      \"prettyName\": \"PHP 5.6 FPM WordPress 4.0\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-5.6-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"bash bin\/install-wp-tests.sh wordpress_test2 root '' localhost 4.0\"\n      ],\n      \"testCommand\": \"phpunit\"\n    },\n    {\n      \"prettyName\": \"PHP 7.0 RC-1\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-7.0-rc-1-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"bash bin\/install-wp-tests.sh wordpress_test3 root '' localhost 3.9\"\n      ],\n      \"testCommand\": \"phpunit\"\n    }\n  ]\n}\ndockunit\/prebuilt-images:php-mysql-phpunit-5.6-fpm, dockunit\/prebuilt-images:php-mysql-phpunit-5.6-fpm, and dockunit\/prebuilt-images:php-mysql-phpunit-7.0-rc-1-fpm are valid Docker images available for use in any Dockerfile.json.\nNode.js\nIt is super easy to test your Node.js applications with Dockunit. Here is a simple Dockunit.json file that tests\nan application in Node.js 0.10.x and 0.12.0 using mocha:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"Node 0.10.x\",\n      \"image\": \"google\/nodejs:latest\",\n      \"beforeScripts\": [\n        \"npm install -g mocha\"\n      ],\n      \"testCommand\": \"mocha\"\n    },\n    {\n      \"prettyName\": \"Node 0.12\",\n      \"image\": \"tlovett1\/nodejs:0.12\",\n      \"beforeScripts\": [\n        \"npm install -g mocha\"\n      ],\n      \"testCommand\": \"mocha\"\n    }\n  ]\n}\ngoogle\/nodejs is a valid Docker image available for use in any Dockerfile.json.\nPython\nDockunit works great with Python. This Dockunit.json file tests in Python 2.7.9 and the latest Python version using nose:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"Python 2.7.9\",\n      \"image\": \"python:2.7.9\",\n      \"beforeScripts\": [\n        \"easy_install nose\"\n      ],\n      \"testCommand\": \"nosetests tests\"\n    },\n    {\n      \"prettyName\": \"Python Latest\",\n      \"image\": \"python:latest\",\n      \"beforeScripts\": [\n        \"easy_install nose\"\n      ],\n      \"testCommand\": \"nosetests tests\"\n    }\n  ]\n}\nRuby\nYou can use Dockunit to test your Ruby scripts. This Dockunit.json file tests a project in Ruby 2.1 and the latest\nstable Ruby version using test-unit:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"Latest version of Ruby\",\n      \"image\": \"ruby:latest\",\n      \"beforeScripts\": [\n        \"bundle install\"\n      ],\n      \"testCommand\": \"bundle exec rake test\"\n    },\n    {\n      \"prettyName\": \"Ruby version 2.1\",\n      \"image\": \"ruby:2.1\",\n      \"beforeScripts\": [\n        \"bundle install\"\n      ],\n      \"testCommand\": \"bundle exec rake test\"\n    }\n  ]\n}\nruby is a valid Docker image available for use in any Dockerfile.json.\nLicense\nDockunit is free software; you can redistribute it and\/or modify it under the terms of the GNU General\nPublic License as published by the Free Software Foundation; either version\n2 of the License, or (at your option) any later version.\n","500":"Dockunit\nContainerized unit testing across any platform and programming language.\nPurpose\nWe all want to test our applications on as many relevant platforms as possible. Sometimes this is easy.\nSometimes it's not. Dockunit let's you define a set of Docker containers to run your tests against. You can run your\ntest framework of choice in your language of choice on any type of environment. In the past many developers, myself\nincluded, have relied on Travis CI to run tests in environments that aren't setup locally (i.e. PHP 5.2). With\nDockunit you don't need to do this anymore.\nRequirements\n\nOSX or a Linux Distribution (Windows not yet tested or officially supported)\nNode.js\nnpm\nDocker\n\nInstallation\n\nMake sure you have Node.js, Docker, and npm install\nInstall via npm:\n\nnpm install -g dockunit\nUsage\nDockunit relies on Dockunit.json files. Each of your projects should have their own Dockunit.json file.\nDockunit.json defines what test commands should be run on what type of containers for any given project. Here is an\nexample Dockunit.json:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"PHP 5.2 on Ubuntu\",\n      \"image\": \"user\/my-php-image\",\n      \"beforeScripts\": [],\n      \"testCommand\": \"phpunit\"\n    },\n    {\n      \"prettyName\": \"PHP 5.6 FPM on Ubuntu\",\n      \"image\": \"user\/my-php-image2\",\n      \"beforeScripts\": [],\n      \"testCommand\": \"phpunit\"\n    }\n  ]\n}\ncontainers contains an array of container objects. Each container object can contain the following properties:\n\nprettyName (required) - This is used in output to help you identify your container.\nimage (required) - This is a valid Docker container image located in the Docker registry. We have a number of handy prebuilt Docker images for use in your Dockunit.json files.\nbeforeScripts (optional) - This is a string array of bash scripts to be run in order.\ntestCommand (required) - This is the actual test command to be run on each container i.e. phpunit or qunit.\n\nThe Dockunit command is:\ndockunit <path-to-project-directory> [--du-verbose] [--du-container] [--help] [--version] ...\nNote: sudo is usually required when run within a Linux distribution since Dockunit runs Docker commands which require special permissions.\n\n<path-to-project-directory> (optional) - If you run dockunit in a folder with a Dockunit.json folder, it will detect it\nautomatically.\n[--du-verbose] (optional) - This will print out light verbose Dockunit output. [--du-verbose=2] will output even more verbose Dockunit output.\n[--du-container] (optional) - Run only one container in your Dockunit.json file by specifying the index of that container in the containers array .i.e --du-container=1.\n[--help] (optional) - This will display usage information for the dockunit command.\n[--version] (optional) - This will display the current installed version of Dockunit.\n... - Any additional arguments and options passed to the command will be passed to your test command. For example,\nif you wanted to pass a few extra options to PHPUnit, you could append them to the end of your dockunit command.\n\nYou can simply run dockunit in any folder with a Dockunit.json to run Dockunit.\nDockunit.json Examples\nEach of your projects should have a Dockunit.json file in the project root. You should define your containers to fit\nyour application's unique needs. Here's a few example Dockunit.json files for a variety of different programming languages and\nenvironments. Feel free to use any of our prebuilt Docker images in your Dockunit.json files or create your own.\nPHP and WordPress\nDockunit and WordPress work well together. WordPress is backwards compatible with PHP 5.2. It's very difficult to test\napplications on PHP 5.2 without some sort of containerized workflow. Here is an example Dockunit.json file that you\ncan use to test your WordPress plugins in PHP 5.2, 5.6, and PHP 7.0 RC 1 (make sure to replace PLUGIN-FILE.php with your plugins main file):\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"PHP-FPM 5.2 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-5.2-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp-install latest\"\n      ],\n      \"testCommand\": \"wp-activate-plugin PLUGIN-FILE.php\"\n    },\n    {\n      \"prettyName\": \"PHP-FPM 5.6 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-5.6-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp core download --path=\/temp\/wp --allow-root\",\n        \"wp core config --path=\/temp\/wp --dbname=test --dbuser=root --allow-root\",\n        \"wp core install --url=http:\/\/localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=\/temp\/wp --allow-root\",\n        \"mkdir \/temp\/wp\/wp-content\/plugins\/test\",\n        \"cp -r . \/temp\/wp\/wp-content\/plugins\/test\"\n      ],\n      \"testCommand\": \"wp plugin activate test --allow-root --path=\/temp\/wp\"\n    },\n    {\n      \"prettyName\": \"PHP-FPM 7.0 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-7.0-rc-1-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp core download --path=\/temp\/wp --allow-root\",\n        \"wp core config --path=\/temp\/wp --dbname=test --dbuser=root --allow-root\",\n        \"wp core install --url=http:\/\/localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=\/temp\/wp --allow-root\",\n        \"mkdir \/temp\/wp\/wp-content\/plugins\/test\",\n        \"cp -r . \/temp\/wp\/wp-content\/plugins\/test\"\n      ],\n      \"testCommand\": \"wp plugin activate test --allow-root --path=\/temp\/wp\"\n    }\n  ]\n}\nHere is an example Dockunit.json file that you can use to test your WordPress themes in PHP 5.2, 5.6, and PHP 7.0 RC 1:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"PHP-FPM 5.2 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-5.2-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp-install latest\"\n      ],\n      \"testCommand\": \"wp-activate-theme test\"\n    },\n    {\n      \"prettyName\": \"PHP-FPM 5.6 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-5.6-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp core download --path=\/temp\/wp --allow-root\",\n        \"wp core config --path=\/temp\/wp --dbname=test --dbuser=root --allow-root\",\n        \"wp core install --url=http:\/\/localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=\/temp\/wp --allow-root\",\n        \"mkdir \/temp\/wp\/wp-content\/themes\/test\",\n        \"cp -r . \/temp\/wp\/wp-content\/themes\/test\"\n      ],\n      \"testCommand\": \"wp theme activate test --allow-root --path=\/temp\/wp\"\n    },\n    {\n      \"prettyName\": \"PHP-FPM 7.0 WordPress Latest\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-wordpress-7.0-rc-1-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"wp core download --path=\/temp\/wp --allow-root\",\n        \"wp core config --path=\/temp\/wp --dbname=test --dbuser=root --allow-root\",\n        \"wp core install --url=http:\/\/localhost --title=Test --admin_user=admin --admin_password=12345 --admin_email=test@test.com --path=\/temp\/wp --allow-root\",\n        \"mkdir \/temp\/wp\/wp-content\/themes\/test\",\n        \"cp -r . \/temp\/wp\/wp-content\/themes\/test\"\n      ],\n      \"testCommand\": \"wp theme activate test --allow-root --path=\/temp\/wp\"\n    }\n  ]\n}\nPHP and WordPress Unit Tests\nHere are some more advanced WordPress examples. That assume you have unit tests setup via WP-CLI.\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"PHP 5.2 FPM WordPress 4.1\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-5.2-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"bash bin\/install-wp-tests.sh wordpress_test root '' localhost 4.1\"\n      ],\n      \"testCommand\": \"phpunit\"\n    },\n    {\n      \"prettyName\": \"PHP 5.6 FPM WordPress 4.0\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-5.6-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"bash bin\/install-wp-tests.sh wordpress_test2 root '' localhost 4.0\"\n      ],\n      \"testCommand\": \"phpunit\"\n    },\n    {\n      \"prettyName\": \"PHP 7.0 RC-1\",\n      \"image\": \"dockunit\/prebuilt-images:php-mysql-phpunit-7.0-rc-1-fpm\",\n      \"beforeScripts\": [\n        \"service mysql start\",\n        \"bash bin\/install-wp-tests.sh wordpress_test3 root '' localhost 3.9\"\n      ],\n      \"testCommand\": \"phpunit\"\n    }\n  ]\n}\ndockunit\/prebuilt-images:php-mysql-phpunit-5.6-fpm, dockunit\/prebuilt-images:php-mysql-phpunit-5.6-fpm, and dockunit\/prebuilt-images:php-mysql-phpunit-7.0-rc-1-fpm are valid Docker images available for use in any Dockerfile.json.\nNode.js\nIt is super easy to test your Node.js applications with Dockunit. Here is a simple Dockunit.json file that tests\nan application in Node.js 0.10.x and 0.12.0 using mocha:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"Node 0.10.x\",\n      \"image\": \"google\/nodejs:latest\",\n      \"beforeScripts\": [\n        \"npm install -g mocha\"\n      ],\n      \"testCommand\": \"mocha\"\n    },\n    {\n      \"prettyName\": \"Node 0.12\",\n      \"image\": \"tlovett1\/nodejs:0.12\",\n      \"beforeScripts\": [\n        \"npm install -g mocha\"\n      ],\n      \"testCommand\": \"mocha\"\n    }\n  ]\n}\ngoogle\/nodejs is a valid Docker image available for use in any Dockerfile.json.\nPython\nDockunit works great with Python. This Dockunit.json file tests in Python 2.7.9 and the latest Python version using nose:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"Python 2.7.9\",\n      \"image\": \"python:2.7.9\",\n      \"beforeScripts\": [\n        \"easy_install nose\"\n      ],\n      \"testCommand\": \"nosetests tests\"\n    },\n    {\n      \"prettyName\": \"Python Latest\",\n      \"image\": \"python:latest\",\n      \"beforeScripts\": [\n        \"easy_install nose\"\n      ],\n      \"testCommand\": \"nosetests tests\"\n    }\n  ]\n}\nRuby\nYou can use Dockunit to test your Ruby scripts. This Dockunit.json file tests a project in Ruby 2.1 and the latest\nstable Ruby version using test-unit:\n{\n  \"containers\": [\n    {\n      \"prettyName\": \"Latest version of Ruby\",\n      \"image\": \"ruby:latest\",\n      \"beforeScripts\": [\n        \"bundle install\"\n      ],\n      \"testCommand\": \"bundle exec rake test\"\n    },\n    {\n      \"prettyName\": \"Ruby version 2.1\",\n      \"image\": \"ruby:2.1\",\n      \"beforeScripts\": [\n        \"bundle install\"\n      ],\n      \"testCommand\": \"bundle exec rake test\"\n    }\n  ]\n}\nruby is a valid Docker image available for use in any Dockerfile.json.\nLicense\nDockunit is free software; you can redistribute it and\/or modify it under the terms of the GNU General\nPublic License as published by the Free Software Foundation; either version\n2 of the License, or (at your option) any later version.\n","501":"require-analyzer\nDetermine dependencies for a given node.js file, directory tree, or module in code or on the command line\nStatus\n\nInstallation\nInstalling npm (node package manager)\n  curl http:\/\/npmjs.org\/install.sh | sh\n\nInstalling require-analyzer\n  [sudo] npm install require-analyzer\n\nNOTE: If you're using npm >= 1.0 then you need to add the -g parameter to install require-analyzer globally.\nUsage\nThere are two distinct ways to use the require-analyzer library: from the command line or through code. The command line tool is designed to work with package.json files so make sure that you have created one for your project first. Checkout jitsu for a quick and easy way to create a package.json.\nFor more information read our blog post at blog.nodejitsu.com.\nCommand-line usage\nUsing require-analyzer from the command line is easy. The binary will attempt to read the package.json file in the current directory, then analyze the dependencies and cross reference the result.\n  $ require-analyzer --help\n  usage: require-analyzer [options] [directory]\n\n  Analyzes the node.js requirements for the target directory. If no directory\n  is supplied then the current directory is used\n\n  options:\n    --update     Update versions for existing dependencies\n    -h, --help   You're staring at it\n\nHere's a sample of require-analyzer analyzing its own dependencies:\n  $ require-analyzer\n  info:  require-analyzer starting in \/Users\/Charlie\/Nodejitsu\/require-analyzer\n  warn:  No dependencies found\n  info:  Analyzing dependencies...\n  info:  Done analyzing raw dependencies\n  info:  Retrieved packages from npm\n  info:  Additional dependencies found\n  data:  {\n  data:    findit: '>= 0.0.3',\n  data:    npm: '>= 0.3.18'\n  data:  }\n  info:  Updating \/Users\/Charlie\/Nodejitsu\/require-analyzer\/package.json\n  info:  require-analyzer updated package.json dependencies\n\nProgrammatic usage\nThe easiest way to use require-analyzer programmatically is through the .analyze() method. This method will use fs.stat() on the path supplied and attempt one of three options:\n\nIf it is a directory that has a package.json, analyze require statements from package.main\nIf it is a directory with no package.json analyze every .js or .coffee file in the directory tree\nIf it is a file, then analyze require statements from that individual file.\n\nLets dive into a quick sample usage:\n  var analyzer = require('require-analyzer');\n\n  var options = {\n    target: 'path\/to\/your\/dependency' \/\/ e.g \/Users\/some-user\/your-package\n    reduce: true\n  };\n\n  var deps = analyzer.analyze(options, function (err, pkgs) {\n    \/\/\n    \/\/ Log all packages that were discovered\n    \/\/\n    console.dir(pkgs);\n  });\n\n  \/\/\n  \/\/ The call the `.analyze()` returns an `EventEmitter` which outputs\n  \/\/ data at various stages of the analysis operation.\n  \/\/\n  deps.on('dependencies', function (raw) {\n    \/\/\n    \/\/ Log the raw list of dependencies (no versions)\n    \/\/\n    console.dir(raw);\n  });\n\n  deps.on('search', function (pkgs) {\n    \/\/\n    \/\/ Log the results from the npm search operation with the current\n    \/\/ active version for each dependency\n    \/\/\n    console.dir(pkgs);\n  });\n\n  deps.on('reduce', function (reduced) {\n    \/\/\n    \/\/ Logs the dependencies after they have been cross-referenced with\n    \/\/ sibling dependencies. (i.e. if 'foo' requires 'bar', 'bar' will be removed).\n    \/\/\n    console.dir(reduced);\n  });\nFurther analyzing dependencies\nSometimes when dealing with dependencies it is necessary to further analyze the dependencies that are returned. require-analyzer has a convenience method for doing just this:\n  var analyzer = require('require-analyzer');\n\n  var current = {\n    'foo': '>= 0.1.0'\n  };\n\n  var updated = {\n    'foo': '>= 0.2.0',\n    'bar': '>= 0.1.0'\n  };\n\n  var updates = analyzer.updates(current, updated);\n\n  \/\/\n  \/\/ This will return an object literal with the differential\n  \/\/ updates between the two sets of dependencies:\n  \/\/\n  \/\/ {\n  \/\/   added: { 'bar': '>= 0.1.0' },\n  \/\/   updated: { 'foo': '>= 0.2.0' }\n  \/\/ }\n  \/\/\nTests\n  npm test\n\nAuthor: Charlie Robbins\n","502":"walkabout.js\nWalkabout.js is an automatic web application tester.  It randomly\nsimulates a user.\nThis code figures out what your application is paying attention to,\nand does it.  It fills in fields with random values.  It finds the\nevents you listen for and fires those events.  It finds internal links\n(like <a href=\"#foo\">) and clicks them.  It's a little like a fuzz\ntester for your app.\nIt has special support for jQuery, and uses source code rewriting to\ndetect what your application is listening for in the absence of\njQuery.  (jQuery leaves evidence on DOM nodes about what is being\nlistened for.)\nYou can use it like so (if you are using jQuery):\n\/\/ This makes something like $('#some-input').val() return random values:\njQuery.fn.val.patch();\n\n\/\/ Now, fiddle around, do 100 random things:\nWalkabout.runManyActions({\n  times: 100\n});\nYou can also give Walkabout hints about what's a valid input; this\nlets it guess application-progressing values more often.\nYou can add data-walkabout-disable=\"1\" to any element to suppress\nactivation of that element or any of its children.\nYou can use data-walkabout-eventname=\"...\" to set attributes on the\nevent that is created, such as data-walkabout-keyup=\"{which: 13}\"\nYou can use data-walkabout-options=\"['a', 'b']\" to give the valid inputs\nfor a field.\nYou can use data-walkabout-edit-value=\"type paste delete move\" to\nhave Walkabout do edit operations inside a textarea or input type=text.  You give a space-separated list of the things to do: type\na key at the cursor (and fire keyup), paste at the cursor (and fire\npaste), delete the selection or delete before or after the cursor, or\nmove the cursor around.\nBookmarklet\nIf you want to try walkabout.js on some random jQuery site, you can\nuse the bookmarklet.  This will\nload Walkabout and also start a simple UI to start the runs.  In\naddition to starting a run it'll also track any uncaught errors and\nany calls to console.warn() or console.error().\nNon-jQuery Support\nThere's some support for working with code that doesn't use jQuery.\nIt might work with other frameworks (by catching their calls to\naddEventListener), but is more intended to work with code that\ndoesn't use a framework.  (Note: if you know equivalent ways to\ndetect what other frameworks are listening for, patches to Walkabout\nwould be welcome, or issues with details and examples that can be\ntested on.)\nThis technique uses code rewriting to capture calls to\n.addEventListener() and .value.  The code can't be weirdly tricky,\nit needs to actually use those literal names -- which is usually fine\nin \"normal\" code, but might not be in fancy or optimized code.  E.g.,\nel[\"addEventListener\"](...) would not be found, nor would\na=\"addEventListener\";el[a](...)\nYou can use Walkabout.rewriteListeners(code) to rewrite the code.\nThe code transformation looks like this:\ndocument.getElementById(\"el\").addEventListener(\"click\", function () {}, true);\nvar value = document.getElementById(\"textarea\").value;\n\n\/\/ Becomes:\nWalkabout.addEventListener(document.getElementById(\"el\"), \"click\", function () {}, true);\nvar value = Walkabout.value(document.getElementById(\"textarea\"));\nAnd to find actions you use Walkabout.findActions(element).\nSee Proxy Server for an option to use this.\nOptions\nBy default Walkabout will only go to links on the current page (i.e.,\nhref=\"#something\").  This is because it will lose its context and\npotentially not load on the next page.  But if you want Walkabout to\nmove to other pages you can use Walkabout.options.anyLocalLinks = true\nOr set it to something like \"\/dir\/\" which will only follow links\nunder \/dir\/.\nIf you use Walkabout.options.loadPersistent = true then it will save\nthe state in localStorage and continue where it left of when another\npage is loaded.\n\nProxy Server\nAn application could include walkabout.js on its own, and if it\ndoesn't use jQuery it could also run Walkabout.rewriteListeners() on\nall its code.  But maybe you don't feel like doing that, maybe you\nwant to try it out without all that work.  Also, by default Walkabout\nwill try to stay on the current page, but in the proxy mode because it\nknows Walkabout will load on each page, it will freely move about the\nsite.\nThere is a proxy server node-proxy.js that makes this easier.  As\nyou might guess, you have to install Node.js to use it.\nThe server expects to receive requests for the website you want to\nactually access.  To do this you have to edit \/etc\/hosts to point\nthe request locally, e.g.:\n127.0.0.1 site-to-test.com\n\nThen when you access http:\/\/site-to-test.com it will connect\nlocally, and the proxy server in turn will forward the request to the\nactual server.  Any HTML responses will have the Walkabout Javascript\nadded, and Javascript will be rewritten.\nNote that it binds to port 80, and so you must run it as root.  This\nisn't awesome, pull requests to drop root welcome.  A tool like\nauthbind also could help.\nWhen you are done you should definitely stop the server, and undo the\n\/etc\/hosts entry.\nNote many live sites seem to notice the proxy, though I don't know\nhow.  My IP got blocked for a week from news.ycombinator.com after\nusing it in my own testing.  Any ideas welcome.\nYou can control the proxy with environmental variables: $PORT for\nthe port to bind to (default 80), $BIND for the interface to bind to\n(default 127.0.0.1; 0.0.0.0 means all interfaces), and $PORT_ALIASES\nwhich looks like domain1:8088;domain2:8080 which lets you proxy from\none port to another (helpful sometimes when you need to proxy to a\nserver running locally).\nLicense\nThis is available under the\nMozilla Public License or the GPL.\nTo Do\nLots of stuff, of course.  But:\n\n\nSometimes the validation options (like data-walkabout-options)\nshould be obeyed, and sometimes they should be ignored.  Obeying\nthem progresses you through the site, disobeying them does some fuzz\ntesting.\n\n\nNot all form controls get triggered, I think.  E.g., checkboxes\ndon't get checked.\n\n\n.val() should figure out better what's a reasonable return value.\nE.g., a textarea returns multi-line strings, a checkbox returns\ntrue\/false.\n\n\nProbably .val() hacking should just go away, and only do \"real\"\nediting of the fields.\n\n\nThe generator could have smarts about what scripts (i.e., action\nsequences) are successful and which are not.  Starting with a\nsuccessful script (that progresses you through the application), and\nthen tweaking that script and increasing randomness at the end of\nthe script.\n\n\nWe could look at code coverage as a score.  Or we could even just\nlook at event coverage - hidden elements often have events bound,\nbut we know they have to be visible to be triggered.  We'd like to\nexplore paths where they are visible.\n\n\nSomething more Gaussian with the values generated.  Like, sometimes\nyou should make a 100 character input.  But maybe not as often as a\n10 character input.  And sometimes a 1 character input.  The values\nshould be biased towards \"fencepost\" values.  E.g., a bug that isn't\ntriggered when you enter 43 is unlike to be triggered by 42, but\nmore likely with 0, -1, or 1000000.\n\n\ndata-walkabout-options should be treated more as additional\nsuggestions instead of the only suggestions.  They represent valid\ninputs that will keep the application going, but invalid inputs are\nstill sometimes interesting.  Still the given options should be\ngenerally preferred when picking random options.\n\n\nOnce a path has shown itself to get us to \"new\" code (via code\ncoverage or that we see new action options) we might want to explore\nthat specific code path further.  Maybe instead of a single random\nnumber seed, we could have a sequence of seeds: [[30491, 10], 50492] meaning use seed 30491 for 10 numbers, then reseed,\nexploring a different path but with the same start as a\nknown-successful path.\n\n\nA corollary of the above: we should have a signature for a set of\nactions, so we can quickly see if a set is unique.\n\n\nConfiguration should be possible externally, not just on attributes\non the DOM.  This would be fairly simple stuff, like:\nWalkabout.options.elements = {\n  \"#name\": {\n    suggest: \"Bob\"\n  },\n};\n\n\n\nSome way of indicating a bug-detector.  Like, you know in some\ncircumstances some element doesn't show when it should or something.\nThese would be functions that would be run in-between actions.\nSimple assertions might work okay, but some tests are expensive or\ntricky.  But in cases when you don't get an \"error\" but have noticed\nthat invalid things can happen (without an exception) a detector\nmight help create a reproducable case.  This could also be a kind of\n\"pause when\" detector.\n\n\nQuickCheck has this notion of making a test as small as possible --\nbasically seeing if you can get to the same error case with a\nsmaller set of actions.  This could be pretty straight forward for\nWalkabout as well.  Though in cases like \"do something, cancel\nsomething, do something again, do error thing\" it might be tricky:\nwe want to trim the first two actions, but trimming either one alone\nwon't make the path possible.  So we have to look for chunks we can\npull out.  With action signatures it might be easier to determine\nsuch chunks.\n\n\nWith code coverage it would be interesting to see which scripts had\n\"changed\" by seeing if any of the code they touch \"changed\".  Then\nthose scripts could be re-run.\n\n\nMaking \"screencaps\" at various points would be interesting.  These\nwould be a freeze of the visible DOM (BrowserMirror-style).  We\ncould do them only when we've determined a path that is\n\"interesting\" (i.e., maybe shows an error), by rerunning from the\nstart.\n\n\nHaving a \"reset app\" hook would be good, so we can truly rerun.\n\n\nCombining signatures, we could try to determine if the app wasn't\ndeterministic despite our efforts.\n\n\nWe should patch Math.random()\n\n\nWe could consider timeouts an actionable thing.  We could keep our\nown fake internal clock (mocking Date and setTimeout\/setInterval),\nand moving it ahead in a predictable way.  We could simulate\nconditions like a computer going to sleep this way.\n\n\nSimulate pagehide and pageview.\n\n\nSimulate WebAPIs,\nat least the non-deterministic ones.  This involves mocking each one\nout (except event ones, which work kind of like other events).\n\n\nMaybe hook up to\nWebDriver so we\ncan issue events that we aren't generally allowed to issue.\n.dispatchEvent() doesn't work with keyboard events, for instance.\nOTOH, we could also reimplement the event dispatch process.\n\n\nPut in touch events on mobile, and suppress hover events.\n\n\n","503":"Entwine \u2013 Support for Concrete UI style programming in jQuery\nBy Hamish Friedlander, with thanks to SilverStripe\nEntwine tries to provide a new model of code organisation \u2013 a replacement for Object Oriented programming that is\nfocused on adding functions to groups of DOM elements based on the structure and contents of those DOM elements. It\u2019s a\nmerging of the model and view layer that initially seems weird, but can give very powerful results.\nWe\u2019re standing on the shoulders of giants here \u2013 combining ideas from Prototype\u2019s behaviour & lowpro and jQuery\u2019s effen\n& livequery (who themselves stole ideals from Self\u2019s Morphic UI and others), but extending & combining the concepts\npresented in those tools to provide a complete alternative to traditional OO concepts \u2013 self-aware methods, inheritance,\npolymorphisim and namespacing without a single class definition.\nRequirements\nCurrently Entwine layers itself on top of jQuery. Any patch level of jQuery from 1.7 through 1.11 or 2.0 through 2.1 should work.\nBecause we patch some internal jQuery APIs there can be a delay between a new version of jQuery being released\nand Entwine providing support.\nGetting Started\n\nWalk through the Tutorial\nWatch the Screencast (shot during a introductory developer meeting at SilverStripe)\nJoin the Google Group and let us know what you think, or what other features you\u2019d like to see\n\nName change\njQuery Entwine used to be called jQuery Concrete. The name was changed to avoid confusion with another product. The concrete function remains as an alias, but all new code should use entwine\nBasic use\nFirst intro\nTo attach methods to DOM nodes, call the `entwine` function on a jQuery selector object, passing a hash listing the method names and bodys\n\n  $('div').entwine({\n    foo: function(..){..},\n    bar: function(..){..}\n  });\n\nYou can then call those methods on any jQuery object.\n\n  $('#a').foo();\n\nAny elements in the jQuery selection that match the selector used during definition (\u2018div\u2019 in this example) will have foo called with that element\nset as this. Any other objects are skipped. The return value will be the return value of foo() for the last matched DOM object in the set\nA proper example\nGiven this DOM structure:\n\n  <body>\n    <div class=\"internal_text\">Internal text<\/div>\n    <div class=\"attribute_text\" rel=\"Attribute text\"><\/div>\n    <div>Nonsense<\/div>\n  <\/body>\n\nAnd this entwine definition\n\n  $('.internal_text').entwine({\n    foo: function(){ console.log(this.text()); }\n  });\n  $('.attribute_text').entwine({\n    foo: function(){ console.log(this.attr('rel')); }\n  });\n\nThen this call\n\n  $('div').foo();\n\nWill log this to the console\n\n  Internal text\n  Attribute text  \n\nLimitations\nWhen defining methods, the jQuery object that entwine is called on must be a plain selector, without context. These examples will not work\n\n  $('div', el).entwine(...)\n  $([ela, elb, elc]).entwine(...)\n  $('<div id=\"a\"><\/div>').entwine(...)\n\nLive\nThe definitions you provide are not bound to the elements that match at definition time. You can declare behaviour prior to the DOM existing in any\nform (i.e. prior to DOMReady) and later calls will function correctly.\nSelector specifity\nWhen there are two definitions for a particular method on a particular DOM node, the function with the most specific selector is used. \nSpecifity is calculated as defined by the CSS 2\/3 spec. This can be seen as subclassing applied to behaviour.\nAnother example. Given this DOM structure\n\n  <body>\n    <div>Internal text<\/div>\n    <div class=\"attribute_text\" rel=\"Attribute text\"><\/div>\n    <div>Nonsense<\/div>\n  <\/body>\n\nAnd this entwine definition\n\n  $('div').entwine({\n    foo: function(){ console.log(this.text()); }\n  });\n  $('.attribute_text').entwine({\n    foo: function(){ console.log(this.attr('rel')); }\n  });\n\nThen this call\n\n  $('div').foo();\n\nWill log this to the console\n\n  Internal text\n  Attribute text\n  Nonsense\n\nEvents\nIf you declare a function with a name starting with \u2018on\u2019, then instead of defining that function, it will be bound to an event of that\nname. Just like other functions this binding will be live, and only the most specific definition will be used\n\n  <head>\n    <script type='text\/javascript'>\n      \/* No need for onready wrapper. Events are bound as needed *\/\n      $('div').entwine({\n        onclick: function(){ this.css({backgroundColor: 'blue'}); }\n      });\n      $('.green').entwine({\n        onclick: function(){ this.css({color: 'green'}); }\n      });\n    <\/script>\n  <body>\n    <div>Background will turn blue when clicked on<\/div>\n    <div>Will also have blue background when clicked on<\/div>\n    <div class='green'>Will have green text when clicked on. Background color will not change<\/div>\n  <\/body>\n\nConstructors \/ Destructors\nDeclaring a function with the name `onmatch` will create a behavior that is called on each object when it matches. Likewise, `onunmatch` will\nbe called when an object that did match this selector stops matching it (because it is removed, or because you\u2019ve changed its properties).\nNote that an onunmatch block must be paired with an onmatch block \u2013 an onunmatch without an onmatch in the same entwine definition block is illegal\nLike other functions, only the most specific definition will be used. However, because property changes are not atomic, this may not work as you\nexpect.\nNamespaces\nTo avoid name clashes, to allow multiple bindings to the same event, and to generally seperate a set of functions from other code you can use namespaces\n\n  $.entwine('foo.bar', function($){\n    $('div').entwine({\n      baz: function(){}\n    });\n  });\n\nYou can then call these functions like this:\n\n  $('div').entwine('foo.bar').baz()\n\nNamespaced functions work just like regular functions (`this` is still set to a matching DOM Node). However, specifity is calculated per namespace.\nThis is particularly useful for events, because given this:\n\n  $('div').entwine({\n    onclick: function(){ this.css({backgroundColor: 'blue'}); }\n  });\n  \n  $.entwine('foo', function($){\n    $('div').entwine({\n      onclick: function(){ this.css({color: 'green'}); }\n    });\n  });\n\nClicking on a div will change the background and foreground color.\nThis is particularly important when writing reusable code, since otherwise you can\u2019t know before hand whether your event handler will be called or not\nAlthough a namespace can be any string, best practise is to name them with dotted-identifier notation.\nNamespaces and scope (or What the hell\u2019s up with that ugly function closure)\nInside a namespace definition, functions remember the namespace they are in, and calls to other functions will be looked up inside that namespace first. \nWhere they don\u2019t exist, they will be looked up in the base namespace\n\n  $.entwine('foo', function($){\n    $('div').entwine({\n      bar: function() { this.baz(); this.qux(); }\n      baz: function() { console.log('baz'); }\n    })\n  })\n  \n  $('div').entwine({\n    qux: function() { console.log('qux'); }\n  })\n\nWill print baz, qux to the console\nNote that \u2018exists\u2019 means that a function is declared in this namespace for any selector, not just a matching one. Given the dom\n\n  <div>Internal text<\/div>\n\nAnd the entwine definitions\n\n  $.entwine('foo', function($){\n    $('div').entwine({\n      bar: function() { this.baz(); }\n    });\n    $('span').entwine({\n      baz: function() { console.log('a'); }\n    });\n  })\n  \n  \n  $('div').entwine({\n    baz: function() { console.log('b'); }\n  })\n\nThen doing $(\u2018div\u2019).bar(); will not display b. Even though the span rule could never match a div, because baz is defined for some rule in the foo namespace, the base namespace will never be checked\nNesting namespace blocks\nYou can also nest declarations. In this next example, we\u2019re defining the functions $().entwine(\u2018zap\u2019).bar() and $().entwine(\u2018zap.pow\u2019).baz()\n\n  $.entwine('zap', function($){\n    $('div').entwine({\n      bar: function() { .. }\n    })\n    $.entwine('pow', function($){\n      $('div').entwine({\n        baz: function() { .. }\n      })\n    })\n  })\n\nCalling to another namespace (and forcing base)\nInside a namespace, namespace lookups are by default relative to the current namespace.\nIn some situations (such as the last example) you may want to force using the base namespace. In this case you can call entwine with the first argument being the base namespace code \u2018.\u2019. For example, if the first definition in the previous example was\n\n  $.entwine('foo', function($){\n    $('div').entwine({\n      bar: function() { this.entwine('.').baz(); }\n    })\n  })\n\nThen b would be output to the console.\nUsing\nSometimes a block outside of a namespace will need to refer to that namespace repeatedly. By passing a function to the entwine function, you can change the looked-up namespace\n\n  $.entwine('foo', function($){\n    $('div').entwine({\n      bar: function() { console.log('a'); }\n    })\n  })\n  \n  $('div').entwine('foo', function(){\n    this.bar();\n    this.bar();\n    this.bar();\n  });\n\nThis equivalent to \/with\/ in javascript, and just like \/with\/, care should be taken to only use this construct in situations that merit it.\nTests\nSpecs are written using the awesome JSpec library.  You can run them in two ways\nAd-hoc\nOpen the file `spec\/spec.html` in any modern browser\nContinuous testing\nJSpec has a command line client which can be used for continuous testing. Make sure ruby is installed and enabled the Gemcutter gem hosting service, like so:\n\n\tsudo gem install gemcutter\n    sudo gem tumble\n\nThen install the jspec binary:\n\n    sudo gem install jspec\n\nThe JSpec command line tool should now be installed. This command will re-run the specs whenever you edit a file:\n\n    jspec spec\/spec.html -p src,spec\n\nLicense\nCopyright \u00a9 2009 Hamish Friedlander (hamish@silverstripe.com) and SilverStripe Limited (www.silverstripe.com). All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\nNeither the name of Hamish Friedlander nor SilverStripe nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","504":"jquery.resizeend\n\n\n\n\n\n\nA jQuery plugin that allows for window resize-end event handling.\nInstallation\nWith yarn\nyarn add jquery.resizeend\nWith npm\nnpm install jquery.resizeend\nIn the browser\nReference your local script:\n<script src=\"node_modules\/jquery.resizeend\/lib\/jquery.resizeend.min.js\"><\/script>\nOr load the script via jsdelivr:\n<script src=\"https:\/\/cdn.jsdelivr.net\/npm\/jquery.resizeend@latest\/lib\/jquery.resizeend.min.js\"><\/script>\nUsage\n$(window).on('resizeend', function(e) {\n  \/\/ ...\n});\nContributing\nFork the repo and clone locally, then run:\nyarn install\nThis will install the devDependencies packages and build the dist folder.\nOnce you've made your desired changes, make sure to write any new tests for\nyour feature and run the tests:\nyarn run lint # lints js\n\nyarn test     # runs test suite\nIf all tests pass, create a pull request.\nLicense\nThis plugin is licensed under the MIT license.\n","505":"redux-await\n\n\n\n\nManage async redux actions sanely\nBreaking Changes!!\nredux-await now takes control of a branch of your state\/reducer tree similar to redux-form, and also like redux-form you need to use this module's version of connect and not react-redux's\nInstall\nnpm install --save redux-await\nUsage\nThis module exposes a middleware, reducer, and connector to take care of async state in a redux\napp. You'll need to:\n\n\nApply the middleware:\nimport { middleware as awaitMiddleware } from 'redux-await';\nlet createStoreWithMiddleware = applyMiddleware(\n  awaitMiddleware\n)(createStore);\n\n\nInstall the reducer into the await path of your combineReducers\nimport reducers from '.\/reducers';\n\n\/\/ old code\n\/\/ const store = applyMiddleware(thunk)(createStore)(reducers);\n\n\/\/ new code\nimport { reducer as awaitReducer } from 'redux-await';\nconst store = applyMiddleware(thunk, awaitMiddleware)(createStore)({\n  ...reducers,\n  await: awaitReducer,\n});\n\n\nUse the connect function from this module and not react-redux's\n\/\/ old code\n\/\/ import { connect } from 'react-redux';\n\n\/\/ new code\nimport { connect } from 'redux-await';\n\nclass FooPage extends Component {\n  render() { \/* ... *\/ }\n}\n\nexport default connect(state => state.foo)(FooPage)\n\n\nNow your action payloads can contain promises, you just need to add AWAIT_MARKER to the\naction like this:\n\/\/ old code\n\/\/export const getTodos = () => ({\n\/\/  type: GET_TODOS,\n\/\/  payload: {\n\/\/    loadedTodos: localStorage.todos,\n\/\/  },\n\/\/});\n\/\/export const addTodo = todo => ({\n\/\/  type: ADD_TODO,\n\/\/  payload: {\n\/\/    savedTodo: todo,\n\/\/  },\n\/\/});\n\n\/\/ new code\nimport { AWAIT_MARKER } from 'redux-await';\nexport const getTodos = () => ({\n  type: GET_TODOS,\n  AWAIT_MARKER,\n  payload: {\n    loadedTodos: api.getTodos(), \/\/ returns promise\n  },\n});\nexport const addTodo = todo => ({\n  type: ADD_TODO,\n  AWAIT_MARKER,\n  payload: {\n    savedTodo: api.saveTodo(todo), \/\/ returns promise\n  },\n});\nNow your containers barely need to change:\nclass Container extends Component {\n  render() {\n    const { todos, statuses, errors } = this.props;\n\n    \/\/ old code\n    \/\/return <div>\n    \/\/  <MyList data={todos} \/>\n    \/\/<\/div>;\n\n    \/\/ new code\n    return <div>\n      { statuses.loadedTodos === 'pending' && <div>Loading...<\/div> }\n      { statuses.loadedTodos === 'success' && <MyList data={loadedTodos} \/> }\n      { statuses.loadedTodos.status === 'failure' && <div>Oops: {errors.loadedTodos.message}<\/div> }\n      { statuses.savedTodo === 'pending' && <div>Saving new savedTodo<\/div> }\n      { statuses.savedTodo === 'failure' && <div>There was an error saving<\/div> }\n    <\/div>;\n  }\n}\n\n\/\/old code\n\/\/ import { connect } from 'react-redux';\n\n\/\/ new code\nimport { connect } from 'redux-await'; \/\/ it just spreads state.await on props\n\nexport default connect(state => state.todos)(Container)\nWhy\nRedux is mostly concerned about how to manage state in a synchronous setting. Async apps create\nchallenges like keeping track of the async status and dealing with async errors.\nWhile it is possible to build an app this way using\nredux-thunk\nand\/or\nredux-promise\nit tends to bloat the app and it makes unit testing needlessly verbose\nredux-await tries to solve all of these problems by keeping track of async payloads by means\nof a middleware and a reducer keeping track of payload properties statuses. Let's walk\nthrough the development of a TODO app (App 1) that starts without any async and then needs to\nstart converting action from sync to async. We'll first try only using redux-thunk to solve\nthis (App 2), and then see how to solve this with redux-await (App 3)\nFor the first version of the app we're going to store the todos in localStorage. Here's a simple way we would do it:\nApp1 demo\nApp 1\nimport React, { Component } from 'react';\nimport ReactDOM from 'react-dom';\nimport { Provider, connect } from 'react-redux';\nimport { applyMiddleware, createStore, combineReducers } from 'redux';\nimport thunk from 'redux-thunk';\nimport createLogger from 'redux-logger';\n\nconst GET_TODOS = 'GET_TODOS';\nconst ADD_TODO = 'ADD_TODO';\nconst SAVE_APP = 'SAVE_APP';\nconst actions = {\n  getTodos() {\n    const todos = JSON.parse(localStorage.todos || '[]');\n    return { type: GET_TODOS, payload: { todos } };\n  },\n  addTodo(todo) {\n    return { type: ADD_TODO, payload: { todo } };\n  },\n  saveApp() {\n    return (dispatch, getState) => {\n      localStorage.todos = JSON.stringify(getState().todos.todos);\n      dispatch({ type: SAVE_APP });\n    }\n  },\n};\nconst initialState = { isAppSynced: false, todos: [] };\nconst todosReducer = (state = initialState, action = {}) => {\n  if (action.type === GET_TODOS) {\n    return { ...state, isAppSynced: true, todos: action.payload.todos };\n  }\n  if (action.type === ADD_TODO) {\n    return { ...state, isAppSynced: false, todos: state.todos.concat(action.payload.todo) };\n  }\n  if (action.type === SAVE_APP) {\n    return { ...state, isAppSynced: true };\n  }\n  return state;\n};\nconst reducer = combineReducers({\n  todos: todosReducer,\n})\nconst store = applyMiddleware(thunk, createLogger())(createStore)(reducer);\n\nclass App extends Component {\n  componentDidMount() {\n    this.props.dispatch(actions.getTodos());\n  }\n  render() {\n    const { dispatch, todos, isAppSynced } = this.props;\n    const { input } = this.refs;\n    return <div>\n      {isAppSynced && 'app is synced up'}\n      <ul>{todos.map(todo => <li>{todo}<\/li>)}<\/ul>\n      <input ref=\"input\" type=\"text\" onBlur={() => dispatch(actions.addTodo(input.value))} \/>\n      <button onClick={() => dispatch(actions.saveApp())}>Sync<\/button>\n      <br \/>\n      <pre>{JSON.stringify(store.getState(), null, 2)}<\/pre>\n    <\/div>;\n  }\n}\nconst ConnectedApp = connect(state => state.todos)(App);\n\nReactDOM.render(<Provider store={store}><ConnectedApp \/><\/Provider>, document.getElementById('root'));\nLooks cool (it's a POC so it's purposely minimal), but let's say you want to start using an API\nwhich is async to store the state, now your app will look something like App 2:\nApp2 demo\nApp 2\nimport React, { Component } from 'react';\nimport ReactDOM from 'react-dom';\nimport { Provider, connect } from 'react-redux';\nimport { applyMiddleware, createStore, combineReducers } from 'redux';\nimport thunk from 'redux-thunk';\nimport createLogger from 'redux-logger';\n\n\/\/ this not an API, this is a tribute\nconst api = {\n  save(data) {\n    return new Promise(resolve => {\n      setTimeout(() => {\n        localStorage.todos = JSON.stringify(data);\n        resolve(true);\n      }, 2000);\n    });\n  },\n  get() {\n    return new Promise(resolve => {\n      setTimeout(() => {\n        resolve(JSON.parse(localStorage.todos || '[]'));\n      }, 1000);\n    });\n  }\n}\n\nconst GET_TODOS_PENDING = 'GET_TODOS_PENDING';\nconst GET_TODOS = 'GET_TODOS';\nconst GET_TODOS_ERROR = 'GET_TODOS_ERROR';\nconst ADD_TODO = 'ADD_TODO';\nconst SAVE_APP_PENDING = 'SAVE_APP_PENDING'\nconst SAVE_APP = 'SAVE_APP';\nconst SAVE_APP_ERROR = 'SAVE_APP_ERROR';\nconst actions = {\n  getTodos() {\n    return dispatch => {\n      dispatch({ type: GET_TODOS_PENDING });\n      api.get()\n        .then(todos => dispatch({ type: GET_TODOS, payload: { todos } }))\n        .catch(error => dispatch({ type: GET_TODOS_ERROR, payload: error, error: true }))\n      ;\n      ;\n    }\n  },\n  addTodo(todo) {\n    return { type: ADD_TODO, payload: { todo } };\n  },\n  saveApp() {\n    return (dispatch, getState) => {\n      dispatch({ type: SAVE_APP_PENDING });\n      api.save(getState().todos.todos)\n        .then(() => dispatch({ type: SAVE_APP }))\n        .catch(error => dispatch({ type: SAVE_APP_ERROR, payload: error, error: true }))\n      ;\n    }\n  },\n};\nconst initialState = {\n  isAppSynced: false,\n  isFetching: false,\n  fetchingError: null,\n  isSaving: false,\n  savingError: null,\n  todos: [],\n};\nconst todosReducer = (state = initialState, action = {}) => {\n  if (action.type === GET_TODOS_PENDING) {\n    return { ...state, isFetching: true, fetchingError: null };\n  }\n  if (action.type === GET_TODOS) {\n    return {\n      ...state,\n      isAppSynced: true,\n      isFetching: false,\n      fetchingError: null,\n      todos: action.payload.todos,\n    };\n  }\n  if (action.type === GET_TODOS_ERROR) {\n    return { ...state, isFetching: false, fetchingError: action.payload.message };\n  }\n  if (action.type === ADD_TODO) {\n    return { ...state, isAppSynced: false, todos: state.todos.concat(action.payload.todo) };\n  }\n  if (action.type === SAVE_APP_PENDING) {\n    return { ...state, isSaving: true, savingError: null };\n  }\n  if (action.type === SAVE_APP) {\n    return { ...state, isAppSynced: true, isSaving: false, savingError: null };\n  }\n  if (action === SAVE_APP_ERROR) {\n    return { ...state, isSaving: false, savingError: action.payload.message }\n  }\n  return state;\n};\nconst reducer = combineReducers({\n  todos: todosReducer,\n})\nconst store = applyMiddleware(thunk, createLogger())(createStore)(reducer);\n\nclass App extends Component {\n  componentDidMount() {\n    this.props.dispatch(actions.getTodos());\n  }\n  render() {\n    const { dispatch, todos, isAppSynced, isFetching, fetchingError, isSaving, savingError } = this.props;\n    const { input } = this.refs;\n    return <div>\n      {isAppSynced && 'app is synced up'}\n      {isFetching && 'getting todos'}\n      {fetchingError && 'there was an error getting todos: ' + fetchingError}\n      {isSaving && 'saving todos'}\n      {savingError && 'there was an error saving todos: ' + savingError}\n      <ul>{todos.map(todo => <li>{todo}<\/li>)}<\/ul>\n      <input ref=\"input\" type=\"text\" onBlur={() => dispatch(actions.addTodo(input.value))} \/>\n      <button onClick={() => dispatch(actions.saveApp())}>Sync<\/button>\n      <br \/>\n      <pre>{JSON.stringify(store.getState(), null, 2)}<\/pre>\n    <\/div>;\n  }\n}\n\nconst ConnectedApp = connect(state => state.todos)(App);\n\nReactDOM.render(<Provider store={store}><ConnectedApp \/><\/Provider>, document.getElementById('root'));\nAs you can see there's a lot of async logic and state we don't want to have to deal with.\nThis is 62 more LOC than the first version. Here's how you would do it in App 3 with\nredux-await:\nApp3 demo\nApp 3\nimport React, { Component } from 'react';\nimport ReactDOM from 'react-dom';\nimport { Provider } from 'react-redux';\nimport { applyMiddleware, createStore, combineReducers } from 'redux';\nimport thunk from 'redux-thunk';\nimport createLogger from 'redux-logger';\nimport {\n  AWAIT_MARKER,\n  createReducer,\n  connect,\n  reducer as awaitReducer,\n  middleware as awaitMiddleware,\n} from 'redux-await';\n\n\/\/ this not an API, this is a tribute\nconst api = {\n  save(data) {\n    return new Promise(resolve => {\n      setTimeout(() => {\n        localStorage.todos = JSON.stringify(data);\n        resolve(true);\n      }, 2000);\n    });\n  },\n  get() {\n    return new Promise(resolve => {\n      setTimeout(() => {\n        resolve(JSON.parse(localStorage.todos || '[]'));\n      }, 1000);\n    });\n  }\n}\n\nconst GET_TODOS = 'GET_TODOS';\nconst ADD_TODO = 'ADD_TODO';\nconst SAVE_APP = 'SAVE_APP';\nconst actions = {\n  getTodos() {\n    return {\n      type: GET_TODOS,\n      AWAIT_MARKER,\n      payload: {\n        todos: api.get(),\n      },\n    };\n  },\n  addTodo(todo) {\n    return { type: ADD_TODO, payload: { todo } };\n  },\n  saveApp() {\n    return (dispatch, getState) => {\n      dispatch({\n        type: SAVE_APP,\n        AWAIT_MARKER,\n        payload: {\n          save: api.save(getState().todos.todos),\n        },\n      });\n    }\n  },\n};\nconst initialState = { isAppSynced: false, todos: [] };\nconst todosReducer = (state = initialState, action = {}) => {\n  if (action.type === GET_TODOS) {\n    return { ...state, isAppSynced: true, todos: action.payload.todos };\n  }\n  if (action.type === ADD_TODO) {\n    return { ...state, isAppSynced: false, todos: state.todos.concat(action.payload.todo) };\n  }\n  if (action.type === SAVE_APP) {\n    return { ...state, isAppSynced: true };\n  }\n  return state;\n};\nconst reducer = combineReducers({\n  todos: todosReducer,\n  await: awaitReducer,\n})\n\nconst store = applyMiddleware(thunk, awaitMiddleware, createLogger())(createStore)(reducer);\n\nclass App extends Component {\n  componentDidMount() {\n    this.props.dispatch(actions.getTodos());\n  }\n  render() {\n    const { dispatch, todos, isAppSynced, statuses, errors } = this.props;\n    const { input } = this.refs;\n    return <div>\n      {isAppSynced && 'app is synced up'}\n      {statuses.todos === 'pending' && 'getting todos'}\n      {statuses.todos === 'failure' && 'there was an error getting todos: ' + errors.todos.message}\n      {statuses.save === 'pending' && 'saving todos'}\n      {errors.save && 'there was an error saving todos: ' + errors.save.message}\n      <ul>{todos.map(todo => <li>{todo}<\/li>)}<\/ul>\n      <input ref=\"input\" type=\"text\" onBlur={() => dispatch(actions.addTodo(input.value))} \/>\n      <button onClick={() => dispatch(actions.saveApp())}>Sync<\/button>\n      <br \/>\n      <pre>{JSON.stringify(store.getState(), null, 2)}<\/pre>\n    <\/div>;\n  }\n}\n\n\nconst ConnectedApp = connect(state => state.todos)(App);\n\nReactDOM.render(<Provider store={store}><ConnectedApp \/><\/Provider>, document.getElementById('root'));\nThis version is very easy to reason about, in fact you can completely ignore the fact that the app is async at all. The todosReducer didn't need to have a single line changed!\nNote that this is 107 LOC compared to app2's 125 LOC\nSome pitfalls to watch out for\nYou must either use this modules connect or manually spread the await part of the tree over\nmapStateToProps, you can also choose to name it something other than await and spread that\nyourself too.\nredux-await will name the statuses and errors prop the same as the payload prop so try to be\nas descriptive as possible when naming payload props since any payload props collision will\noverwrite the statuses\/errors value. For a CRUD app don't always name it something like\nrecords because when you're loading users.records the app will also think you're loading\ntodos.records\nHow it works:\nThe middleware checks to see if the AWAIT_MARKER was set on the action\nand if it was then dispatches three events with a [AWAIT_META_CONTAINER]\nproperty on the meta property of the action.\nThe reducer listens for actions with a meta of [AWAIT_META_CONTAINER] and\nwhen found will set the await property of the state accordingly.\n","506":"Conductor.js\n\nConductor.js is a library for creating sandboxed, re-usable apps that\ncan be embedded inside a host application.\nThe advantage of using Conductor.js over a standard <iframe> is that\nit uses a well-defined set of events that allow the app and its host\nenvironment to communicate. Because of this, apps from different vendors\ncan be embedded securely, yet still interact in meaningful ways.\nUnderstanding Conductor.js\nTo understand the benefit of architecting your application using\nConductor, let's look at an example use case.\nImagine that you are authoring a next-generation banking web\napplication. When your customers visit their account online, you would\nlike to present them with a list of their most recent transactions:\n\nThis is fine, but is the same as every other bank. You, being a\nnext-generation, technology-driven bank, want to make this data more\nmeaningful to your customers. Most transactions have some metadata\nassociated with them. What if you could allow merchants to customize how\ntheir transactions appeared?\n\nObviously, you could write custom code for each merchant that might have\na transaction go through your bank. But there are many merchants in the\nworld, and you would most likely only have the resources to customize\nthe most popular.\nThe better option is to have each merchant write and maintain their own\ntransaction card. But how do you run that code in a way that doesn't\nmake your customers' private financial data vulnerable to attack?\nRunning the code directly is out of the question. Normally, you might\njust create an <iframe> for each item in the transaction. That's fine,\nbut how do provide information about the transaction to each transaction\ncard? And what if you want to get information out of the card?\nFor example, imagine we wanted to group transactions by type. Instead of\nguessing, we could ask each transaction what category it belongs to,\nthen show the user only travel-related transactions, for example.\nConductor.js allows you to define interfaces between a host environment\nand the cards that run inside of it, without having to write messy and\nbrittle transports over postMessage.\nCards are also designed to be run on the server, so they can be indexed\nand otherwise manipulated without having to create a memory- and\nCPU-intensive virtual browser.\nAnd, because cards are run inside either iframes or Web Workers, they\ncannot get access to any data that is not explicitly provided.\nThe above example is just one use for Conductor.js. There are many\ndifferent scenarios where being able to embed secure, third-party code\ncan allow you to build a platform of re-usable components that can\ninteract with their host environment.\nAuthoring Conductor.js Cards\nA card is an application that can be embedded in a parent environment\nusing Conductor.js. The entry point to a card is an HTML page that loads\nConductor.js.\nAt its most basic, your card should call the Conductor.card method.\nThe object passed to Conductor.card defines the behavior of the card,\nand contains callbacks and other hooks that will be invoked in response\nto requests from the containing environment.\nFor example, the activate hook is invoked automatically once all\ndependencies have finished loading, and communication with the parent\nenvironment has been set up:\nConductor.card({\n  activate: function() {\n    this.alert(\"Hello!\")\n  }\n});\nLoading Dependencies\nA card can load additional javascript dependencies by using the\nConductor.require method:\nConductor.require('alert.js');\nNote that files loaded via Conductor.require are loaded\nasynchronously. That means that, assuming the Model class was\ndefined in models.js, this would not work:\nConductor.require('models.js');\nvar person = new Model();\nMake sure you only access dependencies once the card's activate method\nhas been called:\nConductor.require('alert.js');\n\nfunction createModel() {\n  var person = new Model();\n}\n\nConductor.card({\n  activate: function() {\n    createModel();\n  }\n});\nYou can include CSS files in your card using the Conductor.requireCSS method:\nConductor.requireCSS('card.css');\nThe path is relative to the card.\nTutorial\nLearn more about authoring cards in the tutorial.\nBuild Tools\nConductor.js uses Grunt to automate building and\ntesting.\nSetup\nBefore you use any of the commands below, make sure you have\ninstalled node.js, which includes npm, the node package manager.\nIf you haven't before, install the grunt CLI tool:\n$ npm install -g grunt-cli\nThis will put the grunt command in your system path, allowing it to be\nrun from any directory.\nNext, install Conductor's dependencies:\n$ npm install\nThis will install all of the packages that Conductor's Gruntfile relies\non into the local node_modules directory.\nBuilding\nConductor is available as either as an AMD module, or as a more\ntraditional distribution that exports the global variable Conductor.\nTo build both versions, run:\ngrunt build\n\nYou can find the built versions of the library in the dist directory.\nTests\nRun the Conductor tests by starting a test server:\ngrunt server\n\nOnce the server is running, visit http:\/\/localhost:8000 in your\nbrowser. Conductor will automatically be rebuilt if you make any changes\nto its constituent files while the server is running.\n","507":"\n\n\n\nStoring settings in the database\nNot all settings belong in settings.py, as it has some particular\nlimitations:\n\n\nSettings are project-wide. This not only requires apps to clutter up\nsettings.py, but also increases the chances of naming conflicts.\nSettings are constant throughout an instance of Django. They cannot be\nchanged without restarting the application.\nSettings require a programmer in order to be changed. This is true even\nif the setting has no functional impact on anything else.\n\n\nMany applications find need to overcome these limitations, and dbsettings\nprovides a convenient way to do so.\nThe main goal in using this application is to define a set of placeholders that\nwill be used to represent the settings that are stored in the database. Then,\nthe settings may be edited at run-time using the provided editor, and all Python\ncode in your application that uses the setting will receive the updated value.\n\nRequirements\n\n\nDbsettings\nPython\nDjango\n\n\n\n==0.10\n3.4 - 3.5\n1.7 - 1.10\n\n3.2 - 3.3\n1.7 - 1.8\n\n2.7\n1.7 - 1.10\n\n==0.9\n3.4 - 3.5\n1.7 - 1.9\n\n3.2 - 3.3\n1.7 - 1.8\n\n2.7\n1.7 - 1.9\n\n==0.8\n3.2\n1.5 - 1.8\n\n2.7\n1.4 - 1.8\n\n2.6\n1.4 - 1.6\n\n==0.7\n3.2\n1.5 - 1.7\n\n2.7\n1.3 - 1.7\n\n2.6\n1.3 - 1.6\n\n==0.6\n3.2\n1.5\n\n2.6 - 2.7\n1.3 - 1.5\n\n<=0.5\n2.6 - 2.7\n1.2* - 1.4\n\n\n\n* Possibly version below 1.2 will work too, but not tested.\n\nInstallation\nTo install the dbsettings package, simply place it anywhere on your\nPYTHONPATH.\n\nProject settings\nIn order to setup database storage, and to let Django know about your use of\ndbsettings, simply add it to your INSTALLED_APPS setting, like so:\nINSTALLED_APPS = (\n    ...\n    'dbsettings',\n    ...\n)\n\nIf your Django project utilizes sites framework, all setting would be related\nto some site. If sites are not present, settings won't be connected to any site\n(and sites framework is no longer required since 0.8.1).\nYou can force to do (not) use sites via DBSETTINGS_USE_SITES = True \/ False\nconfiguration variable (put it in project's settings.py).\nBy default, values stored in database are limited to 255 characters per setting.\nYou can change this limit with DBSETTINGS_VALUE_LENGTH configuration variable.\nIf you change this value after migrations were run, you need to manually alter\nthe dbsettings_setting table schema.\n\nURL Configuration\nIn order to edit your settings at run-time, you'll need to configure a URL to\naccess the provided editors. You'll just need to add a single line, defining\nthe base URL for the editors, as dbsettings has its own URLconf to handle\nthe rest. You may choose any location you like:\nurlpatterns = patterns('',\n    ...\n    (r'^settings\/', include('dbsettings.urls')),\n    ...\n)\n\n\nA note about caching\nThis framework utilizes Django's built-in cache framework, which is used to\nminimize how often the database needs to be accessed. During development,\nDjango's built-in server runs in a single process, so all cache backends will\nwork just fine.\nMost productions environments, including mod_python, FastCGI or WSGI, run multiple\nprocesses, which some backends don't fully support. When using the simple\nor locmem backends, updates to your settings won't be reflected immediately\nin all workers, causing your application to ignore the new changes.\nNo other backends exhibit this behavior, but since simple is the default,\nmake sure to specify a proper backend when moving to a production environment.\nAlternatively you can disable caching of settings by setting\nDBSETTINGS_USE_CACHE = False in settings.py. Beware though: every\naccess of any setting will result in database hit.\n\nUsage\nThese database-backed settings can be applied to any model in any app, or even\nin the app itself. All the tools necessary to do so are available within the\ndbsettings module. A single import provides everything you'll need:\nimport dbsettings\n\n\nDefining a group of settings\nSettings are be defined in groups that allow them to be referenced together\nunder a single attribute. Defining a group uses a declarative syntax similar\nto that of models, by declaring a new subclass of the Group class and\npopulating it with values.\nclass ImageLimits(dbsettings.Group):\n    maximum_width = dbsettings.PositiveIntegerValue()\n    maximum_height = dbsettings.PositiveIntegerValue()\n\nYou may name your groups anything you like, and they may be defined in any\nmodule. This allows them to be imported from common applications if applicable.\n\nDefining individual settings\nWithin your groups, you may define any number of individual settings by simply\nassigning the value types to appropriate names. The names you assign them to\nwill be the attribute names you'll use to reference the setting later, so be\nsure to choose names accordingly.\nFor the editor, the default description of each setting will be retrieved from\nthe attribute name, similar to how the verbose_name of model fields is\nretrieved. Also like model fields, however, an optional argument may be provided\nto define a more fitting description. It's recommended to leave the first letter\nlower-case, as it will be capitalized as necessary, automatically.\nclass EmailOptions(dbsettings.Group):\n    enabled = dbsettings.BooleanValue('whether to send emails or not')\n    sender = dbsettings.StringValue('address to send emails from')\n    subject = dbsettings.StringValue(default='SiteMail')\n\nFor more descriptive explanation, the help_text argument can be used. It\nwill be shown in the editor.\nThe default argument is very useful - it specify an initial value of the\nsetting.\nIn addition, settings may be supplied with a list of available options, through\nthe use of of the choices argument. This works exactly like the choices\nargument for model fields, and that of the newforms ChoiceField.\nThe widget used for a value can be overriden using the widget keyword. For example:\npayment_instructions = dbsettings.StringValue(\n    help_text=\"Printed on every invoice.\",\n    default=\"Payment to Example XYZ\\nBank name here\\nAccount: 0123456\\nSort: 01-02-03\",\n    widget=forms.Textarea\n)\n\nA full list of value types is available later in this document, but the process\nand arguments are the same for each.\n\nAssigning settings\nOnce your settings are defined and grouped properly, they must be assigned to a\nlocation where they will be referenced later. This is as simple as instantiating\nthe settings group in the appropriate location. This may be at the module level\nor within any standard Django model.\nGroup instance may receive one optional argument: verbose name of the group.\nThis name will be displayed in the editor.\nemail = EmailOptions()\n\nclass Image(models.Model):\n    image = models.ImageField(upload_to='\/upload\/path')\n    caption = models.TextField()\n\n    limits = ImageLimits('Dimension settings')\n\nMultiple groups may be assigned to the same module or model, and they can even\nbe combined into a single group by using standard addition syntax:\noptions = EmailOptions() + ImageLimits()\n\nTo separate and tag settings nicely in the editor, use verbose names:\noptions = EmailOptions('Email') + ImageLimits('Dimesions')\n\n\nDatabase setup\nA single model is provided for database storage, and this model must be\ninstalled in your database before you can use the included editors or the\npermissions that will be automatically created. This is a simple matter of\nrunning manage.py syncdb or manage.py migrate now that your settings\nare configured.\nThis step need only be repeate when settings are added to a new application,\nas it will create the appropriate permissions. Once those are in place, new\nsettings may be added to existing applications with no impact on the database.\n\nUsing your settings\nOnce the above steps are completed, you're ready to make use of database-backed\nsettings.\n\nEditing settings\nWhen first defined, your settings will default to None (or False in\nthe case of BooleanValue), so their values must be set using one of the\nsupplied editors before they can be considered useful (however, if the setting\nhad the default argument passed in the constructor, its value is already\nuseful - equal to the defined default).\nThe editor will be available at the URL configured earlier.\nFor example, if you used the prefix of 'settings\/', the URL \/settings\/\nwill provide an editor of all available settings, while \/settings\/myapp\/\nwould contain a list of just the settings for myapp.\nURL patterns are named: 'site_settings' and 'app_settings', respectively.\nThe editors are restricted to staff members, and the particular settings that\nwill be available to users is based on permissions that are set for them. This\nmeans that superusers will automatically be able to edit all settings, while\nother staff members will need to have permissions set explicitly.\n\nAccessing settings in Python\nOnce settings have been assigned to an appropriate location, they may be\nreferenced as standard Python attributes. The group becomes an attribute of the\nlocation where it was assigned, and the individual values are attributes of the\ngroup.\nIf any settings are referenced without being set to a particular value, they\nwill default to None (or False in the case of BooleanValue, or\nwhatever was passed as default). In the\nfollowing example, assume that EmailOptions were just added to the project\nand the ImageLimits were added earlier and already set via editor.\n>>> from myproject.myapp import models\n\n# EmailOptions are not defined\n>>> models.email.enabled\nFalse\n>>> models.email.sender\n>>> models.email.subject\n'SiteMail'  # Since default was defined\n\n# ImageLimits are defined\n>>> models.Image.limits.maximum_width\n1024\n>>> models.Image.limits.maximum_height\n768\n\nThese settings are accessible from any Python code, making them especially\nuseful in model methods and views. Each time the attribute is accessed, it will\nretrieve the current value, so your code doesn't need to worry about what\nhappens behind the scenes.\ndef is_valid(self):\n    if self.width > Image.limits.maximum_width:\n        return False\n    if self.height > Image.limits.maximum_height:\n        return False\nreturn True\n\nAs mentioned, views can make use of these settings as well.\nfrom myproject.myapp.models import email\n\ndef submit(request):\n\n    ...\n    # Deal with a form submission\n    ...\n\n    if email.enabled:\n        from django.core.mail import send_mail\n    send_mail(email.subject, 'message', email.sender, [request.user.email])\n\nSettings can be not only read, but also written. The admin editor is more\nuser-friendly, but in case code need to change something:\nfrom myproject.myapp.models import Image\n\ndef low_disk_space():\n    Image.limits.maximum_width = Image.limits.maximum_height = 200\n\nEvery write is immediately commited to the database and proper cache key is deleted.\n\nA note about model instances\nSince settings aren't related to individual model instances, any settings that\nare set on models may only be accessed by the model class itself. Attempting to\naccess settings on an instance will raise an AttributeError.\n\nValue types\nThere are several various value types available for database-backed settings.\nSelect the one most appropriate for each individual setting, but all types use\nthe same set of arguments.\n\nBooleanValue\nPresents a checkbox in the editor, and returns True or False in Python.\n\nDurationValue\nPresents a set of inputs suitable for specifying a length of time. This is\nrepresented in Python as a timedelta object.\n\nFloatValue\nPresents a standard input field, which becomes a float in Python.\n\nIntegerValue\nPresents a standard input field, which becomes an int in Python.\n\nPercentValue\nSimilar to IntegerValue, but with a limit requiring that the value be\nbetween 0 and 100. In addition, when accessed in Python, the value will be\ndivided by 100, so that it is immediately suitable for calculations.\nFor instance, if a myapp.taxes.sales_tax was set to 5 in the editor,\nthe following calculation would be valid:\n>>> 5.00 * myapp.taxes.sales_tax\n0.25\n\n\nPositiveIntegerValue\nSimilar to IntegerValue, but limited to positive values and 0.\n\nStringValue\nPresents a standard input, accepting any text string up to 255\n(or DBSETTINGS_VALUE_LENGTH) characters. In\nPython, the value is accessed as a standard string.\n\nDateTimeValue\nPresents a standard input field, which becomes a datetime in Python.\nUser input will be parsed according to DATETIME_INPUT_FORMATS setting.\nIn code, one can assign to field string or datetime object:\n# These two statements has the same effect\nmyapp.Feed.next_feed = '2012-06-01 00:00:00'\nmyapp.Feed.next_feed = datetime.datetime(2012, 6, 1, 0, 0, 0)\n\n\nDateValue\nPresents a standard input field, which becomes a date in Python.\nUser input will be parsed according to DATE_INPUT_FORMATS setting.\nSee DateTimeValue for the remark about assigning.\n\nTimeValue\nPresents a standard input field, which becomes a time in Python.\nUser input will be parsed according to TIME_INPUT_FORMATS setting.\nSee DateTimeValue for the remark about assigning.\n\nImageValue\n(requires PIL or Pillow imaging library to work)\nAllows to upload image and view its preview.\nImageValue has optional upload_to keyword, which specify path\n(relative to MEDIA_ROOT), where uploaded images will be stored.\nIf keyword is not present, files will be saved directly under\nMEDIA_ROOT.\n\nPasswordValue\nPresents a standard password input. Retain old setting value if not changed.\n\nSetting defaults for a distributed application\nDistributed applications often have need for certain default settings that are\nuseful for the common case, but which may be changed to suit individual\ninstallations. For such cases, a utility is provided to enable applications to\nset any applicable defaults.\nLiving at dbsettings.utils.set_defaults, this utility is designed to be used\nwithin the app's management.py. This way, when the application is installed\nusing syncdb\/migrate, the default settings will also be installed to the database.\nThe function requires a single positional argument, which is the models\nmodule for the application. Any additional arguments must represent the actual\nsettings that will be installed. Each argument is a 3-tuple, of the following\nformat: (class_name, setting_name, value).\nIf the value is intended for a module-level setting, simply set class_name\nto an empty string. The value for setting_name should be the name given to\nthe setting itself, while the name assigned to the group isn't supplied, as it\nisn't used for storing the value.\nFor example, the following code in management.py would set defaults for\nsome of the settings provided earlier in this document:\nfrom django.conf import settings\nfrom dbsettings.utils import set_defaults\nfrom myproject.myapp import models as myapp\n\nset_defaults(myapp,\n    ('', 'enabled', True)\n    ('', 'sender', settings.ADMINS[0][1]) # Email of the first listed admin\n    ('Image', 'maximum_width', 800)\n    ('Image', 'maximum_height', 600)\n)\n\n\n\nChangelog\n\n0.10.0 (25\/09\/2016)\n\nAdded compatibility with Django 1.10\n\n\n0.9.3 (02\/06\/2016)\n\nFixed (hopefully for good) problem with ImageValue in Python 3 (thanks rolexCoder)\n\n\n0.9.2 (01\/05\/2016)\n\nFixed bug when saving non-required settings\nFixed problem with ImageValue in Python 3 (thanks rolexCoder)\n\n\n0.9.1 (10\/01\/2016)\n\nFixed Sites app being optional (thanks rolexCoder)\n\n\n0.9.0 (25\/12\/2015)\n\nAdded compatibility with Django 1.9 (thanks Alonso)\nDropped compatibility with Django 1.4, 1.5, 1.6\n\n\n0.8.2 (17\/09\/2015)\n\nAdded migrations to distro\nAdd configuration option to change max length of setting values from 255 to whatever\nAdd configuration option to disable caching (thanks nwaxiomatic)\nFixed PercentValue rendering (thanks last-partizan)\n\n\n0.8.1 (21\/06\/2015)\n\nMade django.contrib.sites framework dependency optional\nAdded migration for app\n\n\n0.8.0 (16\/04\/2015)\n\nSwitched to using django.utils.six instead of standalone six.\nAdded compatibility with Django 1.8\nDropped compatibility with Django 1.3\n\n\n0.7.4 (24\/03\/2015)\n\nAdded default values for fields.\nFixed Python 3.3 compatibility\nAdded creation of folders with ImageValue\n\n\n0.7.3, 0.7.2\npypi problems\n0.7.1 (11\/03\/2015)\n\nFixed pypi distribution.\n\n\n0.7 (06\/07\/2014)\n\nAdded PasswordValue\nAdded compatibility with Django 1.6 and 1.7.\n\n\n0.6 (16\/09\/2013)\n\nAdded compatibility with Django 1.5 and python3, dropped support for Django 1.2.\nFixed permissions: added permission for editing non-model (module-level) settings\nMake PIL\/Pillow not required in setup.py\n\n\n0.5 (11\/10\/2012)\n\nFixed error occuring when test are run with LANGUAGE_CODE different than 'en'\nAdded verbose_name option for Groups\nCleaned code\n\n\n0.4.1 (02\/10\/2012)\n\nFixed Image import\n\n\n0.4 (30\/09\/2012)\n\nNamed urls\nAdded polish translation\n\n\n0.3 (04\/09\/2012)\nIncluded testrunner in distribution\n0.2 (05\/07\/2012)\n\nFixed errors appearing when module-level and model-level settings have\nsame attribute names\nCorrected the editor templates admin integration\nUpdated README\n\n\n0.1 (29\/06\/2012)\nInitial PyPI release\n\n","508":"django-predicate\ndjango-predicate provides a Q like object to facilitate the question: \"would\nthis model instance be part of a query\" but without running the query or even\nsaving the object.\n\nQuickstart\nInstall django-predicate:\npip install django-predicate\nThen use the P object just as you would Q objects:\nfrom predicate import P\n\np = P(some_field__startswith=\"hello\", age__gt=20)\nYou can then call the eval method with a model instance to check whether it\npasses the conditions:\nmodel_instance = MyModel(some_field=\"hello there\", age=21)\nother_model_instance = MyModel(some_field=\"hello there\", age=10)\np.eval(model_instance)\n>>> True\np.eval(other_model_instance)\n>>> False\nor you can use Python's in operator.\nmodel_instance in p\n>>> True\nEven though a predicate is not a true container class - it can be used as (and\nwas designed as being) a virtual \"set\" of objects that meets some condiiton.\nLike Q objects, P objects can be &'ed  and |'ed together to form more\ncomplex logic groupings.\nIn fact, P objects are actually a subclass of Q objects, so you can use them in\nqueryset filter statements:\nqs = MyModel.objects.filter(p)\nP objects also support QuerySet-like filtering operations that can be\napplied to an arbitrary iterable: P.get(iterable), P.filter(iterable),\nand P.exclude(iterable):\nmodel_instance = MyModel(some_field=\"hello there\", age=21)\nother_model_instance = MyModel(some_field=\"hello there\", age=10)\np.filter([model_instance, other_model_instance]) == [model_instance]\n>>> True\np.get([model_instance, other_model_instance]) == model_instance\n>>> True\np.exclude([model_instance, other_model_instance]) == [other_model_instance]\n>>> True\nIf you have a situation where you want to use querysets and predicates based on\nthe same conditions, it is far better to start with the predicate. Because of\nthe way querysets assume a SQL context, it is non-trivial to reverse engineer\nthem back into a predicate. However as seen above, it is very straightforward\nto create a queryset based on a predicate.\n","509":"NO LONGER SUPPORTED: This project will receive no more updates and pull\nrequests and issues will no longer be monitored. I recommend everyone begin to\nadopt the migrations support that is in Django 1.7 and use South until you are\non Django 1.7.\n\nNashvegas\nThe purpose of this app is to enable a plug and play method for managing\ndatabase changes.\nDatabase migrations is a large topic with a lot of different approaches.  This\napproach worked well for my needs and maybe it will for you as well.\n\nDocumentation\nYou can find the documentation in the docs\/ folder of the repo or online at:\n\nhttp:\/\/nashvegas.readthedocs.org\n\nInstallation\nInstallation is simple:\n$ pip install nashvegas\n\nThen add nashvegas to your INSTALLED_APPS in your Django settings.py\nfile.\n\nSupport\nYou can either log issues on the Github issue tracker for this project or pop\ninto #nashvegas on Freenode.\n","510":"dmc\ndmc is python library for date and time manipulation.\nUsage of standard library modules such as datetime, time, pytz etc is dangerous and error prone.\nThis library is very opinionated about how to treat dates, times and intervals\nso as to prevent developers from shooting themselves in the foot. Also, the API\nis quote a bit more convinient for most use cases.\nOverview\ndmc is really just a wrapper around several other python libraries, but does so\nin a way that makes using these libraries safe.\nSome things to keep in mind:\n\nThere is no such thing as a naive time. ALL times involve a timezone, and that timezone is UTC.\nThe only time we deal with non-UTC timezones is when we parse or display a time.\nWe support math with dates, and math with times, but you can't do date math with times.\nFor example: You can't add \"1 day\" to 2014-03-28 02:00:00, because that\ndoesn't actually make sense. Do you mean add 24 hours? Then do that. Did\nyou mean you want 02:00:00 on 3\/29, sure we can do that, but you have to\ndecide.\n\nWe have a few primary objects:\n\nTime - Represents a precise time in UTC.\nTimeInterval - Represents a change in time, like 10 minutes, or 4000 hours.\nTimeSpan - a range of Times of some TimeInterval length\nTimeIterator - generates time instances based on a TimeSpan and a TimeInterval\nDate - Represents a calendar date. Only the modern Gregorian calendar is supported.\nDateInterval - Represents a change in calendar date, like '2 days', 'next month', or 'next monday'\nDateSpan - a range of dates of some DateInterval.\nDateIterator - genreates dates based on DateSpan and an DateInterval\n\nUsage\n>> start_t = dmc.Time.now()\n>> print start_t\n\"2014-03-29T20:16:58.265249Z\"\n\n>> print start.to_str(tz='Americas\/Los_Angeles')\n\"2014-03-29T12:16:58.265249-7:00\"\n\n>> print start.to_str(local=True)\n\"2014-03-29T12:16:58.265249-7:00\"\n\n>> start.to_str(\"YYYY-MM-DD HH:MM\", tz='Americas\/Los_Angeles')\n\"2014-03-29 12:16\"\n\n>> print start_t.to_timestamp()\n1396120755.748726\n\n>> print start_t.to_human()\n\"10 minutes ago\"\n\n>> start_t = dmc.Time.from_timestamp(1396120755.748726)\n\n>> print start_t.to_datetime()\ndatetime.datetime(2014, 3, 29, 12, 16, 58, 0, tzinfo=<UTC>)\n\n>> d = dmc.Date(2014, 3, 28)\n>> print d\n\"2014-03-28\"\n\n>> dmc.Date.from_str(\"3\/28\/2014\")\ndmc.Date(2014, 28, 3)\n\n>> start_t, _ = dmc.TimeSpan.from_date(d)\n>> print start_t\n\"2014-03-28T00:00:00Z\"\n\n# 3 weeks from now\n>> d += dmc.DateInterval(weeks=3)\n\n# Next Sunday\n>> d += dmc.DateInterval(weekday=0)\n\n>> start_t, end_t = dmc.TimeSpan.from_date(d)\n\n>> t = dmc.Time.now()\n>> t += dmc.TimeInterval(minutes=30)\n\n>> today_span = dmc.Date.today().to_timespan()\n>> for t in dmc.TimeIterator(today_span, dmc.TimeInterval(hours=1)):\n>>    print t\n\"2014-03-28T00:00:00Z\"\n\"2014-03-28T01:00:00Z\"\n\"2014-03-28T02:00:00Z\"\n....\n\nTesting\nWhen you're working with date and time sensitive code, it's often very helpful\nto be able to mock out the current time or date. dmc makes this easy:\n>> dmc.set_mock_time(dmc.Time().now() - dmc.TimeInterval(hours=1))\n>> dmc.clear_mock_time()\n\nOr, with a friendly context manager:\n>> with dmc.MockTime(...):\n.... pass\n\nWhat's wrong with datetime\nThis fun old wiki page is enlightening: https:\/\/wiki.python.org\/moin\/WorkingWithTime\nDatetime was supposed to be the solution. I still remember when I stumbed\nacross mx.Datetime. Mind blown. Such a better world. (datetime was added in\npython 2.3, in 2003, over 10 years ago). You'd think that dealing with dates\nand times would be solved problem. But twice a year I wake up to the collective\n\"oh shit\" as developers remember daylight savings time.\nThere is also PEP-431 that\nattempts to fix datetime timezones. Or is it just patching over how insane it\nis to handle timezones in this way?\nThere are also a lot of glaring holes in the API datetime provides us. We fill\nthose holes with a cornocopia of several other modules.  Basically, if you're\ndoing anything remotely complex with dates and times you need to understand and\nmake us of:\n\ndatetime\ntime\ncalendar\npytz\ndateutil\n\ndmc attempts to combine all these together into a consistent interface so you don't have to.\nFor specific examples, try these:\ntimestamps\nIt's easy to create a a datetime from a timestamp:\n>> d = datetime.datetime.fromtimestamp(12312412.0)\n\nOh wait, what timezone was that in? What we should have done was\n>> d = datetime.datetime.utcfromtimestamp(12312412.0)\n\nPop, quiz, how do you convert back?\n>> import time\n>> time.mktime(d.timetuple())\n\nOr wait, did I mean:\n>> time.mktime(d.utctimetuple())\n\nformatting \/ parsing\n>> d.isoformat()\n'2014-04-17T15:32:01.219333'\n\nNow how do I parse an isoformat? There are several solutions on stackoverflow.\n>> datetime.datetime.strptime(\"2014-04-17T15:32:01\", \"%Y-%m-%dT%H:%M:%S\" )\n\nBut of course parsing iso8601 is more complicated than that:\n>> import re\n>> s = \"2008-09-03T20:56:35.450686Z\"\n>> d = datetime.datetime(*map(int, re.split('[^\\d]', s)[:-1]))\n\nOk, there are some 3rd party libraries:\n>> dateutil.parser.parse('2014-04-17T15:32:01.219333Z')\n\nOf course you need to be real careful with this library, if there is something\nit doesn't recognize you'll just get a datetime filled in with values from the\ncurrent time (!!!! wtf)\nOh good, there is a python module for JUST THIS ONE FORMAT:\n>> import iso8601\n>> iso8601.parse_date(\"2007-01-25T12:00:00Z\")\n   datetime.datetime(2007, 1, 25, 12, 0, tzinfo=<iso8601.iso8601.Utc ...>)\n\nI don't even want to think about whether iso8601.iso8601.Utc == pytz.UTC.\ntimezones\n>> datetime.datetime.now()\n\nWhat timezone is this in?\nOh right, what I actually need to do is:\n>> import pytz\n>> d = pytz.UTC.localize(datetime.datetime.utcnow())\n\nHow about some arithmetic:\n>> now() + datetime.timedelta(days=1)\n\nIs this 24 hours from now? Is this the same number hours since midnight but the\nnext calendar day? Unfortunately the difference between these interpretations\nonly becomes obvious twice a year, in only some political regions of the world.\nWhat if I wanted to enumerate time ranges for a day in localtime, and then run\nsome queries that are in UTC.\n>> import pytz\n>> tz = pytz.timezone('US\/Pacific')\n>> start_dt = datetime.datetime(2014, 3, 9, 0, 0, 0, tzinfo=tz)\n>> end_dt = start_dt + datetime.timedelta(days=1)\n>> d = start_dt\n\nwhile d < end_dt:\n    run_query(d.astimezone(pytz.UTC), d.astimezone(pytz.UTC) + datetime.timedelta(hours=1))\n    d += datetime.timedelta(hours=1)\n\nHow many errors can you spot?\nOh, my favorite:\n>> import pytz\n>> tz = pytz.timezone('US\/Pacific')\n>> start_dt = datetime.datetime(2014, 3, 6, 0, 0, 0, tzinfo=tz)\n\n>> d = start_dt\n>> for _ in range(7):\n...   print d\n...   d += datetime.timedelta(days=1\n2014-03-06 00:00:00-08:00\n2014-03-07 00:00:00-08:00\n2014-03-08 00:00:00-08:00\n2014-03-09 00:00:00-08:00\n2014-03-10 00:00:00-08:00\n2014-03-11 00:00:00-08:00\n2014-03-12 00:00:00-08:00\n\nSee anything wrong here? 2014-03-10 00:00:00-08:00 doesn't make any sense.\nBecause on the 2014-03-10, the timezone should be -07:00. There are fun\nswitcheroo ways around this:\n>> d = start_dt\n>> for _ in range(7):\n...   print d.astimezone(pytz.UTC).astimezone(pytz.timezone('US\/Pacific'))\n...   d += datetime.timedelta(days=1)\n\nThis is common enough that pytz actually provides a function for it:\n>> print pytz.timezone('US\/Pacific').normalize(d)\n\nProject Status\nExperimental. Still estabilishing interfaces.\nBasic Date and Time formatting and parsing are written and tested.\nIntervals and Span are being actively developed.\nDMC still requires some real world use before APIs are firmly set.\n","511":"dmc\ndmc is python library for date and time manipulation.\nUsage of standard library modules such as datetime, time, pytz etc is dangerous and error prone.\nThis library is very opinionated about how to treat dates, times and intervals\nso as to prevent developers from shooting themselves in the foot. Also, the API\nis quote a bit more convinient for most use cases.\nOverview\ndmc is really just a wrapper around several other python libraries, but does so\nin a way that makes using these libraries safe.\nSome things to keep in mind:\n\nThere is no such thing as a naive time. ALL times involve a timezone, and that timezone is UTC.\nThe only time we deal with non-UTC timezones is when we parse or display a time.\nWe support math with dates, and math with times, but you can't do date math with times.\nFor example: You can't add \"1 day\" to 2014-03-28 02:00:00, because that\ndoesn't actually make sense. Do you mean add 24 hours? Then do that. Did\nyou mean you want 02:00:00 on 3\/29, sure we can do that, but you have to\ndecide.\n\nWe have a few primary objects:\n\nTime - Represents a precise time in UTC.\nTimeInterval - Represents a change in time, like 10 minutes, or 4000 hours.\nTimeSpan - a range of Times of some TimeInterval length\nTimeIterator - generates time instances based on a TimeSpan and a TimeInterval\nDate - Represents a calendar date. Only the modern Gregorian calendar is supported.\nDateInterval - Represents a change in calendar date, like '2 days', 'next month', or 'next monday'\nDateSpan - a range of dates of some DateInterval.\nDateIterator - genreates dates based on DateSpan and an DateInterval\n\nUsage\n>> start_t = dmc.Time.now()\n>> print start_t\n\"2014-03-29T20:16:58.265249Z\"\n\n>> print start.to_str(tz='Americas\/Los_Angeles')\n\"2014-03-29T12:16:58.265249-7:00\"\n\n>> print start.to_str(local=True)\n\"2014-03-29T12:16:58.265249-7:00\"\n\n>> start.to_str(\"YYYY-MM-DD HH:MM\", tz='Americas\/Los_Angeles')\n\"2014-03-29 12:16\"\n\n>> print start_t.to_timestamp()\n1396120755.748726\n\n>> print start_t.to_human()\n\"10 minutes ago\"\n\n>> start_t = dmc.Time.from_timestamp(1396120755.748726)\n\n>> print start_t.to_datetime()\ndatetime.datetime(2014, 3, 29, 12, 16, 58, 0, tzinfo=<UTC>)\n\n>> d = dmc.Date(2014, 3, 28)\n>> print d\n\"2014-03-28\"\n\n>> dmc.Date.from_str(\"3\/28\/2014\")\ndmc.Date(2014, 28, 3)\n\n>> start_t, _ = dmc.TimeSpan.from_date(d)\n>> print start_t\n\"2014-03-28T00:00:00Z\"\n\n# 3 weeks from now\n>> d += dmc.DateInterval(weeks=3)\n\n# Next Sunday\n>> d += dmc.DateInterval(weekday=0)\n\n>> start_t, end_t = dmc.TimeSpan.from_date(d)\n\n>> t = dmc.Time.now()\n>> t += dmc.TimeInterval(minutes=30)\n\n>> today_span = dmc.Date.today().to_timespan()\n>> for t in dmc.TimeIterator(today_span, dmc.TimeInterval(hours=1)):\n>>    print t\n\"2014-03-28T00:00:00Z\"\n\"2014-03-28T01:00:00Z\"\n\"2014-03-28T02:00:00Z\"\n....\n\nTesting\nWhen you're working with date and time sensitive code, it's often very helpful\nto be able to mock out the current time or date. dmc makes this easy:\n>> dmc.set_mock_time(dmc.Time().now() - dmc.TimeInterval(hours=1))\n>> dmc.clear_mock_time()\n\nOr, with a friendly context manager:\n>> with dmc.MockTime(...):\n.... pass\n\nWhat's wrong with datetime\nThis fun old wiki page is enlightening: https:\/\/wiki.python.org\/moin\/WorkingWithTime\nDatetime was supposed to be the solution. I still remember when I stumbed\nacross mx.Datetime. Mind blown. Such a better world. (datetime was added in\npython 2.3, in 2003, over 10 years ago). You'd think that dealing with dates\nand times would be solved problem. But twice a year I wake up to the collective\n\"oh shit\" as developers remember daylight savings time.\nThere is also PEP-431 that\nattempts to fix datetime timezones. Or is it just patching over how insane it\nis to handle timezones in this way?\nThere are also a lot of glaring holes in the API datetime provides us. We fill\nthose holes with a cornocopia of several other modules.  Basically, if you're\ndoing anything remotely complex with dates and times you need to understand and\nmake us of:\n\ndatetime\ntime\ncalendar\npytz\ndateutil\n\ndmc attempts to combine all these together into a consistent interface so you don't have to.\nFor specific examples, try these:\ntimestamps\nIt's easy to create a a datetime from a timestamp:\n>> d = datetime.datetime.fromtimestamp(12312412.0)\n\nOh wait, what timezone was that in? What we should have done was\n>> d = datetime.datetime.utcfromtimestamp(12312412.0)\n\nPop, quiz, how do you convert back?\n>> import time\n>> time.mktime(d.timetuple())\n\nOr wait, did I mean:\n>> time.mktime(d.utctimetuple())\n\nformatting \/ parsing\n>> d.isoformat()\n'2014-04-17T15:32:01.219333'\n\nNow how do I parse an isoformat? There are several solutions on stackoverflow.\n>> datetime.datetime.strptime(\"2014-04-17T15:32:01\", \"%Y-%m-%dT%H:%M:%S\" )\n\nBut of course parsing iso8601 is more complicated than that:\n>> import re\n>> s = \"2008-09-03T20:56:35.450686Z\"\n>> d = datetime.datetime(*map(int, re.split('[^\\d]', s)[:-1]))\n\nOk, there are some 3rd party libraries:\n>> dateutil.parser.parse('2014-04-17T15:32:01.219333Z')\n\nOf course you need to be real careful with this library, if there is something\nit doesn't recognize you'll just get a datetime filled in with values from the\ncurrent time (!!!! wtf)\nOh good, there is a python module for JUST THIS ONE FORMAT:\n>> import iso8601\n>> iso8601.parse_date(\"2007-01-25T12:00:00Z\")\n   datetime.datetime(2007, 1, 25, 12, 0, tzinfo=<iso8601.iso8601.Utc ...>)\n\nI don't even want to think about whether iso8601.iso8601.Utc == pytz.UTC.\ntimezones\n>> datetime.datetime.now()\n\nWhat timezone is this in?\nOh right, what I actually need to do is:\n>> import pytz\n>> d = pytz.UTC.localize(datetime.datetime.utcnow())\n\nHow about some arithmetic:\n>> now() + datetime.timedelta(days=1)\n\nIs this 24 hours from now? Is this the same number hours since midnight but the\nnext calendar day? Unfortunately the difference between these interpretations\nonly becomes obvious twice a year, in only some political regions of the world.\nWhat if I wanted to enumerate time ranges for a day in localtime, and then run\nsome queries that are in UTC.\n>> import pytz\n>> tz = pytz.timezone('US\/Pacific')\n>> start_dt = datetime.datetime(2014, 3, 9, 0, 0, 0, tzinfo=tz)\n>> end_dt = start_dt + datetime.timedelta(days=1)\n>> d = start_dt\n\nwhile d < end_dt:\n    run_query(d.astimezone(pytz.UTC), d.astimezone(pytz.UTC) + datetime.timedelta(hours=1))\n    d += datetime.timedelta(hours=1)\n\nHow many errors can you spot?\nOh, my favorite:\n>> import pytz\n>> tz = pytz.timezone('US\/Pacific')\n>> start_dt = datetime.datetime(2014, 3, 6, 0, 0, 0, tzinfo=tz)\n\n>> d = start_dt\n>> for _ in range(7):\n...   print d\n...   d += datetime.timedelta(days=1\n2014-03-06 00:00:00-08:00\n2014-03-07 00:00:00-08:00\n2014-03-08 00:00:00-08:00\n2014-03-09 00:00:00-08:00\n2014-03-10 00:00:00-08:00\n2014-03-11 00:00:00-08:00\n2014-03-12 00:00:00-08:00\n\nSee anything wrong here? 2014-03-10 00:00:00-08:00 doesn't make any sense.\nBecause on the 2014-03-10, the timezone should be -07:00. There are fun\nswitcheroo ways around this:\n>> d = start_dt\n>> for _ in range(7):\n...   print d.astimezone(pytz.UTC).astimezone(pytz.timezone('US\/Pacific'))\n...   d += datetime.timedelta(days=1)\n\nThis is common enough that pytz actually provides a function for it:\n>> print pytz.timezone('US\/Pacific').normalize(d)\n\nProject Status\nExperimental. Still estabilishing interfaces.\nBasic Date and Time formatting and parsing are written and tested.\nIntervals and Span are being actively developed.\nDMC still requires some real world use before APIs are firmly set.\n","512":"Valor\n \nPython HTTP clients for APIs represented by JSON Schema.\nThis is still super-early days yet, many things probably don't work. Use at your own risk.\nAmong most other things, docs aren't done, but check this out:\n$ heroku auth:whoami\njacob@heroku.com\n$ heroku apps\nancient-thicket-4976\narcane-reef-4005\n...\n\n$ python -i heroku.py\n>>> heroku.account.info().email\nu'jacob@heroku.com'\n>>> [app.name for app in heroku.app.list()]\n[u'ancient-thicket-4976', u'arcane-reef-4005', ...]\nThen see heroku.py as an example of how this works.\n\n\nWhat's with the name? The Ruby version of the same thing is Heroics. Heroics. Valor. See what I did there?\n","513":"django-pdfutils\nA simple django app to generate PDF documents.\n\nInstallation\n\nIn your settings.py, add pdfutils to your INSTALLED_APPS.\n(r'^reports\/', include(pdfutils.site.urls)), to your urls.py\nAdd pdfutils.autodiscover() to your urls.py\nCreate a report.py file in any installed django application.\nCreate your report(s)\nProfit!\n\nNote: If you are using buildout, don't forget to put pdfutils\nin your eggs section or else the django-pdfutils dependencies wont\nbe installed.\n\nExample report\nReports are basically views with custom methods and properties.\n# -*- coding: utf-8 -*-\n\nfrom django.contrib.auth.models import User\nfrom django.core.urlresolvers import reverse\nfrom django.utils.translation import ugettext as _\n\nfrom pdfutils.reports import Report\nfrom pdfutils.sites import site\n\n\nclass MyUserReport(Report):\n    title = _('Users')\n    template_name = 'myapp\/reports\/users-report.html'\n    slug = 'users-report'\n    orientation = 'portrait'\n\n    def get_users(self):\n        return User.objects.filter(is_staff=True)\n\n    def get_styles(self):\n        \"\"\"\n        It is possible to add or override style like so\n        \"\"\"\n        self.add_styles('myapp\/css\/users-report.css')\n        return super(AccountStatementReport, self).get_styles()\n\n    def filename(self):\n        \"\"\"\n        The filename can be generated dynamically and translated\n        \"\"\"\n        return _('Users-report-%(count)s.pdf') % {'count': self.get_users().count() }\n\n    def get_context_data(self):\n        \"\"\"\n        Context data is injected just like a normal view\n        \"\"\"\n        context = super(AccountStatementReport, self).get_context_data()\n        context['user_list'] = self.get_users()\n        return context\n\nsite.register(MyUserReport)\nThe slug should obviously be unique since it is used to build the report URL.\nFor example, with the default settings and URLs, the URL for report above would be \/reports\/users-report\/.\n\nExample template\n<html>\n    <head>\n        {{ STYLES|safe }}\n    <\/head>\n    <body class=\"{% if landscape %}landscape{% else %}portrait{% endif %}\">\n        <ul>\n            {% for user in user_list %}\n            <li>{{ user }}<\/li>\n            {% endfor %}\n        <\/ul>\n        <a href=\"{% url 'pdfutils:your_report_slug' %}?format=html\">Add ?format=html for easy template debug<\/a>\n    <\/body>\n<\/html>\nSome template variables are injected by default in reports:\n\ntitle\nslug\norientation\nMEDIA_URL\nSTATIC_URL\nSTYLES\n\n\nOverriding default CSS\nSince the default CSS (base.css, portrait.css, landscape.css) are normal static files, they can be overrided\nfrom any other django app which has a pdfutils folder in their static folder.\nNote: Be sure your applications are listed in the right order in INSTALLED_APPS !\n\nDependencies\n\ndjango >=1.4, < 1.5.99\ndecorator == 3.4.0, <= 3.9.9\nPIL == 1.1.7\nreportlab == 2.5\nhtml5lib == 0.90\nhttplib2 == 0.9\npyPdf == 1.13\nxhtml2pdf == 0.0.4\ndjango-xhtml2pdf == 0.0.3\n\nNote: dependencies versions are specified in setup.py. The amount of time required to find the right\ncombination of dependency versions is largely to blame for the creation of this project.\n","514":"PokemonGo-SlackBot\nReceive Slack notification whenever a Pok\u00e9mon spawns near a given location!\nFor instructions please check:\nhttps:\/\/www.themarketingtechnologist.co\/pokemon-go-slack-notifications\/\n####UPDATE #1: Now also works in Japanese thanks to https:\/\/github.com\/ttymsd!\n####UPDATE #2: Thanks to Bart persoons, you can also hook up to Homey (https:\/\/www.athom.com\/en\/)\nInstructions are in UPDATE #2 below the blog post: https:\/\/www.themarketingtechnologist.co\/pokemon-go-slack-notifications\/\n####Update #3: Languages, stability and lured Pok\u00e9mon!\nSeveral updates:\nMessage are now available in French and German thanks to Vincent. They can be set by setting the locale. The locale that needs to be used for the emojis can be set separately with -iL, default is English.\nAs we also see in the app and at Pokevision, server quite often seem to have troubles. Therefore, the script now has a reconnect features instead of stopping the script in cases of issues. From experience, it's advisable to use a Google account, since it's more stable than PTC.\nAnd last but not least, lured Pok\u00e9mon! Pokevision doesn't show these (yet), so many thanks go to the tip by Daniel.\n","515":"django-filepicker\n\n\nA django plugin to make integrating with Filepicker.io even easier\n##Installation\n\n\nInstall the python package:\npip install django-filepicker\n\n\n\nAdd your file picker api key to your settings.py file. You api key can be\nfound in the developer portal.\nFILEPICKER_API_KEY = <your api key>\n\n\n\nConfigure your media root.\nCWD = os.getcwd()\nMEDIA_ROOT = os.path.join(CWD, 'media')\n\n\n\nAdd a filepicker field to your model and set the upload_to value.\n# *Please note, that FPFileField handle only one file*\n# In demo you can see how to handle multiple files upload.\nfpfile = django_filepicker.models.FPFileField(upload_to='uploads')\n\n\n\nModify your view to accept the uploaded files along with the post data.\nform = models.TestModelForm(request.POST, request.FILES)\nif form.is_valid():\n    #Save will read the data and upload it to the location\n    # defined in TestModel\n    form.save()\n\n\n\nAdd the form.media variable above your other JavaScript calls.\n<head>\n    <title>Form Template Example<\/title>\n    <!--  Normally this would go into a block defined in base.html that\n          occurs before other JavaScript calls. -->\n    {{ form.media }}\n<\/head>\n\n<body>\n    <form method=\"POST\" action=\"\/\" enctype=\"multipart\/form-data\">\n        {{ form.as_p }}\n        <input type=\"submit\" \/>\n    <\/form>\n<\/body>\n\n\n\n##Demo\nTo see how all the pieces come together, see the example code in demo\/, which you can run with the standard\npython manage.py runserver command\n###models.py\nimport django_filepicker\nclass TestModel(models.Model):\n#FPFileField is a field that will render as a filepicker dragdrop widget, but\n#When accessed will provide a File-like interface (so you can do fpfile.read(), for instance)\nfpfile = django_filepicker.models.FPFileField(upload_to='uploads')\n###views.py\n#building the form - automagically turns the uploaded fpurl into a File object\nform = models.TestModelForm(request.POST, request.FILES)\nif form.is_valid():\n#Save will read the data and upload it to the location defined in TestModel\nform.save()\nBe sure to also provide your Filepicker.io api key, either as a parameter to the FPFileField or in settings.py as FILEPICKER_API_KEY\n##Components\n###Models\nThe filepicker django library defines the FPFileField model field so you can get all the benefits of using Filepicker.io as a drop-in replacement for the standard django FileField. No need to change any of your view logic.\n###Forms\nSimilarly with the FPFileField for models, the filepicker django library defines a FPFileField for forms as well, that likewise serves as a drop-in replacement for the standard django FileField. There is also the FPUrlField if you want to store the Filepicker.io URL instead\n###Middleware\nAlso included is a middleware library that will take any Filepicker.io urls passed to the server, download the contents, and place the result in request.FILES. This way, you can keep your backend code for handling file uploads the same as before while adding all the front-end magic that Filepicker.io provides\nIf you have any questions, don't hesitate to reach out at contact@filepicker.io. For more information, see https:\/\/filepicker.io\nOpen-sourced under the MIT License. Pull requests encouraged!\n","516":"Powerhose\nPowerhose turns your CPU-bound tasks into I\/O-bound tasks so your Python applications\nare easier to scale.\n\nPowerhose is an implementation of the\nRequest-Reply Broker\npattern in ZMQ.\nSee http:\/\/powerhose.readthedocs.org for a full documentation.\n","517":"Loop View for Android\nEnglish \u4e2d\u6587\nAndroid LoopView is a powerful widget for unlimited rotation picture, It provides some configuration options and good control the appearance and operational requirements.\nSimple usage picture:\n\nCustom layout usage picture:\n\nUsing LoopView in your application\nIf you are building with Gradle, simply add the following line to the dependencies section of your build.gradle file:\nsupport\nimplementation 'com.kevin:loopview:1.5.6'\n\nandroidX\nimplementation 'com.kevin:loopview:2.0.1'\n\nSimple Usage\nConfigured as View in layout.xml\nTo add the LoopView to your application, specify <com.kevin.loopview.BannerView in your layout XML.\n<com.kevin.loopview.BannerView\n    android:id=\"@+id\/main_act_banner\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"192dp\">\n<\/com.kevin.loopview.BannerView>\n\nConfigured Programmatically\nBannerView mBannerView = (BannerView) this.findViewById(R.id.main_act_banner);\n\/\/ set image loader, can set up any image engine.\nmBannerView.setImageLoader(new ImageLoader() {\n    @Override\n    public void loadImage(ImageView imageView, String url, int placeholder) {\n        Glide.with(imageView.getContext()).load(url).into(imageView);\n    }\n});\nString json = LocalFileUtils.getStringFormAsset(this, \"loopview_date.json\");\nLoopData loopData = new Gson().fromJson(json, LoopData.class);\nmBannerView.setData(loopData);\n\/\/ begin to loop\nmBannerView.startAutoLoop();\n\nmBannerView.setOnItemClickListener(new BaseLoopAdapter.OnItemClickListener() {\n    @Override\n    public void onItemClick(View view, LoopData.ItemData itemData, int position) {\n        \/\/ Open connection with browser\n        Intent intent = new Intent();\n        intent.setData(Uri.parse(itemData.link));\n        intent.setAction(Intent.ACTION_VIEW);\n        startActivity(intent);\n    }\n});\n\nMore configuration Usage\nXML Usage\nIf you decide to use BannerView as a view, you can define it in your xml layout like this:\nxmlns:app=\"http:\/\/schemas.android.com\/apk\/res-auto\"\n\n<com.kevin.loopview.BannerView\n    android:id=\"@+id\/adloop_act_adloopview\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"192dp\"\n    app:loop_interval=\"5000\"\n    app:loop_scrollDuration=\"2000\"\n    app:loop_dotMargin=\"5dp\"\n    app:loop_autoLoop=\"[true|false]\"\n    app:loop_alwaysShowDot=\"[true|false]\"\n    app:loop_dotSelector=\"@drawable\/ad_dots_selector\"\n    app:loop_placeholder=\"@mipmap\/ic_launcher\"\n    app:loop_layout=\"@layout\/ad_loopview_layout\">\n<\/com.kevin.loopview.BannerView>\n\nProgramme Usage\n\/\/ Set page switching transition time\nmLoopView.setScrollDuration(1000);\n\/\/ Set time interval\nmLoopView.setInterval(3000);\n\/\/ To initialize the data in a collection\nmLoopView.setData(List<Map<String, String>> data);\n\/\/ Initialized data in entity mode\nmLoopView.setData(LoopData rotateData);\n\/\/ Initialized data in a collection mode\nmLoopView.setData(List<String> images);\n\/\/ Initialized data in a collection mode\nmLoopView.setData(List<String> images, List<String> links);\n\/\/ Initialized data in a collection mode\nmLoopView.setData(List<String> images, List<String> descs, List<String> links);\n\/\/ Get the running loop date\nmLoopView.getData();\n\/\/ Begin to auto Loop\nmLoopView.startAutoLoop();\n\/\/ Begin to auto Loop delay\nmLoopView.startAutoLoop(long delayTimeInMills);\n\/\/ Stop to auto Loop\nmLoopView.stopAutoLoop();\n\/\/ Set a custom loop layout\nmLoopView.setLoopLayout(int layoutResId);\n\nNotes:\nIn custom layout you must to use those ids loop_view_pager in ViewPager loop_view_dots in indicate point parent LinearLayout and loop_view_desc in description TextView;\nMake sure you at least have loop_view_pager.\nLicense\nCopyright 2015 Kevin zhou\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","518":"CircleImageView\nCircleImageView is a component which display circle image with customization options\n\n\nRelease notes\n1.2.0\n\nImplemented animation for pressed event\nAdded new attribute pressedRingWidth, pressedRingColor\n\n1.1.4\n\nFix bug (issue #3 on github)\n\n1.1.3\n\nImprove component performance\nOptimization of image drawing\n\n1.1.2\n\nSupport padding parameters (paddingLeft, paddingTop, paddingRight, paddingBottom).\n\n1.1.1\n\nFix bug (issue #1 on github).\n\n1.1.0\n\nshows shadow for image;\nshows shadow for border.\n\n1.0.0\n\nchange background for *.png images;\nshows images;\nrealize click listener for this component;\nshows border (component is selected or component is unselected).\n\nUsage\nTo make a circular ImageView, add this CircleImageView library to your project and add CircleImageView in your layout XML.\nYou can also grab it via Gradle:\n      compile 'com.alexzh:circleimageview:1.2.0'\nor Maven:\n<dependency>\n    <groupId>com.alexzh<\/groupId>\n    <artifactId>circleimageview<\/artifactId>\n    <version>1.2.0<\/version>\n    <type>pom<\/type>\n<\/dependency>\nXML\n    <com.alexzh.circleimageview.CircleImageView\n        android:id=\"@+id\/imageView\"\n        android:layout_width=\"192dp\"\n        android:layout_height=\"192dp\"\n        android:clickable=\"true\"\n        android:src=\"@drawable\/android_logo\"\n        android:layout_alignParentTop=\"true\"\n        android:layout_centerHorizontal=\"true\"\n        android:layout_marginTop=\"10dp\"\n        app:view_backgroundColor=\"@color\/colorPrimary\"\n        app:view_shadowRadius=\"4dp\"\n        app:view_shadowDx=\"2dp\"\n        app:view_shadowDy=\"0dp\"\n        app:view_shadowColor=\"@color\/grey\"\n        app:view_borderWidth=\"4dp\"\n        app:view_selectedColor=\"@color\/blue\"\n        app:view_borderColor=\"@android:color\/darker_gray\"\/>\nYou may use the following properties in your XML to customize your CircularImageView.\nProperties:\n\napp:view_backgroundColor    (color)\napp:view_borderColor        (color)\napp:view_borderWidth        (dimension)\napp:view_selectedColor      (color)\napp:view_shadowRadius       (dimension)\napp:view_shadowDx           (dimension)\napp:view_shadowDy           (dimension)\napp:view_shadowColor        (color)\n\nJAVA\n\n\nsetOnItemSelectedClickListener(ItemSelectedListener listener) - let's handle onSelected(View view) and onUnselected(View view)\n\n\nonSelected(View view) - view is selected\n\n\nonUnselected(View view) - view is unselected\n\n\nDeveloped By\nAliaksandr Zhukovich - http:\/\/alexzh.com\nLicense\nCopyright (C) 2016 Aliaksandr Zhukovich (http:\/\/alexzh.com)\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\t     \n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","519":"CircleImageView\nCircleImageView is a component which display circle image with customization options\n\n\nRelease notes\n1.2.0\n\nImplemented animation for pressed event\nAdded new attribute pressedRingWidth, pressedRingColor\n\n1.1.4\n\nFix bug (issue #3 on github)\n\n1.1.3\n\nImprove component performance\nOptimization of image drawing\n\n1.1.2\n\nSupport padding parameters (paddingLeft, paddingTop, paddingRight, paddingBottom).\n\n1.1.1\n\nFix bug (issue #1 on github).\n\n1.1.0\n\nshows shadow for image;\nshows shadow for border.\n\n1.0.0\n\nchange background for *.png images;\nshows images;\nrealize click listener for this component;\nshows border (component is selected or component is unselected).\n\nUsage\nTo make a circular ImageView, add this CircleImageView library to your project and add CircleImageView in your layout XML.\nYou can also grab it via Gradle:\n      compile 'com.alexzh:circleimageview:1.2.0'\nor Maven:\n<dependency>\n    <groupId>com.alexzh<\/groupId>\n    <artifactId>circleimageview<\/artifactId>\n    <version>1.2.0<\/version>\n    <type>pom<\/type>\n<\/dependency>\nXML\n    <com.alexzh.circleimageview.CircleImageView\n        android:id=\"@+id\/imageView\"\n        android:layout_width=\"192dp\"\n        android:layout_height=\"192dp\"\n        android:clickable=\"true\"\n        android:src=\"@drawable\/android_logo\"\n        android:layout_alignParentTop=\"true\"\n        android:layout_centerHorizontal=\"true\"\n        android:layout_marginTop=\"10dp\"\n        app:view_backgroundColor=\"@color\/colorPrimary\"\n        app:view_shadowRadius=\"4dp\"\n        app:view_shadowDx=\"2dp\"\n        app:view_shadowDy=\"0dp\"\n        app:view_shadowColor=\"@color\/grey\"\n        app:view_borderWidth=\"4dp\"\n        app:view_selectedColor=\"@color\/blue\"\n        app:view_borderColor=\"@android:color\/darker_gray\"\/>\nYou may use the following properties in your XML to customize your CircularImageView.\nProperties:\n\napp:view_backgroundColor    (color)\napp:view_borderColor        (color)\napp:view_borderWidth        (dimension)\napp:view_selectedColor      (color)\napp:view_shadowRadius       (dimension)\napp:view_shadowDx           (dimension)\napp:view_shadowDy           (dimension)\napp:view_shadowColor        (color)\n\nJAVA\n\n\nsetOnItemSelectedClickListener(ItemSelectedListener listener) - let's handle onSelected(View view) and onUnselected(View view)\n\n\nonSelected(View view) - view is selected\n\n\nonUnselected(View view) - view is unselected\n\n\nDeveloped By\nAliaksandr Zhukovich - http:\/\/alexzh.com\nLicense\nCopyright (C) 2016 Aliaksandr Zhukovich (http:\/\/alexzh.com)\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\t     \n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","520":"\u6a21\u4eff\u58a8\u8ff9\u5929\u6c143.0\u5f15\u5bfc\u754c\u9762\n\n\u5c4f\u5e55\u5f55\u5236\u4f7f\u7528\u7684licecap\nhttp:\/\/www.cockos.com\/licecap\/\nREADME.md\u7f16\u8f91\u53c2\u8003\nhttp:\/\/www.tuicool.com\/articles\/zIJrEjn\n@author \u5f20\u5174\u4e1a\nhttp:\/\/blog.csdn.net\/xyz_lmn\n\u6211\u7684\u65b0\u6d6a\u5fae\u535a\uff1a@\u5f20\u5174\u4e1aTBOW\n","521":"CHelper is distrubuted under LGPL\n","522":"CHelper is distrubuted under LGPL\n","523":"multi-column-list-adapter  \u00a0\u00a0 \nMultiColumnListAdapter is a cursor adapter that enables you to make a ListView that looks like a GridView. One important benefit is that you can add headers and footers to your grid-looking list.\n\nSample App\nThe sample app demonstrates how to use the MultiColumnListAdapter to create a GridView-looking layout with a header. You can build it from source or install it from the Play Store.\nDownload\nGrab the library from Maven central\n<dependency>\n    <groupId>com.twotoasters.multicolumnlistadapter<\/groupId>\n    <artifactId>library<\/artifactId>\n    <version>1.0.0<\/version>\n<\/dependency>\nor Gradle:\ncompile 'com.twotoasters.multicolumnlistadapter:library:1.0.+'\nCredit\nMultiColumnListAdapter was created by Two Toasters in development with Ebates.\nLicense\nCopyright 2014 Two Toasters\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","524":"cleo-primer\nThe cleo-primer package provides a basic RESTful implementation of partial, out-of-order and real-time typeahead services.\nIt is built on Cleo, an open source package from LinkedIn.\nHomepage\nFind out more about Cleo at http:\/\/sna-projects.com\/cleo.\nLicense\nApache Public License (APL) 2.0\nArtifacts\ncleo-primer.war\nMaven\ngroupId: com.sna-projects.cleo\nartifactId: cleo-primer\nversion: 1.0\nBuild the war\nmvn clean package\n\nLaunch WebApp\nLaunch the cleo-primer web application using the next command from the\nmain folder:\nMAVEN_OPTS=\"-Xms1g -Xmx1g\" mvn jetty:run -Dcleo.instance.name=Company -Dcleo.instance.type=cleo.primer.GenericTypeaheadInstance -Dcleo.instance.conf=src\/main\/resources\/config\/generic-typeahead\n\nYou can customize your web application by choosing different values for parameters\ncleo.instance.name, cleo.instance.type and cleo.instance.conf. Depending on the size\nof your data sets, you may need to specify a different JVM heap size.\nPost a list of new elements\n.\/scripts\/post-element-list.sh dat\/nasdaq-company-list.xml\n\nPost a new element\n.\/scripts\/post-element.sh dat\/nasdaq-google.xml dat\/nasdaq-intel.xml\n\nSearch\nVisit the URL below to try out cleo-primer.\nhttp:\/\/localhost:8080\/cleo-primer\n\nhttp:\/\/localhost:8080\/cleo-primer\/rest\/elements\/search?query=goo\n\nEclipse\nSet up Eclipse by executing the command below:\nmvn eclipse:eclipse\n\nInside Eclipse, select Preferences > Java > Build Path > Classpath Variables. Define a new classpath variable M2_REPO and assign maven repository.\nFor more information, check out http:\/\/maven.apache.org\/guides\/mini\/guide-ide-eclipse.html\nContribute\nFor help please see the discussion group.  Bugs and feature requests can be filed here.\n","525":"Geo Cluster Facet Plugin for elasticsearch\nThis plugin provides a facet for elasticsearch to aggregate documents with geo_point fields.\nA naive, distance-based algorithm is used to build rectangular (and potentially overlapping) clusters with a weighted center.\nTo install the plugin, run:\nbin\/plugin --url https:\/\/github.com\/zenobase\/geocluster-facet\/releases\/download\/0.0.11\/geocluster-facet-0.0.11.jar --install geocluster-facet\n\nVersions\n\n\n\ngeocluster-facet\nelasticsearch\n\n\n\n\n\n\t\t\t\t0.0.12 -> master\n        \t\t\n\n1.4.x -> 1.7.x\n\n\n0.0.11\n1.2.x, 1.3.x\n\n\n0.0.10\n1.0.x, 1.1.x\n\n\n0.0.9\n0.90.6, 0.90.7\n\n\n0.0.8\n0.90.5\n\n\n0.0.7\n0.90.3\n\n\n0.0.6\n0.90.2\n\n\n0.0.5\n0.90.0, 0.90.1\n\n\n0.0.2 -> 0.0.4\n0.20.x\n\n\n0.0.1\n0.19.x\n\n\n\nParameters\n\n\n\nfield\nThe name of a field of type `geo_point`.\n\n\nfactor\nControls the amount of clustering, from 0.0 (don't cluster any points) to 1.0 (create a single cluster containing all points). \n\t\t\tDefaults to 0.1. This value is relative to the size of the area that contains points, so it does not need to be adjusted e.g. when \n\t\t\tzooming in on a map.\n\n\n\nExample\nQuery:\n{\n    \"query\" : { ... }\n    \"facets\" : {\n        \"places\" : { \n            \"geo_cluster\" : {\n                \"field\" : \"location\",\n                \"factor\" : 0.5\n            }\n        }\n    }\n}\nSearchSourceBuilder search = ...\nsearch.facet(new GeoClusterFacetBuilder(\"places\", \"location\", 0.5));\nResult:\n{\n    ...\n    \"facets\" : {\n        \"geo_cluster\" : [ {\n        \t\"count\" : 1,\n        \t\"lat\" : 36.08,\n        \t\"lon\" : -115.17\n        }, {\n            \"count\" : 3,\n            \"lat\" : 39.75,\n            \"lon\" : -104.87,\n            \"lat_min\" : 37.00,\n            \"lat_max\" : 41.00,\n            \"lon_min\" : -109.05,\n            \"lon_max\" : -102.04\n        } ]\n    }\n}\nLicense\nThis software is licensed under the Apache 2 license, quoted below.\n\nCopyright 2012-2014 Zenobase LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not\nuse this file except in compliance with the License. You may obtain a copy of\nthe License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations under\nthe License.\n\n","526":"Geo Cluster Facet Plugin for elasticsearch\nThis plugin provides a facet for elasticsearch to aggregate documents with geo_point fields.\nA naive, distance-based algorithm is used to build rectangular (and potentially overlapping) clusters with a weighted center.\nTo install the plugin, run:\nbin\/plugin --url https:\/\/github.com\/zenobase\/geocluster-facet\/releases\/download\/0.0.11\/geocluster-facet-0.0.11.jar --install geocluster-facet\n\nVersions\n\n\n\ngeocluster-facet\nelasticsearch\n\n\n\n\n\n\t\t\t\t0.0.12 -> master\n        \t\t\n\n1.4.x -> 1.7.x\n\n\n0.0.11\n1.2.x, 1.3.x\n\n\n0.0.10\n1.0.x, 1.1.x\n\n\n0.0.9\n0.90.6, 0.90.7\n\n\n0.0.8\n0.90.5\n\n\n0.0.7\n0.90.3\n\n\n0.0.6\n0.90.2\n\n\n0.0.5\n0.90.0, 0.90.1\n\n\n0.0.2 -> 0.0.4\n0.20.x\n\n\n0.0.1\n0.19.x\n\n\n\nParameters\n\n\n\nfield\nThe name of a field of type `geo_point`.\n\n\nfactor\nControls the amount of clustering, from 0.0 (don't cluster any points) to 1.0 (create a single cluster containing all points). \n\t\t\tDefaults to 0.1. This value is relative to the size of the area that contains points, so it does not need to be adjusted e.g. when \n\t\t\tzooming in on a map.\n\n\n\nExample\nQuery:\n{\n    \"query\" : { ... }\n    \"facets\" : {\n        \"places\" : { \n            \"geo_cluster\" : {\n                \"field\" : \"location\",\n                \"factor\" : 0.5\n            }\n        }\n    }\n}\nSearchSourceBuilder search = ...\nsearch.facet(new GeoClusterFacetBuilder(\"places\", \"location\", 0.5));\nResult:\n{\n    ...\n    \"facets\" : {\n        \"geo_cluster\" : [ {\n        \t\"count\" : 1,\n        \t\"lat\" : 36.08,\n        \t\"lon\" : -115.17\n        }, {\n            \"count\" : 3,\n            \"lat\" : 39.75,\n            \"lon\" : -104.87,\n            \"lat_min\" : 37.00,\n            \"lat_max\" : 41.00,\n            \"lon_min\" : -109.05,\n            \"lon_max\" : -102.04\n        } ]\n    }\n}\nLicense\nThis software is licensed under the Apache 2 license, quoted below.\n\nCopyright 2012-2014 Zenobase LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not\nuse this file except in compliance with the License. You may obtain a copy of\nthe License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations under\nthe License.\n\n","527":"Libral\n\nLibral is a systems management library that makes it possible to query and\nmodify system resources (files, packages, services, etc.) through a\ndesired-state API. Its goals are to:\n\nProvide a management API for UNIX-like systems to query and modify\nsystem resources\nMake managing new kinds of resources very simple; in particular, there is\nno need to write native code to add new providers and they can be\narbitrary scripts that adhere to one of a number of simple calling\nconventions\nExpress both current state and desired state in a simple, unified form\nGuarantee that all changes are done idempotently by enforcing desired\nstate only when changes are needed\nHave a very small default footprint that enables use of libral in\nresource-constrained environments such as devices or containers\nBe versatile enough to serve as the basis of more extensive configuration\nmanagement systems, such as\nPuppet, without being directly\ndependent on any one of them.\n\nGetting in touch\n\nMailing list: libral\nIRC: irc.freenode.net:#libral\n\nBuilding and installation\nIf you just want to quickly try out libral, you can download a\nprecompiled tarball\n(GPG signature)\nthat should work on any Linux machine that has glibc 2.12 or later. (It\nmight actually work with glibc 2.8 or later.) If you succeed in that, please\nlet us know.\nWe have reports of the precompiled binaries working on 64-bit Fedora 21-25,\nRHEL 5-7 (and related distributions like CentOS), Debian 6-9,\nUbuntu 10.04-16.10, and SLES 11-12. The binaries will not work on\n32-bit and pre-glibc 2.8 systems: RHEL 4, SLES 10, ...\nIn case you do need to build from source, which is not required for\nprovider development, only if you want to work on the core libral library,\nthis document contains instructions on building libral.\nDocker\nYou can also try out libral in the context of a Docker container. The\nDockerfile in the repository allows for building an image quickly based\non the above mentioned precompiled tarball.\ndocker build -t puppet\/libral .\n\nRunning this can then be done with Docker, for instance the following\ninvocation will launch ralsh in the context of the container.\ndocker run --rm -t puppet\/libral\n\nThis is intended for exploring the CLI and experimenting with providers.\nUsage\nAfter you build libral, or after you download and unpack the\nprecompiled tarball,\nyou can try things out by running ralsh:\n    # If you downloaded the precompiled tarball\n    alias ralsh=$TARBALL_LOCATION\/ral\/bin\/ralsh\n\n    # Only if you built libral from source\n    export RALSH_DATA_DIR=$LIBRAL_CHECKOUT\/data\n    alias ralsh=$LIBRAL_CHECKOUT\/bin\/ralsh\n\n    # list available types\n    ralsh\n    # list all instances of a type\n    ralsh mount\n    # list a specific instance\n    ralsh service crond\n    # make a change for the better\n    ralsh service crond ensure=stopped\n\n    # Do the same against a remote system to which you have ssh access\n    ralsh -t ahost\n    ralsh -t ahost package sudo\nThe default output from ralsh is meant for human consumption and looks a\nlot like Puppet. It is also possible to have ralsh produce\nJSON output by passing the --json flag.\nMany of the providers that libral knows about are separate\nscripts. ralsh searches them in the following order. In each case, the\nproviders must be executable scripts in a subdirectory providers in the\nmentioned directory ending in .prov:\n\nif the environment variable RALSH_DATA_DIR is set, look in the\ndirectory this variable is set to\nif the --include option to ralsh is given, look in that directory\n(the option can be given multiple times)\ndefault to a directory determined at build time, by default\n\/usr\/share\/libral\/data\n\nRunning inside a container\nThe\nprecompiled tarball\ncontains a statically linked ralsh and all supporting files. After\nunpacking the tarball, you can copy it into a container and run it like\nthis:\n    CONTAINER=<some_container>\n    docker cp $TARBALL_LOCATION\/ral $CONTAINER:\/tmp\n    docker exec $CONTAINER \/bin\/sh -c \/tmp\/ral\/bin\/ralsh\n    docker exec $CONTAINER \/bin\/sh -c '\/tmp\/ral\/bin\/ralsh user root'\nWriting providers\nWhat resources libral can manage is determined by what providers are\navailable. Some providers are built in and implemented in C++, but doing\nthat is of course labor intensive and should only be done for good\nreason. It is much simpler, and recommended, that new providers first be\nimplemented as external providers. External providers are nothing more than\nscripts or other executables that follow one of libral's calling\nconventions. The different calling conventions trade off implementation\ncomplexity for expressive power.\nThe following calling conventions are available. If you are just getting\nstarted with libral, you should write your first providers using the\nsimple or json calling convention:\n\nsimple\njson: input\/output via JSON\njson_batch (planned,maybe): input\/output via JSON, can operate on multiple resources at once\nnative\n\nFor all of these, you will also want to read up on the\nmetadata that each provider needs to produce to describe\nitself.\nTo start a new provider, follow these steps:\n\nDecide on some working directory DIR, run mkdir $DIR\/providers, and\nCreate a file $DIR\/providers\/myprovider.prov and make it executable\nMake sure that running myprovider.prov ral_action=describe returns\nvalid provider metadata, especially a valid type. For\nnow it doesn't matter what the type is, let's call it MYTYPE\nRun ralsh -I $DIR $MYTYPE. This will ask the provider to list all\ninstances of the type using the list action. Get that working\nRun ralsh -I $DIR $MYTYPE NAME. This will ask the provider to find a\nresource with name NAME using the find action. Get that working,\ntoo.\nRun ralsh -I $DIR $MYTYPE NAME ATTR=VALUE.... This will ask the\nprovider to update the resource NAME using the update action.\nYou're done; your provider is ready for a pull request here ;)\n\nTodo list\n\n finish mount provider\n add a shell provider\n event reporting on update\n simple type system and checking for provider attributes\n produce help\/details about providers\n add a json calling convention and use it in a provider\nmore core providers\n\n cron\n file\n group (groupXXX)\n host\n package (besides dnf and yum)\n service (besides systemd, upstart and sysv)\n user (userXXX)\n\n\neven more core providers\n\n interface\n k5login\n mailalias\n selboolean\n selmodule\n sshkey\n ssh-authorized-key\n vlan\n yumrepo\n\n\n add a remote provider (using an HTTP API)\n adapt providers to multiple OS (maybe using mount)\n add support for running providers over ssh (see this PR for details)\n noop mode\n expand the type system to cover as much of Puppet 4 as is reasonable\n\nSome language\nNaming things is hard; here's the terms libral uses:\n\nType: the abstract description of an entity we need to manage; this is\nthe external interface through which entities are managed. I am not\nentirely convinced this belongs in libral at all\nProvider: something that knows how to manage a certain kind of thing\nwith very specific means; for example something that knows how to manage\nusers with the user* commands, or how to manage mounts on Linux\nResource: an instance of something that a provider manages. This is\nclosely tied both to what is being managed and how it is being\nmanaged. The important thing is that resources expose a desired-state\ninterface and therefore abstract away the details of how changes are made\n\nFIXME: we need some conventions around some special resource\nproperties; especially, namevar should always be 'name' and the primary key\nfor all resources from this provider, and 'ensure' should have a special\nmeaning (or should it?)\nOpen questions\n\nDo we need types at all at this level ?\nCan we get away without any provider selection logic ? There are two\nreasons why provider selection is necessary:\n\nadjust to system differences. Could we push those to compile time ?\nmanage different things that are somewhat similar, like system packages\nand gems, or local users and LDAP users ? This would push this\nmodelling decision into a layer above libral.\n\n\nWould it be better to make providers responsible for event generation\nrather than doing that in the framework ?\n\n","528":"DSST: Discriminative Scale Space Tracker\nDSST is part of the Visual Object Tracking Repository,\nwhich aims at providing a central repository for state-of-the-art tracking algorithms that are freely available.\nThe source code for this tracker was submitted for the VOT2014 challenge.\nA minor adaption was made by using MATLAB's imresize() function\ninstead of a custom mex file to increase portability.\nDependencies:\n\nMatlab\nImage Processing Toolbox\nSignal Processing Toolbox\n\nThe following description was copied literally from the original author.\nABSTRACT\nThe Discriminative Scale Space Tracker (DSST), proposed in [2], extends the Minimum Output Sum of Squared\nErrors (MOSSE) tracker [1] with robust scale estimation. The MOSSE tracker works by training a discriminative\ncorrelation filter on a set of observed sample grayscale patches. This correlation filter is then applied to estimate the\ntarget translation in the next frame. The DSST additionally learns a one-dimensional discriminative scale filter, that\nis used to estimate the target size. The scale filter is trained by extracting several sample patches at different scales\naround the current target position in the image. Each sample is represented by a fixed-length feature vector based on\nHOG. These samples are used to learn a multi-channel one-dimensional discriminative filter for scale estimation. This\nscale filter is generic and can be combined with any tracker that is limited to only estimating the target translation.\nGiven a new image, the DSST first applies a translation filter to obtain the most probable target location. The scale\nfilter is then applied at this location to estimate the target size. The tracking model is then updated with the new\ninformation information of the target and background appearance. For the translation filter, we combine the intensity\nfeatures employed in the MOSSE tracker with a pixel-dense representation of HOG-features.\n[1] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui. Visual object tracking using adaptive correlation filters. In CVPR,\n2010.\n[2] M. Danelljan, G. H\u00e4ger, F. Shahbaz Khan, and M. Felsberg. Accurate scale estimation for robust visual tracking. In Proceedings\nof the British Machine Vision Conference (BMVC), 2014.\nREADME\nThis MATLAB code integrates the Discriminative Scale Space Tracker (DSST) [1] into the VOT 2014 evaluation kit. The implementation is built upon the code provided by [2]. The code provided by [3] is used for computing the HOG features.\nInstructions:\nUse the wrapper function \"wrapper.m\" without trax.\nThe code may be publicly available from the VOT homepage.\nContact:\nMartin Danelljan\nmartin.danelljan@liu.se\n[1] Martin Danelljan, Gustav H\u00e4ger, Fahad Shahbaz Khan and Michael Felsberg.\n\"Accurate Scale Estimation for Robust Visual Tracking\".\nProceedings of the British Machine Vision Conference (BMVC), 2014.\n[2] J. Henriques, R. Caseiro, P. Martins, and J. Batista.\n\"Exploiting the circulant structure of tracking-by-detection with kernels.\"\nIn ECCV, 2012.\n[3] Piotr Doll\u00e1r.\n\"Piotr's Image and Video Matlab Toolbox (PMT).\"\nhttp:\/\/vision.ucsd.edu\/~pdollar\/toolbox\/doc\/index.html.\n","529":"\n\n\nLibrary: Virgil Crypto\nLibrary features | Supported algorithms | Build | Benchmark | Docs | Support\nIntroduction\nWelcome to Virgil Security!\nVirgil Security guides software developers into the forthcoming security world in which everything will be encrypted (and passwords will be eliminated).  In this world, the days of developers having to raise millions of dollars to build a secure chat, secure email, secure file-sharing, or a secure anything have come to an end.  Now developers can instead focus on building features that give them a competitive market advantage while end-users can enjoy the privacy and security they increasingly demand.\nVirgil Security offers this security via a stack of security libraries (ECIES with Crypto Agility wrapped in Virgil Cryptogram) and all the necessary infrastructure to enable seamless, end-to-end encryption for any application, platform or device. End-to-end encryption can be used for a variety of important reasons: compliance with regulations like HIPAA and GDPR, the transfer and storage of PII, general user privacy as a feature, breach risk mitigation and more.\nVirgil Crypto also has all required cryptographic functions and primitives to perform an implementation of Pythia technology.\nSee below for currently available languages and platforms. Get in touch with us to get beta access to our Key infrastructure.\nLibrary Features\nSupported crypto operations\n\nGenerate keys;\nEncrypt data;\nDecrypt data;\nSign data;\nVerify data;\nPythia protocol.\n\nSupported platforms\nCrypto Library is suitable for the following platforms:\n\nDesktop (Windows, Linux, MacOS);\nMobile (iOS, Android, watchOS, tvOS);\nWeb (WebAssembly, AsmJS)\n\nSupported languages\nCrypto Library is written in C++ [CDN] and supports bindings for the following programming languages:\n\nGo [CDN]\nPHP [CDN]\nPython [CDN]\nRuby [CDN]\nJava [CDN]\nC# [CDN]\nAsmJS [CDN]\nNodeJS [CDN]\nWebAssembly [CDN]\n\nSwift\/Objective_C language can use the Virgil Crypto Library directly, without any bind.\nAvailable Wrappers\nVirgil also has special wrappers for simplifying Crypto Library implementation in your digital solutions. We support wrappers for the following programming languages:\n\nGo\nObjective-C\/Swift\nC#\/.NET\nRuby\nPython\nPHP\nJS\n\nSupported algorithms\n\n\n\nPurpose\nAlgorithm, Source\n\n\n\n\nKey Generation, PRNG\nNIST SP 800-90A\n\n\nKey Derivation\nKDF2*, HKDF\n\n\nKey Exchange\nX25519*, ECDH, RSA\n\n\nHashing\nSHA-2 (256\/384*\/512), Blake2\n\n\nDigital Signature\nEd25519*, ECDSA, RSASSA-PSS\n\n\nEntropy Source\nLinux \/dev\/urandom, Windows CryptGenRandom()\n\n\nSymmetric Algorithms\nAES GCM*, AES CBC\n\n\nElliptic Curves\nX25519, Ed25519*, Koblitz (secp192k1, secp224k1, secp256k1), Brainpool (bp256r1, bp384r1, bp512r1),  NIST (secp256r1, secp192r1, secp224r1, secp384r1, secp521r1)\n\n\n\n\n* - used by default.\n\nBuild\nPrerequisites\nThe page lists the prerequisite packages which need to be installed on the different platforms to be able to configure and to build Virgil Crypto Library.\n\nCompiler:\n\ng++ (version >= 4.9), or\nclang++ (version >= 3.6), or\nmsvc++ (version >= 14.0)\n\n\nBuild tools:\n\ncmake (version >= 3.10)\nmake\n\n\nOther tools:\n\ngit\nswig (version >= 3.0.12), optional for C++ build\ndoxygen (optional)\n\n\n\nBuild the Library\nThis section describes how to build Virgil Crypto Library for \u0430 particular OS.\nStep 1 - Get source code\n\nOpen Terminal.\nGet the source code:\n\n> git clone https:\/\/github.com\/VirgilSecurity\/virgil-crypto.git\nStep 2 - Run a build Script\nUnix-like OS:\n> cd virgil-crypto\n> .\/utils\/build.sh\nWindows OS:\n> cd virgil-crypto\n> .\/utils\/build.bat\nRun the build script with the option -h to get help on how to build a library for a necessary OS, Platforms or languages.\nBuild command has the following syntax:\n .\/utils\/build.sh [--target=<target>] [--feature=<feature>] [--src=<src_dir>] [--build=<build_dir>] [--install=<install_dir>]\nwhere the command options are:\n\n<target> - (default = cpp) target to build which contains two parts <name>[-<version>], where <name>:\n\n\n\n\n<name>\nbuild information\n\n\n\n\ncpp\nbuild C++ library\n\n\nmacos\nbuild framework for Apple macOSX, requirements: OS X, Xcode\n\n\nios\nbuild framework for Apple iOS, requirements: OS X, Xcode\n\n\nwatchos\nbuild framework for Apple WatchOS, requirements: OS X, Xcode\n\n\ntvos\nbuild framework for Apple TVOS, requirements: OS X, Xcode\n\n\nphp\nbuild PHP library, requirements: php-dev\n\n\npython\nbuild Python library\n\n\nruby\nbuild Ruby library\n\n\njava\nbuild Java library, requirements: $JAVA_HOME\n\n\njava_android\nbuild Java library under Android platform, requirements: $ANDROID_NDK\n\n\nnet\nbuild .NET library, requirements: .NET or Mono\n\n\nnet_macos\nbuild .NET library under Apple macOSX platform, requirements: Mono, OS X, Xcode\n\n\nnet_ios\nbuild .NET library under Apple iOS platform, requirements: Mono, OS X, Xcode\n\n\nnet_applewatchos\nbuild .NET library under WatchOS platform, requirements: Mono, OS X, Xcode\n\n\nnet_appletvos\nbuild .NET library under TVOS platform, requirements: Mono, OS X, Xcode\n\n\nnet_android\nbuild .NET library under Android platform, requirements: Mono, $ANDROID_NDK\n\n\nasmjs\nbuild AsmJS library, requirements: $EMSDK_HOME\n\n\nwebasm\nbuild WebAssembly library, requirements: $EMSDK_HOME\n\n\nnodejs\nbuild NodeJS module\n\n\ngo\nbuild Golang library\n\n\n\n\n<feature> - available features:\n\npythia - ask to enable feature Pythia. Some targets enable this feature by default.\n\n\n<src_dir> - a path to the directory where a root CMakeLists.txt file is located (default = .).\n<build_dir> - a path to the directory where temp files will be stored (default = build\/<target>).\n<install_dir> - a path to the directory where library files will be installed (default = install\/<target>).\n\n\nAll available Crypto Library versions you can find here.\n\nBenchmark\nYou can find out benchmark of the Crypto Library in the benchmark.md file.\nDocs\nWe always try to make cryptography accessible for programmers, and the documentation below can get you started today.\n\nCrypto Library API\nLibrary usage examples\n\nGenerate a key pair\nimport and export keys\ngenerate and verify signature\nencrypt and decrypt data\n\n\nVirgil CLI for the Crypto Library\n\nSupport\nOur developer support team is here to help you. Find out more information on our Help Center.\nYou can find us on Twitter or send us email support@VirgilSecurity.com.\nAlso, get extra help from our support team on Slack.\nLicense\nBSD 3-Clause. See LICENSE for details.\n","530":"\nShark\nA framework for controlling, recording, training, and running a self driving robotic car. Focused now on an implementation that runs on 1\/10th scale rc cars with pwm servo steering and ESC throttle controller driven by an 9865 Servo board with a RaspberryPi 3 or Jetson TX2.\n\n\nGoals\n\nUse ZeroMQ and C\/Python to create a mesh of components that are flexible and fast\nRun all code on the robot ( pi 3 \/ jetson tx2 )\nManage things with a mobile device via web page and joystick controller\nTrain in the cloud\n\nBuild your bot\n\ncheck docs\/bot_setup.md\n\nOnce you are ready, you can install them as services.\nCheck shark.service and sharkweb.service to run on startup.\n\nWorkflow\nCheck camera output:\n\nNavigate to web page\nSelect 'robot'. You should see a live image from camera\n\nLogging:\n\npress X on PS3 Sixaxis controls to enable recording\nleft analog to steer, right analog forward to throttle\nonly recording when throttle is non zero\npress X to disable recording\n\nEdit logs:\n\nNavigate to web page\nSelect 'log'\nSelect 'view\/edit logs'\nobserve recorded logs\nif unwanted frames, use slider, 'set trim start', and 'set trim end' to set range\nuse 'trim log' button to remove unwanted frames\n\nManual Training:\n\ncopy logs to your PC: scp me@pi.local:~\/projects\/shark\/log\/*.jpg ~\/projects\/shark\/log\ntrain model on your PC: python train.py mymodel\ncp mymodel to pi: scp mymodel me@pi.local:~\/projects\/shark\/model\/\non the pi: python shark.py --model mymodel\n\nWeb EC2 Based Training:\n\ncheck docs\/aws_setup.md\nNavigate to web page\nSelect 'ec2' button\nSelect 'start ec2'\nWait for 1 to 2 minutes and select 'check ec2' until the machine is ready.\nSelect 'prepare host' to copy code to remote host\nFrom the home menu select 'log'\nSelect 'upload logs'\nFrom the home menu select 'train'\nSelect 'model' and name the new model\nOptionally select 'epochs' and set upper limit of epochs\nSelect 'train' to start training. Feedback varies depending on browser. Better luck from a desktop browser. Tested mostly on Firefox.\nWhen complete, select 'push model' to tell robot predict loop to load that trained result.\nReady to test self driving\nWhen you are done with server, select 'release ec2' to shutdown remote machine.\n\nSelf Driving:\n\nhit triangle toggle start self driving.\nuse dpad up and down to modify throttle scale\n\n","531":"generator-material-app\nYet another yeoman generator for scaffolding a simple MEAN stack application using some material design elements.\n\nThis project is still under heavy development.\nFeatures\nThe generator supports group based ACL's and socket.io communication with the server API.\nInstall\n$ npm install generator-material-app\nQuick Start\nTo quickly scaffold an application use the following commands (Answer all questions with hitting the enter key):\n$ mkdir app && cd $_\n$ yo material-app\n$ gulp build\n$ npm start\nWill generate something like this:\n\nCreate user dialog\nNavigate to http:\/\/localhost:9001 to see the generated application where you can administer the users of your application\nAdd API\nThis will add a server API and a corresponding client route to manage your cats including test stubs and documentation:\n$ yo material-app:api cat\n$ yo material-app:apiroute cat\n$ gulp build\n$ npm start\nGenerate Documentation\nThe documentation is by now generated for server side code only:\ngulp jsdoc\nRun Tests\nFor running the generated test start the following gulp tasks\ngulp unit:server\ngulp unit:client\nDeploy\nFor now, run commands:\nNODE_ENV=production gulp build\nNODE_ENV=production npm start\n\nAnd manually seed database with NODE_ENV=production npm run seed if you choosed to auth your app.\nYou can't sign into app without any users. You can specify users and seed data in server\/config\/seed.js.\nEnvironment Variables\n\nDATABASE_NAME\nMONGO_URI || 'mongodb:\/\/localhost\/' + process.env.DATABASE_NAME\nMONGO_OPTIONS\n\nModelDefinition\nModefy factory YourResourceDefinition in your-resource.service.js.\nFor example:\nModelDefinitions({\n  name: {type: 'text', required: true},\n  info: 'text',\n  nested: {\n    name: {\n      type: 'text',\n      desc: 'Nested Name'\n    }\n  }\n})\n\nFor detail options.\nList of Generators\n\n\nApplication scaffold\n\nmaterial-app (alias for material-app:app) - The directory name will be used as the application name\n\n\n\nServer API\n\nmaterial-app:api - Pass the name of the API items as an argument\n\n\n\nClient Generators\n\nmaterial-app:apiroute - Pass the name of the route as an argument\nmaterial-app:decorator - Pass the name of the decorator as an argument\nmaterial-app:directive - Pass the name of the directive as an argument\nmaterial-app:route - Pass the name of the route as an argument\nmaterial-app:controller - Pass the name of the controller as an argument\nmaterial-app:filter - Pass the name of the filter as an argument\nmaterial-app:service - Pass the name of the service as an argument\nmaterial-app:provider - Pass the name of the provider as an argument\nmaterial-app:factory - Pass the name of the factory as an argument\n\n\n\nAPIs\nPropDefintion\n\ntypes supported now:\n\ntypes for input(corresponding Mongoose type in ()), like 'text'(String), 'url'(String), 'number'(Number), 'date'(Date), 'password'(String),\n'select' - type values in options array correspond to mongoose type\n'select\/resource' - mongoose ObjectId, use resource to simulate mongoose ref\n\n\ncommon options\n\ntype - different types of property\nNotice: name: {type: 'text'} can be short in name: 'text' but type: {type: 'text'} can't\ndesc - name of prop displayed in form, detail and list\n\ndefault is capitalized last name of nested name\n\n\ndisplayKey - key to display in md-select\n\ndefault is 'name' when type === 'select\/resource'\n\n\ndisplayPriority - when set to 'low', field in list will auto-hide when\n\nwhen screen width is less than 1200 because of css class .hide-in-narrow\nnarrowMode in ModelViewGroup is true\n\n\n\n\nvalidation options\n\nrequired - ng-required\nformat - regex for ng-validate\nremoteUnique - resource name to check unique from server\nrepeatInput - force repeat field, usually for type=password\nvalidators - for ng-messages, above validations can be written in validators uniformly\n\nrequired\npattern\nremote-unique\nrepeat-input\n\n\n\n\nspecial config options for 'select'\n\n\noptions - static options for select\n\n\nvalueKey - key to value in md-select\n\ndefault is '_id' when type === 'select\/resource'\n\n\n\nresource - static resource for 'select\/resource'\n\nonly work when type === ''\n\n\n\nparams - static params for 'select\/resource'\n\n\ngetOptions - async function returns a promise to load options upon md-select is open, resource and params can be dynamic with this\n\n\n\n\nBelow is an example with all options, which can be generated into app with the demo option\nvar typeMap = {\n  User: User,\n  ClientModelDoc: ClientModelDoc\n};\n\nreturn ModelDefinitions({\n  name: {\n    type: 'text',\n    format: {\n      value: \/^[a-zA-Z]{6,18}$\/,\n      error: 'Should be 6-18 letters.'\n    },\n    required: true,\n    remoteUnique: 'ClientModelDoc'\n  },\n  repeatName: {\n    type: 'text',\n    repeatInput: 'name',\n    desc: 'Repeat Name',\n    displayPriority: 'low'\n  },\n  wholeName: {\n    type: 'text',\n    validators: {\n      required: true,\n      'remote-unique': {\n        value: 'ClientModelDoc',\n        error: 'Override default error'\n      },\n      pattern: \/^[a-zA-Z0-9]{8,12}$\/\n    }\n  },\n  user: {\n    type: 'select\/resource',\n    required: true,\n    resource: User\n  },\n  rootUser: {\n    type: 'select\/resource',\n    resource: User,\n    params: {\n      role: 'root'\n    },\n    displayKey: '_id'\n  },\n  anyType: {\n    type: 'select',\n    options: ['User', 'ClientModelDoc']\n  },\n  anyTypeRef: {\n    type: 'select',\n    getOptions: function(model) {\n      var resource = typeMap[model.anyType];\n      if (!resource || !resource.query) return $q.when([]);\n      return resource.query().$promise;\n    },\n    displayKey: 'name',\n    valueKey: '_id'\n  },\n  important: 'text',\n  notImportant: {\n    type: 'text',\n    desc: 'Not Important',\n    displayPriority: 'low'\n  },\n  nested: {\n    name: 'text',\n    repeatName: {\n      type: 'text',\n      repeatInput: 'nested.name',\n      desc: 'Nested Repeat Name',\n      displayPriority: 'low'\n    },\n    wholeName: {\n      type: 'text',\n      desc: 'Whole Name',\n      remoteUnique: 'ClientModelDoc',\n      auto: function (nestedModel) {\n        return nestedModel.nested.firstName + ' ' + nestedModel.nested.secondName;\n      }\n    },\n    firstName: {\n      type: 'text',\n      desc: 'First Name',\n      displayPriority: 'low'\n    },\n    secondName: {\n      type: 'text',\n      desc: 'Second Name',\n      displayPriority: 'low'\n    },\n  },\n  info: 'text',\n  \/\/active: 'boolean'\n})\n\nPurpose\nThis generator is suited for prototyping simple CRUD applications. The generated code is somehow following  John Papa's Styleguide for Angular applications. Every generator generates a test stub\nfor easily adding tests to your application. Note that there is not much material design in the layout yet, despite the use of\nthe Angular Material Design components.\nUsed Technologies\n\nNode.js\nMongoDB\nExpress\nmongoose\nsocket.io\nAngular.js\nui.router\nSASS\nGulp\n\nScreenshots\n\nSet password dialog\n","532":"Phantom-Jasmine\nPhantom-Jasmine is a simple set of two scripts for running your Jasmine Tests via Phantom.js (http:\/\/www.phantomjs.org\/).\nThe first script lib\/console-runner.js is a plugin to Jasmine that outputs test results (with ansi color codes) via console.log (included with a script tag inside TestRunner.html).\nThe second script lib\/run_jasmine_test.coffee takes an html file as it's first and only argument and then executes any Jasmine tests\nthat file loads. See below for more detail.\nInstallation\nAssuming you have PhantomJs setup and installed...\nsudo npm install phantom-jasmine -g\n\nRunning Tests\nphantom-jasmine examples\/TestRunner.html\n\nOn some running OS X you might have to pass in the full url, ex:\nphantom-jasmine file:\/\/localhost\/Users\/bob\/phantom-jasmine\/examples\/TestRunner.html \n\nIf everything works you should see output like this in your terminal:\nUtil : should fail for the example\nError: Expected false to be truthy.\n\nFinished\n-----------------\n3 specs, 1 failure in 0.024s.\n\nTo run your own tests with Phantom-Jasmine just look at TestRunner.html and modify\/copy the script tags accordingly.\n","533":"An Unofficial Google+ JavaScript API\nIt has been 6 months since we have seen any Circle\/Posts\/Followers\nWrite\/Read API for Google+. Since Google+ is by nature asynchronous\nwe could tap into their XHR calls and imitate the requests.\nWho uses this API:\n My Hangouts Chrome Extension https:\/\/plus.google.com\/116935358560979346551\/about\n Circle Management Chrome Extension https:\/\/plus.google.com\/100283465826629314254\/about\n Map My Circles for Google+\u2122 https:\/\/chrome.google.com\/webstore\/detail\/mcfifkeppchbjlepfbepfhjpkhfalcoa\n Nuke Comments on Google+ https:\/\/chrome.google.com\/webstore\/detail\/nfgaadooldinkdjpjbnbgnoaepmajdfh\n Do Share on Google+ https:\/\/chrome.google.com\/webstore\/detail\/oglhhmnmdocfhmhlekfdecokagmbchnf\nWho made this:\nCore Contributors\n\nMohamed Mansour (Maintainer, Core Dev), - https:\/\/github.com\/mohamedmansour\nTzafrir Rehan (Core Dev) - https:\/\/github.com\/tzafrir\n\nContributors\n\nJingqin Lynn (Contributed newPost)\nRyan Peggs (Contributed Bug fixes)\n\nWhat is it about:\nI provide you a very basic asynchronous Google+ API, in the current\nrelease, you can do the following:\n\nCreate, Modify, Sort, Query, Remove Circles\nQuery, Modify your Profile Information\nAdd, Remove, Move People from and to Circles\nReal-Time Search\nLookup Posts, Manage comments by reporting and deleting.\n\nThis is a fully read and write API.\nNative Examples:\n\/\/ Create an instance of the API.\nvar plus = new GooglePlusAPI();\n\n\/\/ Initialize the API so we could get a new session if it exists we reuse it.\nplus.init();\n\n\/\/ Refresh your circle information.\nplus.refreshCircles(function() {\n\n   \/\/ Let us see who added me to their circle.\n   plus.getPeopleWhoAddedMe(function(people) {\n     console.log(people);\n   });\n\n   \/\/ Let us see who is in my circles.\n   plus.getPeopleInMyCircles(function(people) {\n     console.log(people);\n   });\n\n   \/\/ Let us see who is in our circles but didn't add us to theirs.\n   plus.getDatabase().getPersonEntity().find({in_my_circle: 'Y', added_me: 'N'}, function(people) {\n     console.log(people);\n   });\n});\n\nAs you see, it is pretty easy to query everything. The possibilities are inifinite\nsince the data is backed up in a WebSQL DataStore, so querying, reporting, etc, would\nbe super quick.\nIf you want to place that in an extension, I have created a bridge, so you can use\nthis in a content script context and extension context safely. To do so, you send a\nmessage as follows:\n\/\/ Initialize the API so we get the authorization token.\nchrome.extension.sendRequest({method: 'PlusAPI', data: {service: 'Plus', method: 'init'}}, function(initResponse) {\n  chrome.extension.sendRequest({method: 'PlusAPI', data: {service: 'Plus', method: 'refreshCircles'}}, function() {\n    \/\/ etc ... The method is the same method we defined previously in the raw example.\n  });\n});\n\nAnother example, lets say we want to search for a hash tag:\n\/\/ Initialize the Google Plus API Wrapper.\nvar api = new GooglePlusAPI();\n\n\/\/ Lets initialize it so we can get the current logged in users session.\napi.init();\n\n\/\/ Search for the API. You have the following enums to choose from for searching:\n\/\/\n\/\/   GooglePlusAPI.SearchType.EVERYTHING\n\/\/   GooglePlusAPI.SearchType.PEOPLE_PAGES;\n\/\/   GooglePlusAPI.SearchType.POSTS\n\/\/   GooglePlusAPI.SearchType.SPARKS\n\/\/   GooglePlusAPI.SearchType.HANGOUTS\n\/\/   GooglePlusAPI.SearchType.HASHTAGS\n\/\/\n\/\/ So lets search for hashtags that have Microsoft inside them.\napi.search(function(resp) {\n  console.log(\"Hash results for Microsoft: \" + resp.data.join(\", \"));\n}, \"microsoft\", { type: GooglePlusAPI.SearchType.HASHTAGS });\n\nA full blown example will be released by the end of this week showing how powerful this could be.\nAs you already know, I am creating a simple circle management utility so you can manage your circles.\nAPI Documentation\nAbstractEntity Members:\n\nString getName() - The table name that this entity holds.\nvoid tableDefinition() - Abstract method that you override to describe the table.\nvoid initialize() - Private method that creates the DDL to execute from tableDefinition\nvoid drop(Function:doneCallback) - Drops the table from cache including the definition.\nvoid clear(Function:doneCallback) - Removes all rows from the table, keeps the definition.\nvoid create(Object[]:obj, Function<Object{status, data}>:callback) - Inserts object(s) into the table.\nvoid destroy(String[]:id, Function<Object{status, data}>:callback) - Deletes object(s) into the table.\nvoid update(Object[]:obj, Function<Object{status, data}>:callback) - Updates object(s) into the table.\nvoid find(Object:obj, Function<Object{status, data}>:callback) - Find a specific object(s) in the table.\nvoid findAll(Function<Object{status, data}>:callback) - Queries for everthing, all the data.\nvoid count(Object:obj, Function<Object{status, data}>:callback) - Counts the number of rows in the table.\nvoid save(Object[]:obj, Function<Object{status, data}>:callback) - Updates otherwise it creates.\n\nPlusDB Entities:\n\nvoid open() - Opens the database\nvoid clearAll(Function:doneCallback) - Drops all tables from the database.\nAbstractEntity getPersonEntity() - Returns the PersonEntity\nAbstractEntity getCircleEntity() - Returns the CircleEntity\nAbstractEntity getPersonCircleEntity() - Returns the PersonCircleEntity\n\nNative querying:\n\nPlusDB getDatabase() - Returns the native Database to do advanced queries\n\nInitialization, fill up the database:\n\nvoid init(Function:doneCallback) - Initializes session and data, you can call it at app start.\nvoid refreshCircles(Function:doneCallback, boolean:opt_onlyCircles) - Queries G+ Service for all circles and people information.\nvoid refreshFollowers(Function:doneCallback) - Queries G+ Service for everyone who is following me.\nvoid refreshFindPeople(Function:doneCallback) - Queries G+ Services for discovering similar people like me.\nvoid refreshInfo(Function:doneCallback(data)) - Refresh my information. Rarely used.\n\nPersistence:\n\nvoid addPeople(Function:doneCallback, String:circleName, Array<String>:usersToAdd) - Adding people to a circle.\nvoid removePeople(Function:doneCallback, String:circleName, Array<String>:usersToRemove) - Removing people from a circle\nvoid createCircle(Function:doneCallback, String:circleName, String:optionalDescription) - Creating a circle.\nvoid removeCircle(Function:doneCallback, String:circleID) - Removing a circle.\nvoid sortCircle(Function:doneCallback, String:circleID, Number:index) - Sort a circle to the given index, G+ will deal with the order\nvoid modifyCircle(Function:doneCallback, String:circleID, String:optionalName, String:optionalDescription) - Modifying circle meta.\nvoid modifyBlocked(Function:doneCallback, Array<String>:usersToModify, boolean:opt_block) - Modify the blocked state of people. Allows blocking and unblocking.\nvoid modifyMute(Function:doneCallback, String:activityID, Boolean:muteStatus) - Sets the mute status for a specific item.\nvoid modifyLockPost(Function:doneCallback, String:activityID, Boolean:toLock) - Sets the mute status for a specific item.\nvoid modifyDisableComments(Function:doneCallback, String:activityID, Boolean:toDisable) - Sets the mute status for a specific item.\nvoid addComment(Function:doneCallback, String: postId, String: content) - Adds a comment.\nvoid deleteComment(Function:doneCallback, String:commentId) - Deleting a comment.\nvoid deleteActivity(Function:doneCallback, String:activityId) - Deleting a post.\nvoid saveProfile(Function:doneCallback, String:introduction) - Save a new introduction.\nvoid reportProfile(Function:doneCallback, String:userId, opt_abuseReason) - Report an abusive profile.\nvoid newPost(Function:doneCallback, String:content) - Creates a new post on the stream.\nvoid fetchLinkMedia(Function:doneCallback(data), String:url - Fetches media items describing a URL such as images, title and description.\n\nRead:\n\nboolean isAuthenticated()\nvoid getProfile(Function({introduction}):callback, String:googleProfileID)\nvoid getInfo(Function({id, name, email, acl}:callback)\nvoid getCircles(Function(CircleEntity[]):callback)\nvoid getCircle(Function(String:circleID, CircleEntity):callback)\nvoid getPeople(Object:obj, Function(PersonEntity[]):callback)\nvoid getPerson(Function(String:googleProfileID, PersonEntity):callback)\nvoid getPeopleInMyCircles(Function(PersonEntity[]):callback)\nvoid getPersonInMyCircles(String:googleProfileID, Function(PersonEntity):callback)\nvoid getPeopleWhoAddedMe(Function(PersonEntity[]):callback)\nvoid getPersonWhoAddedMe(String:googleProfileID, Function(PersonEntity):callback)\nvoid search(Function(data):callback, String:query, Object:{category, precache, burst, burst_size})\nvoid lookupUsers(Function(data):callback, Array<String:googleProfileID>)\nvoid lookupPost(Function(data):callback, String:googleProfileID, String:postProfileID)\nvoid lookupActivities(Function(data):callback, String:circleID, String:personID, String:pageToken)\nvoid getPages(Function(data):callback)\nvoid getCommunities(Function(data):callback)\nvoid getCommunity(Function(data):callback, String:communityId)\n\nPrivate Members (only for internal API):\n\nObject _parseJSON(String:input) - Parses the Google Irregular JSON\nXMLHttpRequest _requestService(Function:callback, String:url, String:postData - Sends an XHR request to Google Service\nString _getSession()- Unique user session that authenticates to persist to your account.\n\nWatch this space!\n","534":"grunt-contrib-stylus v1.2.0  \n\nCompile Stylus files to CSS\n\nGetting Started\nIf you haven't used Grunt before, be sure to check out the Getting Started guide, as it explains how to create a Gruntfile as well as install and use Grunt plugins. Once you're familiar with that process, you may install this plugin with this command:\nnpm install grunt-contrib-stylus --save-dev\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\ngrunt.loadNpmTasks('grunt-contrib-stylus');\nThis plugin was designed to work with Grunt 0.4.x. If you're still using grunt v0.3.x it's strongly recommended that you upgrade, but in case you can't please use v0.3.1.\nStylus task\nRun this task with the grunt stylus command.\nTask targets, files and options may be specified according to the grunt Configuring tasks guide.\nThis task comes preloaded with nib.\nOptions\ncompress\nType: Boolean\nDefault: true\nSpecifies if we should compress the compiled css. Compression is always disabled when --debug flag is passed to grunt.\nlinenos\nType: Boolean\nDefault: false\nSpecifies if the generated CSS file should contain comments indicating the corresponding stylus line.\nfirebug\nType: Boolean\nDefault: false\nSpecifies if the generated CSS file should contain debug info that can be used by the FireStylus Firebug plugin.\npaths\nType: Array\nSpecifies directories to scan for @import directives when parsing.\ndefine\nType: Object\nAllows you to define global variables in Gruntfile that will be accessible in Stylus files.\nrawDefine\nType: Boolean|Array|String\nIf set to true, defines global variables in Gruntfile without casting objects to Stylus lists. Allows using a JavaScript object in Gruntfile to be accessible as a Stylus Hash. See Stylus's issue tracker for details. stylus\/stylus#1286\nAllows passing an array or string to specify individual keys to define \"raw\", casting all other keys as default Stylus behavior.\nurlfunc\nType: String|Object\nIf String: specifies function name that should be used for embedding images as Data URI.\nIf Object:\n\nname - Type: String. Function name that should be used for embedding images as Data URI.\n[ limit ] - Type: Number|Boolean Default: 30000. Bytesize limit defaulting to 30Kb (30000), use false to disable the limit.\n[ [paths ] - Type: Array, Default: []. Image resolution path(s).\n\nSee url() for details.\nuse\nType: Array\nAllows passing of stylus plugins to be used during compile.\nimport\nType: Array\nImport given stylus packages into every compiled .styl file, as if you wrote @import '...'\nin every single one of said files.\ninclude css\nType: Boolean\nDefault: false\nWhen including a css file in your app.styl by using @import \"style.css\", by default it will not include the full script, use true to compile into one script.\n( NOTICE: the object key contains a space \"include css\" )\nresolve url\nType: Boolean\nDefault: false\nTelling Stylus to generate url(\"bar\/baz.png\") in the compiled CSS files accordingly from @import \"bar\/bar.styl\" and url(\"baz.png\"), which makes relative pathes work in Stylus.\nAll urls are resolved relatively to position of resulting .css file\n( NOTICE: the object key contains a space \"resolve url\" and Stylus resolves the url only if it finds the provided file )\nresolve url nocheck\nType: Boolean\nDefault: false\nLike resolve url, but without file existence check. Fixing some current issues.\n( NOTICE: the object key contains two spaces \"resolve url nocheck\" )\nbanner\nType: String\nDefault: ''\nThis string will be prepended to the beginning of the compiled output.\nrelativeDest\nType: String\nDefault: ''\nPath to be joined and resolved with each file dest to get new one. Mostly useful for files specified using wildcards:\noptions: {\n  relativeDest: 'out'\n},\nfiles: [{\n  src: ['src\/components\/*\/*.styl'],\n  ext: '.css'\n}]\nwill generate src\/components\/*\/out\/*.css files.\nExamples\nstylus: {\n  compile: {\n    options: {\n      paths: ['path\/to\/import', 'another\/to\/import'],\n      relativeDest: '..\/out', \/\/path to be joined and resolved with each file dest to get new one.\n                              \/\/mostly useful for files specified using wildcards\n      urlfunc: 'data-uri', \/\/ use data-uri('test.png') in our code to trigger Data URI embedding\n      use: [\n        function () {\n          return testPlugin('yep'); \/\/ plugin with options\n        },\n        require('fluidity') \/\/ use stylus plugin at compile time\n      ],\n      import: [      \/\/  @import 'foo', 'bar\/moo', etc. into every .styl file\n        'foo',       \/\/  that is compiled. These might be findable based on values you gave\n        'bar\/moo'    \/\/  to `paths`, or a plugin you added under `use`\n      ]\n    },\n    files: {\n      'path\/to\/result.css': 'path\/to\/source.styl', \/\/ 1:1 compile\n      'path\/to\/another.css': ['path\/to\/sources\/*.styl', 'path\/to\/more\/*.styl'] \/\/ compile and concat into single file\n    }\n  }\n}\nRelease History\n\n2016-03-08\u2003\u2003\u2003v1.2.0\u2003\u2003\u2003Stylus ^0.54.\n2016-03-04\u2003\u2003\u2003v1.1.0\u2003\u2003\u2003Add \"resolve url nocheck\". Fixes #146. Remove peerDeps. Point main script to task.\n2016-01-19\u2003\u2003\u2003v1.0.0\u2003\u2003\u2003Bump stylus to 0.53.\n2015-07-21\u2003\u2003\u2003v0.22.0\u2003\u2003\u2003Add relativeDest option. Bump stylus to 0.52.\n2015-03-09\u2003\u2003\u2003v0.21.0\u2003\u2003\u2003Bump stylus to 0.50.\n2014-09-22\u2003\u2003\u2003v0.20.0\u2003\u2003\u2003Bump stylus to 0.49.\n2014-08-26\u2003\u2003\u2003v0.19.0\u2003\u2003\u2003Bump dependencies. Bump stylus to 0.48.\n2014-07-02\u2003\u2003\u2003v0.18.0\u2003\u2003\u2003Bump stylus to 0.47.\n2014-06-04\u2003\u2003\u2003v0.17.0\u2003\u2003\u2003Bump stylus to 0.46.\n2014-05-12\u2003\u2003\u2003v0.16.0\u2003\u2003\u2003Bump stylus to 0.45.\n2014-05-01\u2003\u2003\u2003v0.15.1\u2003\u2003\u2003Add support for rawDefine.\n2014-04-23\u2003\u2003\u2003v0.15.0\u2003\u2003\u2003Bump stylus to 0.44.\n2014-04-08\u2003\u2003\u2003v0.14.0\u2003\u2003\u2003Bump stylus to 0.43.\n2014-03-01\u2003\u2003\u2003v0.13.2\u2003\u2003\u2003Fix limit option for urlfunc. Update copyright to 2014.\n2014-02-27\u2003\u2003\u2003v0.13.1\u2003\u2003\u2003grunt.template.process is not needed.\n2014-02-22\u2003\u2003\u2003v0.13.0\u2003\u2003\u2003Adds Data URI Image Inlining options. Fix \"resolve url\" option. Use chalk module to colorize terminal output. Emphasize spaces in object keys in the README.\n2014-01-08\u2003\u2003\u2003v0.12.0\u2003\u2003\u2003Update to stylus 0.42.0.\n2013-12-02\u2003\u2003\u2003v0.11.0\u2003\u2003\u2003Update to stylus 0.41.0.\n2013-11-07\u2003\u2003\u2003v0.10.0\u2003\u2003\u2003Update to stylus 0.40.0 and nib 1.0.1.\n2013-10-20\u2003\u2003\u2003v0.9.0\u2003\u2003\u2003Update to stylus 0.38.0.\n2013-08-20\u2003\u2003\u2003v0.8.0\u2003\u2003\u2003Update to stylus 0.37.0 and nib to 1.0.0.\n2013-07-31\u2003\u2003\u2003v0.7.0\u2003\u2003\u2003Update to stylus 0.35.\n2013-07-11\u2003\u2003\u2003v0.6.0\u2003\u2003\u2003Update to stylus 0.33.\n2013-03-10\u2003\u2003\u2003v0.5.0\u2003\u2003\u2003Upgrade to stylus 0.32.1.\n2013-02-22\u2003\u2003\u2003v0.4.1\u2003\u2003\u2003Support stylus define option.\n2013-02-15\u2003\u2003\u2003v0.4.0\u2003\u2003\u2003First official release for Grunt 0.4.0.\n2013-01-23\u2003\u2003\u2003v0.4.0rc7\u2003\u2003\u2003Updating grunt\/gruntplugin dependencies to rc7. Changing in-development grunt\/gruntplugin dependency versions from tilde version ranges to specific versions.\n2013-01-09\u2003\u2003\u2003v0.4.0rc5\u2003\u2003\u2003Updating to work with grunt v0.4.0rc5. Switching to this.file API.\n2012-12-15\u2003\u2003\u2003v0.4.0a\u2003\u2003\u2003Conversion to grunt v0.4 conventions. Remove Node.js v0.6 and grunt v0.3 support. Merge grunt-stylus features (plugin loading, embedding). Remove experimental destination wildcards.\n2012-10-12\u2003\u2003\u2003v0.3.1\u2003\u2003\u2003Rename grunt-contrib-lib dep to grunt-lib-contrib.\n2012-09-24\u2003\u2003\u2003v0.3.0\u2003\u2003\u2003Options no longer accepted from global config key. Individually compile into dest, maintaining folder structure.\n2012-09-17\u2003\u2003\u2003v0.2.2\u2003\u2003\u2003Tests refactored, better watch integration.\n2012-09-10\u2003\u2003\u2003v0.2.0\u2003\u2003\u2003Refactored from grunt-contrib into individual repo.\n\n\nTask submitted by Eric Woroshow\nThis file was generated on Fri Mar 18 2016 21:06:50.\n","535":"Feeling RESTful Theme\nVersion 2\nA WordPress React JS theme updated with a design for A Day of Rest Boston 2017\n\nPurpose-built theme for the A Day of Rest Boston conference.\nThis is an update of the FeelingRestful v1 theme.\nDifferences between v1 and v2\nThere are some differences between the two.\nPlease note that version 1 has been release tagged.\nChanges\n\n\nAdded dynamic menu capabilities. Works with WordPress Menu settings. Has a built in menu walker in React.\n\n\nUpdated design modifications: ADOR Boston website which was a design change.\nThis theme has been visually redesigned for ADOR Boston. Therefore, there are CSS and HTML changes to enable that.\n\n\nRemoval of Preview_Postmeta\n\n\nBug fixes\n\n\nRequirements\nRequired homepage setup:\nA page with the slug \"home-page\" is used for the homepage. Please create this page if using a fresh installation.\nRequired Plugins\n\nWordPress version 4.7+ or WordPress REST API\nModular Page Builder\nWP-API Menus\n\nAdded support for\n\nTestimonials by WooThemes\n\nBuilding\nnpm install\ngrunt build\n\nDeveloping\nnpm install\ngrunt webpack:watch-dev # build javascript\ngrunt watch sass #\u00a0compile sass (watch does not trigger LiveReload on sass changes)\n\n","536":"Demopack\nFire up a pre-configured, live reloading Webpack server that supports React, (S)CSS and files for quick demos and tutorials.\nLike create-react-app, but without needing to create a new project and npm install all the things.\nSupports JavaScript, JSX, Sass, CSS Modules and images out of the box. 0 configuration required!\nCatch this quick demo on Youtube to learn more :).\nBut why?\nI love Webpack, and I love tools like create-react-app, but they come with some set up cost.\n\nFor Webpack, it takes a reasonable amount of time to configure and set it up. I love this for full projects, but for quick experiments\/demos, it's pretty slow.\nSimilarly create-react-app is great but the setup time in generating a new project can be relatively slow, especially on slow networks when you have to install all the dependencies!\n\nThe idea behind demopack is that it's a pre-packaged Webpack. It includes and configures everything you need, so all you have to do is install demopack once, and then when you run it there's no more setup.\nDemopack is not designed for fully featured projects, but for small demos or one off projects where you need no extra configuration. For \"proper\" projects I recommend either Webpack from scratch or a tool like create-react-app.\nInstallation\nPlease note that demopack is very new and I'm sure it might not work right for all people on all machines. Please open an issue if you have any problems or suggestions!\nYou should install demopack globally so you can easily run it in a directory.\nUnfortunately demopack is taken on npm, so you need to install the scoped version:\nnpm install --global @jackfranklin\/demopack\nyarn global add @jackfranklin\/demopack\n\nYou'll need to be running Node 8 or higher.\nUsage\nGiven a folder with the following file in it:\n\/\/ index.js\nconsole.log('Hello World!');\nRunning demopack in that directory will:\n\nFire up a Webpack server on port 8080 (or another free port).\nGenerate an index.html file that loads your JavaScript.\nParses any JavaScript (including JSX support) and SCSS for you.\n\nWhat Demopack supports\nDemopack has the following Webpack loaders configured out of the box:\n\nbabel-loader which parses .js and .jsx files. It runs using babel-preset-env, babel-preset-react and babel-preset-stage-0. This should mean any fancy JS you want to write should be supported :)\nsass-loader, css-loader and style-loader to load your CSS and SASS (but regular CSS works just fine). If you pass --css-modules CSS Modules will be supported.\nfile-loader to load gif, png, jpg and svg files.\n\nDemopack will run a small local server and will refresh automatically when you change files. Any errors are shown in the terminal and in the page.\nIf there's anything you think should be supported out of the box, feel free to open an issue and we can discuss it.\nProduction build\nThe primary goal of demopack is to be used for development, but you can generate a production build if you like with the --build flag.\nThis will run all the same plugins, with some extras (CSS is pulled into a separate file, JS & CSS is minified).\nThe build will output into .\/demopack-built and will contain an index.html file alongside any assets.\nConfiguration\nThe goal of demopack is to be preconfigured out of the box, but there are some options you can configure. Running demopack --help will show you these:\nOptions:\n  --help          Show help                                            [boolean]\n\n  --version       Show version number                                  [boolean]\n\n  --open-browser  Automatically open the browser when you run demopack. [default: true]\n\n  --css-modules   Enable CSS Modules support.                   [default: false]\n\n  --entry         The JavaScript file that demopack should build from. [default: \"index.js\"]\n\n  --build         Output static files into a directory. JS and CSS will be minified [default: false]\n\nChangelog\n0.2.0\n\nInitial release :)\n\nPrior art \/ credit\n\nThanks to Eduard and his work on create-elm-app which I took a bunch of Webpack code from to make the Demopack terminal output super tidy.\nThanks also to everyone behind create-react-app which was the inspiration behind Demopack initially.\n\n","537":"\ud83d\udea8 NO LONGER ACTIVE \ud83d\udea8\nTrolly\nA Python wrapper around the Trello API. Provides a group of Python classes to represent Trello Objects. None of the\nclasses cache values as they are designed to be inherited and extended to suit the needs of each user. Each class\nincludes a basic set of methods based on general use cases. This library was based on work done by\nsarumont. Very little was kept from this code, but still props on the initial\nwork.\nChanges\n0.2\nTrolly has recently been updated to start the deprecation of non-pythonic conventions and improve PEP8 compliance.\nAll the old pascal cased methods are now deprecated, they have been left in and point to the new method names. There\nare no breaking changes in this release but it is recommended you change your code to use the pythonic naming conventions.\nGetting Started\nDependencies\nThis library requires python 2.5+ or 3+.\nBefore getting stated with this library you will need a few extra things:\n\nhttplib2\nAn API key for your Trello user\nUser authorisation token ( see below for how to obtain )\n\nInstallation\nTo install for python 2 or 3 you can either download the source and run:\nsudo python setup.py install\n\nAlternatively you can use:\nsudo pip install Trolly\n\nAuthorisation\nUser Authorisation Token\nA user authorisation token isn't too hard to get hold of. There are instruction on how to get one on the\nTrello Documentation. For those too\nlazy there is a python class in the library called Authorise(). To use this class simply navigate to authorise.py in a\nterminal and type:\npython authorise.py -a API_KEY APPLICATION_NAME WHEN_TO_EXPIRE\n\nThe API key and application names are required but the \"WHEN_TO_EXPIRE\" will default to 1day if not specified. Running\nthis file will return a URL. Copy and paste it into your browser and away you go. You might want to store this somewhere\nfor future use, especially if you have set it to never expire.\nOauth\nThis library (currently) has no Oauth support however the code this was based on includes Oauth support. So for\ninspiration on how to extend the Client class to include this check out the link above.\nOverview\nThere are a number of methods for each of the Trello objects. Some accept query parameters, these are for API methods\nthat will accept a wide range of values and return a lot of information. For example\nGET Boards will take a lot of query parameters to\nallow you to whittle down the information to the bar minimum. This is extremely useful for extending the classes without\nmuch extra programming.\nTrello Client\nThis class holds the bulk of all the methods for communicating with the trello API and returning the Trello objects.\nA client instance is required by every Trello object, because of this it makes extending and overiding methods in this\nclass very effective. This is where you would override the creating of an object, e.g. a Card, with your own object.\nExample usage of the client:\nclient = trolly.client.Client(settings.API_KEY, settings.TOKEN)\n\nprint('Member: %s' % client.get_member())\n\nprint('Organisations:')\nfor organisation in client.get_organisations():\n    print(' - %s' % organisation)\n\nprint('Organisations:')\nfor organisation in client.get_organisations():\n    print(' - %s' % organisation)\n\nprint('Boards:')\nfor board in client.get_boards():\n    print(' - %s' % board)\n\nprint('Cards:')\nfor card in client.get_cards():\n    print(' - %s' % card)\n\n# Get all information from a card (works for boards, lists, etc. too):\nprint('Detailed cards:')\nfor card in client.get_cards(actions='all'):\n    print(' - %s: %s' % (card, card.data))\n\nTrello Object\nThis class is inhereted my all Trello object classes ( Board, List, Card, etc ). The class takes only one argument, a\nTrello client instance. It masks calls to the client as belonging to the class ( for the purposes of modularity ).\nThere are also a number of methods for fetching ( Board, List, etc ) JSON from the API. Since the only thing that\ndiffers from class to class is the base URI, this is taken as the only argument.\nThere are a number of methods for creating Trello objects. The createOBJECTNAME methods in this class can be extended\neasily by passing them keyword arguments.\nExtending Trello Classes\nExtending these classes is the premise on which they were built. Below outlines an example of how this can be acheived.\nIf for example we wanted to pass extra variables to our a Trello Card object then we can do the below:\nclass MyList( List ):\n\n    def __init__( self, trello_client, list_id, name = '' ):\n        super( MyList, self ).__init__( trello_client, list_id, name )\n\n    def getCards( self ):\n        cards = self.getCardsJson( self.base_uri )\n        return self.createCard( card_json = cards[0], test = 'this is a test argument' )\n\nThis MyList class overrides the getCards method to add the extra variable we need. This would need to be done to any\nTrello object that will return a custom card.\nWe declare and pass the extra ('test') variable as a keyword argument here. We then need to extend the card class to\nallow for the extra variables:\nclass MyCard( Card ):\n\n    def __init__( self, trello_client, card_id, test, name = '' ):\n        super( MyCard, self ).__init__( trello_client, card_id, name )\n        self.test_arg = test\n\nFinally, we extend and override the Client. Overriding the client means that any object that calls createCard will\ncreate one of our new client classes.\nclass MyClient( Client ):\n\n    def __init__( self, api_key, user_auth_token ):\n        super( MyClient, self ).__init__( api_key, user_auth_token )\n\n    def createCard( self, card_json, **kwargs ):\n\n        return MyCard( \n                trello_client = self,\n                card_id = card_json['id'],\n                name = card_json['name'],\n                test = kwargs['test']\n            )\n\nThe above client will fail though if you fail to pass a \"test\" keyword argument. To get around this you could use:\nkwargs.get('test',\"default value\")\n\nThis will help avoid a value not being passed. You could also, instead of extending the object creation, add\na method to cache the details you want using the objects getObjectInformation method.\nHope this helps and happy Trelloing!\nRunning Test\nIn order to run the tests you will need:\n\nAPI Key\nUser authorisation token\nAn organisation ID\nA board ID\nA list ID\nA card ID\nA checklist ID\nA member ID\n\nIt's quite a lot of information to get hold of (sorry). If you don't need everything you can just comment out\nthe tests you don't need.\nTo run the tests navigate to the Trolly in a terminal and run:\nPYTHONPATH=. python test\/tests.py\n\nLicence\nThis code is licenced under the MIT Licence\nContributors\n\nRick van Hatten\n\n","538":"Ansible Role: blockinfile\nThis role contains no tasks, but provides blockinfile module\nwhich might be useful when you want to maintain multi-line snippets\nin config files in \/etc.\nAnsible Galaxy Page: https:\/\/galaxy.ansible.com\/list#\/roles\/1475\nRequest for review:\nThe pull request to ansible-modules-extras\nhas been made to include blockinfile module\nin the official distribution of Ansible,\nwhich enables you to use blockinfile as a standard module without this role!\nIf you use this module and feel it's useful,\nplease leave some endorsement comments on the PR.\nI greatly appreciate if you're\nan eligible reviewer (existing module author)\nand could take some time to review the PR,\notherwise if you could ask reviewers of your acquiaintance for the review.\nIt needs two +1 votes from reviewers in order to be nominated for inclusion.\nblockinfile Module\nThis module will insert\/update\/remove a block of multi-line text\nsurrounded by the marker lines.\nExample task:\n- blockinfile:\n    dest: \/etc\/network\/interfaces\n    block: |\n      iface eth0 inet static\n          address 192.168.0.1\n          netmask 255.255.255.0\nText inserted\/updated by the task in \/etc\/network\/interfaces:\n# BEGIN ANSIBLE MANAGED BLOCK\niface eth0 inet static\n    address 192.168.0.1\n    netmask 255.255.255.0\n# END ANSIBLE MANAGED BLOCK\n\nIt uses marker lines # {BEGIN\/END} ANSIBLE MANAGED BLOCK as default.\nYou can specify alternative marker lines by marker option\nwhen you need to update files in other formats like HTML,\nor run multiple blockinfile tasks on the same file.\nOptions\nIf this section doesn't show nicely in Ansible Galaxy Page,\nplease refer to equivalent in\nGitHub Page.\n\n\n\nparameter\nrequired\ndefault\nchoices\ncomments\n\n\nbackup\nno\nno\nyesno\nCreate a backup file including the timestamp information so you can get the original file back if you somehow clobbered it incorrectly.\n\nblock\nno\n\n\nThe text to insert inside the marker lines. If it's missing or an empty string, the block will be removed as if state were specified to absent.\naliases: content\n\ncreate\nno\nno\nyesno\nCreate a new file if it doesn't exist.\n\ndest\nyes\n\n\nThe file to modify.\naliases: name, destfile\n\nfollow (added in 1.8)\nno\nno\nyesno\nThis flag indicates that filesystem links, if they exist, should be followed.\n\ngroup\nno\n\n\nname of the group that should own the file\/directory, as would be fed to chown\n\ninsertafter\nno\nEOF\nEOF*regex*\nIf specified, the block will be inserted after the last match of specified regular expression. A special value is available; EOF for inserting the block at the end of the file.  If specified regular expresion has no matches, EOF will be used instead.\n\ninsertbefore\nno\n\nBOF*regex*\nIf specified, the block will be inserted before the last match of specified regular expression. A special value is available; BOF for inserting the block at the beginning of the file.  If specified regular expresion has no matches, the block will be inserted at the end of the file.\n\nmarker\nno\n# {mark} ANSIBLE MANAGED BLOCK\n\nThe marker line template. \"{mark}\" will be replaced with \"BEGIN\" or \"END\".\n\nmode\nno\n\n\nmode the file or directory should be. For those used to \/usr\/bin\/chmod remember that modes are actually octal numbers (like 0644). Leaving off the leading zero will likely have unexpected results. As of version 1.8, the mode may be specified as a symbolic mode (for example, u+rwx or u=rw,g=r,o=r).\n\nowner\nno\n\n\nname of the user that should own the file\/directory, as would be fed to chown\n\nselevel\nno\ns0\n\nlevel part of the SELinux file context. This is the MLS\/MCS attribute, sometimes known as the range. _default feature works as for seuser.\n\nserole\nno\n\n\nrole part of SELinux file context, _default feature works as for seuser.\n\nsetype\nno\n\n\ntype part of SELinux file context, _default feature works as for seuser.\n\nseuser\nno\n\n\nuser part of SELinux file context. Will default to system policy, if applicable. If set to _default, it will use the user portion of the policy if available\n\nstate\nno\npresent\npresentabsent\nWhether the block should be there or not.\n\nvalidate\nno\nNone\n\nThe validation command to run before copying into place. The path to the file to validate is passed in via '%s' which must be present as in the example below. The command is passed securely so shell features like expansion and pipes won't work.\n\nExamples\n- name: insert\/update \"Match User\" configuation block in \/etc\/ssh\/sshd_config\n  blockinfile:\n    dest: \/etc\/ssh\/sshd_config\n    block: |\n      Match User ansible-agent\n      PasswordAuthentication no\n- name: insert\/update eth0 configuration stanza in \/etc\/network\/interfaces\n        (it might be better to copy files into \/etc\/network\/interfaces.d\/)\n  blockinfile:\n    dest: \/etc\/network\/interfaces\n    block: |\n      iface eth0 inet static\n          address 192.168.0.1\n          netmask 255.255.255.0\n- name: insert\/update HTML surrounded by custom markers after <body> line\n  blockinfile:\n    dest: \/var\/www\/html\/index.html\n    marker: \"<!-- {mark} ANSIBLE MANAGED BLOCK -->\"\n    insertafter: \"<body>\"\n    content: |\n      <h1>Welcome to {{ansible_hostname}}<\/h1>\n      <p>Last updated on {{ansible_date_time.iso8601}}<\/p>\n- name: remove HTML as well as surrounding markers\n  blockinfile:\n    dest: \/var\/www\/html\/index.html\n    marker: \"<!-- {mark} ANSIBLE MANAGED BLOCK -->\"\n    content: \"\"\nRequirements\nNone.\nRole Variables\nNone.\nDependencies\nNone.\nExample Playbook\nComplete playbook\nthat makes SSH password authentication for specific user prohibited,\nthen restarts sshd if needed.\n---\n- hosts: all\n  remote_user: ansible-agent\n  sudo: yes\n  roles:\n    - yaegashi.blockinfile\n  tasks:\n    - name: Prohibit SSH password authentication for $SUDO_USER\n      blockinfile:\n        dest: \/etc\/ssh\/sshd_config\n        backup: yes\n        content: |\n          Match User {{ansible_env.SUDO_USER}}\n          PasswordAuthentication no\n      notify: Restart sshd\n  handlers:\n    - name: Restart sshd\n      service\n        name: ssh\n        state: restarted\nLicense\nGPLv3+\nAuthor Information\nYAEGASHI Takeshi\n","539":"FragmentStack\nA helper class for managing a stack of Fragments in a single container.\nLicense\nCopyright 2013 Simon Vig Therkildsen\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n","540":"DragArea\nThis is an android library project that provides drag and drop operations\non top of android-10.\nStarting in android-11 (Android 3.0 \/ Honeycomb) Google provided drag and\ndrop API's of their own. This library is based loosely on those API's\nand so may be useful to people wishing to develop for Android 2.0 who\nwant an easyish upgrade path.\nDocumentation is available at\nhttp:\/\/doffm.github.com\/android-dragarea\/.\nA small example using this library can be found at\nhttps:\/\/github.com\/doffm\/android-dragexample.\n","541":"IMClient\n\u57fa\u4e8eXMPP\u534f\u8bae\u7684 Android \u5373\u65f6\u901a\u8baf\u5ba2\u6237\u7aef\n\u5373\u65f6\u901a\u8baf\u516c\u5f00\u8bfe\n1.\u5730\u5740\uff1ahttp:\/\/pan.baidu.com\/s\/1qWnYNyG\n2.\u5bc6\u7801\uff1ah7yu\n3.\u4ea4\u6d41\u7fa4\uff1a324632947\n\n\u5982\u4f55\u4f7f\u7528\n\u66f4\u6539IM.java\u91cc\u7684HOST\u4e3a\u4f60\u7684openfire\u5730\u5740\npublic static final String HOST = \"192.168.1.123\";\n##\u622a\u56fe\n\n\n\n","542":"JGet - Download files from anywhere\n\n\nGetting Started\nThis is basic java downloader library, analog to wget\nInstalling\nStep 1: Add to maven dependencies\n        <dependency>\n            <groupId>com.github.elvinmahmudov<\/groupId>\n            <artifactId>jget<\/artifactId>\n            <version>1.1<\/version>\n        <\/dependency>\n\nStep 2: Create JGet instance and add task to it\npublic class Main {\n\n    private static String MODEL_NAME = \"com.elvinmahmudov.dynocom.MathTeacher\";\n\n    public static void main(String[] args) throws IOException {\n        JGet jget = JGet.getInstance();\n        String url = \"https:\/\/file-examples-com.github.io\/uploads\/2017\/04\/file_example_MP4_480_1_5MG.mp4\";\n        String saveDirectory = \"download\";\n\n        JTask task = new JTask(url, saveDirectory, \"test.mp4\");\n        jget.addTask(task);\n        jget.start();\n    }\n\n}\n\n","543":"THIS DOESN'T WORK anymore with newer Catalyst drivers (since 15.7)\nOpenEncodeVFW\nVFW encoder for AMD VCE h264 encoder. Usable with Virtualdub, Dxtory etc.\nhttps:\/\/github.com\/jackun\/openencodevfw\/archive\/master.zip\nExtra settings are saved to registry under HKCU\\Software\\OpenEncodeVFW\nAs OpenEncode has been deprecated by AMD for a long time already, it appears the support has been finally dropped from Catalyst 15.7\nYou may need to install MSVC++ 2013 runtimes.\nLast MSVC++2010 commit.\nNOTE: You need to install x86 version for 32bit codec even if your Windows is 64 bit.\nNOTE: VCE on cards\/APUs prior to Tonga only go up to 1080p and solid 1080p60 recording can not be guaranteed (yet) unfortunately.\nCompatible hardware\nAMD's GCN based cards and APUs.\nFrom AMD's blog:\n\n\n\nVCE Version\nProduct Family\nDistinguishing Features\n\n\n\n\nVCE 1.0\nRadeon HD 7900 series\/Radeon R9 280X dGPU\nFirst release: AVC \u2013 I,P and DEM\n\n\n\nRadeon HD 7800 series dGPU\n\n\n\n\nRadeon R9 270X\/270 dGPU\n\n\n\n\nRadeon HD 7700 series\/Radeon R7 250X dGPU\n\n\n\n\nA10 \u2013 58XX (and other variations) APU\n\n\n\n\nA10 \u2013 68XX APU\n\n\n\n\n\n\n\n\nVCE 2.0\nRadeon R9 390x\/390\/290x\/290 dGPU\nSVC (temporal) + B-pictures + DEM improvements\n\n\n\nRadeon R7 260X\/260 dGPU\n\n\n\n\nA10 \u2013 7850K APU\n\n\n\n\nA4-5350, A4-3850, or E1-2650 APU\n\n\n\n\nA4-1200\/A6-1450 APU\n\n\n\n\n\n\n\n\nVCE 3.0\nRadeon R9 Fury\/285 dGPU\n4K\n\n\n\nInstalling\n\nUnpack the archive somewhere, right click on install.bat and click Run as Administrator.\n\nIf it complains about missing files, try the more manual version:\n\nUnpack the archive somewhere, open command prompt as administrator by typing cmd to start menu or \"Metro\" and press SHIFT+CTRL+Enter or right click on the icon and click Run as Administrator.\nGo to unpacked folder by typing into opened command prompt cd some\\where\\OpenEncodeVFW-bin.\nType install.bat and press enter to run the installer.\n\nUninstalling\nIf uninstaller fails its job, manually remove these registry keys:\nHKLM\\SYSTEM\\CurrentControlSet\\Control\\MediaResources\\icm\\VIDC.H264\nHKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\drivers.desc\\OPENENCODEVFW.DLL\nHKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Drivers32\\VIDC.H264\nHKLM\\Software\\Wow6432Node\\Microsoft\\Windows NT\\CurrentVersion\\drivers.desc\\OPENENCODEVFW.DLL\nHKLM\\Software\\Wow6432Node\\Microsoft\\Windows NT\\CurrentVersion\\Drivers32\\VIDC.H264\n\nand OPENENCODEVFW.DLL in %WINDIR%\\syswow64 or %WINDIR%\\system32\nRecommended usage\n\n32 bit input format\nwidth\/height multiples of 2\n\nSome setting descriptions\n\nFixed QP basically keeps picture quality constant across all frames.\nCBR keeps constant bitrate so picture quality gets worse if there is frequently fast motion in video and bitrate is too low or wastes harddrive space if frame could have been compressed more. Seems to fluctuate too much though.\nVBR uses variable bitrate, tries to keep in target bitrate but rises bitrate a little bit if needed or lowers if frame can be compressed more.\nCABAC is more efficient and resource intensive encoding option.\nSearch range is motion vector range. Specifies how wide the codec looks for moved pixels so it can just say that these pixels moved to x,y and just save that. Higher (max 36?) is better and more resource intensive encoding option.\nProfiles \/ levels: start from http:\/\/en.wikipedia.org\/wiki\/H.264\/MPEG-4_AVC#Profiles . Colorspace is limited to Y'UV420.\n\nProbably not very accurate descriptions :P\nAlso:\n\nSend FPS sets encoder framerate properties to video framerate, but not all framerates are supported by encoder. Untick to treat all videos as having 30 fps, but this may make encoding inefficient and increase bitrate more than necessary.\nSpeedy Math tries to speed up OpenCL floating point math by making it less accurate, but should be good enough.\nSwitch byte order : for the rare case when input bitmap is RGB(A) instead of BGR(A).\nHeader insertion : adds SPS\/PPS to every frame, may make cutting\/splitting video easier. More of a 'debug' feature.\n\nQuickset buttons for speed vs quality:\n\nSpeed : encodes 1080p at 60+ fps (theoretical max 80+)\nBalanced : encodes 1080p at 40+ fps\nQuality : encodes 1080p at 30+ fps (can probably do 720p@60)\n\nWith newer AMD cards (hawaii+), seem to support B-frames, though VCE may not actually generate B-frames with OpenVideo, and AVI kinda sucks with these (see). You may need to remux to MKV\/MP4 for better audio\/video sync.\n(Also maybe ffmpeg -fflags +genpts)\n","544":"OctaForge\nTHIS REPOSITORY IS ARCHIVED AND DEPRECATED.\nDevelopment continues on https:\/\/git.octaforge.org.\nFor installation, read INSTALL.md.\nRunning depends on the platform.\nOn Unix-like systems (OS X only when compiled with the provided Makefile), run\nit using bin_unix\/client_YOUROS_YOURARCH from either the root directory or\nbin_unix.\nOn OS X, you can use the way above and you can also run it from within the\nXcode project (development builds) or you can run the generated app bundle\nafter deploying (the .dmg file also contains gamedata and is ready to ship).\nOn Windows, execute bin_winYOURARCH\\client_win_YOURARCH.exe either from\nthe root directory or from the directory with the binary.\nSamee goes for server, just change \"client\" to \"server\". Using the -d2 option\nto client binary you can get the same dedicated server too (-d1 will start a\nlistenserver).\nIn case of problems, delete contents of your OF home directory\n($HOME\/.octaforge on Unix-like operating systems,\nDocuments\\My Games\\OctaForge on Windows)\nThe variable \"game\" influences what gamescript will be run. In local sessions\nit's used directly, but when connecting to a server is involved, the server\nwill send the gamescript name to the client (on the server the same variable\nis used to specify it; modify server-init.cfg appropriately).\nExample:\n\/game drawing.basic\n\nThis results in media\/scripts\/gamescripts\/drawing\/basic.oct being run.\nThere is just one map, \"test\", bundled by default. Use \/newmap to create a\nnew map, just like in Tesseract.\nIf a problem persists, report it into our\nissue tracker: https:\/\/github.com\/OctaForge\/OF-Engine\/issues\nhttp:\/\/octaforge.org\/\n","545":"OctaForge\nTHIS REPOSITORY IS ARCHIVED AND DEPRECATED.\nDevelopment continues on https:\/\/git.octaforge.org.\nFor installation, read INSTALL.md.\nRunning depends on the platform.\nOn Unix-like systems (OS X only when compiled with the provided Makefile), run\nit using bin_unix\/client_YOUROS_YOURARCH from either the root directory or\nbin_unix.\nOn OS X, you can use the way above and you can also run it from within the\nXcode project (development builds) or you can run the generated app bundle\nafter deploying (the .dmg file also contains gamedata and is ready to ship).\nOn Windows, execute bin_winYOURARCH\\client_win_YOURARCH.exe either from\nthe root directory or from the directory with the binary.\nSamee goes for server, just change \"client\" to \"server\". Using the -d2 option\nto client binary you can get the same dedicated server too (-d1 will start a\nlistenserver).\nIn case of problems, delete contents of your OF home directory\n($HOME\/.octaforge on Unix-like operating systems,\nDocuments\\My Games\\OctaForge on Windows)\nThe variable \"game\" influences what gamescript will be run. In local sessions\nit's used directly, but when connecting to a server is involved, the server\nwill send the gamescript name to the client (on the server the same variable\nis used to specify it; modify server-init.cfg appropriately).\nExample:\n\/game drawing.basic\n\nThis results in media\/scripts\/gamescripts\/drawing\/basic.oct being run.\nThere is just one map, \"test\", bundled by default. Use \/newmap to create a\nnew map, just like in Tesseract.\nIf a problem persists, report it into our\nissue tracker: https:\/\/github.com\/OctaForge\/OF-Engine\/issues\nhttp:\/\/octaforge.org\/\n","546":"rdis\nC++ code for the RDIS algorithm from \"Recursive Decomposition for Nonconvex Optimization.\" Friesen and Domingos, IJCAI 2015.\nMain platform this was tested on: Mac OS X 10.9 64 bit\nOther platforms this hopefully works on: Mac OS X 32 bit, Linux 32 bit & 64 bit\nBinaries are included for Mac OS X 64 bit (built on 10.9 Yosemite) in rdis\/bin\/macosx64, otherwise you will have to build them (see below).\nTo build (if all dependencies are accessible from default system paths), simply call the following:\ncmake .\nmake -j8\nRDIS builds in <BASE_DIR>\/build and there should be three executables: testRDIS, optBA, and optSinusoid.\n\ntestRDIS: run the debug tests (if no parameters are specified on the command line), or optimize a polynomial specified in a file.\ne.g., run .\/build\/testRDIS\ne.g., run .\/build\/testRDIS -f data\/testpoly.txt\n\nsee the notes in data\/testpoly.txt for writing your own polynomial for RDIS to optimize\n\n\noptBA: optimize a bundle adjustment function;\ne.g., run .\/build\/optBA -f data\/ladybug-problem-49-7776-pre.txt --ncams 5 --npts 30\noptSinusoid: optimize a high-dimensional sinusoid\ne.g., run .\/build\/optSinusoid\ncalling .\/build\/<executable> --help will print the command line options\n\nDependencies:\n\nNOTE: cmake vars EXT_INCL_DIR and EXT_LIB_DIR specify paths in which the build should look for the following libraries (if they're enabled) except for boost, instead of specifying each one for each library.\nBoost 1.55.0 (required, not included) (or potentially later, but 1.55.0 is tested) from http:\/\/www.boost.org\/\n\nRequired boost libs: system, filesystem, program_options, and chrono.\nYou must build these yourself. If the paths to the boost include folder and libs are non-standard (e.g., not in \/usr\/local\/include or \/usr\/include), then you can specify the paths when calling cmake as cmake -D BOOST_ROOT=\/path\/to\/boost . or you can specify BOOST_INCLUDEDIR and BOOST_LIBRARYDIR separately.\n\nBuilding these is easy; simply follow the Getting Started instructions at www.boost.org.\n\n\n\n\nPaToH (not required, included, used by default) hypergraph partitioning library from http:\/\/bmi.osu.edu\/umit\/software.html.\n\nPre-built libs are included in the repo already, so this should (ideally) just work.\nCan be disabled when calling cmake with cmake -D USE_PATOH=FALSE .\nYou can change the search path by setting the cmake variable PATOH_DIR.\n\n\nHMetis (not required, not included) hypergraph partitioning library from http:\/\/glaros.dtc.umn.edu\/gkhome\/metis\/hmetis\/overview.\n\nNothing is included, but once you download it you can specify the path in cmake variable HMETIS_DIR.\n\n\nlevmar (not required, not included) Levenberg-Marquardt optimizer from http:\/\/users.ics.forth.gr\/~lourakis\/levmar\/.\n\nNothing is included, but once you download it you can specify the path in cmake variable LEVMAR_DIR.\nNote that this library requires that LAPACK (and BLAS) are installed and work on your system.\n\n\n\nKnown Issues:\n\nIf the minimum is on the border of the domain, RDIS will do poorly because it does not handle constraints properly (a non-infinite domain constitutes box constraints on the function).\n\n","547":"rdis\nC++ code for the RDIS algorithm from \"Recursive Decomposition for Nonconvex Optimization.\" Friesen and Domingos, IJCAI 2015.\nMain platform this was tested on: Mac OS X 10.9 64 bit\nOther platforms this hopefully works on: Mac OS X 32 bit, Linux 32 bit & 64 bit\nBinaries are included for Mac OS X 64 bit (built on 10.9 Yosemite) in rdis\/bin\/macosx64, otherwise you will have to build them (see below).\nTo build (if all dependencies are accessible from default system paths), simply call the following:\ncmake .\nmake -j8\nRDIS builds in <BASE_DIR>\/build and there should be three executables: testRDIS, optBA, and optSinusoid.\n\ntestRDIS: run the debug tests (if no parameters are specified on the command line), or optimize a polynomial specified in a file.\ne.g., run .\/build\/testRDIS\ne.g., run .\/build\/testRDIS -f data\/testpoly.txt\n\nsee the notes in data\/testpoly.txt for writing your own polynomial for RDIS to optimize\n\n\noptBA: optimize a bundle adjustment function;\ne.g., run .\/build\/optBA -f data\/ladybug-problem-49-7776-pre.txt --ncams 5 --npts 30\noptSinusoid: optimize a high-dimensional sinusoid\ne.g., run .\/build\/optSinusoid\ncalling .\/build\/<executable> --help will print the command line options\n\nDependencies:\n\nNOTE: cmake vars EXT_INCL_DIR and EXT_LIB_DIR specify paths in which the build should look for the following libraries (if they're enabled) except for boost, instead of specifying each one for each library.\nBoost 1.55.0 (required, not included) (or potentially later, but 1.55.0 is tested) from http:\/\/www.boost.org\/\n\nRequired boost libs: system, filesystem, program_options, and chrono.\nYou must build these yourself. If the paths to the boost include folder and libs are non-standard (e.g., not in \/usr\/local\/include or \/usr\/include), then you can specify the paths when calling cmake as cmake -D BOOST_ROOT=\/path\/to\/boost . or you can specify BOOST_INCLUDEDIR and BOOST_LIBRARYDIR separately.\n\nBuilding these is easy; simply follow the Getting Started instructions at www.boost.org.\n\n\n\n\nPaToH (not required, included, used by default) hypergraph partitioning library from http:\/\/bmi.osu.edu\/umit\/software.html.\n\nPre-built libs are included in the repo already, so this should (ideally) just work.\nCan be disabled when calling cmake with cmake -D USE_PATOH=FALSE .\nYou can change the search path by setting the cmake variable PATOH_DIR.\n\n\nHMetis (not required, not included) hypergraph partitioning library from http:\/\/glaros.dtc.umn.edu\/gkhome\/metis\/hmetis\/overview.\n\nNothing is included, but once you download it you can specify the path in cmake variable HMETIS_DIR.\n\n\nlevmar (not required, not included) Levenberg-Marquardt optimizer from http:\/\/users.ics.forth.gr\/~lourakis\/levmar\/.\n\nNothing is included, but once you download it you can specify the path in cmake variable LEVMAR_DIR.\nNote that this library requires that LAPACK (and BLAS) are installed and work on your system.\n\n\n\nKnown Issues:\n\nIf the minimum is on the border of the domain, RDIS will do poorly because it does not handle constraints properly (a non-infinite domain constitutes box constraints on the function).\n\n","548":"Purpose\nExpose opencv to the node environment.\nFeatures\n- cv::Point, cv::Size, cv::Rect, etc replaced by object notation:\n\tPoint -> {x: 0, y:0}, Size -> {width: 33, height: 33}\n\n- Checks the types of parameters as well as ranges of values on each native call (harder to crash app from script)\n- Friendly exception messages on invalid parameters \n\nExample\nSee scripts\/effects.coffee for a full demo.\nInstallation - MacOS X\n$ brew install opencv --build32\n$ npm install -g opencv-node\n$ coffee scripts\/tests\n\n(tested with GCC 4.2.1 and node 0.8.0)\nTroubleshooting\nIf brew complains \"SHA1 mismatch\" error you may find that updating homebrew fixes the issue:\n$ brew update\nIf brew complains \"No available formula\" you will need to tap the science repository:\n$ brew tap homebrew\/science\nInstallation - Windows\n\n\nDownload OpenCV from http:\/\/sourceforge.net\/projects\/opencvlibrary\/files\/latest\/download\n\n\nExtract it to a folder, eg C:\\OpenCV\n\n\nOpen a Visual Studio command prompt and type\n set OPENCV_ROOT=C:\/OpenCV\n npm install -g opencv-node\n\n\n\n(tested with Visual Studio 2010 and node 0.8.8)\nAPI Differences\nSome functions have a more js-friendly API\/syntax.\nThe void functions which return their output in an argument passed by reference, return the result directly:\ncv::split returns an Array and takes only 1 argument\ncv::HoughCircles returns an Array\ncv::HoughLines returns an Array\ncv::cornerSubPix returns an Array\n\n* others?\n\nLicense\nBSD\nDisclaimer\nPlease report any bugs or missing functions. This module has never been used in production and is generally\nmeant to be used for experimentation.\n","549":"Purpose\nExpose opencv to the node environment.\nFeatures\n- cv::Point, cv::Size, cv::Rect, etc replaced by object notation:\n\tPoint -> {x: 0, y:0}, Size -> {width: 33, height: 33}\n\n- Checks the types of parameters as well as ranges of values on each native call (harder to crash app from script)\n- Friendly exception messages on invalid parameters \n\nExample\nSee scripts\/effects.coffee for a full demo.\nInstallation - MacOS X\n$ brew install opencv --build32\n$ npm install -g opencv-node\n$ coffee scripts\/tests\n\n(tested with GCC 4.2.1 and node 0.8.0)\nTroubleshooting\nIf brew complains \"SHA1 mismatch\" error you may find that updating homebrew fixes the issue:\n$ brew update\nIf brew complains \"No available formula\" you will need to tap the science repository:\n$ brew tap homebrew\/science\nInstallation - Windows\n\n\nDownload OpenCV from http:\/\/sourceforge.net\/projects\/opencvlibrary\/files\/latest\/download\n\n\nExtract it to a folder, eg C:\\OpenCV\n\n\nOpen a Visual Studio command prompt and type\n set OPENCV_ROOT=C:\/OpenCV\n npm install -g opencv-node\n\n\n\n(tested with Visual Studio 2010 and node 0.8.8)\nAPI Differences\nSome functions have a more js-friendly API\/syntax.\nThe void functions which return their output in an argument passed by reference, return the result directly:\ncv::split returns an Array and takes only 1 argument\ncv::HoughCircles returns an Array\ncv::HoughLines returns an Array\ncv::cornerSubPix returns an Array\n\n* others?\n\nLicense\nBSD\nDisclaimer\nPlease report any bugs or missing functions. This module has never been used in production and is generally\nmeant to be used for experimentation.\n","550":"cocos2dx-extensions\nExtensions for Cocos2dx:\n1.Dynamic CCLabelTTF - \u52a8\u6001\u663e\u793a\u6570\u5b57\n2.Gray Sprite - \u521b\u5efa\u7070\u8272\u56fe\n3.Turn Card - \u7ffb\u724c\u6548\u679c\n4.Zoom Controller - \u573a\u666f\u591a\u70b9\u805a\u7126\u7f29\u653e\n","551":"Motorola SMI\nXT890|Scorpion Mini Intel|Razr i\nMotorola Razr i Intel-atom\nSource from Turl, Oxavelar, Motorola, Intel, OPENSuse, Google(android) and HazouPH\n","552":"This repository is part of the codebender.cc maker and artist web platform.\nAnd what's that?\ncodebender comes to fill the need for reliable and easy to use tools for makers. A need that from our own experience could not be totally fulfilled by any of the existing solutions. Things like installing libraries, updating the software or installing the IDE can be quite a painful process.\nIn addition to the above, the limited features provided (e.g. insufficient highlighting, indentation and autocompletion) got us starting building codebender, a completely web-based IDE, that requires no installation and offers a great code editor. It also stores your sketches on the cloud.\nThat way, you can still access your sketches safely even if your laptop is stolen or your hard drive fails! codebender also takes care of compilation, giving you extremely descriptive warnings on terrible code. On top of that, when you are done, you can upload your code to your Arduino straight from the browser without installing anything.\nCurrently codebender.cc is running its beta and we are trying to fix issues that may (will) come up so that we can launch and offer our services to everyone!\nIf you like what we do you can also support our campaign on indiegogo to also get early access to codebender!\nHow do these \"arduino-files\" come into the picture?\ncodebender.cc runs on PHP Fog, a PaaS that helps us run our PHP projects in a fast and scallable manner. However, we need to run our compiler on some VPS.\nThe compiler repository includes all the necessary files needed to run the compiler as a service. This includes the arduino examples, libraries, etc. This is a repository that includes all of these necessary files, which the compiler repository then includes as a submodule.\n","553":"adventures-reactive-web-dev\nAdventures in Reactive Web Development.\nExploring various libraries, frameworks, and techniques for reactive web development, including:\n\nAngular 2\nCycle.js\nElm\nReact\n\nwith Redux\nwith RxJS\n\n\nYolk\n\nAlso part of the adventure:\n\nsnabbdom\nmost and most-subject\n\nRequirements\nYou need to have Node.js installed to run the examples.\nThere are several ways to install Node.js, including Homebrew and nvm.\nOne-time setup\nYou only need to run this command once:\nnpm i -g bower\n\nThis installs Bower. It is used to install the client-side CSS used in the examples - namely, Bootstrap. Technically, it's not required to run the examples, but it does make them look nicer.\nThe same server is used for every example. You only need to run this once:\n# from the top-level directory\nnpm i\nbower i\n\nOne-time setup per example\nFor each example that you wish to run, you also need to run these commands once:\n# from the top-level directory\ncd client-<example> # replace with directory of the example you want to run\nnpm i\n\nRunning the example\nTo run an example, open two terminal windows.\nIn the first terminal window:\n# from the top-level directory\nnpm start\n\nIn the second terminal window:\n# from the top-level directory\ncd client-<example> # replace with directory of the example you want to run\nnpm run dev\n\nViewing the example and making changes\nOpen http:\/\/localhost:3013 to view the example.\nYou can experiment with making changes by editing the code under the client-<example> directory. You only have to save the file to see your changes. Recompilation and browser page reload happen automatically.\nBranches\nThe master branch contains the most up-to-date code. However, you may be interested in other branches.\nSimply run git checkout <branchname> to check out a branch, replacing <branchname> with one of the following:\n\nng2-webpack : Angular 2 example using RxJS instead of Redux\nng2-systemjs : Angular 2 example using SystemJS instead of Webpack\nreact-router : React example using React-Router\nclient-elm branches : there are 4 branches for the\n4-part article\nabout the Elm example.\n\nFinally, please note that the examples use Webpack. I left the ng2-systemjs branch there for reference, but I had a lot of trouble using SystemJS and I did not like having to add a bunch of <script> tags. I prefer the Webpack setup, which includes automatic recompilation and page refresh, so once I had that set up and working, I went with it for the rest of the examples.\n","554":"Radial Bubble Tree\n![bubble tree screenshot](http:\/\/driven-by-data.net\/wp\/wp-content\/uploads\/2011\/06\/Bildschirmfoto-2011-07-02-um-23.02.50.png bubbletree screenshot right)\nBubbleTree is a library for interactive visualization of hierarchical data. Originally developed mainly for spending data, the library is now completely independent from the OpenSpending platform. BubbleTree is built on top of jQuery and RaphaelJS.\nDocumentation\nPlease refer to the docs in the wiki pages.\nCopyright and License\nCopyright (c) 2011,2012 Open Knowledge Foundation\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nAttribution Request\nIf you use Bubbletree to make a visualization we'd love it (but it's not required!) if you added a small credit text and link e.g.\nBuilt with <a href=\"http:\/\/okfn.org\/\">Open Knowledge Foundation's<\/a> <a href=\"https:\/\/github.com\/okfn\/bubbletree\">Bubbletree library<\/a>\n\nAuthors\nGregor Aisch with a very small amount of input from Rufus Pollock as part of work on the OpenSpending project and with financial support from Publish What You Fund and the Shuttleworth Foundation.\nUseful commands\n\nnpm run build to rebuild library and demos\nnpm run review to check code style\n\nInstallation\nnpm install bubbletree\nor\nbower install bubbletree\n","555":"Yeoman Generator for jQuery Boilerplate \n\nA Yeoman generator that provides a functional boilerplate to easily create jQuery plugins out of the box.\n\nUsage\n\nInstall the generator by running: npm install -g generator-jquery-boilerplate\nRun: yo jquery-boilerplate\nStart writing your jQuery plugin :)\n\nWanna know more?\nCheck jQuery Boilerplate's repository.\nHistory\nCheck Releases for detailed changelog.\nLicense\nMIT License \u00a9 Zeno Rocha\n","556":"see the homepage\nEasy WebSocket\nlike WebSocket but no server setup and available in any browser\nEasyWebSocket aims to make realtimes webapps in every browser without the trouble\nto setup Websocket servers.\nHow to use it\nInclude the following in your webpage and it just works.\n<script src=\"http:\/\/EasyWebsocket.org\/easyWebSocket.min.js\"><\/script>\t\n<script>\n    var socket = new EasyWebSocket(\"ws:\/\/example.com\/resource\");\n    socket.onopen\t= function(){\n        socket.send(\"hello world.\")\n    }\n    socket.onmessage= function(event){\n        alert(\"received\"+ event.data)\n    }\n<\/script>\n\nStep 1: You connect the socket to a given url\nStep 2: What you send() thru this socket is sent to all sockets connected the same url\nSee this code live. No server\nsetup, no cross-origin issue to care about... It is that easy!\n","557":"AngularJS - Animation Article\nRun with\n# run this from the root of the repo\nnpm install http-server\nhttp-server -p 8000 app\/\nYou can then access the website via:\nhttp:\/\/localhost:8000\/\nBlog Article\nThis article will teach you everything you need to know:\nhttp:\/\/www.yearofmoo.com\/2013\/04\/animation-in-angularjs.html\nDemo\nThe application can be demoed via:\nhttp:\/\/yearofmoo-articles.github.com\/angularjs-animation-article\/app\/\n","558":"grunt-protractor-runner\n\n\nA Grunt plugin for running Protractor runner.\n\nGetting Started\nThis plugin requires Grunt >=0.4.1.\nFor Protractor 5.x.x, please use version v5.x.x of this plugin.\nFor Protractor 4.x.x, please use version v4.x.x of this plugin.\nFor Protractor 3.x.x, please use version v3.x.x of this plugin.\nFor Protractor 2.x.x, please use version v2.x.x of this plugin.\nIf you haven't used Grunt before, be sure to check out the Getting Started guide, as it explains how to create a Gruntfile as well as install and use Grunt plugins. Once you're familiar with that process, you may install this plugin with this command:\nnpm install grunt-protractor-runner --save-dev\nThis plugin will install protractor module locally as a normal dependency.\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\ngrunt.loadNpmTasks('grunt-protractor-runner');\nFinally you need a Selenium server. If you don't have one set up already, you can install a local standalone version with this command:\n.\/node_modules\/grunt-protractor-runner\/scripts\/webdriver-manager-update\nThe \"protractor\" task\nOverview\nIn your project's Gruntfile, add a section named protractor to the data object passed into grunt.initConfig().\ngrunt.initConfig({\n  protractor: {\n    options: {\n      configFile: \"node_modules\/protractor\/example\/conf.js\", \/\/ Default config file\n      keepAlive: true, \/\/ If false, the grunt process stops when the test fails.\n      noColor: false, \/\/ If true, protractor will not use colors in its output.\n      args: {\n        \/\/ Arguments passed to the command\n      }\n    },\n    your_target: {   \/\/ Grunt requires at least one target to run so you can simply put 'all: {}' here too.\n      options: {\n        configFile: \"e2e.conf.js\", \/\/ Target-specific config file\n        args: {} \/\/ Target-specific arguments\n      }\n    },\n  },\n})\nOptions\noptions.configFile\nType: String\nDefault value: No default value\nA protractor config file.\noptions.keepAlive\nType: Boolean\nDefault value: false (true before v1.0.0)\nIf true, grunt process continues even if the test fails. This option is useful when using with grunt watch.\nIf false, grunt process stops when the test fails.\noptions.noColor\nType: Boolean\nDefault value: false\nIf true, protractor will not give colored output.\nIf false, protractor will give colored output, as it does by default.\noptions.debug\nType: Boolean\nDefault value: false\nIf true, grunt will pass 'debug' as second argument to protractor CLI to enable node CLI debugging as described in Protractor Debugging documentation.\noptions.args\nType: Object\nDefault value: {}\nArguments passed to the command. These arguments can also be supplied via command-line too. Ex.grunt protractor --specs=specs\/some-test.js  or for object options grunt protractor --cucumberOpts={\\\"tags\\\":\\\"@quick\\\"} or --params='{ \"location\" : { \"href\" : \"some url\" } }'\nPassing object argument with --params.xxx.yyy=zzz is not supported at the moment. If you need this behaviour, please join the discussion in #148 .\nSupported arguments are below.\n\nseleniumAddress string: A running selenium address to use\nseleniumServerJar string: Location of the standalone selenium server .jar file\nseleniumPort string: Optional port for the standalone selenium server\nbaseUrl string: URL to prepend to all relative paths\nrootElement string: Element housing ng-app, if not html or body\nspecs array: Array of spec files to test. Ex. [\"spec1.js\",\"spec2.js\"]\nexclude array: Array of files to exclude from testing. Ex. [\"spec2.js\"]\nsuite string or array: Suite or Array of suites to run. Ex. [\"suite1\", \"suite2\"]\nincludeStackTrace boolean: Print stack trace on error\nverbose boolean: Print full spec names\nbrowser string: Browser name, e.g. chrome or firefox\nparams object: Param object to be passed to the test as browser.params\nchromeDriver string: Location of chrome driver overridng the property in config file\ndirectConnect boolean: To connect directly to the browser Drivers. This option is only available for Firefox and Chrome.\nsauceUser string: Username for a SauceLabs account\nsauceKey string: Access Key for a SauceLabs account\nsauceSeleniumAddress string: Customize the URL Protractor uses to connect to sauce labs (for example, if you are tunneling selenium traffic through a sauce connect tunnel). Default is ondemand.saucelabs.com:80\/wd\/hub\ncapabilities object: Capabilities object to be passed to the test, e.g. browserName, platform and version\nframework string: Limited support for using mocha as the test framework instead of jasmine.\nframeworkPath string: When framework is set to custom, set this path relative to the config file or absolute\ncucumberOpts object: Cucumber framework options object to be passed to the test, e.g. require, tags and format\nmochaOpts object: Mocha test framework options object to be passed\nbeforeLaunch string: You can specify a file containing code to run once configs are read but before any environment setup. This will only run once, and before onPrepare.\nonPrepare string: You can specify a file containing code to run once protractor is ready and available, and before the specs are executed. If multiple capabilities are being run, this will run once per capability.\nwebDriverProxy string: WebDriver proxy configuration to run remote tests\n\noptions.output\nType: String\nDefault value: false\nThe file that the task should output the results to.\noptions.outputOptions\nType: Object\nDefault value: {}\nOptions for output file. For details see: fs.createWriteStream's options\noptions.nodeBin\nType: String\nDefault value: node\nPath to the node binary file. Useful if node is not on the PATH.\noptions.webdriverManagerUpdate\nType: Boolean\nDefault value: false\nIf true, webdriver-manager update will run and install\/update selenium driver.\nTests\nRun npm install to install dependencies.\nThen run grunt or npm test to test the module. You will encounter these.\n\nRuns unit and e2e tests\nIt opens chrome a couple of times without warnings or errors.\nA test task fails but the test process keeps alive and continues to the next test tasks.\n\nContributing\nIn lieu of a formal styleguide, take care to maintain the existing coding style. Add unit tests for any new or changed functionality. Lint and test your code using Grunt.\nFAQ\nQ: Want to global installed protractor?\nThis plugin installs protractor module locally as a normal dependency.\nIn case you want to use the plugin with the global installed protractor command. You can do it with these steps below.\n\nRemove local install protractor by rm -rf node_modules\/protractor\nInstall protractor globally  with npm install -g protractor\nMake sure that node can resolve the module with require() mechanism. See Module loading from the global folders for more information.\nRun webdriver-manager update to install\/update selenium driver for global install protractor.\n\nQ: Error: Could not find chromedriver at....\nYou need to install\/update selenium webdriver for protractor.\n\nRun webdriver-manager update or node scripts\/webdriver-manager-update or node .\/node_modules\/protractor\/bin\/webdriver-manager update\n\nRelease History\n\n\n5.0.0\n\nUpgrade protractor to version 5 (#185)\n\n\n\n4.0.0\n\nAccept array for suite argument (#172)\nUpgrade protractor to version 4 (#168)\n\n\n\n3.2.0\n\nSupport --frameworkPath in options.args (#155, #156)\nSupport grunt version >=0.4.0\" (#154)\n\n\n\n3.1.0\n\nAdd options.outputOptions (#143)\nSupport webDriverProxy in options.args (#147)\nRemove referenceConf.js as default value of options.configFile because it does not exist anymore\n\n\n\n3.0.0\n\nUpdate protractor to version 3\nUpdate other dependencies including through2 and split to latest version\n\n\n\n2.1.2\n\nFix boolean parameters in object.args.params (#130)\nModify unit tests to run nodeunit test faster and after protractor task\n\n\n\n2.1.1\n\nFix EINVAL error when run in git bash shell (#134)\n\n\n\n2.1.0\n\nAdd options.webdriverManagerUpdate option (#125)\nFix support for object option via command-line (#116)\n\n\n\n2.0.0\n\nUpgrade protractor to ^2.0.0 (#114)\nchromeOnly in options.args is deprecated. Replaced by directConnect (#114)\nSupport beforeLaunch and onPrepare in options.args (#110)\nWhen one of the tests fails, throw warning instead of fatal error so that grunt can still use --force to continue. (#103)\n\n\n\n1.2.1\n\nMove split and through2 from devDependencies to dependencies (#104)\n\n\n\n1.2.0\n\nAdd options.nodeBin to specify node binary (#96)\nSupport --directConnect and --sauceSeleniumAddress in options.args (#95, #101)\nAdd options.output (#80)\nMerge README.md PRs (#89, #91)\nFix plugin test for protractor>=v1.5.0\nFix TravisCI test\n\n\n\n1.1.4\n\nMove webdriver-manager update step from problematic postinstall to pretest\n\n\n\n1.1.3\n\nAttempt to fix webdriver-manager postinstall problem with webdriver-manager script (#83)\n\n\n\n1.1.2\n\nAttempt to fix webdriver-manager path in package.json postinstall\nAdd Travis CI build configuration\n\n\n\n1.1.1\n\nRun webdriver-manager update on postinstall (#41)\n\n\n\n1.1.0\n\nUpdate protractor to version 1.x.x\n\n\n\n1.0.1\n\nPass specified command line params to the subprocess (#68)\nMake npm test to run and handle interactive debugger by itself (#66)\nFixed argsTest\n\n\n\n1.0.0\n\nChange default value of options.keepAlive to false (#50)\n\n\n\n0.2.5\n\nSupport --mochaOpts, --suite and --exclude in options.args (#52, #53, #57)\n\n\n\n0.2.4\n\nSupport --cucumberOpts in options.args (#46)\n\n\n\n0.2.3\n\nTemporarily remove automatically download\/update webdriver-manager because it fails in some environment such as Windows (#41)\n\n\n\n0.2.2\n\nAdd protractor module as a normal dependency and automatically download\/update webdriver with webdriver-manager after installed (#29, #39)\nSupport --framework in options.args (#36)\n\n\n\n0.2.1\n\nSupport --capabilities in options.args (#33)\n\n\n\n0.2.0\n\nAble to use either local or global install protractor the same way as how require() function works (#29)\nMove protractor from peerDependencies to devDependencies. These changes might break some user modules. (See FAQ above for explanation) (#29)\n\n\n\n0.1.11 - Support SauceLabs account config in options.args (#27)\n\n\n0.1.10\n\nSupport --chromeOnly in options.args (#23)\nSupport options.noColor to turn color off in protractor output (#24)\n\n\n\n0.1.9\n\nAble to supply options.args via command-line arguments (#20)\nFixed merging task-level and target-level options\n\n\n\n0.1.8 - Support --chromeDriver in options.args (#17)\n\n\n0.1.7 - Support --browser and --params arguments passed to the protractor command using config in options.args (#12)\n\n\n0.1.6 - Change protractor(peerDependencies) to support version to 0.x (#8, #9, #10)\n\n\n0.1.5 - Added options.debug (#7)\n\n\n0.1.4 - Change protractor(peerDependencies) to support version to 0.10.x - 0.11.x (#6)\n\n\n0.1.3 - Fixed Windows command\n\n\n0.1.2 - Added keepAlive option.\n\n\n","559":"jquery.tweetable.js\nA simple li'l plugin that lets you make site content easily tweetable.\nInspired (and by inspired I mean I stole this) from a recent New York Times article doing the exact same thing.\nYou can see a demo of the plugin in action here.\nUsage\nDependencies\nI mean, jquery is in the name and everything ...\nThe Basics\nUsing the plugin is super duper easy:\n$('[data-tweetable]').tweetable();\nThat will grab all elements with the data-tweetable attribute and create clickable links out of them.  If the attribute in question has a value (e.g. data-tweetable='I love lamp'), the tweet's text will be set to that value; otherwise, it is set to whatever text is within the given element.\n(If you're using a selector which is not an attribute, then be sure to set dataAttr so it knows where to pick up the text.)\nThe links are unstyled by default, but those links also are created with a given class (by default tweetable) so you can style them to your heart's content.\nOptions\nOptions are pretty simple, and you can pass them as so:\n$('.awesome-text').tweetable({\n\tvia: 'justinmduke'\n});\nThe defaults:\n\/\/ Defaults\n{\n\tdataAttr: 'data-tweetable',\n\tlinkClass: 'tweetable',\n\tvia: null,\n\trelated: null,\n\turl: window.location.pathname\n}\nThe first two should be fairly obvious: via, related, and url all correspond to what you're passing to Twitter in terms of data.\n","560":"react-native-smart-scroll-view\n\n\nA pure JS React Native Component for IOS.\nA wrapper around react-native ScrollView to handle keyboard events and auto adjust input fields to be visible above keyboard on focus.\nTakes in your components and recursively searches for any component (i.e. TextInput) that is given smartScrollOptions as a prop. Further props are added to these components to ensure they are always visible above the keyboard and within the ScrollView when focused.\nThere is also the option to autofocus the next component with smartScrollOptions on TextInput submission, and the ability to autofocus any component by setting the smartScrollOptionsprops appropriately and specifying the index of the component (more info below) .\nGreat for use with forms which have multiple TextInput fields!\nGetting Started\n\nInstallation\nProperties\nExample Usage\nTODO\n\nInstallation\n$ npm i react-native-smart-scroll-view --save\nProperties\nIn wrapping around the ScrollView and using the TextInput to control keyboard we have used their native properties to create our functionality. You can still add most props to TextInputs and we will allow you to pass some props to the ScrollView but do so with care.\nSmartScrollView Props\n\n\n\nProp\nDefault\nType\nDescription\n\n\n\n\nforceFocusField\nundefined\nnumber or string\nForce scroll the view to the TextInput field at the specified index (smart children indexed in order from 0) or 'scrollRef' you have given to your smart child (see smartScrollOptions below)\n\n\nscrollContainerStyle\n{flex: 1}\nnumber\nStyle options for the View that wraps the ScrollView, the ScrollView will take up all available space.\n\n\nscrollPadding\n5\nnumber\nPadding between the top of the keyboard\/ScrollView and the focused TextInput field\n\n\ncontentContainerStyle\n{flex: 1}\nnumber\nSet to the ScrollView contentContainerStyle prop\n\n\nzoomScale\n1\nnumber\nSet to the ScrollView zoomScale prop\n\n\nshowsVerticalScrollIndicator\ntrue\nbool\nSet to the ScrollView showsVerticalScrollIndicator prop\n\n\ncontentInset\n{top: 0, left: 0, bottom: 0, right: 0}\nobject\nSet to the ScrollView contentInset prop\n\n\nonScroll\n() => {}\nfunc\nSet to the ScrollView onScroll function. It will be called alongside our own\n\n\nonRefFocus\n()=>{}\nfunc\nGives back the 'ref' of the node whenever a smart component is focused\n\n\n\nSmart Component Props\nSmart components can be the native 'TextInput' s, other component like 'View' s or your own custom components.\nFor each component that you would like to use, provide the prop smartScrollOptions alongside the normal props. Beware* some props of native components like TextInputs may be modified by the Smart Scroll View (see below).\nsmartScrollOptions - An object with the following keys:\n\n\n\nKey\nType\nDescription\n\n\n\n\ntype\nenum (text,custom)\nFor type 'text' the 'moveToNext' and 'onSubmitEditing' options can be set (see below). For type 'custom' further scrolling must be done by forcing the index\n\n\nmoveToNext\nbool\nIf true, the next TextInput field will be focused when the submit button on the keyboard is pressed. Should be set to false or omitted for the last input field on the page. Warning this will not work if keyboardType for the TextInput is set to 'number-pad', 'decimal-pad', 'phone-pad' or 'numeric' as they do not have a return key\n\n\nonSubmitEditing(next)\nfunc\nOptional function that takes a callback.  When invoked, the callback will focus the next TextInput field. If no function is specified the next TextInput field is focused. Example: (next) => { if (condition) { next() } }\n\n\nscrollRef\nstring\nTo be used in conjunction with the 'forceFocusField' prop of the 'SmartScrollView'. Use 'scrollRef' to reference a particular component which can then be set to forceFocusField to have control where the focus is\n\n\n\nHow We Modify TextInput Props\nFor any component which has 'smartScrollOptions.type = text', it is inferred that it is either a 'TextInput' component or contains a 'TextInput' component. The props of the enclosing 'TextInput' component are modified in the following way.\n\nWe attach our own onFocus function and will call yours alongside.\nIf moveToNext in smartScrollOptions is true:\n\nTheonSubmitEditing is replaced with our own. See above.\nblurOnSubmit is set to false\n\n\n\nExample Usage\nCode for the above gif is found here\nHere is another example of the smart-scroll-view in action.\n\nTo run the code yourself and play around, open and run the Xcode project.\nopen SuperScrollingFormExample\/ios\/SuperScrollingFormExample.xcodeproj\nTODO\n\nAllow for more types other than text input to have smart scroll functionality.\n\ni.e. a customisable picker component that can be used to replace keyboard to allow the user to select a value from a picker.\nAny image, button, slider....\n\n\nAllow for header\/banner above keyboard.\nBetter animations....\nYour issues\/suggestions!\n\nFeel free to comment, question, create issues, submit PRs... to make this view even smarter\n","561":"highlight\nA simple, pluggable API for syntax highlighting.\nSyntax highlighters tend to have pretty opinionated APIs, both in terms of when to highlight, and how to determine the language. And lots bundle the languages directly into the core library, which makes it much harder to reason about them individually, or to have the smallest possible file size if you don't need the esoteric ones.\nSo... we made this one. The API is very simple, yet still gives you full control. The language definitions are all separate plugins, so you get the smallest possible build size, and so that they're simpler for everyone to contribute to. Because regexes are already hard enough to read as it is!\nInstallation\n$ component install segmentio\/highlight\n\nExample\nvar Highlight = require('highlight')\nvar html = require('highlight-xml');\nvar js = require('highlight-javascript');\n\nvar highlight = new Highlight()\n  .use(html)\n  .use(js);\n\nvar el = document.querySelector('.code-sample');\nhighlight.element(el);\n...or if you're lazy, you can just pass a selector string:\nhighlight.element('.code-sample');\n...or if you're incredibly lazy, you can just highlight everything:\nhighlight.all();\nLanguages\n\nBash\nCSS\nC#\nGo\nHTML\nJava\nJavascript\nJSON\n.NET\nObjective-C\nPHP\nPython\nRuby\nSQL\nXML\nYAML\n\nAPI\nnew Highlight()\nCreate a new Highlight instance.\n#use(plugin)\nApply a plugin function, for example language syntaxes.\n#string(string, language)\nHighlight a string of code of a given language.\n#element(el, [language])\nHighlight an el. If you don't pass a language, it will use the data-language attribute:\n<pre data-language=\"css\"><code>YOUR CODE HERE<\/code><\/pre>\n#elements(els, [language])\nHighlight a series of els.\n#all()\nHighlight all of the elements in the DOM that have a data-language attribute.\n#prefix(string)\nSet the CSS class name prefix string.\n#language(name, grammar)\nDefine a new language by name with a grammar.\n#parse(string, language)\nReturn an AST for a given string and language.\n#stringify(ast)\nConvert an AST into a string of HTML.\nLicense\nThe MIT License\nCopyright \u00a9 2014 Segment.io\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","562":"es6-react-mixins\n \nes6-react-mixins is a module that lets you augment your ES6 React component classes with any number of custom ES6 mixins. You can also use it to merge traditional pre-ES6 React mixin objects into your ES6 React classes.\nInspired by this gist by Sebastian Markb\u00e5ge the strategy is transient class hierarchies \u2013 instead of locking classes into permanent is a roles, class realtionships are assembled and re-assembled at will.\nES6 mixins are functions that return classes. The base parameter is used internally to construct the mixin chain. There's no need to extend React.component, you get that for free.\nconst es6Mixin = base => class extends base {\n  componentWillMount() {\n    console.log(\"augmented componentWillMount\");\n  }\n  render() {\n    console.log(\"augmented render\");\n  }\n};\nReact components invoke mixins with a call to super.\nimport mixin from 'es6-react-mixins';\nimport React from 'react';\n\nclass MyComponent extends mixin(es6Mixin) {\n  componentWillMount() {\n    super.componentWillMount();\n  }\n  render() {\n    super.render();\n    return <div>hello\/div>;\n  }\n}\n\nReact.render(<MyComponent>, document.body);\nThe API works with any number of mixins. Obviously order matters with multiple mixins \u2013 each super call works its way up the mixin hierarchy.\nconst mixin1 = base => class extends base {\n  componentWillMount() {\n    super.componentWillMount();\n    console.log(\"mixin1 componentWillMount\");\n  }\n  render() {\n    super.render();\n    console.log(\"mixin1 render\");\n  }\n};\n\nconst mixin2 = base => class extends base {\n  componentWillMount() {\n    super.componentWillMount();\n    console.log(\"mixin2 componentWillMount\");\n  }\n  render() {\n    super.render();\n    console.log(\"mixin2 render\");\n  }\n};\n\nclass MyComponent extends mixin(mixin1, mixin2) {\n  componentWillMount() {\n    super.componentWillMount();\n  }\n  render() {\n    super.render();\n    return <div>hello\/div>;\n  }\n}\nNotice that any mixin can call super, even though there may be no other mixins to hear it. Every mixin descends from a base mixin with no-op implementations of the react lifecycle methods.\nes6-react-mixins also accepts traditional plain object mixins (a la pre-ES6 React), adapting them to ES6 style mixins internally.\nvar reactMixin = {\n  componentWillMount: function () {\n    console.log\n  },\n  render: function () {\n    this.reactMixin_rendered = true;\n  }\n};\n\nclass MyComponent extends mixin(reactMixin) {\n  componentWillMount() {\n    super.componentWillMount();\n  }\n  render() {\n    super.render();\n    return <div>hello\/div>;\n  }\n}\nInstallation\nnpm install es6-react-mixins\n\nThe source is written in es6 but there's an npm prebublish step which transpiles to es5 and dumps into lib directory - which is the default import.\nTesting\nnpm test\n\nContributions\nYes please!\n","563":"dm_bot\n** This bot has been decomissioned **\nThis bot is more of a learning exercise than anything else, no harm intended.\nIt works by using phantom.js and selenium to grab a screenshot of the whole page and mark some points for reference. The next step is to crop just the article out of the page by using pillow, and then save it as a jpg to save bandwidth. Once it has done that it can post a comment on the reddit post with a link to the image.\nNeeds all this installed to work:\n\npraw        - get reddit posts and comment on them\nselenium    - get screenshot through phantom.js\npillow      - crop screenshot to just the article\nlibjpeg8    - allows pillow to save as jpeg\nphantom.js  - renders the screenshot\n\n\nLink to \/u\/DailMail_Bot. See him in action!\nI was originally using imgur, however imgur would compress images over 1MB so they were unreadable. I now use a.pomf.se as it allows for the full image to be shown.\nI am aware the code is not in good shape, I keep just patching it manually to keep up with new image hosts, it is just intended to be a fun script, not a work of art!\nI have been banned from:\n\n\/r\/sydney\n\/r\/texas\n\/r\/imagesofthe1970s\n\/r\/ImagesOfThe2010s\n\/r\/ImagesOfEngland\n\/r\/DoctorWhumour\n\/r\/historyblogs\n\/r\/nufcirclejerk\n\/r\/nufcirclejerk\n\/r\/The_Donald\n\/r\/Mr_Trump\n\/r\/Minecraft\n\/r\/ireland\n\/r\/youranonnews\n\/r\/army\n\/r\/Conservatives_R_Us\n\/r\/conservatives\n\/r\/skeptic\n\/r\/hipsterhuskies\n\/r\/science\n\/r\/SandersForPresident\n\/r\/politota\n\/r\/UKIP\n\/r\/ukipparty\n\/r\/gaming\n\/r\/Romania\n\/r\/worldnews\n\/r\/photoshopbattles\n\/r\/unitedkingdom\n\/r\/funfacts\n\/r\/travel\n\/r\/China\n\/r\/interestingasfuck\n\/r\/conspiratard\n\/r\/inthenews\n\/r\/funny\n\/r\/politics\n\/r\/NaziHunting\n\/r\/Celebs\n\/r\/Conservative\n\/r\/UpliftingNews\n\/r\/WTF\n\/r\/news\n\/r\/AnyMore\n\/r\/necrodancer\n\/r\/celebnsfw\n\/r\/space\n\/r\/reactiongifs\n\/r\/nottheonion\n\/r\/ukpolitics\n\/r\/WhiteRights\n\/r\/unitedkingdom\n\/r\/european\n\/r\/ebola\n\/r\/soccer\n\nList of potential image hosts\n","564":"notice\nSince I don't work on this any more for 2 years and it may not work, I don't have time to fix issues yet.\nIf anyone want to use a free version of Slack , just go here: https:\/\/workjoy.online\nslack-backup\nSlack is great, it would be better if you can easily back-up all Slack chat history into your secured host automatically and help browsing history easily .\nPreinstall Requiments:\n\npython 2.7\nvirtualenv\n\nIf you want to test locally, you should install all modules in requirements.txt into your virtualenv.\nYou can use a free hosted service here:\nhttp:\/\/slackbk.herokuapp.com\nInstallation in Heroku\nClone the source code into your local machine\ngit clone git@github.com:suoinguon\/slack-backup.git\n\nInstall heroku toolbelt\nhttps:\/\/toolbelt.heroku.com\nCreate an heroku app\ncd slack-backup\n\nheroku create    \n\nPush source code into heroku\ngit push heroku master    \n\nCreate database schema\nheroku run python manage.py migrate\n\nGet your Slack client_id and client_secret\nhttps:\/\/api.slack.com\/applications\nSet heroku environment variables with your client_id & client_secret\nheroku config:set SLACK_CLIENT_ID=[your_client_id]    \n\nheroku config:set SLACK_CLIENT_SECRET=[your_client_secret]    \n\nAdd Heroku Sendgrid and Scheduler\nheroku addons:add sendgrid:starter    \n\nheroku addons:add scheduler    \n\nSet cron job for to automatically back-up your slack history\nOpen the scheduler\nheroku addons:open scheduler\n\nhttps:\/\/scheduler.heroku.com\/dashboard\nAdd follow in command into your heroku schedule, set frequency to \"Every 10 minutes\"\npython manage.py parse_channels    \n\nOpen and start using it\nheroku open    \n\nDon't know heroku & don't have time? I can help you to deploy.\nhong (at) vietnamdevelopers.com    \n\n","565":"Wasserstein GAN\n\nCode for the paper\n\u5b9f\u88c5\u306b\u3064\u3044\u3066\nSee also:\n\nUnrolled Generative Adversarial Networks\nChainer implementation of Unrolled GAN\n\n\n\nRequirements\n\nChainer\n\nMixture of Gaussians Dataset\n\nMNIST\ngenerated images\n\nAnimeface Dataset\nloss\n\ngenerated images\n\n","566":"ObjC2RubyMotion Converter for Sublime Text 2\/3\n\nA command plugin that enables to convert Objective-C code to Ruby Motion.\nScreenshot:\n\nUsing iShowU, KeyCastr, GIFBrewery\nHow It Works\nCode in the line of the cursor or selection are converted:\n\/\/ original\n_window = [[UIWindow alloc] initWithFrame:[[UIScreen mainScreen] bounds]];\nself.window.rootViewController = self.myNavController;\n[self.window makeKeyAndVisible];\n\n\/\/ select lines and run \"objc_to_ruby_motion\"\n_window = UIWindow.alloc.initWithFrame(UIScreen.mainScreen.bounds)\nself.window.rootViewController = self.myNavController\nself.window.makeKeyAndVisible\nInstall\nPackage Control\nInstall the ObjC2RubyMotion package from Package Control.\nManual\nClone this repository from your Sublime packages directory:\nMacosx\n$ cd ~\/Library\/Application\\ Support\/Sublime\\ Text\\ 2\/Packages\n$ git clone https:\/\/github.com\/kyamaguchi\/SublimeObjC2RubyMotion.git ObjC2RubyMotion\n\nKey Binding\nBy default,\nFor Conversion\nsuper+ctrl+i objc_to_ruby_motion\nConversions\nIn internal order\n\nReplace NSString @\"String\" -> \"String\"\nRemove inline comments \/\/\nConvert blocks (may be not perfect)\nConvert square brackets expression  [[Obj alloc] init] -> Obj.alloc.init\nRemove semicolon ; at the end of line\nRemove autorelease at the end\nRemove type declaration for Object Type * before =\nYES\/NO\nFloat 100.0f -> 100\nCGRectMake CGRectMake(10, 10, 20, 20) -> [[10, 10], [20, 20]]\n\nNOT supported\n\nComplex block\nif else conditions etc.\nactions action:@selector(tapped:)\nMethod name and args conversion - (NSInteger)tableView:(UITableView *)tableView numberOfRowsInSection:(NSInteger)section\nOthers\n\nNote\nThis converter is not intended to convert perfectly. This is intended to help the conversion of Objective-C code snippets.\nSome complex expression may not be converted correctly.\nTests\n\ud83d\ude04 Fortunately ObjC2RubyMotion has tests.\nRun test from command line\n$ cd ~\/Library\/Application\\ Support\/Sublime\\ Text\\ 2\/Packages\/ObjC2RubyMotion\n$ python tests\/all_test.py\n\nOR\n\ud83d\udc0e Use guard\n# Requirement: ruby\n$ gem install guard\n$ gem install guard-shell\n$ guard\n\nCustomize\n\n\nFork it\n\n\nRemove original ObjC2RubyMotion and clone yours OR add your repository as another git remote\n\n\n$ cd ~\/Library\/Application\\ Support\/Sublime\\ Text\\ 2\/Packages\n$ git clone git@github.com:yourname\/SublimeObjC2RubyMotion.git ObjC2RubyMotion\n\n\nCopy test file and write new test\n\ncp tests\/test_basic.py tests\/test_custom.py\n\n\ud83d\udc0d Change and Test\n\nNormally, you should change CodeConverter.py and test_*.py.\n$ guard is recommended.\nNote\nProbably most users of this plugin are rubyist, not pythonista.\nThis converter is mostly composed of regular expression for now.\nIf we try to improve this to convert more complex expressions, probably we need to replace the converter with the one using parser\/tokenizer\/scanner.\nFork is welcomed.\nCare about unexpected string replacements. They could happen and they will be problems.\n","567":"DJ Skeletor\nDJ Skeletor is a skeleton Django project handy for quick bootstrapping of new\nempty Django projects. It will help you get up and running with your project\nin seconds.\nThe repository contains an empty, relocatable Django project with a selection\nof useful Django application and setup for development, production and\n(automated) test settings and environments.\nQuickstart\n# prepare the virtual environment\nmkvirtualenv --no-site-packages myenv\n\n# get the skeleton project\ngit clone https:\/\/github.com\/senko\/dj-skeletor.git myproject\ncd myproject\n\n# set up the development environment\nmake dev-setup\n\n# run your fully operational Django project\npython manage.py runserver_plus\n\nBatteries included\nThe development environment by default includes:\n\nSouth\nfor database migrations (both development and production use it)\nDjango Debug Toolbar\nfor displaying extra information about view execution\nSQLite database (dev.db in the project root directory)\nIntegrated view debugger making it easy to debug crashes directly from the\nbrowser (Werkzeug and django-extension's\nrunserver_plus)\nFull SQL statement logging\nBeefed-up Django shell with model auto-loading and\nIPython REPL\nFlake8 source code checker\n(style, passive code analysis)\nConsole E-mail backend set by default in dev for simple E-mail send testing\nAutomated testing all set-up with\nnose, optionally creating test\ncoverage reports, and using the in-memory SQLite database (and disabled\nSouth) to speed up test execution\nDisabled cache for easier debugging\n\nThe production environment by default includes:\n\nSouth for database migrations (both development and production use it)\nGunicorn integration\nDjango Compressor\nfor CSS\/JS asset minification and compilation\nDatabase auto-discovery via environment settings, compatible with Heroku\nSentry client (raven_compat)\nfor exception logging (used only if SENTRY_DSN variable is set in\nsettings or environment)\nLocal-memory cache (although memcached is strongly recommended if available)\n\nThe extended tour\nAfter setting up your new Django project (see Quickstart above), try these:\n# make sure all tests pass (you'll need to write them first, though :)\nmake test\n\n# get a test coverage report (outputs to stdout, saves HTML format in\n# cover\/index.html and produces Cobertura report compatible with Jenkins)\nmake coverage\n\n# clean up test artifacts, *.pyc files and cached compressed assets\nmake clean\n\n# check if the code follows PEP8 and is free of obvious errors\n# this also includes cyclomatic complexity check and will complain if your\n# code is too complex (configurable by editing the Makefile)\nmake lint\n\n# update the environment (eg. after pulling in new code)\nmake dev-update\n\n# open up the new and improved Django shell\npython manage.py shell_plus\n\nYearn for more? Django-extension comes with tons of useful management commands,\nrun python manage.py help to get an overview.\nThe production setup\nThe production environment can't be set up automatically (at it may require\nsetting up databaes details and other per-server settings manually), but there\nare some helper Makefile tasks to speed it up.\nTo set up the production environment for a DJ Skeletor-powered project, loosely\nfollow this procedure:\n# prepare the virtual environment\nmkvirtualenv --no-site-packages myenv\n\n# get your project\ngit clone <myproject-url>\ncd myproject\n\n# install the requirements\nmake reqs\/prod\n\n# create a project\/settings\/local.py settings file with per-server config\nvim project\/settings\/local.py\n\n# run automatic update (db sync\/migrations, collectstatic)\nmake prod-update\n\n# your production environment is now ready\npython manage.py run_gunicorn\n\nOf course, your mileage may vary.\nThe settings files\nThe settings files base (base settings used in all environments),\nprod (production settings), dev (local development settings) and\ntest (settings used when running automated tests) should contain only the\nsettings used by all developers\/servers.\nPer-server (or per-developer) settings should go into local module\n(ie. project\/settings\/local.py). The usual pattern for this module is to\nfirst import everything from the settings variant that best matches your\nenvironment (prod for servers, dev for local development), and then\noverride\/add settings as needed.\nExample production settings just specifying the production database:\n# file: project\/settings\/local.py\nfrom .prod import *\n\nDATABASES = {\n    'default': { ... }\n}\n\nYou shouldn't need to add local.py to the repository (in fact, git is\nalready set up to ignore it). If some setting needs to be shared by everyone,\nit should probably be added to base, dev or prod.\nThe local settings file isn't required. If it doesn't exist, the production\nsetup will be used by default. This is useful if you don't have per-server\nsettings or they're deployed via Unix environment (as they are on eg. Heroku\nand similar cloud hosting providers).\nEnvironment settings\nIn either production or develoment mode, settings can also be set via\nthe environment variables. The following variables are supported:\n\nDATABASE_URL - Heroku-compatible database URL\nDEBUG - String true enables DEBUG, any other disables\nTEMPLATE_DEBUG - String true enables TEMPLATE_DEBUG, any other disables\nCOMPRESS_ENABLED - String true enables django-compressor, any other\ndisables\nSQL_DEBUG - String true enables SQL statement logging, any other\ndisables (disabled by default, available only if using dev.py)\nCACHE_BACKEND - String value to put into CACHES['default']['BACKEND']\nEMAIL_BACKEND - String value for EMAIL_BACKEND (only if using dev.py)\n\nNote that values from local.py override environment settings! You probably\nwant to use either the local settings file or the environment settings, not\nmix them.\nHeroku support\nThe production setup uses database autodiscovery so if you have a (promoted)\ndatabase in Heroku, it will automatically get picked up.\nFor Heroku, you'll probably want to add the Procfile file with contents\nsimilar to this:\nweb: python manage.py run_gunicorn --workers=4 --bind=0.0.0.0:$PORT\n\nIf your web app supports uploading of media (eg. images, videos or other\nfiles) by users, you'll probably need the django-storages app to\nautomatically host them somewhere else (eg on Amazon S3). When\ndjango-storages is set up, the collecstatic management command (run as\npart of make prod-update) will copy the static assets to the specified\nservice as well.\nAfter pushing the new code to Heroku for update, you should make sure to run\nall the needed management commands to migrate the database, etc:\nheroku run make prod-update\n\nDjango Debug Toolbar\nDjango Debug Toolbar is set up so it's always visible in the dev\nenvironment, no matter what the client IP is, and always hidden in\nthe production environment.\nSentry \/ Raven\nTo use the Sentry client, you'll need a server to point it to. Installing\nSentry server is easy as:\n# mkvirtualenv --no-site-packages sentry-env\n# pip install sentry\n# sentry init\n# sentry start\n\nYou'll want to install Sentry into its own environment as it requires\nDjango 1.2 or 1.3 at the moment.\nIf you don't want to install Sentry yourself, you can use a hosted\nversion at http:\/\/getsentry.com\/.\nWhen you connect to your (or hosted) Sentry server and create a new project\nthere, you'll be given Sentry DSN which you need to put into production\nsettings to activate Sentry exception logging.\nCompressor\nDjango Compressor can minify and compile your CSS and JS assets. DJ Skeletor\ncomes with Compressor support, but to make use of it, you need to use\n{% compress %} tags in your templates.\nBy default Compressor runs in online mode, and files are compressed\nand cached (if needed) when the template that uses them is first served.\nOptionally, it can also use offline mode (COMPRESSOR_OFFLINE) in which\nthe static files are pre-compressed in deployment phase. To activate this,\nyou'll need to activate the COMPRESSOR_OFFLINE setting (it's commented\nout in settings\/prod.py by default) and update Makefile to run the\ncompressor in the deployment phase.\nNote that if you enable offline mode, you will need to run compress after\nevery template or static file change, so it's recommended to only use it\nfor deployed\/production environments.\nTest code coverage\nDJ Skeletor comes with support for nose test runner and code coverage\nreporting through coverage.py.\nTo run a normal test without code coverage report, run make test.\nTo run a test with a coverage report, run make coverage. The report\nis generated in HTML format in the cover\/ subdirectory, and in the\nCobertura format in coverage.xml file (useful for integrating with\nContinuous Integration systems, such as Jenkins). The test run also produces\nnosetests.xml file in the standard JUnit format, also useful for integration\nwith Jenkins or other CI systems.\nDeployments via git\nIf deployments are done via git (and not fabric, see below), it's\nrecommended to create another Makefile target that will do the deploy, for\nexample:\ndeploy:\n  git pull\n  $(MAKE) update\n  # command to restart the service(s) as neccessary\n\nFabric\nA fabfile is provided with common tasks for rsyncing local directory to\nthe server for use while developing the project, and for deploying the\nproject using git clone\/pull.\nUseful commands:\n\nserver - host to connect to (same as -H, but accepts only one argument)\nenv - virtualenv name on the server, as used with virtualenvwrapper\/workon\nproject_path - full path to the project directory on the server\nrsync - use rsync to copy the local folder to the project directory on the server\nsetup - set up the project instance on the server (clones the origin\nrepository, creates a virtual environment, initialises the database and\nruns the tests)\ndeploy - deploy a new version of project on the server using git pull\ncollecstatic, syncdb, migrate, runserver - run manage.py command\nupdate - combines collecstatic, syncdb, migrate\ntest - run manage.py test with the test settings enabled\n\nFor all the commands, run 'fab -l' or look at the source.\nExamples:\nCopy local directory to the server, update database and static files, and\nrun tests (only files changed from last copy are going to be copied):\nfab server:my.server.com env:myenv project_path:\/path\/to\/project rsync update test\n\nDeploy a new instance of a project on a server ('myenv' will be newly created,\ncode will be cloned into \/path\/to\/project):\nfab server:my.server.com env:myenv project_path:\/path\/to\/project \\\n    setup:origin=http:\/\/github.com\/senko\/dj-skeletor\n\nDeploy a new version of the project on the server (a new git tag will be\ncreated for each deployment, so it's easy to roll-back if needed):\nfab server:my.server.com env:myenv project_path:\/path\/to\/project deploy\n\nCustomization\nEveryone has a slightly different workflow, so you'll probably want to\ncustomize the default fabric tasks or combine them. You can either customize\nfabfile.py and commit the changes to your repository, or you can create\nlocal_fabfile.py, which will be loaded if it exists. The latter can be useful\nif you have per-team-member fabric customizations you don't want to commit\nto the repository.\nRenaming the project\nBy default, DJ Skeletor names the project project, so it's generic enough\nto not requiring the change for each project, so the initial setup is\na bit faster (and the manage.py logic is simpler).\nIf you do want to change the project name though, there's couple of things\nyou need to do. For example, if you want to rename the project to foo:\n\nrename the folder: git mv project foo\nupdate Makefile, manage.py and\u02dbfabfile to set PROJECT_NAME to foo\ncommit the changes to your git repository and you're done!\n\n","568":"spec\nWhat is it?\nspec is a Python (2.6+ and 3.3+) testing tool that turns this:\n\ninto this:\n\nSpecifically, spec provides:\n\nColorized, specification style output\nColorized tracebacks and summary\nOptional timing display for slow tests\nTest-running CLI tool which enables useful non-default options and implements\nrelaxed test discovery for less test_annoying.py:TestBoilerplate.test_code\nand more readable.py:Classes.and_methods.\n\nSpec-style output\nspec is a BDD-esque\nnose plugin designed to provide \"specification\"\nstyle test output (similar to Java's\nTestDox or Ruby's\nRSpec). Spec-style output provides a more\nstructured view of what your tests assert, compared to nose\/unittest's\ndefault \"flat\" mode of operation.\nFor example, this nose-style test module:\nclass TestShape(object):\n    def test_has_sides(self):\n        pass\n\n    def test_can_calculate_its_perimeter(self):\n        pass\n\nclass TestSquare(object):\n    def test_is_a_shape(self):\n        pass\n\n    def test_has_four_sides(self):\n        pass\n\n    def test_has_sides_of_equal_length(self):\n        pass\nnormally tests like so, in a single flat list:\nTestShape.test_has_sides ... ok\nTestShape.test_can_calculate_its_perimeter ... ok\nTestSquare.test_has_four_sides ... ok\nTestSquare.test_has_sides_of_equal_length ... ok\nTestSquare.test_is_a_shape ... ok\n\nWith spec enabled (--with-spec), the tests are visually grouped by class,\nand the member names are tweaked to read more like regular English:\nShape\n- has sides\n- can calculate its perimeter\n\nSquare\n- has four sides\n- has sides of equal length\n- is a shape\n\nIn other words:\n\nClass-based tests are arranged with the class name as the subject, and the\nmethods as the specifications;\nAny module-level tests are arranged with the module name as the subject;\nAll objects' docstrings are used as their descriptions, if found. Otherwise:\n\nCamelCaseNames (typically classes) have any leading\/trailing Test\nstripped, as well as any trailing underscore;\nCamelCaseNames also get turned into sentences if necessary, so e.g.\nCamelCaseNames becomes Camel case names;\nunderscored_names have any leading\/trailing test (with its attached\nunderscore) stripped;\nunderscored_names have underscores turned into spaces;\n\n\n\nTest runner\nspec ships with a same-name command-line tool which may be used as a more\nliberal nosetests. In addition to toggling a number of useful default options\n(such as nose's builtin --detailed-errors) spec-the-program will honor\nany and all public objects defined within your project's tests directory,\nmeaning any file, function or class whose name does not begin with an\nunderscore ('_') and which is defined locally.\nFor example, given the following code inside tests\/feature_name.py:\nfrom external_module import a_function, AClass\n\ndef _helper_function(args):\n    return a_function(args)\n\nclass _Parent(object):\n    def this_will_not_get_tested():\n        pass\n\nclass Feature(_Parent):\n    def should_have_some_attribute(self):\n        _helper_function(AClass)\n\n    def does_something_awesome(self):\n        self._helper_method()\n\n    def _helper_method(self):\n        pass\n\ndef something_tested_by_itself_outside_a_class():\n    pass\nonly the following items will be picked up as test cases:\n\nFeature.should_have_some_attribute\nFeature.does_something_awesome\nsomething_tested_by_itself_outside_a_class\n\nThe imported function and class, the underscored functions\/methods, and the\nmethods inherited from a parent class, are all ignored.\nEnhanced output via the Spec class\nAs with some other spec-style tools, spec provides a means for nesting your\ntest \"contexts\" so they display nicely during test runs. Just use the Spec\nclass as your primary superclass and inner classes will get parsed\nautomatically.\nFor example:\nfrom spec import Spec\n\nclass ClassUnderTest(Spec):\n    def it_behaves_like_this(self):\n        # ...\n\n    class init:\n        \"__init__\"\n        def takes_arg1(self):\n            # ...\n\n        def takes_arg2(self):\n            # ...\nThe above results in output like so:\nClass under test\n- it behaves like this\n\n    __init__\n    - takes arg1\n    - takes arg2\n\nThis indentation makes output even easier to follow & helps keep things\norganized.\nAccessing outer classes from inner ones\nFrequently, you may have a useful setup method in your outer class, and wish\nto access objects attached to self from inner classes. As of Spec 0.11.0 this\nis now possible and is quite transparent; failed attribute lookups will check\nan instantiated + setup'd copy of the outer class:\nclass MainClass(Spec):\n    def setup(self):\n        self.x = 'y'\n\n    def outer_test(self):\n        assert self.x == 'y'\n\n    class some_inner_class:\n        def inner_test(self):\n            # Here, because some_inner_class has no real 'x' attribute, we end\n            # up seeing the outer class' value.\n            assert self.x == 'y'\nRight now this support is pretty basic and assumes your setup methods are\nliterally named setup, not setUp or whatnot. This will likely improve in\nthe future.\nUsage tips\nFollowing from spec-the-tool's discovery algorithm, and spec-the-plugin's\nname transformation, we suggest the following for both readable code and\nreadable test output:\n\nStore tests in tests\/, with whatever file-by-file organization you like\nbest;\nWithin files, import the classes under test normally, e.g. from mymodule import MyClass;\nName the test classes identically, but with a trailing underscore to avoid\nname collisions, and inheriting from the Spec class, e.g. class MyClass_(Spec): [...]\nName their methods like English sentences, e.g. def has_attribute_X(self): [...].\nTests not yet filled out should call the skip function, which raises a\n\"skip this test\" exception Nose handles correctly.\n\nFor example:\nfrom spec import Spec, skip\nfrom mypackage import MyClass, MyOtherClass\n\nclass MyClass_(Spec):\n    def has_attribute_A(self):\n        skip()\n\nclass MyOtherClass_(Spec):\n    def also_has_attribute_A(self):\n        skip()\n\n    def has_attribute_B(Spec):\n        skip()\ntests as:\nMyClass\n- has attribute A\n\nMyOtherClass\n- also has attribute A\n- has attribute B\n\nActivation \/ command-line use\nAfter installation via setup.py, pip or what have you, nosetests will\nexpose these new additional options\/flags:\n\n--with-spec: enables the plugin and prints out your tests in specification\nformat. Also automatically sets --verbose (i.e. the spec output is a\nverbose format.)\n--no-spec-color: disables color output. Normally, successes are green,\nfailures\/errors are red, and\nskipped tests are\nyellow.\n--spec-doctests: enables (experimental) support for doctests.\n--with-timing: enables timing display for tests that take >=\nTIMING_THRESHOLD (see below) to run.\n--timing-threshold: configuration of timing threshold (in seconds) for\n--with-timing. Defaults to 0.1.\n\nWhy would I want to use it?\nSpecification-style output can make large test suites easier to read, and like\nany other BDD tool, it's more about framing the way we think about (and view)\nour tests, and less about introducing new technical methods for writing them.\nWhere did it come from?\nspec is heavily based on the spec plugin for Titus Brown's\npinocchio\nset of Nose extensions. Said plugin was originally written by Michal\nKwiatkowski. Both pinocchio and its spec plugin are copyright \u00a9 2007\nin the above two gentlemen's names, respectively.\nThis version of the plugin was created and distributed by Jeff Forcier, \u00a9\n2011. It tweaks the original source to be Python 2.7 compatible, based on\nsimilar\nchanges.\nIt also fixes a handful of bugs such as broken\nSkipTest\ncompatibility under Nose 1.x, and then adds some additional functionality on\ntop (most notably the spec command-line tool.)\nWhat's the license?\nBecause this is heavily derivative of pinocchio, spec is licensed the same\nway -- under the MIT\nlicense.\n","569":"Django-Pluggables\nDjango-Pluggables provides a design pattern for making reusable apps \"pluggable\" so that apps can exist at multiple URL locations and be presented by other apps or models.\n\nOverview\nDjango-Pluggables is a design pattern that endows reusable applications with a few additional features:\n\nApplications can exist at multiple URL locations (e.g. http:\/\/example.com\/foo\/app\/ and http:\/\/example.com\/bar\/app\/).\nApplications can be \"parented\" to other applications or objects which can then deliver specialized context information.\nPosting form data and error handling can happen in locations that make sense to the user, as opposed to the common practice of using templatetags and standalone error or preview pages for form data processing.\nViews and templates remain generic and reusable.\n\n\nInstallation\nDjango-Pluggables can be added to your Django project like any other reusable app. To see the examples in action, fun the following commands:\nFirst off, the native Mac implementation of sed is not as robust as we expect. Therefore install Super Sed  Then run\n$ easy_install Fabric==0.1.1 pip virtualenv\n$ fab bootstrap\n$ .\/examples\/sillywalks\/manage.py runserver\n\nVoila! You now have a fully functional site running at http:\/\/127.0.0.1:8000\/\n\nUsage\nTo utilize Django-Pluggables, you will need to do a little bit of configuration. First you must have a two applications:\n\nA reusable app that will be made Pluggable.\nAn app that will expose an instance of the Pluggable app in its URL hierarchy (this app does not need to be reusable).\n\nInstallation and development of these two apps can begin normally. The individual functions of the apps should be standalone. The Django-Pluggables pattern delivers needed context and parameters to the app that is being made Pluggable when it is called from the URL hierarchy of the other app.\nTo set up the relationship between these two apps, it is necessary to define two items:\n\nThe URLconf that defines the URLs that will invoke the Pluggable app instance.\nThe pluggable definition that will supply the Pluggable app with its required context and parameters.\n\n\nNote\nIn the following examples we will use two imaginary apps: sillywalks and complaints. These would be imported in your modules like so:\nfrom sillywalks.models import Walk\nfrom complaints.models import Complaint\n\nIn the examples we discuss below, the sillywalks app lives at the URL:\nhttp:\/\/example.com\/sillywalks\/\n\nThe sillywalks app utilizes the complaints app to field complaints about various types of walk. To provide an excellent user experience, the complaints forms and functionality should be presented at a URL beneath the sillywalks URL:\nhttp:\/\/example.com\/sillywalks\/<walk_name>\/complaints\/\n\nAlso, the presentation of complaints functionality beneath the sillywalks app should adhere to the sillywalks templates pattern, including various walk-specific information that must be supplied through additional, non- complaints app context.\n\n\nDefining the URLconf\nIn order to make the complaints app pluggable, we first must define a pluggable URLs file for it:\nclass ComplaintsPluggable(PluggableApp):\n    urlpatterns = patterns('',\n        url(r'^$', 'complaints.views.view_complaint', name='complaints_view_complaint'),\n        url(r'^activity_feed\/$', 'complaints.views.submit_complaint', name='complaints_submit_complaint'),\n\n...\n\n\nNote\nOnly one pluggable URLs file is required for an app that you wish to make pluggable. This URLconf may be subclassed for additional flexibility when using the pluggable app in conjunction with other apps, but it is likely that one definition of pluggable URLs will be sufficient for most use cases.\n\n\nDefining the Pluggable Config and Context\nOnce the complaints app has been set up with the pluggable URLs, it is necessary to define the context that will be delivered to the complaints app and add these URLs to the URLconf for the sillywalks app:\nfrom complaints.urls import ComplaintsPluggable\n\nclass SillyWalkComplaints(ComplaintsPluggable):\n    def pluggable_view_context(self, request, complaint_id):\n        return dict(complaint_id=complaint_id)\n\n    def pluggable_template_context(self, request, slug):\n        return dict(silly_walk=SillyWalk.objects.get(slug=slug))\n\n    def pluggable_config(self):\n        return dict(base_template='sillywalks\/base.html')\n\nurlpatterns = SillyWalkComplaints('sillywalk')\n\n\nMore\nThe primary repository for Django-Pluggables is located at:\nhttp:\/\/github.com\/nowells\/django-pluggables\/\nDjango-Pluggables was created by Nowell Strite.\n","570":"Sublime IPython Notebook\nThis is a Sublime Text 3 plugin that emulates IPython notebook interface inside Sublime.\nNOTE: this plugin does not work with the latest versions of Ipython Notebook! (2.0 or greater)\nDisclaimer\nWhile the plugin looks stable so far and I am trying to preserve as much of the notebook data as possible, there are no guarantees that you data will be safe. Do not use it for the notebooks that contain valuable data without doing a backup.\nHow to use\n\nConnect to the notebook server using \"Open IPython Notebook\" command. Choose a notebook you want to open and it will open in a separate buffer.\nI am trying to support keyboard shortcuts from the web version of the notebook. Currently you can use:\n\nshift+enter - execute current cell\nctrl+enter - execute current cell inplace\nctrl+m, d - delete current cell\nctrl+m, a - add cell above current\nctrl+m, b - add cell below current\nctrl+m, n - select next cell\nctlr+m, p - select previous cell\nctrl+m, y - code cell\nctrl+m, m - markdown cell\nctrl+m, t - raw cell\nctrl+m, s - save notebook (ctrl+s and super+s will work too)\n\n\n\nNotes\n\nYou can use %pylab inline. You will not be able to see the plots, but they will be saved in the notebook and available when viewing it through the web interface.\nI am using websocket-client library from https:\/\/github.com\/liris\/websocket-client and (slightly patched) subset of the IPython. You do not have to install them separately.\nST3 port was contributed by chirswl\nDark theme, support for password-protected servers and nicer last-used-server picker was contributed by z-m-k\n\nVintage Mode\nIn Vintage mode, for the navigation keys to work as expected in IPython Notebook buffer, you need to modify some keybindings. Add the following to your Key Bindings - User.\n\n\nAdd a context key to Shift+Enter so you can run a cell with Shift+Enter in the command mode:\n{ \"keys\": [\"shift+enter\"], \"command\": \"set_motion\", \"args\": {\n    \"motion\": \"move\",\n    \"motion_args\": {\"by\": \"lines\", \"forward\": true, \"extend\": true }},\n    \"context\": [\n        { \"key\": \"setting.command_mode\"},\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": false },\n        ]\n},\n\n\n\nCommand mode Up\/Down navigation keys:\n{ \"keys\": [\"j\"], \"command\": \"set_motion\", \"args\": {\n    \"motion\": \"move\",\n    \"motion_args\": {\"by\": \"lines\", \"forward\": true, \"extend\": true },\n    \"linewise\": true },\n    \"context\": [\n        { \"key\": \"setting.command_mode\"},\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": false },\n        ]\n},\n\n{\n    \"keys\": [\"j\"], \"command\": \"inb_move_up\",\n    \"context\" : [\n        { \"key\": \"setting.command_mode\"}\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": true },\n        { \"key\": \"auto_complete_visible\", \"operator\": \"equal\", \"operand\": false },\n        ]\n},\n\n{ \"keys\": [\"k\"], \"command\": \"set_motion\", \"args\": {\n    \"motion\": \"move\",\n    \"motion_args\": {\"by\": \"lines\", \"forward\": false, \"extend\": true },\n    \"linewise\": true },\n    \"context\": [\n        { \"key\": \"setting.command_mode\"},\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": false },\n        ]\n},\n\n{\n    \"keys\": [\"k\"], \"command\": \"inb_move_down\",\n    \"context\" : [\n        { \"key\": \"setting.command_mode\"},\n        { \"key\": \"setting.ipython_notebook\", \"operator\": \"equal\", \"operand\": true },\n        { \"key\": \"auto_complete_visible\", \"operator\": \"equal\", \"operand\": false },\n        ]\n},\n\n\n\n","571":"Live Blog\nSourcefabric's Live Blog is an open source web app that enables journalists to provide immediate and ongoing coverage on evolving news events. Find out more here: https:\/\/www.sourcefabric.org\/en\/liveblog\/.\nSee an example of Live Blog in action here: http:\/\/www.zeit.de\/politik\/deutschland\/2013-09\/bundestagswahl-2013-live\nLicense: AGPLv3\nCopyright: Sourcefabric z.\u00fa.\nFeatures\n\nAccess to sources: Drag and drop search results from social media and other content networks into your Live Blog timeline.\nThemes: Customise Live Blog to match your site\u2019s design and make it look great on mobile and tablet.\nUser roles: Build a team work\ufb02ow for multiple collaborators with appropriate sign-offs and permissions.\nEasy embedding: Simply copy and paste an embed code from the Live Blog interface and start spreading the news.\nBlog chaining: Connect several blogs to syndicate your content to other media outlets.\nSEO solution: With our new embed plugin you can integrate an html version of the blogs into your website: https:\/\/github.com\/liveblog\/plugin-liveblog-embed-server\/tree\/master\n\nInstallation\nPlease check the installation guide: https:\/\/github.com\/superdesk\/Live-Blog\/blob\/master\/INSTALL.md\nDocumentation\n\nLive Blog RESTful API documentation: http:\/\/docs.sourcefabric.org\/projects\/live-blog-restful-api\/en\/latest\/\n\nManuals and Tutorials\n\nManual for journalists: https:\/\/www.sourcefabric.org\/en\/liveblog\/manuals\/\nHow to create your own themes: http:\/\/www.sourcefabric.org\/en\/community\/blog\/2097\/Building-themes-for-Live-Blog.htm\n\nSuperdesk\nLive Blog is based on Superdesk technology.\nHow To Contribute\nCommit messages\nEvery commit has to have a meaningful commit message in the form:\n[JIRA ref] [JIRA Title] or [Title]\n<empty line>\n[Description]\n\nWhere JIRA ref is the Issue code eg. LB-13.\nFor trivial changes you can ommit JIRA ref or Description or both eg. Add travis.yml files\nPull requests\nEvery pull request has to have a meaningful message and if not specified in the commits, a good description of what has been done.\n","572":"SpringStudy\n\u5bf9Spring\u6846\u67b6\u7684\u5b66\u4e60\uff0c\u5305\u62ec\u5b98\u65b9\u6587\u6863\u7684\u7ffb\u8bd1\uff0cSpring\u6846\u67b6\u7684\u5e94\u7528\uff0cSpring\u6e90\u7801\u7684\u5256\u6790\u3002\u3002\u3002\n","573":"Docker\/Vagrant build runners for TeamCity\nFeatures\nPlugin detects and reports installed vagrant and docker\nUse the dedicated build runner to run your build script under virtualized environment\nwith help of Docker\/Vagrant build runner\nLicense\nApache 2.0\nsee LICENSE.txt for details\nDownloading Build\nDownload the latest build from TeamCity\nInstallation\nTeamCity Server and TeamCity Build Agents are required to be runnung under JRE 1.7+\nTo install plugin, put downloaded plugin .zip file into <TeamCity Data Directory>\/plugins folder and restart TeamCity Server.\nMake sure downloaded .zip file is not corrupted and is not sources .zip from GitHub.\nFor more details, there is documentation\nSupported Versions\nPlugin is tested to work with TeamCity 8.1.\nIt should work with 8.0 (and maybe 7.1.x)\nAgent and server are expected to run JRE 1.7\nBuilding\n\ncall ant -f fetch.xml fetch\nopen the project in IntelliJ IDEA 13.1\nmake all artifacts\n\nIn this repo you will find\n\nTeamCity server and agent plugin bundle\nPlugin version will be patched if building with IDEA build runner in TeamCity\nRun configuration server to run\/debug plugin under TeamCity (use http:\/\/localhost:8111\/bs)\npre-configured IDEA settings to support references to TeamCity\nUses $TeamCityDistribution$ IDEA path variable as path to TeamCity home (unpacked .tar.gz or .exe distribution)\nBunch of libraries for most recent needed TeamCity APIs\nModule with TestNG tests that uses TeamCity Tests API\n\nFor details see https:\/\/github.com\/jonnyzzz\/TeamCity.PluginTemplate\nDependencies fetch via simple-maven\nThis is simple possible ant script that could be used to fetch\nmaven dependencies in Intellij IDEA based projects.\nCall fetch.xml to fetch all missing dependencies\nThe script also updates IDEA library files to include new-ly added libraries to IDEA project.\nIt's expected you'll no changed files on script re-run. If you do => post the issue with diff\nWe use  Maven Ant Tasks\ninside. So <dependency> element under <maven-fetch> is the same\nelement as under Maven Ant Tasks\nFor details see https:\/\/github.com\/jonnyzzz\/intellij-ant-maven\nNotice\nSome code of this plugin was borrowed from NuGet plugin\nwhich is licensed under Apache 2.0\nNote\nThis plugin was created with [https:\/\/github.com\/jonnyzzz\/TeamCity.PluginTemplate](TeamCity Plugin Template)\nThis is my (Eugene Petrenko) private home project\n","574":"Lifecycle Sorter for Android Studio\nNote: This project is no longer maintained. Feel free to fork and fix issues\/add enhancements.\nThis plugin sorts the lifecycle methods of an Activity or Fragment in the order that they are called in an application.\n\n Activity Lifecycle \n Fragment Lifecycle \n\nUsage\nYou can find the option to sort the lifecycle methods under the Code > \"Sort Lifecycle Methods\".\nAll you need to do is place the cursor inside of your class and select \"Sort Lifecycle Methods\" or use \nthe keyboard shortcut control+alt+k. It's as simple as that!\nActivity before sorting\n\nActivity after sorting\n\nThis is my first plugin and thought it would be a handy tool, not only for myself but other Android developers. \nHope you guys enjoy!\n","575":"Docker with SpringBoot, tomcat and MySQL\nRead how to build this project from scratch here -> tutorial\n(It's very messy right now)\nHow to run this demo?\nStep 0 - Requirements\nHere are the tools you need:\n\n\nDocker (duh)\nEclipse (I'm using Eclipse Photon)\nJava10\n\n\nStep 1 - Clone this repo\ngit clone https:\/\/github.com\/HechengLi\/Docker_SpringBoot_Tomcat_MySQL_Demo.git\nStep 2 - Import TianMiao into eclipse as a maven project\n\nOpen your eclipse\nOn the taskbar click File -> Import -> Maven -> Existing Maven Project\nSelect the folder TianMiao the Root Directory (You should see \/pom.xml com.example:TianMiao:0.0.1-SNAPSHOT:war in Projects)\nClick Finish\n\nStep 3 - Build TianMiao as a war file\n\nRight click TianMiao in Package Explorer -> Run As -> Run Configurations\nType 'clean install -Dmaven.test.skip=true' in Goals\nClick Apply then Run (you should have TianMiao.war under Docker_SpringBoot_Tomcat_MySQL\\TianMiao\\target)\n\nStep 4 - Run the project with docker\n\nOpen your commandline, cd to the git directory\nMake sure you have docker app running\nRun 'docker-compose -f stack.yml up' (add -d if you want it to run in background)\n\nStep 5 - Rerun if there's an error on first run\n\nIf you get an error while starting tomcat, it probably is because the docker container running Tomcat doesn't wait for MySQL to finish running it's setup script.\nWait for MySQL to finish running its script (it will log ...ready for connections...)\nStop all containers and start again should fix the problem.\n\nStep 6 - Check if it works (suggestion - use postman)\n\nSend Get Request to 'http:\/\/localhost:8080\/TianMiao\/api\/users' to retrive data\nSend Post Request to 'http:\/\/localhost:8080\/TianMiao\/api\/users' with json {'username': 'anyusername'} to add data\n\n","576":"Litho - Picasso\nLitho Picasso is a Litho compatible library, which provides an Image Component compatible with Picasso. Litho-PicassoX supports a wide part of Picasso's functionality and is compatible with Android X. If you notice that anything is missing ping me :)\nHow to use\nAdd the following dependency on your app's build.gradle:\nimplementation 'com.github.charbgr:litho-picassox:1.0'\nAdd PicassoImage component on your own Component :)\nPicassoImage.create(componentContext)\n    .imageUrl(image)\n    .fit(true)\n    .centerCrop(true)\n    .buildWithLayout();\nHere you can find a sample ComponentSpec that uses PicassoImage component.\nOther\nLitho repository: https:\/\/github.com\/facebook\/litho\nLitho documentation: http:\/\/fblitho.com\/docs\/getting-started\nLicense\nMIT License\n\nCopyright (c) 2017 Vasilis Charalampakis & Pavlos-Petros Tournaris\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n","577":"VolleySample\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51e6\u7406\u3092\u9ad8\u901f\u306b\u3001\u7c21\u5358\u306b\u5b9f\u88c5\u3067\u304d\u308b Volley \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u3063\u305f\u30b5\u30f3\u30d7\u30eb\u3067\u3059\u3002\n\u3053\u3061\u3089\u306e\u30d6\u30ed\u30b0\u306e\u53c2\u8003\u306b\u304a\u4f7f\u3044\u304f\u3060\u3055\u3044\u3002\nAndroid Tips #51 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u901a\u4fe1\u30fb\u30ad\u30e3\u30c3\u30b7\u30e5\u51e6\u7406\u3092\u3088\u308a\u901f\u304f\u3001\u7c21\u5358\u306b\u5b9f\u88c5\u3067\u304d\u308b\u30e9\u30a4\u30d6\u30e9\u30ea \u201cVolley\u201d \u3092\u4f7f\u3063\u3066\u307f\u305f | Developers.IO\n","578":"MyGank\n\u63cf\u8ff0\n\u4e00\u4e2a\u5177\u6709\u7528\u6237\u767b\u5f55\u6ce8\u518c\uff0c\u5e72\u8d27\u6536\u85cf\u529f\u80fd\u7684\u5e72\u8d27\u96c6\u4e2d\u8425\u7b2c\u4e09\u65b9\u5ba2\u6237\u7aef\u3002\u4e2a\u4eba\u5b66\u4e60\u9879\u76ee\uff0c\u524d\u7aef\u4f7f\u7528Android\u5c55\u793a\uff0c\u540e\u7aef\u4f7f\u7528javaee\u4e2d\u8f6c\uff0c\u6570\u636e\u6765\u6e90\u5e72\u8d27\u96c6\u4e2d\u8425\u3002\n\u622a\u56fe\n\n\n\n\n\u4e3b\u8981\u6280\u672f\nAndroid:\n\nMvp\nRxJava   ,  RxAndroid\nRetrofit\nBufferKnife\nGlide\nBAVH\nPhotoView\nAndroid-skin-support\n\njavaee:\n\nSpring\nMybatis\nMybatis.spring\n\n\u4e0b\u8f7d\u5730\u5740\nhttps:\/\/fir.im\/skq1\n\u6700\u540e\n\n\u611f\u8c22\u5e72\u8d27\u96c6\u4e2d\u8425\u63d0\u4f9b\u7684\u6570\u636e\n\n","579":"DoubleCommand\nA keyboard remap tool for Mac OS X.\nContributors\nDoubleCommand is open source, free software (GPL) developed by a team of contributors including:\n\nMichael Baltaks\nTyler Bunnell\nAdam Strzelecki\nGuillaume Outters\nGerolf Seitz\nRyan Walklin\nMike Ter Louw\nand others...\n\n","580":"DoubleCommand\nA keyboard remap tool for Mac OS X.\nContributors\nDoubleCommand is open source, free software (GPL) developed by a team of contributors including:\n\nMichael Baltaks\nTyler Bunnell\nAdam Strzelecki\nGuillaume Outters\nGerolf Seitz\nRyan Walklin\nMike Ter Louw\nand others...\n\n","581":"Warning: This project is unmaintained (due to MaxMind API deprecation) and currently uses an old database from 2017\ngeoloc is a command line tool for bulk geolocation queries written in C++.\nOnce its binary database has been built, geoloc performs geolocation\nqueries offline.\nExamples:\nBulk lookup from apache access.log:\n$ cat access.log | awk '{print $1}' | geoloc -f - | column -t\n\n10.172.47.117  AU  02  Sydney         -33.8001  151.3123   AS1610581   BIGCableCo\n10.36.87.70    AU  07  Melbourne      -37.8266  144.7834   AS1370775   Micronode+PTY+LTD\n10.88.81.165   US  CA  San+Francisco  37.6777   -122.2221  AS49335653  Big+Flare,+Inc\n\nQuery some IPs:\n$ geoloc -q 8.8.8.8 192.30.252.131 --headers | column -t\n\nip              country  region  city           latitude  longitude  as_num   as_text\n8.8.8.8         US       CA      Mountain+View  37.3860   -122.0838  AS15169  Google+Inc.\n192.30.252.131  US       CA      San+Francisco  37.7697   -122.3933  AS36459  GitHub,+Inc.\n\ngeoloc is designed to run fast and load fast:\n$ wc -l \/tmp\/ip_list\n\n 1000000 \/tmp\/ip_list\n\n$ time geoloc -f \/tmp\/ip_list > \/tmp\/res1\n\nreal    0m6.131s\nuser    0m5.662s\nsys     0m0.369s\n\n$ time geoloc -q 8.8.8.8 192.30.252.131 > \/tmp\/res2\n\nreal    0m0.010s\nuser    0m0.002s\nsys     0m0.005s\n\n\nInstallation\nThe program is designed as a portable application, to run out of ~\/bin,\nwith the database stored in ~\/var\/db\/geoloc\/geodata.bin.\nTo install:\n$ git clone https:\/\/github.com\/loadzero\/geoloc.git && cd geoloc\n$ .\/configure\n$ make\n$ make install\n\nThe configure script will check for these dependencies:\n\niconv\nunzip\nwget\nmake\nc++\n\nDuring installation, data will be downloaded from\nMaxMind to create the database.\nAn update script will be installed into ~\/bin\/_geoloc_update.sh. Run\nthis script when you would like to update your geolocation database. MaxMind\nupdates their source data once a month.\nI have tested on OSX 10.9.5 and Ubuntu 14.04. Other unices are likely to work\nwith minimal or no changes. It is unlikely to work on windows, due to the use\nof mmap.\nDesign and Implementation\nThe code operates in two phases, packing and query. The packing phase is all\nabout converting the data into a machine optimal format, namely relocatable\nsorted vectors. The query phase simply mmaps that data, and performs a\nstd::upper_bound binary search on it to find the IPs.\nThere is an outline of the code, roughly in topological order\nhere, that contains a summary of each module.\nAttribution\nThis software includes GeoLite data created by MaxMind available from\nhttp:\/\/www.maxmind.com\n","582":"Warning: This project is unmaintained (due to MaxMind API deprecation) and currently uses an old database from 2017\ngeoloc is a command line tool for bulk geolocation queries written in C++.\nOnce its binary database has been built, geoloc performs geolocation\nqueries offline.\nExamples:\nBulk lookup from apache access.log:\n$ cat access.log | awk '{print $1}' | geoloc -f - | column -t\n\n10.172.47.117  AU  02  Sydney         -33.8001  151.3123   AS1610581   BIGCableCo\n10.36.87.70    AU  07  Melbourne      -37.8266  144.7834   AS1370775   Micronode+PTY+LTD\n10.88.81.165   US  CA  San+Francisco  37.6777   -122.2221  AS49335653  Big+Flare,+Inc\n\nQuery some IPs:\n$ geoloc -q 8.8.8.8 192.30.252.131 --headers | column -t\n\nip              country  region  city           latitude  longitude  as_num   as_text\n8.8.8.8         US       CA      Mountain+View  37.3860   -122.0838  AS15169  Google+Inc.\n192.30.252.131  US       CA      San+Francisco  37.7697   -122.3933  AS36459  GitHub,+Inc.\n\ngeoloc is designed to run fast and load fast:\n$ wc -l \/tmp\/ip_list\n\n 1000000 \/tmp\/ip_list\n\n$ time geoloc -f \/tmp\/ip_list > \/tmp\/res1\n\nreal    0m6.131s\nuser    0m5.662s\nsys     0m0.369s\n\n$ time geoloc -q 8.8.8.8 192.30.252.131 > \/tmp\/res2\n\nreal    0m0.010s\nuser    0m0.002s\nsys     0m0.005s\n\n\nInstallation\nThe program is designed as a portable application, to run out of ~\/bin,\nwith the database stored in ~\/var\/db\/geoloc\/geodata.bin.\nTo install:\n$ git clone https:\/\/github.com\/loadzero\/geoloc.git && cd geoloc\n$ .\/configure\n$ make\n$ make install\n\nThe configure script will check for these dependencies:\n\niconv\nunzip\nwget\nmake\nc++\n\nDuring installation, data will be downloaded from\nMaxMind to create the database.\nAn update script will be installed into ~\/bin\/_geoloc_update.sh. Run\nthis script when you would like to update your geolocation database. MaxMind\nupdates their source data once a month.\nI have tested on OSX 10.9.5 and Ubuntu 14.04. Other unices are likely to work\nwith minimal or no changes. It is unlikely to work on windows, due to the use\nof mmap.\nDesign and Implementation\nThe code operates in two phases, packing and query. The packing phase is all\nabout converting the data into a machine optimal format, namely relocatable\nsorted vectors. The query phase simply mmaps that data, and performs a\nstd::upper_bound binary search on it to find the IPs.\nThere is an outline of the code, roughly in topological order\nhere, that contains a summary of each module.\nAttribution\nThis software includes GeoLite data created by MaxMind available from\nhttp:\/\/www.maxmind.com\n","583":"A Cocos2d-x Asynchronous Http Request Libray That Let You Make Asynchrounous Http Requests From Cocos2d-x A FUN!\nYou can find the library file:\n\nCCHttpRequest.h\nCCHttpRequest.cpp\n\nUnder CCHttpRequestExample\/Classes\nThis library had been written as a cocos2d-x extension, Use it is very simple:\nCCHttpRequest *gateway = CCHttpRequest::sharedHttpRequest();\n\n\/\/Get Request\nstd::string url = \"http:\/\/www.baidu.com\";\ngateway->addGetTask(url, NULL, NULL);\n\n\/\/Post Request\nstd::string postData = \"key=value\";\ngateway->addPostTask(url, postData, NULL, NULL);\n\n\/\/Download File\nstd::vector<std::string> downloads;\ndownloads.push_back(\"http:\/\/www.baidu.com\/index.php\");\ngateway->addDownloadTask(urls, NULL, NULL);\n\nYou can specify a callback just by simply replace the NULL parameter with your selector, So you can process the response data in your callback method.\n","584":"A Cocos2d-x Asynchronous Http Request Libray That Let You Make Asynchrounous Http Requests From Cocos2d-x A FUN!\nYou can find the library file:\n\nCCHttpRequest.h\nCCHttpRequest.cpp\n\nUnder CCHttpRequestExample\/Classes\nThis library had been written as a cocos2d-x extension, Use it is very simple:\nCCHttpRequest *gateway = CCHttpRequest::sharedHttpRequest();\n\n\/\/Get Request\nstd::string url = \"http:\/\/www.baidu.com\";\ngateway->addGetTask(url, NULL, NULL);\n\n\/\/Post Request\nstd::string postData = \"key=value\";\ngateway->addPostTask(url, postData, NULL, NULL);\n\n\/\/Download File\nstd::vector<std::string> downloads;\ndownloads.push_back(\"http:\/\/www.baidu.com\/index.php\");\ngateway->addDownloadTask(urls, NULL, NULL);\n\nYou can specify a callback just by simply replace the NULL parameter with your selector, So you can process the response data in your callback method.\n","585":"Setup instruction (OS X)\n\n\nGet dependencies (96.9 MB) and extract them to ofxPCL folder.\n $ curl -O -L http:\/\/cl.ly\/1D1Q3G072q3D\/download\/ofxpcl_16_libs.zip\n $ unzip ofxpcl_16_libs.zip\n\n\n\nChange Project.xcconfig like\n OFXPCL_PATH = $(OF_PATH)\/addons\/ofxPCL\n\n OFXPCL_OTHER_LDFLAGS = -L$(OFXPCL_PATH)\/libs\/pcl\/lib\/osx -lpcl_common -lpcl_features -lpcl_filters -lpcl_geometry -lpcl_io -lpcl_io_ply -lpcl_kdtree -lpcl_keypoints -lpcl_octree -lpcl_registration -lpcl_sample_consensus -lpcl_search -lpcl_segmentation -lpcl_surface -lpcl_tracking -lqhull\n\n OFXPCL_HEADER_SEARCH_PATHS = $(OFXPCL_PATH)\/libs\/pcl\/include\/ $(OFXPCL_PATH)\/libs\/pcl\/include\/eigen3 $(OFXPCL_PATH)\/libs\/pcl\/include\/pcl-1.6\n\n OFXPCL_LD_RUNPATH_SEARCH_PATHS = @executable_path\/..\/..\/..\/..\/..\/..\/..\/addons\/ofxPCL\/libs\/pcl\/lib\/osx @executable_path\/..\/..\/..\/data\/pcl\/lib\n\n LD_RUNPATH_SEARCH_PATHS = $(OFXPCL_LD_RUNPATH_SEARCH_PATHS)\n\n OTHER_LDFLAGS = $(OF_CORE_LIBS) $(OFXPCL_OTHER_LDFLAGS)\n HEADER_SEARCH_PATHS = $(OF_CORE_HEADERS) $(OFXPCL_HEADER_SEARCH_PATHS)\n\n\n\nAdd ofxPCL\/src filder to Xcode project.\n\n\nCopy libraries to data folder\n $ python copyfiles.py PATH_TO_YOUR_PROJECT\n\n\n\n","586":"Launchy\nLaunchy is a free utility designed to help you forget about your start menu, your desktop icons,\nand your file manager. Launchy indexes and launches your applications, documents, project files,\nfolders, and bookmarks with just a few keystrokes!\nThis is a fork from SourceForge project https:\/\/sourceforge.net\/projects\/launchy\/\n","587":"Launchy\nLaunchy is a free utility designed to help you forget about your start menu, your desktop icons,\nand your file manager. Launchy indexes and launches your applications, documents, project files,\nfolders, and bookmarks with just a few keystrokes!\nThis is a fork from SourceForge project https:\/\/sourceforge.net\/projects\/launchy\/\n","588":"\n  \nOKCash OK\n<iframe src=\"https:\/\/discordapp.com\/widget?id=213747404745211904&theme=dark\" width=\"350\" height=\"500\" allowtransparency=\"true\" frameborder=\"0\"><\/iframe>\nJoin the new communications server\n\n\n\n\nDownload Supported Platforms\n  \n  \nDownload the Instant OK-Blockchain (Fast Sync for first time users):\n https:\/\/github.com\/okcashpro\/ok-blockchain\/releases\n\nOKCash is digital cash. OK is the official symbol.\nTransactions have real fast confirmations, making them virtually instant.\nYou can send OKCash to family or friends, or pay for goods or services, anywhere in the world.\nThe OKCash network is decentralized and free from middlemen, giving you back control of your finances and providing a secure network for all of your payments.\nSome of the OKCash features: Fast, Efficient, Social, Community based (like Bitcoin), Transparency, Private Messages, friendly to use, Universal name, Multicultural, Multi-platform, IoT(Internet of Things), Tor and i2p compatible.\n\nVisit OK Smart Links and join the different channels\nWhat is Okcash?  http:\/\/okcash.co\nOK videos: Youtube\nOkcashTalk Forums *new:  http:\/\/okcashtalk.org\nBitcoinTalk Ann: https:\/\/bitcointalk.org\/index.php?topic=1028368.0\nOK Twitter community: https:\/\/twitter.com\/OkcashCrypto\nOK Facebook community: https:\/\/www.facebook.com\/OKCashCrypto\/\nOK Reddit community: https:\/\/reddit.com\/r\/okcash\nOK Communications Channels: https:\/\/discord.gg\/grvpc8c\n\nOKCash Development process\nDevelopers work in their own forks, then submit pull requests when\nthey think their feature or bug fix is ready.\nThe patch will be accepted if there is broad consensus that it is a\ngood thing.  Developers should expect to rework and resubmit patches\nif they don't match the project's coding conventions (see coding.txt)\nor are controversial.\nThe master branch is regularly built and tested, but is not guaranteed\nto be completely stable. Tags are regularly created to indicate new\nstable release versions of OKCash.\nFeature branches are created when there are major new features being\nworked on by several people.\nFrom time to time a pull request will become outdated. If this occurs, and\nthe pull is no longer automatically merge able; a comment on the pull will\nbe used to issue a warning of closure. The pull will be closed 15 days\nafter the warning if action is not taken by the author. Pull requests closed\nin this manner will have their corresponding issue labelled 'stagnant'.\nIssues with no commits will be given a similar warning, and closed after\n15 days from their last activity. Issues closed in this manner will be\nlabelled 'stale'.\n"},"watchers":{"0":28,"1":49,"2":6,"3":11,"4":5,"5":3,"6":2,"7":7,"8":18,"9":1,"10":2,"11":10,"12":2,"13":16,"14":3,"15":4,"16":2,"17":0,"18":11,"19":1,"20":178,"21":397,"22":27,"23":10,"24":20,"25":20,"26":39,"27":19,"28":6,"29":10,"30":1,"31":9,"32":1,"33":3,"34":1,"35":1,"36":3,"37":2,"38":2,"39":34,"40":31,"41":53,"42":13,"43":15,"44":8,"45":23,"46":4,"47":21,"48":4,"49":40,"50":17,"51":16,"52":24,"53":31,"54":15,"55":4,"56":5,"57":5,"58":2,"59":20,"60":154,"61":15,"62":85,"63":33,"64":2,"65":4,"66":36,"67":2,"68":0,"69":347,"70":73,"71":291,"72":44,"73":83,"74":112,"75":99,"76":403,"77":38,"78":2,"79":156,"80":8,"81":6,"82":2,"83":8,"84":2,"85":4,"86":5,"87":2,"88":3,"89":160,"90":19,"91":1,"92":4,"93":161,"94":3,"95":53,"96":10,"97":13,"98":15,"99":7,"100":22,"101":4,"102":5,"103":28,"104":8,"105":2,"106":1,"107":5,"108":1,"109":74,"110":5,"111":5,"112":2,"113":4,"114":6,"115":3,"116":17,"117":1,"118":6,"119":150,"120":51,"121":136,"122":34,"123":48,"124":29,"125":3,"126":1,"127":76,"128":21,"129":6,"130":16,"131":3,"132":3,"133":3,"134":8,"135":4,"136":1,"137":1,"138":2,"139":32,"140":13,"141":11,"142":9,"143":26,"144":24,"145":4,"146":4,"147":1,"148":6,"149":0,"150":16,"151":4,"152":17,"153":3,"154":2,"155":4,"156":5,"157":0,"158":3,"159":0,"160":1,"161":6,"162":142,"163":2,"164":2,"165":3,"166":2,"167":20,"168":1,"169":8,"170":79,"171":30,"172":63,"173":3,"174":22,"175":25,"176":5,"177":14,"178":5,"179":164,"180":113,"181":5,"182":9,"183":2,"184":4,"185":3,"186":3,"187":5,"188":3,"189":10,"190":6,"191":16,"192":0,"193":12,"194":5,"195":3,"196":1,"197":9,"198":2,"199":39,"200":96,"201":34,"202":23,"203":25,"204":18,"205":22,"206":10,"207":14,"208":40,"209":113,"210":2,"211":25,"212":19,"213":6,"214":22,"215":23,"216":33,"217":20,"218":15,"219":23,"220":107,"221":50,"222":28,"223":14,"224":19,"225":10,"226":13,"227":34,"228":52,"229":22,"230":4,"231":12,"232":16,"233":22,"234":7,"235":6,"236":3,"237":6,"238":47,"239":33,"240":11,"241":2,"242":18,"243":22,"244":18,"245":10,"246":15,"247":8,"248":11,"249":2,"250":8,"251":13,"252":3,"253":12,"254":7,"255":6,"256":2,"257":3,"258":19,"259":12,"260":18,"261":26,"262":8,"263":7,"264":5,"265":24,"266":13,"267":12,"268":14,"269":47,"270":12,"271":8,"272":4,"273":10,"274":9,"275":9,"276":13,"277":24,"278":6,"279":17,"280":15,"281":15,"282":1,"283":10,"284":12,"285":57,"286":7,"287":5,"288":14,"289":9,"290":21,"291":8,"292":17,"293":16,"294":7,"295":6,"296":15,"297":14,"298":24,"299":9,"300":5,"301":4,"302":40,"303":4,"304":12,"305":10,"306":7,"307":38,"308":12,"309":26,"310":39,"311":26,"312":15,"313":40,"314":3,"315":8,"316":13,"317":6,"318":19,"319":10,"320":19,"321":11,"322":18,"323":28,"324":20,"325":17,"326":22,"327":37,"328":19,"329":4,"330":12,"331":17,"332":7,"333":19,"334":3,"335":5,"336":3,"337":4,"338":14,"339":6,"340":9,"341":13,"342":39,"343":24,"344":9,"345":40,"346":5,"347":41,"348":8,"349":15,"350":13,"351":10,"352":8,"353":3,"354":21,"355":15,"356":15,"357":11,"358":13,"359":15,"360":12,"361":6,"362":18,"363":1,"364":10,"365":21,"366":50,"367":11,"368":18,"369":9,"370":7,"371":8,"372":26,"373":16,"374":4,"375":16,"376":7,"377":10,"378":13,"379":12,"380":23,"381":15,"382":5,"383":18,"384":18,"385":2,"386":15,"387":32,"388":20,"389":54,"390":15,"391":22,"392":22,"393":42,"394":21,"395":7,"396":3,"397":23,"398":15,"399":4,"400":8,"401":14,"402":10,"403":12,"404":15,"405":8,"406":27,"407":7,"408":5,"409":2,"410":7,"411":18,"412":21,"413":6,"414":3,"415":41,"416":8,"417":8,"418":141,"419":8,"420":10,"421":16,"422":9,"423":11,"424":8,"425":16,"426":27,"427":7,"428":5,"429":4,"430":2,"431":16,"432":4,"433":5,"434":4,"435":12,"436":16,"437":0,"438":12,"439":21,"440":7,"441":6,"442":20,"443":8,"444":4,"445":6,"446":8,"447":17,"448":8,"449":1,"450":7,"451":5,"452":6,"453":11,"454":93,"455":6,"456":32,"457":19,"458":6,"459":8,"460":8,"461":16,"462":3,"463":26,"464":29,"465":6,"466":32,"467":11,"468":12,"469":6,"470":14,"471":12,"472":11,"473":5,"474":8,"475":45,"476":21,"477":6,"478":20,"479":8,"480":4,"481":10,"482":3,"483":6,"484":9,"485":8,"486":19,"487":11,"488":8,"489":19,"490":7,"491":12,"492":28,"493":3,"494":6,"495":12,"496":15,"497":28,"498":6,"499":8,"500":12,"501":12,"502":13,"503":3,"504":6,"505":6,"506":20,"507":9,"508":9,"509":5,"510":3,"511":6,"512":6,"513":4,"514":14,"515":31,"516":10,"517":5,"518":8,"519":17,"520":10,"521":9,"522":10,"523":13,"524":6,"525":12,"526":21,"527":115,"528":7,"529":31,"530":11,"531":22,"532":11,"533":24,"534":7,"535":24,"536":4,"537":11,"538":13,"539":5,"540":8,"541":24,"542":1,"543":18,"544":12,"545":99,"546":8,"547":24,"548":7,"549":9,"550":12,"551":25,"552":9,"553":6,"554":35,"555":5,"556":6,"557":14,"558":12,"559":11,"560":4,"561":43,"562":4,"563":12,"564":5,"565":5,"566":5,"567":10,"568":5,"569":4,"570":9,"571":25,"572":12,"573":10,"574":4,"575":1,"576":5,"577":10,"578":2,"579":11,"580":47,"581":3,"582":16,"583":17,"584":8,"585":5,"586":12,"587":9,"588":42},"stars":{"0":"161","1":"621","2":"26","3":"123","4":"93","5":"87","6":"81","7":"74","8":"124","9":"39","10":"26","11":"164","12":"5","13":"373","14":"13","15":"1","16":"3","17":"3","18":"2","19":"3","20":"8.6k","21":"5.4k","22":"391","23":"31","24":"166","25":"201","26":"441","27":"137","28":"10","29":"61","30":"19","31":"10","32":"17","33":"15","34":"2","35":"5","36":"4","37":"10","38":"5","39":"182","40":"93","41":"381","42":"138","43":"59","44":"39","45":"181","46":"9","47":"24","48":"8","49":"226","50":"186","51":"42","52":"93","53":"465","54":"52","55":"95","56":"28","57":"12","58":"18","59":"354","60":"2.2k","61":"211","62":"3.1k","63":"191","64":"42","65":"15","66":"25","67":"9","68":"2","69":"2.6k","70":"388","71":"4.7k","72":"267","73":"509","74":"883","75":"1.2k","76":"4.2k","77":"537","78":"39","79":"3.7k","80":"59","81":"109","82":"13","83":"75","84":"22","85":"23","86":"5","87":"5","88":"12","89":"2.5k","90":"100","91":"28","92":"36","93":"1.2k","94":"11","95":"439","96":"101","97":"128","98":"57","99":"376","100":"114","101":"45","102":"15","103":"73","104":"61","105":"6","106":"0","107":"24","108":"4","109":"58","110":"16","111":"6","112":"13","113":"1","114":"14","115":"8","116":"47","117":"0","118":"10","119":"748","120":"2.2k","121":"1.3k","122":"92","123":"218","124":"221","125":"13","126":"22","127":"398","128":"87","129":"7","130":"16","131":"23","132":"4","133":"0","134":"2","135":"4","136":"0","137":"0","138":"0","139":"134","140":"57","141":"61","142":"13","143":"26","144":"291","145":"9","146":"7","147":"5","148":"34","149":"0","150":"56","151":"19","152":"42","153":"3","154":"27","155":"8","156":"4","157":"0","158":"5","159":"46","160":"6","161":"10","162":"578","163":"5","164":"7","165":"5","166":"0","167":"49","168":"0","169":"36","170":"494","171":"184","172":"129","173":"16","174":"134","175":"108","176":"17","177":"42","178":"9","179":"2.6k","180":"920","181":"25","182":"23","183":"14","184":"9","185":"1","186":"9","187":"1","188":"4","189":"20","190":"24","191":"30","192":"5","193":"31","194":"16","195":"1","196":"8","197":"26","198":"2","199":683,"200":659,"201":564,"202":532,"203":509,"204":504,"205":501,"206":498,"207":472,"208":461,"209":529,"210":462,"211":317,"212":302,"213":300,"214":296,"215":285,"216":274,"217":268,"218":231,"219":607,"220":588,"221":531,"222":477,"223":446,"224":400,"225":327,"226":325,"227":324,"228":311,"229":269,"230":153,"231":148,"232":144,"233":128,"234":126,"235":116,"236":115,"237":111,"238":109,"239":254,"240":252,"241":252,"242":252,"243":251,"244":251,"245":251,"246":250,"247":249,"248":245,"249":176,"250":175,"251":167,"252":157,"253":157,"254":154,"255":152,"256":145,"257":145,"258":140,"259":157,"260":155,"261":152,"262":151,"263":150,"264":148,"265":147,"266":146,"267":146,"268":142,"269":109,"270":106,"271":104,"272":101,"273":99,"274":99,"275":98,"276":97,"277":94,"278":94,"279":236,"280":234,"281":231,"282":231,"283":230,"284":222,"285":217,"286":217,"287":216,"288":216,"289":140,"290":139,"291":138,"292":138,"293":137,"294":137,"295":136,"296":136,"297":136,"298":132,"299":215,"300":213,"301":210,"302":206,"303":204,"304":203,"305":201,"306":199,"307":198,"308":194,"309":118,"310":118,"311":116,"312":114,"313":115,"314":115,"315":112,"316":112,"317":343,"318":342,"319":326,"320":307,"321":302,"322":301,"323":299,"324":298,"325":298,"326":295,"327":133,"328":132,"329":132,"330":131,"331":131,"332":130,"333":130,"334":130,"335":126,"336":131,"337":130,"338":124,"339":123,"340":122,"341":120,"342":120,"343":117,"344":115,"345":114,"346":81,"347":81,"348":81,"349":80,"350":80,"351":80,"352":78,"353":78,"354":77,"355":75,"356":188,"357":187,"358":186,"359":185,"360":184,"361":184,"362":183,"363":183,"364":180,"365":180,"366":182,"367":182,"368":181,"369":178,"370":175,"371":175,"372":173,"373":168,"374":164,"375":153,"376":152,"377":150,"378":147,"379":144,"380":141,"381":86,"382":86,"383":86,"384":86,"385":84,"386":84,"387":83,"388":83,"389":83,"390":83,"391":196,"392":196,"393":196,"394":195,"395":195,"396":195,"397":192,"398":190,"399":189,"400":189,"401":140,"402":139,"403":138,"404":137,"405":137,"406":110,"407":110,"408":109,"409":109,"410":109,"411":108,"412":108,"413":108,"414":107,"415":107,"416":81,"417":80,"418":80,"419":80,"420":77,"421":228,"422":226,"423":225,"424":216,"425":211,"426":110,"427":109,"428":109,"429":108,"430":108,"431":106,"432":106,"433":106,"434":106,"435":105,"436":93,"437":93,"438":92,"439":91,"440":91,"441":90,"442":90,"443":89,"444":89,"445":62,"446":62,"447":62,"448":61,"449":61,"450":60,"451":60,"452":60,"453":59,"454":59,"455":234,"456":231,"457":224,"458":105,"459":105,"460":105,"461":105,"462":132,"463":125,"464":123,"465":123,"466":122,"467":85,"468":85,"469":84,"470":83,"471":165,"472":165,"473":165,"474":165,"475":165,"476":165,"477":164,"478":163,"479":163,"480":163,"481":105,"482":105,"483":105,"484":90,"485":90,"486":89,"487":89,"488":63,"489":63,"490":63,"491":63,"492":63,"493":63,"494":62,"495":62,"496":62,"497":157,"498":157,"499":157,"500":155,"501":155,"502":155,"503":155,"504":155,"505":154,"506":154,"507":100,"508":100,"509":99,"510":99,"511":99,"512":99,"513":98,"514":97,"515":97,"516":97,"517":103,"518":103,"519":102,"520":102,"521":102,"522":102,"523":102,"524":102,"525":101,"526":101,"527":62,"528":62,"529":62,"530":61,"531":183,"532":178,"533":178,"534":177,"535":177,"536":176,"537":100,"538":99,"539":105,"540":104,"541":103,"542":103,"543":56,"544":55,"545":55,"546":55,"547":54,"548":54,"549":54,"550":54,"551":53,"552":53,"553":152,"554":152,"555":152,"556":152,"557":151,"558":151,"559":151,"560":151,"561":150,"562":150,"563":97,"564":96,"565":95,"566":95,"567":95,"568":95,"569":94,"570":94,"571":93,"572":88,"573":88,"574":88,"575":88,"576":88,"577":88,"578":88,"579":58,"580":58,"581":58,"582":58,"583":58,"584":58,"585":58,"586":58,"587":57,"588":57},"forks":{"0":"98","1":"150","2":"48","3":"58","4":"42","5":"25","6":"34","7":"43","8":"32","9":"20","10":"7","11":"365","12":"5","13":"48","14":"4","15":"5","16":"2","17":"8","18":"4","19":"2","20":"495","21":"493","22":"111","23":"45","24":"57","25":"108","26":"197","27":"98","28":"14","29":"12","30":"16","31":"5","32":"4","33":"0","34":"4","35":"0","36":"3","37":"1","38":"1","39":"105","40":"31","41":"190","42":"60","43":"49","44":"18","45":"71","46":"5","47":"12","48":"4","49":"102","50":"51","51":"23","52":"33","53":"103","54":"22","55":"28","56":"28","57":"6","58":"18","59":"99","60":"787","61":"77","62":"364","63":"15","64":"7","65":"30","66":"9","67":"14","68":"36","69":"1.4k","70":"401","71":"1k","72":"304","73":"288","74":"229","75":"333","76":"697","77":"103","78":"50","79":"778","80":"20","81":"9","82":"10","83":"18","84":"4","85":"3","86":"3","87":"2","88":"3","89":"859","90":"57","91":"9","92":"29","93":"141","94":"31","95":"118","96":"22","97":"32","98":"15","99":"114","100":"60","101":"11","102":"14","103":"31","104":"29","105":"6","106":"5","107":"28","108":"4","109":"25","110":"11","111":"9","112":"31","113":"23","114":"3","115":"5","116":"48","117":"11","118":"13","119":"352","120":"1.1k","121":"695","122":"37","123":"117","124":"486","125":"12","126":"11","127":"177","128":"82","129":"9","130":"4","131":"5","132":"5","133":"4","134":"3","135":"1","136":"0","137":"0","138":"0","139":"42","140":"14","141":"21","142":"18","143":"24","144":"116","145":"4","146":"4","147":"2","148":"12","149":"69","150":"24","151":"22","152":"11","153":"5","154":"6","155":"4","156":"3","157":"2","158":"1","159":"35","160":"9","161":"7","162":"351","163":"7","164":"2","165":"1","166":"1","167":"30","168":"0","169":"19","170":"155","171":"46","172":"23","173":"9","174":"29","175":"32","176":"6","177":"31","178":"8","179":"615","180":"310","181":"6","182":"1","183":"3","184":"2","185":"4","186":"4","187":"4","188":"0","189":"33","190":"13","191":"13","192":"9","193":"20","194":"6","195":"6","196":"5","197":"9","198":"1","199":110,"200":374,"201":211,"202":37,"203":32,"204":30,"205":60,"206":26,"207":60,"208":128,"209":51,"210":0,"211":134,"212":30,"213":104,"214":62,"215":92,"216":72,"217":208,"218":33,"219":155,"220":356,"221":205,"222":229,"223":131,"224":55,"225":41,"226":52,"227":108,"228":77,"229":28,"230":5,"231":7,"232":33,"233":33,"234":11,"235":29,"236":64,"237":25,"238":38,"239":93,"240":26,"241":10,"242":54,"243":20,"244":33,"245":26,"246":29,"247":15,"248":63,"249":31,"250":11,"251":44,"252":10,"253":62,"254":20,"255":4,"256":28,"257":8,"258":32,"259":63,"260":29,"261":60,"262":14,"263":80,"264":11,"265":52,"266":12,"267":41,"268":7,"269":38,"270":44,"271":81,"272":13,"273":28,"274":7,"275":57,"276":21,"277":56,"278":15,"279":76,"280":36,"281":23,"282":21,"283":18,"284":6,"285":75,"286":19,"287":12,"288":12,"289":14,"290":18,"291":40,"292":20,"293":19,"294":15,"295":10,"296":8,"297":32,"298":25,"299":71,"300":41,"301":16,"302":75,"303":30,"304":12,"305":59,"306":44,"307":103,"308":25,"309":19,"310":72,"311":54,"312":154,"313":45,"314":64,"315":22,"316":60,"317":37,"318":17,"319":82,"320":56,"321":110,"322":9,"323":16,"324":139,"325":39,"326":115,"327":33,"328":10,"329":13,"330":22,"331":30,"332":11,"333":37,"334":12,"335":15,"336":14,"337":21,"338":42,"339":10,"340":38,"341":41,"342":44,"343":40,"344":24,"345":60,"346":1,"347":10,"348":16,"349":56,"350":81,"351":23,"352":10,"353":7,"354":23,"355":12,"356":29,"357":42,"358":41,"359":46,"360":62,"361":15,"362":13,"363":5,"364":66,"365":53,"366":21,"367":48,"368":64,"369":15,"370":63,"371":11,"372":68,"373":64,"374":32,"375":50,"376":21,"377":42,"378":47,"379":6,"380":56,"381":24,"382":22,"383":43,"384":17,"385":7,"386":29,"387":46,"388":14,"389":72,"390":44,"391":95,"392":9,"393":85,"394":19,"395":11,"396":18,"397":112,"398":39,"399":51,"400":21,"401":22,"402":32,"403":40,"404":16,"405":10,"406":33,"407":26,"408":21,"409":19,"410":53,"411":30,"412":20,"413":22,"414":35,"415":71,"416":16,"417":50,"418":8,"419":30,"420":12,"421":48,"422":39,"423":19,"424":51,"425":59,"426":23,"427":12,"428":7,"429":27,"430":7,"431":6,"432":17,"433":21,"434":11,"435":56,"436":58,"437":14,"438":54,"439":25,"440":17,"441":201,"442":139,"443":30,"444":7,"445":16,"446":6,"447":22,"448":13,"449":6,"450":41,"451":25,"452":28,"453":18,"454":29,"455":62,"456":63,"457":78,"458":32,"459":54,"460":9,"461":25,"462":42,"463":91,"464":94,"465":10,"466":100,"467":27,"468":10,"469":34,"470":47,"471":36,"472":29,"473":12,"474":6,"475":64,"476":86,"477":13,"478":12,"479":52,"480":5,"481":18,"482":19,"483":25,"484":57,"485":7,"486":61,"487":27,"488":11,"489":43,"490":23,"491":2,"492":49,"493":5,"494":4,"495":43,"496":8,"497":7,"498":18,"499":11,"500":15,"501":10,"502":9,"503":13,"504":19,"505":4,"506":14,"507":84,"508":4,"509":18,"510":3,"511":21,"512":2,"513":12,"514":29,"515":30,"516":11,"517":51,"518":27,"519":54,"520":48,"521":52,"522":36,"523":19,"524":26,"525":29,"526":22,"527":14,"528":28,"529":22,"530":16,"531":52,"532":60,"533":39,"534":66,"535":28,"536":11,"537":31,"538":20,"539":19,"540":31,"541":69,"542":0,"543":11,"544":11,"545":10,"546":18,"547":33,"548":8,"549":16,"550":39,"551":23,"552":306,"553":5,"554":72,"555":28,"556":24,"557":61,"558":128,"559":11,"560":33,"561":9,"562":15,"563":16,"564":26,"565":9,"566":7,"567":26,"568":17,"569":7,"570":9,"571":44,"572":84,"573":28,"574":22,"575":19,"576":7,"577":69,"578":16,"579":6,"580":47,"581":15,"582":22,"583":26,"584":10,"585":22,"586":9,"587":18,"588":60},"commits":{"0":"411","1":"294","2":"9","3":"184","4":"16","5":"70","6":"46","7":"23","8":"130","9":"5","10":"31","11":"352","12":"598","13":"533","14":"29","15":"364","16":"7","17":"239","18":"192","19":"120","20":"338","21":"29","22":"1,070","23":"55","24":"561","25":"6,657","26":"157","27":"209","28":"151","29":"736","30":"7","31":"2,468","32":"13","33":"237","34":"140","35":"479","36":"84","37":"15","38":"127","39":"7,249","40":"1,714","41":"8,099","42":"1,568","43":"1,172","44":"3,023","45":"2,566","46":"216","47":"1,423","48":"261","49":"321","50":"659","51":"210","52":"23","53":"12","54":"64","55":"7","56":"22","57":"57","58":"183","59":"80","60":"2,265","61":"26","62":"843","63":"139","64":"200","65":"96","66":"243","67":"87","68":"61","69":"1,331","70":"123","71":"4,468","72":"10","73":"32","74":"747","75":"230","76":"2,364","77":"67","78":"4","79":"29","80":"16","81":"778","82":"5","83":"1,694","84":"34","85":"15","86":"187","87":"80","88":"19","89":"5,893","90":"1,330","91":"11","92":"28","93":"194","94":"1,778","95":"1,591","96":"2,803","97":"165","98":"1,117","99":"52","100":"1,384","101":"84","102":"1,120","103":"1,567","104":"386","105":"10","106":"1","107":"8","108":"111","109":"400","110":"15","111":"95","112":"72","113":"12","114":"897","115":"609","116":"34","117":"52","118":"489","119":"469","120":"3,860","121":"1,528","122":"3,218","123":"22","124":"129","125":"5","126":"7","127":"127","128":"35","129":"230","130":"94","131":"426","132":"236","133":"10","134":"26","135":"7","136":"77","137":"3","138":"6","139":"8","140":"8,225","141":"4,880","142":"156","143":"2,054","144":"2,478","145":"8","146":"352","147":"2","148":"11,434","149":"12","150":"4,338","151":"30","152":"316","153":"36","154":"11","155":"8","156":"42","157":"8","158":"7","159":"12","160":"12","161":"1,604","162":"1,120","163":"80","164":"9","165":"13","166":"12","167":"200","168":"4","169":"2","170":"9","171":"565","172":"1,241","173":"9","174":"1,586","175":"228","176":"7","177":"110","178":"940","179":"5,810","180":"19,237","181":"5","182":"70","183":"117","184":"624","185":"29","186":"890","187":"243","188":"471","189":"33","190":"602","191":"2,084","192":"52","193":"568","194":"113","195":"300","196":"223","197":"1,268","198":"14","199":"133","200":"610","201":"111","202":"1,636","203":"81","204":"53","205":"1,528","206":"198","207":"23","208":"1,464","209":"76","210":"11","211":"26","212":"41","213":"20","214":"36","215":"67","216":"12","217":"13","218":"599","219":"36","220":"238","221":"54","222":"8,048","223":"4","224":"12","225":"18","226":"44","227":"42","228":"416","229":"182","230":"51","231":"424","232":"164","233":"3","234":"31","235":"90","236":"3","237":"67","238":"96","239":"137","240":"26","241":"8","242":"119","243":"149","244":"86","245":"550","246":"225","247":"54","248":"45","249":"228","250":"692","251":"31","252":"126","253":"77","254":"25","255":"19","256":"122","257":"50","258":"752","259":"61","260":"30","261":"1,374","262":"52","263":"14","264":"32","265":"159","266":"90","267":"7","268":"224","269":"96","270":"7","271":"76","272":"14","273":"28","274":"245","275":"58","276":"45","277":"3,903","278":"46","279":"91","280":"43","281":"16","282":"24","283":"6","284":"170","285":"214","286":"74","287":"4","288":"73","289":"63","290":"3,623","291":"70","292":"67","293":"151","294":"34","295":"150","296":"67","297":"62","298":"73","299":"30","300":"15","301":"96","302":"5","303":"7","304":"146","305":"26","306":"12","307":"519","308":"12","309":"104","310":"5","311":"158","312":"61","313":"47","314":"3","315":"148","316":"14","317":"152","318":"85","319":"209","320":"1,309","321":"41","322":"75","323":"66","324":"95","325":"26","326":"119","327":"32","328":"39","329":"110","330":"159","331":"28","332":"18","333":"42","334":"386","335":"6","336":"10","337":"7","338":"2","339":"31","340":"26","341":"17","342":"204","343":"1,152","344":"13","345":"708","346":"24","347":"1,484","348":"9","349":"20","350":"89","351":"7","352":"422","353":"8","354":"160","355":"3,035","356":"24","357":"85","358":"45","359":"29","360":"41","361":"36","362":"219","363":"361","364":"72","365":"28","366":"65","367":"10","368":"7","369":"171","370":"105","371":"692","372":"485","373":"33","374":"45","375":"116","376":"17","377":"8","378":"17","379":"10","380":"165","381":"417","382":"55","383":"108","384":"192","385":"31","386":"35","387":"104,142","388":"807","389":"57","390":"56","391":"11","392":"35","393":"1,291","394":"167","395":"223","396":"14","397":"27","398":"13","399":"275","400":"150","401":"90","402":"83","403":"13","404":"7","405":"33","406":"102","407":"4","408":"45","409":"47","410":"8","411":"135","412":"43","413":"15","414":"13","415":"11","416":"9","417":"35","418":"2","419":"29","420":"9","421":"832","422":"66","423":"32","424":"16","425":"299","426":"379","427":"503","428":"101","429":"34","430":"15","431":"22","432":"58","433":"215","434":"411","435":"44","436":"5","437":"14","438":"58","439":"293","440":"7","441":"65","442":"162","443":"9","444":"23","445":"2","446":"34","447":"2,109","448":"44","449":"9","450":"32","451":"14,488","452":"2","453":"4","454":"68","455":"35","456":"113","457":"267","458":"207","459":"13","460":"105","461":"2","462":"102","463":"803","464":"10","465":"31","466":"9","467":"2","468":"494","469":"5","470":"76","471":"396","472":"49","473":"17","474":"116","475":"403","476":"1","477":"70","478":"91","479":"80","480":"111","481":"47","482":"228","483":"72","484":"5","485":"95","486":"52","487":"54","488":"286","489":"6","490":"1","491":"3,495","492":"8,265","493":"5","494":"843","495":"4","496":"386","497":"67","498":"25","499":"118","500":"56","501":"206","502":"42","503":"115","504":"47","505":"38","506":"236","507":"181","508":"140","509":"191","510":"12","511":"78","512":"27","513":"89","514":"32","515":"99","516":"205","517":"119","518":"58","519":"152","520":"14","521":"256","522":"77","523":"15","524":"54","525":"51","526":"337","527":"347","528":"8","529":"1,493","530":"39","531":"169","532":"15","533":"159","534":"221","535":"154","536":"20","537":"139","538":"34","539":"2","540":"9","541":"50","542":"13","543":"132","544":"2,720","545":"1,372","546":"17","547":"1,075","548":"48","549":"596","550":"13","551":"611","552":"30","553":"270","554":"185","555":"68","556":"121","557":"11","558":"238","559":"16","560":"71","561":"29","562":"64","563":"56","564":"29","565":"29","566":"84","567":"94","568":"185","569":"79","570":"89","571":"13,037","572":"335","573":"189","574":"36","575":"24","576":"15","577":"4","578":"18","579":"249","580":"5,983","581":"13","582":"116","583":"4","584":"412","585":"28","586":"728","587":"3","588":"177"}}