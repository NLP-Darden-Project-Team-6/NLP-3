language,readme,stemmed,lemmatized,clean
JavaScript ,"FCC League-For-Good
This is a free, open-source web application designed to help sports leagues track their player and team stats, and simplify the other day-to-day tasks involved with team management. It was designed to accommodate many popular sports.
Join Us On Slack!
You can now join us on slack. Get Invite Here
Getting Started
Prerequisites

NodeJS
MongoDB

In order for the authorization component of this app to work, it needs to be registered with Google. Here is a helpful walkthrough of that process: https://developers.google.com/identity/sign-in/web/devconsole-project.
You will want to register Type as Web application, set Authorized JavaScript origins to
http://localhost:4000 (if you're running the application locally) and set the Authorized
redirect URI to http://localhost:4000/auth/google/callback (this can be set through the Google API
Console Dashboard under Credentials if not offered as an option during setup).
You will also need to enable the ""Google+ API"" on the Google API Console Dashboard - if you forget,
Google will display an error message (with a link to the API) the first time you try to log in.
Steps

Fork and clone the repo
Run npm install
In the root directory, create a .env file and place the following:

MONGO_URI = Your database uri - typically mongodb://localhost:27017/your_project_name if your MongoDB is local
GOOGLE_CLIENT_ID = Client id assigned by Google
GOOGLE_CLIENT_SECRET = Client secret assigned by Google
SESSION_SECRET = Any random string of characters
GOOGLE_CALLBACK_URL = http://localhost:4000/auth/google/callback (Use for the callback url in the Google API console)


Open a new terminal session and run mongod if mongodb is on the local machine
Run npm run dev
Navigate to localhost:4000 in your browser

Available Scripts
In the project directory, the following commands are available:
npm install
Installs all the dependencies
npm run dev
Builds the app for development. It is watched by webpack for any changes in the front end.
How to Contribute
We warmly welcome contributions from anyone. Check out our how to contribute section to find out how you can do so.
",fcc leagueforgood thi is a free opensourc web applic design to help sport leagu track their player and team stat and simplifi the other daytoday task involv with team manag it wa design to accommod mani popular sport join us on slack you can now join us on slack get invit here get start prerequisit nodej mongodb in order for the author compon of thi app to work it need to be regist with googl here is a help walkthrough of that process httpsdevelopersgooglecomidentitysigninwebdevconsoleproject you will want to regist type as web applic set author javascript origin to httplocalhost4000 if your run the applic local and set the author redirect uri to httplocalhost4000authgooglecallback thi can be set through the googl api consol dashboard under credenti if not offer as an option dure setup you will also need to enabl the googl api on the googl api consol dashboard if you forget googl will display an error messag with a link to the api the first time you tri to log in step fork and clone the repo run npm instal in the root directori creat a env file and place the follow mongo_uri your databas uri typic mongodblocalhost27017your_project_nam if your mongodb is local google_client_id client id assign by googl google_client_secret client secret assign by googl session_secret ani random string of charact google_callback_url httplocalhost4000authgooglecallback use for the callback url in the googl api consol open a new termin session and run mongod if mongodb is on the local machin run npm run dev navig to localhost4000 in your browser avail script in the project directori the follow command are avail npm instal instal all the depend npm run dev build the app for develop it is watch by webpack for ani chang in the front end how to contribut we warmli welcom contribut from anyon check out our how to contribut section to find out how you can do so,fcc leagueforgood this is a free opensource web application designed to help sport league track their player and team stats and simplify the other daytoday task involved with team management it wa designed to accommodate many popular sport join u on slack you can now join u on slack get invite here getting started prerequisite nodejs mongodb in order for the authorization component of this app to work it need to be registered with google here is a helpful walkthrough of that process httpsdevelopersgooglecomidentitysigninwebdevconsoleproject you will want to register type a web application set authorized javascript origin to httplocalhost4000 if youre running the application locally and set the authorized redirect uri to httplocalhost4000authgooglecallback this can be set through the google api console dashboard under credential if not offered a an option during setup you will also need to enable the google api on the google api console dashboard if you forget google will display an error message with a link to the api the first time you try to log in step fork and clone the repo run npm install in the root directory create a env file and place the following mongo_uri your database uri typically mongodblocalhost27017your_project_name if your mongodb is local google_client_id client id assigned by google google_client_secret client secret assigned by google session_secret any random string of character google_callback_url httplocalhost4000authgooglecallback use for the callback url in the google api console open a new terminal session and run mongod if mongodb is on the local machine run npm run dev navigate to localhost4000 in your browser available script in the project directory the following command are available npm install installs all the dependency npm run dev build the app for development it is watched by webpack for any change in the front end how to contribute we warmly welcome contribution from anyone check out our how to contribute section to find out how you can do so,fcc leagueforgood free opensource web application designed help sport league track player team stats simplify daytoday task involved team management designed accommodate many popular sport join u slack join u slack get invite getting started prerequisite nodejs mongodb order authorization component app work need registered google helpful walkthrough process httpsdevelopersgooglecomidentitysigninwebdevconsoleproject want register type web application set authorized javascript origin httplocalhost4000 youre running application locally set authorized redirect uri httplocalhost4000authgooglecallback set google api console dashboard credential offered option setup also need enable google api google api console dashboard forget google display error message link api first time try log step fork clone repo run npm install root directory create env file place following mongo_uri database uri typically mongodblocalhost27017your_project_name mongodb local google_client_id client id assigned google google_client_secret client secret assigned google session_secret random string character google_callback_url httplocalhost4000authgooglecallback use callback url google api console open new terminal session run mongod mongodb local machine run npm run dev navigate localhost4000 browser available script project directory following command available npm install installs dependency npm run dev build app development watched webpack change front end contribute warmly welcome contribution anyone check contribute section find
JavaScript ,"nba
Node.js client for nba.com API endpoints
npm install nba
NOTES:
BROWSER USAGE
This package can't be used from the browser because of CORS restrictions imposed by nba.com. Currently the hostnames are hardcoded so this package can't be used with a proxy host but if you want support for this use case please open an issue!
BLACKLISTED IP ADDRESSES:
It appears as though the NBA has blacklisted certain blocks of IP addresses, specifically those of cloud hosting providers including AWS. As such, you may hit a situation where an application using this package works fine on your local machine, but doesn't work at all when deployed to a cloud server. Annoyingly, requests from these IPs seem to just hang. More information here and here -- the second issue has a curl command somewhere which will quickly tell you if NBA is accepting requests from your IP. (Incidentally, this is also the same reason the TravisCI build is always ""broken"" but tests all pass locally). There is a simple pass-through server in scripts/proxy that can be used to get around this restriction; you can put the proxy server somewhere that can reach NBA.com (e.g. not on AWS or Heroku or similar) and host your actual application on a cloud provider.
NBA API
The stats.nba.com uses a large number of undocumented JSON endpoints to provide the statistics tables and charts displayed on that website. This library provides a JavaScript client for interacting with many of those API endpoints.
Getting Started
NBA.findPlayer(str) will return an object with a player's name, their ID, and their team information. This method is built into the package.
All methods in the NBA.stats namespace require an object to be passed in as a parameter. The keys to the object are in the docs for the stats namespace here
const NBA = require(""nba"");
const curry = NBA.findPlayer('Stephen Curry');
console.log(curry);
/* logs the following:
{
  firstName: 'Stephen',
  lastName: 'Curry',
  playerId: 201939,
  teamId: 1610612744,
  fullName: 'Stephen Curry',
  downcaseName: 'stephen curry'
}
*/
NBA.stats.playerInfo({ PlayerID: curry.playerId }).then(console.log);
For more example API calls, see /test/integration/stats.js
Stability Warning
This is a client for an unstable and undocumented API. While I try to follow semver for changes to the JavaScript API this library exposes, the underlying HTTP API can (and has) changed without warning. In particular, the NBA has repeatedly deprecated endpoints, or added certain required headers without which requests will fail. Further, this library comes bundled with a (relatively) up-to-date list of current NBA players which is subject to change at any time -- the specific contents of it should not be considered part of this library's API contract.
Usability
To put it nicely, the NBA's API endpoints are a little clunky to work with. This library tries to strike a balance between being usable but not making assumptions about how the data will be used. Specifically, the NBA sends data in a concise ""table"" form where the column headers come first then each result is an array of values that need to be matched with the proper header. This library does a simple transformation to zip the header and values arrays into a header-keyed object. Beyond that, it tries to not do too much. This is important to note because sometimes the various ""result sets"" that come back on a single endpoint seem sort of arbitrary. The underlying HTTP API doesn't seem to follow standard REST practices; rather it seems the endpoints are tied directly to the data needed by specific tables and charts displayed on stats.nba.com. This is what I mean by ""clunky"" to work with -- it can be tricky to assemble the data you need for a specific analysis from the various endpoints available.
Documentation
still lots to do here...
There are four primary parts of this library

Top-level methods
stats namespace — docs
sportVu namespace
synergy namespace

Transport Layer
In some cases you will want to use a different transport layer to handle HTTP requests. Perhaps you have an HTTP client library you like better than what I used here. Better yet, you want to get stats for the WNBA or the G League. The following code snippet shows how to use the withTransport method to create a new client with your own transport function.
// here we are getting stats for the WNBA!

const nba = require(""nba"");
const getJSON = require(""nba/src/get-json"");

// for the G League, try ""stats.gleague.nba.com""
const newHost = ""stats.wnba.com"";

const transport = (url, params, options) => {
  // simply swap the host and then defer the rest to the built in getJSON function
  const fixedURL = url.replace(""stats.nba.com"", ""stats.wnba.com"");
  return getJSON(fixedURL, params, options);
};

// create a new stats client here with our WNBA transport
const wnbaStats = nba.stats.withTransport(transport);

(async () => {
  const result = await wnbaStats.playerInfo({ PlayerID: ""1628886"" });
  console.log(result);
})();
""I don't use Node.js""
Please take a look at nba-client-template. The relevant part of the repo is a single JSON document from which many programming languages can dynamically generate an API client. The repo contains (sloppy) examples in Ruby and Python. Compiled languages can use code generation techniques to the same effect -- there's a (again, sloppy) example in Go. If you'd like me to publish it to a specific registry so you can install it with your language's package manager, please open an issue. Please note, however, that package only includes  the endpoints exposed by this library under the stats namespace -- sportvu and synergy endpoints aren't yet included in it. I also plan to add a command-line interface to this library so that it can be easily driven as a child process by another program.

",nba nodej client for nbacom api endpoint npm instal nba note browser usag thi packag cant be use from the browser becaus of cor restrict impos by nbacom current the hostnam are hardcod so thi packag cant be use with a proxi host but if you want support for thi use case pleas open an issu blacklist ip address it appear as though the nba ha blacklist certain block of ip address specif those of cloud host provid includ aw as such you may hit a situat where an applic use thi packag work fine on your local machin but doesnt work at all when deploy to a cloud server annoyingli request from these ip seem to just hang more inform here and here the second issu ha a curl command somewher which will quickli tell you if nba is accept request from your ip incident thi is also the same reason the travisci build is alway broken but test all pass local there is a simpl passthrough server in scriptsproxi that can be use to get around thi restrict you can put the proxi server somewher that can reach nbacom eg not on aw or heroku or similar and host your actual applic on a cloud provid nba api the statsnbacom use a larg number of undocu json endpoint to provid the statist tabl and chart display on that websit thi librari provid a javascript client for interact with mani of those api endpoint get start nbafindplayerstr will return an object with a player name their id and their team inform thi method is built into the packag all method in the nbastat namespac requir an object to be pass in as a paramet the key to the object are in the doc for the stat namespac here const nba requirenba const curri nbafindplayerstephen curri consolelogcurri log the follow firstnam stephen lastnam curri playerid 201939 teamid 1610612744 fullnam stephen curri downcasenam stephen curri nbastatsplayerinfo playerid curryplayerid thenconsolelog for more exampl api call see testintegrationstatsj stabil warn thi is a client for an unstabl and undocu api while i tri to follow semver for chang to the javascript api thi librari expos the underli http api can and ha chang without warn in particular the nba ha repeatedli deprec endpoint or ad certain requir header without which request will fail further thi librari come bundl with a rel uptod list of current nba player which is subject to chang at ani time the specif content of it should not be consid part of thi librari api contract usabl to put it nice the nba api endpoint are a littl clunki to work with thi librari tri to strike a balanc between be usabl but not make assumpt about how the data will be use specif the nba send data in a concis tabl form where the column header come first then each result is an array of valu that need to be match with the proper header thi librari doe a simpl transform to zip the header and valu array into a headerkey object beyond that it tri to not do too much thi is import to note becaus sometim the variou result set that come back on a singl endpoint seem sort of arbitrari the underli http api doesnt seem to follow standard rest practic rather it seem the endpoint are tie directli to the data need by specif tabl and chart display on statsnbacom thi is what i mean by clunki to work with it can be tricki to assembl the data you need for a specif analysi from the variou endpoint avail document still lot to do here there are four primari part of thi librari toplevel method stat namespac doc sportvu namespac synergi namespac transport layer in some case you will want to use a differ transport layer to handl http request perhap you have an http client librari you like better than what i use here better yet you want to get stat for the wnba or the g leagu the follow code snippet show how to use the withtransport method to creat a new client with your own transport function here we are get stat for the wnba const nba requirenba const getjson requirenbasrcgetjson for the g leagu tri statsgleaguenbacom const newhost statswnbacom const transport url param option simpli swap the host and then defer the rest to the built in getjson function const fixedurl urlreplacestatsnbacom statswnbacom return getjsonfixedurl param option creat a new stat client here with our wnba transport const wnbastat nbastatswithtransporttransport async const result await wnbastatsplayerinfo playerid 1628886 consolelogresult i dont use nodej pleas take a look at nbaclienttempl the relev part of the repo is a singl json document from which mani program languag can dynam gener an api client the repo contain sloppi exampl in rubi and python compil languag can use code gener techniqu to the same effect there a again sloppi exampl in go if youd like me to publish it to a specif registri so you can instal it with your languag packag manag pleas open an issu pleas note howev that packag onli includ the endpoint expos by thi librari under the stat namespac sportvu and synergi endpoint arent yet includ in it i also plan to add a commandlin interfac to thi librari so that it can be easili driven as a child process by anoth program,nba nodejs client for nbacom api endpoint npm install nba note browser usage this package cant be used from the browser because of cors restriction imposed by nbacom currently the hostnames are hardcoded so this package cant be used with a proxy host but if you want support for this use case please open an issue blacklisted ip address it appears a though the nba ha blacklisted certain block of ip address specifically those of cloud hosting provider including aws a such you may hit a situation where an application using this package work fine on your local machine but doesnt work at all when deployed to a cloud server annoyingly request from these ip seem to just hang more information here and here the second issue ha a curl command somewhere which will quickly tell you if nba is accepting request from your ip incidentally this is also the same reason the travisci build is always broken but test all pas locally there is a simple passthrough server in scriptsproxy that can be used to get around this restriction you can put the proxy server somewhere that can reach nbacom eg not on aws or heroku or similar and host your actual application on a cloud provider nba api the statsnbacom us a large number of undocumented json endpoint to provide the statistic table and chart displayed on that website this library provides a javascript client for interacting with many of those api endpoint getting started nbafindplayerstr will return an object with a player name their id and their team information this method is built into the package all method in the nbastats namespace require an object to be passed in a a parameter the key to the object are in the doc for the stats namespace here const nba requirenba const curry nbafindplayerstephen curry consolelogcurry log the following firstname stephen lastname curry playerid 201939 teamid 1610612744 fullname stephen curry downcasename stephen curry nbastatsplayerinfo playerid curryplayerid thenconsolelog for more example api call see testintegrationstatsjs stability warning this is a client for an unstable and undocumented api while i try to follow semver for change to the javascript api this library expose the underlying http api can and ha changed without warning in particular the nba ha repeatedly deprecated endpoint or added certain required header without which request will fail further this library come bundled with a relatively uptodate list of current nba player which is subject to change at any time the specific content of it should not be considered part of this library api contract usability to put it nicely the nbas api endpoint are a little clunky to work with this library try to strike a balance between being usable but not making assumption about how the data will be used specifically the nba sends data in a concise table form where the column header come first then each result is an array of value that need to be matched with the proper header this library doe a simple transformation to zip the header and value array into a headerkeyed object beyond that it try to not do too much this is important to note because sometimes the various result set that come back on a single endpoint seem sort of arbitrary the underlying http api doesnt seem to follow standard rest practice rather it seems the endpoint are tied directly to the data needed by specific table and chart displayed on statsnbacom this is what i mean by clunky to work with it can be tricky to assemble the data you need for a specific analysis from the various endpoint available documentation still lot to do here there are four primary part of this library toplevel method stats namespace doc sportvu namespace synergy namespace transport layer in some case you will want to use a different transport layer to handle http request perhaps you have an http client library you like better than what i used here better yet you want to get stats for the wnba or the g league the following code snippet show how to use the withtransport method to create a new client with your own transport function here we are getting stats for the wnba const nba requirenba const getjson requirenbasrcgetjson for the g league try statsgleaguenbacom const newhost statswnbacom const transport url params option simply swap the host and then defer the rest to the built in getjson function const fixedurl urlreplacestatsnbacom statswnbacom return getjsonfixedurl params option create a new stats client here with our wnba transport const wnbastats nbastatswithtransporttransport async const result await wnbastatsplayerinfo playerid 1628886 consolelogresult i dont use nodejs please take a look at nbaclienttemplate the relevant part of the repo is a single json document from which many programming language can dynamically generate an api client the repo contains sloppy example in ruby and python compiled language can use code generation technique to the same effect there a again sloppy example in go if youd like me to publish it to a specific registry so you can install it with your language package manager please open an issue please note however that package only includes the endpoint exposed by this library under the stats namespace sportvu and synergy endpoint arent yet included in it i also plan to add a commandline interface to this library so that it can be easily driven a a child process by another program,nba nodejs client nbacom api endpoint npm install nba note browser usage package cant used browser cors restriction imposed nbacom currently hostnames hardcoded package cant used proxy host want support use case please open issue blacklisted ip address appears though nba blacklisted certain block ip address specifically cloud hosting provider including aws may hit situation application using package work fine local machine doesnt work deployed cloud server annoyingly request ip seem hang information second issue curl command somewhere quickly tell nba accepting request ip incidentally also reason travisci build always broken test pas locally simple passthrough server scriptsproxy used get around restriction put proxy server somewhere reach nbacom eg aws heroku similar host actual application cloud provider nba api statsnbacom us large number undocumented json endpoint provide statistic table chart displayed website library provides javascript client interacting many api endpoint getting started nbafindplayerstr return object player name id team information method built package method nbastats namespace require object passed parameter key object doc stats namespace const nba requirenba const curry nbafindplayerstephen curry consolelogcurry log following firstname stephen lastname curry playerid 201939 teamid 1610612744 fullname stephen curry downcasename stephen curry nbastatsplayerinfo playerid curryplayerid thenconsolelog example api call see testintegrationstatsjs stability warning client unstable undocumented api try follow semver change javascript api library expose underlying http api changed without warning particular nba repeatedly deprecated endpoint added certain required header without request fail library come bundled relatively uptodate list current nba player subject change time specific content considered part library api contract usability put nicely nbas api endpoint little clunky work library try strike balance usable making assumption data used specifically nba sends data concise table form column header come first result array value need matched proper header library simple transformation zip header value array headerkeyed object beyond try much important note sometimes various result set come back single endpoint seem sort arbitrary underlying http api doesnt seem follow standard rest practice rather seems endpoint tied directly data needed specific table chart displayed statsnbacom mean clunky work tricky assemble data need specific analysis various endpoint available documentation still lot four primary part library toplevel method stats namespace doc sportvu namespace synergy namespace transport layer case want use different transport layer handle http request perhaps http client library like better used better yet want get stats wnba g league following code snippet show use withtransport method create new client transport function getting stats wnba const nba requirenba const getjson requirenbasrcgetjson g league try statsgleaguenbacom const newhost statswnbacom const transport url params option simply swap host defer rest built getjson function const fixedurl urlreplacestatsnbacom statswnbacom return getjsonfixedurl params option create new stats client wnba transport const wnbastats nbastatswithtransporttransport async const result await wnbastatsplayerinfo playerid 1628886 consolelogresult dont use nodejs please take look nbaclienttemplate relevant part repo single json document many programming language dynamically generate api client repo contains sloppy example ruby python compiled language use code generation technique effect there sloppy example go youd like publish specific registry install language package manager please open issue please note however package includes endpoint exposed library stats namespace sportvu synergy endpoint arent yet included also plan add commandline interface library easily driven child process another program
JavaScript ,"SportsLeague: Laravel 5.4 based system for various sports leagues to manage their teams/players/games.
It is a demo project for demonstrating what can be generated with QuickAdminPanel tool.
SportsLeague is all generated with QuickAdmin except for front-end code.
Clickable live-demo
demo-sportsleague.quickadminpanel.com

Email: admin@admin.com
Pass: password



How to use

Clone the repository with git clone
Copy .env.example file to .env and edit database credentials there
Run composer install
Run php artisan key:generate
Run php artisan migrate --seed (it has some seeded data for your testing)
That's it: launch the main URL or go to /login and login with default credentials admin@admin.com - password

License
Basically, feel free to use and re-use any way you want.

More from our LaravelDaily Team

Check out our adminpanel generator QuickAdminPanel
Read our Blog with Laravel Tutorials
FREE E-book: 50 Laravel Quick Tips (and counting)
Subscribe to our YouTube channel Laravel Business
Enroll in our Laravel Online Courses

",sportsleagu laravel 54 base system for variou sport leagu to manag their teamsplayersgam it is a demo project for demonstr what can be gener with quickadminpanel tool sportsleagu is all gener with quickadmin except for frontend code clickabl livedemo demosportsleaguequickadminpanelcom email adminadmincom pass password how to use clone the repositori with git clone copi envexampl file to env and edit databas credenti there run compos instal run php artisan keygener run php artisan migrat seed it ha some seed data for your test that it launch the main url or go to login and login with default credenti adminadmincom password licens basic feel free to use and reus ani way you want more from our laraveldaili team check out our adminpanel gener quickadminpanel read our blog with laravel tutori free ebook 50 laravel quick tip and count subscrib to our youtub channel laravel busi enrol in our laravel onlin cours,sportsleague laravel 54 based system for various sport league to manage their teamsplayersgames it is a demo project for demonstrating what can be generated with quickadminpanel tool sportsleague is all generated with quickadmin except for frontend code clickable livedemo demosportsleaguequickadminpanelcom email adminadmincom pas password how to use clone the repository with git clone copy envexample file to env and edit database credential there run composer install run php artisan keygenerate run php artisan migrate seed it ha some seeded data for your testing thats it launch the main url or go to login and login with default credential adminadmincom password license basically feel free to use and reuse any way you want more from our laraveldaily team check out our adminpanel generator quickadminpanel read our blog with laravel tutorial free ebook 50 laravel quick tip and counting subscribe to our youtube channel laravel business enroll in our laravel online course,sportsleague laravel 54 based system various sport league manage teamsplayersgames demo project demonstrating generated quickadminpanel tool sportsleague generated quickadmin except frontend code clickable livedemo demosportsleaguequickadminpanelcom email adminadmincom pas password use clone repository git clone copy envexample file env edit database credential run composer install run php artisan keygenerate run php artisan migrate seed seeded data testing thats launch main url go login login default credential adminadmincom password license basically feel free use reuse way want laraveldaily team check adminpanel generator quickadminpanel read blog laravel tutorial free ebook 50 laravel quick tip counting subscribe youtube channel laravel business enroll laravel online course
JavaScript ,"Team Colors

Team Colors is a reference of HEX, RGB, CMYK, and Pantone color values of major league sports teams.
How-To
Install: yarn install
Development: yarn start
Build: yarn build
How It Works
Site is built on the react framework. index.html is the shell container for react app. If javascript is not supported, a link is shown to the raw JSON data which has all color information.
Color data is housed in a single .json file src/teams.json. Any changes to team colors can be done there. Note on colors: Color definitions for each team are in arrays and grouped by color mode. Color values should match index position in the array across color modes, for example:
colors:
  rgb: TEAMS-RGB-BLUE, TEAMS-RGB-RED
  hex: TEAMS-HEX-BLUE, TEAMS-HEX-RED

Source artwork for each team is grouped by league in sketch. Production versions of these logo should be in .svg format in public/img.
Edit Team Color or Name
Find teams .json file in src/teams.json, and edit the info you need.
Add a Team

Determine the team’s league
Following the established pattern, add the team’s name and colors the .json file
Add a vector logo for the team in its corresponding .sketch league file in sketch with the team’s name (as referenced in its .json file) in lowercase with hyphens, i.e. ""utah-jazz""
Export the team’s .svg logo to public/img/
Preferably, optimize the svg (with a tool like SVGO)
Run yarn build, commit, push

Official Color References
NBA
All NBA colors are official (source user & pass: nbamedia).
The NBA only provides RGB, CMYK, and Pantone colors for each team, so the HEX color is a programmatic conversion of the RGB color.
NFL
All NFL colors are official (see sources below).
The NFL provides official RGB, HEX, CMYK, and Pantone colors (so none of the colors you see on Team Colors are conversions).
The NFL has logo slicks which detail team color values. These are provided on a per-conference basis. Note: each of these source links are over 100MB in size, so they take a while to download.

AFC North
AFC South
AFC East
AFC West
NFC North
NFC South
NFC East
NFC West

MLB
MLB colors have been extracted from the official “RGB Digital Art” spot color logo slicks provided at MLB Press Box (user account required). They were not explicitly stated values, but they are color values pulled from individual team logos in an official MLB document.
The extracted colors are in HEX form and their RGB counterparts are generated programmatically.

American League logo slick
National League logo slick

NHL
NHL colors are official. As per Michael Sharer of the NHL.
MLS
MLS colors are currently approximations, with the exceptions listed below. I am working on getting official colors of the remaining teams.

Philadelphia Union

EPL
These leagues’ teams and colors are currently approximations. I am working on getting official colors. If you know how/where to find them, please open an issue here in Github.
To-Dos

 Switch to flex for layout
 Improve filtering with fuzzy string search
 Improve error states for when data doesn't render
 Consider alternatives to no-js users rather than just ""here's the raw data"" (something that doesn't required a build if a single color in the JSON file is changed)
 Possibly add team id manually to JSON file ??

",team color team color is a refer of hex rgb cmyk and panton color valu of major leagu sport team howto instal yarn instal develop yarn start build yarn build how it work site is built on the react framework indexhtml is the shell contain for react app if javascript is not support a link is shown to the raw json data which ha all color inform color data is hous in a singl json file srcteamsjson ani chang to team color can be done there note on color color definit for each team are in array and group by color mode color valu should match index posit in the array across color mode for exampl color rgb teamsrgbblu teamsrgbr hex teamshexblu teamshexr sourc artwork for each team is group by leagu in sketch product version of these logo should be in svg format in publicimg edit team color or name find team json file in srcteamsjson and edit the info you need add a team determin the team leagu follow the establish pattern add the team name and color the json file add a vector logo for the team in it correspond sketch leagu file in sketch with the team name as referenc in it json file in lowercas with hyphen ie utahjazz export the team svg logo to publicimg prefer optim the svg with a tool like svgo run yarn build commit push offici color refer nba all nba color are offici sourc user pass nbamedia the nba onli provid rgb cmyk and panton color for each team so the hex color is a programmat convers of the rgb color nfl all nfl color are offici see sourc below the nfl provid offici rgb hex cmyk and panton color so none of the color you see on team color are convers the nfl ha logo slick which detail team color valu these are provid on a perconfer basi note each of these sourc link are over 100mb in size so they take a while to download afc north afc south afc east afc west nfc north nfc south nfc east nfc west mlb mlb color have been extract from the offici rgb digit art spot color logo slick provid at mlb press box user account requir they were not explicitli state valu but they are color valu pull from individu team logo in an offici mlb document the extract color are in hex form and their rgb counterpart are gener programmat american leagu logo slick nation leagu logo slick nhl nhl color are offici as per michael sharer of the nhl ml ml color are current approxim with the except list below i am work on get offici color of the remain team philadelphia union epl these leagu team and color are current approxim i am work on get offici color if you know howwher to find them pleas open an issu here in github todo switch to flex for layout improv filter with fuzzi string search improv error state for when data doesnt render consid altern to noj user rather than just here the raw data someth that doesnt requir a build if a singl color in the json file is chang possibl add team id manual to json file,team color team color is a reference of hex rgb cmyk and pantone color value of major league sport team howto install yarn install development yarn start build yarn build how it work site is built on the react framework indexhtml is the shell container for react app if javascript is not supported a link is shown to the raw json data which ha all color information color data is housed in a single json file srcteamsjson any change to team color can be done there note on color color definition for each team are in array and grouped by color mode color value should match index position in the array across color mode for example color rgb teamsrgbblue teamsrgbred hex teamshexblue teamshexred source artwork for each team is grouped by league in sketch production version of these logo should be in svg format in publicimg edit team color or name find team json file in srcteamsjson and edit the info you need add a team determine the team league following the established pattern add the team name and color the json file add a vector logo for the team in it corresponding sketch league file in sketch with the team name a referenced in it json file in lowercase with hyphen ie utahjazz export the team svg logo to publicimg preferably optimize the svg with a tool like svgo run yarn build commit push official color reference nba all nba color are official source user pas nbamedia the nba only provides rgb cmyk and pantone color for each team so the hex color is a programmatic conversion of the rgb color nfl all nfl color are official see source below the nfl provides official rgb hex cmyk and pantone color so none of the color you see on team color are conversion the nfl ha logo slick which detail team color value these are provided on a perconference basis note each of these source link are over 100mb in size so they take a while to download afc north afc south afc east afc west nfc north nfc south nfc east nfc west mlb mlb color have been extracted from the official rgb digital art spot color logo slick provided at mlb press box user account required they were not explicitly stated value but they are color value pulled from individual team logo in an official mlb document the extracted color are in hex form and their rgb counterpart are generated programmatically american league logo slick national league logo slick nhl nhl color are official a per michael sharer of the nhl ml ml color are currently approximation with the exception listed below i am working on getting official color of the remaining team philadelphia union epl these league team and color are currently approximation i am working on getting official color if you know howwhere to find them please open an issue here in github todos switch to flex for layout improve filtering with fuzzy string search improve error state for when data doesnt render consider alternative to nojs user rather than just here the raw data something that doesnt required a build if a single color in the json file is changed possibly add team id manually to json file,team color team color reference hex rgb cmyk pantone color value major league sport team howto install yarn install development yarn start build yarn build work site built react framework indexhtml shell container react app javascript supported link shown raw json data color information color data housed single json file srcteamsjson change team color done note color color definition team array grouped color mode color value match index position array across color mode example color rgb teamsrgbblue teamsrgbred hex teamshexblue teamshexred source artwork team grouped league sketch production version logo svg format publicimg edit team color name find team json file srcteamsjson edit info need add team determine team league following established pattern add team name color json file add vector logo team corresponding sketch league file sketch team name referenced json file lowercase hyphen ie utahjazz export team svg logo publicimg preferably optimize svg tool like svgo run yarn build commit push official color reference nba nba color official source user pas nbamedia nba provides rgb cmyk pantone color team hex color programmatic conversion rgb color nfl nfl color official see source nfl provides official rgb hex cmyk pantone color none color see team color conversion nfl logo slick detail team color value provided perconference basis note source link 100mb size take download afc north afc south afc east afc west nfc north nfc south nfc east nfc west mlb mlb color extracted official rgb digital art spot color logo slick provided mlb press box user account required explicitly stated value color value pulled individual team logo official mlb document extracted color hex form rgb counterpart generated programmatically american league logo slick national league logo slick nhl nhl color official per michael sharer nhl ml ml color currently approximation exception listed working getting official color remaining team philadelphia union epl league team color currently approximation working getting official color know howwhere find please open issue github todos switch flex layout improve filtering fuzzy string search improve error state data doesnt render consider alternative nojs user rather here raw data something doesnt required build single color json file changed possibly add team id manually json file
JavaScript ,"vue-sports

A Vue.js project

仿凤凰新闻体育板块+赛事数据
体育新闻板块已经完成(暂时未用到vuex) ，正在弄体育赛事数据接口(计划加入vuex)



Build Setup
# install dependencies
npm install

# serve with hot reload at localhost:8080
npm run dev

# build for production with minification
npm run build

# build for production and view the bundle analyzer report
npm run build --report
目标功能

 体育新闻 -- 完成
 比赛数据 -- 未完成

",vuesport a vuej project vuex vuex build setup instal depend npm instal serv with hot reload at localhost8080 npm run dev build for product with minif npm run build build for product and view the bundl analyz report npm run build report,vuesports a vuejs project vuex vuex build setup install dependency npm install serve with hot reload at localhost8080 npm run dev build for production with minification npm run build build for production and view the bundle analyzer report npm run build report,vuesports vuejs project vuex vuex build setup install dependency npm install serve hot reload localhost8080 npm run dev build production minification npm run build build production view bundle analyzer report npm run build report
JavaScript ,"
Courtside: pick up sports app.
Make game plans and let all your friends know about it.

Installation


To install requirements:
$ pip install -r requirements.txt


You will need to create a keys file in keys module for the twitter and fb app keys, ones there are old and not used.




Contributions
All software contributions are welcome and encouraged.

Twitter

Mahdi Yusuf @myusuf3
Omar Shammas @omarshammas
Serena Ngai @serenangai

",courtsid pick up sport app make game plan and let all your friend know about it instal to instal requir pip instal r requirementstxt you will need to creat a key file in key modul for the twitter and fb app key one there are old and not use contribut all softwar contribut are welcom and encourag twitter mahdi yusuf myusuf3 omar shamma omarshamma serena ngai serenangai,courtside pick up sport app make game plan and let all your friend know about it installation to install requirement pip install r requirementstxt you will need to create a key file in key module for the twitter and fb app key one there are old and not used contribution all software contribution are welcome and encouraged twitter mahdi yusuf myusuf3 omar shammas omarshammas serena ngai serenangai,courtside pick sport app make game plan let friend know installation install requirement pip install r requirementstxt need create key file key module twitter fb app key one old used contribution software contribution welcome encouraged twitter mahdi yusuf myusuf3 omar shammas omarshammas serena ngai serenangai
JavaScript ,"微信小程序 Sports News(体育新闻) 持续更新
小程序预览



使用步骤


将仓库克隆到本地：
$ git clone https://github.com/havenxie/weapp-sportsnews.git weapp-sportsnews --depth 1

$ cd weapp-sportsnews


打开微信Web开发者工具



我用的是0.11.122100版本
不需要所谓的破解，网上所谓的破解只是针对之前的0.9.092100版本，新的官方版本不需要破解！
下载链接：""https://mp.weixin.qq.com/debug/wxadoc/dev/devtools/download.html""


选择添加项目，填写或选择相应信息


AppID：点击右下角无AppID（我也没有资格拿到）
项目名称：随便填写，因为不涉及到部署，所以无所谓
项目目录：选择刚刚克隆的文件夹
点击添加项目



你可以选择在微信Web开发者工具中编码（也可选择你喜欢的编辑器，我用的是sublime,现在发现vs code比sublime好用多了）


通过左下角重启按钮，刷新编码过后的预览


代码中用到了大量ES6的语法，可能需要node环境，请自行安装


剩下的可以自由发挥了


设置页没有做，因为不知道要做些什么，假如你有兴趣的话可以自己发挥


下载后将images中的GIF5.jpg删除。


有什么不明白或者想找我交流的朋友可以和我联系



个人微信
个人公众号









至此告一段落，啥时候再有兴趣再来继续添加功能吧。
有兴趣的小伙伴可以一起来提交代码。。。
许可
MIT © havenxie
",sport news git clone httpsgithubcomhavenxieweappsportsnewsgit weappsportsnew depth 1 cd weappsportsnew web 011122100 09092100 httpsmpweixinqqcomdebugwxadocdevdevtoolsdownloadhtml appidappid websublimev codesublim es6nod imagesgif5jpg mit havenxi,sport news git clone httpsgithubcomhavenxieweappsportsnewsgit weappsportsnews depth 1 cd weappsportsnews web 011122100 09092100 httpsmpweixinqqcomdebugwxadocdevdevtoolsdownloadhtml appidappid websublimevs codesublime es6node imagesgif5jpg mit havenxie,sport news git clone httpsgithubcomhavenxieweappsportsnewsgit weappsportsnews depth 1 cd weappsportsnews web 011122100 09092100 httpsmpweixinqqcomdebugwxadocdevdevtoolsdownloadhtml appidappid websublimevs codesublime es6node imagesgif5jpg mit havenxie
JavaScript ,"TableChamp
Tablesports leaderboard app
Track each ping pong, pool, foosball, air hockey, or shuffleboard game that's played. Find out who really is number one (in your office, or out of your group of friends).
What is it?
With TableChamp, you can add players, track every game that is played, and always know who's #1.

You can view stats on each player, including their 20 most recent games:

You can manage all of the settings of the app in one convenient sidebar:

You can even select from one of 14 languages:

How does it work?
TableChamp is written entirely in JS/HTML/CSS. There is no back-end code (like python, or PHP). It uses FireBase as a back-end real-time DB to store all of the data, and manage the user authentication.
Installation
1) You'll need a hosting account for the JS/HTML/CSS files
NOTE: you can run a FireBase app locally, but you'll need to follow these instructions to get set up with FireBase CLI.
Just clone this entire project to your server. Once you've done that, move on to step 2.
2) You'll need to sign up for a free FireBase account

Even if you have a large team, the free FireBase account should offer plenty of resources.
Once you've signed up for a free FireBase account, move on to the next step.
3) Create a new FireBase app

Go through the process of creating a new FireBase Project. You can name it ""Table Champ"", or anything you'd like.

Find the ""Add to your web app"" option, and click it:

You now have all of the information you need to connect to connect the app to FireBase:

Once you have your FireBase API info, move on to the next step
4) Copy your FireBase info to the /js/firebase-key.js file
Open up /js/firebase-key.js:

Paste in the FireBase apiKey, authDomain, and databaseURL from step 3 above:

Once you've done this, save your changes, and move on to the next step.
5) Add your first FireBase user
FireBase handles storing all of your data, as well as authentication. We'll need to set up a user in the FireBase admin, so that you can log into your app. I'll walk you through how to add a single user, but you can add as many login users as you'd like.
NOTE: Users are separate from players. Users are set up in the FireBase admin, and have an email & password attached to them so that you can log in. Players are managed from the settings section once you've logged into your app.

All you need to enter to set up a user is an email, and a password.
Once you've added your first user, continue to the next step.
6) Create a database instance
From your FireBase console, click into the Database section:

Create a new ""Real-time database"" (not a Firestore DB - note: they try and get you to create a Firestore DB by default).
Once you've created your real-time DB, you'll need to change the security rules. Click the ""Rules"" tab and and replace what's there with the following:
{
  ""rules"": {
    "".read"": true,
    "".write"": true
  }
}

Here's what it should look like:

7) Login, and add your players
Now you can log into your app for the first time. Go to the index.html file (wherever it's being hosted from step 1 above). You should see:

Once you've logged in, you should see:

Enter your organizations name, and the game you'll be tracking:

Then click on the Players tab:

Click ""Add Players"" and enter the names of your players (one name per line):

You're all set
You should be ready to start tracking games:

",tablechamp tablesport leaderboard app track each ping pong pool foosbal air hockey or shuffleboard game that play find out who realli is number one in your offic or out of your group of friend what is it with tablechamp you can add player track everi game that is play and alway know who 1 you can view stat on each player includ their 20 most recent game you can manag all of the set of the app in one conveni sidebar you can even select from one of 14 languag how doe it work tablechamp is written entir in jshtmlcss there is no backend code like python or php it use firebas as a backend realtim db to store all of the data and manag the user authent instal 1 youll need a host account for the jshtmlcss file note you can run a firebas app local but youll need to follow these instruct to get set up with firebas cli just clone thi entir project to your server onc youv done that move on to step 2 2 youll need to sign up for a free firebas account even if you have a larg team the free firebas account should offer plenti of resourc onc youv sign up for a free firebas account move on to the next step 3 creat a new firebas app go through the process of creat a new firebas project you can name it tabl champ or anyth youd like find the add to your web app option and click it you now have all of the inform you need to connect to connect the app to firebas onc you have your firebas api info move on to the next step 4 copi your firebas info to the jsfirebasekeyj file open up jsfirebasekeyj past in the firebas apikey authdomain and databaseurl from step 3 abov onc youv done thi save your chang and move on to the next step 5 add your first firebas user firebas handl store all of your data as well as authent well need to set up a user in the firebas admin so that you can log into your app ill walk you through how to add a singl user but you can add as mani login user as youd like note user are separ from player user are set up in the firebas admin and have an email password attach to them so that you can log in player are manag from the set section onc youv log into your app all you need to enter to set up a user is an email and a password onc youv ad your first user continu to the next step 6 creat a databas instanc from your firebas consol click into the databas section creat a new realtim databas not a firestor db note they tri and get you to creat a firestor db by default onc youv creat your realtim db youll need to chang the secur rule click the rule tab and and replac what there with the follow rule read true write true here what it should look like 7 login and add your player now you can log into your app for the first time go to the indexhtml file wherev it be host from step 1 abov you should see onc youv log in you should see enter your organ name and the game youll be track then click on the player tab click add player and enter the name of your player one name per line your all set you should be readi to start track game,tablechamp tablesports leaderboard app track each ping pong pool foosball air hockey or shuffleboard game thats played find out who really is number one in your office or out of your group of friend what is it with tablechamp you can add player track every game that is played and always know who 1 you can view stats on each player including their 20 most recent game you can manage all of the setting of the app in one convenient sidebar you can even select from one of 14 language how doe it work tablechamp is written entirely in jshtmlcss there is no backend code like python or php it us firebase a a backend realtime db to store all of the data and manage the user authentication installation 1 youll need a hosting account for the jshtmlcss file note you can run a firebase app locally but youll need to follow these instruction to get set up with firebase cli just clone this entire project to your server once youve done that move on to step 2 2 youll need to sign up for a free firebase account even if you have a large team the free firebase account should offer plenty of resource once youve signed up for a free firebase account move on to the next step 3 create a new firebase app go through the process of creating a new firebase project you can name it table champ or anything youd like find the add to your web app option and click it you now have all of the information you need to connect to connect the app to firebase once you have your firebase api info move on to the next step 4 copy your firebase info to the jsfirebasekeyjs file open up jsfirebasekeyjs paste in the firebase apikey authdomain and databaseurl from step 3 above once youve done this save your change and move on to the next step 5 add your first firebase user firebase handle storing all of your data a well a authentication well need to set up a user in the firebase admin so that you can log into your app ill walk you through how to add a single user but you can add a many login user a youd like note user are separate from player user are set up in the firebase admin and have an email password attached to them so that you can log in player are managed from the setting section once youve logged into your app all you need to enter to set up a user is an email and a password once youve added your first user continue to the next step 6 create a database instance from your firebase console click into the database section create a new realtime database not a firestore db note they try and get you to create a firestore db by default once youve created your realtime db youll need to change the security rule click the rule tab and and replace whats there with the following rule read true write true here what it should look like 7 login and add your player now you can log into your app for the first time go to the indexhtml file wherever it being hosted from step 1 above you should see once youve logged in you should see enter your organization name and the game youll be tracking then click on the player tab click add player and enter the name of your player one name per line youre all set you should be ready to start tracking game,tablechamp tablesports leaderboard app track ping pong pool foosball air hockey shuffleboard game thats played find really number one office group friend tablechamp add player track every game played always know who 1 view stats player including 20 recent game manage setting app one convenient sidebar even select one 14 language work tablechamp written entirely jshtmlcss backend code like python php us firebase backend realtime db store data manage user authentication installation 1 youll need hosting account jshtmlcss file note run firebase app locally youll need follow instruction get set firebase cli clone entire project server youve done move step 2 2 youll need sign free firebase account even large team free firebase account offer plenty resource youve signed free firebase account move next step 3 create new firebase app go process creating new firebase project name table champ anything youd like find add web app option click information need connect connect app firebase firebase api info move next step 4 copy firebase info jsfirebasekeyjs file open jsfirebasekeyjs paste firebase apikey authdomain databaseurl step 3 youve done save change move next step 5 add first firebase user firebase handle storing data well authentication well need set user firebase admin log app ill walk add single user add many login user youd like note user separate player user set firebase admin email password attached log player managed setting section youve logged app need enter set user email password youve added first user continue next step 6 create database instance firebase console click database section create new realtime database firestore db note try get create firestore db default youve created realtime db youll need change security rule click rule tab replace whats following rule read true write true here look like 7 login add player log app first time go indexhtml file wherever hosted step 1 see youve logged see enter organization name game youll tracking click player tab click add player enter name player one name per line youre set ready start tracking game
JavaScript ,"Yahoo! Fantasy API Node Module
This is a node module created to wrap the Yahoo! Fantasy Sports API (link). At the moment, not all subresources are available, nor are any of the 'collection' elements. I do hope to add them, and they have been added to the code, but as of now this project is very much in an open beta phase.
The API is designed to act as a helper for those interacting with the Y! Fantasy API. The goal is for ease of use for the user, both in terms of querying endpoints and parsing responses. I've noticed that in working with the API, the data is not always the easiest to understand, so hopefully what I have created here will help people out.
Installation
You can install the module via npm by running:
$ npm install yahoo-fantasy

Licence
This module is available under the MIT Licence
Documentation
More complete documentation can be found using the application sandbox. This sandbox is always a work in progress, if I've learned anything it's that nothing is ever complete.
The API can be used by simply importing the module and querying data, since version 4.0 the authentication flow has been built into the library to make things easier for users.
// import the library
const YahooFantasy = require('yahoo-fantasy');

// you can get an application key/secret by creating a new application on Yahoo!
const yf = new YahooFantasy(
  Y!APPLICATION_KEY, // Yahoo! Application Key
  Y!APPLICATION_SECRET, // Yahoo! Application Secret
  tokenCallbackFunction, // callback function when user token is refreshed (optional)
  redirectUri // redirect endpoint when user authenticates (optional)
);

// you can authenticate a user by setting a route to call the auth function
// note: from v4.0 on, public queries are now supported; that is, you can query
// public resources without authenticating a user (ie/ game meta, player meta,
// and information from public leagues)
yf.auth(
  response // response object to redirect the user to the Yahoo! login screen
)

// you also need to set up the callback route (defined as the redirect uri above)
// note: this will automatically set the user and refresh token if the request is
// successful, but you can also call them manually, described below
yf.authCallback(
  request, // the request will contain the auth code from Yahoo!
  callback // callback function that will be called after the token has been retrieved
)

// if you're not authenticating via the library you'll need to set the Yahoo!
// token for the user
yf.setUserToken(
  Y!CLIENT_TOKEN
);

// you can do the same for the refresh token...
// if you set this and the token expires (lasts an hour) then the token will automatically
// refresh and call the above ""tokenCallbackFunction"" that you've specified to persist the
// token elsewhere
yf.setRefreshToken(
  Y!CLIENT_REFRESH_TOKEN
);

// query a resource/subresource
yf.{resource}.{subresource} (
  {possible argument(s)},
  function cb(err, data) {
    // handle error
    // callback function
    // do your thing
  }
);

Starting with v3.1.0 you can also use a promise chain to query resources and subresources
yf.{resource}.{subresource} (
  {possible argument(s)}
)
.then(data => // do your thing)
.catch(err => // handle error)

This also opens the door to use async/await in version of node that support it
try {
  let data = await yf.{resource}.{subresource} (
    {possible argument(s)}
  )

  // do your thing
} catch(err) {
  // handle error
}

Bugs & Issues
This project is very much still a work in progress, please report any issues via the GitHub issues page.
Changelog
4.1.1

Small change to the way the resource and collection files are being imported as it was causing issues on some hosts...

4.1.0

Maybe would have made sense as a 5.0.0 as there may be breaking changes, but I haven't been able to find any yet...
the authCallback() function will now return an object with the user's access_token and refresh_token
the auth() function will accept a ""state"" string, allowing for state persistence through the authentication process
re-enabled the transactions.fetch() collection call
cleaned up the ""wavier_days"" and ""stat_categories"" objects on league resources
added deprecation warnings to the game.leagues and game.players functions as they're not very useful in that context

4.0.0

Added auth(), authCallback, setRefreshToken() functions to the library
Automatically handle refreshing of the token and call a user defined function when the token has expired
Added support for public queries
General cleanup

3.2.0

Added ""players"" subresource to ""league"" in order to obtain weekly / season stats for a player based on league settings
Fixed a bug where the starting status wasn't properly being returned due to a shift in how the data was being returned
Removed use of ""request"" library for size and performance reasons
General code optimizations and improvements

3.1.2

Updated outdated dependencies

3.1.1

Resolve error when no team logo is present (Issue #42)

3.1.0

Introduced promise based flow for all endpoints as an alternative to callbacks. Thanks Marois!

3.0.4

Fixed a bug in the players.league collection call where it was trying to use split on an array... (Issue #46).
Fixed similar bugs in other places...

3.0.3

Added the ability to specify a date or week when querying the team.stats resource.
Unit test fixes (Issue #42). Thanks Marios!
Updated ""vulnerable"" dependencies.

3.0.2

Fixed an issue with the user.game_leagues resource, where the data was not at all user friendly (renamed leagues to games at the top level of the return object)

3.0.1

Fixed some typos in some import statements which caused issues on some servers

3.0.0

Major refactor to use ES6?... 2015? ...2018? Whatever the hell they're calling it now...
Using ES Modules (mjs) files where possible
Removed transactions collections (they'll be back!)

2.0.4

Added a fix to give a cleaner value for the new ""batting order"" attribute in the player oject.

2.0.3

Fixed a bug where the league players collection was not properly parsing the ownership subresource

2.0.2

Fixed a bug where ""mapTeamPoints"" helper function was not defining ""self"". Thanks platky!

2.0.1

Removed the code that added a ""reason"" to errors coming from Yahoo! as it was breaking other errors. Retry notifications should now be handled within the application using the module.

2.0.0

Moved to Yahoo!'s OAuth2.0 authentication mechanism.

1.0.2

Fixed game resource roster postions callback bug.

1.0.1

Fixed a typo that was breaking team mapping.

1.0.0

Breaking changes
Fixed NFL scoreboard/matchups bug (Issue #19)
In fixing this bug I realized that my ""team"" set up was really only useful for MLB fantasy, so I rewrote team mapping to work better across all sports and give additional details that weren't previously reported. This will cause errors if you are using the team.manager attribute in your code.

0.5.3

Fixed a bug where leagueFetch was throwing an error, thanks danielspector!

0.5.2

Fixed a bug where player stats by week url was not being created properly, thanks withsmilo!

0.5.1

Fixed a bug where collections that contained subresources would return no data.

0.5.0

Added ""Transactions"" collection with functionality to add players, drop players, and add/drop players, thanks again githubsmilo!

0.4.4

Fixed a bug in player.draft_analysis, thanks githubsmilo!

0.4.3

Added weeks param for league.scoreboard
Added weeks param for team.matchups
Fixed a bug where individual players weren't mapping properly
Minor code cleanup

0.4.2

Added the ability to specify a date or week when querying the roster resource.
Cleaned up the player normalization model
Fixed a bug where the team.roster call was erroring

0.4.1

Fixes to how POST data is handled

0.4.0

Significantly restructured the code to have more consistency and set it up better for future plans, namely POST methods and proper unit testing
Removed the ""refresh user token"" and instead return the error to the user who can handle the refresh within their application.

0.3.1

Additional player attributes added, thanks ryus08!

0.3.0

Added a method to refresh the user's token if it has expired.

0.2.2

Hotfix to fix ""Teams"" collection - use error first convention

0.2.0

Made helper classes more consistent
Added collections for games, leagues, players, and teams
Moved to error first convention because JavaScript

0.1.2

Added 'Team Matchups' subresource
Added 'League Scoreboard' subresource
Minor code cleanup and improvements

0.1.1

Refactored module to fix a bug where user sessions were not necessarily unique because of require caching.

0.1

Initial release.

",yahoo fantasi api node modul thi is a node modul creat to wrap the yahoo fantasi sport api link at the moment not all subresourc are avail nor are ani of the collect element i do hope to add them and they have been ad to the code but as of now thi project is veri much in an open beta phase the api is design to act as a helper for those interact with the y fantasi api the goal is for eas of use for the user both in term of queri endpoint and pars respons ive notic that in work with the api the data is not alway the easiest to understand so hope what i have creat here will help peopl out instal you can instal the modul via npm by run npm instal yahoofantasi licenc thi modul is avail under the mit licenc document more complet document can be found use the applic sandbox thi sandbox is alway a work in progress if ive learn anyth it that noth is ever complet the api can be use by simpli import the modul and queri data sinc version 40 the authent flow ha been built into the librari to make thing easier for user import the librari const yahoofantasi requireyahoofantasi you can get an applic keysecret by creat a new applic on yahoo const yf new yahoofantasi yapplication_key yahoo applic key yapplication_secret yahoo applic secret tokencallbackfunct callback function when user token is refresh option redirecturi redirect endpoint when user authent option you can authent a user by set a rout to call the auth function note from v40 on public queri are now support that is you can queri public resourc without authent a user ie game meta player meta and inform from public leagu yfauth respons respons object to redirect the user to the yahoo login screen you also need to set up the callback rout defin as the redirect uri abov note thi will automat set the user and refresh token if the request is success but you can also call them manual describ below yfauthcallback request the request will contain the auth code from yahoo callback callback function that will be call after the token ha been retriev if your not authent via the librari youll need to set the yahoo token for the user yfsetusertoken yclient_token you can do the same for the refresh token if you set thi and the token expir last an hour then the token will automat refresh and call the abov tokencallbackfunct that youv specifi to persist the token elsewher yfsetrefreshtoken yclient_refresh_token queri a resourcesubresourc yfresourcesubresourc possibl argument function cberr data handl error callback function do your thing start with v310 you can also use a promis chain to queri resourc and subresourc yfresourcesubresourc possibl argument thendata do your thing catcherr handl error thi also open the door to use asyncawait in version of node that support it tri let data await yfresourcesubresourc possibl argument do your thing catcherr handl error bug issu thi project is veri much still a work in progress pleas report ani issu via the github issu page changelog 411 small chang to the way the resourc and collect file are be import as it wa caus issu on some host 410 mayb would have made sens as a 500 as there may be break chang but i havent been abl to find ani yet the authcallback function will now return an object with the user access_token and refresh_token the auth function will accept a state string allow for state persist through the authent process reenabl the transactionsfetch collect call clean up the wavier_day and stat_categori object on leagu resourc ad deprec warn to the gameleagu and gameplay function as theyr not veri use in that context 400 ad auth authcallback setrefreshtoken function to the librari automat handl refresh of the token and call a user defin function when the token ha expir ad support for public queri gener cleanup 320 ad player subresourc to leagu in order to obtain weekli season stat for a player base on leagu set fix a bug where the start statu wasnt properli be return due to a shift in how the data wa be return remov use of request librari for size and perform reason gener code optim and improv 312 updat outdat depend 311 resolv error when no team logo is present issu 42 310 introduc promis base flow for all endpoint as an altern to callback thank maroi 304 fix a bug in the playersleagu collect call where it wa tri to use split on an array issu 46 fix similar bug in other place 303 ad the abil to specifi a date or week when queri the teamstat resourc unit test fix issu 42 thank mario updat vulner depend 302 fix an issu with the usergame_leagu resourc where the data wa not at all user friendli renam leagu to game at the top level of the return object 301 fix some typo in some import statement which caus issu on some server 300 major refactor to use es6 2015 2018 whatev the hell theyr call it now use es modul mj file where possibl remov transact collect theyll be back 204 ad a fix to give a cleaner valu for the new bat order attribut in the player oject 203 fix a bug where the leagu player collect wa not properli pars the ownership subresourc 202 fix a bug where mapteampoint helper function wa not defin self thank platki 201 remov the code that ad a reason to error come from yahoo as it wa break other error retri notif should now be handl within the applic use the modul 200 move to yahoo oauth20 authent mechan 102 fix game resourc roster postion callback bug 101 fix a typo that wa break team map 100 break chang fix nfl scoreboardmatchup bug issu 19 in fix thi bug i realiz that my team set up wa realli onli use for mlb fantasi so i rewrot team map to work better across all sport and give addit detail that werent previous report thi will caus error if you are use the teammanag attribut in your code 053 fix a bug where leaguefetch wa throw an error thank danielspector 052 fix a bug where player stat by week url wa not be creat properli thank withsmilo 051 fix a bug where collect that contain subresourc would return no data 050 ad transact collect with function to add player drop player and adddrop player thank again githubsmilo 044 fix a bug in playerdraft_analysi thank githubsmilo 043 ad week param for leaguescoreboard ad week param for teammatchup fix a bug where individu player werent map properli minor code cleanup 042 ad the abil to specifi a date or week when queri the roster resourc clean up the player normal model fix a bug where the teamrost call wa error 041 fix to how post data is handl 040 significantli restructur the code to have more consist and set it up better for futur plan name post method and proper unit test remov the refresh user token and instead return the error to the user who can handl the refresh within their applic 031 addit player attribut ad thank ryus08 030 ad a method to refresh the user token if it ha expir 022 hotfix to fix team collect use error first convent 020 made helper class more consist ad collect for game leagu player and team move to error first convent becaus javascript 012 ad team matchup subresourc ad leagu scoreboard subresourc minor code cleanup and improv 011 refactor modul to fix a bug where user session were not necessarili uniqu becaus of requir cach 01 initi releas,yahoo fantasy api node module this is a node module created to wrap the yahoo fantasy sport api link at the moment not all subresources are available nor are any of the collection element i do hope to add them and they have been added to the code but a of now this project is very much in an open beta phase the api is designed to act a a helper for those interacting with the y fantasy api the goal is for ease of use for the user both in term of querying endpoint and parsing response ive noticed that in working with the api the data is not always the easiest to understand so hopefully what i have created here will help people out installation you can install the module via npm by running npm install yahoofantasy licence this module is available under the mit licence documentation more complete documentation can be found using the application sandbox this sandbox is always a work in progress if ive learned anything it that nothing is ever complete the api can be used by simply importing the module and querying data since version 40 the authentication flow ha been built into the library to make thing easier for user import the library const yahoofantasy requireyahoofantasy you can get an application keysecret by creating a new application on yahoo const yf new yahoofantasy yapplication_key yahoo application key yapplication_secret yahoo application secret tokencallbackfunction callback function when user token is refreshed optional redirecturi redirect endpoint when user authenticates optional you can authenticate a user by setting a route to call the auth function note from v40 on public query are now supported that is you can query public resource without authenticating a user ie game meta player meta and information from public league yfauth response response object to redirect the user to the yahoo login screen you also need to set up the callback route defined a the redirect uri above note this will automatically set the user and refresh token if the request is successful but you can also call them manually described below yfauthcallback request the request will contain the auth code from yahoo callback callback function that will be called after the token ha been retrieved if youre not authenticating via the library youll need to set the yahoo token for the user yfsetusertoken yclient_token you can do the same for the refresh token if you set this and the token expires last an hour then the token will automatically refresh and call the above tokencallbackfunction that youve specified to persist the token elsewhere yfsetrefreshtoken yclient_refresh_token query a resourcesubresource yfresourcesubresource possible argument function cberr data handle error callback function do your thing starting with v310 you can also use a promise chain to query resource and subresources yfresourcesubresource possible argument thendata do your thing catcherr handle error this also open the door to use asyncawait in version of node that support it try let data await yfresourcesubresource possible argument do your thing catcherr handle error bug issue this project is very much still a work in progress please report any issue via the github issue page changelog 411 small change to the way the resource and collection file are being imported a it wa causing issue on some host 410 maybe would have made sense a a 500 a there may be breaking change but i havent been able to find any yet the authcallback function will now return an object with the user access_token and refresh_token the auth function will accept a state string allowing for state persistence through the authentication process reenabled the transactionsfetch collection call cleaned up the wavier_days and stat_categories object on league resource added deprecation warning to the gameleagues and gameplayers function a theyre not very useful in that context 400 added auth authcallback setrefreshtoken function to the library automatically handle refreshing of the token and call a user defined function when the token ha expired added support for public query general cleanup 320 added player subresource to league in order to obtain weekly season stats for a player based on league setting fixed a bug where the starting status wasnt properly being returned due to a shift in how the data wa being returned removed use of request library for size and performance reason general code optimization and improvement 312 updated outdated dependency 311 resolve error when no team logo is present issue 42 310 introduced promise based flow for all endpoint a an alternative to callback thanks marois 304 fixed a bug in the playersleague collection call where it wa trying to use split on an array issue 46 fixed similar bug in other place 303 added the ability to specify a date or week when querying the teamstats resource unit test fix issue 42 thanks marios updated vulnerable dependency 302 fixed an issue with the usergame_leagues resource where the data wa not at all user friendly renamed league to game at the top level of the return object 301 fixed some typo in some import statement which caused issue on some server 300 major refactor to use es6 2015 2018 whatever the hell theyre calling it now using e module mjs file where possible removed transaction collection theyll be back 204 added a fix to give a cleaner value for the new batting order attribute in the player oject 203 fixed a bug where the league player collection wa not properly parsing the ownership subresource 202 fixed a bug where mapteampoints helper function wa not defining self thanks platky 201 removed the code that added a reason to error coming from yahoo a it wa breaking other error retry notification should now be handled within the application using the module 200 moved to yahoo oauth20 authentication mechanism 102 fixed game resource roster postions callback bug 101 fixed a typo that wa breaking team mapping 100 breaking change fixed nfl scoreboardmatchups bug issue 19 in fixing this bug i realized that my team set up wa really only useful for mlb fantasy so i rewrote team mapping to work better across all sport and give additional detail that werent previously reported this will cause error if you are using the teammanager attribute in your code 053 fixed a bug where leaguefetch wa throwing an error thanks danielspector 052 fixed a bug where player stats by week url wa not being created properly thanks withsmilo 051 fixed a bug where collection that contained subresources would return no data 050 added transaction collection with functionality to add player drop player and adddrop player thanks again githubsmilo 044 fixed a bug in playerdraft_analysis thanks githubsmilo 043 added week param for leaguescoreboard added week param for teammatchups fixed a bug where individual player werent mapping properly minor code cleanup 042 added the ability to specify a date or week when querying the roster resource cleaned up the player normalization model fixed a bug where the teamroster call wa erroring 041 fix to how post data is handled 040 significantly restructured the code to have more consistency and set it up better for future plan namely post method and proper unit testing removed the refresh user token and instead return the error to the user who can handle the refresh within their application 031 additional player attribute added thanks ryus08 030 added a method to refresh the user token if it ha expired 022 hotfix to fix team collection use error first convention 020 made helper class more consistent added collection for game league player and team moved to error first convention because javascript 012 added team matchup subresource added league scoreboard subresource minor code cleanup and improvement 011 refactored module to fix a bug where user session were not necessarily unique because of require caching 01 initial release,yahoo fantasy api node module node module created wrap yahoo fantasy sport api link moment subresources available collection element hope add added code project much open beta phase api designed act helper interacting fantasy api goal ease use user term querying endpoint parsing response ive noticed working api data always easiest understand hopefully created help people installation install module via npm running npm install yahoofantasy licence module available mit licence documentation complete documentation found using application sandbox sandbox always work progress ive learned anything nothing ever complete api used simply importing module querying data since version 40 authentication flow built library make thing easier user import library const yahoofantasy requireyahoofantasy get application keysecret creating new application yahoo const yf new yahoofantasy yapplication_key yahoo application key yapplication_secret yahoo application secret tokencallbackfunction callback function user token refreshed optional redirecturi redirect endpoint user authenticates optional authenticate user setting route call auth function note v40 public query supported query public resource without authenticating user ie game meta player meta information public league yfauth response response object redirect user yahoo login screen also need set callback route defined redirect uri note automatically set user refresh token request successful also call manually described yfauthcallback request request contain auth code yahoo callback callback function called token retrieved youre authenticating via library youll need set yahoo token user yfsetusertoken yclient_token refresh token set token expires last hour token automatically refresh call tokencallbackfunction youve specified persist token elsewhere yfsetrefreshtoken yclient_refresh_token query resourcesubresource yfresourcesubresource possible argument function cberr data handle error callback function thing starting v310 also use promise chain query resource subresources yfresourcesubresource possible argument thendata thing catcherr handle error also open door use asyncawait version node support try let data await yfresourcesubresource possible argument thing catcherr handle error bug issue project much still work progress please report issue via github issue page changelog 411 small change way resource collection file imported causing issue host 410 maybe would made sense 500 may breaking change havent able find yet authcallback function return object user access_token refresh_token auth function accept state string allowing state persistence authentication process reenabled transactionsfetch collection call cleaned wavier_days stat_categories object league resource added deprecation warning gameleagues gameplayers function theyre useful context 400 added auth authcallback setrefreshtoken function library automatically handle refreshing token call user defined function token expired added support public query general cleanup 320 added player subresource league order obtain weekly season stats player based league setting fixed bug starting status wasnt properly returned due shift data returned removed use request library size performance reason general code optimization improvement 312 updated outdated dependency 311 resolve error team logo present issue 42 310 introduced promise based flow endpoint alternative callback thanks marois 304 fixed bug playersleague collection call trying use split array issue 46 fixed similar bug place 303 added ability specify date week querying teamstats resource unit test fix issue 42 thanks marios updated vulnerable dependency 302 fixed issue usergame_leagues resource data user friendly renamed league game top level return object 301 fixed typo import statement caused issue server 300 major refactor use es6 2015 2018 whatever hell theyre calling using e module mjs file possible removed transaction collection theyll back 204 added fix give cleaner value new batting order attribute player oject 203 fixed bug league player collection properly parsing ownership subresource 202 fixed bug mapteampoints helper function defining self thanks platky 201 removed code added reason error coming yahoo breaking error retry notification handled within application using module 200 moved yahoo oauth20 authentication mechanism 102 fixed game resource roster postions callback bug 101 fixed typo breaking team mapping 100 breaking change fixed nfl scoreboardmatchups bug issue 19 fixing bug realized team set really useful mlb fantasy rewrote team mapping work better across sport give additional detail werent previously reported cause error using teammanager attribute code 053 fixed bug leaguefetch throwing error thanks danielspector 052 fixed bug player stats week url created properly thanks withsmilo 051 fixed bug collection contained subresources would return data 050 added transaction collection functionality add player drop player adddrop player thanks githubsmilo 044 fixed bug playerdraft_analysis thanks githubsmilo 043 added week param leaguescoreboard added week param teammatchups fixed bug individual player werent mapping properly minor code cleanup 042 added ability specify date week querying roster resource cleaned player normalization model fixed bug teamroster call erroring 041 fix post data handled 040 significantly restructured code consistency set better future plan namely post method proper unit testing removed refresh user token instead return error user handle refresh within application 031 additional player attribute added thanks ryus08 030 added method refresh user token expired 022 hotfix fix team collection use error first convention 020 made helper class consistent added collection game league player team moved error first convention javascript 012 added team matchup subresource added league scoreboard subresource minor code cleanup improvement 011 refactored module fix bug user session necessarily unique require caching 01 initial release
JavaScript ,"#Angular Sports Ticker Directive
##What is this?
An Angular directive that approximates a responsive (for screen sizes greater than 767px) sports news ticker (similar to a popular sports news network's ""bottomline"").
##Why?
This was a fun-side project I used to teach myself more about Angular and CSS animations. There are definitely some flaws and it's certainly not as polished as it could be, but I'm putting it out there anyway.
##How do I use it?

Include the sportsTicker.js and sportsTicker.css in your document
Add the ""sportsTicker"" module as a dependency in your Angular app
Add the <sportsticker> tag to your page's markup:
<sportsticker feed=""feed"" message-delay=""4000"" scroll-speed-factor=""6.25""></sportsticker>
Have an Angular controller provide a JSON ""feed"" to the directive (see feed.json for examples of all item types)

See the demo app for a complete example.
##Caveats:
The ticker is hidden on mobile devices (i.e., for devices with a max-width of 767px), and performance on tablets is likely shaky at best.  I just didn't think I could provide a nice mobile experience with the limited screen real estate, and this code isn't optimized for the non-desktop experience (e.g., no hardware-acceleration on animations).  In fact, the code isn't really optimized at all.  As I stated, this was just a side-project, and should not be viewed as battle-tested, production-ready code.
I don't have any immediate plans to address these limitations, so feel free to clone and do with this what you wish, if anything.  Code is MIT licensed, so go crazy.
##Acknowledgements

modernizr
backstretch
prefix-free
normalize.css
very cool faux NFL logos

",angular sport ticker direct what is thi an angular direct that approxim a respons for screen size greater than 767px sport news ticker similar to a popular sport news network bottomlin whi thi wa a funsid project i use to teach myself more about angular and css anim there are definit some flaw and it certainli not as polish as it could be but im put it out there anyway how do i use it includ the sportstickerj and sportstickercss in your document add the sportstick modul as a depend in your angular app add the sportstick tag to your page markup sportstick feedfe messagedelay4000 scrollspeedfactor625sportstick have an angular control provid a json feed to the direct see feedjson for exampl of all item type see the demo app for a complet exampl caveat the ticker is hidden on mobil devic ie for devic with a maxwidth of 767px and perform on tablet is like shaki at best i just didnt think i could provid a nice mobil experi with the limit screen real estat and thi code isnt optim for the nondesktop experi eg no hardwareacceler on anim in fact the code isnt realli optim at all as i state thi wa just a sideproject and should not be view as battletest productionreadi code i dont have ani immedi plan to address these limit so feel free to clone and do with thi what you wish if anyth code is mit licens so go crazi acknowledg modernizr backstretch prefixfre normalizecss veri cool faux nfl logo,angular sport ticker directive what is this an angular directive that approximates a responsive for screen size greater than 767px sport news ticker similar to a popular sport news network bottomline why this wa a funside project i used to teach myself more about angular and cs animation there are definitely some flaw and it certainly not a polished a it could be but im putting it out there anyway how do i use it include the sportstickerjs and sportstickercss in your document add the sportsticker module a a dependency in your angular app add the sportsticker tag to your page markup sportsticker feedfeed messagedelay4000 scrollspeedfactor625sportsticker have an angular controller provide a json feed to the directive see feedjson for example of all item type see the demo app for a complete example caveat the ticker is hidden on mobile device ie for device with a maxwidth of 767px and performance on tablet is likely shaky at best i just didnt think i could provide a nice mobile experience with the limited screen real estate and this code isnt optimized for the nondesktop experience eg no hardwareacceleration on animation in fact the code isnt really optimized at all a i stated this wa just a sideproject and should not be viewed a battletested productionready code i dont have any immediate plan to address these limitation so feel free to clone and do with this what you wish if anything code is mit licensed so go crazy acknowledgement modernizr backstretch prefixfree normalizecss very cool faux nfl logo,angular sport ticker directive angular directive approximates responsive screen size greater 767px sport news ticker similar popular sport news network bottomline funside project used teach angular cs animation definitely flaw certainly polished could im putting anyway use include sportstickerjs sportstickercss document add sportsticker module dependency angular app add sportsticker tag page markup sportsticker feedfeed messagedelay4000 scrollspeedfactor625sportsticker angular controller provide json feed directive see feedjson example item type see demo app complete example caveat ticker hidden mobile device ie device maxwidth 767px performance tablet likely shaky best didnt think could provide nice mobile experience limited screen real estate code isnt optimized nondesktop experience eg hardwareacceleration animation fact code isnt really optimized stated sideproject viewed battletested productionready code dont immediate plan address limitation feel free clone wish anything code mit licensed go crazy acknowledgement modernizr backstretch prefixfree normalizecss cool faux nfl logo
JavaScript ,"Statistics and Data Analysis


Mini javascript statistics library for nodejs or the browser.
No production dependencies.
Current Library Coverage

Standard Deviation
Mean
Median (sorts before calculating)
Median Absolute Deviation (MAD)
Outlier Detection & Filtering using Iglewicz and Hoaglin's method (MAD) - Use this if the order of your data does not matter.
Outlier Detection & Filtering using Median Differencing (Default method) - Use this if the order of your data matters. This looks at the difference between adjacent points best for time series data.

Node.js / Browserify / ES6 module
$ npm install stats-analysis

var stats = require(""./stats-analysis"") // include statistics library
Browser
<script src=""https://unpkg.com/stats-analysis""></script>
window.stats
Usage
var arr = [-2, 1, 2, 3, 3, 4, 15]

//standard deviation
stats.stdev(arr).toFixed(2) * 1 // Round to 2dp and convert to number
> 4.98

//mean
stats.mean(arr).toFixed(2) * 1
> 3.57

//median
stats.median(arr)
> 2

//median absolute deviation
stats.MAD(arr)
> 1

// Outlier detection. Returns indexes of outliers
stats.indexOfOutliers(arr)  // Default theshold of 3
> [6]

stats.indexOfOutliers(arr, 6) // Supply higher threshold to allow more outliers.

// Outlier filtering. Returns array with outliers removed.
stats.filterOutliers(arr)
> [-2, 1, 2, 3, 3, 4]
To use different outlier methods:
stats.filterOutliers(arr, stats.outlierMethod.medianDiff)
stats.filterOutliers(arr, stats.outlierMethod.medianDiff, 6) // Different threshold
stats.filterOutliers(arr, stats.outlierMethod.MAD) // Default

stats.indexOfOutliers(arr, stats.outlierMethod.medianDiff)
stats.indexOfOutliers(arr, stats.outlierMethod.medianDiff, 6) // Different threshold
stats.indexOfOutliers(arr, stats.outlierMethod.MAD) // Default
Development
Mocha is used as the testing framework.
Istanbul and codecov used for code coverage.
Commands:
$ npm install   // Grab mocha
$ npm run lint  // Ensure code consistency with standard
$ npm test      // Run tests
$ npm run cov   // Run code coverage. (Ensure 100%)
Resources
Engineering statistics handbook:
http://www.itl.nist.gov/div898/handbook/index.htm
Contribute to the library

Fork it!
Create your feature branch: git checkout -b my-new-feature
Make changes and ensure tests and code coverage all pass.
Commit your changes: git commit -m 'Add some feature'
Push to the branch: git push origin my-new-feature
Submit a pull request :D

License
MIT
",statist and data analysi mini javascript statist librari for nodej or the browser no product depend current librari coverag standard deviat mean median sort befor calcul median absolut deviat mad outlier detect filter use iglewicz and hoaglin method mad use thi if the order of your data doe not matter outlier detect filter use median differenc default method use thi if the order of your data matter thi look at the differ between adjac point best for time seri data nodej browserifi es6 modul npm instal statsanalysi var stat requirestatsanalysi includ statist librari browser script srchttpsunpkgcomstatsanalysisscript windowstat usag var arr 2 1 2 3 3 4 15 standard deviat statsstdevarrtofixed2 1 round to 2dp and convert to number 498 mean statsmeanarrtofixed2 1 357 median statsmedianarr 2 median absolut deviat statsmadarr 1 outlier detect return index of outlier statsindexofoutliersarr default theshold of 3 6 statsindexofoutliersarr 6 suppli higher threshold to allow more outlier outlier filter return array with outlier remov statsfilteroutliersarr 2 1 2 3 3 4 to use differ outlier method statsfilteroutliersarr statsoutliermethodmediandiff statsfilteroutliersarr statsoutliermethodmediandiff 6 differ threshold statsfilteroutliersarr statsoutliermethodmad default statsindexofoutliersarr statsoutliermethodmediandiff statsindexofoutliersarr statsoutliermethodmediandiff 6 differ threshold statsindexofoutliersarr statsoutliermethodmad default develop mocha is use as the test framework istanbul and codecov use for code coverag command npm instal grab mocha npm run lint ensur code consist with standard npm test run test npm run cov run code coverag ensur 100 resourc engin statist handbook httpwwwitlnistgovdiv898handbookindexhtm contribut to the librari fork it creat your featur branch git checkout b mynewfeatur make chang and ensur test and code coverag all pass commit your chang git commit m add some featur push to the branch git push origin mynewfeatur submit a pull request d licens mit,statistic and data analysis mini javascript statistic library for nodejs or the browser no production dependency current library coverage standard deviation mean median sort before calculating median absolute deviation mad outlier detection filtering using iglewicz and hoaglins method mad use this if the order of your data doe not matter outlier detection filtering using median differencing default method use this if the order of your data matter this look at the difference between adjacent point best for time series data nodejs browserify es6 module npm install statsanalysis var stats requirestatsanalysis include statistic library browser script srchttpsunpkgcomstatsanalysisscript windowstats usage var arr 2 1 2 3 3 4 15 standard deviation statsstdevarrtofixed2 1 round to 2dp and convert to number 498 mean statsmeanarrtofixed2 1 357 median statsmedianarr 2 median absolute deviation statsmadarr 1 outlier detection return index of outlier statsindexofoutliersarr default theshold of 3 6 statsindexofoutliersarr 6 supply higher threshold to allow more outlier outlier filtering return array with outlier removed statsfilteroutliersarr 2 1 2 3 3 4 to use different outlier method statsfilteroutliersarr statsoutliermethodmediandiff statsfilteroutliersarr statsoutliermethodmediandiff 6 different threshold statsfilteroutliersarr statsoutliermethodmad default statsindexofoutliersarr statsoutliermethodmediandiff statsindexofoutliersarr statsoutliermethodmediandiff 6 different threshold statsindexofoutliersarr statsoutliermethodmad default development mocha is used a the testing framework istanbul and codecov used for code coverage command npm install grab mocha npm run lint ensure code consistency with standard npm test run test npm run cov run code coverage ensure 100 resource engineering statistic handbook httpwwwitlnistgovdiv898handbookindexhtm contribute to the library fork it create your feature branch git checkout b mynewfeature make change and ensure test and code coverage all pas commit your change git commit m add some feature push to the branch git push origin mynewfeature submit a pull request d license mit,statistic data analysis mini javascript statistic library nodejs browser production dependency current library coverage standard deviation mean median sort calculating median absolute deviation mad outlier detection filtering using iglewicz hoaglins method mad use order data matter outlier detection filtering using median differencing default method use order data matter look difference adjacent point best time series data nodejs browserify es6 module npm install statsanalysis var stats requirestatsanalysis include statistic library browser script srchttpsunpkgcomstatsanalysisscript windowstats usage var arr 2 1 2 3 3 4 15 standard deviation statsstdevarrtofixed2 1 round 2dp convert number 498 mean statsmeanarrtofixed2 1 357 median statsmedianarr 2 median absolute deviation statsmadarr 1 outlier detection return index outlier statsindexofoutliersarr default theshold 3 6 statsindexofoutliersarr 6 supply higher threshold allow outlier outlier filtering return array outlier removed statsfilteroutliersarr 2 1 2 3 3 4 use different outlier method statsfilteroutliersarr statsoutliermethodmediandiff statsfilteroutliersarr statsoutliermethodmediandiff 6 different threshold statsfilteroutliersarr statsoutliermethodmad default statsindexofoutliersarr statsoutliermethodmediandiff statsindexofoutliersarr statsoutliermethodmediandiff 6 different threshold statsindexofoutliersarr statsoutliermethodmad default development mocha used testing framework istanbul codecov used code coverage command npm install grab mocha npm run lint ensure code consistency standard npm test run test npm run cov run code coverage ensure 100 resource engineering statistic handbook httpwwwitlnistgovdiv898handbookindexhtm contribute library fork create feature branch git checkout b mynewfeature make change ensure test code coverage pas commit change git commit add feature push branch git push origin mynewfeature submit pull request license mit
JavaScript ,"Online-Courses-Learning
Computer Science
Data Science

IBM Data Science Professional Certificate - Coursera

What is Data Science? - Coursera - Github
Open Source tools for Data Science - Coursera - Github
Data Science Methodology - Coursera - Github
Python for Data Science - Coursera - Github
Databases and SQL for Data Science - Coursera - Github
Data Analysis with Python - Coursera - Github
Data Visualization with Python - Coursera - Github



Machine Learning

Machine Learning with TensorFlow on Google Cloud Platform - Google Cloud

How Google does Machine Learning
Launching into Machine Learning
Intro to TensorFlow
Feature Engineering
Art and Science of Machine Learning



Programming Language


Python Programming Language

Python for Everody - University of Michigan - Coursera

Programming for Everybody (Getting Started with Python) - Coursera - Github
Python Data Structures - Coursera - Github
Using Python to Access Web Data - Coursera - Github
Using Databases with Python - Coursera - Github
Capstone: Retrieving, Processing, and Visualizing Data with Python - Coursera - Github





Go Programming Language

Programming with Google Go - University of California, Irvine - Coursera

Getting Started with Go - Coursera - Github
Functions, Methods, and Interfaces in Go - Coursera - Github
Concurrency in Go - Coursera - Github





MATLAB Programming Language

Introduction to Programming with MATLAB - Vanderbilt University - Coursera - Github



JavaScript Programming Language

Introduction to Computer Programming - University of London - Coursera - Github



Operating System

Open Source Software Development, Linux and Git - The Linux Foundation - Coursera

Open Source Software Development Methods - Coursera - Github
Linux for Developers - Coursera - Github
Linux Tools for Developers - Coursera - Github
Using Git for Distributed Development - Coursera - Github



Mechanical Engineering

Mechanics of Materials - Georgia Institute of Technology

Mechanics of Materials I: Fundamentals of Stress & Strain and Axial Loading - Coursera - Github
Mechanics of Materials II: Thin-Walled Pressure Vessels and Torsion - Coursera - Github



Mathematics

Calculus: Single Variable - University of Pennsylvania

Calculus: Single Variable Part 1 - Functions - Coursera - Github
Calculus: Single Variable Part 2 - Differentiation - Coursera - Github
Calculus: Single Variable Part 3 - Integration - Coursera - Github
Calculus: Single Variable Part 4 - Applications - Coursera - Github



Robotics

An Introduction to Programming the Internet of Things (IOT) - University of California - Coursera

Introduction to the Internet of Things and Embedded Systems - Coursera - Github
The Arduino Platform and C Programming - Coursera - Github
Interfacing with the Arduino - Coursera - Github
The Raspberry Pi Platform and Python Programming for the Raspberry Pi - Coursera - Github
Interfacing with the Raspberry Pi - Coursera - Github
Programming for the Internet of Things Project - Coursera - Github



",onlinecourseslearn comput scienc data scienc ibm data scienc profession certif coursera what is data scienc coursera github open sourc tool for data scienc coursera github data scienc methodolog coursera github python for data scienc coursera github databas and sql for data scienc coursera github data analysi with python coursera github data visual with python coursera github machin learn machin learn with tensorflow on googl cloud platform googl cloud how googl doe machin learn launch into machin learn intro to tensorflow featur engin art and scienc of machin learn program languag python program languag python for everodi univers of michigan coursera program for everybodi get start with python coursera github python data structur coursera github use python to access web data coursera github use databas with python coursera github capston retriev process and visual data with python coursera github go program languag program with googl go univers of california irvin coursera get start with go coursera github function method and interfac in go coursera github concurr in go coursera github matlab program languag introduct to program with matlab vanderbilt univers coursera github javascript program languag introduct to comput program univers of london coursera github oper system open sourc softwar develop linux and git the linux foundat coursera open sourc softwar develop method coursera github linux for develop coursera github linux tool for develop coursera github use git for distribut develop coursera github mechan engin mechan of materi georgia institut of technolog mechan of materi i fundament of stress strain and axial load coursera github mechan of materi ii thinwal pressur vessel and torsion coursera github mathemat calculu singl variabl univers of pennsylvania calculu singl variabl part 1 function coursera github calculu singl variabl part 2 differenti coursera github calculu singl variabl part 3 integr coursera github calculu singl variabl part 4 applic coursera github robot an introduct to program the internet of thing iot univers of california coursera introduct to the internet of thing and embed system coursera github the arduino platform and c program coursera github interfac with the arduino coursera github the raspberri pi platform and python program for the raspberri pi coursera github interfac with the raspberri pi coursera github program for the internet of thing project coursera github,onlinecourseslearning computer science data science ibm data science professional certificate coursera what is data science coursera github open source tool for data science coursera github data science methodology coursera github python for data science coursera github database and sql for data science coursera github data analysis with python coursera github data visualization with python coursera github machine learning machine learning with tensorflow on google cloud platform google cloud how google doe machine learning launching into machine learning intro to tensorflow feature engineering art and science of machine learning programming language python programming language python for everody university of michigan coursera programming for everybody getting started with python coursera github python data structure coursera github using python to access web data coursera github using database with python coursera github capstone retrieving processing and visualizing data with python coursera github go programming language programming with google go university of california irvine coursera getting started with go coursera github function method and interface in go coursera github concurrency in go coursera github matlab programming language introduction to programming with matlab vanderbilt university coursera github javascript programming language introduction to computer programming university of london coursera github operating system open source software development linux and git the linux foundation coursera open source software development method coursera github linux for developer coursera github linux tool for developer coursera github using git for distributed development coursera github mechanical engineering mechanic of material georgia institute of technology mechanic of material i fundamental of stress strain and axial loading coursera github mechanic of material ii thinwalled pressure vessel and torsion coursera github mathematics calculus single variable university of pennsylvania calculus single variable part 1 function coursera github calculus single variable part 2 differentiation coursera github calculus single variable part 3 integration coursera github calculus single variable part 4 application coursera github robotics an introduction to programming the internet of thing iot university of california coursera introduction to the internet of thing and embedded system coursera github the arduino platform and c programming coursera github interfacing with the arduino coursera github the raspberry pi platform and python programming for the raspberry pi coursera github interfacing with the raspberry pi coursera github programming for the internet of thing project coursera github,onlinecourseslearning computer science data science ibm data science professional certificate coursera data science coursera github open source tool data science coursera github data science methodology coursera github python data science coursera github database sql data science coursera github data analysis python coursera github data visualization python coursera github machine learning machine learning tensorflow google cloud platform google cloud google machine learning launching machine learning intro tensorflow feature engineering art science machine learning programming language python programming language python everody university michigan coursera programming everybody getting started python coursera github python data structure coursera github using python access web data coursera github using database python coursera github capstone retrieving processing visualizing data python coursera github go programming language programming google go university california irvine coursera getting started go coursera github function method interface go coursera github concurrency go coursera github matlab programming language introduction programming matlab vanderbilt university coursera github javascript programming language introduction computer programming university london coursera github operating system open source software development linux git linux foundation coursera open source software development method coursera github linux developer coursera github linux tool developer coursera github using git distributed development coursera github mechanical engineering mechanic material georgia institute technology mechanic material fundamental stress strain axial loading coursera github mechanic material ii thinwalled pressure vessel torsion coursera github mathematics calculus single variable university pennsylvania calculus single variable part 1 function coursera github calculus single variable part 2 differentiation coursera github calculus single variable part 3 integration coursera github calculus single variable part 4 application coursera github robotics introduction programming internet thing iot university california coursera introduction internet thing embedded system coursera github arduino platform c programming coursera github interfacing arduino coursera github raspberry pi platform python programming raspberry pi coursera github interfacing raspberry pi coursera github programming internet thing project coursera github
JavaScript ,"OnDataEngineering.net
Welcome to the content for the http://OnDataEngineering.net site
For details on how to contribute please see http://OnDataEngineering.net/site/contributing/
This content is licensed under a Creative Commons Attribution 4.0 International License. For more details please please see http://OnDataEngineering.net/site/content-license.
All code in this repository (primarily to be found in the _includes and _layouts directory) is licensed under the Apache 2.0 licence.
Serving this content locally
This is entirely optional, but if you'd like to view the content in a browser and check pages have been correctly created and metadata correctly set then you can do this under either Windows, Linux or Mac as follows:


Install the latest stable version of Ruby v2 from http://www.ruby-lang.org/en/downloads/.  Under Windows, you'll also need to install the appropriate development kit - see http://jekyll-windows.juthilo.com/1-ruby-and-devkit/.


Install the latest stable version of RubyGems from http://rubygems.org/pages/download


Install the required ruby gems (including Jekyll) by running ""gem install bundler"" and then ""bundle install""


Start the jekyll server via ""bundle exec jekyll serve""


Go to http://localhost:4000 to view the site


Note that this is not the full OnDataEngineering site, but a cut down simplified version for the purposes of creating and checking content.
There's some basic checking of metadata included, which will show up as error messages at the top of the relevent page.  These can also be searched for with ""grep -r ERROR _site""
",ondataengineeringnet welcom to the content for the httpondataengineeringnet site for detail on how to contribut pleas see httpondataengineeringnetsitecontribut thi content is licens under a creativ common attribut 40 intern licens for more detail pleas pleas see httpondataengineeringnetsitecontentlicens all code in thi repositori primarili to be found in the _includ and _layout directori is licens under the apach 20 licenc serv thi content local thi is entir option but if youd like to view the content in a browser and check page have been correctli creat and metadata correctli set then you can do thi under either window linux or mac as follow instal the latest stabl version of rubi v2 from httpwwwrubylangorgendownload under window youll also need to instal the appropri develop kit see httpjekyllwindowsjuthilocom1rubyanddevkit instal the latest stabl version of rubygem from httprubygemsorgpagesdownload instal the requir rubi gem includ jekyl by run gem instal bundler and then bundl instal start the jekyl server via bundl exec jekyl serv go to httplocalhost4000 to view the site note that thi is not the full ondataengin site but a cut down simplifi version for the purpos of creat and check content there some basic check of metadata includ which will show up as error messag at the top of the relev page these can also be search for with grep r error _site,ondataengineeringnet welcome to the content for the httpondataengineeringnet site for detail on how to contribute please see httpondataengineeringnetsitecontributing this content is licensed under a creative common attribution 40 international license for more detail please please see httpondataengineeringnetsitecontentlicense all code in this repository primarily to be found in the _includes and _layouts directory is licensed under the apache 20 licence serving this content locally this is entirely optional but if youd like to view the content in a browser and check page have been correctly created and metadata correctly set then you can do this under either window linux or mac a follows install the latest stable version of ruby v2 from httpwwwrubylangorgendownloads under window youll also need to install the appropriate development kit see httpjekyllwindowsjuthilocom1rubyanddevkit install the latest stable version of rubygems from httprubygemsorgpagesdownload install the required ruby gem including jekyll by running gem install bundler and then bundle install start the jekyll server via bundle exec jekyll serve go to httplocalhost4000 to view the site note that this is not the full ondataengineering site but a cut down simplified version for the purpose of creating and checking content there some basic checking of metadata included which will show up a error message at the top of the relevent page these can also be searched for with grep r error _site,ondataengineeringnet welcome content httpondataengineeringnet site detail contribute please see httpondataengineeringnetsitecontributing content licensed creative common attribution 40 international license detail please please see httpondataengineeringnetsitecontentlicense code repository primarily found _includes _layouts directory licensed apache 20 licence serving content locally entirely optional youd like view content browser check page correctly created metadata correctly set either window linux mac follows install latest stable version ruby v2 httpwwwrubylangorgendownloads window youll also need install appropriate development kit see httpjekyllwindowsjuthilocom1rubyanddevkit install latest stable version rubygems httprubygemsorgpagesdownload install required ruby gem including jekyll running gem install bundler bundle install start jekyll server via bundle exec jekyll serve go httplocalhost4000 view site note full ondataengineering site cut simplified version purpose creating checking content there basic checking metadata included show error message top relevent page also searched grep r error _site
JavaScript ,"
 ****







Dexcalibur
Dexcalibur is an Android reverse engineering platform focus on instrumentation automation. Its particularity is to use dynamic analysis to improve static analysis heuristics. It aims automate boring tasks related to dynamic instrumentation, such as :

Decompile/disass intercepted bytecode at runtime
Write hook code and Manage lot of hook message
Search interesting pattern / things to hook
Process data gathered by hook (dex file, invoked method, class loader, ...)
and so ...
But not only that, because Dexcalibur has own static analysis engine and it is able to execute partial piece of smali.

Official documentation is available here (website - work in progress).
See the latest news here : http://docs.dexcalibur.org/News.html
Show Dexcalibur demo videos : Demo: Less than 1 minute to hook 61 methods ? Not a problem. (youtube)
How to support Dexcalibur ?
Contribute !
Don't hesitate ! There are several ways to contribute :

Make a pull request related to a fix or a new feature
Create an issue to help me to patch/involves tools
Help me to develop UI
Send me a mail with your feedback
etc ...

A. Installation
A.1 New install
Ensure following dependencies are installed :

NodeJS 12.x LTS (upper non-LTS versions can cause issues during installation if there is not prebuilt binaries for frida-node, see Issue #27. Else, you can rebuild frida-node)
Java >= 8

Run command:
$  npm install -g dexcalibur

And start Dexcalibur with:
$  dexcalibur

Visit http://127.0.0.1:8000 and follow instruction.
Your default port number 8000 is already in use ? Specify a custom port by using --port=<number> like $  dexcalibur --port=9999�
Fill the form with the location of your workspace and default listening port, and submit it.
The workspace will contain a folder for each application you reverse using Dexcalibur.

Dexcalibur will create the workspace if the folder not exists. A standalone version of android platform tools, APKtool, and more will be downloaded into this workspace.

Once install is done, restart Dexcalibur by killing it and doing (again)
$ dexcalibur

For more information, please visit intallation guide
Or use Docker (DEPRECATED) (See docker guide):
A.2 Update
From version <= 0.6.x
You are using a previous version of Dexcalibur ?
Follow same steps than a new install, and when you should enter workspace path, enter your current workspace location.
From version >= 0.7
Just by doing:
$  npm install -g dexcalibur

Existing configuration and workspace will be detected automatically.
C. Screenshots
Following screenshots illustrate the automatic update of xrefs at runtime.


D. Features and limitations
Actually, the biggest limitation is Dexcalibur is not able to generate source code of hook targeting native function (into JNI library). However, you can declare manually a Frida's Interceptor by editing a hook.
Assuming Dexcalibur does not provide (for the moment) features to analyse native part such as JNI library or JNA, only features and limitations related to Java part have been detailled.
Analysis accuracy depends of the completeness of the Android API image used during early steps of the analysis. That means, if you use a DEX file generated from the Android.jar file from Android SDK, some references to internal methods, fields, or classes from Android java API could be missing. Better results are obtained when the analysis start from a ""boot.oat"" file extracted directly from a real device running the expected Android version.
D.1 Features
D.1.A Static analyzer
TODO : write text
D.1.B Hook manager
TODO : write text
D.1.C Dexcalibur's smali VM
Tracked behaviors
Static analyzer involved into ""Run smali (VM)"" action is able to discover and accept but track following behaviors :

Out-of-bound destination register (register out of v0 - v255)
Out-of-bound source register (register out of v0 - v65535)
Detect invalid instruction throwing implicitely an internal exception
Detect some piece of valid bytecode non-compliant with Android specification
Compute length of undefined array
Fill undefined array
and more ...

Actually, handlers/listeners for such invalid instruction are not supported but events are tracked and rendered.
Dexcalibur IR
The VM produces a custom and simplified Intermediate Representation (IR) which is displayed only to help analyst to perform its analysis.
Depending of the value of the callstack depth and configuration, IR can include or not instruction executed into called function. If the execution enters into a try block and continues to return, but never excute catch, then the catch block will not be rendered. In fact the purpose of Dexcalibur IR is to render only ""what is executed"" or ""what  could be executed depending of some symbol's value"" into VM context.
Dexcalibur IR helps to read a cleaned version of bytcode by removing useless goto and opaque predicate. Dexcalibur IR can be generated by the VM with 2 simplifying levels :
1st level IR, could be used if you don't trust 2th level IR  :

no CFG simplifying : conditions and incondtionnal jumps are rendered.
every move into a register are rendered

2th level :

Hide assign if the register is not modified with an unknown value before its use.
Always TRUE/FALSE predicate are removed
Inconditional jump such goto are removed under certain conditions : single predecessor of targeted basic block, etc ...
Resolve & replace Method.inoke() call by called method if possible.
Instructions into a Try block are not rendered if an exception is thrown before
...

Android API mock
TODO
Details
Smali VM follows steps :

Init VM : stack memory, heap, classloaders, method area, ...
The VM load class declaring the method.
(Optionnal) If the class has static blocks, clinit() is executed.  It helps to solve concrete value stored into static properties
Load method metadata
Execute method's instructions, if PseudoCodeMaker is enable, Dexcalibur IR is generated.

How VM handles invoke-* instruction ?

When an invoke-* happens, the local symbol table is saved, and the invoked method is loaded.
If the class declaring the invoked method  has never been loaded, the class is loaded
If the method has never been loaded, the method is loaded (by MethodArea) and its local symbol table initialized by importing symbols of arguments from caller's symbol table.
Invoked method is push into callstack (StackMemory).
Method instruction are executed.
Return is push into stack memory
Caller give flow control

D.1.D Application Topology  analyzers
Manifest analysis (LIMITED)
Before the first run, the Android manifest of the application is parsed. Actually, anomalies into the manifest
such insecure configuration are really detected at this level.
The only purpose of Android manifest parsing is to populate other kind of analyzers.
Permission analysis
Every permissions extracted from the Manifest are listed and identified and compared to Android specification of the target Android API version.
Dexcalibur provides - only in some case - a description of the permission purpose, the minimal Android API version, ...
Activities analysis
Providers analysis
Services analysis
Receivers analysis
D.1.E Runtime monitoring (not implemented)
Network monitoring
Intent monitoring
File access monitoring
D.1.F Collaborating features
You cannot find multi-user menu ? Not a probleme, there is not a menu but minimalistic collaborative work can be achieve.
Dexcalibur runs a web server.  So, if several people are on the same network of this web server and if host firewall is well configured, you can be several to work on the same Dexcalibur instance.
Actual limitations are :

No authentication : everybody into the network can send request to Dexcalibur instance and doing RCE the host through search engine.
No identifying : modifying are not tracked, so, if someone rename a symbol, you could not know who renamed it. Similar case : you are not able to know who created a specific hook.
Single device instrumentation : if several devices are connected to Dexcalibur's host, and even if you can choose the device to instrument, instrumentation and hook messages are linked to the last device selected. So, you cannot generate instrumention for several devices simultaneously.

E. Github Contributors
A special thanks to contributors :

ubamrein
jhscheer
eybisi
monperrus

F. Troubleshoots
F.1 Dexcalibur continues to start into ""install mode""
Before to go deeper :

Ensure you are connected to Internet : Apktool and target platform are downloaded during install
Did you have tried to reinstall it by doing dexcalibur --reinstall command ? If no, try it.

First, check if global settings have been saved into <user_directory>/.dexcalibur/
$ ls -la ~/.dexcalibur      

total 8
drwxr-xr-x   3 test_user  staff    96 29 avr 11:41 .
drwxr-xr-x+ 87 test_user  staff  2784 29 avr 11:47 ..
-rw-r--r--   1 test_user  staff   204 29 avr 11:41 config.json


$ cat ~/.dexcalibur/config.json 

{
    ""workspace"":""/Users/test_user/dexcaliburWS3"",
    ""registry"":""https://github.com/FrenchYeti/dexcalibur-registry/raw/master/"",
    ""registryAPI"":""https://api.github.com/repos/FrenchYeti/dexcalibur-registry/contents/""
}

Next, check if structure of Dexcalibur workspace is as following (content of /api folder may differs).
$ ls -la ~/dexcaliburWS/.dxc/*
/Users/test_user/dexcaliburWS/.dxc/api:
total 0
drwxr-xr-x  3 test_user  staff   96 29 avr 11:41 .
drwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..
drwxr-xr-x  8 test_user  staff  256 29 avr 11:41 sdk_androidapi_29_google

/Users/test_user/dexcaliburWS/.dxc/bin:
total 34824
drwxr-xr-x   4 test_user  staff       128 29 avr 11:41 .
drwxr-xr-x   7 test_user  staff       224 29 avr 11:41 ..
-rwxr-xr-x   1 test_user  staff  17661172 29 avr 11:41 apktool.jar
drwxr-xr-x  18 test_user  staff       576 29 avr 11:41 platform-tools

/Users/test_user/dexcaliburWS/.dxc/cfg:
total 8
drwxr-xr-x  3 test_user  staff   96 29 avr 11:41 .
drwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..
-rw-r--r--  1 test_user  staff  314 29 avr 11:41 config.json

/Users/test_user/dexcaliburWS/.dxc/dev:
total 0
drwxr-xr-x  2 test_user  staff   64 29 avr 11:41 .
drwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..

/Users/test_user/dexcaliburWS/.dxc/tmp:
total 0
drwxr-xr-x  2 test_user  staff   64 29 avr 11:41 .
drwxr-xr-x  7 test_user  staff  224 29 avr 11:41 ..

G. FAQ
My device not appears when into device list
If you use a physical device connected over USB, ensure developper mode and USB debugging are enabled.
If you use a virtual device, go to /splash.html,  select Device Manager,  click Connect over TCP ... and follow instructions. If you don't know IP address of your device, let Dexcalibur detect it by checking box automatic configuration.
USB debugging is enabled, but my device not appears when into device list

Connect/disconnect USB and ensure your computer is allowed.
Select file transfert

Why enroll a new device ?
You need to enroll the target device before to be able to use it.
During enrollment Dexcalibur gather device metadata and push a compatible version of Frida server.
Such metadata are used to select right frida-server and frida-gadget targets.
My device is listed into Device Manager, but it cannot be enrolled
If a red exclamation mark ! appears on a line into device list, then your desktop is not allowed by device. You probably need to confirm
If your device is listed into DeviceManager and the column online is checked, then click enroll
G.1 My device is listed into Device Manager
If your device is listed into DeviceManager and the column online is checked, then click enroll
How to use an emulator instead of a physical device ?
Dexcalibur version < v0.7 was not able to detect automatically emulated device and use it due to an incomplete ADB output parsing.
Since version >= v0.7, once your virtual device is running, go to /splash.html or click on DEXCALIBUR into navigation bar.
Click on Device Manager button into left menu, and click the Refresh button at top of array.
You should have a row starting by the ADB ID of your virtual device.
How to use a device over TCP ?
First, as any target device, you should enroll it.
Click Connect over TCP ... to add a new device over TCP or to connect an enrolled device over TCP.
If the device has never been enrolled, so enrollment will be perform through TCP.
In some case, connection over TCP is slower than over USB. So enrollement can take additional time.
If the device has been enrolled over USB, so the new prefered transport type for this device becomes TCP.
How to contribute to the dexcalibur ?
Create a pull request on this repository or create an issue.
How to contribute to the documentation?
Create a pull request on dexcalibur-doc repository.
Documentation is available at here (doc website) and here (wiki)
H. Sponsors








They offered a license for All Products <3



I. Resources
There is actually few documentation and training resources about Dexcalibur. If you successfully used Dexcalibur to win CTF challenge or to find vulnerability, i highly encourage you to share your experience.

Slides of Pass the SALT 2019 (PDF)
Youtube : demonstration
CLI User Guide
User Guide
Troubleshoots
Screenshots

J. They wrote something about Dexcalibur

Awesome Frida
Awesome OpenSource Security
n0secure.org - PassTheSalt2019 J2
rootshell.be - PassTheSalt2019 Wrap Up
PentesterLand - the 5 hacking newsletter 61
Technology Knowledge Database
Xuanwu Lab Security
Mobile Gitbook
274 - AppsSec Ezine
ysh329 / Android Reverse Engineering

",dexcalibur dexcalibur is an android revers engin platform focu on instrument autom it particular is to use dynam analysi to improv static analysi heurist it aim autom bore task relat to dynam instrument such as decompiledisass intercept bytecod at runtim write hook code and manag lot of hook messag search interest pattern thing to hook process data gather by hook dex file invok method class loader and so but not onli that becaus dexcalibur ha own static analysi engin and it is abl to execut partial piec of smali offici document is avail here websit work in progress see the latest news here httpdocsdexcaliburorgnewshtml show dexcalibur demo video demo less than 1 minut to hook 61 method not a problem youtub how to support dexcalibur contribut dont hesit there are sever way to contribut make a pull request relat to a fix or a new featur creat an issu to help me to patchinvolv tool help me to develop ui send me a mail with your feedback etc a instal a1 new instal ensur follow depend are instal nodej 12x lt upper nonlt version can caus issu dure instal if there is not prebuilt binari for fridanod see issu 27 els you can rebuild fridanod java 8 run command npm instal g dexcalibur and start dexcalibur with dexcalibur visit http1270018000 and follow instruct your default port number 8000 is alreadi in use specifi a custom port by use portnumb like dexcalibur port9999 fill the form with the locat of your workspac and default listen port and submit it the workspac will contain a folder for each applic you revers use dexcalibur dexcalibur will creat the workspac if the folder not exist a standalon version of android platform tool apktool and more will be download into thi workspac onc instal is done restart dexcalibur by kill it and do again dexcalibur for more inform pleas visit intal guid or use docker deprec see docker guid a2 updat from version 06x you are use a previou version of dexcalibur follow same step than a new instal and when you should enter workspac path enter your current workspac locat from version 07 just by do npm instal g dexcalibur exist configur and workspac will be detect automat c screenshot follow screenshot illustr the automat updat of xref at runtim d featur and limit actual the biggest limit is dexcalibur is not abl to gener sourc code of hook target nativ function into jni librari howev you can declar manual a frida interceptor by edit a hook assum dexcalibur doe not provid for the moment featur to analys nativ part such as jni librari or jna onli featur and limit relat to java part have been detail analysi accuraci depend of the complet of the android api imag use dure earli step of the analysi that mean if you use a dex file gener from the androidjar file from android sdk some refer to intern method field or class from android java api could be miss better result are obtain when the analysi start from a bootoat file extract directli from a real devic run the expect android version d1 featur d1a static analyz todo write text d1b hook manag todo write text d1c dexcalibur smali vm track behavior static analyz involv into run smali vm action is abl to discov and accept but track follow behavior outofbound destin regist regist out of v0 v255 outofbound sourc regist regist out of v0 v65535 detect invalid instruct throw implicit an intern except detect some piec of valid bytecod noncompli with android specif comput length of undefin array fill undefin array and more actual handlerslisten for such invalid instruct are not support but event are track and render dexcalibur ir the vm produc a custom and simplifi intermedi represent ir which is display onli to help analyst to perform it analysi depend of the valu of the callstack depth and configur ir can includ or not instruct execut into call function if the execut enter into a tri block and continu to return but never excut catch then the catch block will not be render in fact the purpos of dexcalibur ir is to render onli what is execut or what could be execut depend of some symbol valu into vm context dexcalibur ir help to read a clean version of bytcod by remov useless goto and opaqu predic dexcalibur ir can be gener by the vm with 2 simplifi level 1st level ir could be use if you dont trust 2th level ir no cfg simplifi condit and incondtionn jump are render everi move into a regist are render 2th level hide assign if the regist is not modifi with an unknown valu befor it use alway truefals predic are remov incondit jump such goto are remov under certain condit singl predecessor of target basic block etc resolv replac methodinok call by call method if possibl instruct into a tri block are not render if an except is thrown befor android api mock todo detail smali vm follow step init vm stack memori heap classload method area the vm load class declar the method optionn if the class ha static block clinit is execut it help to solv concret valu store into static properti load method metadata execut method instruct if pseudocodemak is enabl dexcalibur ir is gener how vm handl invok instruct when an invok happen the local symbol tabl is save and the invok method is load if the class declar the invok method ha never been load the class is load if the method ha never been load the method is load by methodarea and it local symbol tabl initi by import symbol of argument from caller symbol tabl invok method is push into callstack stackmemori method instruct are execut return is push into stack memori caller give flow control d1d applic topolog analyz manifest analysi limit befor the first run the android manifest of the applic is pars actual anomali into the manifest such insecur configur are realli detect at thi level the onli purpos of android manifest pars is to popul other kind of analyz permiss analysi everi permiss extract from the manifest are list and identifi and compar to android specif of the target android api version dexcalibur provid onli in some case a descript of the permiss purpos the minim android api version activ analysi provid analysi servic analysi receiv analysi d1e runtim monitor not implement network monitor intent monitor file access monitor d1f collabor featur you cannot find multius menu not a problem there is not a menu but minimalist collabor work can be achiev dexcalibur run a web server so if sever peopl are on the same network of thi web server and if host firewal is well configur you can be sever to work on the same dexcalibur instanc actual limit are no authent everybodi into the network can send request to dexcalibur instanc and do rce the host through search engin no identifi modifi are not track so if someon renam a symbol you could not know who renam it similar case you are not abl to know who creat a specif hook singl devic instrument if sever devic are connect to dexcalibur host and even if you can choos the devic to instrument instrument and hook messag are link to the last devic select so you cannot gener instrument for sever devic simultan e github contributor a special thank to contributor ubamrein jhscheer eybisi monperru f troubleshoot f1 dexcalibur continu to start into instal mode befor to go deeper ensur you are connect to internet apktool and target platform are download dure instal did you have tri to reinstal it by do dexcalibur reinstal command if no tri it first check if global set have been save into user_directorydexcalibur ls la dexcalibur total 8 drwxrxrx 3 test_us staff 96 29 avr 1141 drwxrxrx 87 test_us staff 2784 29 avr 1147 rwrr 1 test_us staff 204 29 avr 1141 configjson cat dexcaliburconfigjson workspaceuserstest_userdexcaliburws3 registryhttpsgithubcomfrenchyetidexcaliburregistryrawmast registryapihttpsapigithubcomreposfrenchyetidexcaliburregistrycont next check if structur of dexcalibur workspac is as follow content of api folder may differ ls la dexcaliburwsdxc userstest_userdexcaliburwsdxcapi total 0 drwxrxrx 3 test_us staff 96 29 avr 1141 drwxrxrx 7 test_us staff 224 29 avr 1141 drwxrxrx 8 test_us staff 256 29 avr 1141 sdk_androidapi_29_googl userstest_userdexcaliburwsdxcbin total 34824 drwxrxrx 4 test_us staff 128 29 avr 1141 drwxrxrx 7 test_us staff 224 29 avr 1141 rwxrxrx 1 test_us staff 17661172 29 avr 1141 apktooljar drwxrxrx 18 test_us staff 576 29 avr 1141 platformtool userstest_userdexcaliburwsdxccfg total 8 drwxrxrx 3 test_us staff 96 29 avr 1141 drwxrxrx 7 test_us staff 224 29 avr 1141 rwrr 1 test_us staff 314 29 avr 1141 configjson userstest_userdexcaliburwsdxcdev total 0 drwxrxrx 2 test_us staff 64 29 avr 1141 drwxrxrx 7 test_us staff 224 29 avr 1141 userstest_userdexcaliburwsdxctmp total 0 drwxrxrx 2 test_us staff 64 29 avr 1141 drwxrxrx 7 test_us staff 224 29 avr 1141 g faq my devic not appear when into devic list if you use a physic devic connect over usb ensur developp mode and usb debug are enabl if you use a virtual devic go to splashhtml select devic manag click connect over tcp and follow instruct if you dont know ip address of your devic let dexcalibur detect it by check box automat configur usb debug is enabl but my devic not appear when into devic list connectdisconnect usb and ensur your comput is allow select file transfert whi enrol a new devic you need to enrol the target devic befor to be abl to use it dure enrol dexcalibur gather devic metadata and push a compat version of frida server such metadata are use to select right fridaserv and fridagadget target my devic is list into devic manag but it cannot be enrol if a red exclam mark appear on a line into devic list then your desktop is not allow by devic you probabl need to confirm if your devic is list into devicemanag and the column onlin is check then click enrol g1 my devic is list into devic manag if your devic is list into devicemanag and the column onlin is check then click enrol how to use an emul instead of a physic devic dexcalibur version v07 wa not abl to detect automat emul devic and use it due to an incomplet adb output pars sinc version v07 onc your virtual devic is run go to splashhtml or click on dexcalibur into navig bar click on devic manag button into left menu and click the refresh button at top of array you should have a row start by the adb id of your virtual devic how to use a devic over tcp first as ani target devic you should enrol it click connect over tcp to add a new devic over tcp or to connect an enrol devic over tcp if the devic ha never been enrol so enrol will be perform through tcp in some case connect over tcp is slower than over usb so enrol can take addit time if the devic ha been enrol over usb so the new prefer transport type for thi devic becom tcp how to contribut to the dexcalibur creat a pull request on thi repositori or creat an issu how to contribut to the document creat a pull request on dexcaliburdoc repositori document is avail at here doc websit and here wiki h sponsor they offer a licens for all product 3 i resourc there is actual few document and train resourc about dexcalibur if you success use dexcalibur to win ctf challeng or to find vulner i highli encourag you to share your experi slide of pass the salt 2019 pdf youtub demonstr cli user guid user guid troubleshoot screenshot j they wrote someth about dexcalibur awesom frida awesom opensourc secur n0secureorg passthesalt2019 j2 rootshellb passthesalt2019 wrap up pentesterland the 5 hack newslett 61 technolog knowledg databas xuanwu lab secur mobil gitbook 274 appssec ezin ysh329 android revers engin,dexcalibur dexcalibur is an android reverse engineering platform focus on instrumentation automation it particularity is to use dynamic analysis to improve static analysis heuristic it aim automate boring task related to dynamic instrumentation such a decompiledisass intercepted bytecode at runtime write hook code and manage lot of hook message search interesting pattern thing to hook process data gathered by hook dex file invoked method class loader and so but not only that because dexcalibur ha own static analysis engine and it is able to execute partial piece of smali official documentation is available here website work in progress see the latest news here httpdocsdexcaliburorgnewshtml show dexcalibur demo video demo le than 1 minute to hook 61 method not a problem youtube how to support dexcalibur contribute dont hesitate there are several way to contribute make a pull request related to a fix or a new feature create an issue to help me to patchinvolves tool help me to develop ui send me a mail with your feedback etc a installation a1 new install ensure following dependency are installed nodejs 12x lts upper nonlts version can cause issue during installation if there is not prebuilt binary for fridanode see issue 27 else you can rebuild fridanode java 8 run command npm install g dexcalibur and start dexcalibur with dexcalibur visit http1270018000 and follow instruction your default port number 8000 is already in use specify a custom port by using portnumber like dexcalibur port9999 fill the form with the location of your workspace and default listening port and submit it the workspace will contain a folder for each application you reverse using dexcalibur dexcalibur will create the workspace if the folder not exists a standalone version of android platform tool apktool and more will be downloaded into this workspace once install is done restart dexcalibur by killing it and doing again dexcalibur for more information please visit intallation guide or use docker deprecated see docker guide a2 update from version 06x you are using a previous version of dexcalibur follow same step than a new install and when you should enter workspace path enter your current workspace location from version 07 just by doing npm install g dexcalibur existing configuration and workspace will be detected automatically c screenshots following screenshots illustrate the automatic update of xrefs at runtime d feature and limitation actually the biggest limitation is dexcalibur is not able to generate source code of hook targeting native function into jni library however you can declare manually a fridas interceptor by editing a hook assuming dexcalibur doe not provide for the moment feature to analyse native part such a jni library or jna only feature and limitation related to java part have been detailled analysis accuracy depends of the completeness of the android api image used during early step of the analysis that mean if you use a dex file generated from the androidjar file from android sdk some reference to internal method field or class from android java api could be missing better result are obtained when the analysis start from a bootoat file extracted directly from a real device running the expected android version d1 feature d1a static analyzer todo write text d1b hook manager todo write text d1c dexcaliburs smali vm tracked behavior static analyzer involved into run smali vm action is able to discover and accept but track following behavior outofbound destination register register out of v0 v255 outofbound source register register out of v0 v65535 detect invalid instruction throwing implicitely an internal exception detect some piece of valid bytecode noncompliant with android specification compute length of undefined array fill undefined array and more actually handlerslisteners for such invalid instruction are not supported but event are tracked and rendered dexcalibur ir the vm produce a custom and simplified intermediate representation ir which is displayed only to help analyst to perform it analysis depending of the value of the callstack depth and configuration ir can include or not instruction executed into called function if the execution enters into a try block and continues to return but never excute catch then the catch block will not be rendered in fact the purpose of dexcalibur ir is to render only what is executed or what could be executed depending of some symbol value into vm context dexcalibur ir help to read a cleaned version of bytcode by removing useless goto and opaque predicate dexcalibur ir can be generated by the vm with 2 simplifying level 1st level ir could be used if you dont trust 2th level ir no cfg simplifying condition and incondtionnal jump are rendered every move into a register are rendered 2th level hide assign if the register is not modified with an unknown value before it use always truefalse predicate are removed inconditional jump such goto are removed under certain condition single predecessor of targeted basic block etc resolve replace methodinoke call by called method if possible instruction into a try block are not rendered if an exception is thrown before android api mock todo detail smali vm follows step init vm stack memory heap classloaders method area the vm load class declaring the method optionnal if the class ha static block clinit is executed it help to solve concrete value stored into static property load method metadata execute method instruction if pseudocodemaker is enable dexcalibur ir is generated how vm handle invoke instruction when an invoke happens the local symbol table is saved and the invoked method is loaded if the class declaring the invoked method ha never been loaded the class is loaded if the method ha never been loaded the method is loaded by methodarea and it local symbol table initialized by importing symbol of argument from caller symbol table invoked method is push into callstack stackmemory method instruction are executed return is push into stack memory caller give flow control d1d application topology analyzer manifest analysis limited before the first run the android manifest of the application is parsed actually anomaly into the manifest such insecure configuration are really detected at this level the only purpose of android manifest parsing is to populate other kind of analyzer permission analysis every permission extracted from the manifest are listed and identified and compared to android specification of the target android api version dexcalibur provides only in some case a description of the permission purpose the minimal android api version activity analysis provider analysis service analysis receiver analysis d1e runtime monitoring not implemented network monitoring intent monitoring file access monitoring d1f collaborating feature you cannot find multiuser menu not a probleme there is not a menu but minimalistic collaborative work can be achieve dexcalibur run a web server so if several people are on the same network of this web server and if host firewall is well configured you can be several to work on the same dexcalibur instance actual limitation are no authentication everybody into the network can send request to dexcalibur instance and doing rce the host through search engine no identifying modifying are not tracked so if someone rename a symbol you could not know who renamed it similar case you are not able to know who created a specific hook single device instrumentation if several device are connected to dexcaliburs host and even if you can choose the device to instrument instrumentation and hook message are linked to the last device selected so you cannot generate instrumention for several device simultaneously e github contributor a special thanks to contributor ubamrein jhscheer eybisi monperrus f troubleshoots f1 dexcalibur continues to start into install mode before to go deeper ensure you are connected to internet apktool and target platform are downloaded during install did you have tried to reinstall it by doing dexcalibur reinstall command if no try it first check if global setting have been saved into user_directorydexcalibur l la dexcalibur total 8 drwxrxrx 3 test_user staff 96 29 avr 1141 drwxrxrx 87 test_user staff 2784 29 avr 1147 rwrr 1 test_user staff 204 29 avr 1141 configjson cat dexcaliburconfigjson workspaceuserstest_userdexcaliburws3 registryhttpsgithubcomfrenchyetidexcaliburregistryrawmaster registryapihttpsapigithubcomreposfrenchyetidexcaliburregistrycontents next check if structure of dexcalibur workspace is a following content of api folder may differs l la dexcaliburwsdxc userstest_userdexcaliburwsdxcapi total 0 drwxrxrx 3 test_user staff 96 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 drwxrxrx 8 test_user staff 256 29 avr 1141 sdk_androidapi_29_google userstest_userdexcaliburwsdxcbin total 34824 drwxrxrx 4 test_user staff 128 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 rwxrxrx 1 test_user staff 17661172 29 avr 1141 apktooljar drwxrxrx 18 test_user staff 576 29 avr 1141 platformtools userstest_userdexcaliburwsdxccfg total 8 drwxrxrx 3 test_user staff 96 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 rwrr 1 test_user staff 314 29 avr 1141 configjson userstest_userdexcaliburwsdxcdev total 0 drwxrxrx 2 test_user staff 64 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 userstest_userdexcaliburwsdxctmp total 0 drwxrxrx 2 test_user staff 64 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 g faq my device not appears when into device list if you use a physical device connected over usb ensure developper mode and usb debugging are enabled if you use a virtual device go to splashhtml select device manager click connect over tcp and follow instruction if you dont know ip address of your device let dexcalibur detect it by checking box automatic configuration usb debugging is enabled but my device not appears when into device list connectdisconnect usb and ensure your computer is allowed select file transfert why enroll a new device you need to enroll the target device before to be able to use it during enrollment dexcalibur gather device metadata and push a compatible version of frida server such metadata are used to select right fridaserver and fridagadget target my device is listed into device manager but it cannot be enrolled if a red exclamation mark appears on a line into device list then your desktop is not allowed by device you probably need to confirm if your device is listed into devicemanager and the column online is checked then click enroll g1 my device is listed into device manager if your device is listed into devicemanager and the column online is checked then click enroll how to use an emulator instead of a physical device dexcalibur version v07 wa not able to detect automatically emulated device and use it due to an incomplete adb output parsing since version v07 once your virtual device is running go to splashhtml or click on dexcalibur into navigation bar click on device manager button into left menu and click the refresh button at top of array you should have a row starting by the adb id of your virtual device how to use a device over tcp first a any target device you should enroll it click connect over tcp to add a new device over tcp or to connect an enrolled device over tcp if the device ha never been enrolled so enrollment will be perform through tcp in some case connection over tcp is slower than over usb so enrollement can take additional time if the device ha been enrolled over usb so the new prefered transport type for this device becomes tcp how to contribute to the dexcalibur create a pull request on this repository or create an issue how to contribute to the documentation create a pull request on dexcaliburdoc repository documentation is available at here doc website and here wiki h sponsor they offered a license for all product 3 i resource there is actually few documentation and training resource about dexcalibur if you successfully used dexcalibur to win ctf challenge or to find vulnerability i highly encourage you to share your experience slide of pas the salt 2019 pdf youtube demonstration cli user guide user guide troubleshoots screenshots j they wrote something about dexcalibur awesome frida awesome opensource security n0secureorg passthesalt2019 j2 rootshellbe passthesalt2019 wrap up pentesterland the 5 hacking newsletter 61 technology knowledge database xuanwu lab security mobile gitbook 274 appssec ezine ysh329 android reverse engineering,dexcalibur dexcalibur android reverse engineering platform focus instrumentation automation particularity use dynamic analysis improve static analysis heuristic aim automate boring task related dynamic instrumentation decompiledisass intercepted bytecode runtime write hook code manage lot hook message search interesting pattern thing hook process data gathered hook dex file invoked method class loader dexcalibur static analysis engine able execute partial piece smali official documentation available website work progress see latest news httpdocsdexcaliburorgnewshtml show dexcalibur demo video demo le 1 minute hook 61 method problem youtube support dexcalibur contribute dont hesitate several way contribute make pull request related fix new feature create issue help patchinvolves tool help develop ui send mail feedback etc installation a1 new install ensure following dependency installed nodejs 12x lts upper nonlts version cause issue installation prebuilt binary fridanode see issue 27 else rebuild fridanode java 8 run command npm install g dexcalibur start dexcalibur dexcalibur visit http1270018000 follow instruction default port number 8000 already use specify custom port using portnumber like dexcalibur port9999 fill form location workspace default listening port submit workspace contain folder application reverse using dexcalibur dexcalibur create workspace folder exists standalone version android platform tool apktool downloaded workspace install done restart dexcalibur killing dexcalibur information please visit intallation guide use docker deprecated see docker guide a2 update version 06x using previous version dexcalibur follow step new install enter workspace path enter current workspace location version 07 npm install g dexcalibur existing configuration workspace detected automatically c screenshots following screenshots illustrate automatic update xrefs runtime feature limitation actually biggest limitation dexcalibur able generate source code hook targeting native function jni library however declare manually fridas interceptor editing hook assuming dexcalibur provide moment feature analyse native part jni library jna feature limitation related java part detailled analysis accuracy depends completeness android api image used early step analysis mean use dex file generated androidjar file android sdk reference internal method field class android java api could missing better result obtained analysis start bootoat file extracted directly real device running expected android version d1 feature d1a static analyzer todo write text d1b hook manager todo write text d1c dexcaliburs smali vm tracked behavior static analyzer involved run smali vm action able discover accept track following behavior outofbound destination register register v0 v255 outofbound source register register v0 v65535 detect invalid instruction throwing implicitely internal exception detect piece valid bytecode noncompliant android specification compute length undefined array fill undefined array actually handlerslisteners invalid instruction supported event tracked rendered dexcalibur ir vm produce custom simplified intermediate representation ir displayed help analyst perform analysis depending value callstack depth configuration ir include instruction executed called function execution enters try block continues return never excute catch catch block rendered fact purpose dexcalibur ir render executed could executed depending symbol value vm context dexcalibur ir help read cleaned version bytcode removing useless goto opaque predicate dexcalibur ir generated vm 2 simplifying level 1st level ir could used dont trust 2th level ir cfg simplifying condition incondtionnal jump rendered every move register rendered 2th level hide assign register modified unknown value use always truefalse predicate removed inconditional jump goto removed certain condition single predecessor targeted basic block etc resolve replace methodinoke call called method possible instruction try block rendered exception thrown android api mock todo detail smali vm follows step init vm stack memory heap classloaders method area vm load class declaring method optionnal class static block clinit executed help solve concrete value stored static property load method metadata execute method instruction pseudocodemaker enable dexcalibur ir generated vm handle invoke instruction invoke happens local symbol table saved invoked method loaded class declaring invoked method never loaded class loaded method never loaded method loaded methodarea local symbol table initialized importing symbol argument caller symbol table invoked method push callstack stackmemory method instruction executed return push stack memory caller give flow control d1d application topology analyzer manifest analysis limited first run android manifest application parsed actually anomaly manifest insecure configuration really detected level purpose android manifest parsing populate kind analyzer permission analysis every permission extracted manifest listed identified compared android specification target android api version dexcalibur provides case description permission purpose minimal android api version activity analysis provider analysis service analysis receiver analysis d1e runtime monitoring implemented network monitoring intent monitoring file access monitoring d1f collaborating feature cannot find multiuser menu probleme menu minimalistic collaborative work achieve dexcalibur run web server several people network web server host firewall well configured several work dexcalibur instance actual limitation authentication everybody network send request dexcalibur instance rce host search engine identifying modifying tracked someone rename symbol could know renamed similar case able know created specific hook single device instrumentation several device connected dexcaliburs host even choose device instrument instrumentation hook message linked last device selected cannot generate instrumention several device simultaneously e github contributor special thanks contributor ubamrein jhscheer eybisi monperrus f troubleshoots f1 dexcalibur continues start install mode go deeper ensure connected internet apktool target platform downloaded install tried reinstall dexcalibur reinstall command try first check global setting saved user_directorydexcalibur l la dexcalibur total 8 drwxrxrx 3 test_user staff 96 29 avr 1141 drwxrxrx 87 test_user staff 2784 29 avr 1147 rwrr 1 test_user staff 204 29 avr 1141 configjson cat dexcaliburconfigjson workspaceuserstest_userdexcaliburws3 registryhttpsgithubcomfrenchyetidexcaliburregistryrawmaster registryapihttpsapigithubcomreposfrenchyetidexcaliburregistrycontents next check structure dexcalibur workspace following content api folder may differs l la dexcaliburwsdxc userstest_userdexcaliburwsdxcapi total 0 drwxrxrx 3 test_user staff 96 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 drwxrxrx 8 test_user staff 256 29 avr 1141 sdk_androidapi_29_google userstest_userdexcaliburwsdxcbin total 34824 drwxrxrx 4 test_user staff 128 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 rwxrxrx 1 test_user staff 17661172 29 avr 1141 apktooljar drwxrxrx 18 test_user staff 576 29 avr 1141 platformtools userstest_userdexcaliburwsdxccfg total 8 drwxrxrx 3 test_user staff 96 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 rwrr 1 test_user staff 314 29 avr 1141 configjson userstest_userdexcaliburwsdxcdev total 0 drwxrxrx 2 test_user staff 64 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 userstest_userdexcaliburwsdxctmp total 0 drwxrxrx 2 test_user staff 64 29 avr 1141 drwxrxrx 7 test_user staff 224 29 avr 1141 g faq device appears device list use physical device connected usb ensure developper mode usb debugging enabled use virtual device go splashhtml select device manager click connect tcp follow instruction dont know ip address device let dexcalibur detect checking box automatic configuration usb debugging enabled device appears device list connectdisconnect usb ensure computer allowed select file transfert enroll new device need enroll target device able use enrollment dexcalibur gather device metadata push compatible version frida server metadata used select right fridaserver fridagadget target device listed device manager cannot enrolled red exclamation mark appears line device list desktop allowed device probably need confirm device listed devicemanager column online checked click enroll g1 device listed device manager device listed devicemanager column online checked click enroll use emulator instead physical device dexcalibur version v07 able detect automatically emulated device use due incomplete adb output parsing since version v07 virtual device running go splashhtml click dexcalibur navigation bar click device manager button left menu click refresh button top array row starting adb id virtual device use device tcp first target device enroll click connect tcp add new device tcp connect enrolled device tcp device never enrolled enrollment perform tcp case connection tcp slower usb enrollement take additional time device enrolled usb new prefered transport type device becomes tcp contribute dexcalibur create pull request repository create issue contribute documentation create pull request dexcaliburdoc repository documentation available doc website wiki h sponsor offered license product 3 resource actually documentation training resource dexcalibur successfully used dexcalibur win ctf challenge find vulnerability highly encourage share experience slide pas salt 2019 pdf youtube demonstration cli user guide user guide troubleshoots screenshots j wrote something dexcalibur awesome frida awesome opensource security n0secureorg passthesalt2019 j2 rootshellbe passthesalt2019 wrap pentesterland 5 hacking newsletter 61 technology knowledge database xuanwu lab security mobile gitbook 274 appssec ezine ysh329 android reverse engineering
JavaScript ,"Blockchain Now
Agenda: End-to-end pipeline for the bitcoin analytics. Reach UI with reactive search and charts.

Course project for Insight Data Engineering program
How to install
TL;DR
Git clone the repository
git clone git@github.com:igorbarinov/blockchainnow.git

From meteorui/ start meteor server
meteor

Open http://localhost:3000 in you favorite Google Chrome browser
When you will ready to publish to your hosting:
meteor publish $(echo ""example"")

change example to any desired (and free) hostname in *.meteor.com domain
e.g. http://blockchainnow.meteor.com
Technology Stack

Bitcoin Core Bitcoin Core
Insight API A bitcoin blockchain API for web wallets
Apache Kafka A high-throughput distributed messaging system
kafka-node Node.js client with Zookeeper integration for Apache Kafka
Logstash Collect, Parse, Transform Logs
Elastic Search Search & Analyze Data in Real Time
Meteor The JavaScript App Platform
Meteor Easy Search Plugin for Meteor

Links

Blockchain Now ""Blockchain Now"" website
Slides
Kafka Manager
Elastic Sample Query

Important:
Please star Awesome Data Engineering repository
",blockchain now agenda endtoend pipelin for the bitcoin analyt reach ui with reactiv search and chart cours project for insight data engin program how to instal tldr git clone the repositori git clone gitgithubcomigorbarinovblockchainnowgit from meteorui start meteor server meteor open httplocalhost3000 in you favorit googl chrome browser when you will readi to publish to your host meteor publish echo exampl chang exampl to ani desir and free hostnam in meteorcom domain eg httpblockchainnowmeteorcom technolog stack bitcoin core bitcoin core insight api a bitcoin blockchain api for web wallet apach kafka a highthroughput distribut messag system kafkanod nodej client with zookeep integr for apach kafka logstash collect pars transform log elast search search analyz data in real time meteor the javascript app platform meteor easi search plugin for meteor link blockchain now blockchain now websit slide kafka manag elast sampl queri import pleas star awesom data engin repositori,blockchain now agenda endtoend pipeline for the bitcoin analytics reach ui with reactive search and chart course project for insight data engineering program how to install tldr git clone the repository git clone gitgithubcomigorbarinovblockchainnowgit from meteorui start meteor server meteor open httplocalhost3000 in you favorite google chrome browser when you will ready to publish to your hosting meteor publish echo example change example to any desired and free hostname in meteorcom domain eg httpblockchainnowmeteorcom technology stack bitcoin core bitcoin core insight api a bitcoin blockchain api for web wallet apache kafka a highthroughput distributed messaging system kafkanode nodejs client with zookeeper integration for apache kafka logstash collect parse transform log elastic search search analyze data in real time meteor the javascript app platform meteor easy search plugin for meteor link blockchain now blockchain now website slide kafka manager elastic sample query important please star awesome data engineering repository,blockchain agenda endtoend pipeline bitcoin analytics reach ui reactive search chart course project insight data engineering program install tldr git clone repository git clone gitgithubcomigorbarinovblockchainnowgit meteorui start meteor server meteor open httplocalhost3000 favorite google chrome browser ready publish hosting meteor publish echo example change example desired free hostname meteorcom domain eg httpblockchainnowmeteorcom technology stack bitcoin core bitcoin core insight api bitcoin blockchain api web wallet apache kafka highthroughput distributed messaging system kafkanode nodejs client zookeeper integration apache kafka logstash collect parse transform log elastic search search analyze data real time meteor javascript app platform meteor easy search plugin meteor link blockchain blockchain website slide kafka manager elastic sample query important please star awesome data engineering repository
JavaScript ,"onexi.github.io
Engineering Computation & Data Science -
",onexigithubio engin comput data scienc,onexigithubio engineering computation data science,onexigithubio engineering computation data science
JavaScript ,"Data-Engineering-Google-Drive-Data-Pipeline
Google Drive Data Pipeline Automatically extracting, transforming and loading data from your Google Drive into your preferred data warehouse on a regular interval (up to a minute).
This repo contains the main operators and the DAG to execute the Pipeline.
",dataengineeringgoogledrivedatapipelin googl drive data pipelin automat extract transform and load data from your googl drive into your prefer data warehous on a regular interv up to a minut thi repo contain the main oper and the dag to execut the pipelin,dataengineeringgoogledrivedatapipeline google drive data pipeline automatically extracting transforming and loading data from your google drive into your preferred data warehouse on a regular interval up to a minute this repo contains the main operator and the dag to execute the pipeline,dataengineeringgoogledrivedatapipeline google drive data pipeline automatically extracting transforming loading data google drive preferred data warehouse regular interval minute repo contains main operator dag execute pipeline
JavaScript ,"BuggyGuiAngularPort
Telemetry system for the UPRM Moonbuggy Engineering team!
Here we collect a lot of rover data in a Database and visualize it to show the judges
This project was generated with Angular CLI version 6.0.1.
Development server
Run ng serve for a dev server. Navigate to http://localhost:4200/. The app will automatically reload if you change any of the source files.
To test the back-end portion of the server run ng build first. Then run npm start. (You should have Nodemon installed on your system npm install -g nodemon). Navigate to http://localhost:3000/ and you can test the requests using this address. App should reload if you change any server files.
If there are changes in the angular files, you run ng build again to reload the server.
Code scaffolding
Run ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module.
Build
Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.
Running unit tests
Run ng test to execute the unit tests via Karma.
Running end-to-end tests
Run ng e2e to execute the end-to-end tests via Protractor.
Further help
To get more help on the Angular CLI use ng help or go check out the Angular CLI README.
",buggyguiangularport telemetri system for the uprm moonbuggi engin team here we collect a lot of rover data in a databas and visual it to show the judg thi project wa gener with angular cli version 601 develop server run ng serv for a dev server navig to httplocalhost4200 the app will automat reload if you chang ani of the sourc file to test the backend portion of the server run ng build first then run npm start you should have nodemon instal on your system npm instal g nodemon navig to httplocalhost3000 and you can test the request use thi address app should reload if you chang ani server file if there are chang in the angular file you run ng build again to reload the server code scaffold run ng gener compon componentnam to gener a new compon you can also use ng gener directivepipeserviceclassguardinterfaceenummodul build run ng build to build the project the build artifact will be store in the dist directori use the prod flag for a product build run unit test run ng test to execut the unit test via karma run endtoend test run ng e2e to execut the endtoend test via protractor further help to get more help on the angular cli use ng help or go check out the angular cli readm,buggyguiangularport telemetry system for the uprm moonbuggy engineering team here we collect a lot of rover data in a database and visualize it to show the judge this project wa generated with angular cli version 601 development server run ng serve for a dev server navigate to httplocalhost4200 the app will automatically reload if you change any of the source file to test the backend portion of the server run ng build first then run npm start you should have nodemon installed on your system npm install g nodemon navigate to httplocalhost3000 and you can test the request using this address app should reload if you change any server file if there are change in the angular file you run ng build again to reload the server code scaffolding run ng generate component componentname to generate a new component you can also use ng generate directivepipeserviceclassguardinterfaceenummodule build run ng build to build the project the build artifact will be stored in the dist directory use the prod flag for a production build running unit test run ng test to execute the unit test via karma running endtoend test run ng e2e to execute the endtoend test via protractor further help to get more help on the angular cli use ng help or go check out the angular cli readme,buggyguiangularport telemetry system uprm moonbuggy engineering team collect lot rover data database visualize show judge project generated angular cli version 601 development server run ng serve dev server navigate httplocalhost4200 app automatically reload change source file test backend portion server run ng build first run npm start nodemon installed system npm install g nodemon navigate httplocalhost3000 test request using address app reload change server file change angular file run ng build reload server code scaffolding run ng generate component componentname generate new component also use ng generate directivepipeserviceclassguardinterfaceenummodule build run ng build build project build artifact stored dist directory use prod flag production build running unit test run ng test execute unit test via karma running endtoend test run ng e2e execute endtoend test via protractor help get help angular cli use ng help go check angular cli readme
JavaScript ,"Pupil
Pupil is a tool for visualizing data from various software-engineering
tools. It works as a command-line tool run from a git-repository. When
run, pupil will gather data from the repository and the GitLab
REST-API and persist it to a configured ArangoDB instance.
Pupil then hosts interactive visualizations about the gathered data
via a web-interface.
Naming
""Pupil"" is the name used in this repository and also the name used in
the source code to refer to itself. In the accompanying master's
thesis, ""pupil"" is referred to as ""zivsed"". On the INSO projects page,
it is described as ""binocular"". Don't get confused, its all the same
thing - naming is hard ¯\_(ツ)_/¯.
Dependencies

node.js >= 8
ArangoDB (tested with 3.1.28)

Installation
Pupil is not yet published on the npm registry. To install it, you
should clone this repository and then link it:
$ git clone git@gitlab.com:romand/pupil.git
$ cd pupil
pupil$ npm link    # <- this will make the `pupil` executable available in your $PATH
Configuration
As pupil needs to access an ArangoDB instance, you have to configure
the database connection before you can use pupil. This can be done in
the global pupil-configuration file ~/.pupilrc. Additionally, the
configuration file also stores authentication data for the consumed
REST-APIs. The configuration file is read by the rc
module. Check its documentation to
see the supported formats. For the purpose of this README, we'll use
json.
Configuration options

gitlab: Object holding gitlab configuration options

url: The URL to the GitLab-Instance you want to connect to. Use the
base-url, not the API-URL (see the example)!
token: The personal access token generated by your GitLab user to
use for authentication (see
https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html)


github: Object holding github configuration options

auth: Can hold any options that the [github npm-module] can take, check its documentation.


arango: Object holding arangodb-configuration
host: Hostname
port: Port
user: username
password: password

A sample configuration file looks like this:
{
  ""gitlab"": {
    ""url"": ""https://gitlab.com/"",
    ""token"": ""YOUR_GITLAB_API_TOKEN""
  },
  ""github"": {
    ""auth"": {
      ""type"": ""basic"",
      ""username"": ""YOUR_GITLAB_USER"",
      ""password"": ""YOUR_GITLAB_PASSWORD""
    }
  },
  ""arango"": {
    ""host"": ""localhost"",
    ""port"": 8529,
    ""user"": ""YOUR_ARANGODB_USER"",
    ""password"": ""YOUR_ARANGODB_PASSWORD""
  }
}
You may override configuration options for specific projects by
placing another .pupilrc file in the project's root directory.
Usage
To run pupil, simply execute pupil from the repository you want to
run pupil on (you can try it on the pupil-repo itself!). Pupil will
try to guess reasonable defaults for configuration based on your
.git/config. A browser window should pop up automatically with
pupil's web-interface showing the indexing progress and the
visualizations.
Contributing
For an explanation of pupil's architecture, please see the Contribution
guidelines for this project
",pupil pupil is a tool for visual data from variou softwareengin tool it work as a commandlin tool run from a gitrepositori when run pupil will gather data from the repositori and the gitlab restapi and persist it to a configur arangodb instanc pupil then host interact visual about the gather data via a webinterfac name pupil is the name use in thi repositori and also the name use in the sourc code to refer to itself in the accompani master thesi pupil is refer to as zivs on the inso project page it is describ as binocular dont get confus it all the same thing name is hard __ depend nodej 8 arangodb test with 3128 instal pupil is not yet publish on the npm registri to instal it you should clone thi repositori and then link it git clone gitgitlabcomromandpupilgit cd pupil pupil npm link thi will make the pupil execut avail in your path configur as pupil need to access an arangodb instanc you have to configur the databas connect befor you can use pupil thi can be done in the global pupilconfigur file pupilrc addit the configur file also store authent data for the consum restapi the configur file is read by the rc modul check it document to see the support format for the purpos of thi readm well use json configur option gitlab object hold gitlab configur option url the url to the gitlabinst you want to connect to use the baseurl not the apiurl see the exampl token the person access token gener by your gitlab user to use for authent see httpsdocsgitlabcomeeuserprofilepersonal_access_tokenshtml github object hold github configur option auth can hold ani option that the github npmmodul can take check it document arango object hold arangodbconfigur host hostnam port port user usernam password password a sampl configur file look like thi gitlab url httpsgitlabcom token your_gitlab_api_token github auth type basic usernam your_gitlab_us password your_gitlab_password arango host localhost port 8529 user your_arangodb_us password your_arangodb_password you may overrid configur option for specif project by place anoth pupilrc file in the project root directori usag to run pupil simpli execut pupil from the repositori you want to run pupil on you can tri it on the pupilrepo itself pupil will tri to guess reason default for configur base on your gitconfig a browser window should pop up automat with pupil webinterfac show the index progress and the visual contribut for an explan of pupil architectur pleas see the contribut guidelin for thi project,pupil pupil is a tool for visualizing data from various softwareengineering tool it work a a commandline tool run from a gitrepository when run pupil will gather data from the repository and the gitlab restapi and persist it to a configured arangodb instance pupil then host interactive visualization about the gathered data via a webinterface naming pupil is the name used in this repository and also the name used in the source code to refer to itself in the accompanying master thesis pupil is referred to a zivsed on the inso project page it is described a binocular dont get confused it all the same thing naming is hard __ dependency nodejs 8 arangodb tested with 3128 installation pupil is not yet published on the npm registry to install it you should clone this repository and then link it git clone gitgitlabcomromandpupilgit cd pupil pupil npm link this will make the pupil executable available in your path configuration a pupil need to access an arangodb instance you have to configure the database connection before you can use pupil this can be done in the global pupilconfiguration file pupilrc additionally the configuration file also store authentication data for the consumed restapis the configuration file is read by the rc module check it documentation to see the supported format for the purpose of this readme well use json configuration option gitlab object holding gitlab configuration option url the url to the gitlabinstance you want to connect to use the baseurl not the apiurl see the example token the personal access token generated by your gitlab user to use for authentication see httpsdocsgitlabcomeeuserprofilepersonal_access_tokenshtml github object holding github configuration option auth can hold any option that the github npmmodule can take check it documentation arango object holding arangodbconfiguration host hostname port port user username password password a sample configuration file look like this gitlab url httpsgitlabcom token your_gitlab_api_token github auth type basic username your_gitlab_user password your_gitlab_password arango host localhost port 8529 user your_arangodb_user password your_arangodb_password you may override configuration option for specific project by placing another pupilrc file in the project root directory usage to run pupil simply execute pupil from the repository you want to run pupil on you can try it on the pupilrepo itself pupil will try to guess reasonable default for configuration based on your gitconfig a browser window should pop up automatically with pupil webinterface showing the indexing progress and the visualization contributing for an explanation of pupil architecture please see the contribution guideline for this project,pupil pupil tool visualizing data various softwareengineering tool work commandline tool run gitrepository run pupil gather data repository gitlab restapi persist configured arangodb instance pupil host interactive visualization gathered data via webinterface naming pupil name used repository also name used source code refer accompanying master thesis pupil referred zivsed inso project page described binocular dont get confused thing naming hard __ dependency nodejs 8 arangodb tested 3128 installation pupil yet published npm registry install clone repository link git clone gitgitlabcomromandpupilgit cd pupil pupil npm link make pupil executable available path configuration pupil need access arangodb instance configure database connection use pupil done global pupilconfiguration file pupilrc additionally configuration file also store authentication data consumed restapis configuration file read rc module check documentation see supported format purpose readme well use json configuration option gitlab object holding gitlab configuration option url url gitlabinstance want connect use baseurl apiurl see example token personal access token generated gitlab user use authentication see httpsdocsgitlabcomeeuserprofilepersonal_access_tokenshtml github object holding github configuration option auth hold option github npmmodule take check documentation arango object holding arangodbconfiguration host hostname port port user username password password sample configuration file look like gitlab url httpsgitlabcom token your_gitlab_api_token github auth type basic username your_gitlab_user password your_gitlab_password arango host localhost port 8529 user your_arangodb_user password your_arangodb_password may override configuration option specific project placing another pupilrc file project root directory usage run pupil simply execute pupil repository want run pupil try pupilrepo pupil try guess reasonable default configuration based gitconfig browser window pop automatically pupil webinterface showing indexing progress visualization contributing explanation pupil architecture please see contribution guideline project
JavaScript ,"HIT-DataManage
提交1.0
提交名为“first commit”
内容

添加了登录、注册方面的后台处理

提交1.1
提交名为“login complete rough”
内容

加入了登录身份验证的过滤器，能够过滤action和页面，未登录的用户无法访问目录下的资源且会重定向到登录界面;
与前台的登录和注册页面合并

提交1.2
提交名为“filter improved”
内容

完善了过滤器，当用户输入不存在的action时，跳转到默认action

提交1.3
提交名为“homepage”
内容

实现了登录后显示的主页

提交1.4
提交名为“addPDO&Event”
内容

实现了添加PDO的功能，添加事件(Event)还在继续完善
第二天应该能实现添加事件功能

提交1.5
提交名为“completeAdd”
内容

实现了添加事件功能

提交1.6
提交名为“completeDelete”
内容

实现了删除PDA和Event操作

提交1.7
提交名为“excelimport”
内容

实现了excel文件的导入操作

提交1.8
提交名为“first_merge_master”
内容

第一次合并总分支

提交1.9
提交名为“fix some problems”
内容

实现excel导入，事件删除，pdo删除

提交2.0
提交名为“complete logout”
内容

实现登出功能

提交2.1
提交名为“complete some functions”
内容

和前端合并

提交2.2
提交名为“charset problems”
内容

主要是尝试解决excel导入时候乱码以及数据库乱码问题

提交2.3
提交名为“fix data format problem on both xls and xlsx”
内容

终于解决了xls和xlsx文件中所有日期格式处理的问题

提交2.4
提交名为“complete events counts”
内容

实现对event个数的计数问题

提交2.5
提交名为“merge with some pages”
内容

和前端merge，实现搜索维度的PDO添加

提交2.6
提交名为“just left search”
内容

基本功能除搜索都实现了

提交2.7
提交名为“complete all functions”
内容

后台完成所有基本功能

提交2.8
提交名为“improve the filter”
内容

完善了过滤器

提交2.9
提交名为“ready to middle-discuss”
内容

和前端无缝连接，完成所有基本功能，准备中期答辩

提交3.0
提交名为“fix a bg bug”
内容

查询时关联数据出错，后台及时修改了

提交3.1
提交名为“add a sum-up model”
内容

在主页添加了一些统计信息

提交3.2
提交名为“fix some bugs”
内容

解决了一系列bug:

注册页面：密码多次匹配，输入不能为空
添加事件页面：若有时间属性，改为日历



",hitdatamanag 10 first commit 11 login complet rough action 12 filter improv actionact 13 homepag 14 addpdoev pdoevent 15 completeadd 16 completedelet pdaevent 17 excelimport excel 18 first_merge_mast 19 fix some problem excelpdo 20 complet logout 21 complet some function 22 charset problem excel 23 fix data format problem on both xl and xlsx xlsxlsx 24 complet event count event 25 merg with some page mergepdo 26 just left search 27 complet all function 28 improv the filter 29 readi to middlediscuss 30 fix a bg bug 31 add a sumup model 32 fix some bug bug,hitdatamanage 10 first commit 11 login complete rough action 12 filter improved actionaction 13 homepage 14 addpdoevent pdoevent 15 completeadd 16 completedelete pdaevent 17 excelimport excel 18 first_merge_master 19 fix some problem excelpdo 20 complete logout 21 complete some function 22 charset problem excel 23 fix data format problem on both xl and xlsx xlsxlsx 24 complete event count event 25 merge with some page mergepdo 26 just left search 27 complete all function 28 improve the filter 29 ready to middlediscuss 30 fix a bg bug 31 add a sumup model 32 fix some bug bug,hitdatamanage 10 first commit 11 login complete rough action 12 filter improved actionaction 13 homepage 14 addpdoevent pdoevent 15 completeadd 16 completedelete pdaevent 17 excelimport excel 18 first_merge_master 19 fix problem excelpdo 20 complete logout 21 complete function 22 charset problem excel 23 fix data format problem xl xlsx xlsxlsx 24 complete event count event 25 merge page mergepdo 26 left search 27 complete function 28 improve filter 29 ready middlediscuss 30 fix bg bug 31 add sumup model 32 fix bug bug
JavaScript ,"






An exciting game of programming and Artificial Intelligence

















In WarriorJS, you are a warrior climbing a tall tower to reach The JavaScript
Sword at the top level. Legend has it that the sword bearer becomes enlightened
in the JavaScript language, but be warned: the journey will not be easy. On each
floor, you need to write JavaScript to instruct the warrior to battle enemies,
rescue captives, and reach the stairs alive...
No matter if you are new to programming or a JavaScript guru, WarriorJS will
put your skills to the test. Will you dare?
Play
Go to warriorjs.com and play from the comfort
of your browser! Sharpen your skills and compete against other players around
the globe. Good luck in your journey, warrior!
Documentation
Although there is some in-game documentation, at some point you may want to
visit the official docs.
Jump straight to some of the most-visited pages:

Gameplay
Towers
Player API

CLI
Wanna play offline? No problem, just follow these steps:

Install WarriorJS CLI with npm:

npm install --global @warriorjs/cli

Launch the game:

warriorjs


Create your warrior.


You'll be pointed to a README file with instructions for the first level.


Check out the Install docs for
more details.
Preview


WarriorJS CLI launched from the
Integrated Terminal
in VS Code. To the left, Player.js, and to
the right, a
Markdown Preview
of README.md.

Contributing
We welcome contributions to WarriorJS! These are the many ways you can help:

Submit patches and features
Make towers (new levels for the
game)
Improve the documentation and website
Report bugs
Follow us on Twitter
Participate in the Spectrum community
And donate financially!

Please read our contribution guide to get started. Also note
that this project is released with a
Contributor Code of Conduct, please make sure to review
and follow it.
Contributors
Thanks goes to each one of our contributors! 🙏
Become a contributor.

Backers
Support us with a monthly donation and help us continue our activities!
Become a backer.

Sponsors
Become a sponsor and get your logo here and on the
official docs!
Become a sponsor.

Acknowledgments
This project was born as a port of
ruby-warrior. Credits for the original
idea go to Ryan Bates.
Special thanks to Guillermo Cura for designing a
wonderful logo.
License
WarriorJS is licensed under a MIT License.
",an excit game of program and artifici intellig in warriorj you are a warrior climb a tall tower to reach the javascript sword at the top level legend ha it that the sword bearer becom enlighten in the javascript languag but be warn the journey will not be easi on each floor you need to write javascript to instruct the warrior to battl enemi rescu captiv and reach the stair aliv no matter if you are new to program or a javascript guru warriorj will put your skill to the test will you dare play go to warriorjscom and play from the comfort of your browser sharpen your skill and compet against other player around the globe good luck in your journey warrior document although there is some ingam document at some point you may want to visit the offici doc jump straight to some of the mostvisit page gameplay tower player api cli wanna play offlin no problem just follow these step instal warriorj cli with npm npm instal global warriorjscli launch the game warriorj creat your warrior youll be point to a readm file with instruct for the first level check out the instal doc for more detail preview warriorj cli launch from the integr termin in vs code to the left playerj and to the right a markdown preview of readmemd contribut we welcom contribut to warriorj these are the mani way you can help submit patch and featur make tower new level for the game improv the document and websit report bug follow us on twitter particip in the spectrum commun and donat financi pleas read our contribut guid to get start also note that thi project is releas with a contributor code of conduct pleas make sure to review and follow it contributor thank goe to each one of our contributor becom a contributor backer support us with a monthli donat and help us continu our activ becom a backer sponsor becom a sponsor and get your logo here and on the offici doc becom a sponsor acknowledg thi project wa born as a port of rubywarrior credit for the origin idea go to ryan bate special thank to guillermo cura for design a wonder logo licens warriorj is licens under a mit licens,an exciting game of programming and artificial intelligence in warriorjs you are a warrior climbing a tall tower to reach the javascript sword at the top level legend ha it that the sword bearer becomes enlightened in the javascript language but be warned the journey will not be easy on each floor you need to write javascript to instruct the warrior to battle enemy rescue captive and reach the stair alive no matter if you are new to programming or a javascript guru warriorjs will put your skill to the test will you dare play go to warriorjscom and play from the comfort of your browser sharpen your skill and compete against other player around the globe good luck in your journey warrior documentation although there is some ingame documentation at some point you may want to visit the official doc jump straight to some of the mostvisited page gameplay tower player api cli wanna play offline no problem just follow these step install warriorjs cli with npm npm install global warriorjscli launch the game warriorjs create your warrior youll be pointed to a readme file with instruction for the first level check out the install doc for more detail preview warriorjs cli launched from the integrated terminal in v code to the left playerjs and to the right a markdown preview of readmemd contributing we welcome contribution to warriorjs these are the many way you can help submit patch and feature make tower new level for the game improve the documentation and website report bug follow u on twitter participate in the spectrum community and donate financially please read our contribution guide to get started also note that this project is released with a contributor code of conduct please make sure to review and follow it contributor thanks go to each one of our contributor become a contributor backer support u with a monthly donation and help u continue our activity become a backer sponsor become a sponsor and get your logo here and on the official doc become a sponsor acknowledgment this project wa born a a port of rubywarrior credit for the original idea go to ryan bates special thanks to guillermo cura for designing a wonderful logo license warriorjs is licensed under a mit license,exciting game programming artificial intelligence warriorjs warrior climbing tall tower reach javascript sword top level legend sword bearer becomes enlightened javascript language warned journey easy floor need write javascript instruct warrior battle enemy rescue captive reach stair alive matter new programming javascript guru warriorjs put skill test dare play go warriorjscom play comfort browser sharpen skill compete player around globe good luck journey warrior documentation although ingame documentation point may want visit official doc jump straight mostvisited page gameplay tower player api cli wanna play offline problem follow step install warriorjs cli npm npm install global warriorjscli launch game warriorjs create warrior youll pointed readme file instruction first level check install doc detail preview warriorjs cli launched integrated terminal v code left playerjs right markdown preview readmemd contributing welcome contribution warriorjs many way help submit patch feature make tower new level game improve documentation website report bug follow u twitter participate spectrum community donate financially please read contribution guide get started also note project released contributor code conduct please make sure review follow contributor thanks go one contributor become contributor backer support u monthly donation help u continue activity become backer sponsor become sponsor get logo official doc become sponsor acknowledgment project born port rubywarrior credit original idea go ryan bates special thanks guillermo cura designing wonderful logo license warriorjs licensed mit license
JavaScript ,"TooAngel Artificial intelligence for screeps






https://screeps.com/
See rendered version:
http://tooangel.github.io/screeps/
This repository is World Driven. Pull Requests
are automatically merged and deployed to the
Screeps TooAngel account.
Quests
Head over to Quests
For in game room visitors:
Happy to see you visiting one of our rooms. Visit FAQ to find answers
Info
This is the AI I'm using for screeps. I managed to reach Top 10
from November 2015 - March 2016. Main Goal is to automate everything, no
manual interaction needed.
The AI is deployable on a private screeps server, follow the information on
Steam or
npm install screeps-bot-tooangel and bots.spawn('screeps-bot-tooangel', ROOMNAME)
Note
This is not a good example for code quality or structure, most LOCs written
while fighting or other occasions which needed quick fixes or in the ingame
editor (getting better :-)). But I think there are a couple of funny ideas.
Every contribution is welcome.
Features

Automatic Base building
External room harvesting
Basic mineral handling
Power harvesting
New rooms claiming on GCL level up
Automatic attack
Rebuild of fallen rooms
Layout visualization
Manual commands
Graphs
Testing

Tweaking
Add a src/friends.js with player names to ignore them from all attack
considerations.
E.g.:
module.exports = ['TooAngel'];
Add a src/config_local.js to overwrite configuration values. Copy
config_local.js.example to src/config_local.js as an example. src/config.js
has the default values.
Debugging
Within the config_local.js certain config.debug flags can be enabled.
To add debug messages Room.debugLog(TYPE, MESSAGE) and
Creep.creepLog(MESSAGE) are suggested. Especially the creepLog allows
granular output of the creep behavior based on the room and the creep role.
Upload
install dependencies
npm install

add your account credentials
to screeps.com
To deploy to the live server provide the credentials.
via env
export email=EMAIL
export password=PASSWORD

via git ignored file
echo ""module.exports = { email: 'your-email@here.tld', password: 'your-secret' };"" > account.screeps.com.js

or edit and rename account.screeps.com.js.sample to account.screeps.com.js
And deploy to the server:
grunt

to private server
Create a .localSync.js file with content:
module.exports = [{
  cwd: 'src',
  src: [
    '*.js'
  ],
  dest: '$HOME/.config/Screeps/scripts/SERVER/default',
}];

grunt local

Develop
grunt dev

Release
Releasing to npm is done automatically by increasing the version and merging to master.
npm version 10.0.1
git push --follow-tags

Every deploy to master is automatically deployed to the live tooangel account.
Testing
node utils/test.js will start a private server and add some bots as test
cases. Within in the tmp-test-server directory the server can be easily
started via screeps start.
Design
More details of the AI design
",tooangel artifici intellig for screep httpsscreepscom see render version httptooangelgithubioscreep thi repositori is world driven pull request are automat merg and deploy to the screep tooangel account quest head over to quest for in game room visitor happi to see you visit one of our room visit faq to find answer info thi is the ai im use for screep i manag to reach top 10 from novemb 2015 march 2016 main goal is to autom everyth no manual interact need the ai is deploy on a privat screep server follow the inform on steam or npm instal screepsbottooangel and botsspawnscreepsbottooangel roomnam note thi is not a good exampl for code qualiti or structur most loc written while fight or other occas which need quick fix or in the ingam editor get better but i think there are a coupl of funni idea everi contribut is welcom featur automat base build extern room harvest basic miner handl power harvest new room claim on gcl level up automat attack rebuild of fallen room layout visual manual command graph test tweak add a srcfriendsj with player name to ignor them from all attack consider eg moduleexport tooangel add a srcconfig_localj to overwrit configur valu copi config_localjsexampl to srcconfig_localj as an exampl srcconfigj ha the default valu debug within the config_localj certain configdebug flag can be enabl to add debug messag roomdebuglogtyp messag and creepcreeplogmessag are suggest especi the creeplog allow granular output of the creep behavior base on the room and the creep role upload instal depend npm instal add your account credenti to screepscom to deploy to the live server provid the credenti via env export emailemail export passwordpassword via git ignor file echo moduleexport email youremailheretld password yoursecret accountscreepscomj or edit and renam accountscreepscomjssampl to accountscreepscomj and deploy to the server grunt to privat server creat a localsyncj file with content moduleexport cwd src src js dest homeconfigscreepsscriptsserverdefault grunt local develop grunt dev releas releas to npm is done automat by increas the version and merg to master npm version 1001 git push followtag everi deploy to master is automat deploy to the live tooangel account test node utilstestj will start a privat server and add some bot as test case within in the tmptestserv directori the server can be easili start via screep start design more detail of the ai design,tooangel artificial intelligence for screeps httpsscreepscom see rendered version httptooangelgithubioscreeps this repository is world driven pull request are automatically merged and deployed to the screeps tooangel account quest head over to quest for in game room visitor happy to see you visiting one of our room visit faq to find answer info this is the ai im using for screeps i managed to reach top 10 from november 2015 march 2016 main goal is to automate everything no manual interaction needed the ai is deployable on a private screeps server follow the information on steam or npm install screepsbottooangel and botsspawnscreepsbottooangel roomname note this is not a good example for code quality or structure most locs written while fighting or other occasion which needed quick fix or in the ingame editor getting better but i think there are a couple of funny idea every contribution is welcome feature automatic base building external room harvesting basic mineral handling power harvesting new room claiming on gcl level up automatic attack rebuild of fallen room layout visualization manual command graph testing tweaking add a srcfriendsjs with player name to ignore them from all attack consideration eg moduleexports tooangel add a srcconfig_localjs to overwrite configuration value copy config_localjsexample to srcconfig_localjs a an example srcconfigjs ha the default value debugging within the config_localjs certain configdebug flag can be enabled to add debug message roomdebuglogtype message and creepcreeplogmessage are suggested especially the creeplog allows granular output of the creep behavior based on the room and the creep role upload install dependency npm install add your account credential to screepscom to deploy to the live server provide the credential via env export emailemail export passwordpassword via git ignored file echo moduleexports email youremailheretld password yoursecret accountscreepscomjs or edit and rename accountscreepscomjssample to accountscreepscomjs and deploy to the server grunt to private server create a localsyncjs file with content moduleexports cwd src src j dest homeconfigscreepsscriptsserverdefault grunt local develop grunt dev release releasing to npm is done automatically by increasing the version and merging to master npm version 1001 git push followtags every deploy to master is automatically deployed to the live tooangel account testing node utilstestjs will start a private server and add some bot a test case within in the tmptestserver directory the server can be easily started via screeps start design more detail of the ai design,tooangel artificial intelligence screeps httpsscreepscom see rendered version httptooangelgithubioscreeps repository world driven pull request automatically merged deployed screeps tooangel account quest head quest game room visitor happy see visiting one room visit faq find answer info ai im using screeps managed reach top 10 november 2015 march 2016 main goal automate everything manual interaction needed ai deployable private screeps server follow information steam npm install screepsbottooangel botsspawnscreepsbottooangel roomname note good example code quality structure locs written fighting occasion needed quick fix ingame editor getting better think couple funny idea every contribution welcome feature automatic base building external room harvesting basic mineral handling power harvesting new room claiming gcl level automatic attack rebuild fallen room layout visualization manual command graph testing tweaking add srcfriendsjs player name ignore attack consideration eg moduleexports tooangel add srcconfig_localjs overwrite configuration value copy config_localjsexample srcconfig_localjs example srcconfigjs default value debugging within config_localjs certain configdebug flag enabled add debug message roomdebuglogtype message creepcreeplogmessage suggested especially creeplog allows granular output creep behavior based room creep role upload install dependency npm install add account credential screepscom deploy live server provide credential via env export emailemail export passwordpassword via git ignored file echo moduleexports email youremailheretld password yoursecret accountscreepscomjs edit rename accountscreepscomjssample accountscreepscomjs deploy server grunt private server create localsyncjs file content moduleexports cwd src src j dest homeconfigscreepsscriptsserverdefault grunt local develop grunt dev release releasing npm done automatically increasing version merging master npm version 1001 git push followtags every deploy master automatically deployed live tooangel account testing node utilstestjs start private server add bot test case within tmptestserver directory server easily started via screeps start design detail ai design
JavaScript ,"



i.am.aiAI Expert Roadmap
Roadmap to becoming an Artificial Intelligence Expert in 2020









Below you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an ai expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.
If you are interested to become an AI EXPERT at AMAI in Germany, or you want to hire an AI Expert, please say hi@am.ai.
Note
👉 An interactive version with links to follow about each bullet of the list can be found at i.am.ai/roadmap 👈
To receive updates star ⭐ and watch 👀 the GitHub Repo to get notified, when we add new content to stay on the top of the most recent research.
Disclaimer
The purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better suited for some cases than the other and remember hip and trendy never means best suited for the job.
Introduction





Data Science Roadmap





Machine Learning Roadmap





Deep Learning Roadmap





Data Engineer Roadmap





Big Data Engineer Roadmap





🚦 Wrap Up
If you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, we will continue to improve this, so you might want to watch/star this repository to revisit.
🙌 Contribution

Have a look at the contribution docs for how to update any of the roadmaps


Open pull request with improvements
Discuss ideas in issues
Spread the word
Reach out with any feedback

Supported By


",iamaiai expert roadmap roadmap to becom an artifici intellig expert in 2020 below you find a set of chart demonstr the path that you can take and the technolog that you would want to adopt in order to becom a data scientist machin learn or an ai expert we made these chart for our new employe to make them ai expert but we want to share them here to help the commun if you are interest to becom an ai expert at amai in germani or you want to hire an ai expert pleas say hiamai note an interact version with link to follow about each bullet of the list can be found at iamairoadmap to receiv updat star and watch the github repo to get notifi when we add new content to stay on the top of the most recent research disclaim the purpos of these roadmap is to give you an idea about the landscap and to guid you if you are confus about what to learn next and not to encourag you to pick what is hip and trendi you should grow some understand of whi one tool would better suit for some case than the other and rememb hip and trendi never mean best suit for the job introduct data scienc roadmap machin learn roadmap deep learn roadmap data engin roadmap big data engin roadmap wrap up if you think ani of the roadmap can be improv pleas do open a pr with ani updat and submit ani issu also we will continu to improv thi so you might want to watchstar thi repositori to revisit contribut have a look at the contribut doc for how to updat ani of the roadmap open pull request with improv discuss idea in issu spread the word reach out with ani feedback support by,iamaiai expert roadmap roadmap to becoming an artificial intelligence expert in 2020 below you find a set of chart demonstrating the path that you can take and the technology that you would want to adopt in order to become a data scientist machine learning or an ai expert we made these chart for our new employee to make them ai expert but we wanted to share them here to help the community if you are interested to become an ai expert at amai in germany or you want to hire an ai expert please say hiamai note an interactive version with link to follow about each bullet of the list can be found at iamairoadmap to receive update star and watch the github repo to get notified when we add new content to stay on the top of the most recent research disclaimer the purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy you should grow some understanding of why one tool would better suited for some case than the other and remember hip and trendy never mean best suited for the job introduction data science roadmap machine learning roadmap deep learning roadmap data engineer roadmap big data engineer roadmap wrap up if you think any of the roadmaps can be improved please do open a pr with any update and submit any issue also we will continue to improve this so you might want to watchstar this repository to revisit contribution have a look at the contribution doc for how to update any of the roadmaps open pull request with improvement discus idea in issue spread the word reach out with any feedback supported by,iamaiai expert roadmap roadmap becoming artificial intelligence expert 2020 find set chart demonstrating path take technology would want adopt order become data scientist machine learning ai expert made chart new employee make ai expert wanted share help community interested become ai expert amai germany want hire ai expert please say hiamai note interactive version link follow bullet list found iamairoadmap receive update star watch github repo get notified add new content stay top recent research disclaimer purpose roadmaps give idea landscape guide confused learn next encourage pick hip trendy grow understanding one tool would better suited case remember hip trendy never mean best suited job introduction data science roadmap machine learning roadmap deep learning roadmap data engineer roadmap big data engineer roadmap wrap think roadmaps improved please open pr update submit issue also continue improve might want watchstar repository revisit contribution look contribution doc update roadmaps open pull request improvement discus idea issue spread word reach feedback supported
JavaScript ,"



i.am.aiAI Expert Roadmap
Roadmap to becoming an Artificial Intelligence Expert in 2020









Below you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an ai expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.
If you are interested to become an AI EXPERT at AMAI in Germany, or you want to hire an AI Expert, please say hi@am.ai.
Note
👉 An interactive version with links to follow about each bullet of the list can be found at i.am.ai/roadmap 👈
To receive updates star ⭐ and watch 👀 the GitHub Repo to get notified, when we add new content to stay on the top of the most recent research.
Disclaimer
The purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better suited for some cases than the other and remember hip and trendy never means best suited for the job.
Introduction





Data Science Roadmap





Machine Learning Roadmap





Deep Learning Roadmap





Data Engineer Roadmap





Big Data Engineer Roadmap





🚦 Wrap Up
If you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, we will continue to improve this, so you might want to watch/star this repository to revisit.
🙌 Contribution

Have a look at the contribution docs for how to update any of the roadmaps


Open pull request with improvements
Discuss ideas in issues
Spread the word
Reach out with any feedback

Supported By


",iamaiai expert roadmap roadmap to becom an artifici intellig expert in 2020 below you find a set of chart demonstr the path that you can take and the technolog that you would want to adopt in order to becom a data scientist machin learn or an ai expert we made these chart for our new employe to make them ai expert but we want to share them here to help the commun if you are interest to becom an ai expert at amai in germani or you want to hire an ai expert pleas say hiamai note an interact version with link to follow about each bullet of the list can be found at iamairoadmap to receiv updat star and watch the github repo to get notifi when we add new content to stay on the top of the most recent research disclaim the purpos of these roadmap is to give you an idea about the landscap and to guid you if you are confus about what to learn next and not to encourag you to pick what is hip and trendi you should grow some understand of whi one tool would better suit for some case than the other and rememb hip and trendi never mean best suit for the job introduct data scienc roadmap machin learn roadmap deep learn roadmap data engin roadmap big data engin roadmap wrap up if you think ani of the roadmap can be improv pleas do open a pr with ani updat and submit ani issu also we will continu to improv thi so you might want to watchstar thi repositori to revisit contribut have a look at the contribut doc for how to updat ani of the roadmap open pull request with improv discuss idea in issu spread the word reach out with ani feedback support by,iamaiai expert roadmap roadmap to becoming an artificial intelligence expert in 2020 below you find a set of chart demonstrating the path that you can take and the technology that you would want to adopt in order to become a data scientist machine learning or an ai expert we made these chart for our new employee to make them ai expert but we wanted to share them here to help the community if you are interested to become an ai expert at amai in germany or you want to hire an ai expert please say hiamai note an interactive version with link to follow about each bullet of the list can be found at iamairoadmap to receive update star and watch the github repo to get notified when we add new content to stay on the top of the most recent research disclaimer the purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy you should grow some understanding of why one tool would better suited for some case than the other and remember hip and trendy never mean best suited for the job introduction data science roadmap machine learning roadmap deep learning roadmap data engineer roadmap big data engineer roadmap wrap up if you think any of the roadmaps can be improved please do open a pr with any update and submit any issue also we will continue to improve this so you might want to watchstar this repository to revisit contribution have a look at the contribution doc for how to update any of the roadmaps open pull request with improvement discus idea in issue spread the word reach out with any feedback supported by,iamaiai expert roadmap roadmap becoming artificial intelligence expert 2020 find set chart demonstrating path take technology would want adopt order become data scientist machine learning ai expert made chart new employee make ai expert wanted share help community interested become ai expert amai germany want hire ai expert please say hiamai note interactive version link follow bullet list found iamairoadmap receive update star watch github repo get notified add new content stay top recent research disclaimer purpose roadmaps give idea landscape guide confused learn next encourage pick hip trendy grow understanding one tool would better suited case remember hip trendy never mean best suited job introduction data science roadmap machine learning roadmap deep learning roadmap data engineer roadmap big data engineer roadmap wrap think roadmaps improved please open pr update submit issue also continue improve might want watchstar repository revisit contribution look contribution doc update roadmaps open pull request improvement discus idea issue spread word reach feedback supported
JavaScript ,"Four legendary heroes were fighting for the land of Vindinium
Making their way in the dangerous woods
Slashing goblins and stealing gold mines
And looking for a tavern where to drink their gold
Warning
The vindinium dot org website has been discontinued, and the domain now belongs to Internet parasites.
Installation
Feel free to install a local instance for your private tournaments.
You need sbt, a MongoDB instance running, and a Unix machine (only Linux has been tested, tho).
git clone git://github.com/ornicar/vindinium
cd vindinium
cd client
./build.sh
cd ..
sbt compile
sbt run
Vindinium is now running on http://localhost:9000.
Optional reverse proxy
Here's an exemple of nginx configuration:
server {
 listen 80;
 server_name my-domain.org;

  location / {
    proxy_http_version 1.1;
    proxy_read_timeout 24h;
    proxy_set_header Host $host;
    proxy_pass  http://127.0.0.1:9000/;
  }
}

Developing on the Client Side stack
while the Server runs with a sbt run, you can go in another terminal in the client/ folder and:

Install once the dependencies with npm install (This requires nodejs to be installed)
Be sure to have grunt installed with npm install -g grunt-cli
Use grunt to compile client sources and watch for client source changes.

Credits
Kudos to:

vjousse for the UI and testing
veloce for the JavaScript and testing
gre for the shiny new JS playground

",four legendari hero were fight for the land of vindinium make their way in the danger wood slash goblin and steal gold mine and look for a tavern where to drink their gold warn the vindinium dot org websit ha been discontinu and the domain now belong to internet parasit instal feel free to instal a local instanc for your privat tournament you need sbt a mongodb instanc run and a unix machin onli linux ha been test tho git clone gitgithubcomornicarvindinium cd vindinium cd client buildsh cd sbt compil sbt run vindinium is now run on httplocalhost9000 option revers proxi here an exempl of nginx configur server listen 80 server_nam mydomainorg locat proxy_http_vers 11 proxy_read_timeout 24h proxy_set_head host host proxy_pass http1270019000 develop on the client side stack while the server run with a sbt run you can go in anoth termin in the client folder and instal onc the depend with npm instal thi requir nodej to be instal be sure to have grunt instal with npm instal g gruntcli use grunt to compil client sourc and watch for client sourc chang credit kudo to vjouss for the ui and test veloc for the javascript and test gre for the shini new js playground,four legendary hero were fighting for the land of vindinium making their way in the dangerous wood slashing goblin and stealing gold mine and looking for a tavern where to drink their gold warning the vindinium dot org website ha been discontinued and the domain now belongs to internet parasite installation feel free to install a local instance for your private tournament you need sbt a mongodb instance running and a unix machine only linux ha been tested tho git clone gitgithubcomornicarvindinium cd vindinium cd client buildsh cd sbt compile sbt run vindinium is now running on httplocalhost9000 optional reverse proxy here an exemple of nginx configuration server listen 80 server_name mydomainorg location proxy_http_version 11 proxy_read_timeout 24h proxy_set_header host host proxy_pass http1270019000 developing on the client side stack while the server run with a sbt run you can go in another terminal in the client folder and install once the dependency with npm install this requires nodejs to be installed be sure to have grunt installed with npm install g gruntcli use grunt to compile client source and watch for client source change credit kudos to vjousse for the ui and testing veloce for the javascript and testing gre for the shiny new j playground,four legendary hero fighting land vindinium making way dangerous wood slashing goblin stealing gold mine looking tavern drink gold warning vindinium dot org website discontinued domain belongs internet parasite installation feel free install local instance private tournament need sbt mongodb instance running unix machine linux tested tho git clone gitgithubcomornicarvindinium cd vindinium cd client buildsh cd sbt compile sbt run vindinium running httplocalhost9000 optional reverse proxy here exemple nginx configuration server listen 80 server_name mydomainorg location proxy_http_version 11 proxy_read_timeout 24h proxy_set_header host host proxy_pass http1270019000 developing client side stack server run sbt run go another terminal client folder install dependency npm install requires nodejs installed sure grunt installed npm install g gruntcli use grunt compile client source watch client source change credit kudos vjousse ui testing veloce javascript testing gre shiny new j playground
JavaScript ,"Halite



Halite is a AI programming competition. Contestants write bots to play an original multi-player turn-based strategy game played on a rectangular grid. For more information about the game, visit our website.
Contributing
See the Contributing Guide.
Questions
See the Forums and our Discord chat.
Authors
Halite I was conceived of and developed by Ben Spector and Michael Truell in 2016. Two Sigma, having had a history of playful programming challenges for its mathematical and software-oriented teams (e.g., see the Robotic Air Hockey Competition) retained Ben and Michael as 2016 summer interns to develop Halite, run an internal Halite Challenge, and ultimately open Halite up to human and non-human coding enthusiasts worldwide. Halite I was a great success, developing a flourishing community of bot builders from around the globe, representing 35+ universities and 20+ organizations.
As a result of the community’s enthusiasm, the Two Sigma team decided to create Halite II, an iteration of the original game with new rules but a similar structure and philosophy. With Ben and Michael as creative advisors, Halite II was developed by David Li, Jaques Clapauch, Harikrishna Menon, Julia Kastner as an evolution of Halite I.
The team considered simply reviving Halite I, but given the progress the community made and the number of open source bots that had been published, the team decided to create Halite II with new game mechanics and a fun background story that fleshes out the Halite universe. Halite involved moving pieces around a board with only up-down-left-right options. In 2017’s Halite II, bots battle for control of a virtual continuous universe, where ships mine planets to grow larger fleets and defeat their opponents.
",halit halit is a ai program competit contest write bot to play an origin multiplay turnbas strategi game play on a rectangular grid for more inform about the game visit our websit contribut see the contribut guid question see the forum and our discord chat author halit i wa conceiv of and develop by ben spector and michael truell in 2016 two sigma have had a histori of play program challeng for it mathemat and softwareori team eg see the robot air hockey competit retain ben and michael as 2016 summer intern to develop halit run an intern halit challeng and ultim open halit up to human and nonhuman code enthusiast worldwid halit i wa a great success develop a flourish commun of bot builder from around the globe repres 35 univers and 20 organ as a result of the commun enthusiasm the two sigma team decid to creat halit ii an iter of the origin game with new rule but a similar structur and philosophi with ben and michael as creativ advisor halit ii wa develop by david li jaqu clapauch harikrishna menon julia kastner as an evolut of halit i the team consid simpli reviv halit i but given the progress the commun made and the number of open sourc bot that had been publish the team decid to creat halit ii with new game mechan and a fun background stori that flesh out the halit univers halit involv move piec around a board with onli updownleftright option in 2017 halit ii bot battl for control of a virtual continu univers where ship mine planet to grow larger fleet and defeat their oppon,halite halite is a ai programming competition contestant write bot to play an original multiplayer turnbased strategy game played on a rectangular grid for more information about the game visit our website contributing see the contributing guide question see the forum and our discord chat author halite i wa conceived of and developed by ben spector and michael truell in 2016 two sigma having had a history of playful programming challenge for it mathematical and softwareoriented team eg see the robotic air hockey competition retained ben and michael a 2016 summer intern to develop halite run an internal halite challenge and ultimately open halite up to human and nonhuman coding enthusiast worldwide halite i wa a great success developing a flourishing community of bot builder from around the globe representing 35 university and 20 organization a a result of the community enthusiasm the two sigma team decided to create halite ii an iteration of the original game with new rule but a similar structure and philosophy with ben and michael a creative advisor halite ii wa developed by david li jaques clapauch harikrishna menon julia kastner a an evolution of halite i the team considered simply reviving halite i but given the progress the community made and the number of open source bot that had been published the team decided to create halite ii with new game mechanic and a fun background story that flesh out the halite universe halite involved moving piece around a board with only updownleftright option in 2017s halite ii bot battle for control of a virtual continuous universe where ship mine planet to grow larger fleet and defeat their opponent,halite halite ai programming competition contestant write bot play original multiplayer turnbased strategy game played rectangular grid information game visit website contributing see contributing guide question see forum discord chat author halite conceived developed ben spector michael truell 2016 two sigma history playful programming challenge mathematical softwareoriented team eg see robotic air hockey competition retained ben michael 2016 summer intern develop halite run internal halite challenge ultimately open halite human nonhuman coding enthusiast worldwide halite great success developing flourishing community bot builder around globe representing 35 university 20 organization result community enthusiasm two sigma team decided create halite ii iteration original game new rule similar structure philosophy ben michael creative advisor halite ii developed david li jaques clapauch harikrishna menon julia kastner evolution halite team considered simply reviving halite given progress community made number open source bot published team decided create halite ii new game mechanic fun background story flesh halite universe halite involved moving piece around board updownleftright option 2017s halite ii bot battle control virtual continuous universe ship mine planet grow larger fleet defeat opponent
JavaScript ,"aima-javascript
Visualization of concepts from Russell And Norvig's ""Artificial Intelligence — A Modern Approach"", and Javascript code for algorithms. Unlike aima-python, aima-java, and other sibling projects, this project is primarily about the visualizations and secondarily about the code.

View the visualizations
How to contribute
Chat on Gitter

Some Javascript visualizations that I admire, and would like to see similar kinds of things here:

Red Blob Games: A* Tutorial
Qiao Search Demo
Nicky Case: Simulating the World
Rafael Matsunaga: Genetic Car Thingy
Lee Yiyuan: 2048 Bot

",aimajavascript visual of concept from russel and norvig artifici intellig a modern approach and javascript code for algorithm unlik aimapython aimajava and other sibl project thi project is primarili about the visual and secondarili about the code view the visual how to contribut chat on gitter some javascript visual that i admir and would like to see similar kind of thing here red blob game a tutori qiao search demo nicki case simul the world rafael matsunaga genet car thingi lee yiyuan 2048 bot,aimajavascript visualization of concept from russell and norvigs artificial intelligence a modern approach and javascript code for algorithm unlike aimapython aimajava and other sibling project this project is primarily about the visualization and secondarily about the code view the visualization how to contribute chat on gitter some javascript visualization that i admire and would like to see similar kind of thing here red blob game a tutorial qiao search demo nicky case simulating the world rafael matsunaga genetic car thingy lee yiyuan 2048 bot,aimajavascript visualization concept russell norvigs artificial intelligence modern approach javascript code algorithm unlike aimapython aimajava sibling project project primarily visualization secondarily code view visualization contribute chat gitter javascript visualization admire would like see similar kind thing red blob game tutorial qiao search demo nicky case simulating world rafael matsunaga genetic car thingy lee yiyuan 2048 bot
JavaScript ,"


















Artificial Intelligence as a Service


Origami is an AI-as-a-service that allows researchers to easily convert their deep learning models into an online service that is widely accessible to everyone without the need to setup the infrastructure, resolve the dependencies, and build a web service around the deep learning model. By lowering the barrier to entry to latest AI algorithms, we provide developers, researchers and students the ability to access any model using a simple REST API call.


The aim of this project is to create a framework that can help people create a web based demo out of their machine learning code and share it. Others can test the model without going into the implementation details. Usually testing models by other people involves a lot of preparation and setup. This project aims to cut that down.


This app is presently under active development and we welcome contributions. Please check out our issues thread to find things to work on, or ping us on Gitter.
Installation Instructions
Windows Installation
VirtualBox
One of the easier ways to get started with Origami on Windows is by using a virtual machine of Ubunutu 16.04 LTS on Oracle's VirtualBox. With Ubuntu installed, Origami can be installed by following the instructions in the next sections. We can install VirtualBox in just two easy steps.
Step One - Downloading Virtual Box
You can install Virtual Box on Oracle's VirtualBox website.

Next, under ""Virtual binaries,"" click on Windows hosts under ""VirtualBox X.X.XX platform packages"" to download the executable file for the latest version of VirtualBox. Wait for this to install and open the file when the download has completed.
Step Two - Starting installation
The .exe file will have the following format: VirtualBox-VersionNumber-BuildNumber-Win.exe.

Once the setup wizard is open, follow the instructions. Everything can be kept as default, but feel free to change anything to your preference.
If you encounter a Windows User Account Control Warning pop-up, click Yes to accept and continue.
When you reach the Network Interface dialouge box, be sure to proceed. VirtualBox will install network interfaces that will interact with the installed your virtual machine(s) and Windows. You will be temporarily disconnected from the Internet, but this connection will be re-established.

When you launch VirtualBox, you should see a screen similar to the one below. Congratualtions, you have successfully installed VirtualBox!

Setting up an Ubuntu 16.04 LTS virtual machine with VirtualBox
Origami works well on an Ubuntu 16.04 LTS virtual machine, which is what we will use when creating our virtual machine.
Disable Hyper-V
Hyper-V is a tool that provides hardware virtualization, or allows virtual machines to run on virtual hardware. While this sounds useful, it can hamper your ability to use a 64-bit version of Ubuntu for your virtual machine. To avoid issues further along the line, we will disable this feature. We need to use a 64-bit virtual machine, as this is required for Docker, which will be used to install Origami.
Note: This will disable other applications that may require Hyper-V, such as Docker for Windows. You can always switch Hyper-V back on, but you will only be able to use VirtualBox or the other application(s) at a time.

Press Windows Key + X and select Apps and Features



Under ""Related settings,"" select Programs and Features



Next, click Turn windows features on or off on the left pane



Find Hyper-V and unmark it



Finally, click OK to save changes and reboot your computer

Installing Ubuntu inside Windows with VirtualBox
Although, below, we install Ubuntu 16.04 for Origami, this method can be used to install all other distributions of Ubuntu. Please be aware that you have at least a minimum of 512 MB of RAM on your computer, but keep in mind 1 or more GB is recommended.
Step One - Downloading the Ubuntu disk image (.iso file)
Navigate to this page to view the Ubuntu 16.04.5 LTS release page. Select the ""64-bit PC (AMD64) desktop image"" and save this for usage laters (install the 32-bit desktop image below the 64-bit option if you plan on using a 32-bit virtual machine).

Step Two - Creating the New Virtual Machine
After installing the disk image, we will create the virtual machine on VirtualBox. Launch VirtualBox and select New to proceed. Type in ""Ubuntu"" into the ""Name:"" field of the New Virtual Machine Wizard pop-up. Conveniently, this should adjust the ""Type:"" and ""Version:"" fields automatically as needed.

Step Three - Setting Base Memory (RAM)
VirtualBox will give a recommendation of how much memory (RAM) to allocate for your virtual machine. If you do not have much RAM, especially 1 GB or less, stick with VirtualBox's recommendation. If you have ample RAM, try to stick to a quarter of your total RAM. If you do not know how much RAM you have, or as a matter of fact do not know what RAM is, stick with the recommendation.

Step Four - Hard Disk
Since this is probably your first time using VirtualBox, create a new hard disk and then click ""Next.""

Step Five - Disk Type
Leave file type as ""VDI (VirtualBox Disk Image)"" and click ""Next.""

Step Six - Storage Details
A dynamically expanding virtual hard drive may be best, as it will only take up the space that you actually use on your virtual machine. However, there has been issues where the virtual hard drive fills up instead of actually expanding. Thus, it is recommended to pick ""Fixed size.""

Step Seven - Disk File Location and Hard Drive Size
Although Origami itself does not take up relatively much space, when installing Docker and other software, hard drive space can run low. Be sure to add as much hard drive space as you can, as it is a bit tedious to expand hard drive space after the virtual machine has been fully set up.

Step Eight - Create the Virtual Hard Drive
Simply click ""Create"" from the dialouge box from the step prior and wait for the virtual hard drive to be created. As this is usually a large file, it may take a bit of time.

Step Nine - Adding the Downloaded Ubuntu Disk Image
Before we boot the virtual machine, we need to add the downloaded Ubuntu disk image (.iso file) onto the virtual machine. While your virtual machine is selected in the left pane, click Settings and then Storage. Next, under ""IDE Controller,"" select Empty and click on the little disk icon. In the menu, click Choose Virtual Optical Disk File... next to the folder icon.

Navigate to the Ubuntu disk image file downloaded earlier and click ""Open.""
Note: Both disk image versions for Ubuntu desktop are downloaded in the image below. As we are using a 64-bit virtual machine, we are opening the 64-bit .iso file.

Afterward, ""Empty"" should now be replaced by the filename of our disk image file, and we can now click OK.

Step Ten - Downloading Ubuntu onto your virtual machine
Double-click your virtual machine to start it up. You may get various pop-ups providing warnings and instructions in regard to operating a virtual machine with VirtualBox. Be sure to read these, and you can mark not to see these again if you would like. Once Ubuntu is booted up, click Install Ubuntu and follow the instructions as if you were installing Ubuntu on an actual hard drive.



Installing Docker
We use Docker to install Origami. As Origami runs well on Ubuntu, we recommend you follow the official Docker documentation here. Use the ""repository method"" for the installation of Docker CE on this site. CE stands for ""Community Edition,"" as is designed for developers and ordinary users. Make sure to install the latest version of Docker (skip step #3 on ""Installing Docker CE""), and if you followed the tutorial above and created an Ubuntu virtual machine, follow the x86_64 architecture command when setting up the repository.
If you are using MacOS, follow the instructions on Docker's site here.
Setting the environment variables
Refer to the below during the installation process as needed.

origami.env stores all the environment variables necessary to run Origami.


HOST should be set to the hostname of the server.
PORT should be set to the port you want the server to listen on.
DB_NAME will be used to set the name for your postgres database.
DB_PASS will be used to set the password for the database user. This is also the admin password.
DB_USER is the username for a user who can modify the database. This is also the admin username.
DB_USER_EMAIL stores the email for the admin.
DB_HOST should be set to postgres in production and localhost in development.
REDIS_HOST should be set to redis and localhost in development.

To create the file, cp origami.env.sample origami.env and edit the file with the above fields.

Origami/outCalls/config.js stores config variables needed by the UI.


CLIENT_IP should be set to the same value as HOST in origami.env
CLIENT_PORT should be set to the same value as PORT in origami.env
For DROPBOX_API_KEY , check step 3 of configuring Origami

Production setup instructions
Use Docker to setup Origami on production
Running the server
You can run the server with the help of docker and docker-compose.
Run  $ docker-compose up
Development setup instructions
This application requires Pip, Node.js v5+, Yarn, and Python 2.7/3.4+ to install
Installing Pip
If you do not already have pip installed, run the following command:
$ sudo apt-get update
$ sudo apt get python-pip

MacOS: $ sudo easy_install pip

Installing Node.js
To install a stable and up-to-date version of Node.js, we will use Node's PPA (Personal Package Archive). Keep in mind this is optimal for Linux Mint and Ubuntu operating systems. Please run the following commands to install the latest version of Node.js:
$ sudo apt-get update
$ sudo apt install curl
$ curl -sL https://deb.nodesource.com/setup_10.x | sudo bash -
$ sudo apt install nodejs
$ node -v

Verify that your Node.js version is v5 or greater.

Installing Yarn
Yarn helps install dependencies and other packages with ease. Here we will use npm (Node Package Manager) to install Yarn. As npm is installed with Node.js, be sure Node is already installed. Notice in the command that we include the -g flag for installation globally, so Yarn can thus be used in all of your projects.
$ sudo apt-get update
$ sudo npm install yarn -g


Installing Python
Finally, we can install Python. Follow these commands to get the most up-to-date version of Python. If you would like a specific version of Python, be sure to include your preference after python (e.g. python 3.6 for Python 3.6). Ubuntu comes with Python installed, which is typically Python 2.7. Below, we install python3, and to use this, we would replace all python commands with python3. Below are the commands:
$ sudo apt-get update
$ sudo apt-get install python3
$ python3 --version


Create a Virtual Environment

$ pip install virtualenv
$ virtualenv venv venv = Name of virtualenv
$ source venv/bin/activate

Note: Step 2 will create a folder named venv in your working directory
Getting the code and dependencies

Clone the repository via git

$ git clone --recursive https://github.com/Cloud-CV/Origami.git && cd Origami/

Renaming origami.env.sample as origami.env and setting environment variables in origami.env

$ cp origami.env.sample origami.env
$ nano origami.env

Here, set the environment variables according to the above instructions on environment variables. Once they have been edited, Ctrl O, Enter, and Ctrl X to save and exit. The following is an example of what this may look like (be sure to include localhost as the necessary values below if you are going to run Origami on your local machine).
set -a
HOST=localhost
PORT=8000
DB_NAME=origami546
DB_PASS=origami546
DB_USER=origami546
DB_USER_EMAIL=example@gmail.com
DB_HOST=localhost
REDIS_HOST=localhost
set +a

Afterward, run the following to set more variables as entailed in the above section for environment variables for Origami/outCalls/config.js
$ nano Origami/outCalls/config.js



Add all of the Python dependencies.
$ pip install -r requirements.txt


Set up the Postgresql database


Install postgresql if you have not already. The -contrib package will add more utilities and added functionality.
sudo apt-get update
sudo apt-get install postgresql postgresql-contrib

Next we will create a database containing the details we will use for Origami. Following the previous example, creating the database may look like the following:
$ sudo service postgresql start
$ sudo -u postgres psql
postgres=# CREATE DATABASE origami546;
postgres=# CREATE USER origami546 WITH PASSWORD 'origami546';
postgres=# ALTER ROLE origami546 SET client_encoding TO 'utf8';
postgres=# ALTER ROLE origami546 SET default_transaction_isolation TO 'read committed';
postgres=# ALTER ROLE origami546 SET timezone TO 'UTC';
postgres=# ALTER USER origami546 CREATEDB;
postgres=# \q



Add all of the Javascript dependencies
$ yarn (preferably)
or
$ npm install


Setup the Redis server


$ docker run -d -p 6379:6379 --name origami-redis redis:alpine


Activate the environment
$ source origami.env


Setting up the database
Create all the tables
$ python manage.py makemigrations
$ python manage.py migrate

Create admin account
$ python manage.py initadmin
Start the server
To ensure everything works out, follow these steps carefully. Make sure all three terminals are running at the same time.

Start the server by

$ python manage.py runserver --noworker

Start the worker

Open a second terminal and run the following:
$ source venv/bin/activate
$ cd Origami/
$ source origami.env
$ python manage.py runworker


Running the server with Yarn

Open a third terminal and run the following:
$ source venv/bin/activate
$ cd Origami/
$ source origami.env
$ yarn run dev


Go to localhost:8000
Visit Read the docs for further instructions on getting started. If you have never created an OAuth App on GitHub, see the below instructions.

Setup Authentication for Virtual Environment


Go to Github Developer Applications and create a new application here.


For local deployments, use the following information:

Application name: Origami
Homepage URL: http://localhost:8000
Application description: Origami
Authorization callback URL: http://localhost:8000/auth/github/login/callback/



Github will provide you with a client ID and secret Key, save these.


Start the application.


$ python manage.py runserver



Open http://localhost:8000/admin


Login with the credentials from your admin account. This should be your username and password you used for the Postgresql if everything was kept consistent.


From the Django admin home page, go to Sites under the Sites category and make sure ""localhost:8000"" is the only site listed under ""DOMAIN NAME"".


Contributing to Origami


Make sure you run tests on your changes before you push the code using:

$ python manage.py test
$ yarn run test



Fix lint issues with the code using:

$ yarn run lint:fix



License
This software is licensed under GNU AGPLv3. Please see the included License file. All external libraries, if modified, will be mentioned below explicitly.
",artifici intellig as a servic origami is an aiasaservic that allow research to easili convert their deep learn model into an onlin servic that is wide access to everyon without the need to setup the infrastructur resolv the depend and build a web servic around the deep learn model by lower the barrier to entri to latest ai algorithm we provid develop research and student the abil to access ani model use a simpl rest api call the aim of thi project is to creat a framework that can help peopl creat a web base demo out of their machin learn code and share it other can test the model without go into the implement detail usual test model by other peopl involv a lot of prepar and setup thi project aim to cut that down thi app is present under activ develop and we welcom contribut pleas check out our issu thread to find thing to work on or ping us on gitter instal instruct window instal virtualbox one of the easier way to get start with origami on window is by use a virtual machin of ubunutu 1604 lt on oracl virtualbox with ubuntu instal origami can be instal by follow the instruct in the next section we can instal virtualbox in just two easi step step one download virtual box you can instal virtual box on oracl virtualbox websit next under virtual binari click on window host under virtualbox xxxx platform packag to download the execut file for the latest version of virtualbox wait for thi to instal and open the file when the download ha complet step two start instal the exe file will have the follow format virtualboxversionnumberbuildnumberwinex onc the setup wizard is open follow the instruct everyth can be kept as default but feel free to chang anyth to your prefer if you encount a window user account control warn popup click ye to accept and continu when you reach the network interfac dialoug box be sure to proceed virtualbox will instal network interfac that will interact with the instal your virtual machin and window you will be temporarili disconnect from the internet but thi connect will be reestablish when you launch virtualbox you should see a screen similar to the one below congratualt you have success instal virtualbox set up an ubuntu 1604 lt virtual machin with virtualbox origami work well on an ubuntu 1604 lt virtual machin which is what we will use when creat our virtual machin disabl hyperv hyperv is a tool that provid hardwar virtual or allow virtual machin to run on virtual hardwar while thi sound use it can hamper your abil to use a 64bit version of ubuntu for your virtual machin to avoid issu further along the line we will disabl thi featur we need to use a 64bit virtual machin as thi is requir for docker which will be use to instal origami note thi will disabl other applic that may requir hyperv such as docker for window you can alway switch hyperv back on but you will onli be abl to use virtualbox or the other applic at a time press window key x and select app and featur under relat set select program and featur next click turn window featur on or off on the left pane find hyperv and unmark it final click ok to save chang and reboot your comput instal ubuntu insid window with virtualbox although below we instal ubuntu 1604 for origami thi method can be use to instal all other distribut of ubuntu pleas be awar that you have at least a minimum of 512 mb of ram on your comput but keep in mind 1 or more gb is recommend step one download the ubuntu disk imag iso file navig to thi page to view the ubuntu 16045 lt releas page select the 64bit pc amd64 desktop imag and save thi for usag later instal the 32bit desktop imag below the 64bit option if you plan on use a 32bit virtual machin step two creat the new virtual machin after instal the disk imag we will creat the virtual machin on virtualbox launch virtualbox and select new to proceed type in ubuntu into the name field of the new virtual machin wizard popup conveni thi should adjust the type and version field automat as need step three set base memori ram virtualbox will give a recommend of how much memori ram to alloc for your virtual machin if you do not have much ram especi 1 gb or less stick with virtualbox recommend if you have ampl ram tri to stick to a quarter of your total ram if you do not know how much ram you have or as a matter of fact do not know what ram is stick with the recommend step four hard disk sinc thi is probabl your first time use virtualbox creat a new hard disk and then click next step five disk type leav file type as vdi virtualbox disk imag and click next step six storag detail a dynam expand virtual hard drive may be best as it will onli take up the space that you actual use on your virtual machin howev there ha been issu where the virtual hard drive fill up instead of actual expand thu it is recommend to pick fix size step seven disk file locat and hard drive size although origami itself doe not take up rel much space when instal docker and other softwar hard drive space can run low be sure to add as much hard drive space as you can as it is a bit tediou to expand hard drive space after the virtual machin ha been fulli set up step eight creat the virtual hard drive simpli click creat from the dialoug box from the step prior and wait for the virtual hard drive to be creat as thi is usual a larg file it may take a bit of time step nine ad the download ubuntu disk imag befor we boot the virtual machin we need to add the download ubuntu disk imag iso file onto the virtual machin while your virtual machin is select in the left pane click set and then storag next under ide control select empti and click on the littl disk icon in the menu click choos virtual optic disk file next to the folder icon navig to the ubuntu disk imag file download earlier and click open note both disk imag version for ubuntu desktop are download in the imag below as we are use a 64bit virtual machin we are open the 64bit iso file afterward empti should now be replac by the filenam of our disk imag file and we can now click ok step ten download ubuntu onto your virtual machin doubleclick your virtual machin to start it up you may get variou popup provid warn and instruct in regard to oper a virtual machin with virtualbox be sure to read these and you can mark not to see these again if you would like onc ubuntu is boot up click instal ubuntu and follow the instruct as if you were instal ubuntu on an actual hard drive instal docker we use docker to instal origami as origami run well on ubuntu we recommend you follow the offici docker document here use the repositori method for the instal of docker ce on thi site ce stand for commun edit as is design for develop and ordinari user make sure to instal the latest version of docker skip step 3 on instal docker ce and if you follow the tutori abov and creat an ubuntu virtual machin follow the x86_64 architectur command when set up the repositori if you are use maco follow the instruct on docker site here set the environ variabl refer to the below dure the instal process as need origamienv store all the environ variabl necessari to run origami host should be set to the hostnam of the server port should be set to the port you want the server to listen on db_name will be use to set the name for your postgr databas db_pass will be use to set the password for the databas user thi is also the admin password db_user is the usernam for a user who can modifi the databas thi is also the admin usernam db_user_email store the email for the admin db_host should be set to postgr in product and localhost in develop redis_host should be set to redi and localhost in develop to creat the file cp origamienvsampl origamienv and edit the file with the abov field origamioutcallsconfigj store config variabl need by the ui client_ip should be set to the same valu as host in origamienv client_port should be set to the same valu as port in origamienv for dropbox_api_key check step 3 of configur origami product setup instruct use docker to setup origami on product run the server you can run the server with the help of docker and dockercompos run dockercompos up develop setup instruct thi applic requir pip nodej v5 yarn and python 2734 to instal instal pip if you do not alreadi have pip instal run the follow command sudo aptget updat sudo apt get pythonpip maco sudo easy_instal pip instal nodej to instal a stabl and uptod version of nodej we will use node ppa person packag archiv keep in mind thi is optim for linux mint and ubuntu oper system pleas run the follow command to instal the latest version of nodej sudo aptget updat sudo apt instal curl curl sl httpsdebnodesourcecomsetup_10x sudo bash sudo apt instal nodej node v verifi that your nodej version is v5 or greater instal yarn yarn help instal depend and other packag with eas here we will use npm node packag manag to instal yarn as npm is instal with nodej be sure node is alreadi instal notic in the command that we includ the g flag for instal global so yarn can thu be use in all of your project sudo aptget updat sudo npm instal yarn g instal python final we can instal python follow these command to get the most uptod version of python if you would like a specif version of python be sure to includ your prefer after python eg python 36 for python 36 ubuntu come with python instal which is typic python 27 below we instal python3 and to use thi we would replac all python command with python3 below are the command sudo aptget updat sudo aptget instal python3 python3 version creat a virtual environ pip instal virtualenv virtualenv venv venv name of virtualenv sourc venvbinactiv note step 2 will creat a folder name venv in your work directori get the code and depend clone the repositori via git git clone recurs httpsgithubcomcloudcvorigamigit cd origami renam origamienvsampl as origamienv and set environ variabl in origamienv cp origamienvsampl origamienv nano origamienv here set the environ variabl accord to the abov instruct on environ variabl onc they have been edit ctrl o enter and ctrl x to save and exit the follow is an exampl of what thi may look like be sure to includ localhost as the necessari valu below if you are go to run origami on your local machin set a hostlocalhost port8000 db_nameorigami546 db_passorigami546 db_userorigami546 db_user_emailexamplegmailcom db_hostlocalhost redis_hostlocalhost set a afterward run the follow to set more variabl as entail in the abov section for environ variabl for origamioutcallsconfigj nano origamioutcallsconfigj add all of the python depend pip instal r requirementstxt set up the postgresql databas instal postgresql if you have not alreadi the contrib packag will add more util and ad function sudo aptget updat sudo aptget instal postgresql postgresqlcontrib next we will creat a databas contain the detail we will use for origami follow the previou exampl creat the databas may look like the follow sudo servic postgresql start sudo u postgr psql postgr creat databas origami546 postgr creat user origami546 with password origami546 postgr alter role origami546 set client_encod to utf8 postgr alter role origami546 set default_transaction_isol to read commit postgr alter role origami546 set timezon to utc postgr alter user origami546 createdb postgr q add all of the javascript depend yarn prefer or npm instal setup the redi server docker run d p 63796379 name origamiredi redisalpin activ the environ sourc origamienv set up the databas creat all the tabl python managepi makemigr python managepi migrat creat admin account python managepi initadmin start the server to ensur everyth work out follow these step care make sure all three termin are run at the same time start the server by python managepi runserv nowork start the worker open a second termin and run the follow sourc venvbinactiv cd origami sourc origamienv python managepi runwork run the server with yarn open a third termin and run the follow sourc venvbinactiv cd origami sourc origamienv yarn run dev go to localhost8000 visit read the doc for further instruct on get start if you have never creat an oauth app on github see the below instruct setup authent for virtual environ go to github develop applic and creat a new applic here for local deploy use the follow inform applic name origami homepag url httplocalhost8000 applic descript origami author callback url httplocalhost8000authgithublogincallback github will provid you with a client id and secret key save these start the applic python managepi runserv open httplocalhost8000admin login with the credenti from your admin account thi should be your usernam and password you use for the postgresql if everyth wa kept consist from the django admin home page go to site under the site categori and make sure localhost8000 is the onli site list under domain name contribut to origami make sure you run test on your chang befor you push the code use python managepi test yarn run test fix lint issu with the code use yarn run lintfix licens thi softwar is licens under gnu agplv3 pleas see the includ licens file all extern librari if modifi will be mention below explicitli,artificial intelligence a a service origami is an aiasaservice that allows researcher to easily convert their deep learning model into an online service that is widely accessible to everyone without the need to setup the infrastructure resolve the dependency and build a web service around the deep learning model by lowering the barrier to entry to latest ai algorithm we provide developer researcher and student the ability to access any model using a simple rest api call the aim of this project is to create a framework that can help people create a web based demo out of their machine learning code and share it others can test the model without going into the implementation detail usually testing model by other people involves a lot of preparation and setup this project aim to cut that down this app is presently under active development and we welcome contribution please check out our issue thread to find thing to work on or ping u on gitter installation instruction window installation virtualbox one of the easier way to get started with origami on window is by using a virtual machine of ubunutu 1604 lts on oracle virtualbox with ubuntu installed origami can be installed by following the instruction in the next section we can install virtualbox in just two easy step step one downloading virtual box you can install virtual box on oracle virtualbox website next under virtual binary click on window host under virtualbox xxxx platform package to download the executable file for the latest version of virtualbox wait for this to install and open the file when the download ha completed step two starting installation the exe file will have the following format virtualboxversionnumberbuildnumberwinexe once the setup wizard is open follow the instruction everything can be kept a default but feel free to change anything to your preference if you encounter a window user account control warning popup click yes to accept and continue when you reach the network interface dialouge box be sure to proceed virtualbox will install network interface that will interact with the installed your virtual machine and window you will be temporarily disconnected from the internet but this connection will be reestablished when you launch virtualbox you should see a screen similar to the one below congratualtions you have successfully installed virtualbox setting up an ubuntu 1604 lts virtual machine with virtualbox origami work well on an ubuntu 1604 lts virtual machine which is what we will use when creating our virtual machine disable hyperv hyperv is a tool that provides hardware virtualization or allows virtual machine to run on virtual hardware while this sound useful it can hamper your ability to use a 64bit version of ubuntu for your virtual machine to avoid issue further along the line we will disable this feature we need to use a 64bit virtual machine a this is required for docker which will be used to install origami note this will disable other application that may require hyperv such a docker for window you can always switch hyperv back on but you will only be able to use virtualbox or the other application at a time press window key x and select apps and feature under related setting select program and feature next click turn window feature on or off on the left pane find hyperv and unmark it finally click ok to save change and reboot your computer installing ubuntu inside window with virtualbox although below we install ubuntu 1604 for origami this method can be used to install all other distribution of ubuntu please be aware that you have at least a minimum of 512 mb of ram on your computer but keep in mind 1 or more gb is recommended step one downloading the ubuntu disk image iso file navigate to this page to view the ubuntu 16045 lts release page select the 64bit pc amd64 desktop image and save this for usage laters install the 32bit desktop image below the 64bit option if you plan on using a 32bit virtual machine step two creating the new virtual machine after installing the disk image we will create the virtual machine on virtualbox launch virtualbox and select new to proceed type in ubuntu into the name field of the new virtual machine wizard popup conveniently this should adjust the type and version field automatically a needed step three setting base memory ram virtualbox will give a recommendation of how much memory ram to allocate for your virtual machine if you do not have much ram especially 1 gb or le stick with virtualboxs recommendation if you have ample ram try to stick to a quarter of your total ram if you do not know how much ram you have or a a matter of fact do not know what ram is stick with the recommendation step four hard disk since this is probably your first time using virtualbox create a new hard disk and then click next step five disk type leave file type a vdi virtualbox disk image and click next step six storage detail a dynamically expanding virtual hard drive may be best a it will only take up the space that you actually use on your virtual machine however there ha been issue where the virtual hard drive fill up instead of actually expanding thus it is recommended to pick fixed size step seven disk file location and hard drive size although origami itself doe not take up relatively much space when installing docker and other software hard drive space can run low be sure to add a much hard drive space a you can a it is a bit tedious to expand hard drive space after the virtual machine ha been fully set up step eight create the virtual hard drive simply click create from the dialouge box from the step prior and wait for the virtual hard drive to be created a this is usually a large file it may take a bit of time step nine adding the downloaded ubuntu disk image before we boot the virtual machine we need to add the downloaded ubuntu disk image iso file onto the virtual machine while your virtual machine is selected in the left pane click setting and then storage next under ide controller select empty and click on the little disk icon in the menu click choose virtual optical disk file next to the folder icon navigate to the ubuntu disk image file downloaded earlier and click open note both disk image version for ubuntu desktop are downloaded in the image below a we are using a 64bit virtual machine we are opening the 64bit iso file afterward empty should now be replaced by the filename of our disk image file and we can now click ok step ten downloading ubuntu onto your virtual machine doubleclick your virtual machine to start it up you may get various popups providing warning and instruction in regard to operating a virtual machine with virtualbox be sure to read these and you can mark not to see these again if you would like once ubuntu is booted up click install ubuntu and follow the instruction a if you were installing ubuntu on an actual hard drive installing docker we use docker to install origami a origami run well on ubuntu we recommend you follow the official docker documentation here use the repository method for the installation of docker ce on this site ce stand for community edition a is designed for developer and ordinary user make sure to install the latest version of docker skip step 3 on installing docker ce and if you followed the tutorial above and created an ubuntu virtual machine follow the x86_64 architecture command when setting up the repository if you are using macos follow the instruction on docker site here setting the environment variable refer to the below during the installation process a needed origamienv store all the environment variable necessary to run origami host should be set to the hostname of the server port should be set to the port you want the server to listen on db_name will be used to set the name for your postgres database db_pass will be used to set the password for the database user this is also the admin password db_user is the username for a user who can modify the database this is also the admin username db_user_email store the email for the admin db_host should be set to postgres in production and localhost in development redis_host should be set to redis and localhost in development to create the file cp origamienvsample origamienv and edit the file with the above field origamioutcallsconfigjs store config variable needed by the ui client_ip should be set to the same value a host in origamienv client_port should be set to the same value a port in origamienv for dropbox_api_key check step 3 of configuring origami production setup instruction use docker to setup origami on production running the server you can run the server with the help of docker and dockercompose run dockercompose up development setup instruction this application requires pip nodejs v5 yarn and python 2734 to install installing pip if you do not already have pip installed run the following command sudo aptget update sudo apt get pythonpip macos sudo easy_install pip installing nodejs to install a stable and uptodate version of nodejs we will use node ppa personal package archive keep in mind this is optimal for linux mint and ubuntu operating system please run the following command to install the latest version of nodejs sudo aptget update sudo apt install curl curl sl httpsdebnodesourcecomsetup_10x sudo bash sudo apt install nodejs node v verify that your nodejs version is v5 or greater installing yarn yarn help install dependency and other package with ease here we will use npm node package manager to install yarn a npm is installed with nodejs be sure node is already installed notice in the command that we include the g flag for installation globally so yarn can thus be used in all of your project sudo aptget update sudo npm install yarn g installing python finally we can install python follow these command to get the most uptodate version of python if you would like a specific version of python be sure to include your preference after python eg python 36 for python 36 ubuntu come with python installed which is typically python 27 below we install python3 and to use this we would replace all python command with python3 below are the command sudo aptget update sudo aptget install python3 python3 version create a virtual environment pip install virtualenv virtualenv venv venv name of virtualenv source venvbinactivate note step 2 will create a folder named venv in your working directory getting the code and dependency clone the repository via git git clone recursive httpsgithubcomcloudcvorigamigit cd origami renaming origamienvsample a origamienv and setting environment variable in origamienv cp origamienvsample origamienv nano origamienv here set the environment variable according to the above instruction on environment variable once they have been edited ctrl o enter and ctrl x to save and exit the following is an example of what this may look like be sure to include localhost a the necessary value below if you are going to run origami on your local machine set a hostlocalhost port8000 db_nameorigami546 db_passorigami546 db_userorigami546 db_user_emailexamplegmailcom db_hostlocalhost redis_hostlocalhost set a afterward run the following to set more variable a entailed in the above section for environment variable for origamioutcallsconfigjs nano origamioutcallsconfigjs add all of the python dependency pip install r requirementstxt set up the postgresql database install postgresql if you have not already the contrib package will add more utility and added functionality sudo aptget update sudo aptget install postgresql postgresqlcontrib next we will create a database containing the detail we will use for origami following the previous example creating the database may look like the following sudo service postgresql start sudo u postgres psql postgres create database origami546 postgres create user origami546 with password origami546 postgres alter role origami546 set client_encoding to utf8 postgres alter role origami546 set default_transaction_isolation to read committed postgres alter role origami546 set timezone to utc postgres alter user origami546 createdb postgres q add all of the javascript dependency yarn preferably or npm install setup the redis server docker run d p 63796379 name origamiredis redisalpine activate the environment source origamienv setting up the database create all the table python managepy makemigrations python managepy migrate create admin account python managepy initadmin start the server to ensure everything work out follow these step carefully make sure all three terminal are running at the same time start the server by python managepy runserver noworker start the worker open a second terminal and run the following source venvbinactivate cd origami source origamienv python managepy runworker running the server with yarn open a third terminal and run the following source venvbinactivate cd origami source origamienv yarn run dev go to localhost8000 visit read the doc for further instruction on getting started if you have never created an oauth app on github see the below instruction setup authentication for virtual environment go to github developer application and create a new application here for local deployment use the following information application name origami homepage url httplocalhost8000 application description origami authorization callback url httplocalhost8000authgithublogincallback github will provide you with a client id and secret key save these start the application python managepy runserver open httplocalhost8000admin login with the credential from your admin account this should be your username and password you used for the postgresql if everything wa kept consistent from the django admin home page go to site under the site category and make sure localhost8000 is the only site listed under domain name contributing to origami make sure you run test on your change before you push the code using python managepy test yarn run test fix lint issue with the code using yarn run lintfix license this software is licensed under gnu agplv3 please see the included license file all external library if modified will be mentioned below explicitly,artificial intelligence service origami aiasaservice allows researcher easily convert deep learning model online service widely accessible everyone without need setup infrastructure resolve dependency build web service around deep learning model lowering barrier entry latest ai algorithm provide developer researcher student ability access model using simple rest api call aim project create framework help people create web based demo machine learning code share others test model without going implementation detail usually testing model people involves lot preparation setup project aim cut app presently active development welcome contribution please check issue thread find thing work ping u gitter installation instruction window installation virtualbox one easier way get started origami window using virtual machine ubunutu 1604 lts oracle virtualbox ubuntu installed origami installed following instruction next section install virtualbox two easy step step one downloading virtual box install virtual box oracle virtualbox website next virtual binary click window host virtualbox xxxx platform package download executable file latest version virtualbox wait install open file download completed step two starting installation exe file following format virtualboxversionnumberbuildnumberwinexe setup wizard open follow instruction everything kept default feel free change anything preference encounter window user account control warning popup click yes accept continue reach network interface dialouge box sure proceed virtualbox install network interface interact installed virtual machine window temporarily disconnected internet connection reestablished launch virtualbox see screen similar one congratualtions successfully installed virtualbox setting ubuntu 1604 lts virtual machine virtualbox origami work well ubuntu 1604 lts virtual machine use creating virtual machine disable hyperv hyperv tool provides hardware virtualization allows virtual machine run virtual hardware sound useful hamper ability use 64bit version ubuntu virtual machine avoid issue along line disable feature need use 64bit virtual machine required docker used install origami note disable application may require hyperv docker window always switch hyperv back able use virtualbox application time press window key x select apps feature related setting select program feature next click turn window feature left pane find hyperv unmark finally click ok save change reboot computer installing ubuntu inside window virtualbox although install ubuntu 1604 origami method used install distribution ubuntu please aware least minimum 512 mb ram computer keep mind 1 gb recommended step one downloading ubuntu disk image iso file navigate page view ubuntu 16045 lts release page select 64bit pc amd64 desktop image save usage laters install 32bit desktop image 64bit option plan using 32bit virtual machine step two creating new virtual machine installing disk image create virtual machine virtualbox launch virtualbox select new proceed type ubuntu name field new virtual machine wizard popup conveniently adjust type version field automatically needed step three setting base memory ram virtualbox give recommendation much memory ram allocate virtual machine much ram especially 1 gb le stick virtualboxs recommendation ample ram try stick quarter total ram know much ram matter fact know ram stick recommendation step four hard disk since probably first time using virtualbox create new hard disk click next step five disk type leave file type vdi virtualbox disk image click next step six storage detail dynamically expanding virtual hard drive may best take space actually use virtual machine however issue virtual hard drive fill instead actually expanding thus recommended pick fixed size step seven disk file location hard drive size although origami take relatively much space installing docker software hard drive space run low sure add much hard drive space bit tedious expand hard drive space virtual machine fully set step eight create virtual hard drive simply click create dialouge box step prior wait virtual hard drive created usually large file may take bit time step nine adding downloaded ubuntu disk image boot virtual machine need add downloaded ubuntu disk image iso file onto virtual machine virtual machine selected left pane click setting storage next ide controller select empty click little disk icon menu click choose virtual optical disk file next folder icon navigate ubuntu disk image file downloaded earlier click open note disk image version ubuntu desktop downloaded image using 64bit virtual machine opening 64bit iso file afterward empty replaced filename disk image file click ok step ten downloading ubuntu onto virtual machine doubleclick virtual machine start may get various popups providing warning instruction regard operating virtual machine virtualbox sure read mark see would like ubuntu booted click install ubuntu follow instruction installing ubuntu actual hard drive installing docker use docker install origami origami run well ubuntu recommend follow official docker documentation use repository method installation docker ce site ce stand community edition designed developer ordinary user make sure install latest version docker skip step 3 installing docker ce followed tutorial created ubuntu virtual machine follow x86_64 architecture command setting repository using macos follow instruction docker site setting environment variable refer installation process needed origamienv store environment variable necessary run origami host set hostname server port set port want server listen db_name used set name postgres database db_pass used set password database user also admin password db_user username user modify database also admin username db_user_email store email admin db_host set postgres production localhost development redis_host set redis localhost development create file cp origamienvsample origamienv edit file field origamioutcallsconfigjs store config variable needed ui client_ip set value host origamienv client_port set value port origamienv dropbox_api_key check step 3 configuring origami production setup instruction use docker setup origami production running server run server help docker dockercompose run dockercompose development setup instruction application requires pip nodejs v5 yarn python 2734 install installing pip already pip installed run following command sudo aptget update sudo apt get pythonpip macos sudo easy_install pip installing nodejs install stable uptodate version nodejs use node ppa personal package archive keep mind optimal linux mint ubuntu operating system please run following command install latest version nodejs sudo aptget update sudo apt install curl curl sl httpsdebnodesourcecomsetup_10x sudo bash sudo apt install nodejs node v verify nodejs version v5 greater installing yarn yarn help install dependency package ease use npm node package manager install yarn npm installed nodejs sure node already installed notice command include g flag installation globally yarn thus used project sudo aptget update sudo npm install yarn g installing python finally install python follow command get uptodate version python would like specific version python sure include preference python eg python 36 python 36 ubuntu come python installed typically python 27 install python3 use would replace python command python3 command sudo aptget update sudo aptget install python3 python3 version create virtual environment pip install virtualenv virtualenv venv venv name virtualenv source venvbinactivate note step 2 create folder named venv working directory getting code dependency clone repository via git git clone recursive httpsgithubcomcloudcvorigamigit cd origami renaming origamienvsample origamienv setting environment variable origamienv cp origamienvsample origamienv nano origamienv set environment variable according instruction environment variable edited ctrl enter ctrl x save exit following example may look like sure include localhost necessary value going run origami local machine set hostlocalhost port8000 db_nameorigami546 db_passorigami546 db_userorigami546 db_user_emailexamplegmailcom db_hostlocalhost redis_hostlocalhost set afterward run following set variable entailed section environment variable origamioutcallsconfigjs nano origamioutcallsconfigjs add python dependency pip install r requirementstxt set postgresql database install postgresql already contrib package add utility added functionality sudo aptget update sudo aptget install postgresql postgresqlcontrib next create database containing detail use origami following previous example creating database may look like following sudo service postgresql start sudo u postgres psql postgres create database origami546 postgres create user origami546 password origami546 postgres alter role origami546 set client_encoding utf8 postgres alter role origami546 set default_transaction_isolation read committed postgres alter role origami546 set timezone utc postgres alter user origami546 createdb postgres q add javascript dependency yarn preferably npm install setup redis server docker run p 63796379 name origamiredis redisalpine activate environment source origamienv setting database create table python managepy makemigrations python managepy migrate create admin account python managepy initadmin start server ensure everything work follow step carefully make sure three terminal running time start server python managepy runserver noworker start worker open second terminal run following source venvbinactivate cd origami source origamienv python managepy runworker running server yarn open third terminal run following source venvbinactivate cd origami source origamienv yarn run dev go localhost8000 visit read doc instruction getting started never created oauth app github see instruction setup authentication virtual environment go github developer application create new application local deployment use following information application name origami homepage url httplocalhost8000 application description origami authorization callback url httplocalhost8000authgithublogincallback github provide client id secret key save start application python managepy runserver open httplocalhost8000admin login credential admin account username password used postgresql everything kept consistent django admin home page go site site category make sure localhost8000 site listed domain name contributing origami make sure run test change push code using python managepy test yarn run test fix lint issue code using yarn run lintfix license software licensed gnu agplv3 please see included license file external library modified mentioned explicitly
JavaScript ,"UnityAI
Reusable Artificial Intelligence Experiments


Current Features

Pathfinding editor for waypoints and pathing visualization
A* Route Planning
Random waypoint navigation
Waypoint to waypoint following using Pathfinder
Navigation mesh processor in the Tools Menu (creates a NavmeshNode network with triangles and vertices from a selected mesh)
Pathing for navigation meshes
Funnel algorithm for navigation meshes
Steering behavior for path following
A basic FPS with path following spiders, ammo, and health spawned by an AI director

Script Locations
This project includes 2 Unity projects:

UnityPathing (a sandbox project for pathfinding experiments)
BasicGame (the FPS demo with pathfinding and an AI director)

Planning scripts are located in BasicGame/Assets/Pathfinding Scripts and gameplay/director scripts are located in BasicGame/Assets/Scripts
##License: MIT
Copyright (c) 2013 Julian Ceipek, Alyssa Bawgus, Eric Tappan, Alex Adkins
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
",unityai reusabl artifici intellig experi current featur pathfind editor for waypoint and path visual a rout plan random waypoint navig waypoint to waypoint follow use pathfind navig mesh processor in the tool menu creat a navmeshnod network with triangl and vertic from a select mesh path for navig mesh funnel algorithm for navig mesh steer behavior for path follow a basic fp with path follow spider ammo and health spawn by an ai director script locat thi project includ 2 uniti project unitypath a sandbox project for pathfind experi basicgam the fp demo with pathfind and an ai director plan script are locat in basicgameassetspathfind script and gameplaydirector script are locat in basicgameassetsscript licens mit copyright c 2013 julian ceipek alyssa bawgu eric tappan alex adkin permiss is herebi grant free of charg to ani person obtain a copi of thi softwar and associ document file the softwar to deal in the softwar without restrict includ without limit the right to use copi modifi merg publish distribut sublicens andor sell copi of the softwar and to permit person to whom the softwar is furnish to do so subject to the follow condit the abov copyright notic and thi permiss notic shall be includ in all copi or substanti portion of the softwar the softwar is provid as is without warranti of ani kind express or impli includ but not limit to the warranti of merchant fit for a particular purpos and noninfring in no event shall the author or copyright holder be liabl for ani claim damag or other liabil whether in an action of contract tort or otherwis aris from out of or in connect with the softwar or the use or other deal in the softwar,unityai reusable artificial intelligence experiment current feature pathfinding editor for waypoints and pathing visualization a route planning random waypoint navigation waypoint to waypoint following using pathfinder navigation mesh processor in the tool menu creates a navmeshnode network with triangle and vertex from a selected mesh pathing for navigation mesh funnel algorithm for navigation mesh steering behavior for path following a basic fps with path following spider ammo and health spawned by an ai director script location this project includes 2 unity project unitypathing a sandbox project for pathfinding experiment basicgame the fps demo with pathfinding and an ai director planning script are located in basicgameassetspathfinding script and gameplaydirector script are located in basicgameassetsscripts license mit copyright c 2013 julian ceipek alyssa bawgus eric tappan alex adkins permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation file the software to deal in the software without restriction including without limitation the right to use copy modify merge publish distribute sublicense andor sell copy of the software and to permit person to whom the software is furnished to do so subject to the following condition the above copyright notice and this permission notice shall be included in all copy or substantial portion of the software the software is provided a is without warranty of any kind express or implied including but not limited to the warranty of merchantability fitness for a particular purpose and noninfringement in no event shall the author or copyright holder be liable for any claim damage or other liability whether in an action of contract tort or otherwise arising from out of or in connection with the software or the use or other dealing in the software,unityai reusable artificial intelligence experiment current feature pathfinding editor waypoints pathing visualization route planning random waypoint navigation waypoint waypoint following using pathfinder navigation mesh processor tool menu creates navmeshnode network triangle vertex selected mesh pathing navigation mesh funnel algorithm navigation mesh steering behavior path following basic fps path following spider ammo health spawned ai director script location project includes 2 unity project unitypathing sandbox project pathfinding experiment basicgame fps demo pathfinding ai director planning script located basicgameassetspathfinding script gameplaydirector script located basicgameassetsscripts license mit copyright c 2013 julian ceipek alyssa bawgus eric tappan alex adkins permission hereby granted free charge person obtaining copy software associated documentation file software deal software without restriction including without limitation right use copy modify merge publish distribute sublicense andor sell copy software permit person software furnished subject following condition copyright notice permission notice shall included copy substantial portion software software provided without warranty kind express implied including limited warranty merchantability fitness particular purpose noninfringement event shall author copyright holder liable claim damage liability whether action contract tort otherwise arising connection software use dealing software
JavaScript ,"
  TimeMap v0


TimeMap is a tool for exploration, monitoring and classification of incidents in time and space.See a live instance here.



Overview
TimeMap is a standalone frontend application that allows to explore and monitor events in time and space. TimeMap uses OpenStreetMap satellite imagery as a backdrop by default, but can also be configured to use mapbox. It uses Leaflet and d3 to visually map information.
The recommended way to run a backend for timemap is using datasheet-server. This allows you to work with a spreadsheet or Google Sheet as a dynamic database for for timemap.
TimeMap has the following high-level features capabilites:

Visualize incidents of particular events on a map.
Visualize and filter these incidents over time, on an adjustable timeline that allows to zoom in and out.
Visualize types of incidents by tag and by category, which can be displayed using different styles.

A fully-functioning live version can be found as a result of the Forensic Architecture investigation of the Battle of Ilovaisk.
Get up and running
These easiest way to get up and running with timemap and datasheet-server is to
follow the tutorial here.
Instructions

Pull this repository.

git clone https://github.com/forensic-architecture/timemap

Install dependencies via yarn (recommended, it's just faster) or npm.

yarn          # npm install

Run it via yarn.

yarn dev      # npm run dev
To run with a file that is not 'config.js' in the root directory, set the CONFIG environment variable:
CONFIG=""myotherconfig.js"" yarn dev

IMPORTANT: Although the application will run just like that, in order for TimeMap to be able to display interesting information, you'll have to make sure to have the capacity to serve data, as well as adjusting some configuration parameters. See next section.
Running without datasheet-server
Technically, timemap is backend agnostic, but it requires a series of endpoints to provide data for it to visualize. The data is expected in JSON format. Some data elements are required and their format has some required fields. Other additional endpoints are optional, and if enabled, they simply add features to your taste.
The combination of all these data types is called the domain of the application in the context of TimeMap.
Contribute
Code of Conduct
Please read before contributing. We endeavour to cultivate a community around timemap and other OSS at Forensic Architecture that is inclusive and respectful. Please join us in this!
Contributing Guide
Learn more about our development process,  i.e. how to propose bugfixes and improvements.
Community
If you have any questions or just want to chat, please join our team fa_open_source on Keybase for community discussion. Keybase is a great platform for encrypted chat and file sharing that we use as a public forum.
License
timemap is distributed under the DoNoHarm license.
",timemap v0 timemap is a tool for explor monitor and classif of incid in time and spacese a live instanc here overview timemap is a standalon frontend applic that allow to explor and monitor event in time and space timemap use openstreetmap satellit imageri as a backdrop by default but can also be configur to use mapbox it use leaflet and d3 to visual map inform the recommend way to run a backend for timemap is use datasheetserv thi allow you to work with a spreadsheet or googl sheet as a dynam databas for for timemap timemap ha the follow highlevel featur capabilit visual incid of particular event on a map visual and filter these incid over time on an adjust timelin that allow to zoom in and out visual type of incid by tag and by categori which can be display use differ style a fullyfunct live version can be found as a result of the forens architectur investig of the battl of ilovaisk get up and run these easiest way to get up and run with timemap and datasheetserv is to follow the tutori here instruct pull thi repositori git clone httpsgithubcomforensicarchitecturetimemap instal depend via yarn recommend it just faster or npm yarn npm instal run it via yarn yarn dev npm run dev to run with a file that is not configj in the root directori set the config environ variabl configmyotherconfigj yarn dev import although the applic will run just like that in order for timemap to be abl to display interest inform youll have to make sure to have the capac to serv data as well as adjust some configur paramet see next section run without datasheetserv technic timemap is backend agnost but it requir a seri of endpoint to provid data for it to visual the data is expect in json format some data element are requir and their format ha some requir field other addit endpoint are option and if enabl they simpli add featur to your tast the combin of all these data type is call the domain of the applic in the context of timemap contribut code of conduct pleas read befor contribut we endeavour to cultiv a commun around timemap and other oss at forens architectur that is inclus and respect pleas join us in thi contribut guid learn more about our develop process ie how to propos bugfix and improv commun if you have ani question or just want to chat pleas join our team fa_open_sourc on keybas for commun discuss keybas is a great platform for encrypt chat and file share that we use as a public forum licens timemap is distribut under the donoharm licens,timemap v0 timemap is a tool for exploration monitoring and classification of incident in time and spacesee a live instance here overview timemap is a standalone frontend application that allows to explore and monitor event in time and space timemap us openstreetmap satellite imagery a a backdrop by default but can also be configured to use mapbox it us leaflet and d3 to visually map information the recommended way to run a backend for timemap is using datasheetserver this allows you to work with a spreadsheet or google sheet a a dynamic database for for timemap timemap ha the following highlevel feature capabilites visualize incident of particular event on a map visualize and filter these incident over time on an adjustable timeline that allows to zoom in and out visualize type of incident by tag and by category which can be displayed using different style a fullyfunctioning live version can be found a a result of the forensic architecture investigation of the battle of ilovaisk get up and running these easiest way to get up and running with timemap and datasheetserver is to follow the tutorial here instruction pull this repository git clone httpsgithubcomforensicarchitecturetimemap install dependency via yarn recommended it just faster or npm yarn npm install run it via yarn yarn dev npm run dev to run with a file that is not configjs in the root directory set the config environment variable configmyotherconfigjs yarn dev important although the application will run just like that in order for timemap to be able to display interesting information youll have to make sure to have the capacity to serve data a well a adjusting some configuration parameter see next section running without datasheetserver technically timemap is backend agnostic but it requires a series of endpoint to provide data for it to visualize the data is expected in json format some data element are required and their format ha some required field other additional endpoint are optional and if enabled they simply add feature to your taste the combination of all these data type is called the domain of the application in the context of timemap contribute code of conduct please read before contributing we endeavour to cultivate a community around timemap and other os at forensic architecture that is inclusive and respectful please join u in this contributing guide learn more about our development process ie how to propose bugfixes and improvement community if you have any question or just want to chat please join our team fa_open_source on keybase for community discussion keybase is a great platform for encrypted chat and file sharing that we use a a public forum license timemap is distributed under the donoharm license,timemap v0 timemap tool exploration monitoring classification incident time spacesee live instance overview timemap standalone frontend application allows explore monitor event time space timemap us openstreetmap satellite imagery backdrop default also configured use mapbox us leaflet d3 visually map information recommended way run backend timemap using datasheetserver allows work spreadsheet google sheet dynamic database timemap timemap following highlevel feature capabilites visualize incident particular event map visualize filter incident time adjustable timeline allows zoom visualize type incident tag category displayed using different style fullyfunctioning live version found result forensic architecture investigation battle ilovaisk get running easiest way get running timemap datasheetserver follow tutorial instruction pull repository git clone httpsgithubcomforensicarchitecturetimemap install dependency via yarn recommended faster npm yarn npm install run via yarn yarn dev npm run dev run file configjs root directory set config environment variable configmyotherconfigjs yarn dev important although application run like order timemap able display interesting information youll make sure capacity serve data well adjusting configuration parameter see next section running without datasheetserver technically timemap backend agnostic requires series endpoint provide data visualize data expected json format data element required format required field additional endpoint optional enabled simply add feature taste combination data type called domain application context timemap contribute code conduct please read contributing endeavour cultivate community around timemap os forensic architecture inclusive respectful please join u contributing guide learn development process ie propose bugfixes improvement community question want chat please join team fa_open_source keybase community discussion keybase great platform encrypted chat file sharing use public forum license timemap distributed donoharm license
JavaScript ,"This repository is a collection of recent experiments I've been working on in three.js.
Three.js is a JavaScript library built on top of the WebGL graphics language.  WebGL is a low level, verbose language used to create graphics in the browser that can be both very performant and very hard to use.  Three.js greatly reduces the amount of boilerplate code you have to write to build rich 3D graphics, and wraps common operations into intuitive constructor functions.  If you're interested in learning three.js, I recently completed two new tutorials on getting started with the three.js library.  You can find them at loftus.xyz
You will find the different simulations in this repo in the examples folder.  To develop and test out the simulations locally, first clone the repo down to your local machine.
git clone https://github.com/MattLoftus/threejs-space-simulations.git

To avoid cross-origin errors when using textures(every example in this repo), you will need to host the files on a local server.  I recommend using python simple server or npm live server.  First navigate to the root of the directory, then run the following command.
python -m SimpleHTTPServer

This will host the folder on port 8000, so you can head over to the browser and type ""localhost:8000"" into the address bar, and you will see a listing for the directory. If you happen to have a version of python on your machine >= 3.0, you may need to run the following command instead.
python2.7 -m SimpleHTTPServer

",thi repositori is a collect of recent experi ive been work on in threej threej is a javascript librari built on top of the webgl graphic languag webgl is a low level verbos languag use to creat graphic in the browser that can be both veri perform and veri hard to use threej greatli reduc the amount of boilerpl code you have to write to build rich 3d graphic and wrap common oper into intuit constructor function if your interest in learn threej i recent complet two new tutori on get start with the threej librari you can find them at loftusxyz you will find the differ simul in thi repo in the exampl folder to develop and test out the simul local first clone the repo down to your local machin git clone httpsgithubcommattloftusthreejsspacesimulationsgit to avoid crossorigin error when use textureseveri exampl in thi repo you will need to host the file on a local server i recommend use python simpl server or npm live server first navig to the root of the directori then run the follow command python m simplehttpserv thi will host the folder on port 8000 so you can head over to the browser and type localhost8000 into the address bar and you will see a list for the directori if you happen to have a version of python on your machin 30 you may need to run the follow command instead python27 m simplehttpserv,this repository is a collection of recent experiment ive been working on in threejs threejs is a javascript library built on top of the webgl graphic language webgl is a low level verbose language used to create graphic in the browser that can be both very performant and very hard to use threejs greatly reduces the amount of boilerplate code you have to write to build rich 3d graphic and wrap common operation into intuitive constructor function if youre interested in learning threejs i recently completed two new tutorial on getting started with the threejs library you can find them at loftusxyz you will find the different simulation in this repo in the example folder to develop and test out the simulation locally first clone the repo down to your local machine git clone httpsgithubcommattloftusthreejsspacesimulationsgit to avoid crossorigin error when using texturesevery example in this repo you will need to host the file on a local server i recommend using python simple server or npm live server first navigate to the root of the directory then run the following command python m simplehttpserver this will host the folder on port 8000 so you can head over to the browser and type localhost8000 into the address bar and you will see a listing for the directory if you happen to have a version of python on your machine 30 you may need to run the following command instead python27 m simplehttpserver,repository collection recent experiment ive working threejs threejs javascript library built top webgl graphic language webgl low level verbose language used create graphic browser performant hard use threejs greatly reduces amount boilerplate code write build rich 3d graphic wrap common operation intuitive constructor function youre interested learning threejs recently completed two new tutorial getting started threejs library find loftusxyz find different simulation repo example folder develop test simulation locally first clone repo local machine git clone httpsgithubcommattloftusthreejsspacesimulationsgit avoid crossorigin error using texturesevery example repo need host file local server recommend using python simple server npm live server first navigate root directory run following command python simplehttpserver host folder port 8000 head browser type localhost8000 address bar see listing directory happen version python machine 30 may need run following command instead python27 simplehttpserver
JavaScript ,"The Kabal Invasion
The Kabal Invasion is a web-based 4X space game. It is coded in PHP/HTML/JS/SQL.









What is it?
A web based space exploration (4x) game based on the old BBS door games that went
by many names (Tradewars, Galactic Warzone, Ultimate Universe, and
many other games like this) but shares no code with them.  It is
written 100% in PHP/HTML/JS/SQL.

Why should I run this?
Web-based games that recreate the door game BBS experience can be fun!
Since it is Free and open source software, anyone can examine, learn, and contribute.

Is this game ready to install and play?
At the moment, we've identified a number of release-blocking issues including broken
password management, user sign-up, and issues with non-functional database calls. Serious
effort is underway to fix all of these issues, and we are working towards a release. In the meantime,
curious developers are encouraged to download, install, and play as the admin user to find issues
and report them. When we get to a point where the game is stable for players,
we will make an announcement, change this note, and release!

License: Affero GPL v 3
Credits:
The Kabal Invasion forked from Blacknova Traders, please visit their sourceforge page for more information about their project. We proudly stand on the shoulders of giants, with BNT originally having hundreds of developers, players, and admins. We honor and appreciate their 15+ year contribution that makes our project possible.
Requirements:
Server (generally, the most recent/current version of each is our recommendation, but these should suffice):

A Linux server. Our primary development platform is Fedora, but most Linux distributions should work, and potentially even OpenBSD.
A webserver capable of TLS such as apache v2.4+ (we have not determined a required minimum).
php v7.4.5+ (needed for type-hinting property types).
mariadb v5.5+ or v10.0+ (needed for utf8mb4 schemas).
pdo PHP extension.

Web:

Chrome v50+ or Firefox v40+ (recommended).
Safari v9.1.2+.
IE v11.

Notes:

TKI will likely run on lighttpd and nginix, however htaccess will not work out of the box - potentially causing security risks. It has not been tested on either.
IIS and/or Windows is NOT supported, please do not ask! (But we welcome code to make it work on IIS)
Development ""Snapshots"" are intended only for developers that are actively involved in the development process, and require additional effort to work (composer, etc).
We make use of Smarty templates, HTML Purifier, Swiftmailer, and Adodb (although we are working to replace adodb with PDO).

Installation:
Please see the /docs/install.md file.
Upgrades:
As is typical with our releases, we highly recommend a fresh install. Upgrades are not supported at this time.
Code quality:
The project began in the early PHP4 era, and as a result, is less than ideal. Substantial progress has been made towards modernization, and we are continuing that process. As a general guideline, we follow PSR-1,2,4, and the upcoming 12, with the major exceptions that we use BSD/Allman brace/brackets and do not yet follow line length limits. Feedback and PR's are welcome and appreciated.
Critical needs:
The two areas we need the most focus in would be the documentation, and testing. Both can be done with little or no knowledge of PHP, and would help us dramatically.
Security issue reporting:
Please report all security issues to thekabal@gmail.com.
Contributing:
Feel free to contribute to the project! We use Gitlab for our issues tracking (provide feedback!), milestone planning, code storage, and releases.
I hope you enjoy the game!
The Kabal
",the kabal invas the kabal invas is a webbas 4x space game it is code in phphtmljssql what is it a web base space explor 4x game base on the old bb door game that went by mani name tradewar galact warzon ultim univers and mani other game like thi but share no code with them it is written 100 in phphtmljssql whi should i run thi webbas game that recreat the door game bb experi can be fun sinc it is free and open sourc softwar anyon can examin learn and contribut is thi game readi to instal and play at the moment weve identifi a number of releaseblock issu includ broken password manag user signup and issu with nonfunct databas call seriou effort is underway to fix all of these issu and we are work toward a releas in the meantim curiou develop are encourag to download instal and play as the admin user to find issu and report them when we get to a point where the game is stabl for player we will make an announc chang thi note and releas licens affero gpl v 3 credit the kabal invas fork from blacknova trader pleas visit their sourceforg page for more inform about their project we proudli stand on the shoulder of giant with bnt origin have hundr of develop player and admin we honor and appreci their 15 year contribut that make our project possibl requir server gener the most recentcurr version of each is our recommend but these should suffic a linux server our primari develop platform is fedora but most linux distribut should work and potenti even openbsd a webserv capabl of tl such as apach v24 we have not determin a requir minimum php v745 need for typehint properti type mariadb v55 or v100 need for utf8mb4 schema pdo php extens web chrome v50 or firefox v40 recommend safari v912 ie v11 note tki will like run on lighttpd and nginix howev htaccess will not work out of the box potenti caus secur risk it ha not been test on either ii andor window is not support pleas do not ask but we welcom code to make it work on ii develop snapshot are intend onli for develop that are activ involv in the develop process and requir addit effort to work compos etc we make use of smarti templat html purifi swiftmail and adodb although we are work to replac adodb with pdo instal pleas see the docsinstallmd file upgrad as is typic with our releas we highli recommend a fresh instal upgrad are not support at thi time code qualiti the project began in the earli php4 era and as a result is less than ideal substanti progress ha been made toward modern and we are continu that process as a gener guidelin we follow psr124 and the upcom 12 with the major except that we use bsdallman bracebracket and do not yet follow line length limit feedback and pr are welcom and appreci critic need the two area we need the most focu in would be the document and test both can be done with littl or no knowledg of php and would help us dramat secur issu report pleas report all secur issu to thekabalgmailcom contribut feel free to contribut to the project we use gitlab for our issu track provid feedback mileston plan code storag and releas i hope you enjoy the game the kabal,the kabal invasion the kabal invasion is a webbased 4x space game it is coded in phphtmljssql what is it a web based space exploration 4x game based on the old bb door game that went by many name tradewars galactic warzone ultimate universe and many other game like this but share no code with them it is written 100 in phphtmljssql why should i run this webbased game that recreate the door game bb experience can be fun since it is free and open source software anyone can examine learn and contribute is this game ready to install and play at the moment weve identified a number of releaseblocking issue including broken password management user signup and issue with nonfunctional database call serious effort is underway to fix all of these issue and we are working towards a release in the meantime curious developer are encouraged to download install and play a the admin user to find issue and report them when we get to a point where the game is stable for player we will make an announcement change this note and release license affero gpl v 3 credit the kabal invasion forked from blacknova trader please visit their sourceforge page for more information about their project we proudly stand on the shoulder of giant with bnt originally having hundred of developer player and admins we honor and appreciate their 15 year contribution that make our project possible requirement server generally the most recentcurrent version of each is our recommendation but these should suffice a linux server our primary development platform is fedora but most linux distribution should work and potentially even openbsd a webserver capable of tl such a apache v24 we have not determined a required minimum php v745 needed for typehinting property type mariadb v55 or v100 needed for utf8mb4 schema pdo php extension web chrome v50 or firefox v40 recommended safari v912 ie v11 note tki will likely run on lighttpd and nginix however htaccess will not work out of the box potentially causing security risk it ha not been tested on either ii andor window is not supported please do not ask but we welcome code to make it work on ii development snapshot are intended only for developer that are actively involved in the development process and require additional effort to work composer etc we make use of smarty template html purifier swiftmailer and adodb although we are working to replace adodb with pdo installation please see the docsinstallmd file upgrade a is typical with our release we highly recommend a fresh install upgrade are not supported at this time code quality the project began in the early php4 era and a a result is le than ideal substantial progress ha been made towards modernization and we are continuing that process a a general guideline we follow psr124 and the upcoming 12 with the major exception that we use bsdallman bracebrackets and do not yet follow line length limit feedback and pr are welcome and appreciated critical need the two area we need the most focus in would be the documentation and testing both can be done with little or no knowledge of php and would help u dramatically security issue reporting please report all security issue to thekabalgmailcom contributing feel free to contribute to the project we use gitlab for our issue tracking provide feedback milestone planning code storage and release i hope you enjoy the game the kabal,kabal invasion kabal invasion webbased 4x space game coded phphtmljssql web based space exploration 4x game based old bb door game went many name tradewars galactic warzone ultimate universe many game like share code written 100 phphtmljssql run webbased game recreate door game bb experience fun since free open source software anyone examine learn contribute game ready install play moment weve identified number releaseblocking issue including broken password management user signup issue nonfunctional database call serious effort underway fix issue working towards release meantime curious developer encouraged download install play admin user find issue report get point game stable player make announcement change note release license affero gpl v 3 credit kabal invasion forked blacknova trader please visit sourceforge page information project proudly stand shoulder giant bnt originally hundred developer player admins honor appreciate 15 year contribution make project possible requirement server generally recentcurrent version recommendation suffice linux server primary development platform fedora linux distribution work potentially even openbsd webserver capable tl apache v24 determined required minimum php v745 needed typehinting property type mariadb v55 v100 needed utf8mb4 schema pdo php extension web chrome v50 firefox v40 recommended safari v912 ie v11 note tki likely run lighttpd nginix however htaccess work box potentially causing security risk tested either ii andor window supported please ask welcome code make work ii development snapshot intended developer actively involved development process require additional effort work composer etc make use smarty template html purifier swiftmailer adodb although working replace adodb pdo installation please see docsinstallmd file upgrade typical release highly recommend fresh install upgrade supported time code quality project began early php4 era result le ideal substantial progress made towards modernization continuing process general guideline follow psr124 upcoming 12 major exception use bsdallman bracebrackets yet follow line length limit feedback pr welcome appreciated critical need two area need focus would documentation testing done little knowledge php would help u dramatically security issue reporting please report security issue thekabalgmailcom contributing feel free contribute project use gitlab issue tracking provide feedback milestone planning code storage release hope enjoy game kabal
JavaScript ,"VAE Latent Space Explorer
This application is a toy visualization that allows you to generate new images of 28x28 numerical digits using a variational autoencoder.
You can view the visualization here
Implementation details
The variational autoencoder was implemented using Keras and the relevant code is located in the scripts directory in a Jupyter notebook.
Once the model is trained, the architecture and weights are saved in a format that can be ingested by tensorflow-js. Tensorflow-js handles
implementing the model architecture and loading the weight in the browser. By leveraging WebGL, the model can efficient generate new image samples when given an appropriate latent space vector in the client browser.
The application uses React.js for interface updates along with html Canvas to draw the image matrices. D3.js was used to generate the scatterplot and handle hover events.
Created by Taylor Denouden (April 2018)
",vae latent space explor thi applic is a toy visual that allow you to gener new imag of 28x28 numer digit use a variat autoencod you can view the visual here implement detail the variat autoencod wa implement use kera and the relev code is locat in the script directori in a jupyt notebook onc the model is train the architectur and weight are save in a format that can be ingest by tensorflowj tensorflowj handl implement the model architectur and load the weight in the browser by leverag webgl the model can effici gener new imag sampl when given an appropri latent space vector in the client browser the applic use reactj for interfac updat along with html canva to draw the imag matric d3j wa use to gener the scatterplot and handl hover event creat by taylor denouden april 2018,vae latent space explorer this application is a toy visualization that allows you to generate new image of 28x28 numerical digit using a variational autoencoder you can view the visualization here implementation detail the variational autoencoder wa implemented using kera and the relevant code is located in the script directory in a jupyter notebook once the model is trained the architecture and weight are saved in a format that can be ingested by tensorflowjs tensorflowjs handle implementing the model architecture and loading the weight in the browser by leveraging webgl the model can efficient generate new image sample when given an appropriate latent space vector in the client browser the application us reactjs for interface update along with html canvas to draw the image matrix d3js wa used to generate the scatterplot and handle hover event created by taylor denouden april 2018,vae latent space explorer application toy visualization allows generate new image 28x28 numerical digit using variational autoencoder view visualization implementation detail variational autoencoder implemented using kera relevant code located script directory jupyter notebook model trained architecture weight saved format ingested tensorflowjs tensorflowjs handle implementing model architecture loading weight browser leveraging webgl model efficient generate new image sample given appropriate latent space vector client browser application us reactjs interface update along html canvas draw image matrix d3js used generate scatterplot handle hover event created taylor denouden april 2018
JavaScript ,"SEH - Space Exploration History
The Solar System
Behold: the entire history of Solar System exploration in one graphic.
Showing all missions beyond Earth orbit send to explore Solar System objects since 1958.
Some more documentary listings/garaphics:
Ground Segment Map overwiew
Missions, Destinations, Launch sites Lists
Rockets in one graphic in order of appearance
Space Telescopes
Extensive description here
Spectral Range of all operational and future space telescopes and some ground based examples for comparison
Spectral Range vs. Angular Resolution for operational scopes
The apps require a HTML5-capable browser, so all fairly new versions of Chrome, Firefox, IE, Opera and what-have-you should work. If your browser only shows a static image, it is too old.
Sources
These sites (and books) helped me greatly to gather all the data represented here:
National Space Science Data Center (NSSDC)
NASA History Astronautics and Aeronautics Chronology Series
Solar System Exploration Mission Profiles
JPL Mission and Spacecraft Library
Encyclopedia Astronautica, Mark Wade
Jonathan's Space Report, Johnathan McDowell
Spaceflight Realtime Simulations and Information, Daniel Muller
Earth Observation Portal, ESA
Gunter's Space Page, Gunter Krebs
Johnston's Archive, Robert Johnston
The Planetary Society, Emily Lakdawalla et.al.
Venera: The Soviet Exploration of Venus, Don P. Mitchell
Russian Space Web, Anatoly Zak
Spaceflight 101, Patrick Blau
Zarya Soviet, Russian and International Space Flight, Robert Christy
Visual Satellite Observing FAQ
Weebau Space Encyclopedia, Pierre Bauduin
and of course all the mission websites linked in the app above.
Images
The Planetary Society, planet images
Planetary Maps, Steve Albers
NASA Visible Earth
Moon shaded relief
Mars MOLA map
Dr. Paul Schenk (Neptune Rings)
Historic Spacecraft, Richard Kruse (Rocket images)
Solar System Data
Planetary Fact Sheets (NSSDC)
Solar System Dynamics (JPL)
The Astronomical Almanac Online (USN)
Gazetteer of Planetary Nomenclature (USGS)
Minor Planet Center (IAU)
Planetary Data System (PDS) - Rings node -
Atmospheres node
Books
Deep Space Chronicle, Asif Siddiqi, NASA History Monograph SP 2002-4524
Soviet Robots in the Solar System, Wesley T. Huntress, Jr. & Mikhail Ya. Marov, Springer 2011
--
Spacecraft positions algorithm with great help from Project Pluto
Released under BSD License
",seh space explor histori the solar system behold the entir histori of solar system explor in one graphic show all mission beyond earth orbit send to explor solar system object sinc 1958 some more documentari listingsgaraph ground segment map overwiew mission destin launch site list rocket in one graphic in order of appear space telescop extens descript here spectral rang of all oper and futur space telescop and some ground base exampl for comparison spectral rang vs angular resolut for oper scope the app requir a html5capabl browser so all fairli new version of chrome firefox ie opera and whathavey should work if your browser onli show a static imag it is too old sourc these site and book help me greatli to gather all the data repres here nation space scienc data center nssdc nasa histori astronaut and aeronaut chronolog seri solar system explor mission profil jpl mission and spacecraft librari encyclopedia astronautica mark wade jonathan space report johnathan mcdowel spaceflight realtim simul and inform daniel muller earth observ portal esa gunter space page gunter kreb johnston archiv robert johnston the planetari societi emili lakdawalla etal venera the soviet explor of venu don p mitchel russian space web anatoli zak spaceflight 101 patrick blau zarya soviet russian and intern space flight robert christi visual satellit observ faq weebau space encyclopedia pierr bauduin and of cours all the mission websit link in the app abov imag the planetari societi planet imag planetari map steve alber nasa visibl earth moon shade relief mar mola map dr paul schenk neptun ring histor spacecraft richard kruse rocket imag solar system data planetari fact sheet nssdc solar system dynam jpl the astronom almanac onlin usn gazett of planetari nomenclatur usg minor planet center iau planetari data system pd ring node atmospher node book deep space chronicl asif siddiqi nasa histori monograph sp 20024524 soviet robot in the solar system wesley t huntress jr mikhail ya marov springer 2011 spacecraft posit algorithm with great help from project pluto releas under bsd licens,seh space exploration history the solar system behold the entire history of solar system exploration in one graphic showing all mission beyond earth orbit send to explore solar system object since 1958 some more documentary listingsgaraphics ground segment map overwiew mission destination launch site list rocket in one graphic in order of appearance space telescope extensive description here spectral range of all operational and future space telescope and some ground based example for comparison spectral range v angular resolution for operational scope the apps require a html5capable browser so all fairly new version of chrome firefox ie opera and whathaveyou should work if your browser only show a static image it is too old source these site and book helped me greatly to gather all the data represented here national space science data center nssdc nasa history astronautics and aeronautics chronology series solar system exploration mission profile jpl mission and spacecraft library encyclopedia astronautica mark wade jonathan space report johnathan mcdowell spaceflight realtime simulation and information daniel muller earth observation portal esa gunters space page gunter krebs johnston archive robert johnston the planetary society emily lakdawalla etal venera the soviet exploration of venus don p mitchell russian space web anatoly zak spaceflight 101 patrick blau zarya soviet russian and international space flight robert christy visual satellite observing faq weebau space encyclopedia pierre bauduin and of course all the mission website linked in the app above image the planetary society planet image planetary map steve albers nasa visible earth moon shaded relief mar mola map dr paul schenk neptune ring historic spacecraft richard kruse rocket image solar system data planetary fact sheet nssdc solar system dynamic jpl the astronomical almanac online usn gazetteer of planetary nomenclature usgs minor planet center iau planetary data system pd ring node atmosphere node book deep space chronicle asif siddiqi nasa history monograph sp 20024524 soviet robot in the solar system wesley t huntress jr mikhail ya marov springer 2011 spacecraft position algorithm with great help from project pluto released under bsd license,seh space exploration history solar system behold entire history solar system exploration one graphic showing mission beyond earth orbit send explore solar system object since 1958 documentary listingsgaraphics ground segment map overwiew mission destination launch site list rocket one graphic order appearance space telescope extensive description spectral range operational future space telescope ground based example comparison spectral range v angular resolution operational scope apps require html5capable browser fairly new version chrome firefox ie opera whathaveyou work browser show static image old source site book helped greatly gather data represented national space science data center nssdc nasa history astronautics aeronautics chronology series solar system exploration mission profile jpl mission spacecraft library encyclopedia astronautica mark wade jonathan space report johnathan mcdowell spaceflight realtime simulation information daniel muller earth observation portal esa gunters space page gunter krebs johnston archive robert johnston planetary society emily lakdawalla etal venera soviet exploration venus p mitchell russian space web anatoly zak spaceflight 101 patrick blau zarya soviet russian international space flight robert christy visual satellite observing faq weebau space encyclopedia pierre bauduin course mission website linked app image planetary society planet image planetary map steve albers nasa visible earth moon shaded relief mar mola map dr paul schenk neptune ring historic spacecraft richard kruse rocket image solar system data planetary fact sheet nssdc solar system dynamic jpl astronomical almanac online usn gazetteer planetary nomenclature usgs minor planet center iau planetary data system pd ring node atmosphere node book deep space chronicle asif siddiqi nasa history monograph sp 20024524 soviet robot solar system wesley huntress jr mikhail ya marov springer 2011 spacecraft position algorithm great help project pluto released bsd license
JavaScript ,"SpaceApps2020 WINNER - Space Nearby

Challenge
An interactive webapp using Google Maps API to reveal locations involved in the space exploration industry nearby!
Note: Google API keys have been temporarily disabled.
Developers:
Jared Bentvelsen


Bassel Rezkalla


Matthew McCracken


Christopher Andrade


Yuvraj Randhawa


",spaceapps2020 winner space nearbi challeng an interact webapp use googl map api to reveal locat involv in the space explor industri nearbi note googl api key have been temporarili disabl develop jare bentvelsen bassel rezkalla matthew mccracken christoph andrad yuvraj randhawa,spaceapps2020 winner space nearby challenge an interactive webapp using google map api to reveal location involved in the space exploration industry nearby note google api key have been temporarily disabled developer jared bentvelsen bassel rezkalla matthew mccracken christopher andrade yuvraj randhawa,spaceapps2020 winner space nearby challenge interactive webapp using google map api reveal location involved space exploration industry nearby note google api key temporarily disabled developer jared bentvelsen bassel rezkalla matthew mccracken christopher andrade yuvraj randhawa
JavaScript ,"Space exploration web app
http://compact.github.io/space/
This app allows you to navigate through space in your browser. It uses three.js to render astronomical bodies.
Workflow
Develop
npm install
bower install
node app.js

Go to http://localhost:3001.
Build
npm install -g grunt
grunt build

The built files are located in /dist/.
Generate docs
npm install -g jsdoc
jsdoc -p -d app/docs app/scripts/ app/data/

",space explor web app httpcompactgithubiospac thi app allow you to navig through space in your browser it use threej to render astronom bodi workflow develop npm instal bower instal node appj go to httplocalhost3001 build npm instal g grunt grunt build the built file are locat in dist gener doc npm instal g jsdoc jsdoc p d appdoc appscript appdata,space exploration web app httpcompactgithubiospace this app allows you to navigate through space in your browser it us threejs to render astronomical body workflow develop npm install bower install node appjs go to httplocalhost3001 build npm install g grunt grunt build the built file are located in dist generate doc npm install g jsdoc jsdoc p d appdocs appscripts appdata,space exploration web app httpcompactgithubiospace app allows navigate space browser us threejs render astronomical body workflow develop npm install bower install node appjs go httplocalhost3001 build npm install g grunt grunt build built file located dist generate doc npm install g jsdoc jsdoc p appdocs appscripts appdata
JavaScript ,"Space exploration web app
http://compact.github.io/space/
This app allows you to navigate through space in your browser. It uses three.js to render astronomical bodies.
Workflow
Develop
npm install
bower install
node app.js

Go to http://localhost:3001.
Build
npm install -g grunt
grunt build

The built files are located in /dist/.
Generate docs
npm install -g jsdoc
jsdoc -p -d app/docs app/scripts/ app/data/

",space explor web app httpcompactgithubiospac thi app allow you to navig through space in your browser it use threej to render astronom bodi workflow develop npm instal bower instal node appj go to httplocalhost3001 build npm instal g grunt grunt build the built file are locat in dist gener doc npm instal g jsdoc jsdoc p d appdoc appscript appdata,space exploration web app httpcompactgithubiospace this app allows you to navigate through space in your browser it us threejs to render astronomical body workflow develop npm install bower install node appjs go to httplocalhost3001 build npm install g grunt grunt build the built file are located in dist generate doc npm install g jsdoc jsdoc p d appdocs appscripts appdata,space exploration web app httpcompactgithubiospace app allows navigate space browser us threejs render astronomical body workflow develop npm install bower install node appjs go httplocalhost3001 build npm install g grunt grunt build built file located dist generate doc npm install g jsdoc jsdoc p appdocs appscripts appdata
JavaScript ,"Wandering Lines
An exploration of lines wandering across a space. Inspired from the work of Anders Hoff, @inconvergent.
Running locally
Make sure npm is installed, download this repo, then from the folder in the command line run npm install to install the dependencies, then run npm run dev:1 to start a local reload server. Then to finish and build the final files run npm run build:1. Replace the 1 in dev:1 and build:1 to run and build each version in the series.
",wander line an explor of line wander across a space inspir from the work of ander hoff inconverg run local make sure npm is instal download thi repo then from the folder in the command line run npm instal to instal the depend then run npm run dev1 to start a local reload server then to finish and build the final file run npm run build1 replac the 1 in dev1 and build1 to run and build each version in the seri,wandering line an exploration of line wandering across a space inspired from the work of anders hoff inconvergent running locally make sure npm is installed download this repo then from the folder in the command line run npm install to install the dependency then run npm run dev1 to start a local reload server then to finish and build the final file run npm run build1 replace the 1 in dev1 and build1 to run and build each version in the series,wandering line exploration line wandering across space inspired work anders hoff inconvergent running locally make sure npm installed download repo folder command line run npm install install dependency run npm run dev1 start local reload server finish build final file run npm run build1 replace 1 dev1 build1 run build version series
JavaScript ,"Rogue Starfarer
Rogue Starfarer is interstellar exploration roguelike created for the 2018 7DRL competition.
How to Build
Just open up index.html in your browser.
How to Play
The green 'X' indicates the position that your ship will move to in the next turn, based on your momentum. You can fire your rockets to plot your course by moving the green 'X' with the arrow keys or numpad. The '0' indicates your course if you make no adjustments. Press space to end the turn.
Beware that the amount you can adjust your ship's course in any one turn is limited by the energy reserves and propulsion of your ship! The Maneuver: -3/Δ on the sidebar indicates that adjusting your trajectory will cost you 3 energy. The more weapons and armor you add to you ship, the more it will cost to maneuver, and the more you upgrade your engines, the less it will cost.
The green arrow indicates the the direction your ship is currently facing. Your ship's facing changes automatically based on your trajectory.
Weapons are mounted on particular hull zones and each weapon can only fire out of that hull zone. Each weapon has its own firing arc determined by its range, hull zone, and your ship's facing. Press 'w' to cycle through weapons, press 'tab' to cycle through eligible targets within the weapon's arc, and press 'f' to fire. You can also target and fire on ships using the mouse.
Exploration
The universe is filled with strange events and mysterious sites to explore. You can land on any planet by flying over it and coming to a stop. If you collide with a planet while traveling faster than speed 1, you will suffer damage. You can board a destroyed or abandoned ship by flying over it. Note that destroyed ships will continue on their original trajectories, so you may find that a tractor beam is helpful to bring them to a stop.
You will also find perplexing anomalies in space and merchant stations that can repair and refit your ship. You can interact with these by flying over them.
Ship Systems
Reactor and Energy Storage
Your ship has a state-of-the-art fusion reactor that continously produces energy which is stored in your ship's capacitor banks. Maneuvering and firing weapons consumes energy. If your current energy is at least 50% of maximum, your shields and warp core will slowly recharge. In normal circumstances your energy reserves will never exceed your maximum. However, if any special item or effect raises your energy reserves above twice your maximum, your reactor will overload and begin to melt down, dealing 1 point of hull damage per turn.
Crew
Your ship has a minimum and maximum crew. For every crewmember you have above the minimum, there is a chance every turn that you will repair 1 point of hull damage. For every crewmember you have below the minimum, there is a chance that your reactor will produce no energy that turn.
Warp Core
Your ship has a warp core which allows it to jump between star systems. To do so you must have the hyperspace coordinates for the new system and your warp core must be fully charged (20/20). Your warp core will recharge 1 point per turn while your ship's energy reserves are above 50%. Jump to hyperspace by pressing the 'j' key.
Shields
Your shields reduce damage from most types of attacks. Your shields will naturally recharge when you are above 50% of your maximum energy.
Weapon Types
There are four main types of weapons: lasers, ion cannons, tractor beams, and neutron beams. Lasers deal damage to shields first, and to hull second. Ion cannons deal damage to shields first and to energy reserves second. Note that ion cannons can reduce a ship's energy reserves to negative levels. Tractor beams reduce a ship's speed, but only work if the target's shields are already down. Neutron beams deal damage directly to a ship's crew, but like tractor beams they are completely blocked by any amount of shields.
Each weapon can only fire once per turn, and only if sufficient energy is available. Mounting a weapon increases your ship's mass (increasing your maneuver cost) and increases your minimum crew requirement. Beyond this, there are no restrictions to the number of weapons you can affix to a hull zone.
Rumors of persist of ancient precusor relics that are far more powerful than the standard types of weapons listed above.
Boarding Tubes
Boarding tubes can bridge the vacuum of space and slice through even the thickest armor. If you and an enemy ship are adjacent, and either ship is at 0 speed, a boarding action will be initiated. Your crewmemebers will fight directly, though an entire shipboard battle can rarely be resolved in a single turn.
",rogu starfar rogu starfar is interstellar explor roguelik creat for the 2018 7drl competit how to build just open up indexhtml in your browser how to play the green x indic the posit that your ship will move to in the next turn base on your momentum you can fire your rocket to plot your cours by move the green x with the arrow key or numpad the 0 indic your cours if you make no adjust press space to end the turn bewar that the amount you can adjust your ship cours in ani one turn is limit by the energi reserv and propuls of your ship the maneuv 3 on the sidebar indic that adjust your trajectori will cost you 3 energi the more weapon and armor you add to you ship the more it will cost to maneuv and the more you upgrad your engin the less it will cost the green arrow indic the the direct your ship is current face your ship face chang automat base on your trajectori weapon are mount on particular hull zone and each weapon can onli fire out of that hull zone each weapon ha it own fire arc determin by it rang hull zone and your ship face press w to cycl through weapon press tab to cycl through elig target within the weapon arc and press f to fire you can also target and fire on ship use the mous explor the univers is fill with strang event and mysteri site to explor you can land on ani planet by fli over it and come to a stop if you collid with a planet while travel faster than speed 1 you will suffer damag you can board a destroy or abandon ship by fli over it note that destroy ship will continu on their origin trajectori so you may find that a tractor beam is help to bring them to a stop you will also find perplex anomali in space and merchant station that can repair and refit your ship you can interact with these by fli over them ship system reactor and energi storag your ship ha a stateoftheart fusion reactor that contin produc energi which is store in your ship capacitor bank maneuv and fire weapon consum energi if your current energi is at least 50 of maximum your shield and warp core will slowli recharg in normal circumst your energi reserv will never exceed your maximum howev if ani special item or effect rais your energi reserv abov twice your maximum your reactor will overload and begin to melt down deal 1 point of hull damag per turn crew your ship ha a minimum and maximum crew for everi crewmemb you have abov the minimum there is a chanc everi turn that you will repair 1 point of hull damag for everi crewmemb you have below the minimum there is a chanc that your reactor will produc no energi that turn warp core your ship ha a warp core which allow it to jump between star system to do so you must have the hyperspac coordin for the new system and your warp core must be fulli charg 2020 your warp core will recharg 1 point per turn while your ship energi reserv are abov 50 jump to hyperspac by press the j key shield your shield reduc damag from most type of attack your shield will natur recharg when you are abov 50 of your maximum energi weapon type there are four main type of weapon laser ion cannon tractor beam and neutron beam laser deal damag to shield first and to hull second ion cannon deal damag to shield first and to energi reserv second note that ion cannon can reduc a ship energi reserv to neg level tractor beam reduc a ship speed but onli work if the target shield are alreadi down neutron beam deal damag directli to a ship crew but like tractor beam they are complet block by ani amount of shield each weapon can onli fire onc per turn and onli if suffici energi is avail mount a weapon increas your ship mass increas your maneuv cost and increas your minimum crew requir beyond thi there are no restrict to the number of weapon you can affix to a hull zone rumor of persist of ancient precusor relic that are far more power than the standard type of weapon list abov board tube board tube can bridg the vacuum of space and slice through even the thickest armor if you and an enemi ship are adjac and either ship is at 0 speed a board action will be initi your crewmemeb will fight directli though an entir shipboard battl can rare be resolv in a singl turn,rogue starfarer rogue starfarer is interstellar exploration roguelike created for the 2018 7drl competition how to build just open up indexhtml in your browser how to play the green x indicates the position that your ship will move to in the next turn based on your momentum you can fire your rocket to plot your course by moving the green x with the arrow key or numpad the 0 indicates your course if you make no adjustment press space to end the turn beware that the amount you can adjust your ship course in any one turn is limited by the energy reserve and propulsion of your ship the maneuver 3 on the sidebar indicates that adjusting your trajectory will cost you 3 energy the more weapon and armor you add to you ship the more it will cost to maneuver and the more you upgrade your engine the le it will cost the green arrow indicates the the direction your ship is currently facing your ship facing change automatically based on your trajectory weapon are mounted on particular hull zone and each weapon can only fire out of that hull zone each weapon ha it own firing arc determined by it range hull zone and your ship facing press w to cycle through weapon press tab to cycle through eligible target within the weapon arc and press f to fire you can also target and fire on ship using the mouse exploration the universe is filled with strange event and mysterious site to explore you can land on any planet by flying over it and coming to a stop if you collide with a planet while traveling faster than speed 1 you will suffer damage you can board a destroyed or abandoned ship by flying over it note that destroyed ship will continue on their original trajectory so you may find that a tractor beam is helpful to bring them to a stop you will also find perplexing anomaly in space and merchant station that can repair and refit your ship you can interact with these by flying over them ship system reactor and energy storage your ship ha a stateoftheart fusion reactor that continously produce energy which is stored in your ship capacitor bank maneuvering and firing weapon consumes energy if your current energy is at least 50 of maximum your shield and warp core will slowly recharge in normal circumstance your energy reserve will never exceed your maximum however if any special item or effect raise your energy reserve above twice your maximum your reactor will overload and begin to melt down dealing 1 point of hull damage per turn crew your ship ha a minimum and maximum crew for every crewmember you have above the minimum there is a chance every turn that you will repair 1 point of hull damage for every crewmember you have below the minimum there is a chance that your reactor will produce no energy that turn warp core your ship ha a warp core which allows it to jump between star system to do so you must have the hyperspace coordinate for the new system and your warp core must be fully charged 2020 your warp core will recharge 1 point per turn while your ship energy reserve are above 50 jump to hyperspace by pressing the j key shield your shield reduce damage from most type of attack your shield will naturally recharge when you are above 50 of your maximum energy weapon type there are four main type of weapon laser ion cannon tractor beam and neutron beam laser deal damage to shield first and to hull second ion cannon deal damage to shield first and to energy reserve second note that ion cannon can reduce a ship energy reserve to negative level tractor beam reduce a ship speed but only work if the target shield are already down neutron beam deal damage directly to a ship crew but like tractor beam they are completely blocked by any amount of shield each weapon can only fire once per turn and only if sufficient energy is available mounting a weapon increase your ship mass increasing your maneuver cost and increase your minimum crew requirement beyond this there are no restriction to the number of weapon you can affix to a hull zone rumor of persist of ancient precusor relic that are far more powerful than the standard type of weapon listed above boarding tube boarding tube can bridge the vacuum of space and slice through even the thickest armor if you and an enemy ship are adjacent and either ship is at 0 speed a boarding action will be initiated your crewmemebers will fight directly though an entire shipboard battle can rarely be resolved in a single turn,rogue starfarer rogue starfarer interstellar exploration roguelike created 2018 7drl competition build open indexhtml browser play green x indicates position ship move next turn based momentum fire rocket plot course moving green x arrow key numpad 0 indicates course make adjustment press space end turn beware amount adjust ship course one turn limited energy reserve propulsion ship maneuver 3 sidebar indicates adjusting trajectory cost 3 energy weapon armor add ship cost maneuver upgrade engine le cost green arrow indicates direction ship currently facing ship facing change automatically based trajectory weapon mounted particular hull zone weapon fire hull zone weapon firing arc determined range hull zone ship facing press w cycle weapon press tab cycle eligible target within weapon arc press f fire also target fire ship using mouse exploration universe filled strange event mysterious site explore land planet flying coming stop collide planet traveling faster speed 1 suffer damage board destroyed abandoned ship flying note destroyed ship continue original trajectory may find tractor beam helpful bring stop also find perplexing anomaly space merchant station repair refit ship interact flying ship system reactor energy storage ship stateoftheart fusion reactor continously produce energy stored ship capacitor bank maneuvering firing weapon consumes energy current energy least 50 maximum shield warp core slowly recharge normal circumstance energy reserve never exceed maximum however special item effect raise energy reserve twice maximum reactor overload begin melt dealing 1 point hull damage per turn crew ship minimum maximum crew every crewmember minimum chance every turn repair 1 point hull damage every crewmember minimum chance reactor produce energy turn warp core ship warp core allows jump star system must hyperspace coordinate new system warp core must fully charged 2020 warp core recharge 1 point per turn ship energy reserve 50 jump hyperspace pressing j key shield shield reduce damage type attack shield naturally recharge 50 maximum energy weapon type four main type weapon laser ion cannon tractor beam neutron beam laser deal damage shield first hull second ion cannon deal damage shield first energy reserve second note ion cannon reduce ship energy reserve negative level tractor beam reduce ship speed work target shield already neutron beam deal damage directly ship crew like tractor beam completely blocked amount shield weapon fire per turn sufficient energy available mounting weapon increase ship mass increasing maneuver cost increase minimum crew requirement beyond restriction number weapon affix hull zone rumor persist ancient precusor relic far powerful standard type weapon listed boarding tube boarding tube bridge vacuum space slice even thickest armor enemy ship adjacent either ship 0 speed boarding action initiated crewmemebers fight directly though entire shipboard battle rarely resolved single turn
JavaScript ,"Opentrons Platform




Overview
Opentrons API
Opentrons App
Contributing

Overview
Opentrons makes robots for biologists.
Our mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other's work. Our robots automate experiments that would otherwise be done by hand, allowing our users to spend more time pursuing answers to the 21st century’s most important questions, and less time pipetting.
This repository contains the source code for the Opentrons API and OT App. We'd love for you to to explore, hack, and build upon them!
Opentrons API
The Opentrons API is a simple framework designed to make writing automated biology lab protocols easy.
We've designed it in a way we hope is accessible to anyone with basic computer and wetlab skills. As a bench scientist, you should be able to code your automated protocols in a way that reads like a lab notebook.
pipette.aspirate(location=trough['A1'], volume=30)
pipette.dispense(location=well_plate['A1'], volume=30)
This example tells the Opentrons OT-2 to pipette 30 µL of liquid from a trough to well plate. Learn more here:

Documentation
Source code

Opentrons App
Easily upload a protocol, calibrate positions, and run your experiment from your computer.

Download Here
Documentation
Source code


Opentrons Protocol Designer
Easily create a protocol to run on your robot with this graphical tool.

Access Here
Documentation
Source code

Contributing
We love contributors! Here is the best way to work with us:


Filing a bug report. We will fix these as quickly as we can, and appreciate your help uncovering bugs in our code.


Submit a pull request with any new features you've added to a branch of the API or App. We will reach out to talk with you about integration testing and launching it into our product!


For more information and development setup instructions, please read the contributing guide.
Enjoy!
",opentron platform overview opentron api opentron app contribut overview opentron make robot for biologist our mission is to provid the scientif commun with a common platform to easili share protocol and reproduc each other work our robot autom experi that would otherwis be done by hand allow our user to spend more time pursu answer to the 21st centuri most import question and less time pipet thi repositori contain the sourc code for the opentron api and ot app wed love for you to to explor hack and build upon them opentron api the opentron api is a simpl framework design to make write autom biolog lab protocol easi weve design it in a way we hope is access to anyon with basic comput and wetlab skill as a bench scientist you should be abl to code your autom protocol in a way that read like a lab notebook pipetteaspiratelocationtrougha1 volume30 pipettedispenselocationwell_platea1 volume30 thi exampl tell the opentron ot2 to pipett 30 l of liquid from a trough to well plate learn more here document sourc code opentron app easili upload a protocol calibr posit and run your experi from your comput download here document sourc code opentron protocol design easili creat a protocol to run on your robot with thi graphic tool access here document sourc code contribut we love contributor here is the best way to work with us file a bug report we will fix these as quickli as we can and appreci your help uncov bug in our code submit a pull request with ani new featur youv ad to a branch of the api or app we will reach out to talk with you about integr test and launch it into our product for more inform and develop setup instruct pleas read the contribut guid enjoy,opentrons platform overview opentrons api opentrons app contributing overview opentrons make robot for biologist our mission is to provide the scientific community with a common platform to easily share protocol and reproduce each others work our robot automate experiment that would otherwise be done by hand allowing our user to spend more time pursuing answer to the 21st century most important question and le time pipetting this repository contains the source code for the opentrons api and ot app wed love for you to to explore hack and build upon them opentrons api the opentrons api is a simple framework designed to make writing automated biology lab protocol easy weve designed it in a way we hope is accessible to anyone with basic computer and wetlab skill a a bench scientist you should be able to code your automated protocol in a way that read like a lab notebook pipetteaspiratelocationtrougha1 volume30 pipettedispenselocationwell_platea1 volume30 this example tell the opentrons ot2 to pipette 30 l of liquid from a trough to well plate learn more here documentation source code opentrons app easily upload a protocol calibrate position and run your experiment from your computer download here documentation source code opentrons protocol designer easily create a protocol to run on your robot with this graphical tool access here documentation source code contributing we love contributor here is the best way to work with u filing a bug report we will fix these a quickly a we can and appreciate your help uncovering bug in our code submit a pull request with any new feature youve added to a branch of the api or app we will reach out to talk with you about integration testing and launching it into our product for more information and development setup instruction please read the contributing guide enjoy,opentrons platform overview opentrons api opentrons app contributing overview opentrons make robot biologist mission provide scientific community common platform easily share protocol reproduce others work robot automate experiment would otherwise done hand allowing user spend time pursuing answer 21st century important question le time pipetting repository contains source code opentrons api ot app wed love explore hack build upon opentrons api opentrons api simple framework designed make writing automated biology lab protocol easy weve designed way hope accessible anyone basic computer wetlab skill bench scientist able code automated protocol way read like lab notebook pipetteaspiratelocationtrougha1 volume30 pipettedispenselocationwell_platea1 volume30 example tell opentrons ot2 pipette 30 l liquid trough well plate learn documentation source code opentrons app easily upload protocol calibrate position run experiment computer download documentation source code opentrons protocol designer easily create protocol run robot graphical tool access documentation source code contributing love contributor best way work u filing bug report fix quickly appreciate help uncovering bug code submit pull request new feature youve added branch api app reach talk integration testing launching product information development setup instruction please read contributing guide enjoy
JavaScript ,"GENtle2
A re-think for the web of the original GENtle.
GENtle2 has been almost entirely rewritten over the past year, and remains
very much in development. Core features will be extracted into their own modules
in the coming months.


Getting started

Clone the repository locally and cd into it
Run the following to install the app and its dependencies and compile it.

npm install --production

Run the following command to start the application

npm start

Open you browser and navigate to http://localhost:3000

Contributing
For more details about how the application works, see CONTRIBUTING.md
",gentle2 a rethink for the web of the origin gentl gentle2 ha been almost entir rewritten over the past year and remain veri much in develop core featur will be extract into their own modul in the come month get start clone the repositori local and cd into it run the follow to instal the app and it depend and compil it npm instal product run the follow command to start the applic npm start open you browser and navig to httplocalhost3000 contribut for more detail about how the applic work see contributingmd,gentle2 a rethink for the web of the original gentle gentle2 ha been almost entirely rewritten over the past year and remains very much in development core feature will be extracted into their own module in the coming month getting started clone the repository locally and cd into it run the following to install the app and it dependency and compile it npm install production run the following command to start the application npm start open you browser and navigate to httplocalhost3000 contributing for more detail about how the application work see contributingmd,gentle2 rethink web original gentle gentle2 almost entirely rewritten past year remains much development core feature extracted module coming month getting started clone repository locally cd run following install app dependency compile npm install production run following command start application npm start open browser navigate httplocalhost3000 contributing detail application work see contributingmd
JavaScript ,"

Installing JBrowse
To install jbrowse, visit http://jbrowse.org/blog and download the latest JBrowse zip file. See instructions at http://jbrowse.org/docs/installation.html for a tutorial on setting up a sample instance.
Install JBrowse from GitHub (for developers)
To install from GitHub, you can simply clone the repo and run the setup.sh script
git clone https://github.com/GMOD/jbrowse
cd jbrowse
./setup.sh

Develop JBrowse or JBrowse plugins
To obtain a jbrowse development environment, e.g. for jbrowse source code editing or plugin development (or just running jbrowse from the github repo)
git clone https://github.com/GMOD/jbrowse
cd jbrowse
./setup.sh # not strictly necessary if you don't need to sample data

If you are going to edit the jbrowse source code, then also run
yarn watch

And keep yarn watch running in the background as you create changes to your code.
To start a temporary dev server, can also run
yarn start

And keep this running in the background, this will launch a webserver running jbrowse on port 8082.
Alternatively, you can put this jbrowse folder in your webserver (e.g. /var/www/html/) directory. The key is, if you are modifying jbrowse or plugin source code, to run yarn watch in the background, so that webpack incorporates your changes in either the main codebase (src/JBrowse folder) or any plugins (plugins/YourPlugin).
Note for users in China
In order to make downloads faster you can set a mirror for the npm registry
npm config set registry http://r.cnpmjs.org
npm config set puppeteer_download_host=http://cnpmjs.org/mirrors
export ELECTRON_MIRROR=""http://cnpmjs.org/mirrors/electron/""

Notes on setting up a JBrowse server


If you don't have a webserver such as apache or nginx, you can run npm run start and open http://localhost:8082/index.html?data=sample_data/json/volvox to see the code running from a small express.js server.


You can alternatively just move the jbrowse folder into a nginx or apache root directory e.g. /var/www/html and then navigate to http://localhost/jbrowse


Note: you should avoid using sudo tasks like ./setup.sh and instead use chown/chmod on folders to your own user as necessary.
Also note: After editing a file, you must re-run the webpack build with npm run build or you can keep webpack running in ""watch"" mode by running  npm run watch.
Also also note: by default git clone will clone the master branch which contains the latest stable release. The latest development branch is called dev. Run git checkout dev after clone to retrieve this
Installing as an npm module
To install jbrowse from NPM directly, you can run.
npm install @gmod/jbrowse

To setup a simple instance, you can use
node_modules/.bin/jb_setup.js
node_modules/.bin/jb_run.js

Then visit http://localhost:3000/?data=sample_data/json/volvox
Contributing
Looking for places to contribute to the codebase?
Check out the ""help wanted"" label.
Running the developer test suites
The Travis-CI suite runs Perl, JavaScript, and Selenium automated tests. To run locally, you can use
prove -Isrc/perl5 -lr tests
node tests/js_tests/run-puppeteer.js http://localhost/jbrowse/tests/js_tests/index.html
pip install selenium nose
MOZ_HEADLESS=1 SELENIUM_BROWSER=firefox JBROWSE_URL='http://localhost/jbrowse/index.html' nosetests

Supported browsers for SELENIUM_BROWSER are 'firefox', 'chrome', 'phantom', and 'travis_saucelabs'.  The Sauce Labs + Travis
one will only work in a properly configured Travis CI build environment.
Manual testing

JBrowse has a free open source account on Browserstack for manual testing.  Contact @rbuels for access.
Generating Packaged Builds
You can also optionally run build steps to create the minimized codebase. Extra perl dependencies Text::Markdown and DateTime are required to run the build step.
make -f build/Makefile

To build the Electron app (JBrowse desktop app), run the following
npm install -g electron-packager
make -f build/Makefile release-electron-all

To run the Electron app in debug mode run the following
npm install -g electron
electron browser/main.js

Making a JBrowse release
NOTE: Beginning in 1.12.4,


Run build/release.sh $newReleaseVersion $nextReleaseVersion-alpha.0 notes.txt, where notes.txt is any additional information to add to a blogpost. Then check its work, and then run the git push command it suggests to you. This makes a tag in the repository for the release, named, e.g. 1.6.3-release.  This should cause Travis CI
to create a release on GitHub under https://github.com/GMOD/jbrowse/releases


Test that the page loads in IE11 on BrowserStack


Add release notes to the new GitHub release that Travis created. Can just paste these from release-notes.md, which is in Markdown format.


Write a twitter post for usejbrowse and JBrowseGossip with the announcement link to the blogpost


Write an email announcing the release, sending to gmod-ajax. If it is a major release, add gmod-announce and make a GMOD news item.


As you can tell, this process could really use some more streamlining and automation.
",instal jbrows to instal jbrows visit httpjbrowseorgblog and download the latest jbrows zip file see instruct at httpjbrowseorgdocsinstallationhtml for a tutori on set up a sampl instanc instal jbrows from github for develop to instal from github you can simpli clone the repo and run the setupsh script git clone httpsgithubcomgmodjbrows cd jbrows setupsh develop jbrows or jbrows plugin to obtain a jbrows develop environ eg for jbrows sourc code edit or plugin develop or just run jbrows from the github repo git clone httpsgithubcomgmodjbrows cd jbrows setupsh not strictli necessari if you dont need to sampl data if you are go to edit the jbrows sourc code then also run yarn watch and keep yarn watch run in the background as you creat chang to your code to start a temporari dev server can also run yarn start and keep thi run in the background thi will launch a webserv run jbrows on port 8082 altern you can put thi jbrows folder in your webserv eg varwwwhtml directori the key is if you are modifi jbrows or plugin sourc code to run yarn watch in the background so that webpack incorpor your chang in either the main codebas srcjbrows folder or ani plugin pluginsyourplugin note for user in china in order to make download faster you can set a mirror for the npm registri npm config set registri httprcnpmjsorg npm config set puppeteer_download_hosthttpcnpmjsorgmirror export electron_mirrorhttpcnpmjsorgmirrorselectron note on set up a jbrows server if you dont have a webserv such as apach or nginx you can run npm run start and open httplocalhost8082indexhtmldatasample_datajsonvolvox to see the code run from a small expressj server you can altern just move the jbrows folder into a nginx or apach root directori eg varwwwhtml and then navig to httplocalhostjbrows note you should avoid use sudo task like setupsh and instead use chownchmod on folder to your own user as necessari also note after edit a file you must rerun the webpack build with npm run build or you can keep webpack run in watch mode by run npm run watch also also note by default git clone will clone the master branch which contain the latest stabl releas the latest develop branch is call dev run git checkout dev after clone to retriev thi instal as an npm modul to instal jbrows from npm directli you can run npm instal gmodjbrows to setup a simpl instanc you can use node_modulesbinjb_setupj node_modulesbinjb_runj then visit httplocalhost3000datasample_datajsonvolvox contribut look for place to contribut to the codebas check out the help want label run the develop test suit the travisci suit run perl javascript and selenium autom test to run local you can use prove isrcperl5 lr test node testsjs_testsrunpuppeteerj httplocalhostjbrowsetestsjs_testsindexhtml pip instal selenium nose moz_headless1 selenium_browserfirefox jbrowse_urlhttplocalhostjbrowseindexhtml nosetest support browser for selenium_brows are firefox chrome phantom and travis_saucelab the sauc lab travi one will onli work in a properli configur travi ci build environ manual test jbrows ha a free open sourc account on browserstack for manual test contact rbuel for access gener packag build you can also option run build step to creat the minim codebas extra perl depend textmarkdown and datetim are requir to run the build step make f buildmakefil to build the electron app jbrows desktop app run the follow npm instal g electronpackag make f buildmakefil releaseelectronal to run the electron app in debug mode run the follow npm instal g electron electron browsermainj make a jbrows releas note begin in 1124 run buildreleasesh newreleasevers nextreleaseversionalpha0 notestxt where notestxt is ani addit inform to add to a blogpost then check it work and then run the git push command it suggest to you thi make a tag in the repositori for the releas name eg 163releas thi should caus travi ci to creat a releas on github under httpsgithubcomgmodjbrowsereleas test that the page load in ie11 on browserstack add releas note to the new github releas that travi creat can just past these from releasenotesmd which is in markdown format write a twitter post for usejbrows and jbrowsegossip with the announc link to the blogpost write an email announc the releas send to gmodajax if it is a major releas add gmodannounc and make a gmod news item as you can tell thi process could realli use some more streamlin and autom,installing jbrowse to install jbrowse visit httpjbrowseorgblog and download the latest jbrowse zip file see instruction at httpjbrowseorgdocsinstallationhtml for a tutorial on setting up a sample instance install jbrowse from github for developer to install from github you can simply clone the repo and run the setupsh script git clone httpsgithubcomgmodjbrowse cd jbrowse setupsh develop jbrowse or jbrowse plugins to obtain a jbrowse development environment eg for jbrowse source code editing or plugin development or just running jbrowse from the github repo git clone httpsgithubcomgmodjbrowse cd jbrowse setupsh not strictly necessary if you dont need to sample data if you are going to edit the jbrowse source code then also run yarn watch and keep yarn watch running in the background a you create change to your code to start a temporary dev server can also run yarn start and keep this running in the background this will launch a webserver running jbrowse on port 8082 alternatively you can put this jbrowse folder in your webserver eg varwwwhtml directory the key is if you are modifying jbrowse or plugin source code to run yarn watch in the background so that webpack incorporates your change in either the main codebase srcjbrowse folder or any plugins pluginsyourplugin note for user in china in order to make downloads faster you can set a mirror for the npm registry npm config set registry httprcnpmjsorg npm config set puppeteer_download_hosthttpcnpmjsorgmirrors export electron_mirrorhttpcnpmjsorgmirrorselectron note on setting up a jbrowse server if you dont have a webserver such a apache or nginx you can run npm run start and open httplocalhost8082indexhtmldatasample_datajsonvolvox to see the code running from a small expressjs server you can alternatively just move the jbrowse folder into a nginx or apache root directory eg varwwwhtml and then navigate to httplocalhostjbrowse note you should avoid using sudo task like setupsh and instead use chownchmod on folder to your own user a necessary also note after editing a file you must rerun the webpack build with npm run build or you can keep webpack running in watch mode by running npm run watch also also note by default git clone will clone the master branch which contains the latest stable release the latest development branch is called dev run git checkout dev after clone to retrieve this installing a an npm module to install jbrowse from npm directly you can run npm install gmodjbrowse to setup a simple instance you can use node_modulesbinjb_setupjs node_modulesbinjb_runjs then visit httplocalhost3000datasample_datajsonvolvox contributing looking for place to contribute to the codebase check out the help wanted label running the developer test suite the travisci suite run perl javascript and selenium automated test to run locally you can use prove isrcperl5 lr test node testsjs_testsrunpuppeteerjs httplocalhostjbrowsetestsjs_testsindexhtml pip install selenium nose moz_headless1 selenium_browserfirefox jbrowse_urlhttplocalhostjbrowseindexhtml nosetests supported browser for selenium_browser are firefox chrome phantom and travis_saucelabs the sauce lab travis one will only work in a properly configured travis ci build environment manual testing jbrowse ha a free open source account on browserstack for manual testing contact rbuels for access generating packaged build you can also optionally run build step to create the minimized codebase extra perl dependency textmarkdown and datetime are required to run the build step make f buildmakefile to build the electron app jbrowse desktop app run the following npm install g electronpackager make f buildmakefile releaseelectronall to run the electron app in debug mode run the following npm install g electron electron browsermainjs making a jbrowse release note beginning in 1124 run buildreleasesh newreleaseversion nextreleaseversionalpha0 notestxt where notestxt is any additional information to add to a blogpost then check it work and then run the git push command it suggests to you this make a tag in the repository for the release named eg 163release this should cause travis ci to create a release on github under httpsgithubcomgmodjbrowsereleases test that the page load in ie11 on browserstack add release note to the new github release that travis created can just paste these from releasenotesmd which is in markdown format write a twitter post for usejbrowse and jbrowsegossip with the announcement link to the blogpost write an email announcing the release sending to gmodajax if it is a major release add gmodannounce and make a gmod news item a you can tell this process could really use some more streamlining and automation,installing jbrowse install jbrowse visit httpjbrowseorgblog download latest jbrowse zip file see instruction httpjbrowseorgdocsinstallationhtml tutorial setting sample instance install jbrowse github developer install github simply clone repo run setupsh script git clone httpsgithubcomgmodjbrowse cd jbrowse setupsh develop jbrowse jbrowse plugins obtain jbrowse development environment eg jbrowse source code editing plugin development running jbrowse github repo git clone httpsgithubcomgmodjbrowse cd jbrowse setupsh strictly necessary dont need sample data going edit jbrowse source code also run yarn watch keep yarn watch running background create change code start temporary dev server also run yarn start keep running background launch webserver running jbrowse port 8082 alternatively put jbrowse folder webserver eg varwwwhtml directory key modifying jbrowse plugin source code run yarn watch background webpack incorporates change either main codebase srcjbrowse folder plugins pluginsyourplugin note user china order make downloads faster set mirror npm registry npm config set registry httprcnpmjsorg npm config set puppeteer_download_hosthttpcnpmjsorgmirrors export electron_mirrorhttpcnpmjsorgmirrorselectron note setting jbrowse server dont webserver apache nginx run npm run start open httplocalhost8082indexhtmldatasample_datajsonvolvox see code running small expressjs server alternatively move jbrowse folder nginx apache root directory eg varwwwhtml navigate httplocalhostjbrowse note avoid using sudo task like setupsh instead use chownchmod folder user necessary also note editing file must rerun webpack build npm run build keep webpack running watch mode running npm run watch also also note default git clone clone master branch contains latest stable release latest development branch called dev run git checkout dev clone retrieve installing npm module install jbrowse npm directly run npm install gmodjbrowse setup simple instance use node_modulesbinjb_setupjs node_modulesbinjb_runjs visit httplocalhost3000datasample_datajsonvolvox contributing looking place contribute codebase check help wanted label running developer test suite travisci suite run perl javascript selenium automated test run locally use prove isrcperl5 lr test node testsjs_testsrunpuppeteerjs httplocalhostjbrowsetestsjs_testsindexhtml pip install selenium nose moz_headless1 selenium_browserfirefox jbrowse_urlhttplocalhostjbrowseindexhtml nosetests supported browser selenium_browser firefox chrome phantom travis_saucelabs sauce lab travis one work properly configured travis ci build environment manual testing jbrowse free open source account browserstack manual testing contact rbuels access generating packaged build also optionally run build step create minimized codebase extra perl dependency textmarkdown datetime required run build step make f buildmakefile build electron app jbrowse desktop app run following npm install g electronpackager make f buildmakefile releaseelectronall run electron app debug mode run following npm install g electron electron browsermainjs making jbrowse release note beginning 1124 run buildreleasesh newreleaseversion nextreleaseversionalpha0 notestxt notestxt additional information add blogpost check work run git push command suggests make tag repository release named eg 163release cause travis ci create release github httpsgithubcomgmodjbrowsereleases test page load ie11 browserstack add release note new github release travis created paste releasenotesmd markdown format write twitter post usejbrowse jbrowsegossip announcement link blogpost write email announcing release sending gmodajax major release add gmodannounce make gmod news item tell process could really use streamlining automation
JavaScript ,"






Escher
Escher is a web-based tool to build, view, share, and embed metabolic maps. The
easiest way to use Escher is to browse or build maps on the
Escher website.
Visit the documentation to get started with
Escher and explore the API.
Check out the
developer docs,
the Gitter chat room, and the
Development Roadmap for information
on Escher development. Feel free to submit bugs and feature requests as Issues,
or, better yet, Pull Requests.
Follow @zakandrewking for Escher updates.
You can help support Escher by citing our publication when you use Escher or
EscherConverter:
Zachary A. King, Andreas Dräger, Ali Ebrahim, Nikolaus Sonnenschein, Nathan
E. Lewis, and Bernhard O. Palsson (2015) Escher: A web application for
building, sharing, and embedding data-rich visualizations of biological
pathways, PLOS Computational Biology 11(8):
e1004321. doi:10.1371/journal.pcbi.1004321
Escher was developed at SBRG. Funding was
provided by The National Science Foundation Graduate Research Fellowship
under Grant no. DGE-1144086, The European Commission as part of a Marie Curie
International Outgoing Fellowship within the EU 7th Framework Program for
Research and Technological Development (EU project AMBiCon, 332020),
and The Novo Nordisk Foundation
through The Center for Biosustainability
at the Technical University of Denmark (NNF10CC1016517)
Building and testing Escher
JavaScript
First, install dependencies with npm (or you can use
yarn):
npm install

Escher uses webpack to manage the build process. To run typical build steps, just run:
npm run build

You can run a development server with:
npm run start
# or for live updates when the source code changes:
npm run watch

To test the JavaScript files, run:
npm run test

Python
Escher has a Python package for generating Escher visualizations from within a
Python data anlaysis session. To learn more about using the features of the
Python package, check out the documentation:
https://escher.readthedocs.io/en/latest/escher-python.html
You can install it with pip:
pip install escher

Jupyter extensions
When you pip install escher, the Jupyter notebook extension should be
installed automatically. If that doesn't work, try:
# The notebook extenstion should install automatically. You can check by running:
jupyter nbextension list
# Make sure you have version >=5 of the `notebook` package
pip install ""notebook>=5""
# To manually install the extension
jupyter nbextension install --py escher
jupyter nbextension enable --py escher
# depending on you environment, you might need the `--sysprefix` flag with those commands
To install the Jupyter lab extension, simply install Escher with pip install escher then
install the extension:
jupyter labextension install @jupyter-widgets/jupyterlab-manager
jupyter labextension install escher
Python/Jupyter Development
For development of the Python package, first build the JavaScript package and
copy it over to the py directory with these commands in the Escher root:
npm install
npm run build
npm run copy

Then in the py directory, install the Python package:
cd py
pip install -e . # installs escher in develop mode and dependencies

For Python testing, run this in the py directory:
cd py
pytest

To develop the Jupyter notebook and Jupyter Lab extensions, you will need
install them with symlinks.
First, install the Python package for development as described above.
For the Jupyter notebooks, run:
cd py
jupyter nbextension install --py --symlink escher
jupyter nbextension enable --py escher

If you are using virtualenv or conda, you can add the --sys-prefix flag to
those commands to keep your environment isolated and reproducible.
When you make changes, you will need to yarn build && yarn copy and refresh
notebook browser tab.
For Jupyter Lab, run (in the root directory):
yarn watch # keep this running as a separate process
jupyter labextension install @jupyter-widgets/jupyterlab-manager
jupyter labextension link
jupyter lab --watch

If you don't see changes when you edit the code, try refreshing or restarting
jupyter lab --watch.
Docs
Build and run the docs::
cd docs
./build_docs
cd _build/html
python -m SimpleHTTPServer # python 2
python -m http.server # python 3

",escher escher is a webbas tool to build view share and emb metabol map the easiest way to use escher is to brows or build map on the escher websit visit the document to get start with escher and explor the api check out the develop doc the gitter chat room and the develop roadmap for inform on escher develop feel free to submit bug and featur request as issu or better yet pull request follow zakandrewk for escher updat you can help support escher by cite our public when you use escher or escherconvert zachari a king andrea drger ali ebrahim nikolau sonnenschein nathan e lewi and bernhard o palsson 2015 escher a web applic for build share and embed datarich visual of biolog pathway plo comput biolog 118 e1004321 doi101371journalpcbi1004321 escher wa develop at sbrg fund wa provid by the nation scienc foundat graduat research fellowship under grant no dge1144086 the european commiss as part of a mari curi intern outgo fellowship within the eu 7th framework program for research and technolog develop eu project ambicon 332020 and the novo nordisk foundat through the center for biosustain at the technic univers of denmark nnf10cc1016517 build and test escher javascript first instal depend with npm or you can use yarn npm instal escher use webpack to manag the build process to run typic build step just run npm run build you can run a develop server with npm run start or for live updat when the sourc code chang npm run watch to test the javascript file run npm run test python escher ha a python packag for gener escher visual from within a python data anlaysi session to learn more about use the featur of the python packag check out the document httpsescherreadthedocsioenlatestescherpythonhtml you can instal it with pip pip instal escher jupyt extens when you pip instal escher the jupyt notebook extens should be instal automat if that doesnt work tri the notebook extenst should instal automat you can check by run jupyt nbextens list make sure you have version 5 of the notebook packag pip instal notebook5 to manual instal the extens jupyt nbextens instal py escher jupyt nbextens enabl py escher depend on you environ you might need the sysprefix flag with those command to instal the jupyt lab extens simpli instal escher with pip instal escher then instal the extens jupyt labextens instal jupyterwidgetsjupyterlabmanag jupyt labextens instal escher pythonjupyt develop for develop of the python packag first build the javascript packag and copi it over to the py directori with these command in the escher root npm instal npm run build npm run copi then in the py directori instal the python packag cd py pip instal e instal escher in develop mode and depend for python test run thi in the py directori cd py pytest to develop the jupyt notebook and jupyt lab extens you will need instal them with symlink first instal the python packag for develop as describ abov for the jupyt notebook run cd py jupyt nbextens instal py symlink escher jupyt nbextens enabl py escher if you are use virtualenv or conda you can add the sysprefix flag to those command to keep your environ isol and reproduc when you make chang you will need to yarn build yarn copi and refresh notebook browser tab for jupyt lab run in the root directori yarn watch keep thi run as a separ process jupyt labextens instal jupyterwidgetsjupyterlabmanag jupyt labextens link jupyt lab watch if you dont see chang when you edit the code tri refresh or restart jupyt lab watch doc build and run the doc cd doc build_doc cd _buildhtml python m simplehttpserv python 2 python m httpserver python 3,escher escher is a webbased tool to build view share and embed metabolic map the easiest way to use escher is to browse or build map on the escher website visit the documentation to get started with escher and explore the api check out the developer doc the gitter chat room and the development roadmap for information on escher development feel free to submit bug and feature request a issue or better yet pull request follow zakandrewking for escher update you can help support escher by citing our publication when you use escher or escherconverter zachary a king andreas drger ali ebrahim nikolaus sonnenschein nathan e lewis and bernhard o palsson 2015 escher a web application for building sharing and embedding datarich visualization of biological pathway plo computational biology 118 e1004321 doi101371journalpcbi1004321 escher wa developed at sbrg funding wa provided by the national science foundation graduate research fellowship under grant no dge1144086 the european commission a part of a marie curie international outgoing fellowship within the eu 7th framework program for research and technological development eu project ambicon 332020 and the novo nordisk foundation through the center for biosustainability at the technical university of denmark nnf10cc1016517 building and testing escher javascript first install dependency with npm or you can use yarn npm install escher us webpack to manage the build process to run typical build step just run npm run build you can run a development server with npm run start or for live update when the source code change npm run watch to test the javascript file run npm run test python escher ha a python package for generating escher visualization from within a python data anlaysis session to learn more about using the feature of the python package check out the documentation httpsescherreadthedocsioenlatestescherpythonhtml you can install it with pip pip install escher jupyter extension when you pip install escher the jupyter notebook extension should be installed automatically if that doesnt work try the notebook extenstion should install automatically you can check by running jupyter nbextension list make sure you have version 5 of the notebook package pip install notebook5 to manually install the extension jupyter nbextension install py escher jupyter nbextension enable py escher depending on you environment you might need the sysprefix flag with those command to install the jupyter lab extension simply install escher with pip install escher then install the extension jupyter labextension install jupyterwidgetsjupyterlabmanager jupyter labextension install escher pythonjupyter development for development of the python package first build the javascript package and copy it over to the py directory with these command in the escher root npm install npm run build npm run copy then in the py directory install the python package cd py pip install e installs escher in develop mode and dependency for python testing run this in the py directory cd py pytest to develop the jupyter notebook and jupyter lab extension you will need install them with symlinks first install the python package for development a described above for the jupyter notebook run cd py jupyter nbextension install py symlink escher jupyter nbextension enable py escher if you are using virtualenv or conda you can add the sysprefix flag to those command to keep your environment isolated and reproducible when you make change you will need to yarn build yarn copy and refresh notebook browser tab for jupyter lab run in the root directory yarn watch keep this running a a separate process jupyter labextension install jupyterwidgetsjupyterlabmanager jupyter labextension link jupyter lab watch if you dont see change when you edit the code try refreshing or restarting jupyter lab watch doc build and run the doc cd doc build_docs cd _buildhtml python m simplehttpserver python 2 python m httpserver python 3,escher escher webbased tool build view share embed metabolic map easiest way use escher browse build map escher website visit documentation get started escher explore api check developer doc gitter chat room development roadmap information escher development feel free submit bug feature request issue better yet pull request follow zakandrewking escher update help support escher citing publication use escher escherconverter zachary king andreas drger ali ebrahim nikolaus sonnenschein nathan e lewis bernhard palsson 2015 escher web application building sharing embedding datarich visualization biological pathway plo computational biology 118 e1004321 doi101371journalpcbi1004321 escher developed sbrg funding provided national science foundation graduate research fellowship grant dge1144086 european commission part marie curie international outgoing fellowship within eu 7th framework program research technological development eu project ambicon 332020 novo nordisk foundation center biosustainability technical university denmark nnf10cc1016517 building testing escher javascript first install dependency npm use yarn npm install escher us webpack manage build process run typical build step run npm run build run development server npm run start live update source code change npm run watch test javascript file run npm run test python escher python package generating escher visualization within python data anlaysis session learn using feature python package check documentation httpsescherreadthedocsioenlatestescherpythonhtml install pip pip install escher jupyter extension pip install escher jupyter notebook extension installed automatically doesnt work try notebook extenstion install automatically check running jupyter nbextension list make sure version 5 notebook package pip install notebook5 manually install extension jupyter nbextension install py escher jupyter nbextension enable py escher depending environment might need sysprefix flag command install jupyter lab extension simply install escher pip install escher install extension jupyter labextension install jupyterwidgetsjupyterlabmanager jupyter labextension install escher pythonjupyter development development python package first build javascript package copy py directory command escher root npm install npm run build npm run copy py directory install python package cd py pip install e installs escher develop mode dependency python testing run py directory cd py pytest develop jupyter notebook jupyter lab extension need install symlinks first install python package development described jupyter notebook run cd py jupyter nbextension install py symlink escher jupyter nbextension enable py escher using virtualenv conda add sysprefix flag command keep environment isolated reproducible make change need yarn build yarn copy refresh notebook browser tab jupyter lab run root directory yarn watch keep running separate process jupyter labextension install jupyterwidgetsjupyterlabmanager jupyter labextension link jupyter lab watch dont see change edit code try refreshing restarting jupyter lab watch doc build run doc cd doc build_docs cd _buildhtml python simplehttpserver python 2 python httpserver python 3
JavaScript ,"


layout
title
permalink




docs
Overview of DIYbiosphere
/docs/introduction/overview/







🎉 Welcome to the DIYbiophere repository 🎉
We really appreciate your interest in our project and we would ❤️ your contributions!
About
The DIYbiosphere is a open-source project to connect Do-It-Yourself Biology (DIYbio) initiatives from all over the world. The goal is to have a shared and common platform that can connect people and ideas in all its possibilities and encourage the DIYbio community to work on a project together.
How it works
The platform functions similar to a wiki but uses GitHub Pages instead; hosting the raw files at https://github.com/DIYbiosphere/sphere and rendering webpages at http://sphere.diybio.org
Each DIYbio initiative has its own entry which are organized into eight collections: projects, startups, labs, incubators, groups, networks, events, and others. An entry is added by creating its own folder in its respective collection, and adding a text file in markdown syntax with a YAML front matter, which is then rendered into its webpage. For example, the file _projects/MyDIYbioProject/MyDIYbioProject.md could look like this:
---
# This is the front matter in YAML; between two lines of three consecutive dashes (---)
title: My DIYbio Project
start-date: 2000
type-org: non-profit
website: http://my-diybio-project.io
tags:
  - open hardware
  - citizen science
---
# This is the text in Markdown syntax; after the front matter

My DIYbio project is about **open hardware** and **citizen science**.

The front matter includes several key: value pairs that render into different elements in the webpage. The Avocado Lab is an example entry for pedagogical purposes. You can check out the raw file raw file rendered into this webpage. See the rendered image below, and by its side the different elements of the page labeled.
{:.ui.fluid.image}
Contribute
To contribute, you need a GitHub account (sign up).
You also need to abide to our Code of Conduct (COC) and consent to our Contributor Terms (CT) determined by our Terms of Use (aka Copyrights)).

TL;DR (Too Long; Didn't Read)

COC: Be kind and respectful. Gross, rude or inappropriate behavior will not be tolerated. Confront wrongdoers directly or report them to the board of directors.
CT: You freely share your contributions to the repository under the MIT license. If your contributions are displayed in the website, you freely waive authorship rights of these contributions (public domain; CC0), otherwise you will specify their copyright. You will also note contributions on behalf of third parties and specify their copyright.


In increasing order of engagement you can contribute to DIYbiosphere by:

SHARING THE LOVE

Share this project with your friends and followers! They might be interested in using the project to find DIYbio initiatives or adding their own. You don't need a GitHub account for this!
⭐ Star the project on GitHub! Starring helps attract potential contributors, especially expert and master developers!


WRITING ENTRIES

Add a new entry:  whether it's your initiative or someone else's
Edit an existing entry: misspellings, outdated information, or just inaccurate, help us keep the entries error-free and up-to-date!


PARTICIPATING IN THE ISSUES

Comment, answer, and vote: search our issues and see if you can help out by moving our issues along:
Submit a new issue: report a bug, ask a question, share your idea and wait for feedback and support from the community.
Fork, commit, pull request your contributions! Tackle a good first issue to get you started


GETTING INVOLVED

Join the development community. The project is managed by members of the DIYbiosphere community. Request membership by submitting an issue enjoy more access privileges to the project!
Join the conversation. You can freely join the Gitter chatroom at gitter.im/diybiosphere/sphere, or in Slack at diybiosphere.slack.com



Copyright
In short: the work in DIYbiosphere is freely available to use, modify and distribute. More specifically:

Files in the Repository are available under the MIT License
Content in the Website is shared under the public domain by CC0 License

Credit our work as “© DIYbiosphere contributors” or “© DIYbiosphere” with a link to the Repository at: https://github.com/DIYbiosphere/sphere, or the Website at: http://sphere.diybio.org
You can review our Terms of Use for a human-readable version of the copyrights, and our Contributor Terms to understand in legal terms the rights granted and/or waived from your Contributions. For further detail you should read in full both MIT and CC0 licenses.
",layout titl permalink doc overview of diybiospher docsintroductionoverview welcom to the diybiopher repositori we realli appreci your interest in our project and we would your contribut about the diybiospher is a opensourc project to connect doityourself biolog diybio initi from all over the world the goal is to have a share and common platform that can connect peopl and idea in all it possibl and encourag the diybio commun to work on a project togeth how it work the platform function similar to a wiki but use github page instead host the raw file at httpsgithubcomdiybiospherespher and render webpag at httpspherediybioorg each diybio initi ha it own entri which are organ into eight collect project startup lab incub group network event and other an entri is ad by creat it own folder in it respect collect and ad a text file in markdown syntax with a yaml front matter which is then render into it webpag for exampl the file _projectsmydiybioprojectmydiybioprojectmd could look like thi thi is the front matter in yaml between two line of three consecut dash titl my diybio project startdat 2000 typeorg nonprofit websit httpmydiybioprojectio tag open hardwar citizen scienc thi is the text in markdown syntax after the front matter my diybio project is about open hardwar and citizen scienc the front matter includ sever key valu pair that render into differ element in the webpag the avocado lab is an exampl entri for pedagog purpos you can check out the raw file raw file render into thi webpag see the render imag below and by it side the differ element of the page label uifluidimag contribut to contribut you need a github account sign up you also need to abid to our code of conduct coc and consent to our contributor term ct determin by our term of use aka copyright tldr too long didnt read coc be kind and respect gross rude or inappropri behavior will not be toler confront wrongdoer directli or report them to the board of director ct you freeli share your contribut to the repositori under the mit licens if your contribut are display in the websit you freeli waiv authorship right of these contribut public domain cc0 otherwis you will specifi their copyright you will also note contribut on behalf of third parti and specifi their copyright in increas order of engag you can contribut to diybiospher by share the love share thi project with your friend and follow they might be interest in use the project to find diybio initi or ad their own you dont need a github account for thi star the project on github star help attract potenti contributor especi expert and master develop write entri add a new entri whether it your initi or someon els edit an exist entri misspel outdat inform or just inaccur help us keep the entri errorfre and uptod particip in the issu comment answer and vote search our issu and see if you can help out by move our issu along submit a new issu report a bug ask a question share your idea and wait for feedback and support from the commun fork commit pull request your contribut tackl a good first issu to get you start get involv join the develop commun the project is manag by member of the diybiospher commun request membership by submit an issu enjoy more access privileg to the project join the convers you can freeli join the gitter chatroom at gitterimdiybiospherespher or in slack at diybiosphereslackcom copyright in short the work in diybiospher is freeli avail to use modifi and distribut more specif file in the repositori are avail under the mit licens content in the websit is share under the public domain by cc0 licens credit our work as diybiospher contributor or diybiospher with a link to the repositori at httpsgithubcomdiybiospherespher or the websit at httpspherediybioorg you can review our term of use for a humanread version of the copyright and our contributor term to understand in legal term the right grant andor waiv from your contribut for further detail you should read in full both mit and cc0 licens,layout title permalink doc overview of diybiosphere docsintroductionoverview welcome to the diybiophere repository we really appreciate your interest in our project and we would your contribution about the diybiosphere is a opensource project to connect doityourself biology diybio initiative from all over the world the goal is to have a shared and common platform that can connect people and idea in all it possibility and encourage the diybio community to work on a project together how it work the platform function similar to a wiki but us github page instead hosting the raw file at httpsgithubcomdiybiospheresphere and rendering webpage at httpspherediybioorg each diybio initiative ha it own entry which are organized into eight collection project startup lab incubator group network event and others an entry is added by creating it own folder in it respective collection and adding a text file in markdown syntax with a yaml front matter which is then rendered into it webpage for example the file _projectsmydiybioprojectmydiybioprojectmd could look like this this is the front matter in yaml between two line of three consecutive dash title my diybio project startdate 2000 typeorg nonprofit website httpmydiybioprojectio tag open hardware citizen science this is the text in markdown syntax after the front matter my diybio project is about open hardware and citizen science the front matter includes several key value pair that render into different element in the webpage the avocado lab is an example entry for pedagogical purpose you can check out the raw file raw file rendered into this webpage see the rendered image below and by it side the different element of the page labeled uifluidimage contribute to contribute you need a github account sign up you also need to abide to our code of conduct coc and consent to our contributor term ct determined by our term of use aka copyright tldr too long didnt read coc be kind and respectful gross rude or inappropriate behavior will not be tolerated confront wrongdoer directly or report them to the board of director ct you freely share your contribution to the repository under the mit license if your contribution are displayed in the website you freely waive authorship right of these contribution public domain cc0 otherwise you will specify their copyright you will also note contribution on behalf of third party and specify their copyright in increasing order of engagement you can contribute to diybiosphere by sharing the love share this project with your friend and follower they might be interested in using the project to find diybio initiative or adding their own you dont need a github account for this star the project on github starring help attract potential contributor especially expert and master developer writing entry add a new entry whether it your initiative or someone el edit an existing entry misspelling outdated information or just inaccurate help u keep the entry errorfree and uptodate participating in the issue comment answer and vote search our issue and see if you can help out by moving our issue along submit a new issue report a bug ask a question share your idea and wait for feedback and support from the community fork commit pull request your contribution tackle a good first issue to get you started getting involved join the development community the project is managed by member of the diybiosphere community request membership by submitting an issue enjoy more access privilege to the project join the conversation you can freely join the gitter chatroom at gitterimdiybiospheresphere or in slack at diybiosphereslackcom copyright in short the work in diybiosphere is freely available to use modify and distribute more specifically file in the repository are available under the mit license content in the website is shared under the public domain by cc0 license credit our work a diybiosphere contributor or diybiosphere with a link to the repository at httpsgithubcomdiybiospheresphere or the website at httpspherediybioorg you can review our term of use for a humanreadable version of the copyright and our contributor term to understand in legal term the right granted andor waived from your contribution for further detail you should read in full both mit and cc0 license,layout title permalink doc overview diybiosphere docsintroductionoverview welcome diybiophere repository really appreciate interest project would contribution diybiosphere opensource project connect doityourself biology diybio initiative world goal shared common platform connect people idea possibility encourage diybio community work project together work platform function similar wiki us github page instead hosting raw file httpsgithubcomdiybiospheresphere rendering webpage httpspherediybioorg diybio initiative entry organized eight collection project startup lab incubator group network event others entry added creating folder respective collection adding text file markdown syntax yaml front matter rendered webpage example file _projectsmydiybioprojectmydiybioprojectmd could look like front matter yaml two line three consecutive dash title diybio project startdate 2000 typeorg nonprofit website httpmydiybioprojectio tag open hardware citizen science text markdown syntax front matter diybio project open hardware citizen science front matter includes several key value pair render different element webpage avocado lab example entry pedagogical purpose check raw file raw file rendered webpage see rendered image side different element page labeled uifluidimage contribute contribute need github account sign also need abide code conduct coc consent contributor term ct determined term use aka copyright tldr long didnt read coc kind respectful gross rude inappropriate behavior tolerated confront wrongdoer directly report board director ct freely share contribution repository mit license contribution displayed website freely waive authorship right contribution public domain cc0 otherwise specify copyright also note contribution behalf third party specify copyright increasing order engagement contribute diybiosphere sharing love share project friend follower might interested using project find diybio initiative adding dont need github account star project github starring help attract potential contributor especially expert master developer writing entry add new entry whether initiative someone el edit existing entry misspelling outdated information inaccurate help u keep entry errorfree uptodate participating issue comment answer vote search issue see help moving issue along submit new issue report bug ask question share idea wait feedback support community fork commit pull request contribution tackle good first issue get started getting involved join development community project managed member diybiosphere community request membership submitting issue enjoy access privilege project join conversation freely join gitter chatroom gitterimdiybiospheresphere slack diybiosphereslackcom copyright short work diybiosphere freely available use modify distribute specifically file repository available mit license content website shared public domain cc0 license credit work diybiosphere contributor diybiosphere link repository httpsgithubcomdiybiospheresphere website httpspherediybioorg review term use humanreadable version copyright contributor term understand legal term right granted andor waived contribution detail read full mit cc0 license
JavaScript ,"


SynBioHub is a Web-based repository for synthetic biology, enabling users to browse, upload, and share synthetic biology designs.
To learn more about the SynBioHub, including installation instructions and documentation, visit the SynBioHub wiki.
To access a sample instance of SynBioHub containing enriched Bacillus subtilis data, features from the Escherichia coli genome, and the complete iGEM Registry of Standard Biological Parts, visit synbiohub.org. To access a bleeding-edge version of SynBioHub, visit dev.synbiohub.org.
Installation
The recommended way to install SynBioHub is via the Docker image.  See Installation for more information.
Manual Installation
SynBioHub has both JavaScript (node.js) and Java components.
Prequisites:
Linux (only tested with Ubuntu 18.04.01) or macOS

If you're using macOS, first install homebrew

A JDK



OS
Command




Ubuntu
apt install default-jdk


Mac
brew install openjdk



Apache Maven



OS
Command




Ubuntu
apt install maven


Mac
brew install maven



node.js >= 11.0.0



OS
Command/Link




Ubuntu
visit https://nodejs.org/en/


Mac
brew install node



OpenLink Virtuoso 7.x.x



OS
Command/Link




Ubuntu
visit https://github.com/openlink/virtuoso-opensource


Mac
brew install virtuoso



rapper



OS
Command




Ubuntu
apt install raptor2-utils


Mac
brew install raptor



jq



OS
Command




Ubuntu
apt install jq


Mac
brew install jq



Ubuntu 18.04.01

Install Virtuoso 7 from source at
https://github.com/openlink/virtuoso-opensource


Switch to the branch stable/7 before installing.
Follow the README on installing virtuoso from source. This involves installing all the dependencies and running build commands.
Currently, Virtuoso does not support versions of OpenSSL 1.1.0 and above, or versions of OpenSSL below 1.0.0. When installing the dependency, build from a binary between those versions from https://www.openssl.org/source/.


Set up the Node.js repository

Download the Node setup script curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -
Update your package repositories apt update


Install the necessary packages apt install default-jdk maven raptor2-utils nodejs jq build-essential python
Start virtuoso process virtuoso-t +configfile /usr/local/virtuoso-opensource/var/lib/virtuoso/db/virtuoso.ini -f

MacOS

Install the necessary packages brew install openjdk maven node virtuoso raptor jq python
Start virtuoso process

cd /usr/local/Cellar/virtuoso/7.2.5.1_1/var/lib/virtuoso/db

The command above is based on where the virtuoso.ini file is located. Your installation might be located
somewhere different than /usr/local/Cellar/virtuoso/7.2.5.1_1/var/lib/virtuoso/db, or the version might be
different (7.2.5.1_1 might be 7.3.6.1_1 or any other version number).
If you're having trouble finding the location of the virtuoso.ini file, run sudo find / -name virtuoso.ini.
Press the control and c keys simultaneously to quit the search.


virtuoso-t -f



Both Systems

Clone the SynBioHub repository git clone https://github.com/SynBioHub/synbiohub
Change to the SynBioHub directory cd synbiohub
Build the Java components with Maven cd java && mvn package
Return to the root directory and install the Node dependencies with yarn cd ../ && yarn install
Make sure that yarn is being used, not 'cmdtest'.
Install nodemon and forever with npm install nodemon -g && npm install forever -g
Add SPARQL update rights to the dba user in virtuoso.


Visit localhost:8890, click conductor on the left hand side, and login with user name dba and password dba.
Visit system admin -> user accounts in the menu at the top.
Find the accound labled dba and edit.Add SPARQL_UPDATE to roles using the menu at the bottom.
If no dba account exists, add one, then add update rights.


Start the SynBioHub process npm start or npm run-script dev

Publishing
The repository is set up to prohibit commits directly to the master branch.
Commits must be made in another branch, and then a GitHub PR used to merge them into master.
GitHub PRs must be approved by at least one other developer before they can be merged into master.
Additionally, they must pass Travis checks, which build a Docker image and run the SBOLTestSuite and SynBioHub integration tests against it.
Each time a PR is merged into master, the Travis checks are re-run on the master branch, and if they succeed the resulting image is pushed by Travis to DockerHub under the tag snapshot-standalone.
Publishing a release
Releases are published automatically using GitHub Actions.
There is an action which fires on release publication.
It publishes an image to Docker Hub under the $VERSION-standalone tag, and updates the synbiohub-docker master branch to point to this version.
More information available here.
",synbiohub is a webbas repositori for synthet biolog enabl user to brows upload and share synthet biolog design to learn more about the synbiohub includ instal instruct and document visit the synbiohub wiki to access a sampl instanc of synbiohub contain enrich bacillu subtili data featur from the escherichia coli genom and the complet igem registri of standard biolog part visit synbiohuborg to access a bleedingedg version of synbiohub visit devsynbiohuborg instal the recommend way to instal synbiohub is via the docker imag see instal for more inform manual instal synbiohub ha both javascript nodej and java compon prequisit linux onli test with ubuntu 180401 or maco if your use maco first instal homebrew a jdk os command ubuntu apt instal defaultjdk mac brew instal openjdk apach maven os command ubuntu apt instal maven mac brew instal maven nodej 1100 os commandlink ubuntu visit httpsnodejsorgen mac brew instal node openlink virtuoso 7xx os commandlink ubuntu visit httpsgithubcomopenlinkvirtuosoopensourc mac brew instal virtuoso rapper os command ubuntu apt instal raptor2util mac brew instal raptor jq os command ubuntu apt instal jq mac brew instal jq ubuntu 180401 instal virtuoso 7 from sourc at httpsgithubcomopenlinkvirtuosoopensourc switch to the branch stable7 befor instal follow the readm on instal virtuoso from sourc thi involv instal all the depend and run build command current virtuoso doe not support version of openssl 110 and abov or version of openssl below 100 when instal the depend build from a binari between those version from httpswwwopensslorgsourc set up the nodej repositori download the node setup script curl sl httpsdebnodesourcecomsetup_6x sudo e bash updat your packag repositori apt updat instal the necessari packag apt instal defaultjdk maven raptor2util nodej jq buildessenti python start virtuoso process virtuosot configfil usrlocalvirtuosoopensourcevarlibvirtuosodbvirtuosoini f maco instal the necessari packag brew instal openjdk maven node virtuoso raptor jq python start virtuoso process cd usrlocalcellarvirtuoso7251_1varlibvirtuosodb the command abov is base on where the virtuosoini file is locat your instal might be locat somewher differ than usrlocalcellarvirtuoso7251_1varlibvirtuosodb or the version might be differ 7251_1 might be 7361_1 or ani other version number if your have troubl find the locat of the virtuosoini file run sudo find name virtuosoini press the control and c key simultan to quit the search virtuosot f both system clone the synbiohub repositori git clone httpsgithubcomsynbiohubsynbiohub chang to the synbiohub directori cd synbiohub build the java compon with maven cd java mvn packag return to the root directori and instal the node depend with yarn cd yarn instal make sure that yarn is be use not cmdtest instal nodemon and forev with npm instal nodemon g npm instal forev g add sparql updat right to the dba user in virtuoso visit localhost8890 click conductor on the left hand side and login with user name dba and password dba visit system admin user account in the menu at the top find the accound labl dba and editadd sparql_upd to role use the menu at the bottom if no dba account exist add one then add updat right start the synbiohub process npm start or npm runscript dev publish the repositori is set up to prohibit commit directli to the master branch commit must be made in anoth branch and then a github pr use to merg them into master github pr must be approv by at least one other develop befor they can be merg into master addit they must pass travi check which build a docker imag and run the sboltestsuit and synbiohub integr test against it each time a pr is merg into master the travi check are rerun on the master branch and if they succeed the result imag is push by travi to dockerhub under the tag snapshotstandalon publish a releas releas are publish automat use github action there is an action which fire on releas public it publish an imag to docker hub under the versionstandalon tag and updat the synbiohubdock master branch to point to thi version more inform avail here,synbiohub is a webbased repository for synthetic biology enabling user to browse upload and share synthetic biology design to learn more about the synbiohub including installation instruction and documentation visit the synbiohub wiki to access a sample instance of synbiohub containing enriched bacillus subtilis data feature from the escherichia coli genome and the complete igem registry of standard biological part visit synbiohuborg to access a bleedingedge version of synbiohub visit devsynbiohuborg installation the recommended way to install synbiohub is via the docker image see installation for more information manual installation synbiohub ha both javascript nodejs and java component prequisites linux only tested with ubuntu 180401 or macos if youre using macos first install homebrew a jdk o command ubuntu apt install defaultjdk mac brew install openjdk apache maven o command ubuntu apt install maven mac brew install maven nodejs 1100 o commandlink ubuntu visit httpsnodejsorgen mac brew install node openlink virtuoso 7xx o commandlink ubuntu visit httpsgithubcomopenlinkvirtuosoopensource mac brew install virtuoso rapper o command ubuntu apt install raptor2utils mac brew install raptor jq o command ubuntu apt install jq mac brew install jq ubuntu 180401 install virtuoso 7 from source at httpsgithubcomopenlinkvirtuosoopensource switch to the branch stable7 before installing follow the readme on installing virtuoso from source this involves installing all the dependency and running build command currently virtuoso doe not support version of openssl 110 and above or version of openssl below 100 when installing the dependency build from a binary between those version from httpswwwopensslorgsource set up the nodejs repository download the node setup script curl sl httpsdebnodesourcecomsetup_6x sudo e bash update your package repository apt update install the necessary package apt install defaultjdk maven raptor2utils nodejs jq buildessential python start virtuoso process virtuosot configfile usrlocalvirtuosoopensourcevarlibvirtuosodbvirtuosoini f macos install the necessary package brew install openjdk maven node virtuoso raptor jq python start virtuoso process cd usrlocalcellarvirtuoso7251_1varlibvirtuosodb the command above is based on where the virtuosoini file is located your installation might be located somewhere different than usrlocalcellarvirtuoso7251_1varlibvirtuosodb or the version might be different 7251_1 might be 7361_1 or any other version number if youre having trouble finding the location of the virtuosoini file run sudo find name virtuosoini press the control and c key simultaneously to quit the search virtuosot f both system clone the synbiohub repository git clone httpsgithubcomsynbiohubsynbiohub change to the synbiohub directory cd synbiohub build the java component with maven cd java mvn package return to the root directory and install the node dependency with yarn cd yarn install make sure that yarn is being used not cmdtest install nodemon and forever with npm install nodemon g npm install forever g add sparql update right to the dba user in virtuoso visit localhost8890 click conductor on the left hand side and login with user name dba and password dba visit system admin user account in the menu at the top find the accound labled dba and editadd sparql_update to role using the menu at the bottom if no dba account exists add one then add update right start the synbiohub process npm start or npm runscript dev publishing the repository is set up to prohibit commits directly to the master branch commits must be made in another branch and then a github pr used to merge them into master github pr must be approved by at least one other developer before they can be merged into master additionally they must pas travis check which build a docker image and run the sboltestsuite and synbiohub integration test against it each time a pr is merged into master the travis check are rerun on the master branch and if they succeed the resulting image is pushed by travis to dockerhub under the tag snapshotstandalone publishing a release release are published automatically using github action there is an action which fire on release publication it publishes an image to docker hub under the versionstandalone tag and update the synbiohubdocker master branch to point to this version more information available here,synbiohub webbased repository synthetic biology enabling user browse upload share synthetic biology design learn synbiohub including installation instruction documentation visit synbiohub wiki access sample instance synbiohub containing enriched bacillus subtilis data feature escherichia coli genome complete igem registry standard biological part visit synbiohuborg access bleedingedge version synbiohub visit devsynbiohuborg installation recommended way install synbiohub via docker image see installation information manual installation synbiohub javascript nodejs java component prequisites linux tested ubuntu 180401 macos youre using macos first install homebrew jdk o command ubuntu apt install defaultjdk mac brew install openjdk apache maven o command ubuntu apt install maven mac brew install maven nodejs 1100 o commandlink ubuntu visit httpsnodejsorgen mac brew install node openlink virtuoso 7xx o commandlink ubuntu visit httpsgithubcomopenlinkvirtuosoopensource mac brew install virtuoso rapper o command ubuntu apt install raptor2utils mac brew install raptor jq o command ubuntu apt install jq mac brew install jq ubuntu 180401 install virtuoso 7 source httpsgithubcomopenlinkvirtuosoopensource switch branch stable7 installing follow readme installing virtuoso source involves installing dependency running build command currently virtuoso support version openssl 110 version openssl 100 installing dependency build binary version httpswwwopensslorgsource set nodejs repository download node setup script curl sl httpsdebnodesourcecomsetup_6x sudo e bash update package repository apt update install necessary package apt install defaultjdk maven raptor2utils nodejs jq buildessential python start virtuoso process virtuosot configfile usrlocalvirtuosoopensourcevarlibvirtuosodbvirtuosoini f macos install necessary package brew install openjdk maven node virtuoso raptor jq python start virtuoso process cd usrlocalcellarvirtuoso7251_1varlibvirtuosodb command based virtuosoini file located installation might located somewhere different usrlocalcellarvirtuoso7251_1varlibvirtuosodb version might different 7251_1 might 7361_1 version number youre trouble finding location virtuosoini file run sudo find name virtuosoini press control c key simultaneously quit search virtuosot f system clone synbiohub repository git clone httpsgithubcomsynbiohubsynbiohub change synbiohub directory cd synbiohub build java component maven cd java mvn package return root directory install node dependency yarn cd yarn install make sure yarn used cmdtest install nodemon forever npm install nodemon g npm install forever g add sparql update right dba user virtuoso visit localhost8890 click conductor left hand side login user name dba password dba visit system admin user account menu top find accound labled dba editadd sparql_update role using menu bottom dba account exists add one add update right start synbiohub process npm start npm runscript dev publishing repository set prohibit commits directly master branch commits must made another branch github pr used merge master github pr must approved least one developer merged master additionally must pas travis check build docker image run sboltestsuite synbiohub integration test time pr merged master travis check rerun master branch succeed resulting image pushed travis dockerhub tag snapshotstandalone publishing release release published automatically using github action action fire release publication publishes image docker hub versionstandalone tag update synbiohubdocker master branch point version information available
JavaScript ,"



Clustergrammer is a web-based tool for visualizing high-dimensional data (e.g. a matrix) as an interactive and shareable hierarchically clustered heatmap. Clustergrammer's front end (Clustergrammer-JS) is built using D3.js and its back-end (Clustergrammer-PY) is built using Python. Clustergrammer produces highly interactive visualizations that enable intuitive exploration of high-dimensional data and has several biology-specific features (e.g. enrichment analysis, see Biology-Specific Features) to facilitate the exploration of gene-level biological data. Click the screenshot below to view an interactive tutorial:

Clustergrammer's interacive features include:

Zooming and Panning
Row and Column Reordering
Interactive Dendrogram
Interactive Dimensionality Reduction
Interactive Categories
Cropping
Row Searching
Biology-Specific Features

Clustergrammer can be used in three main ways (this repo contains the source code for Clustergrammer-JS):

Clustergrammer Web App (http://amp.pharm.mssm.edu/clustergrammer/)
Clustergrammer Jupyter Widget
Clustergrammer-JS and Clustergrammer-PY libraries

For information about building a webpage or app using Clustergrammer see: Web-Development with Clustergrammer
What's New
Clustergrammer2
 

Clustergrammer is being re-built using the WebGL library regl. The new in-development front-end is Clustergrammer-GL and the new in-development Jupyter widget is Clustergrammer2. The above notebook shows how Clustergrammer2 can be used to load a small dataset and visualize a large random DataFrame. By running the notebook on MyBinder using Jupyter Lab it can also be used to visualize a user uploaded dataset. Please see the video tutorial above for more information.
For additional examples and tutorials please see:

Case Studies and Tutorials
Clustergrammer2-Notebooks GitHub repository

JupyterCon 2018 Presentation

Clustergrammer was recently presented at JupyterCon 2018 (see slides).
Using Clustergrammer
Pleae see Clustergramer's documentation for detailed information or select a specific topic below:

Getting Started
Interacting with the Visualization
Web-Development with Clustergrammer (example pages)
Clustergrammer Web App and Clustergrammer Jupyter Widget
Matrix Formats and Input/Output
Core libraries: Clustergrammer-JS and Clustergrammer-PY
App Integration Examples
Case Studies and Examples
Biology-Specific Features
Developing Clustergrammer

Citing Clustergrammer
Please consider supporting Clustergrammer by citing our publication:
Fernandez, N. F. et al. Clustergrammer, a web-based heatmap visualization and analysis tool for high-dimensional biological data. Sci. Data 4:170151 doi: 10.1038/sdata.2017.151 (2017).
Licensing
Clustergrammer was developed by the Ma'ayan lab at the Icahn School of Medicine at Mount Sinai for the BD2K-LINCS DCIC and the KMC-IDG. Clustergrammer's license and third-party licenses are in the LICENSES directory and more information can be found at Clustergrammer License.
Please contact us for support, licensing questions, comments, and suggestions.
",clustergramm is a webbas tool for visual highdimension data eg a matrix as an interact and shareabl hierarch cluster heatmap clustergramm front end clustergrammerj is built use d3j and it backend clustergrammerpi is built use python clustergramm produc highli interact visual that enabl intuit explor of highdimension data and ha sever biologyspecif featur eg enrich analysi see biologyspecif featur to facilit the explor of genelevel biolog data click the screenshot below to view an interact tutori clustergramm interac featur includ zoom and pan row and column reorder interact dendrogram interact dimension reduct interact categori crop row search biologyspecif featur clustergramm can be use in three main way thi repo contain the sourc code for clustergrammerj clustergramm web app httpamppharmmssmeduclustergramm clustergramm jupyt widget clustergrammerj and clustergrammerpi librari for inform about build a webpag or app use clustergramm see webdevelop with clustergramm what new clustergrammer2 clustergramm is be rebuilt use the webgl librari regl the new indevelop frontend is clustergrammergl and the new indevelop jupyt widget is clustergrammer2 the abov notebook show how clustergrammer2 can be use to load a small dataset and visual a larg random datafram by run the notebook on mybind use jupyt lab it can also be use to visual a user upload dataset pleas see the video tutori abov for more inform for addit exampl and tutori pleas see case studi and tutori clustergrammer2notebook github repositori jupytercon 2018 present clustergramm wa recent present at jupytercon 2018 see slide use clustergramm pleae see clustergram document for detail inform or select a specif topic below get start interact with the visual webdevelop with clustergramm exampl page clustergramm web app and clustergramm jupyt widget matrix format and inputoutput core librari clustergrammerj and clustergrammerpi app integr exampl case studi and exampl biologyspecif featur develop clustergramm cite clustergramm pleas consid support clustergramm by cite our public fernandez n f et al clustergramm a webbas heatmap visual and analysi tool for highdimension biolog data sci data 4170151 doi 101038sdata2017151 2017 licens clustergramm wa develop by the maayan lab at the icahn school of medicin at mount sinai for the bd2klinc dcic and the kmcidg clustergramm licens and thirdparti licens are in the licens directori and more inform can be found at clustergramm licens pleas contact us for support licens question comment and suggest,clustergrammer is a webbased tool for visualizing highdimensional data eg a matrix a an interactive and shareable hierarchically clustered heatmap clustergrammers front end clustergrammerjs is built using d3js and it backend clustergrammerpy is built using python clustergrammer produce highly interactive visualization that enable intuitive exploration of highdimensional data and ha several biologyspecific feature eg enrichment analysis see biologyspecific feature to facilitate the exploration of genelevel biological data click the screenshot below to view an interactive tutorial clustergrammers interacive feature include zooming and panning row and column reordering interactive dendrogram interactive dimensionality reduction interactive category cropping row searching biologyspecific feature clustergrammer can be used in three main way this repo contains the source code for clustergrammerjs clustergrammer web app httpamppharmmssmeduclustergrammer clustergrammer jupyter widget clustergrammerjs and clustergrammerpy library for information about building a webpage or app using clustergrammer see webdevelopment with clustergrammer whats new clustergrammer2 clustergrammer is being rebuilt using the webgl library regl the new indevelopment frontend is clustergrammergl and the new indevelopment jupyter widget is clustergrammer2 the above notebook show how clustergrammer2 can be used to load a small dataset and visualize a large random dataframe by running the notebook on mybinder using jupyter lab it can also be used to visualize a user uploaded dataset please see the video tutorial above for more information for additional example and tutorial please see case study and tutorial clustergrammer2notebooks github repository jupytercon 2018 presentation clustergrammer wa recently presented at jupytercon 2018 see slide using clustergrammer pleae see clustergramers documentation for detailed information or select a specific topic below getting started interacting with the visualization webdevelopment with clustergrammer example page clustergrammer web app and clustergrammer jupyter widget matrix format and inputoutput core library clustergrammerjs and clustergrammerpy app integration example case study and example biologyspecific feature developing clustergrammer citing clustergrammer please consider supporting clustergrammer by citing our publication fernandez n f et al clustergrammer a webbased heatmap visualization and analysis tool for highdimensional biological data sci data 4170151 doi 101038sdata2017151 2017 licensing clustergrammer wa developed by the maayan lab at the icahn school of medicine at mount sinai for the bd2klincs dcic and the kmcidg clustergrammers license and thirdparty license are in the license directory and more information can be found at clustergrammer license please contact u for support licensing question comment and suggestion,clustergrammer webbased tool visualizing highdimensional data eg matrix interactive shareable hierarchically clustered heatmap clustergrammers front end clustergrammerjs built using d3js backend clustergrammerpy built using python clustergrammer produce highly interactive visualization enable intuitive exploration highdimensional data several biologyspecific feature eg enrichment analysis see biologyspecific feature facilitate exploration genelevel biological data click screenshot view interactive tutorial clustergrammers interacive feature include zooming panning row column reordering interactive dendrogram interactive dimensionality reduction interactive category cropping row searching biologyspecific feature clustergrammer used three main way repo contains source code clustergrammerjs clustergrammer web app httpamppharmmssmeduclustergrammer clustergrammer jupyter widget clustergrammerjs clustergrammerpy library information building webpage app using clustergrammer see webdevelopment clustergrammer whats new clustergrammer2 clustergrammer rebuilt using webgl library regl new indevelopment frontend clustergrammergl new indevelopment jupyter widget clustergrammer2 notebook show clustergrammer2 used load small dataset visualize large random dataframe running notebook mybinder using jupyter lab also used visualize user uploaded dataset please see video tutorial information additional example tutorial please see case study tutorial clustergrammer2notebooks github repository jupytercon 2018 presentation clustergrammer recently presented jupytercon 2018 see slide using clustergrammer pleae see clustergramers documentation detailed information select specific topic getting started interacting visualization webdevelopment clustergrammer example page clustergrammer web app clustergrammer jupyter widget matrix format inputoutput core library clustergrammerjs clustergrammerpy app integration example case study example biologyspecific feature developing clustergrammer citing clustergrammer please consider supporting clustergrammer citing publication fernandez n f et al clustergrammer webbased heatmap visualization analysis tool highdimensional biological data sci data 4170151 doi 101038sdata2017151 2017 licensing clustergrammer developed maayan lab icahn school medicine mount sinai bd2klincs dcic kmcidg clustergrammers license thirdparty license license directory information found clustergrammer license please contact u support licensing question comment suggestion
JavaScript ,"Boolean Network Simulator
This software provides a browseable user interface,
which allows the user to load Boolean networks in various formats,
display them and interactively simulate them.
It's coded in JavaScript, so it should work platform-independent.
A recent version of a HTML5-capable internet browser is required for proper rendering,
Chromium recommended (http://www.chromium.org/getting-involved/download-chromium).
Software license: GNU Affero GPL v3
Download using
git clone https://github.com/matthiasbock/BooleSim.git

After downloading, open or drag'n'drop index.html into your web browser.
Click here for help: https://github.com/matthiasbock/BooleSim/wiki
Built upon previous work located on Google Code at http://biographer.googlecode.com/
For questions, wishes or bug reports please visit https://github.com/matthiasbock/BooleSim/issues
Have fun!
",boolean network simul thi softwar provid a browseabl user interfac which allow the user to load boolean network in variou format display them and interact simul them it code in javascript so it should work platformindepend a recent version of a html5capabl internet browser is requir for proper render chromium recommend httpwwwchromiumorggettinginvolveddownloadchromium softwar licens gnu affero gpl v3 download use git clone httpsgithubcommatthiasbockboolesimgit after download open or dragndrop indexhtml into your web browser click here for help httpsgithubcommatthiasbockboolesimwiki built upon previou work locat on googl code at httpbiographergooglecodecom for question wish or bug report pleas visit httpsgithubcommatthiasbockboolesimissu have fun,boolean network simulator this software provides a browseable user interface which allows the user to load boolean network in various format display them and interactively simulate them it coded in javascript so it should work platformindependent a recent version of a html5capable internet browser is required for proper rendering chromium recommended httpwwwchromiumorggettinginvolveddownloadchromium software license gnu affero gpl v3 download using git clone httpsgithubcommatthiasbockboolesimgit after downloading open or dragndrop indexhtml into your web browser click here for help httpsgithubcommatthiasbockboolesimwiki built upon previous work located on google code at httpbiographergooglecodecom for question wish or bug report please visit httpsgithubcommatthiasbockboolesimissues have fun,boolean network simulator software provides browseable user interface allows user load boolean network various format display interactively simulate coded javascript work platformindependent recent version html5capable internet browser required proper rendering chromium recommended httpwwwchromiumorggettinginvolveddownloadchromium software license gnu affero gpl v3 download using git clone httpsgithubcommatthiasbockboolesimgit downloading open dragndrop indexhtml web browser click help httpsgithubcommatthiasbockboolesimwiki built upon previous work located google code httpbiographergooglecodecom question wish bug report please visit httpsgithubcommatthiasbockboolesimissues fun
JavaScript ,"The Noctua Stack
The Noctua Stack is a curation platform developped by the Gene Ontology Consortium. The stack is composed of:

Minerva: the backend data server to retrieve, store, update and delete annotations.
Barista: an authentication layer controling and formating all communications from/to Minerva.
Noctua: the website to browse the annotations in production and development and provide an editorial platform to produce Gene Ontology Causal Activity Models (or GO-CAMs) using either the simple UI Noctua Form or the more advanced Graph Editor.

The biological knowledge are stored in RDF/OWL using the blazegraph triplestore implementation.
In effect, any piece of knowledge stored in RDF/OWL is a triple { subject, predicate, object } defining a relationship (or association) between a subject and an object. Those triples are also commonly stored in Turtle files.
Installation
Pre-requisite
You must have npm installed. On ubuntu/debian, simply type:
sudo apt-get install nodejs

On OSX, it is also possible to install npm either from nodejs.org or using brew:
brew install node

Steps for a local Installation
# The full Noctua stack is a multi-repositorie project; optionally create a main directory for the stack to contain all the repositories.
# These instruction assume that ""gulp"" is in your path; if local-only, use: `./node_modules/.bin/gulp`.

# Creating a local directory for our work.
mkdir noctua-stack && cd noctua-stack

# Repo containing metadata (users, groups, etc.).
git clone https://github.com/geneontology/go-site.git
# The data repo to start the store and save to.
git clone https://github.com/geneontology/noctua-models.git
# Repo for the backend server.
git clone https://github.com/geneontology/minerva.git
# Repo for the Noctua client and middleware (Barista).
git clone https://github.com/geneontology/noctua.git

# Build the Minerva server (and CLI).
cd minerva && sh ./build-cli.sh && cd ..

# Create default authentication users with your favorite editor.
mkdir barista
vim barista/local.yaml
-
 uri: 'http://orcid.org/XXXX-XXXX-XXXX-XXXX'
 username: my_username
 password: my_password

# Install Noctua Form (old ""simple-annoton-editor"")
git clone https://github.com/geneontology/noctua-form.git
git clone https://github.com/geneontology/noctua-landing-page.git

# Install Noctua as an all-local installation.
cd noctua
npm install
cp config/startup.yaml.stack-dev ./startup.yaml

# Edit configuration file (barista, user, group, noctua models location, minerva memory to at least 16GB, link to NoctuaForm / SAE)
vim startup.yaml

# Build the stack and Blazegraph Journal (triplestore)
./node_modules/.bin/gulp build
# If running first time.
./node_modules/.bin/gulp batch-minerva-destroy-journal
./node_modules/.bin/gulp batch-minerva-destroy-ontology-journal
./node_modules/.bin/gulp batch-minerva-create-journal

# Then launch the stack, waiting for each to successfully start up:
./node_modules/.bin/gulp run-minerva &> minerva.log &
./node_modules/.bin/gulp run-barista &> barista.log &
./node_modules/.bin/gulp run-noctua &> noctua.log &

Additional notes
Gulp Tasks

doc - build the docs, available in doc/
test - need more here
build - assemble the apps for running
watch - development file monitor
clean - clean out /doc and /deploy

In addition, the last 3 lines of the installation steps launch all the 3 layers of the Noctua Stack:
gulp run-barista &> barista.log &
gulp run-minerva &> minerva.log &
gulp run-noctua &> noctua.log &

And Gulp can be used to both destroy and create blazegraph journals (triplestore):
gulp batch-minerva-destroy-journal
gulp batch-minerva-destroy-ontology-journal
gulp batch-minerva-create-journal

Users & groups
Barista, the authentication layer needs two files to run: users.yaml and groups.yaml.
These files defined who is authorized to log in to the Noctua Stack to perform biological curations.

To know more about curation with the Noctua Stack, visit our wiki.
To request an account to curate with the Noctua Stack, contact us

Libraries and CLI to communicate with the Noctua Stack
bbop-manager-minerva
This is the high-level API with OWL formatted requests (e.g. add individual, add fact or evidence using class expressions).
https://github.com/berkeleybop/bbop-manager-minerva
minerva-requests
This is the request object used to format specific queries to Minerva. It is composed of a basic request object as well as a request_set designed to chain multiple request objects and speed up complex tasks.
https://github.com/berkeleybop/minerva-requests
Some useful details about the API are described here
CLI (REPL)
The Noctua REPL is a recommended step for anyone trying to learn the syntax and how to build requests to Minerva in the Noctua Stack.
As any REPL, it allows for the rapid testing of multiple commands and to check the responses from barista.
This project can be considered as a basic prototype for any other client wanting to interact with the stack.
https://github.com/geneontology/noctua-repl
Known issues
The bulk of major issues and feature requests are handled by the
tracker (https://github.com/geneontology/noctua/issues). If something is
not mentioned here or in the tracker, please contact Seth Carbon or Chris Mungall.

Sometimes, when moving instance or relations near a boundary, the
relations will fall out of sync; either move nearby instances or
refresh the model
Sometimes, when editing an instance, the relations (edges) will
fall out of sync; either move nearby instances or refresh the
model
The endpoint scheme is reversed between creation and instantiation
TODO, etc.

",the noctua stack the noctua stack is a curat platform develop by the gene ontolog consortium the stack is compos of minerva the backend data server to retriev store updat and delet annot barista an authent layer control and format all commun fromto minerva noctua the websit to brows the annot in product and develop and provid an editori platform to produc gene ontolog causal activ model or gocam use either the simpl ui noctua form or the more advanc graph editor the biolog knowledg are store in rdfowl use the blazegraph triplestor implement in effect ani piec of knowledg store in rdfowl is a tripl subject predic object defin a relationship or associ between a subject and an object those tripl are also commonli store in turtl file instal prerequisit you must have npm instal on ubuntudebian simpli type sudo aptget instal nodej on osx it is also possibl to instal npm either from nodejsorg or use brew brew instal node step for a local instal the full noctua stack is a multirepositori project option creat a main directori for the stack to contain all the repositori these instruct assum that gulp is in your path if localonli use node_modulesbingulp creat a local directori for our work mkdir noctuastack cd noctuastack repo contain metadata user group etc git clone httpsgithubcomgeneontologygositegit the data repo to start the store and save to git clone httpsgithubcomgeneontologynoctuamodelsgit repo for the backend server git clone httpsgithubcomgeneontologyminervagit repo for the noctua client and middlewar barista git clone httpsgithubcomgeneontologynoctuagit build the minerva server and cli cd minerva sh buildclish cd creat default authent user with your favorit editor mkdir barista vim baristalocalyaml uri httporcidorgxxxxxxxxxxxxxxxx usernam my_usernam password my_password instal noctua form old simpleannotoneditor git clone httpsgithubcomgeneontologynoctuaformgit git clone httpsgithubcomgeneontologynoctualandingpagegit instal noctua as an allloc instal cd noctua npm instal cp configstartupyamlstackdev startupyaml edit configur file barista user group noctua model locat minerva memori to at least 16gb link to noctuaform sae vim startupyaml build the stack and blazegraph journal triplestor node_modulesbingulp build if run first time node_modulesbingulp batchminervadestroyjourn node_modulesbingulp batchminervadestroyontologyjourn node_modulesbingulp batchminervacreatejourn then launch the stack wait for each to success start up node_modulesbingulp runminerva minervalog node_modulesbingulp runbarista baristalog node_modulesbingulp runnoctua noctualog addit note gulp task doc build the doc avail in doc test need more here build assembl the app for run watch develop file monitor clean clean out doc and deploy in addit the last 3 line of the instal step launch all the 3 layer of the noctua stack gulp runbarista baristalog gulp runminerva minervalog gulp runnoctua noctualog and gulp can be use to both destroy and creat blazegraph journal triplestor gulp batchminervadestroyjourn gulp batchminervadestroyontologyjourn gulp batchminervacreatejourn user group barista the authent layer need two file to run usersyaml and groupsyaml these file defin who is author to log in to the noctua stack to perform biolog curat to know more about curat with the noctua stack visit our wiki to request an account to curat with the noctua stack contact us librari and cli to commun with the noctua stack bbopmanagerminerva thi is the highlevel api with owl format request eg add individu add fact or evid use class express httpsgithubcomberkeleybopbbopmanagerminerva minervarequest thi is the request object use to format specif queri to minerva it is compos of a basic request object as well as a request_set design to chain multipl request object and speed up complex task httpsgithubcomberkeleybopminervarequest some use detail about the api are describ here cli repl the noctua repl is a recommend step for anyon tri to learn the syntax and how to build request to minerva in the noctua stack as ani repl it allow for the rapid test of multipl command and to check the respons from barista thi project can be consid as a basic prototyp for ani other client want to interact with the stack httpsgithubcomgeneontologynoctuarepl known issu the bulk of major issu and featur request are handl by the tracker httpsgithubcomgeneontologynoctuaissu if someth is not mention here or in the tracker pleas contact seth carbon or chri mungal sometim when move instanc or relat near a boundari the relat will fall out of sync either move nearbi instanc or refresh the model sometim when edit an instanc the relat edg will fall out of sync either move nearbi instanc or refresh the model the endpoint scheme is revers between creation and instanti todo etc,the noctua stack the noctua stack is a curation platform developped by the gene ontology consortium the stack is composed of minerva the backend data server to retrieve store update and delete annotation barista an authentication layer controling and formating all communication fromto minerva noctua the website to browse the annotation in production and development and provide an editorial platform to produce gene ontology causal activity model or gocams using either the simple ui noctua form or the more advanced graph editor the biological knowledge are stored in rdfowl using the blazegraph triplestore implementation in effect any piece of knowledge stored in rdfowl is a triple subject predicate object defining a relationship or association between a subject and an object those triple are also commonly stored in turtle file installation prerequisite you must have npm installed on ubuntudebian simply type sudo aptget install nodejs on osx it is also possible to install npm either from nodejsorg or using brew brew install node step for a local installation the full noctua stack is a multirepositorie project optionally create a main directory for the stack to contain all the repository these instruction assume that gulp is in your path if localonly use node_modulesbingulp creating a local directory for our work mkdir noctuastack cd noctuastack repo containing metadata user group etc git clone httpsgithubcomgeneontologygositegit the data repo to start the store and save to git clone httpsgithubcomgeneontologynoctuamodelsgit repo for the backend server git clone httpsgithubcomgeneontologyminervagit repo for the noctua client and middleware barista git clone httpsgithubcomgeneontologynoctuagit build the minerva server and cli cd minerva sh buildclish cd create default authentication user with your favorite editor mkdir barista vim baristalocalyaml uri httporcidorgxxxxxxxxxxxxxxxx username my_username password my_password install noctua form old simpleannotoneditor git clone httpsgithubcomgeneontologynoctuaformgit git clone httpsgithubcomgeneontologynoctualandingpagegit install noctua a an alllocal installation cd noctua npm install cp configstartupyamlstackdev startupyaml edit configuration file barista user group noctua model location minerva memory to at least 16gb link to noctuaform sae vim startupyaml build the stack and blazegraph journal triplestore node_modulesbingulp build if running first time node_modulesbingulp batchminervadestroyjournal node_modulesbingulp batchminervadestroyontologyjournal node_modulesbingulp batchminervacreatejournal then launch the stack waiting for each to successfully start up node_modulesbingulp runminerva minervalog node_modulesbingulp runbarista baristalog node_modulesbingulp runnoctua noctualog additional note gulp task doc build the doc available in doc test need more here build assemble the apps for running watch development file monitor clean clean out doc and deploy in addition the last 3 line of the installation step launch all the 3 layer of the noctua stack gulp runbarista baristalog gulp runminerva minervalog gulp runnoctua noctualog and gulp can be used to both destroy and create blazegraph journal triplestore gulp batchminervadestroyjournal gulp batchminervadestroyontologyjournal gulp batchminervacreatejournal user group barista the authentication layer need two file to run usersyaml and groupsyaml these file defined who is authorized to log in to the noctua stack to perform biological curations to know more about curation with the noctua stack visit our wiki to request an account to curate with the noctua stack contact u library and cli to communicate with the noctua stack bbopmanagerminerva this is the highlevel api with owl formatted request eg add individual add fact or evidence using class expression httpsgithubcomberkeleybopbbopmanagerminerva minervarequests this is the request object used to format specific query to minerva it is composed of a basic request object a well a a request_set designed to chain multiple request object and speed up complex task httpsgithubcomberkeleybopminervarequests some useful detail about the api are described here cli repl the noctua repl is a recommended step for anyone trying to learn the syntax and how to build request to minerva in the noctua stack a any repl it allows for the rapid testing of multiple command and to check the response from barista this project can be considered a a basic prototype for any other client wanting to interact with the stack httpsgithubcomgeneontologynoctuarepl known issue the bulk of major issue and feature request are handled by the tracker httpsgithubcomgeneontologynoctuaissues if something is not mentioned here or in the tracker please contact seth carbon or chris mungall sometimes when moving instance or relation near a boundary the relation will fall out of sync either move nearby instance or refresh the model sometimes when editing an instance the relation edge will fall out of sync either move nearby instance or refresh the model the endpoint scheme is reversed between creation and instantiation todo etc,noctua stack noctua stack curation platform developped gene ontology consortium stack composed minerva backend data server retrieve store update delete annotation barista authentication layer controling formating communication fromto minerva noctua website browse annotation production development provide editorial platform produce gene ontology causal activity model gocams using either simple ui noctua form advanced graph editor biological knowledge stored rdfowl using blazegraph triplestore implementation effect piece knowledge stored rdfowl triple subject predicate object defining relationship association subject object triple also commonly stored turtle file installation prerequisite must npm installed ubuntudebian simply type sudo aptget install nodejs osx also possible install npm either nodejsorg using brew brew install node step local installation full noctua stack multirepositorie project optionally create main directory stack contain repository instruction assume gulp path localonly use node_modulesbingulp creating local directory work mkdir noctuastack cd noctuastack repo containing metadata user group etc git clone httpsgithubcomgeneontologygositegit data repo start store save git clone httpsgithubcomgeneontologynoctuamodelsgit repo backend server git clone httpsgithubcomgeneontologyminervagit repo noctua client middleware barista git clone httpsgithubcomgeneontologynoctuagit build minerva server cli cd minerva sh buildclish cd create default authentication user favorite editor mkdir barista vim baristalocalyaml uri httporcidorgxxxxxxxxxxxxxxxx username my_username password my_password install noctua form old simpleannotoneditor git clone httpsgithubcomgeneontologynoctuaformgit git clone httpsgithubcomgeneontologynoctualandingpagegit install noctua alllocal installation cd noctua npm install cp configstartupyamlstackdev startupyaml edit configuration file barista user group noctua model location minerva memory least 16gb link noctuaform sae vim startupyaml build stack blazegraph journal triplestore node_modulesbingulp build running first time node_modulesbingulp batchminervadestroyjournal node_modulesbingulp batchminervadestroyontologyjournal node_modulesbingulp batchminervacreatejournal launch stack waiting successfully start node_modulesbingulp runminerva minervalog node_modulesbingulp runbarista baristalog node_modulesbingulp runnoctua noctualog additional note gulp task doc build doc available doc test need build assemble apps running watch development file monitor clean clean doc deploy addition last 3 line installation step launch 3 layer noctua stack gulp runbarista baristalog gulp runminerva minervalog gulp runnoctua noctualog gulp used destroy create blazegraph journal triplestore gulp batchminervadestroyjournal gulp batchminervadestroyontologyjournal gulp batchminervacreatejournal user group barista authentication layer need two file run usersyaml groupsyaml file defined authorized log noctua stack perform biological curations know curation noctua stack visit wiki request account curate noctua stack contact u library cli communicate noctua stack bbopmanagerminerva highlevel api owl formatted request eg add individual add fact evidence using class expression httpsgithubcomberkeleybopbbopmanagerminerva minervarequests request object used format specific query minerva composed basic request object well request_set designed chain multiple request object speed complex task httpsgithubcomberkeleybopminervarequests useful detail api described cli repl noctua repl recommended step anyone trying learn syntax build request minerva noctua stack repl allows rapid testing multiple command check response barista project considered basic prototype client wanting interact stack httpsgithubcomgeneontologynoctuarepl known issue bulk major issue feature request handled tracker httpsgithubcomgeneontologynoctuaissues something mentioned tracker please contact seth carbon chris mungall sometimes moving instance relation near boundary relation fall sync either move nearby instance refresh model sometimes editing instance relation edge fall sync either move nearby instance refresh model endpoint scheme reversed creation instantiation todo etc
JavaScript ,"
BSD-licensed implementation of the Synthetic Biology Open Language (SBOL) in JavaScript.
Requires a JavaScript environment with ES6 class support (e.g. recent versions of node, Chrome, ...)
Features:

Read generic RDF, XML
Serialize SBOL XML, JSON
Build SBOL documents programatically

Installation
npm install sboljs

Usage
var SBOLDocument = require('sboljs')

SBOLDocument.loadRDFFile('foo.xml', function(err, doc) {

    doc.componentDefinitions.forEach(function(componentDefinition) {

        console.log(componentDefinition.name)

    })

})

Documentation
",bsdlicens implement of the synthet biolog open languag sbol in javascript requir a javascript environ with es6 class support eg recent version of node chrome featur read gener rdf xml serial sbol xml json build sbol document programat instal npm instal sbolj usag var sboldocu requiresbolj sboldocumentloadrdffilefooxml functionerr doc doccomponentdefinitionsforeachfunctioncomponentdefinit consolelogcomponentdefinitionnam document,bsdlicensed implementation of the synthetic biology open language sbol in javascript requires a javascript environment with es6 class support eg recent version of node chrome feature read generic rdf xml serialize sbol xml json build sbol document programatically installation npm install sboljs usage var sboldocument requiresboljs sboldocumentloadrdffilefooxml functionerr doc doccomponentdefinitionsforeachfunctioncomponentdefinition consolelogcomponentdefinitionname documentation,bsdlicensed implementation synthetic biology open language sbol javascript requires javascript environment es6 class support eg recent version node chrome feature read generic rdf xml serialize sbol xml json build sbol document programatically installation npm install sboljs usage var sboldocument requiresboljs sboldocumentloadrdffilefooxml functionerr doc doccomponentdefinitionsforeachfunctioncomponentdefinition consolelogcomponentdefinitionname documentation
Python ,"PYDFS-LINEUP-OPTIMIZER 
pydfs-lineup-optimizer is a tool for creating optimal lineups for daily fantasy sport.
Installation
To install pydfs-lineup-optimizer, simply run:
$ pip install pydfs-lineup-optimizer

Support
Now it supports following dfs sites:



League
DraftKings
FanDuel
FantasyDraft
Yahoo
FanBall
DraftKings Captain Mode
FanDuel Single Game
DraftKings Tiers




NFL
+
+
+
+
+
+
+
-


NBA
+
+
+
+
-
+
+
+


NHL
+
+
+
+
-
-
-
+


MLB
+
+
+
+
-
+
-
+


WNBA
+
+
-
-
-
+
-
-


Golf
+
+
+
+
-
-
-
-


Soccer
+
-
-
+
-
+
-
-


CFL
+
-
-
-
-
-
-
-


LOL
-
+
-
-
-
+
+
-


MMA
+
+
-
-
-
-
-
-


NASCAR
+
+
-
-
-
-
-
-


Tennis
+
-
-
-
-
-
-
-


CSGO
+
-
-
-
-
-
-
-



Documentation
Documentation is available at https://pydfs-lineup-optimizer.readthedocs.io/en/latest
Example
Here is an example for evaluating optimal lineup for Yahoo fantasy NBA. It loads players list from ""yahoo-NBA.csv"" and select 10 best lineups.
from pydfs_lineup_optimizer import Site, Sport, get_optimizer


optimizer = get_optimizer(Site.YAHOO, Sport.BASKETBALL)
optimizer.load_players_from_csv(""yahoo-NBA.csv"")
for lineup in optimizer.optimize(10):
    print(lineup)
",pydfslineupoptim pydfslineupoptim is a tool for creat optim lineup for daili fantasi sport instal to instal pydfslineupoptim simpli run pip instal pydfslineupoptim support now it support follow df site leagu draftk fanduel fantasydraft yahoo fanbal draftk captain mode fanduel singl game draftk tier nfl nba nhl mlb wnba golf soccer cfl lol mma nascar tenni csgo document document is avail at httpspydfslineupoptimizerreadthedocsioenlatest exampl here is an exampl for evalu optim lineup for yahoo fantasi nba it load player list from yahoonbacsv and select 10 best lineup from pydfs_lineup_optim import site sport get_optim optim get_optimizersiteyahoo sportbasketbal optimizerload_players_from_csvyahoonbacsv for lineup in optimizeroptimize10 printlineup,pydfslineupoptimizer pydfslineupoptimizer is a tool for creating optimal lineup for daily fantasy sport installation to install pydfslineupoptimizer simply run pip install pydfslineupoptimizer support now it support following dfs site league draftkings fanduel fantasydraft yahoo fanball draftkings captain mode fanduel single game draftkings tier nfl nba nhl mlb wnba golf soccer cfl lol mma nascar tennis csgo documentation documentation is available at httpspydfslineupoptimizerreadthedocsioenlatest example here is an example for evaluating optimal lineup for yahoo fantasy nba it load player list from yahoonbacsv and select 10 best lineup from pydfs_lineup_optimizer import site sport get_optimizer optimizer get_optimizersiteyahoo sportbasketball optimizerload_players_from_csvyahoonbacsv for lineup in optimizeroptimize10 printlineup,pydfslineupoptimizer pydfslineupoptimizer tool creating optimal lineup daily fantasy sport installation install pydfslineupoptimizer simply run pip install pydfslineupoptimizer support support following dfs site league draftkings fanduel fantasydraft yahoo fanball draftkings captain mode fanduel single game draftkings tier nfl nba nhl mlb wnba golf soccer cfl lol mma nascar tennis csgo documentation documentation available httpspydfslineupoptimizerreadthedocsioenlatest example example evaluating optimal lineup yahoo fantasy nba load player list yahoonbacsv select 10 best lineup pydfs_lineup_optimizer import site sport get_optimizer optimizer get_optimizersiteyahoo sportbasketball optimizerload_players_from_csvyahoonbacsv lineup optimizeroptimize10 printlineup
Python ,"Sportsreference: A free sports API written for python






Contents

Installation
Examples
Get instances of all NHL teams for the 2018 season
Print every NBA team's name and abbreviation
Get a specific NFL team's season information
Print the date of every game for a NCAA Men's Basketball team
Print the number of interceptions by the away team in a NCAA Football game
Get a Pandas DataFrame of all stats for a MLB game
Find the number of goals a football team has scored


Documentation
Testing


Sportsreference is a free python API that pulls the stats from
www.sports-reference.com and allows them to be easily be used in python-based
applications, especially ones involving data analytics and machine learning.
Sportsreference exposes a plethora of sports information from major sports
leagues in North America, such as the MLB, NBA, College Football and Basketball,
NFL, and NHL. Sportsreference also now supports Professional Football (or
Soccer) for thousands of teams from leagues around the world. Every sport has
its own set of valid API queries ranging from the list of teams in a league, to
the date and time of a game, to the total number of wins a team has secured
during the season, and many, many more metrics that paint a more detailed
picture of how a team has performed during a game or throughout a season.

Installation
The easiest way to install sportsreference is by downloading the latest
released binary from PyPI using PIP. For instructions on installing PIP, visit
PyPA.io for detailed steps on
installing the package manager for your local environment.
Next, run:
pip install sportsreference

to download and install the latest official release of sportsreference on
your machine. You now have the latest stable version of sportsreference
installed and can begin using it following the examples below!
If the bleeding-edge version of sportsreference is desired, clone this
repository using git and install all of the package requirements with PIP:
git clone https://github.com/roclark/sportsreference
cd sportsreference
pip install -r requirements.txt

Once complete, create a Python wheel for your default version of Python by
running the following command:
python setup.py sdist bdist_wheel

This will create a .whl file in the dist directory which can be installed
with the following command:
pip install dist/*.whl


Examples
The following are a few examples showcasing how easy it can be to collect
an abundance of metrics and information from all of the tracked leagues. The
examples below are only a miniscule subset of the total number of statistics
that can be pulled using sportsreference. Visit the documentation on
Read The Docs for a
complete list of all information exposed by the API.

Get instances of all NHL teams for the 2018 season
from sportsreference.nhl.teams import Teams

teams = Teams(2018)

Print every NBA team's name and abbreviation
from sportsreference.nba.teams import Teams

teams = Teams()
for team in teams:
    print(team.name, team.abbreviation)

Get a specific NFL team's season information
from sportsreference.nfl.teams import Teams

teams = Teams()
lions = teams('DET')

Print the date of every game for a NCAA Men's Basketball team
from sportsreference.ncaab.schedule import Schedule

purdue_schedule = Schedule('purdue')
for game in purdue_schedule:
    print(game.date)

Print the number of interceptions by the away team in a NCAA Football game
from sportsreference.ncaaf.boxscore import Boxscore

championship_game = Boxscore('2018-01-08-georgia')
print(championship_game.away_interceptions)

Get a Pandas DataFrame of all stats for a MLB game
from sportsreference.mlb.boxscore import Boxscore

game = Boxscore('BOS201806070')
df = game.dataframe

Find the number of goals a football team has scored
from sportsreference.fb.team import Team

tottenham = Team('Tottenham Hotspur')
print(tottenham.goals_scored)

Documentation
Two blog posts detailing the creation and basic usage of sportsreference can
be found on The Medium at the following links:

Part 1: Creating a public sports API
Part 2: Pull any sports metric in 10 lines of Python

The second post in particular is a great guide for getting started with
sportsreference and is highly recommended for anyone who is new to the
package.
Complete documentation is hosted on
readthedocs.org. Refer to
the documentation for a full list of all metrics and information exposed by
sportsreference. The documentation is auto-generated using Sphinx based on the
docstrings in the sportsreference package.

Testing
Sportsreference contains a testing suite which aims to test all major portions
of code for proper functionality. To run the test suite against your
environment, ensure all of the requirements are installed by running:
pip install -r requirements.txt

Next, start the tests by running py.test while optionally including coverage
flags which identify the amount of production code covered by the testing
framework:
py.test --cov=sportsreference --cov-report term-missing tests/

If the tests were successful, it will return a green line will show a message at
the end of the output similar to the following:
======================= 380 passed in 245.56 seconds =======================

If a test failed, it will show the number of failed and what went wrong within
the test output. If that's the case, ensure you have the latest version of code
and are in a supported environment. Otherwise, create an issue on GitHub to
attempt to get the issue resolved.
",sportsrefer a free sport api written for python content instal exampl get instanc of all nhl team for the 2018 season print everi nba team name and abbrevi get a specif nfl team season inform print the date of everi game for a ncaa men basketbal team print the number of intercept by the away team in a ncaa footbal game get a panda datafram of all stat for a mlb game find the number of goal a footbal team ha score document test sportsrefer is a free python api that pull the stat from wwwsportsreferencecom and allow them to be easili be use in pythonbas applic especi one involv data analyt and machin learn sportsrefer expos a plethora of sport inform from major sport leagu in north america such as the mlb nba colleg footbal and basketbal nfl and nhl sportsrefer also now support profession footbal or soccer for thousand of team from leagu around the world everi sport ha it own set of valid api queri rang from the list of team in a leagu to the date and time of a game to the total number of win a team ha secur dure the season and mani mani more metric that paint a more detail pictur of how a team ha perform dure a game or throughout a season instal the easiest way to instal sportsrefer is by download the latest releas binari from pypi use pip for instruct on instal pip visit pypaio for detail step on instal the packag manag for your local environ next run pip instal sportsrefer to download and instal the latest offici releas of sportsrefer on your machin you now have the latest stabl version of sportsrefer instal and can begin use it follow the exampl below if the bleedingedg version of sportsrefer is desir clone thi repositori use git and instal all of the packag requir with pip git clone httpsgithubcomroclarksportsrefer cd sportsrefer pip instal r requirementstxt onc complet creat a python wheel for your default version of python by run the follow command python setuppi sdist bdist_wheel thi will creat a whl file in the dist directori which can be instal with the follow command pip instal distwhl exampl the follow are a few exampl showcas how easi it can be to collect an abund of metric and inform from all of the track leagu the exampl below are onli a miniscul subset of the total number of statist that can be pull use sportsrefer visit the document on read the doc for a complet list of all inform expos by the api get instanc of all nhl team for the 2018 season from sportsreferencenhlteam import team team teams2018 print everi nba team name and abbrevi from sportsreferencenbateam import team team team for team in team printteamnam teamabbrevi get a specif nfl team season inform from sportsreferencenflteam import team team team lion teamsdet print the date of everi game for a ncaa men basketbal team from sportsreferencencaabschedul import schedul purdue_schedul schedulepurdu for game in purdue_schedul printgamed print the number of intercept by the away team in a ncaa footbal game from sportsreferencencaafboxscor import boxscor championship_gam boxscore20180108georgia printchampionship_gameaway_intercept get a panda datafram of all stat for a mlb game from sportsreferencemlbboxscor import boxscor game boxscorebos201806070 df gamedatafram find the number of goal a footbal team ha score from sportsreferencefbteam import team tottenham teamtottenham hotspur printtottenhamgoals_scor document two blog post detail the creation and basic usag of sportsrefer can be found on the medium at the follow link part 1 creat a public sport api part 2 pull ani sport metric in 10 line of python the second post in particular is a great guid for get start with sportsrefer and is highli recommend for anyon who is new to the packag complet document is host on readthedocsorg refer to the document for a full list of all metric and inform expos by sportsrefer the document is autogener use sphinx base on the docstr in the sportsrefer packag test sportsrefer contain a test suit which aim to test all major portion of code for proper function to run the test suit against your environ ensur all of the requir are instal by run pip instal r requirementstxt next start the test by run pytest while option includ coverag flag which identifi the amount of product code cover by the test framework pytest covsportsrefer covreport termmiss test if the test were success it will return a green line will show a messag at the end of the output similar to the follow 380 pass in 24556 second if a test fail it will show the number of fail and what went wrong within the test output if that the case ensur you have the latest version of code and are in a support environ otherwis creat an issu on github to attempt to get the issu resolv,sportsreference a free sport api written for python content installation example get instance of all nhl team for the 2018 season print every nba team name and abbreviation get a specific nfl team season information print the date of every game for a ncaa men basketball team print the number of interception by the away team in a ncaa football game get a panda dataframe of all stats for a mlb game find the number of goal a football team ha scored documentation testing sportsreference is a free python api that pull the stats from wwwsportsreferencecom and allows them to be easily be used in pythonbased application especially one involving data analytics and machine learning sportsreference expose a plethora of sport information from major sport league in north america such a the mlb nba college football and basketball nfl and nhl sportsreference also now support professional football or soccer for thousand of team from league around the world every sport ha it own set of valid api query ranging from the list of team in a league to the date and time of a game to the total number of win a team ha secured during the season and many many more metric that paint a more detailed picture of how a team ha performed during a game or throughout a season installation the easiest way to install sportsreference is by downloading the latest released binary from pypi using pip for instruction on installing pip visit pypaio for detailed step on installing the package manager for your local environment next run pip install sportsreference to download and install the latest official release of sportsreference on your machine you now have the latest stable version of sportsreference installed and can begin using it following the example below if the bleedingedge version of sportsreference is desired clone this repository using git and install all of the package requirement with pip git clone httpsgithubcomroclarksportsreference cd sportsreference pip install r requirementstxt once complete create a python wheel for your default version of python by running the following command python setuppy sdist bdist_wheel this will create a whl file in the dist directory which can be installed with the following command pip install distwhl example the following are a few example showcasing how easy it can be to collect an abundance of metric and information from all of the tracked league the example below are only a miniscule subset of the total number of statistic that can be pulled using sportsreference visit the documentation on read the doc for a complete list of all information exposed by the api get instance of all nhl team for the 2018 season from sportsreferencenhlteams import team team teams2018 print every nba team name and abbreviation from sportsreferencenbateams import team team team for team in team printteamname teamabbreviation get a specific nfl team season information from sportsreferencenflteams import team team team lion teamsdet print the date of every game for a ncaa men basketball team from sportsreferencencaabschedule import schedule purdue_schedule schedulepurdue for game in purdue_schedule printgamedate print the number of interception by the away team in a ncaa football game from sportsreferencencaafboxscore import boxscore championship_game boxscore20180108georgia printchampionship_gameaway_interceptions get a panda dataframe of all stats for a mlb game from sportsreferencemlbboxscore import boxscore game boxscorebos201806070 df gamedataframe find the number of goal a football team ha scored from sportsreferencefbteam import team tottenham teamtottenham hotspur printtottenhamgoals_scored documentation two blog post detailing the creation and basic usage of sportsreference can be found on the medium at the following link part 1 creating a public sport api part 2 pull any sport metric in 10 line of python the second post in particular is a great guide for getting started with sportsreference and is highly recommended for anyone who is new to the package complete documentation is hosted on readthedocsorg refer to the documentation for a full list of all metric and information exposed by sportsreference the documentation is autogenerated using sphinx based on the docstrings in the sportsreference package testing sportsreference contains a testing suite which aim to test all major portion of code for proper functionality to run the test suite against your environment ensure all of the requirement are installed by running pip install r requirementstxt next start the test by running pytest while optionally including coverage flag which identify the amount of production code covered by the testing framework pytest covsportsreference covreport termmissing test if the test were successful it will return a green line will show a message at the end of the output similar to the following 380 passed in 24556 second if a test failed it will show the number of failed and what went wrong within the test output if thats the case ensure you have the latest version of code and are in a supported environment otherwise create an issue on github to attempt to get the issue resolved,sportsreference free sport api written python content installation example get instance nhl team 2018 season print every nba team name abbreviation get specific nfl team season information print date every game ncaa men basketball team print number interception away team ncaa football game get panda dataframe stats mlb game find number goal football team scored documentation testing sportsreference free python api pull stats wwwsportsreferencecom allows easily used pythonbased application especially one involving data analytics machine learning sportsreference expose plethora sport information major sport league north america mlb nba college football basketball nfl nhl sportsreference also support professional football soccer thousand team league around world every sport set valid api query ranging list team league date time game total number win team secured season many many metric paint detailed picture team performed game throughout season installation easiest way install sportsreference downloading latest released binary pypi using pip instruction installing pip visit pypaio detailed step installing package manager local environment next run pip install sportsreference download install latest official release sportsreference machine latest stable version sportsreference installed begin using following example bleedingedge version sportsreference desired clone repository using git install package requirement pip git clone httpsgithubcomroclarksportsreference cd sportsreference pip install r requirementstxt complete create python wheel default version python running following command python setuppy sdist bdist_wheel create whl file dist directory installed following command pip install distwhl example following example showcasing easy collect abundance metric information tracked league example miniscule subset total number statistic pulled using sportsreference visit documentation read doc complete list information exposed api get instance nhl team 2018 season sportsreferencenhlteams import team team teams2018 print every nba team name abbreviation sportsreferencenbateams import team team team team team printteamname teamabbreviation get specific nfl team season information sportsreferencenflteams import team team team lion teamsdet print date every game ncaa men basketball team sportsreferencencaabschedule import schedule purdue_schedule schedulepurdue game purdue_schedule printgamedate print number interception away team ncaa football game sportsreferencencaafboxscore import boxscore championship_game boxscore20180108georgia printchampionship_gameaway_interceptions get panda dataframe stats mlb game sportsreferencemlbboxscore import boxscore game boxscorebos201806070 df gamedataframe find number goal football team scored sportsreferencefbteam import team tottenham teamtottenham hotspur printtottenhamgoals_scored documentation two blog post detailing creation basic usage sportsreference found medium following link part 1 creating public sport api part 2 pull sport metric 10 line python second post particular great guide getting started sportsreference highly recommended anyone new package complete documentation hosted readthedocsorg refer documentation full list metric information exposed sportsreference documentation autogenerated using sphinx based docstrings sportsreference package testing sportsreference contains testing suite aim test major portion code proper functionality run test suite environment ensure requirement installed running pip install r requirementstxt next start test running pytest optionally including coverage flag identify amount production code covered testing framework pytest covsportsreference covreport termmissing test test successful return green line show message end output similar following 380 passed 24556 second test failed show number failed went wrong within test output thats case ensure latest version code supported environment otherwise create issue github attempt get issue resolved
Python ,"SportsBook
A sports data scraping and analysis tool
This project is in its very early days and currently only supports football (soccer).
Fearure list:


Import leagues and fixtures from one of two available online sources.


See a visual comparison of the teams playing in any game.


Run a benchmark comparison on a selected fixture.

This allows you to see the results of games where both teams have played the same opponent.
It shows results where both teams faced the same opponent at home, then away and then where
the home team have faced the opponent at home and the away team have faced the opponent away.
Only games where three of the above comparisons are shown. If the home team, at home, haven't
played the same team as the awyay team have played away on three occasions, the game is ommitted
from predictions.



Run manual comparison of two teams from any loaded leagues generating (somewhat inaccurate) predictions.


Run predictions on all loaded fixtures (more accurate as the teams are guaranteed to be from the same leagues).

Select from using all available league data to compare home and away goal scoring and conceding form or only using data from games
where the home team has played the same team at home as the away team as played away, making the data a fairer representation of the
team's capabilities.



Filter predictions to show games where specific requirements are met (eg. prediction of home side winning by 2 goals).


Filter filtered predictions further with other filters.


Filter predictions using special filters (either produced by guest contributors or specially designed filters for specific bet types).


Display filtered predictions and all predictions on screen.


Change the range of dates or games the predictions cover.


Display results from throughout the whole current season of each league.


Change the range of dates the results cover.


Produce a spreadsheet of all predictions or filtered predictions with a wealth of current stats for each team in each prediction.


Export currently loaded league data to a JSON file.


Import the league data from a JSON file.


The project is growing fairly quickly. I'd love to hear what your thoughts are and even keep you up to date with new features if you like. Join the Slack group here if you're interested:
https://join.slack.com/t/sportsbookgroup/shared_invite/enQtNDc4MjYwNzMwNzg4LTAzMDk0MDM3OWFiMGJhZWU2MzAyMzQyNGI4OTlhNjgxMWRlNTZjOTAzMTM3ODdhMDIxNDU3YjI2MzM4OTlmZjg
It'd be great to hear from you so please pop in and say hi!
",sportsbook a sport data scrape and analysi tool thi project is in it veri earli day and current onli support footbal soccer fearur list import leagu and fixtur from one of two avail onlin sourc see a visual comparison of the team play in ani game run a benchmark comparison on a select fixtur thi allow you to see the result of game where both team have play the same oppon it show result where both team face the same oppon at home then away and then where the home team have face the oppon at home and the away team have face the oppon away onli game where three of the abov comparison are shown if the home team at home havent play the same team as the awyay team have play away on three occas the game is ommit from predict run manual comparison of two team from ani load leagu gener somewhat inaccur predict run predict on all load fixtur more accur as the team are guarante to be from the same leagu select from use all avail leagu data to compar home and away goal score and conced form or onli use data from game where the home team ha play the same team at home as the away team as play away make the data a fairer represent of the team capabl filter predict to show game where specif requir are met eg predict of home side win by 2 goal filter filter predict further with other filter filter predict use special filter either produc by guest contributor or special design filter for specif bet type display filter predict and all predict on screen chang the rang of date or game the predict cover display result from throughout the whole current season of each leagu chang the rang of date the result cover produc a spreadsheet of all predict or filter predict with a wealth of current stat for each team in each predict export current load leagu data to a json file import the leagu data from a json file the project is grow fairli quickli id love to hear what your thought are and even keep you up to date with new featur if you like join the slack group here if your interest httpsjoinslackcomtsportsbookgroupshared_inviteenqtndc4mjywnzmwnzg4ltazmdk0mdm3owfimgjhzwu2mzaymzqyngi4otlhnjgxmwrlntzjotazmtm3oddhmdixndu3yji2mzm4otlmzjg itd be great to hear from you so pleas pop in and say hi,sportsbook a sport data scraping and analysis tool this project is in it very early day and currently only support football soccer fearure list import league and fixture from one of two available online source see a visual comparison of the team playing in any game run a benchmark comparison on a selected fixture this allows you to see the result of game where both team have played the same opponent it show result where both team faced the same opponent at home then away and then where the home team have faced the opponent at home and the away team have faced the opponent away only game where three of the above comparison are shown if the home team at home havent played the same team a the awyay team have played away on three occasion the game is ommitted from prediction run manual comparison of two team from any loaded league generating somewhat inaccurate prediction run prediction on all loaded fixture more accurate a the team are guaranteed to be from the same league select from using all available league data to compare home and away goal scoring and conceding form or only using data from game where the home team ha played the same team at home a the away team a played away making the data a fairer representation of the team capability filter prediction to show game where specific requirement are met eg prediction of home side winning by 2 goal filter filtered prediction further with other filter filter prediction using special filter either produced by guest contributor or specially designed filter for specific bet type display filtered prediction and all prediction on screen change the range of date or game the prediction cover display result from throughout the whole current season of each league change the range of date the result cover produce a spreadsheet of all prediction or filtered prediction with a wealth of current stats for each team in each prediction export currently loaded league data to a json file import the league data from a json file the project is growing fairly quickly id love to hear what your thought are and even keep you up to date with new feature if you like join the slack group here if youre interested httpsjoinslackcomtsportsbookgroupshared_inviteenqtndc4mjywnzmwnzg4ltazmdk0mdm3owfimgjhzwu2mzaymzqyngi4otlhnjgxmwrlntzjotazmtm3oddhmdixndu3yji2mzm4otlmzjg itd be great to hear from you so please pop in and say hi,sportsbook sport data scraping analysis tool project early day currently support football soccer fearure list import league fixture one two available online source see visual comparison team playing game run benchmark comparison selected fixture allows see result game team played opponent show result team faced opponent home away home team faced opponent home away team faced opponent away game three comparison shown home team home havent played team awyay team played away three occasion game ommitted prediction run manual comparison two team loaded league generating somewhat inaccurate prediction run prediction loaded fixture accurate team guaranteed league select using available league data compare home away goal scoring conceding form using data game home team played team home away team played away making data fairer representation team capability filter prediction show game specific requirement met eg prediction home side winning 2 goal filter filtered prediction filter filter prediction using special filter either produced guest contributor specially designed filter specific bet type display filtered prediction prediction screen change range date game prediction cover display result throughout whole current season league change range date result cover produce spreadsheet prediction filtered prediction wealth current stats team prediction export currently loaded league data json file import league data json file project growing fairly quickly id love hear thought even keep date new feature like join slack group youre interested httpsjoinslackcomtsportsbookgroupshared_inviteenqtndc4mjywnzmwnzg4ltazmdk0mdm3owfimgjhzwu2mzaymzqyngi4otlhnjgxmwrlntzjotazmtm3oddhmdixndu3yji2mzm4otlmzjg itd great hear please pop say hi
Python ,"Sports Tracker Liberator
Under a catchy name lies an implementation which uses/implements Endomondo Mobile Api.
Status
Currently only Endomondo Api is somewhat implemented. Retrieving stuff somewhat works, and submitting new workouts works as in most basic form. All social media crap is ignored.
Usage
Authentication
Endomondo implements basic token mechanism, except that earlier versions didn't do that correctly and was only protected by can't be arsed with it -securitysystem. Later App versions tried to fix this, while maintaining backward compatibility by implementing a second, secureToken param. This is mainly used for social media crap, so we'll conveniently ignore it.
To authenticate, one needs existing email and password:
from endomondo import MobileApi
endomondo = MobileApi(email='email@example.com', password='p4ssw0rd')

auth_token = endomondo.get_auth_token()

This will return an auth_token, which can be stored on keychain or similar. In future, it should be used to skip whole login juggalloo.
endomondo = MobileApi(auth_token=auth_token)

Retrieving workouts:
To retrieve latest workouts:
endomondo.get_workouts()

Endomondo Mobile Api provides some oddities, and one of them is that maxResults actually work! And as usually in REST APIs, before date can be defined:
workouts = endomondo.get_workouts(maxResults=2)
endomondo.get_workouts(before=workouts[-1].start_time)

Structure for workouts page is similar to other providers, except paging links are missing.
Retrieving single workout
MobileApi.get_workout() accepts either existing workout, or workout ID.
workout = endomondo.get_workout(workoutId='234246')
[...]
reload = endomondo.get_workout(workout)

For workouts, or workout history, you can pass fields param to define, which attributes you are interested at. Again, a bit suprisingly, this works.
social_workout = endomondo.get_workout(fields=['lcp_count'])

Thease attributes aren't documented anywhere, but most of known can be found ín MobileApi.get_workout(), and they follow somewhat logical naming conventions.
Creating a workout and a track.
Some, if not most of program logic lies in mobile app, and server just stores data. This means that one needs to calculate most of their data by themselves.
Currently only most basic workout creation is supported. Later if/when workout update is added, more features becomes available. This is somewhat related to implementation of Endomondo Mobile Api, as only sport type, calories, hydration, duration and distance in form of track point is created. After initial creation, rest of data is updated to it.
To create workout:
from endomondo import MobileApi, Workout, TrackPoint
from datetime import datetime

endomondo = MobileApi(auth_token='1234')

workout = Workout()

workout.start_time = datetime.utcnow()

# See ``workout.sports``
workout.sport = 0

# Units are in user' local units, and only Metric is supported.
# `distance` is in km.
# Note that at creation time, this is not required. It's only used for automatic track point generation.
workout.distance = 1.5

# Duration in seconds
workout.duration = 600

endomondo.post_workout(workout=workout, properties={'audioMessage': 'false'})

if workout.id:
	print ""Saved!""

Altought track points aren't technically required by Endomondo backend, which is likely just for a sake of backwards compatibility, and in practice you always want to have at least one TrackPoint in your Workout. MobileApi.post_workout() can automate this, and will create one if none exists.
Other
For other, tested, functionality see main.py
Disclaimer, legalese and everything else.
This is not affiliated or endorset by Endomondo, or any other party. If you are copying this for a commercial project, be aware that it might be so that clean room implementation rules aren't fully complied with.
",sport tracker liber under a catchi name lie an implement which usesimpl endomondo mobil api statu current onli endomondo api is somewhat implement retriev stuff somewhat work and submit new workout work as in most basic form all social media crap is ignor usag authent endomondo implement basic token mechan except that earlier version didnt do that correctli and wa onli protect by cant be ars with it securitysystem later app version tri to fix thi while maintain backward compat by implement a second securetoken param thi is mainli use for social media crap so well conveni ignor it to authent one need exist email and password from endomondo import mobileapi endomondo mobileapiemailemailexamplecom passwordp4ssw0rd auth_token endomondoget_auth_token thi will return an auth_token which can be store on keychain or similar in futur it should be use to skip whole login juggalloo endomondo mobileapiauth_tokenauth_token retriev workout to retriev latest workout endomondoget_workout endomondo mobil api provid some odditi and one of them is that maxresult actual work and as usual in rest api befor date can be defin workout endomondoget_workoutsmaxresults2 endomondoget_workoutsbeforeworkouts1start_tim structur for workout page is similar to other provid except page link are miss retriev singl workout mobileapiget_workout accept either exist workout or workout id workout endomondoget_workoutworkoutid234246 reload endomondoget_workoutworkout for workout or workout histori you can pass field param to defin which attribut you are interest at again a bit suprisingli thi work social_workout endomondoget_workoutfieldslcp_count theas attribut arent document anywher but most of known can be found n mobileapiget_workout and they follow somewhat logic name convent creat a workout and a track some if not most of program logic lie in mobil app and server just store data thi mean that one need to calcul most of their data by themselv current onli most basic workout creation is support later ifwhen workout updat is ad more featur becom avail thi is somewhat relat to implement of endomondo mobil api as onli sport type calori hydrat durat and distanc in form of track point is creat after initi creation rest of data is updat to it to creat workout from endomondo import mobileapi workout trackpoint from datetim import datetim endomondo mobileapiauth_token1234 workout workout workoutstart_tim datetimeutcnow see workoutsport workoutsport 0 unit are in user local unit and onli metric is support distanc is in km note that at creation time thi is not requir it onli use for automat track point gener workoutdist 15 durat in second workoutdur 600 endomondopost_workoutworkoutworkout propertiesaudiomessag fals if workoutid print save altought track point arent technic requir by endomondo backend which is like just for a sake of backward compat and in practic you alway want to have at least one trackpoint in your workout mobileapipost_workout can autom thi and will creat one if none exist other for other test function see mainpi disclaim legales and everyth els thi is not affili or endorset by endomondo or ani other parti if you are copi thi for a commerci project be awar that it might be so that clean room implement rule arent fulli compli with,sport tracker liberator under a catchy name lie an implementation which usesimplements endomondo mobile api status currently only endomondo api is somewhat implemented retrieving stuff somewhat work and submitting new workout work a in most basic form all social medium crap is ignored usage authentication endomondo implement basic token mechanism except that earlier version didnt do that correctly and wa only protected by cant be arsed with it securitysystem later app version tried to fix this while maintaining backward compatibility by implementing a second securetoken param this is mainly used for social medium crap so well conveniently ignore it to authenticate one need existing email and password from endomondo import mobileapi endomondo mobileapiemailemailexamplecom passwordp4ssw0rd auth_token endomondoget_auth_token this will return an auth_token which can be stored on keychain or similar in future it should be used to skip whole login juggalloo endomondo mobileapiauth_tokenauth_token retrieving workout to retrieve latest workout endomondoget_workouts endomondo mobile api provides some oddity and one of them is that maxresults actually work and a usually in rest apis before date can be defined workout endomondoget_workoutsmaxresults2 endomondoget_workoutsbeforeworkouts1start_time structure for workout page is similar to other provider except paging link are missing retrieving single workout mobileapiget_workout accepts either existing workout or workout id workout endomondoget_workoutworkoutid234246 reload endomondoget_workoutworkout for workout or workout history you can pas field param to define which attribute you are interested at again a bit suprisingly this work social_workout endomondoget_workoutfieldslcp_count thease attribute arent documented anywhere but most of known can be found n mobileapiget_workout and they follow somewhat logical naming convention creating a workout and a track some if not most of program logic lie in mobile app and server just store data this mean that one need to calculate most of their data by themselves currently only most basic workout creation is supported later ifwhen workout update is added more feature becomes available this is somewhat related to implementation of endomondo mobile api a only sport type calorie hydration duration and distance in form of track point is created after initial creation rest of data is updated to it to create workout from endomondo import mobileapi workout trackpoint from datetime import datetime endomondo mobileapiauth_token1234 workout workout workoutstart_time datetimeutcnow see workoutsports workoutsport 0 unit are in user local unit and only metric is supported distance is in km note that at creation time this is not required it only used for automatic track point generation workoutdistance 15 duration in second workoutduration 600 endomondopost_workoutworkoutworkout propertiesaudiomessage false if workoutid print saved altought track point arent technically required by endomondo backend which is likely just for a sake of backwards compatibility and in practice you always want to have at least one trackpoint in your workout mobileapipost_workout can automate this and will create one if none exists other for other tested functionality see mainpy disclaimer legalese and everything else this is not affiliated or endorset by endomondo or any other party if you are copying this for a commercial project be aware that it might be so that clean room implementation rule arent fully complied with,sport tracker liberator catchy name lie implementation usesimplements endomondo mobile api status currently endomondo api somewhat implemented retrieving stuff somewhat work submitting new workout work basic form social medium crap ignored usage authentication endomondo implement basic token mechanism except earlier version didnt correctly protected cant arsed securitysystem later app version tried fix maintaining backward compatibility implementing second securetoken param mainly used social medium crap well conveniently ignore authenticate one need existing email password endomondo import mobileapi endomondo mobileapiemailemailexamplecom passwordp4ssw0rd auth_token endomondoget_auth_token return auth_token stored keychain similar future used skip whole login juggalloo endomondo mobileapiauth_tokenauth_token retrieving workout retrieve latest workout endomondoget_workouts endomondo mobile api provides oddity one maxresults actually work usually rest apis date defined workout endomondoget_workoutsmaxresults2 endomondoget_workoutsbeforeworkouts1start_time structure workout page similar provider except paging link missing retrieving single workout mobileapiget_workout accepts either existing workout workout id workout endomondoget_workoutworkoutid234246 reload endomondoget_workoutworkout workout workout history pas field param define attribute interested bit suprisingly work social_workout endomondoget_workoutfieldslcp_count thease attribute arent documented anywhere known found n mobileapiget_workout follow somewhat logical naming convention creating workout track program logic lie mobile app server store data mean one need calculate data currently basic workout creation supported later ifwhen workout update added feature becomes available somewhat related implementation endomondo mobile api sport type calorie hydration duration distance form track point created initial creation rest data updated create workout endomondo import mobileapi workout trackpoint datetime import datetime endomondo mobileapiauth_token1234 workout workout workoutstart_time datetimeutcnow see workoutsports workoutsport 0 unit user local unit metric supported distance km note creation time required used automatic track point generation workoutdistance 15 duration second workoutduration 600 endomondopost_workoutworkoutworkout propertiesaudiomessage false workoutid &#9; print saved altought track point arent technically required endomondo backend likely sake backwards compatibility practice always want least one trackpoint workout mobileapipost_workout automate create one none exists tested functionality see mainpy disclaimer legalese everything else affiliated endorset endomondo party copying commercial project aware might clean room implementation rule arent fully complied
Python ,"NBA Player Movements
This is a script for visualization of NBA games from raw SportVU logs.
If you admire both Spurs' and Warriors' ball movement, Brad Stevens' playbook, or just miss KD in OKC you'll find this entertaining.
Examples




Usage

Clone this repo:

$ git clone https://github.com/linouk23/NBA-Player-Movements


Choose any NBA game from data/2016.NBA.Raw.SportVU.Game.Logs directory.


Generate an animation for the play by running the following script:


$ python3 main.py --path=Celtics@Lakers.json --event=140
required arguments:
  --path PATH    a path to json file to read the events from

optional arguments:
  --event EVENT  an index of the event to create the animation to
                 (the indexing start with zero, if you index goes beyond out
                 the total number of events (plays), it will show you the last
                 one of the game)
  -h, --help     show the help message and exit

",nba player movement thi is a script for visual of nba game from raw sportvu log if you admir both spur and warrior ball movement brad steven playbook or just miss kd in okc youll find thi entertain exampl usag clone thi repo git clone httpsgithubcomlinouk23nbaplayermov choos ani nba game from data2016nbarawsportvugamelog directori gener an anim for the play by run the follow script python3 mainpi pathcelticslakersjson event140 requir argument path path a path to json file to read the event from option argument event event an index of the event to creat the anim to the index start with zero if you index goe beyond out the total number of event play it will show you the last one of the game h help show the help messag and exit,nba player movement this is a script for visualization of nba game from raw sportvu log if you admire both spur and warrior ball movement brad stevens playbook or just miss kd in okc youll find this entertaining example usage clone this repo git clone httpsgithubcomlinouk23nbaplayermovements choose any nba game from data2016nbarawsportvugamelogs directory generate an animation for the play by running the following script python3 mainpy pathcelticslakersjson event140 required argument path path a path to json file to read the event from optional argument event event an index of the event to create the animation to the indexing start with zero if you index go beyond out the total number of event play it will show you the last one of the game h help show the help message and exit,nba player movement script visualization nba game raw sportvu log admire spur warrior ball movement brad stevens playbook miss kd okc youll find entertaining example usage clone repo git clone httpsgithubcomlinouk23nbaplayermovements choose nba game data2016nbarawsportvugamelogs directory generate animation play running following script python3 mainpy pathcelticslakersjson event140 required argument path path path json file read event optional argument event event index event create animation indexing start zero index go beyond total number event play show last one game h help show help message exit
Python ,"SportScanner

This project is no longer actively maintained. Please submit PRs and I will review but support is patchy at best! Thanks for watching!
Scanner and Metadata Agent for Plex that uses www.thesportsdb.com
#Installation
Plex main folder location:
* '%LOCALAPPDATA%\Plex Media Server\'                                        # Windows Vista/7/8
* '%USERPROFILE%\Local Settings\Application Data\Plex Media Server\'         # Windows XP, 2003, Home Server
* '$HOME/Library/Application Support/Plex Media Server/'                     # Mac OS
* '$PLEX_HOME/Library/Application Support/Plex Media Server/',               # Linux
* '/var/lib/plexmediaserver/Library/Application Support/Plex Media Server/', # Debian,Fedora,CentOS,Ubuntu
* '/usr/local/plexdata/Plex Media Server/',                                  # FreeBSD
* '/usr/pbi/plexmediaserver-amd64/plexdata/Plex Media Server/',              # FreeNAS
* '${JAIL_ROOT}/var/db/plexdata/Plex Media Server/',                         # FreeNAS
* '/c/.plex/Library/Application Support/Plex Media Server/',                 # ReadyNAS
* '/share/MD0_DATA/.qpkg/PlexMediaServer/Library/Plex Media Server/',        # QNAP
* '/volume1/Plex/Library/Application Support/Plex Media Server/',            # Synology, Asustor
* '/raid0/data/module/Plex/sys/Plex Media Server/',                          # Thecus
* '/raid0/data/PLEX_CONFIG/Plex Media Server/'                               # Thecus Plex community


Download the latest release from https://github.com/mmmmmtasty/SportScanner/releases
Extract files
Copy the extracted directory ""Scanners"" into your Plex main folder location - check the list above for more clues
Copy the extracted directory ""SportScanner.bundle"" into the Plug-ins directory in your main folder location - check the list above for more clues
You may need to restart Plex
Create a new library and under Advanced options you should be able to select ""SportScanner"" as both your scanner and metadata agent.

#Media Format
The SportScanner scanner requires one of two folder structures to work correctly, the first of which matches Plex's standard folder structure.
##RECOMMENDED METHOD
Follow the Plex standards for folder structure - TV Show\Season<files>. For SportScanner, TV Shows = League Name. For example for 2015/2016 NHL you would do something like the following:

~LibraryRoot/NHL/Season 1516/NHL.2015.09.25.New-York-Islanders.vs.Philadelphia-Flyers.720p.HDTV.60fps.x264-Reborn4HD_h.mp4

In this scenario you still need all the information in the file name, I aim to remove that requirement down the line. The only information that comes only from the folder structure is the season.
##Alternative naming standard
You can also choose to ignore the season directory and have the scanner work it out with a folder structure like so:

~LibraryRoot/Ice Hockey/NHL/NHL.2015.09.25.New-York-Islanders.vs.Philadelphia-Flyers.720p.HDTV.60fps.x264-Reborn4HD_h.mp4

THERE IS A DOWN SIDE TO THIS! For this to work you must include a file in each league directory called ""SportScanner.txt"" that contains information about how the seasons work for this sport. The first line in the file will always be ""XXXX"" or ""XXYY"". ""XXXX"" means that the seasons happens within one calendar year and will therefore be named ""2015"" of ""1999"" for example. ""XXYY"" means that a season occurs across two seasons and will take the format ""1516"" or ""9899"" for example. When you define the season as ""XXYY"" you MUST then on the next line write the integer values of a month and a day in the form ""month,day"". This should be a a month and a day somewhere in the off-season for that sport. This tells the scanner when one season has finished and the next one is beginning to ensure that it puts files in the correct season based off the date the event happened. As an example, if you are trying to add NHL you would create a file at the following path:

~LibraryRoot/Ice Hockey/NHL/SportScanner.txt

In this instance the contents of this file would be as follows, saying that seasons should be in ""XXYY"" format and a date in the middle of the off-season is 1st July:
XXYY
7,1
NOT RECOMMENDED (but works for now)
SportScanner does not actually pay attention to the name of the League directory when it comes to matching events - all info has to be in the filename. This means that you can still group all sports together and as long as they share a season format you can create a SportScanner.txt file as outlined above and everything will work.
This is rubbish, it kind of accidentally works, I don't recommend it as I will cut it out as part of improvement works in future.
#Known Issues

No posters for seasons
Can only handle individual files, not multipart or those in folders
All information must be in the filename regardless of the directory structure.

",sportscann thi project is no longer activ maintain pleas submit pr and i will review but support is patchi at best thank for watch scanner and metadata agent for plex that use wwwthesportsdbcom instal plex main folder locat localappdataplex media server window vista78 userprofileloc settingsappl dataplex media server window xp 2003 home server homelibraryappl supportplex media server mac os plex_homelibraryappl supportplex media server linux varlibplexmediaserverlibraryappl supportplex media server debianfedoracentosubuntu usrlocalplexdataplex media server freebsd usrpbiplexmediaserveramd64plexdataplex media server freena jail_rootvardbplexdataplex media server freena cplexlibraryappl supportplex media server readyna sharemd0_dataqpkgplexmediaserverlibraryplex media server qnap volume1plexlibraryappl supportplex media server synolog asustor raid0datamoduleplexsysplex media server thecu raid0dataplex_configplex media server thecu plex commun download the latest releas from httpsgithubcommmmmmtastysportscannerreleas extract file copi the extract directori scanner into your plex main folder locat check the list abov for more clue copi the extract directori sportscannerbundl into the plugin directori in your main folder locat check the list abov for more clue you may need to restart plex creat a new librari and under advanc option you should be abl to select sportscann as both your scanner and metadata agent media format the sportscann scanner requir one of two folder structur to work correctli the first of which match plex standard folder structur recommend method follow the plex standard for folder structur tv showseasonfil for sportscann tv show leagu name for exampl for 20152016 nhl you would do someth like the follow libraryrootnhlseason 1516nhl20150925newyorkislandersvsphiladelphiaflyers720phdtv60fpsx264reborn4hd_hmp4 in thi scenario you still need all the inform in the file name i aim to remov that requir down the line the onli inform that come onli from the folder structur is the season altern name standard you can also choos to ignor the season directori and have the scanner work it out with a folder structur like so libraryrootic hockeynhlnhl20150925newyorkislandersvsphiladelphiaflyers720phdtv60fpsx264reborn4hd_hmp4 there is a down side to thi for thi to work you must includ a file in each leagu directori call sportscannertxt that contain inform about how the season work for thi sport the first line in the file will alway be xxxx or xxyy xxxx mean that the season happen within one calendar year and will therefor be name 2015 of 1999 for exampl xxyy mean that a season occur across two season and will take the format 1516 or 9899 for exampl when you defin the season as xxyy you must then on the next line write the integ valu of a month and a day in the form monthday thi should be a a month and a day somewher in the offseason for that sport thi tell the scanner when one season ha finish and the next one is begin to ensur that it put file in the correct season base off the date the event happen as an exampl if you are tri to add nhl you would creat a file at the follow path libraryrootic hockeynhlsportscannertxt in thi instanc the content of thi file would be as follow say that season should be in xxyy format and a date in the middl of the offseason is 1st juli xxyy 71 not recommend but work for now sportscann doe not actual pay attent to the name of the leagu directori when it come to match event all info ha to be in the filenam thi mean that you can still group all sport togeth and as long as they share a season format you can creat a sportscannertxt file as outlin abov and everyth will work thi is rubbish it kind of accident work i dont recommend it as i will cut it out as part of improv work in futur known issu no poster for season can onli handl individu file not multipart or those in folder all inform must be in the filenam regardless of the directori structur,sportscanner this project is no longer actively maintained please submit pr and i will review but support is patchy at best thanks for watching scanner and metadata agent for plex that us wwwthesportsdbcom installation plex main folder location localappdataplex medium server window vista78 userprofilelocal settingsapplication dataplex medium server window xp 2003 home server homelibraryapplication supportplex medium server mac o plex_homelibraryapplication supportplex medium server linux varlibplexmediaserverlibraryapplication supportplex medium server debianfedoracentosubuntu usrlocalplexdataplex medium server freebsd usrpbiplexmediaserveramd64plexdataplex medium server freenas jail_rootvardbplexdataplex medium server freenas cplexlibraryapplication supportplex medium server readynas sharemd0_dataqpkgplexmediaserverlibraryplex medium server qnap volume1plexlibraryapplication supportplex medium server synology asustor raid0datamoduleplexsysplex medium server thecus raid0dataplex_configplex medium server thecus plex community download the latest release from httpsgithubcommmmmmtastysportscannerreleases extract file copy the extracted directory scanner into your plex main folder location check the list above for more clue copy the extracted directory sportscannerbundle into the plugins directory in your main folder location check the list above for more clue you may need to restart plex create a new library and under advanced option you should be able to select sportscanner a both your scanner and metadata agent medium format the sportscanner scanner requires one of two folder structure to work correctly the first of which match plexs standard folder structure recommended method follow the plex standard for folder structure tv showseasonfiles for sportscanner tv show league name for example for 20152016 nhl you would do something like the following libraryrootnhlseason 1516nhl20150925newyorkislandersvsphiladelphiaflyers720phdtv60fpsx264reborn4hd_hmp4 in this scenario you still need all the information in the file name i aim to remove that requirement down the line the only information that come only from the folder structure is the season alternative naming standard you can also choose to ignore the season directory and have the scanner work it out with a folder structure like so libraryrootice hockeynhlnhl20150925newyorkislandersvsphiladelphiaflyers720phdtv60fpsx264reborn4hd_hmp4 there is a down side to this for this to work you must include a file in each league directory called sportscannertxt that contains information about how the season work for this sport the first line in the file will always be xxxx or xxyy xxxx mean that the season happens within one calendar year and will therefore be named 2015 of 1999 for example xxyy mean that a season occurs across two season and will take the format 1516 or 9899 for example when you define the season a xxyy you must then on the next line write the integer value of a month and a day in the form monthday this should be a a month and a day somewhere in the offseason for that sport this tell the scanner when one season ha finished and the next one is beginning to ensure that it put file in the correct season based off the date the event happened a an example if you are trying to add nhl you would create a file at the following path libraryrootice hockeynhlsportscannertxt in this instance the content of this file would be a follows saying that season should be in xxyy format and a date in the middle of the offseason is 1st july xxyy 71 not recommended but work for now sportscanner doe not actually pay attention to the name of the league directory when it come to matching event all info ha to be in the filename this mean that you can still group all sport together and a long a they share a season format you can create a sportscannertxt file a outlined above and everything will work this is rubbish it kind of accidentally work i dont recommend it a i will cut it out a part of improvement work in future known issue no poster for season can only handle individual file not multipart or those in folder all information must be in the filename regardless of the directory structure,sportscanner project longer actively maintained please submit pr review support patchy best thanks watching scanner metadata agent plex us wwwthesportsdbcom installation plex main folder location localappdataplex medium server window vista78 userprofilelocal settingsapplication dataplex medium server window xp 2003 home server homelibraryapplication supportplex medium server mac o plex_homelibraryapplication supportplex medium server linux varlibplexmediaserverlibraryapplication supportplex medium server debianfedoracentosubuntu usrlocalplexdataplex medium server freebsd usrpbiplexmediaserveramd64plexdataplex medium server freenas jail_rootvardbplexdataplex medium server freenas cplexlibraryapplication supportplex medium server readynas sharemd0_dataqpkgplexmediaserverlibraryplex medium server qnap volume1plexlibraryapplication supportplex medium server synology asustor raid0datamoduleplexsysplex medium server thecus raid0dataplex_configplex medium server thecus plex community download latest release httpsgithubcommmmmmtastysportscannerreleases extract file copy extracted directory scanner plex main folder location check list clue copy extracted directory sportscannerbundle plugins directory main folder location check list clue may need restart plex create new library advanced option able select sportscanner scanner metadata agent medium format sportscanner scanner requires one two folder structure work correctly first match plexs standard folder structure recommended method follow plex standard folder structure tv showseasonfiles sportscanner tv show league name example 20152016 nhl would something like following libraryrootnhlseason 1516nhl20150925newyorkislandersvsphiladelphiaflyers720phdtv60fpsx264reborn4hd_hmp4 scenario still need information file name aim remove requirement line information come folder structure season alternative naming standard also choose ignore season directory scanner work folder structure like libraryrootice hockeynhlnhl20150925newyorkislandersvsphiladelphiaflyers720phdtv60fpsx264reborn4hd_hmp4 side work must include file league directory called sportscannertxt contains information season work sport first line file always xxxx xxyy xxxx mean season happens within one calendar year therefore named 2015 1999 example xxyy mean season occurs across two season take format 1516 9899 example define season xxyy must next line write integer value month day form monthday month day somewhere offseason sport tell scanner one season finished next one beginning ensure put file correct season based date event happened example trying add nhl would create file following path libraryrootice hockeynhlsportscannertxt instance content file would follows saying season xxyy format date middle offseason 1st july xxyy 71 recommended work sportscanner actually pay attention name league directory come matching event info filename mean still group sport together long share season format create sportscannertxt file outlined everything work rubbish kind accidentally work dont recommend cut part improvement work future known issue poster season handle individual file multipart folder information must filename regardless directory structure
Python ,"Sports Betting with RL
Overview
This is the code for this video on Youtube by Siraj Raval on Sports Betting using Reinforcement Learning. This is apart of the Move 37 course at the School of AI.
Dependencies
None.
Usage
Type python value_iteration.py into terminal and it will run.
History
This is an adapted version of the ""Gambler's Problem"" that I've applied to sports betting. Details below
-The Gambler Problem as discussed in Example 4.3 in Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.
-The problem from the book is described below:
Gambler’s Problem: A gambler has the opportunity to make bets
on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as
many dollars as he has staked on that flip; if it is tails, he loses his stake. The game
ends when the gambler wins by reaching his goal of $100, or loses by running out of
money. On each flip, the gambler must decide what portion of his capital to stake,
in integer numbers of dollars. This problem can be formulated as an undiscounted,
episodic, finite MDP. The state is the gambler’s capital, s ∈ {1, 2, . . . , 99} and the
actions are stakes, a ∈ {0, 1, . . . , min(s, 100−s)}. The reward is zero on all transitions
except those on which the gambler reaches his goal, when it is +1. The state-value
function then gives the probability of winning from each state. A policy is a mapping
from levels of capital to stakes. The optimal policy maximizes the probability of
reaching the goal. Let ph denote the probability of the coin coming up heads. If ph
is known, then the entire problem is known and it can be solved, for instance, by
value iteration
",sport bet with rl overview thi is the code for thi video on youtub by siraj raval on sport bet use reinforc learn thi is apart of the move 37 cours at the school of ai depend none usag type python value_iterationpi into termin and it will run histori thi is an adapt version of the gambler problem that ive appli to sport bet detail below the gambler problem as discuss in exampl 43 in reinforc learn an introduct by richard s sutton and andrew g barto the problem from the book is describ below gambler problem a gambler ha the opportun to make bet on the outcom of a sequenc of coin flip if the coin come up head he win as mani dollar as he ha stake on that flip if it is tail he lose hi stake the game end when the gambler win by reach hi goal of 100 or lose by run out of money on each flip the gambler must decid what portion of hi capit to stake in integ number of dollar thi problem can be formul as an undiscount episod finit mdp the state is the gambler capit s 1 2 99 and the action are stake a 0 1 min 100 the reward is zero on all transit except those on which the gambler reach hi goal when it is 1 the statevalu function then give the probabl of win from each state a polici is a map from level of capit to stake the optim polici maxim the probabl of reach the goal let ph denot the probabl of the coin come up head if ph is known then the entir problem is known and it can be solv for instanc by valu iter,sport betting with rl overview this is the code for this video on youtube by siraj raval on sport betting using reinforcement learning this is apart of the move 37 course at the school of ai dependency none usage type python value_iterationpy into terminal and it will run history this is an adapted version of the gambler problem that ive applied to sport betting detail below the gambler problem a discussed in example 43 in reinforcement learning an introduction by richard s sutton and andrew g barto the problem from the book is described below gambler problem a gambler ha the opportunity to make bet on the outcome of a sequence of coin flip if the coin come up head he win a many dollar a he ha staked on that flip if it is tail he loses his stake the game end when the gambler win by reaching his goal of 100 or loses by running out of money on each flip the gambler must decide what portion of his capital to stake in integer number of dollar this problem can be formulated a an undiscounted episodic finite mdp the state is the gambler capital s 1 2 99 and the action are stake a 0 1 min 100 the reward is zero on all transition except those on which the gambler reach his goal when it is 1 the statevalue function then give the probability of winning from each state a policy is a mapping from level of capital to stake the optimal policy maximizes the probability of reaching the goal let ph denote the probability of the coin coming up head if ph is known then the entire problem is known and it can be solved for instance by value iteration,sport betting rl overview code video youtube siraj raval sport betting using reinforcement learning apart move 37 course school ai dependency none usage type python value_iterationpy terminal run history adapted version gambler problem ive applied sport betting detail gambler problem discussed example 43 reinforcement learning introduction richard sutton andrew g barto problem book described gambler problem gambler opportunity make bet outcome sequence coin flip coin come head win many dollar staked flip tail loses stake game end gambler win reaching goal 100 loses running money flip gambler must decide portion capital stake integer number dollar problem formulated undiscounted episodic finite mdp state gambler capital 1 2 99 action stake 0 1 min 100 reward zero transition except gambler reach goal 1 statevalue function give probability winning state policy mapping level capital stake optimal policy maximizes probability reaching goal let ph denote probability coin coming head ph known entire problem known solved instance value iteration
Python ,"YHandler
Yahoo Fantasy Sports OAuth And Request Handler
This is the Python Script I use to access the Yahoo Fantasy Sports API via OAuth for my desktop app. It's still far from polished, and not the most generalized, but has been updated to work with newer version of the requests library. It should be okay with future versions of Requests as the OAuth support has been written specifically for Yahoo's OAuth 1.0a process, which allows refresh of the access token. It looks like they now also support OAuth 2.0, but still remain backward compatible with OAuth1.0a.

Installation
You can install using pip:
pip install YHandler


How To Use
Copy the auth.json.sample file and rename to auth.json and then place your consumer key, and consumer secret in the auth.json file.
In [1]: from YHandler import YHandler, YQuery

In [2]: handler = YHandler()

In [3]: query = YQuery(handler, 'nfl')

In [4]: query.get_games_info()
Out[4]:
[{'code': 'nfl',
  'key': '359',
  'name': 'Football',
  'season': '2016',
  'type': '2016'}]

In [5]: query.get_games_info(True)
Out[5]:
[{'code': 'nfl',
  'key': '359',
  'name': 'Football',
  'season': '2016',
  'type': '2016'}]

In [10]: query.get_user_leagues()
Out[10]:
[{'id': '577090',
  'is_finished': True,
  'name': 'IniTeCh',
  'season': '2015',
  'week': '16'},
 {'id': '126737',
  'is_finished': False,
  'name': 'Yahoo Public 126737',
  'season': '2016',
  'week': '1'}]

In [17]: query.find_player(126737, 'antonio brown')
Out[17]: [{'id': '24171', 'name': 'Antonio Brown', 'team': 'Pittsburgh Steelers'}]

In [18]: query.get_player_week_stats(24171, '8')
Out[18]:
{'0': {'detail': 'Games Played', 'name': 'GP', 'value': '0'},
 '1': {'detail': 'Passing Attempts', 'name': 'Pass Att', 'value': '0'},
 '10': {'detail': 'Rushing Touchdowns', 'name': 'Rush TD', 'value': '0'},
 '11': {'detail': 'Receptions', 'name': 'Rec', 'value': '0'},
 '12': {'detail': 'Reception Yards', 'name': 'Rec Yds', 'value': '0'},
 '13': {'detail': 'Reception Touchdowns', 'name': 'Rec TD', 'value': '0'},
 '14': {'detail': 'Return Yards', 'name': 'Ret Yds', 'value': '0'},
 '15': {'detail': 'Return Touchdowns', 'name': 'Ret TD', 'value': '0'},
 '16': {'detail': '2-Point Conversions', 'name': '2-PT', 'value': '0'},
 '17': {'detail': 'Fumbles', 'name': 'Fum', 'value': '0'},
 '18': {'detail': 'Fumbles Lost', 'name': 'Fum Lost', 'value': '0'},
 '2': {'detail': 'Completions', 'name': 'Comp', 'value': '0'},
 '3': {'detail': 'Incomplete Passes', 'name': 'Inc', 'value': '0'},
 '4': {'detail': 'Passing Yards', 'name': 'Pass Yds', 'value': '0'},
 '5': {'detail': 'Passing Touchdowns', 'name': 'Pass TD', 'value': '0'},
 '57': {'detail': 'Offensive Fumble Return TD',
  'name': 'Fum Ret TD',
  'value': '0'},
 '58': {'detail': 'Pick Sixes Thrown', 'name': 'Pick Six', 'value': '0'},
 '59': {'detail': '40+ Yard Completions', 'name': '40 Yd Comp', 'value': '0'},
 '6': {'detail': 'Interceptions', 'name': 'Int', 'value': '0'},
 '60': {'detail': '40+ Yard Passing Touchdowns',
  'name': '40 Yd Pass TD',
  'value': '0'},
 '61': {'detail': '40+ Yard Run', 'name': '40 Yd Rush', 'value': '0'},
 '62': {'detail': '40+ Yard Rushing Touchdowns',
  'name': '40 Yd Rush TD',
  'value': '0'},
 '63': {'detail': '40+ Yard Receptions', 'name': '40 Yd Rec', 'value': '0'},
 '64': {'detail': '40+ Yard Reception Touchdowns',
  'name': '40 Yd Rec TD',
  'value': '0'},
 '7': {'detail': 'Sacks', 'name': 'Sack', 'value': '0'},
 '78': {'detail': 'Targets', 'name': 'Targets', 'value': '0'},
 '79': {'detail': 'Passing 1st Downs', 'name': 'Pass 1st Downs', 'value': '0'},
 '8': {'detail': 'Rushing Attempts', 'name': 'Rush Att', 'value': '0'},
 '80': {'detail': 'Receiving 1st Downs',
  'name': 'Rec 1st Downs',
  'value': '0'},
 '81': {'detail': 'Rushing 1st Downs', 'name': 'Rush 1st Downs', 'value': '0'},
 '9': {'detail': 'Rushing Yards', 'name': 'Rush Yds', 'value': '0'}}

",yhandler yahoo fantasi sport oauth and request handler thi is the python script i use to access the yahoo fantasi sport api via oauth for my desktop app it still far from polish and not the most gener but ha been updat to work with newer version of the request librari it should be okay with futur version of request as the oauth support ha been written specif for yahoo oauth 10a process which allow refresh of the access token it look like they now also support oauth 20 but still remain backward compat with oauth10a instal you can instal use pip pip instal yhandler how to use copi the authjsonsampl file and renam to authjson and then place your consum key and consum secret in the authjson file in 1 from yhandler import yhandler yqueri in 2 handler yhandler in 3 queri yqueryhandl nfl in 4 queryget_games_info out4 code nfl key 359 name footbal season 2016 type 2016 in 5 queryget_games_infotru out5 code nfl key 359 name footbal season 2016 type 2016 in 10 queryget_user_leagu out10 id 577090 is_finish true name initech season 2015 week 16 id 126737 is_finish fals name yahoo public 126737 season 2016 week 1 in 17 queryfind_player126737 antonio brown out17 id 24171 name antonio brown team pittsburgh steeler in 18 queryget_player_week_stats24171 8 out18 0 detail game play name gp valu 0 1 detail pass attempt name pass att valu 0 10 detail rush touchdown name rush td valu 0 11 detail recept name rec valu 0 12 detail recept yard name rec yd valu 0 13 detail recept touchdown name rec td valu 0 14 detail return yard name ret yd valu 0 15 detail return touchdown name ret td valu 0 16 detail 2point convers name 2pt valu 0 17 detail fumbl name fum valu 0 18 detail fumbl lost name fum lost valu 0 2 detail complet name comp valu 0 3 detail incomplet pass name inc valu 0 4 detail pass yard name pass yd valu 0 5 detail pass touchdown name pass td valu 0 57 detail offens fumbl return td name fum ret td valu 0 58 detail pick six thrown name pick six valu 0 59 detail 40 yard complet name 40 yd comp valu 0 6 detail intercept name int valu 0 60 detail 40 yard pass touchdown name 40 yd pass td valu 0 61 detail 40 yard run name 40 yd rush valu 0 62 detail 40 yard rush touchdown name 40 yd rush td valu 0 63 detail 40 yard recept name 40 yd rec valu 0 64 detail 40 yard recept touchdown name 40 yd rec td valu 0 7 detail sack name sack valu 0 78 detail target name target valu 0 79 detail pass 1st down name pass 1st down valu 0 8 detail rush attempt name rush att valu 0 80 detail receiv 1st down name rec 1st down valu 0 81 detail rush 1st down name rush 1st down valu 0 9 detail rush yard name rush yd valu 0,yhandler yahoo fantasy sport oauth and request handler this is the python script i use to access the yahoo fantasy sport api via oauth for my desktop app it still far from polished and not the most generalized but ha been updated to work with newer version of the request library it should be okay with future version of request a the oauth support ha been written specifically for yahoo oauth 10a process which allows refresh of the access token it look like they now also support oauth 20 but still remain backward compatible with oauth10a installation you can install using pip pip install yhandler how to use copy the authjsonsample file and rename to authjson and then place your consumer key and consumer secret in the authjson file in 1 from yhandler import yhandler yquery in 2 handler yhandler in 3 query yqueryhandler nfl in 4 queryget_games_info out4 code nfl key 359 name football season 2016 type 2016 in 5 queryget_games_infotrue out5 code nfl key 359 name football season 2016 type 2016 in 10 queryget_user_leagues out10 id 577090 is_finished true name initech season 2015 week 16 id 126737 is_finished false name yahoo public 126737 season 2016 week 1 in 17 queryfind_player126737 antonio brown out17 id 24171 name antonio brown team pittsburgh steelers in 18 queryget_player_week_stats24171 8 out18 0 detail game played name gp value 0 1 detail passing attempt name pas att value 0 10 detail rushing touchdown name rush td value 0 11 detail reception name rec value 0 12 detail reception yard name rec yds value 0 13 detail reception touchdown name rec td value 0 14 detail return yard name ret yds value 0 15 detail return touchdown name ret td value 0 16 detail 2point conversion name 2pt value 0 17 detail fumble name fum value 0 18 detail fumble lost name fum lost value 0 2 detail completion name comp value 0 3 detail incomplete pass name inc value 0 4 detail passing yard name pas yds value 0 5 detail passing touchdown name pas td value 0 57 detail offensive fumble return td name fum ret td value 0 58 detail pick six thrown name pick six value 0 59 detail 40 yard completion name 40 yd comp value 0 6 detail interception name int value 0 60 detail 40 yard passing touchdown name 40 yd pas td value 0 61 detail 40 yard run name 40 yd rush value 0 62 detail 40 yard rushing touchdown name 40 yd rush td value 0 63 detail 40 yard reception name 40 yd rec value 0 64 detail 40 yard reception touchdown name 40 yd rec td value 0 7 detail sack name sack value 0 78 detail target name target value 0 79 detail passing 1st down name pas 1st down value 0 8 detail rushing attempt name rush att value 0 80 detail receiving 1st down name rec 1st down value 0 81 detail rushing 1st down name rush 1st down value 0 9 detail rushing yard name rush yds value 0,yhandler yahoo fantasy sport oauth request handler python script use access yahoo fantasy sport api via oauth desktop app still far polished generalized updated work newer version request library okay future version request oauth support written specifically yahoo oauth 10a process allows refresh access token look like also support oauth 20 still remain backward compatible oauth10a installation install using pip pip install yhandler use copy authjsonsample file rename authjson place consumer key consumer secret authjson file 1 yhandler import yhandler yquery 2 handler yhandler 3 query yqueryhandler nfl 4 queryget_games_info out4 code nfl key 359 name football season 2016 type 2016 5 queryget_games_infotrue out5 code nfl key 359 name football season 2016 type 2016 10 queryget_user_leagues out10 id 577090 is_finished true name initech season 2015 week 16 id 126737 is_finished false name yahoo public 126737 season 2016 week 1 17 queryfind_player126737 antonio brown out17 id 24171 name antonio brown team pittsburgh steelers 18 queryget_player_week_stats24171 8 out18 0 detail game played name gp value 0 1 detail passing attempt name pas att value 0 10 detail rushing touchdown name rush td value 0 11 detail reception name rec value 0 12 detail reception yard name rec yds value 0 13 detail reception touchdown name rec td value 0 14 detail return yard name ret yds value 0 15 detail return touchdown name ret td value 0 16 detail 2point conversion name 2pt value 0 17 detail fumble name fum value 0 18 detail fumble lost name fum lost value 0 2 detail completion name comp value 0 3 detail incomplete pass name inc value 0 4 detail passing yard name pas yds value 0 5 detail passing touchdown name pas td value 0 57 detail offensive fumble return td name fum ret td value 0 58 detail pick six thrown name pick six value 0 59 detail 40 yard completion name 40 yd comp value 0 6 detail interception name int value 0 60 detail 40 yard passing touchdown name 40 yd pas td value 0 61 detail 40 yard run name 40 yd rush value 0 62 detail 40 yard rushing touchdown name 40 yd rush td value 0 63 detail 40 yard reception name 40 yd rec value 0 64 detail 40 yard reception touchdown name 40 yd rec td value 0 7 detail sack name sack value 0 78 detail target name target value 0 79 detail passing 1st down name pas 1st down value 0 8 detail rushing attempt name rush att value 0 80 detail receiving 1st down name rec 1st down value 0 81 detail rushing 1st down name rush 1st down value 0 9 detail rushing yard name rush yds value 0
Python ,"跑步是不可能跑步的
高校体育app自动跑步
仅供交流学习使用，not for evil use :)
使用了百度地图api自动寻路（虽然可能很绕 已经挺像真人跑的喽 :) ）


使用方法
有 本地直接运行 和 运行微信机器人 两种方法， 微信机器人 只建议有服务器的同学使用
本地直接运行
windows

下载 /dist/run.exe， 运行，输入账号密码即可完成一次锻炼（为了避免封号， 完全模拟了跑步流程，耗时较长，未完成前不要关闭）

linux/macos
git clone https://github.com/FengLi666/sports.git
cd sports
pip3 install -r requirement.txt
export PYTHONPATH='.'
python3 ./mysports/run.py


输入账号密码
默认情况下跑步数据在一段时间后才会提交给app服务器(即你要保持这个进程一直运行）
如果想立即提交跑步数据
可以使用如下命令
python3 ./mysports/run.py --debug True

运行微信机器人
git clone https://github.com/FengLi666/sports.git
cd sports
pip3 install -r requirement.txt
export PYTHONPATH='.'
python3 ./wechat_bot/wechat_bot.py

具体见代码

感谢 @RyuBAI
",app not for evil use api window distrunex linuxmaco git clone httpsgithubcomfengli666sportsgit cd sport pip3 instal r requirementtxt export pythonpath python3 mysportsrunpi app python3 mysportsrunpi debug true git clone httpsgithubcomfengli666sportsgit cd sport pip3 instal r requirementtxt export pythonpath python3 wechat_botwechat_botpi ryubai,app not for evil use api window distrunexe linuxmacos git clone httpsgithubcomfengli666sportsgit cd sport pip3 install r requirementtxt export pythonpath python3 mysportsrunpy app python3 mysportsrunpy debug true git clone httpsgithubcomfengli666sportsgit cd sport pip3 install r requirementtxt export pythonpath python3 wechat_botwechat_botpy ryubai,app evil use api window distrunexe linuxmacos git clone httpsgithubcomfengli666sportsgit cd sport pip3 install r requirementtxt export pythonpath python3 mysportsrunpy app python3 mysportsrunpy debug true git clone httpsgithubcomfengli666sportsgit cd sport pip3 install r requirementtxt export pythonpath python3 wechat_botwechat_botpy ryubai
Python ,"  
   
 
 


sports-betting
sports-betting is a tool that makes it easy to create machine learning based
models for sports betting and evaluate their performance. It is compatible with
scikit-learn.

Documentation
Installation documentation, API documentation, and examples can be found on the
documentation.

Dependencies
sports-betting is tested to work under Python 3.6+. The dependencies are the
following:

numpy(>=1.1)
scikit-learn(>=0.21)

Additionally, to run the examples, you need matplotlib(>=2.0.0) and
pandas(>=0.22).

Installation
sports-betting is currently available on the PyPi's repository and you can
install it via pip:
pip install -U sports-betting

The package is released also in Anaconda Cloud platform:
conda install -c algowit sports-betting

If you prefer, you can clone it and run the setup.py file. Use the following
commands to get a copy from GitHub and install all dependencies:
git clone https://github.com/AlgoWit/sports-betting.git
cd sports-betting
pip install .

Or install using pip and GitHub:
pip install -U git+https://github.com/AlgoWit/sports-betting.git


Testing
After installation, you can use pytest to run the test suite:
make test

",sportsbet sportsbet is a tool that make it easi to creat machin learn base model for sport bet and evalu their perform it is compat with scikitlearn document instal document api document and exampl can be found on the document depend sportsbet is test to work under python 36 the depend are the follow numpy11 scikitlearn021 addit to run the exampl you need matplotlib200 and pandas022 instal sportsbet is current avail on the pypi repositori and you can instal it via pip pip instal u sportsbet the packag is releas also in anaconda cloud platform conda instal c algowit sportsbet if you prefer you can clone it and run the setuppi file use the follow command to get a copi from github and instal all depend git clone httpsgithubcomalgowitsportsbettinggit cd sportsbet pip instal or instal use pip and github pip instal u githttpsgithubcomalgowitsportsbettinggit test after instal you can use pytest to run the test suit make test,sportsbetting sportsbetting is a tool that make it easy to create machine learning based model for sport betting and evaluate their performance it is compatible with scikitlearn documentation installation documentation api documentation and example can be found on the documentation dependency sportsbetting is tested to work under python 36 the dependency are the following numpy11 scikitlearn021 additionally to run the example you need matplotlib200 and pandas022 installation sportsbetting is currently available on the pypis repository and you can install it via pip pip install u sportsbetting the package is released also in anaconda cloud platform conda install c algowit sportsbetting if you prefer you can clone it and run the setuppy file use the following command to get a copy from github and install all dependency git clone httpsgithubcomalgowitsportsbettinggit cd sportsbetting pip install or install using pip and github pip install u githttpsgithubcomalgowitsportsbettinggit testing after installation you can use pytest to run the test suite make test,sportsbetting sportsbetting tool make easy create machine learning based model sport betting evaluate performance compatible scikitlearn documentation installation documentation api documentation example found documentation dependency sportsbetting tested work python 36 dependency following numpy11 scikitlearn021 additionally run example need matplotlib200 pandas022 installation sportsbetting currently available pypis repository install via pip pip install u sportsbetting package released also anaconda cloud platform conda install c algowit sportsbetting prefer clone run setuppy file use following command get copy github install dependency git clone httpsgithubcomalgowitsportsbettinggit cd sportsbetting pip install install using pip github pip install u githttpsgithubcomalgowitsportsbettinggit testing installation use pytest run test suite make test
Python ,"Data Engineering Projects

Project 1: Data Modeling with Postgres
In this project, we apply Data Modeling with Postgres and build an ETL pipeline using Python. A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Currently, they are collecting data in json format and the analytics team is particularly interested in understanding what songs users are listening to.
Link: Data_Modeling_with_Postgres
Project 2: Data Modeling with Cassandra
In this project, we apply Data Modeling with Cassandra and build an ETL pipeline using Python. We will build a Data Model around our queries that we want to get answers for.
For our use case we want below answers:

Get details of a song that was herad on the music app history during a particular session.
Get songs played by a user during particular session on music app.
Get all users from the music app history who listened to a particular song.

Link : Data_Modeling_with_Apache_Cassandra
Project 3: Data Warehouse
In this project, we apply the Data Warehouse architectures we learnt and build a Data Warehouse on AWS cloud. We build an ETL pipeline to extract and transform data stored in json format in s3 buckets and move the data to Warehouse hosted on Amazon Redshift.
Use Redshift IaC script - Redshift_IaC_README
Link  - Data_Warehouse
Project 4: Data Lake
In this project, we will build a Data Lake on AWS cloud using Spark and AWS EMR cluster. The data lake will serve as a Single Source of Truth for the Analytics Platform. We will write spark jobs to perform ELT operations that picks data from landing zone on S3 and transform and stores data on the S3 processed zone.
Link: Data_Lake
Project 5: Data Pipelines with Airflow
In this project, we will orchestrate our Data Pipeline workflow using an open-source Apache project called Apache Airflow. We will schedule our ETL jobs in Airflow, create project related custom plugins and operators and automate the pipeline execution.
Link:  Airflow_Data_Pipelines
Project 6: Api Data to Postgres
In this project, we build an etl pipeline to fetch data from yelp API and insert it into the Postgres Database. This project is a very basic example of fetching real time data from an open source API.
Link: API to Postgres
CAPSTONE PROJECT
Udacity provides their own crafted Capstone project with dataset that include data on immigration to the United States, and supplementary datasets that include data on airport codes, U.S. city demographics, and temperature data.
I worked on my own open-ended project. 
Here is the link - goodreads_etl_pipeline
",data engin project project 1 data model with postgr in thi project we appli data model with postgr and build an etl pipelin use python a startup want to analyz the data theyv been collect on song and user activ on their new music stream app current they are collect data in json format and the analyt team is particularli interest in understand what song user are listen to link data_modeling_with_postgr project 2 data model with cassandra in thi project we appli data model with cassandra and build an etl pipelin use python we will build a data model around our queri that we want to get answer for for our use case we want below answer get detail of a song that wa herad on the music app histori dure a particular session get song play by a user dure particular session on music app get all user from the music app histori who listen to a particular song link data_modeling_with_apache_cassandra project 3 data warehous in thi project we appli the data warehous architectur we learnt and build a data warehous on aw cloud we build an etl pipelin to extract and transform data store in json format in s3 bucket and move the data to warehous host on amazon redshift use redshift iac script redshift_iac_readm link data_warehous project 4 data lake in thi project we will build a data lake on aw cloud use spark and aw emr cluster the data lake will serv as a singl sourc of truth for the analyt platform we will write spark job to perform elt oper that pick data from land zone on s3 and transform and store data on the s3 process zone link data_lak project 5 data pipelin with airflow in thi project we will orchestr our data pipelin workflow use an opensourc apach project call apach airflow we will schedul our etl job in airflow creat project relat custom plugin and oper and autom the pipelin execut link airflow_data_pipelin project 6 api data to postgr in thi project we build an etl pipelin to fetch data from yelp api and insert it into the postgr databas thi project is a veri basic exampl of fetch real time data from an open sourc api link api to postgr capston project udac provid their own craft capston project with dataset that includ data on immigr to the unit state and supplementari dataset that includ data on airport code us citi demograph and temperatur data i work on my own openend project here is the link goodreads_etl_pipelin,data engineering project project 1 data modeling with postgres in this project we apply data modeling with postgres and build an etl pipeline using python a startup want to analyze the data theyve been collecting on song and user activity on their new music streaming app currently they are collecting data in json format and the analytics team is particularly interested in understanding what song user are listening to link data_modeling_with_postgres project 2 data modeling with cassandra in this project we apply data modeling with cassandra and build an etl pipeline using python we will build a data model around our query that we want to get answer for for our use case we want below answer get detail of a song that wa herad on the music app history during a particular session get song played by a user during particular session on music app get all user from the music app history who listened to a particular song link data_modeling_with_apache_cassandra project 3 data warehouse in this project we apply the data warehouse architecture we learnt and build a data warehouse on aws cloud we build an etl pipeline to extract and transform data stored in json format in s3 bucket and move the data to warehouse hosted on amazon redshift use redshift iac script redshift_iac_readme link data_warehouse project 4 data lake in this project we will build a data lake on aws cloud using spark and aws emr cluster the data lake will serve a a single source of truth for the analytics platform we will write spark job to perform elt operation that pick data from landing zone on s3 and transform and store data on the s3 processed zone link data_lake project 5 data pipeline with airflow in this project we will orchestrate our data pipeline workflow using an opensource apache project called apache airflow we will schedule our etl job in airflow create project related custom plugins and operator and automate the pipeline execution link airflow_data_pipelines project 6 api data to postgres in this project we build an etl pipeline to fetch data from yelp api and insert it into the postgres database this project is a very basic example of fetching real time data from an open source api link api to postgres capstone project udacity provides their own crafted capstone project with dataset that include data on immigration to the united state and supplementary datasets that include data on airport code u city demographic and temperature data i worked on my own openended project here is the link goodreads_etl_pipeline,data engineering project project 1 data modeling postgres project apply data modeling postgres build etl pipeline using python startup want analyze data theyve collecting song user activity new music streaming app currently collecting data json format analytics team particularly interested understanding song user listening link data_modeling_with_postgres project 2 data modeling cassandra project apply data modeling cassandra build etl pipeline using python build data model around query want get answer use case want answer get detail song herad music app history particular session get song played user particular session music app get user music app history listened particular song link data_modeling_with_apache_cassandra project 3 data warehouse project apply data warehouse architecture learnt build data warehouse aws cloud build etl pipeline extract transform data stored json format s3 bucket move data warehouse hosted amazon redshift use redshift iac script redshift_iac_readme link data_warehouse project 4 data lake project build data lake aws cloud using spark aws emr cluster data lake serve single source truth analytics platform write spark job perform elt operation pick data landing zone s3 transform store data s3 processed zone link data_lake project 5 data pipeline airflow project orchestrate data pipeline workflow using opensource apache project called apache airflow schedule etl job airflow create project related custom plugins operator automate pipeline execution link airflow_data_pipelines project 6 api data postgres project build etl pipeline fetch data yelp api insert postgres database project basic example fetching real time data open source api link api postgres capstone project udacity provides crafted capstone project dataset include data immigration united state supplementary datasets include data airport code u city demographic temperature data worked openended project link goodreads_etl_pipeline
Python ,"

PyQtGraph
A pure-Python graphics library for PyQt/PySide/PyQt5/PySide2
Copyright 2020 Luke Campagnola, University of North Carolina at Chapel Hill
http://www.pyqtgraph.org
PyQtGraph is intended for use in mathematics / scientific / engineering applications.
Despite being written entirely in python, the library is fast due to its
heavy leverage of numpy for number crunching, Qt's GraphicsView framework for
2D display, and OpenGL for 3D display.
Requirements

Python 2.7, or 3.x
Required

PyQt 4.8+, PySide, PyQt5, or PySide2
numpy


Optional

scipy for image processing
pyopengl for 3D graphics
hdf5 for large hdf5 binary format support



Qt Bindings Test Matrix
The following table represents the python environments we test in our CI system.  Our CI system uses Ubuntu 18.04, Windows Server 2019, and macOS 10.15 base images.



Qt-Bindings
Python 2.7
Python 3.6
Python 3.7
Python 3.8




PyQt-4
✅
❌
❌
❌


PySide1
✅
❌
❌
❌


PyQt5-5.9
❌
✅
❌
❌


PySide2-5.13
❌
❌
✅
❌


PyQt5-Latest
❌
❌
❌
✅


PySide2-Latest
❌
❌
❌
✅




pyqtgraph has had some incompatibilities with PySide2 versions 5.6-5.11, and we recommend you avoid those versions if possible
on macOS with Python 2.7 and Qt4 bindings (PyQt4 or PySide) the openGL related visualizations do not work reliably

Support

Report issues on the GitHub issue tracker
Post questions to the mailing list / forum or StackOverflow

Installation Methods

From PyPI:

Last released version: pip install pyqtgraph
Latest development version: pip install git+https://github.com/pyqtgraph/pyqtgraph@master


From conda

Last released version: conda install -c conda-forge pyqtgraph


To install system-wide from source distribution: python setup.py install
Many linux package repositories have release versions.
To use with a specific project, simply copy the pyqtgraph subdirectory
anywhere that is importable from your project.

Documentation
The official documentation lives at https://pyqtgraph.readthedocs.io
The easiest way to learn pyqtgraph is to browse through the examples; run python -m pyqtgraph.examples to launch the examples application.
",pyqtgraph a purepython graphic librari for pyqtpysidepyqt5pyside2 copyright 2020 luke campagnola univers of north carolina at chapel hill httpwwwpyqtgraphorg pyqtgraph is intend for use in mathemat scientif engin applic despit be written entir in python the librari is fast due to it heavi leverag of numpi for number crunch qt graphicsview framework for 2d display and opengl for 3d display requir python 27 or 3x requir pyqt 48 pysid pyqt5 or pyside2 numpi option scipi for imag process pyopengl for 3d graphic hdf5 for larg hdf5 binari format support qt bind test matrix the follow tabl repres the python environ we test in our ci system our ci system use ubuntu 1804 window server 2019 and maco 1015 base imag qtbind python 27 python 36 python 37 python 38 pyqt4 pyside1 pyqt559 pyside2513 pyqt5latest pyside2latest pyqtgraph ha had some incompat with pyside2 version 56511 and we recommend you avoid those version if possibl on maco with python 27 and qt4 bind pyqt4 or pysid the opengl relat visual do not work reliabl support report issu on the github issu tracker post question to the mail list forum or stackoverflow instal method from pypi last releas version pip instal pyqtgraph latest develop version pip instal githttpsgithubcompyqtgraphpyqtgraphmast from conda last releas version conda instal c condaforg pyqtgraph to instal systemwid from sourc distribut python setuppi instal mani linux packag repositori have releas version to use with a specif project simpli copi the pyqtgraph subdirectori anywher that is import from your project document the offici document live at httpspyqtgraphreadthedocsio the easiest way to learn pyqtgraph is to brows through the exampl run python m pyqtgraphexampl to launch the exampl applic,pyqtgraph a purepython graphic library for pyqtpysidepyqt5pyside2 copyright 2020 luke campagnola university of north carolina at chapel hill httpwwwpyqtgraphorg pyqtgraph is intended for use in mathematics scientific engineering application despite being written entirely in python the library is fast due to it heavy leverage of numpy for number crunching qts graphicsview framework for 2d display and opengl for 3d display requirement python 27 or 3x required pyqt 48 pyside pyqt5 or pyside2 numpy optional scipy for image processing pyopengl for 3d graphic hdf5 for large hdf5 binary format support qt binding test matrix the following table represents the python environment we test in our ci system our ci system us ubuntu 1804 window server 2019 and macos 1015 base image qtbindings python 27 python 36 python 37 python 38 pyqt4 pyside1 pyqt559 pyside2513 pyqt5latest pyside2latest pyqtgraph ha had some incompatibility with pyside2 version 56511 and we recommend you avoid those version if possible on macos with python 27 and qt4 binding pyqt4 or pyside the opengl related visualization do not work reliably support report issue on the github issue tracker post question to the mailing list forum or stackoverflow installation method from pypi last released version pip install pyqtgraph latest development version pip install githttpsgithubcompyqtgraphpyqtgraphmaster from conda last released version conda install c condaforge pyqtgraph to install systemwide from source distribution python setuppy install many linux package repository have release version to use with a specific project simply copy the pyqtgraph subdirectory anywhere that is importable from your project documentation the official documentation life at httpspyqtgraphreadthedocsio the easiest way to learn pyqtgraph is to browse through the example run python m pyqtgraphexamples to launch the example application,pyqtgraph purepython graphic library pyqtpysidepyqt5pyside2 copyright 2020 luke campagnola university north carolina chapel hill httpwwwpyqtgraphorg pyqtgraph intended use mathematics scientific engineering application despite written entirely python library fast due heavy leverage numpy number crunching qts graphicsview framework 2d display opengl 3d display requirement python 27 3x required pyqt 48 pyside pyqt5 pyside2 numpy optional scipy image processing pyopengl 3d graphic hdf5 large hdf5 binary format support qt binding test matrix following table represents python environment test ci system ci system us ubuntu 1804 window server 2019 macos 1015 base image qtbindings python 27 python 36 python 37 python 38 pyqt4 pyside1 pyqt559 pyside2513 pyqt5latest pyside2latest pyqtgraph incompatibility pyside2 version 56511 recommend avoid version possible macos python 27 qt4 binding pyqt4 pyside opengl related visualization work reliably support report issue github issue tracker post question mailing list forum stackoverflow installation method pypi last released version pip install pyqtgraph latest development version pip install githttpsgithubcompyqtgraphpyqtgraphmaster conda last released version conda install c condaforge pyqtgraph install systemwide source distribution python setuppy install many linux package repository release version use specific project simply copy pyqtgraph subdirectory anywhere importable project documentation official documentation life httpspyqtgraphreadthedocsio easiest way learn pyqtgraph browse example run python pyqtgraphexamples launch example application
Python ,"Data Engineering 101: Building a Data Pipeline
This repository contains the files and data from the workshop as well as resources around Data Engineering. For the workshop (and after) we will use a Discord chatroom to keep the conversation going: https://discord.gg/86cYcgU.
And/or please do not hesitate to reach out to me directly via email at inquiries@jonathan.industries or over twitter @memoryphoneme
The presentation can be found on Slideshare here or in this repository (presentation.pdf). Video can be found here.


Throughout this workshop, you will learn how to make a scalable and sustainable data pipeline in Python with Luigi

Learning Objectives

Run a simple 1 stage Luigi flow reading/writing to local files
Write a Luigi flow containing stages with multiple dependencies

Visualize the progress of the flow using the centralized scheduler
Parameterize the flow from the command line
Output parameter specific output files


Manage serialization to/from a Postgres database
Integrate a Hadoop Map/Reduce task into an existing flow
Parallelize non-dependent stages of a multi-stage Luigi flow
Schedule a local Luigi job to run once every day
Run any arbitrary shell command in a repeatable way

Prerequisites
Prior experience with Python and the scientific Python stack is beneficial.  The workshop will focus on using the Luigi framework, but will have code from the following lobraries as well:

numpy
scikit-learn
Flask

Run the Code
Local

Install libraries and dependencies: pip install -r requirements.txt
Start the UI server: luigid --background --logdir logs
Navigate with a web browser to http://localhost:[port] where [port] is the port the luigid server has started on (luigid defaults to port 8082)
start the API Server: python app.py
Evaluate Model: python ml-pipeline.py EvaluateModel --input-dir text --lam 0.8
Run evaluation server (at localhost:9191): topmodel/topmodel_server.py
Run the final pipeline: python ml-pipeline.py BuildModels --input-dir text --num-topics 10 --lam 0.8

--
For parallelism, set --workers (note this is Task parallelism):
python ml-pipeline.py BuildModels --input-dir text --num-topics 10 --lam 0.8 --workers 4
Hadoop

Start Hadoop cluster: bin/start-dfs.sh; sbin/start-yarn.sh
Setup Directory Structure: hadoop fs -mkdir /tmp/text
Get files on cluster: hadoop fs -put ./data/text /tmp/text
Retrieve results: hadoop fs -getmerge /tmp/text-count/2012-06-01 ./counts.txt
View results: head ./counts.txt

Flask

docker run -it -v /LOCAL/PATH/TO/REPO/data-engineering-101:/root/workshop clearspandex/pydata-seattle bash
pip2 install flask
ipython2 app.py

Libraries Used

luigi
scikit-learn
nltk
ipdb

Whats in here?
text/                   20newsgroups text files
topmodel/               Stripe's topmodel evaluation library
example_luigi.py        example scaffold of a luigi pipeline
hadoop_word_count.py    example luigi pipeline using Hadoop
ml-pipeline.py          luigi pipeline covered in workshop
app.py                  Flask server to deploy a scikit-learn model
LICENSE                 Details of rights of use and distribution
presentation.pdf        lecture slides from presentation
readme.md               this file!

The Data
The data (in the text/ folder) is from the 20 newsgroups dataset, a standard benchmarking dataset for machine learning and NLP.  Each file in text corresponds to a single 'document' (or post) from one of two selected newsgroups (comp.sys.ibm.pc.hardware or alt.atheism).  The first line provides which group the document is from and everything thereafter is the body of the post.
comp.sys.ibm.pc.hardware
I'm looking for a better method to back up files.  Currently using a MaynStream
250Q that uses DC 6250 tapes.  I will need to have a capacity of 600 Mb to 1Gb
for future backups.  Only DOS files.

I would be VERY appreciative of information about backup devices or
manufacturers of these products.  Flopticals, DAT, tape, anything.  
If possible, please include price, backup speed, manufacturer (phone #?), 
and opinions about the quality/reliability.

Please E-Mail, I'll send summaries to those interested.

Thanx in advance,

Resources/References

Questioning the Lambda Architecture
Luigi: NYC Data Science Meetup
The Log: What every software engineer should know about real-time data's unifying abstraction
I (heart) Log
Why Loggly Loves Apache Kafka
Buffer's New Data Architecture
Putting Apache Kafka to Use
Metric Driven Development
The Unified Logging Infrastructure for Data Analytics at Twitter
Stream Processing and Mining just got more interesting
How to Beat the CAP Theorem
Beating the CAP Theorem Checklist

License
Copyright 2015 Jonathan Dinu.
All files and content licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License
",data engin 101 build a data pipelin thi repositori contain the file and data from the workshop as well as resourc around data engin for the workshop and after we will use a discord chatroom to keep the convers go httpsdiscordgg86cycgu andor pleas do not hesit to reach out to me directli via email at inquiriesjonathanindustri or over twitter memoryphonem the present can be found on slideshar here or in thi repositori presentationpdf video can be found here throughout thi workshop you will learn how to make a scalabl and sustain data pipelin in python with luigi learn object run a simpl 1 stage luigi flow readingwrit to local file write a luigi flow contain stage with multipl depend visual the progress of the flow use the central schedul parameter the flow from the command line output paramet specif output file manag serial tofrom a postgr databas integr a hadoop mapreduc task into an exist flow parallel nondepend stage of a multistag luigi flow schedul a local luigi job to run onc everi day run ani arbitrari shell command in a repeat way prerequisit prior experi with python and the scientif python stack is benefici the workshop will focu on use the luigi framework but will have code from the follow lobrari as well numpi scikitlearn flask run the code local instal librari and depend pip instal r requirementstxt start the ui server luigid background logdir log navig with a web browser to httplocalhostport where port is the port the luigid server ha start on luigid default to port 8082 start the api server python apppi evalu model python mlpipelinepi evaluatemodel inputdir text lam 08 run evalu server at localhost9191 topmodeltopmodel_serverpi run the final pipelin python mlpipelinepi buildmodel inputdir text numtop 10 lam 08 for parallel set worker note thi is task parallel python mlpipelinepi buildmodel inputdir text numtop 10 lam 08 worker 4 hadoop start hadoop cluster binstartdfssh sbinstartyarnsh setup directori structur hadoop fs mkdir tmptext get file on cluster hadoop fs put datatext tmptext retriev result hadoop fs getmerg tmptextcount20120601 countstxt view result head countstxt flask docker run it v localpathtorepodataengineering101rootworkshop clearspandexpydataseattl bash pip2 instal flask ipython2 apppi librari use luigi scikitlearn nltk ipdb what in here text 20newsgroup text file topmodel stripe topmodel evalu librari example_luigipi exampl scaffold of a luigi pipelin hadoop_word_countpi exampl luigi pipelin use hadoop mlpipelinepi luigi pipelin cover in workshop apppi flask server to deploy a scikitlearn model licens detail of right of use and distribut presentationpdf lectur slide from present readmemd thi file the data the data in the text folder is from the 20 newsgroup dataset a standard benchmark dataset for machin learn and nlp each file in text correspond to a singl document or post from one of two select newsgroup compsysibmpchardwar or altath the first line provid which group the document is from and everyth thereaft is the bodi of the post compsysibmpchardwar im look for a better method to back up file current use a maynstream 250q that use dc 6250 tape i will need to have a capac of 600 mb to 1gb for futur backup onli do file i would be veri appreci of inform about backup devic or manufactur of these product floptic dat tape anyth if possibl pleas includ price backup speed manufactur phone and opinion about the qualityreli pleas email ill send summari to those interest thanx in advanc resourcesrefer question the lambda architectur luigi nyc data scienc meetup the log what everi softwar engin should know about realtim data unifi abstract i heart log whi loggli love apach kafka buffer new data architectur put apach kafka to use metric driven develop the unifi log infrastructur for data analyt at twitter stream process and mine just got more interest how to beat the cap theorem beat the cap theorem checklist licens copyright 2015 jonathan dinu all file and content licens under creativ common attributionnoncommercialsharealik 40 intern public licens,data engineering 101 building a data pipeline this repository contains the file and data from the workshop a well a resource around data engineering for the workshop and after we will use a discord chatroom to keep the conversation going httpsdiscordgg86cycgu andor please do not hesitate to reach out to me directly via email at inquiriesjonathanindustries or over twitter memoryphoneme the presentation can be found on slideshare here or in this repository presentationpdf video can be found here throughout this workshop you will learn how to make a scalable and sustainable data pipeline in python with luigi learning objective run a simple 1 stage luigi flow readingwriting to local file write a luigi flow containing stage with multiple dependency visualize the progress of the flow using the centralized scheduler parameterize the flow from the command line output parameter specific output file manage serialization tofrom a postgres database integrate a hadoop mapreduce task into an existing flow parallelize nondependent stage of a multistage luigi flow schedule a local luigi job to run once every day run any arbitrary shell command in a repeatable way prerequisite prior experience with python and the scientific python stack is beneficial the workshop will focus on using the luigi framework but will have code from the following lobraries a well numpy scikitlearn flask run the code local install library and dependency pip install r requirementstxt start the ui server luigid background logdir log navigate with a web browser to httplocalhostport where port is the port the luigid server ha started on luigid default to port 8082 start the api server python apppy evaluate model python mlpipelinepy evaluatemodel inputdir text lam 08 run evaluation server at localhost9191 topmodeltopmodel_serverpy run the final pipeline python mlpipelinepy buildmodels inputdir text numtopics 10 lam 08 for parallelism set worker note this is task parallelism python mlpipelinepy buildmodels inputdir text numtopics 10 lam 08 worker 4 hadoop start hadoop cluster binstartdfssh sbinstartyarnsh setup directory structure hadoop f mkdir tmptext get file on cluster hadoop f put datatext tmptext retrieve result hadoop f getmerge tmptextcount20120601 countstxt view result head countstxt flask docker run it v localpathtorepodataengineering101rootworkshop clearspandexpydataseattle bash pip2 install flask ipython2 apppy library used luigi scikitlearn nltk ipdb whats in here text 20newsgroups text file topmodel stripe topmodel evaluation library example_luigipy example scaffold of a luigi pipeline hadoop_word_countpy example luigi pipeline using hadoop mlpipelinepy luigi pipeline covered in workshop apppy flask server to deploy a scikitlearn model license detail of right of use and distribution presentationpdf lecture slide from presentation readmemd this file the data the data in the text folder is from the 20 newsgroups dataset a standard benchmarking dataset for machine learning and nlp each file in text corresponds to a single document or post from one of two selected newsgroups compsysibmpchardware or altatheism the first line provides which group the document is from and everything thereafter is the body of the post compsysibmpchardware im looking for a better method to back up file currently using a maynstream 250q that us dc 6250 tape i will need to have a capacity of 600 mb to 1gb for future backup only do file i would be very appreciative of information about backup device or manufacturer of these product flopticals dat tape anything if possible please include price backup speed manufacturer phone and opinion about the qualityreliability please email ill send summary to those interested thanx in advance resourcesreferences questioning the lambda architecture luigi nyc data science meetup the log what every software engineer should know about realtime data unifying abstraction i heart log why loggly love apache kafka buffer new data architecture putting apache kafka to use metric driven development the unified logging infrastructure for data analytics at twitter stream processing and mining just got more interesting how to beat the cap theorem beating the cap theorem checklist license copyright 2015 jonathan dinu all file and content licensed under creative common attributionnoncommercialsharealike 40 international public license,data engineering 101 building data pipeline repository contains file data workshop well resource around data engineering workshop use discord chatroom keep conversation going httpsdiscordgg86cycgu andor please hesitate reach directly via email inquiriesjonathanindustries twitter memoryphoneme presentation found slideshare repository presentationpdf video found throughout workshop learn make scalable sustainable data pipeline python luigi learning objective run simple 1 stage luigi flow readingwriting local file write luigi flow containing stage multiple dependency visualize progress flow using centralized scheduler parameterize flow command line output parameter specific output file manage serialization tofrom postgres database integrate hadoop mapreduce task existing flow parallelize nondependent stage multistage luigi flow schedule local luigi job run every day run arbitrary shell command repeatable way prerequisite prior experience python scientific python stack beneficial workshop focus using luigi framework code following lobraries well numpy scikitlearn flask run code local install library dependency pip install r requirementstxt start ui server luigid background logdir log navigate web browser httplocalhostport port port luigid server started luigid default port 8082 start api server python apppy evaluate model python mlpipelinepy evaluatemodel inputdir text lam 08 run evaluation server localhost9191 topmodeltopmodel_serverpy run final pipeline python mlpipelinepy buildmodels inputdir text numtopics 10 lam 08 parallelism set worker note task parallelism python mlpipelinepy buildmodels inputdir text numtopics 10 lam 08 worker 4 hadoop start hadoop cluster binstartdfssh sbinstartyarnsh setup directory structure hadoop f mkdir tmptext get file cluster hadoop f put datatext tmptext retrieve result hadoop f getmerge tmptextcount20120601 countstxt view result head countstxt flask docker run v localpathtorepodataengineering101rootworkshop clearspandexpydataseattle bash pip2 install flask ipython2 apppy library used luigi scikitlearn nltk ipdb whats text 20newsgroups text file topmodel stripe topmodel evaluation library example_luigipy example scaffold luigi pipeline hadoop_word_countpy example luigi pipeline using hadoop mlpipelinepy luigi pipeline covered workshop apppy flask server deploy scikitlearn model license detail right use distribution presentationpdf lecture slide presentation readmemd file data data text folder 20 newsgroups dataset standard benchmarking dataset machine learning nlp file text corresponds single document post one two selected newsgroups compsysibmpchardware altatheism first line provides group document everything thereafter body post compsysibmpchardware im looking better method back file currently using maynstream 250q us dc 6250 tape need capacity 600 mb 1gb future backup do file would appreciative information backup device manufacturer product flopticals dat tape anything possible please include price backup speed manufacturer phone opinion qualityreliability please email ill send summary interested thanx advance resourcesreferences questioning lambda architecture luigi nyc data science meetup log every software engineer know realtime data unifying abstraction heart log loggly love apache kafka buffer new data architecture putting apache kafka use metric driven development unified logging infrastructure data analytics twitter stream processing mining got interesting beat cap theorem beating cap theorem checklist license copyright 2015 jonathan dinu file content licensed creative common attributionnoncommercialsharealike 40 international public license
Python ,"




Theme
Status




Python Version



Latest PyPI Release



Latest Conda Release



master Branch Build



develop Branch Build



Documentation Build



License



Code Style



Questions




What is Kedro?

""The centre of your data pipeline.""

Kedro is an open-source Python framework that applies software engineering best-practice to data and machine-learning pipelines.  You can use it, for example, to optimise the process of taking a machine learning model into a production environment. You can use Kedro to organise a single user project running on a local environment, or collaborate within a team on an enterprise-level project.
We provide a standard approach so that you can:

Worry less about how to write production-ready code,
Spend more time building data pipelines that are robust, scalable, deployable, reproducible and versioned,
Standardise the way that your team collaborates across your project.

How do I install Kedro?
kedro is a Python package built for Python 3.6, 3.7 and 3.8.
To install Kedro from the Python Package Index (PyPI) simply run:
pip install kedro

You can also install kedro using conda, a package and environment manager program bundled with Anaconda. With conda already installed, simply run:
conda install -c conda-forge kedro

Our Get Started guide contains full installation instructions, and includes how to set up Python virtual environments.
We also recommend the frequently asked questions and the API reference documentation for additional information.
What are the main features of Kedro?

A pipeline visualisation generated using Kedro-Viz



Feature
What is this?




Project Template
A standard, modifiable and easy-to-use project template based on Cookiecutter Data Science.


Data Catalog
A series of lightweight data connectors used for saving and loading data across many different file formats and file systems including local and network file systems, cloud object stores, and HDFS. The Data Catalog also includes data and model versioning for file-based systems. Used with a Python or YAML API.


Pipeline Abstraction
Automatic resolution of dependencies between pure Python functions and data pipeline visualisation using Kedro-Viz.


The Journal
An ability to reproduce pipeline runs with saved pipeline run results.


Coding Standards
Test-driven development using pytest, produce well-documented code using Sphinx, create linted code with support for flake8, isort and black and make use of the standard Python logging library.


Flexible Deployment
Deployment strategies that include the use of Docker with Kedro-Docker, conversion of Kedro pipelines into Airflow DAGs with Kedro-Airflow, leveraging a REST API endpoint with Kedro-Server (coming soon) and serving Kedro pipelines as a Python package. Kedro can be deployed locally, on-premise and cloud (AWS, Azure and Google Cloud Platform) servers, or clusters (EMR, EC2, Azure HDinsight and Databricks).



How do I use Kedro?
The Kedro documentation includes three examples to help get you started:

A typical ""Hello World"" example, for an entry-level description of the main Kedro concepts
The more detailed ""spaceflights"" tutorial to give you hands-on experience as you learn about Kedro

Additional documentation includes:

An overview of Kedro architecture
How to use the CLI offered by kedro_cli.py (kedro new, kedro run, ...)


Note: The CLI is a convenient tool for being able to run kedro commands but you can also invoke the Kedro CLI as a Python module with python -m kedro

Every Kedro function or class has extensive help, which you can call from a Python session as follows if the item is in local scope:
from kedro.io import MemoryDataSet
help(MemoryDataSet)

Why does Kedro exist?
Kedro is built upon our collective best-practice (and mistakes) trying to deliver real-world ML applications that have vast amounts of raw unvetted data. We developed Kedro to achieve the following:

Collaboration on an analytics codebase when different team members have varied exposure to software engineering best-practice
A focus on maintainable data and ML pipelines as the standard, instead of a singular activity of deploying models in production
A way to inspire the creation of reusable analytics code so that we never start from scratch when working on a new project
Efficient use of time because we're able to quickly move from experimentation into production

The humans behind Kedro
Kedro was originally designed by Aris Valtazanos and Nikolaos Tsaousis to solve challenges they faced in their project work.
Their work was later turned into an internal product by Peteris Erins, Ivan Danov, Nikolaos Kaltsas, Meisam Emamjome and Nikolaos Tsaousis.
Currently the core Kedro team consists of Yetunde Dada, Ivan Danov, Richard Westenra, Dmitrii Deriabin, Lorena Balan, Kiyohito Kunii, Zain Patel, Lim Hoang, Andrii Ivaniuk, Jo Stichbury, Laís Carvalho, Merel Theisen, Gabriel Comym, and Liam Brummitt
Former core team members with significant contributions include: Gordon Wrigley, Nasef Khan and Anton Kirilenko.
And last but not least, all the open-source contributors whose work went into all Kedro releases.
Can I contribute?
Yes! Want to help build Kedro? Check out our guide to contributing to Kedro.
Where can I learn more?
There is a growing community around Kedro. Have a look at the Kedro FAQs to find projects using Kedro and links to articles, podcasts and talks.
Who is using Kedro?

AI Singapore
Caterpillar
ElementAI
Jungle Scout
MercadoLibre Argentina
Mosaic Data Science
NaranjaX
Open Data Science LatAm
Retrieva
Roche
UrbanLogiq
XP
Dendra Systems

What licence do you use?
Kedro is licensed under the Apache 2.0 License.
We're hiring!
Do you want to be part of the team that builds Kedro and other great products at QuantumBlack? If so, you're in luck! QuantumBlack is currently hiring Software Engineers who love using data to drive their decisions. Take a look at our open positions and see if you're a fit.
",theme statu python version latest pypi releas latest conda releas master branch build develop branch build document build licens code style question what is kedro the centr of your data pipelin kedro is an opensourc python framework that appli softwar engin bestpractic to data and machinelearn pipelin you can use it for exampl to optimis the process of take a machin learn model into a product environ you can use kedro to organis a singl user project run on a local environ or collabor within a team on an enterpriselevel project we provid a standard approach so that you can worri less about how to write productionreadi code spend more time build data pipelin that are robust scalabl deploy reproduc and version standardis the way that your team collabor across your project how do i instal kedro kedro is a python packag built for python 36 37 and 38 to instal kedro from the python packag index pypi simpli run pip instal kedro you can also instal kedro use conda a packag and environ manag program bundl with anaconda with conda alreadi instal simpli run conda instal c condaforg kedro our get start guid contain full instal instruct and includ how to set up python virtual environ we also recommend the frequent ask question and the api refer document for addit inform what are the main featur of kedro a pipelin visualis gener use kedroviz featur what is thi project templat a standard modifi and easytous project templat base on cookiecutt data scienc data catalog a seri of lightweight data connector use for save and load data across mani differ file format and file system includ local and network file system cloud object store and hdf the data catalog also includ data and model version for filebas system use with a python or yaml api pipelin abstract automat resolut of depend between pure python function and data pipelin visualis use kedroviz the journal an abil to reproduc pipelin run with save pipelin run result code standard testdriven develop use pytest produc welldocu code use sphinx creat lint code with support for flake8 isort and black and make use of the standard python log librari flexibl deploy deploy strategi that includ the use of docker with kedrodock convers of kedro pipelin into airflow dag with kedroairflow leverag a rest api endpoint with kedroserv come soon and serv kedro pipelin as a python packag kedro can be deploy local onpremis and cloud aw azur and googl cloud platform server or cluster emr ec2 azur hdinsight and databrick how do i use kedro the kedro document includ three exampl to help get you start a typic hello world exampl for an entrylevel descript of the main kedro concept the more detail spaceflight tutori to give you handson experi as you learn about kedro addit document includ an overview of kedro architectur how to use the cli offer by kedro_clipi kedro new kedro run note the cli is a conveni tool for be abl to run kedro command but you can also invok the kedro cli as a python modul with python m kedro everi kedro function or class ha extens help which you can call from a python session as follow if the item is in local scope from kedroio import memorydataset helpmemorydataset whi doe kedro exist kedro is built upon our collect bestpractic and mistak tri to deliv realworld ml applic that have vast amount of raw unvet data we develop kedro to achiev the follow collabor on an analyt codebas when differ team member have vari exposur to softwar engin bestpractic a focu on maintain data and ml pipelin as the standard instead of a singular activ of deploy model in product a way to inspir the creation of reusabl analyt code so that we never start from scratch when work on a new project effici use of time becaus were abl to quickli move from experiment into product the human behind kedro kedro wa origin design by ari valtazano and nikolao tsaousi to solv challeng they face in their project work their work wa later turn into an intern product by peteri erin ivan danov nikolao kaltsa meisam emamjom and nikolao tsaousi current the core kedro team consist of yetund dada ivan danov richard westenra dmitrii deriabin lorena balan kiyohito kunii zain patel lim hoang andrii ivaniuk jo stichburi la carvalho merel theisen gabriel comym and liam brummitt former core team member with signific contribut includ gordon wrigley nasef khan and anton kirilenko and last but not least all the opensourc contributor whose work went into all kedro releas can i contribut ye want to help build kedro check out our guid to contribut to kedro where can i learn more there is a grow commun around kedro have a look at the kedro faq to find project use kedro and link to articl podcast and talk who is use kedro ai singapor caterpillar elementai jungl scout mercadolibr argentina mosaic data scienc naranjax open data scienc latam retrieva roch urbanlogiq xp dendra system what licenc do you use kedro is licens under the apach 20 licens were hire do you want to be part of the team that build kedro and other great product at quantumblack if so your in luck quantumblack is current hire softwar engin who love use data to drive their decis take a look at our open posit and see if your a fit,theme status python version latest pypi release latest conda release master branch build develop branch build documentation build license code style question what is kedro the centre of your data pipeline kedro is an opensource python framework that applies software engineering bestpractice to data and machinelearning pipeline you can use it for example to optimise the process of taking a machine learning model into a production environment you can use kedro to organise a single user project running on a local environment or collaborate within a team on an enterpriselevel project we provide a standard approach so that you can worry le about how to write productionready code spend more time building data pipeline that are robust scalable deployable reproducible and versioned standardise the way that your team collaborates across your project how do i install kedro kedro is a python package built for python 36 37 and 38 to install kedro from the python package index pypi simply run pip install kedro you can also install kedro using conda a package and environment manager program bundled with anaconda with conda already installed simply run conda install c condaforge kedro our get started guide contains full installation instruction and includes how to set up python virtual environment we also recommend the frequently asked question and the api reference documentation for additional information what are the main feature of kedro a pipeline visualisation generated using kedroviz feature what is this project template a standard modifiable and easytouse project template based on cookiecutter data science data catalog a series of lightweight data connector used for saving and loading data across many different file format and file system including local and network file system cloud object store and hdfs the data catalog also includes data and model versioning for filebased system used with a python or yaml api pipeline abstraction automatic resolution of dependency between pure python function and data pipeline visualisation using kedroviz the journal an ability to reproduce pipeline run with saved pipeline run result coding standard testdriven development using pytest produce welldocumented code using sphinx create linted code with support for flake8 isort and black and make use of the standard python logging library flexible deployment deployment strategy that include the use of docker with kedrodocker conversion of kedro pipeline into airflow dag with kedroairflow leveraging a rest api endpoint with kedroserver coming soon and serving kedro pipeline a a python package kedro can be deployed locally onpremise and cloud aws azure and google cloud platform server or cluster emr ec2 azure hdinsight and databricks how do i use kedro the kedro documentation includes three example to help get you started a typical hello world example for an entrylevel description of the main kedro concept the more detailed spaceflight tutorial to give you handson experience a you learn about kedro additional documentation includes an overview of kedro architecture how to use the cli offered by kedro_clipy kedro new kedro run note the cli is a convenient tool for being able to run kedro command but you can also invoke the kedro cli a a python module with python m kedro every kedro function or class ha extensive help which you can call from a python session a follows if the item is in local scope from kedroio import memorydataset helpmemorydataset why doe kedro exist kedro is built upon our collective bestpractice and mistake trying to deliver realworld ml application that have vast amount of raw unvetted data we developed kedro to achieve the following collaboration on an analytics codebase when different team member have varied exposure to software engineering bestpractice a focus on maintainable data and ml pipeline a the standard instead of a singular activity of deploying model in production a way to inspire the creation of reusable analytics code so that we never start from scratch when working on a new project efficient use of time because were able to quickly move from experimentation into production the human behind kedro kedro wa originally designed by aris valtazanos and nikolaos tsaousis to solve challenge they faced in their project work their work wa later turned into an internal product by peteris erin ivan danov nikolaos kaltsas meisam emamjome and nikolaos tsaousis currently the core kedro team consists of yetunde dada ivan danov richard westenra dmitrii deriabin lorena balan kiyohito kunii zain patel lim hoang andrii ivaniuk jo stichbury la carvalho merel theisen gabriel comym and liam brummitt former core team member with significant contribution include gordon wrigley nasef khan and anton kirilenko and last but not least all the opensource contributor whose work went into all kedro release can i contribute yes want to help build kedro check out our guide to contributing to kedro where can i learn more there is a growing community around kedro have a look at the kedro faq to find project using kedro and link to article podcasts and talk who is using kedro ai singapore caterpillar elementai jungle scout mercadolibre argentina mosaic data science naranjax open data science latam retrieva roche urbanlogiq xp dendra system what licence do you use kedro is licensed under the apache 20 license were hiring do you want to be part of the team that build kedro and other great product at quantumblack if so youre in luck quantumblack is currently hiring software engineer who love using data to drive their decision take a look at our open position and see if youre a fit,theme status python version latest pypi release latest conda release master branch build develop branch build documentation build license code style question kedro centre data pipeline kedro opensource python framework applies software engineering bestpractice data machinelearning pipeline use example optimise process taking machine learning model production environment use kedro organise single user project running local environment collaborate within team enterpriselevel project provide standard approach worry le write productionready code spend time building data pipeline robust scalable deployable reproducible versioned standardise way team collaborates across project install kedro kedro python package built python 36 37 38 install kedro python package index pypi simply run pip install kedro also install kedro using conda package environment manager program bundled anaconda conda already installed simply run conda install c condaforge kedro get started guide contains full installation instruction includes set python virtual environment also recommend frequently asked question api reference documentation additional information main feature kedro pipeline visualisation generated using kedroviz feature project template standard modifiable easytouse project template based cookiecutter data science data catalog series lightweight data connector used saving loading data across many different file format file system including local network file system cloud object store hdfs data catalog also includes data model versioning filebased system used python yaml api pipeline abstraction automatic resolution dependency pure python function data pipeline visualisation using kedroviz journal ability reproduce pipeline run saved pipeline run result coding standard testdriven development using pytest produce welldocumented code using sphinx create linted code support flake8 isort black make use standard python logging library flexible deployment deployment strategy include use docker kedrodocker conversion kedro pipeline airflow dag kedroairflow leveraging rest api endpoint kedroserver coming soon serving kedro pipeline python package kedro deployed locally onpremise cloud aws azure google cloud platform server cluster emr ec2 azure hdinsight databricks use kedro kedro documentation includes three example help get started typical hello world example entrylevel description main kedro concept detailed spaceflight tutorial give handson experience learn kedro additional documentation includes overview kedro architecture use cli offered kedro_clipy kedro new kedro run note cli convenient tool able run kedro command also invoke kedro cli python module python kedro every kedro function class extensive help call python session follows item local scope kedroio import memorydataset helpmemorydataset kedro exist kedro built upon collective bestpractice mistake trying deliver realworld ml application vast amount raw unvetted data developed kedro achieve following collaboration analytics codebase different team member varied exposure software engineering bestpractice focus maintainable data ml pipeline standard instead singular activity deploying model production way inspire creation reusable analytics code never start scratch working new project efficient use time able quickly move experimentation production human behind kedro kedro originally designed aris valtazanos nikolaos tsaousis solve challenge faced project work work later turned internal product peteris erin ivan danov nikolaos kaltsas meisam emamjome nikolaos tsaousis currently core kedro team consists yetunde dada ivan danov richard westenra dmitrii deriabin lorena balan kiyohito kunii zain patel lim hoang andrii ivaniuk jo stichbury la carvalho merel theisen gabriel comym liam brummitt former core team member significant contribution include gordon wrigley nasef khan anton kirilenko last least opensource contributor whose work went kedro release contribute yes want help build kedro check guide contributing kedro learn growing community around kedro look kedro faq find project using kedro link article podcasts talk using kedro ai singapore caterpillar elementai jungle scout mercadolibre argentina mosaic data science naranjax open data science latam retrieva roche urbanlogiq xp dendra system licence use kedro licensed apache 20 license hiring want part team build kedro great product quantumblack youre luck quantumblack currently hiring software engineer love using data drive decision take look open position see youre fit
Python ,"

















Geomancer is a geospatial feature engineering library. It leverages geospatial
data such as OpenStreetMap (OSM) alongside a
data warehouse like BigQuery. You can use this to create, share, and iterate
geospatial features for your downstream tasks (analysis, modelling,
visualization, etc.).



Features
Geomancer can perform geospatial feature engineering for all types of vector data
(i.e. points, lines, polygons).

Feature primitives for geospatial feature engineering
Ability to switch out data warehouses (BigQuery, SQLite, PostgreSQL (In Progress))
Compile and share your features using our SpellBook

Setup and Installation
Installing the library
Geomancer can be installed using pip.
$ pip install geomancer

This will install all dependencies for every data-warehouse we support. If
you wish to do this only for a specific warehouse, then you can add an
identifier:
$ pip install geomancer[bq] # For BigQuery
$ pip install geomancer[sqlite] # For SQLite
$ pip install geomancer[psql] # For PostgreSQL

Alternatively, you can also clone the repository then run install.
$ git clone https://github.com/thinkingmachines/geomancer.git
$ cd geomancer
$ python setup.py install

Setting up your data warehouse
Geomancer is powered by a geospatial data warehouse: we highly-recommend using
BigQuery as your data warehouse and
Geofabrik's OSM catalog as your
source of Points and Lines of interest.

You can see the set-up instructions in this link
Basic Usage
All of the feature engineering functions in Geomancer are called ""spells"". For
example, you want to get the distance to the nearest supermarket for each
point.
from geomancer.spells import DistanceToNearest

# Load your dataset in a pandas dataframe
# df = load_dataset()

dist_spell = DistanceToNearest(
    ""supermarket"",
    source_table=""ph_osm.gis_osm_pois_free_1"",
    feature_name=""dist_supermarket"",
    dburl=""bigquery://project-name"",
).cast(df)
You can specify the type of filter  using the format {column}:{filter}.  By
default, the column value is fclass. For example, if you wish to look for
roads on a bridge, then pass bridge:T:
from geomancer.spells import DistanceToNearest

# Load the dataset in a pandas dataframe
# df = load_dataset()

dist_spell = DistanceToNearest(
    ""bridge:T"",
    source_table=""ph_osm.gis_osm_roads_free_1"",
    feature_name=""dist_road_bridges"",
    dburl=""bigquery://project-name"",
).cast(df)
Compose multiple spells into a ""spell book"" which you can export as a JSON file.
from geomancer.spells import DistanceToNearest
from geomancer.spellbook import SpellBook

spellbook = SpellBook([
    DistanceToNearest(
        ""supermarket"",
        source_table=""ph_osm.gis_osm_pois_free_1"",
        feature_name=""dist_supermarket"",
        dburl=""bigquery://project-name"",
    ),
    DistanceToNearest(
        ""embassy"",
        source_table=""ph_osm.gis_osm_pois_free_1"",
        feature_name=""dist_embassy"",
        dburl=""bigquery://project-name"",
    ),
])
spellbook.to_json(""dist_supermarket_and_embassy.json"")
You can share the generated file so other people can re-use your feature extractions
with their own datasets.
from geomancer.spellbook import SpellBook

# Load the dataset in a pandas dataframe
# df = load_dataset()

spellbook = SpellBook.read_json(""dist_supermarket_and_embassy.json"")
dist_supermarket_and_embassy = spellbook.cast(df)
Contributing
This project is open for contributors! Contibutions can come in the form of
feature requests, bug fixes, documentation, tutorials and the like! We highly
recommend to file an Issue first before submitting a Pull
Request.
Simply fork this repository and make a Pull Request! We'd definitely appreciate:

Implementation of new features
Bug Reports
Documentation
Testing

Also, we have a
CONTRIBUTING
and a CODE_OF_CONDUCT,
so please check that one out!
License
MIT License © 2019, Thinking Machines Data Science
",geomanc is a geospati featur engin librari it leverag geospati data such as openstreetmap osm alongsid a data warehous like bigqueri you can use thi to creat share and iter geospati featur for your downstream task analysi model visual etc featur geomanc can perform geospati featur engin for all type of vector data ie point line polygon featur primit for geospati featur engin abil to switch out data warehous bigqueri sqlite postgresql in progress compil and share your featur use our spellbook setup and instal instal the librari geomanc can be instal use pip pip instal geomanc thi will instal all depend for everi datawarehous we support if you wish to do thi onli for a specif warehous then you can add an identifi pip instal geomancerbq for bigqueri pip instal geomancersqlit for sqlite pip instal geomancerpsql for postgresql altern you can also clone the repositori then run instal git clone httpsgithubcomthinkingmachinesgeomancergit cd geomanc python setuppi instal set up your data warehous geomanc is power by a geospati data warehous we highlyrecommend use bigqueri as your data warehous and geofabrik osm catalog as your sourc of point and line of interest you can see the setup instruct in thi link basic usag all of the featur engin function in geomanc are call spell for exampl you want to get the distanc to the nearest supermarket for each point from geomancerspel import distancetonearest load your dataset in a panda datafram df load_dataset dist_spel distancetonearest supermarket source_tableph_osmgis_osm_pois_free_1 feature_namedist_supermarket dburlbigqueryprojectnam castdf you can specifi the type of filter use the format columnfilt by default the column valu is fclass for exampl if you wish to look for road on a bridg then pass bridget from geomancerspel import distancetonearest load the dataset in a panda datafram df load_dataset dist_spel distancetonearest bridget source_tableph_osmgis_osm_roads_free_1 feature_namedist_road_bridg dburlbigqueryprojectnam castdf compos multipl spell into a spell book which you can export as a json file from geomancerspel import distancetonearest from geomancerspellbook import spellbook spellbook spellbook distancetonearest supermarket source_tableph_osmgis_osm_pois_free_1 feature_namedist_supermarket dburlbigqueryprojectnam distancetonearest embassi source_tableph_osmgis_osm_pois_free_1 feature_namedist_embassi dburlbigqueryprojectnam spellbookto_jsondist_supermarket_and_embassyjson you can share the gener file so other peopl can reus your featur extract with their own dataset from geomancerspellbook import spellbook load the dataset in a panda datafram df load_dataset spellbook spellbookread_jsondist_supermarket_and_embassyjson dist_supermarket_and_embassi spellbookcastdf contribut thi project is open for contributor contibut can come in the form of featur request bug fix document tutori and the like we highli recommend to file an issu first befor submit a pull request simpli fork thi repositori and make a pull request wed definit appreci implement of new featur bug report document test also we have a contribut and a code_of_conduct so pleas check that one out licens mit licens 2019 think machin data scienc,geomancer is a geospatial feature engineering library it leverage geospatial data such a openstreetmap osm alongside a data warehouse like bigquery you can use this to create share and iterate geospatial feature for your downstream task analysis modelling visualization etc feature geomancer can perform geospatial feature engineering for all type of vector data ie point line polygon feature primitive for geospatial feature engineering ability to switch out data warehouse bigquery sqlite postgresql in progress compile and share your feature using our spellbook setup and installation installing the library geomancer can be installed using pip pip install geomancer this will install all dependency for every datawarehouse we support if you wish to do this only for a specific warehouse then you can add an identifier pip install geomancerbq for bigquery pip install geomancersqlite for sqlite pip install geomancerpsql for postgresql alternatively you can also clone the repository then run install git clone httpsgithubcomthinkingmachinesgeomancergit cd geomancer python setuppy install setting up your data warehouse geomancer is powered by a geospatial data warehouse we highlyrecommend using bigquery a your data warehouse and geofabriks osm catalog a your source of point and line of interest you can see the setup instruction in this link basic usage all of the feature engineering function in geomancer are called spell for example you want to get the distance to the nearest supermarket for each point from geomancerspells import distancetonearest load your dataset in a panda dataframe df load_dataset dist_spell distancetonearest supermarket source_tableph_osmgis_osm_pois_free_1 feature_namedist_supermarket dburlbigqueryprojectname castdf you can specify the type of filter using the format columnfilter by default the column value is fclass for example if you wish to look for road on a bridge then pas bridget from geomancerspells import distancetonearest load the dataset in a panda dataframe df load_dataset dist_spell distancetonearest bridget source_tableph_osmgis_osm_roads_free_1 feature_namedist_road_bridges dburlbigqueryprojectname castdf compose multiple spell into a spell book which you can export a a json file from geomancerspells import distancetonearest from geomancerspellbook import spellbook spellbook spellbook distancetonearest supermarket source_tableph_osmgis_osm_pois_free_1 feature_namedist_supermarket dburlbigqueryprojectname distancetonearest embassy source_tableph_osmgis_osm_pois_free_1 feature_namedist_embassy dburlbigqueryprojectname spellbookto_jsondist_supermarket_and_embassyjson you can share the generated file so other people can reuse your feature extraction with their own datasets from geomancerspellbook import spellbook load the dataset in a panda dataframe df load_dataset spellbook spellbookread_jsondist_supermarket_and_embassyjson dist_supermarket_and_embassy spellbookcastdf contributing this project is open for contributor contibutions can come in the form of feature request bug fix documentation tutorial and the like we highly recommend to file an issue first before submitting a pull request simply fork this repository and make a pull request wed definitely appreciate implementation of new feature bug report documentation testing also we have a contributing and a code_of_conduct so please check that one out license mit license 2019 thinking machine data science,geomancer geospatial feature engineering library leverage geospatial data openstreetmap osm alongside data warehouse like bigquery use create share iterate geospatial feature downstream task analysis modelling visualization etc feature geomancer perform geospatial feature engineering type vector data ie point line polygon feature primitive geospatial feature engineering ability switch data warehouse bigquery sqlite postgresql progress compile share feature using spellbook setup installation installing library geomancer installed using pip pip install geomancer install dependency every datawarehouse support wish specific warehouse add identifier pip install geomancerbq bigquery pip install geomancersqlite sqlite pip install geomancerpsql postgresql alternatively also clone repository run install git clone httpsgithubcomthinkingmachinesgeomancergit cd geomancer python setuppy install setting data warehouse geomancer powered geospatial data warehouse highlyrecommend using bigquery data warehouse geofabriks osm catalog source point line interest see setup instruction link basic usage feature engineering function geomancer called spell example want get distance nearest supermarket point geomancerspells import distancetonearest load dataset panda dataframe df load_dataset dist_spell distancetonearest supermarket source_tableph_osmgis_osm_pois_free_1 feature_namedist_supermarket dburlbigqueryprojectname castdf specify type filter using format columnfilter default column value fclass example wish look road bridge pas bridget geomancerspells import distancetonearest load dataset panda dataframe df load_dataset dist_spell distancetonearest bridget source_tableph_osmgis_osm_roads_free_1 feature_namedist_road_bridges dburlbigqueryprojectname castdf compose multiple spell spell book export json file geomancerspells import distancetonearest geomancerspellbook import spellbook spellbook spellbook distancetonearest supermarket source_tableph_osmgis_osm_pois_free_1 feature_namedist_supermarket dburlbigqueryprojectname distancetonearest embassy source_tableph_osmgis_osm_pois_free_1 feature_namedist_embassy dburlbigqueryprojectname spellbookto_jsondist_supermarket_and_embassyjson share generated file people reuse feature extraction datasets geomancerspellbook import spellbook load dataset panda dataframe df load_dataset spellbook spellbookread_jsondist_supermarket_and_embassyjson dist_supermarket_and_embassy spellbookcastdf contributing project open contributor contibutions come form feature request bug fix documentation tutorial like highly recommend file issue first submitting pull request simply fork repository make pull request wed definitely appreciate implementation new feature bug report documentation testing also contributing code_of_conduct please check one license mit license 2019 thinking machine data science
Python ,"Datasets
We provide the datasets used in our empirical studies and evaluations. The description is provided according to the type of datasets and the associated work.
If you use our dataset, please cite our relevant paper in your publication. The bib  is also provided.

There are three groups of datasets according to the dataset's characteristics.


agile sprints
These are the datasets on the iterative development (e.g. sprint). Our work on these datasets focuses on predicting delivery capability in iterative development. We published the work in IEEE TSE


delay issues
These are the datasets on the delayed issues. We used these datasets in our work on predicting delayed issues which we published in MSR2015, ASE2015, and in the journal of Empirical Software Engineering.


story points
These are the datasets on the story point estimation. We provide the datasets and the models from our work on a deep learning model for estimating story points which we published in IEEE TSE.


You can also find preprints in the folders.
Visit our homepage for more informaiton SEA@UOW

bib

IEEE TSE2018: A deep learning model for estimating story points

[1] M. Choetkiertikul, H. K. Dam, T. Tran, T. T. M. Pham, A. Ghose, and T. Menzies, “A deep learning model for estimating story points,” IEEE Trans. Softw. Eng., vol. PP, no. 99, p. 1, 2018.
@article{Choetkiertikul2018,
author = {Choetkiertikul, M and Dam, H K and Tran, T and Pham, T T M and Ghose, A and Menzies, T},
doi = {10.1109/TSE.2018.2792473},
issn = {0098-5589 VO  - PP},
journal = {IEEE Transactions on Software Engineering},
keywords = {Estimation,Machine learning,Planning,Predictive models,Software,Springs,deep learning,effort estimation,software analytics,story point estimation},
number = {99},
pages = {1},
title = {{A deep learning model for estimating story points}},
volume = {PP},
year = {2018}
}


IEEE TSE2017: Predicting Delivery Capability in Iterative Software Development

[1] M. Choetkiertikul, H. K. Dam, T. Tran, A. Ghose, and J. Grundy, “Predicting Delivery Capability in Iterative Software Development,” IEEE Trans. Softw. Eng., vol. 14, no. 8, pp. 1–1, 2017.
@article{Choetkiertikul2017,
title = {{Predicting Delivery Capability in Iterative Software Development}},
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya and Grundy, John},
doi = {10.1109/TSE.2017.2693989},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {8},
pages = {1--1},
volume = {14},
year = {2017}
}


EMSE2017: Predicting the delay of issues with due dates in software projects

[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, “Predicting the delay of issues with due dates in software projects,” Empir. Softw. Eng., vol. 22, no. 3, pp. 1223–1263, 2017.
@article{Choetkiertikul2017,
title = {{Predicting the delay of issues with due dates in software projects}},
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
doi = {10.1007/s10664-016-9496-7},
issn = {15737616},
journal = {Empirical Software Engineering},
number = {3},
pages = {1223--1263},
publisher = {Empirical Software Engineering},
volume = {22},
year = {2017}
}


MSR2015: Characterization and prediction of issue-related risks in software projects

[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, “Characterization and prediction of issue-related risks in software projects,” in Proceedings of the 12th Working Conference on Mining Software Repositories (MSR), 2015, pp. 280–291.
@inproceedings{Morakot2015,
title = {{Characterization and Prediction of Issue-Related Risks in Software Projects}},
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
booktitle = {Proceedings of the 12th IEEE/ACM Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2015.33},
isbn = {978-0-7695-5594-2},
issn = {21601860},
pages = {280--291},
publisher = {IEEE},
url = {http://ieeexplore.ieee.org/document/7180087/},
year = {2015}
}


ASE2015: Predicting delays in software projects using networked classification

[1] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, “Predicting delays in software projects using networked classification,” in Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2015, pp. 353–364.
@inproceedings{Choetkiertikul2015,
title = {{Predicting delays in software projects using networked classification}},
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
doi = {10.1109/ASE.2015.55},
isbn = {9781509000241},
pages = {353--364},
year = {2015}
}


",dataset we provid the dataset use in our empir studi and evalu the descript is provid accord to the type of dataset and the associ work if you use our dataset pleas cite our relev paper in your public the bib is also provid there are three group of dataset accord to the dataset characterist agil sprint these are the dataset on the iter develop eg sprint our work on these dataset focus on predict deliveri capabl in iter develop we publish the work in ieee tse delay issu these are the dataset on the delay issu we use these dataset in our work on predict delay issu which we publish in msr2015 ase2015 and in the journal of empir softwar engin stori point these are the dataset on the stori point estim we provid the dataset and the model from our work on a deep learn model for estim stori point which we publish in ieee tse you can also find preprint in the folder visit our homepag for more informaiton seauow bib ieee tse2018 a deep learn model for estim stori point 1 m choetkiertikul h k dam t tran t t m pham a ghose and t menzi a deep learn model for estim stori point ieee tran softw eng vol pp no 99 p 1 2018 articlechoetkiertikul2018 author choetkiertikul m and dam h k and tran t and pham t t m and ghose a and menzi t doi 101109tse20182792473 issn 00985589 vo pp journal ieee transact on softwar engin keyword estimationmachin learningplanningpredict modelssoftwarespringsdeep learningeffort estimationsoftwar analyticsstori point estim number 99 page 1 titl a deep learn model for estim stori point volum pp year 2018 ieee tse2017 predict deliveri capabl in iter softwar develop 1 m choetkiertikul h k dam t tran a ghose and j grundi predict deliveri capabl in iter softwar develop ieee tran softw eng vol 14 no 8 pp 11 2017 articlechoetkiertikul2017 titl predict deliveri capabl in iter softwar develop author choetkiertikul morakot and dam hoa khanh and tran truyen and ghose aditya and grundi john doi 101109tse20172693989 issn 00985589 journal ieee transact on softwar engin number 8 page 11 volum 14 year 2017 emse2017 predict the delay of issu with due date in softwar project 1 m choetkiertikul h k dam t tran and a ghose predict the delay of issu with due date in softwar project empir softw eng vol 22 no 3 pp 12231263 2017 articlechoetkiertikul2017 titl predict the delay of issu with due date in softwar project author choetkiertikul morakot and dam hoa khanh and tran truyen and ghose aditya doi 101007s1066401694967 issn 15737616 journal empir softwar engin number 3 page 12231263 publish empir softwar engin volum 22 year 2017 msr2015 character and predict of issuerel risk in softwar project 1 m choetkiertikul h k dam t tran and a ghose character and predict of issuerel risk in softwar project in proceed of the 12th work confer on mine softwar repositori msr 2015 pp 280291 inproceedingsmorakot2015 titl character and predict of issuerel risk in softwar project author choetkiertikul morakot and dam hoa khanh and tran truyen and ghose aditya booktitl proceed of the 12th ieeeacm work confer on mine softwar repositori msr doi 101109msr201533 isbn 9780769555942 issn 21601860 page 280291 publish ieee url httpieeexploreieeeorgdocument7180087 year 2015 ase2015 predict delay in softwar project use network classif 1 m choetkiertikul h k dam t tran and a ghose predict delay in softwar project use network classif in proceed of the 30th ieeeacm intern confer on autom softwar engin ase 2015 pp 353364 inproceedingschoetkiertikul2015 titl predict delay in softwar project use network classif author choetkiertikul morakot and dam hoa khanh and tran truyen and ghose aditya booktitl proceed of the 30th ieeeacm intern confer on autom softwar engin ase doi 101109ase201555 isbn 9781509000241 page 353364 year 2015,datasets we provide the datasets used in our empirical study and evaluation the description is provided according to the type of datasets and the associated work if you use our dataset please cite our relevant paper in your publication the bib is also provided there are three group of datasets according to the datasets characteristic agile sprint these are the datasets on the iterative development eg sprint our work on these datasets focus on predicting delivery capability in iterative development we published the work in ieee tse delay issue these are the datasets on the delayed issue we used these datasets in our work on predicting delayed issue which we published in msr2015 ase2015 and in the journal of empirical software engineering story point these are the datasets on the story point estimation we provide the datasets and the model from our work on a deep learning model for estimating story point which we published in ieee tse you can also find preprints in the folder visit our homepage for more informaiton seauow bib ieee tse2018 a deep learning model for estimating story point 1 m choetkiertikul h k dam t tran t t m pham a ghose and t menzies a deep learning model for estimating story point ieee trans softw eng vol pp no 99 p 1 2018 articlechoetkiertikul2018 author choetkiertikul m and dam h k and tran t and pham t t m and ghose a and menzies t doi 101109tse20182792473 issn 00985589 vo pp journal ieee transaction on software engineering keywords estimationmachine learningplanningpredictive modelssoftwarespringsdeep learningeffort estimationsoftware analyticsstory point estimation number 99 page 1 title a deep learning model for estimating story point volume pp year 2018 ieee tse2017 predicting delivery capability in iterative software development 1 m choetkiertikul h k dam t tran a ghose and j grundy predicting delivery capability in iterative software development ieee trans softw eng vol 14 no 8 pp 11 2017 articlechoetkiertikul2017 title predicting delivery capability in iterative software development author choetkiertikul morakot and dam hoa khanh and tran truyen and ghose aditya and grundy john doi 101109tse20172693989 issn 00985589 journal ieee transaction on software engineering number 8 page 11 volume 14 year 2017 emse2017 predicting the delay of issue with due date in software project 1 m choetkiertikul h k dam t tran and a ghose predicting the delay of issue with due date in software project empir softw eng vol 22 no 3 pp 12231263 2017 articlechoetkiertikul2017 title predicting the delay of issue with due date in software project author choetkiertikul morakot and dam hoa khanh and tran truyen and ghose aditya doi 101007s1066401694967 issn 15737616 journal empirical software engineering number 3 page 12231263 publisher empirical software engineering volume 22 year 2017 msr2015 characterization and prediction of issuerelated risk in software project 1 m choetkiertikul h k dam t tran and a ghose characterization and prediction of issuerelated risk in software project in proceeding of the 12th working conference on mining software repository msr 2015 pp 280291 inproceedingsmorakot2015 title characterization and prediction of issuerelated risk in software project author choetkiertikul morakot and dam hoa khanh and tran truyen and ghose aditya booktitle proceeding of the 12th ieeeacm working conference on mining software repository msr doi 101109msr201533 isbn 9780769555942 issn 21601860 page 280291 publisher ieee url httpieeexploreieeeorgdocument7180087 year 2015 ase2015 predicting delay in software project using networked classification 1 m choetkiertikul h k dam t tran and a ghose predicting delay in software project using networked classification in proceeding of the 30th ieeeacm international conference on automated software engineering ase 2015 pp 353364 inproceedingschoetkiertikul2015 title predicting delay in software project using networked classification author choetkiertikul morakot and dam hoa khanh and tran truyen and ghose aditya booktitle proceeding of the 30th ieeeacm international conference on automated software engineering ase doi 101109ase201555 isbn 9781509000241 page 353364 year 2015,datasets provide datasets used empirical study evaluation description provided according type datasets associated work use dataset please cite relevant paper publication bib also provided three group datasets according datasets characteristic agile sprint datasets iterative development eg sprint work datasets focus predicting delivery capability iterative development published work ieee tse delay issue datasets delayed issue used datasets work predicting delayed issue published msr2015 ase2015 journal empirical software engineering story point datasets story point estimation provide datasets model work deep learning model estimating story point published ieee tse also find preprints folder visit homepage informaiton seauow bib ieee tse2018 deep learning model estimating story point 1 choetkiertikul h k dam tran pham ghose menzies deep learning model estimating story point ieee trans softw eng vol pp 99 p 1 2018 articlechoetkiertikul2018 author choetkiertikul dam h k tran pham ghose menzies doi 101109tse20182792473 issn 00985589 vo pp journal ieee transaction software engineering keywords estimationmachine learningplanningpredictive modelssoftwarespringsdeep learningeffort estimationsoftware analyticsstory point estimation number 99 page 1 title deep learning model estimating story point volume pp year 2018 ieee tse2017 predicting delivery capability iterative software development 1 choetkiertikul h k dam tran ghose j grundy predicting delivery capability iterative software development ieee trans softw eng vol 14 8 pp 11 2017 articlechoetkiertikul2017 title predicting delivery capability iterative software development author choetkiertikul morakot dam hoa khanh tran truyen ghose aditya grundy john doi 101109tse20172693989 issn 00985589 journal ieee transaction software engineering number 8 page 11 volume 14 year 2017 emse2017 predicting delay issue due date software project 1 choetkiertikul h k dam tran ghose predicting delay issue due date software project empir softw eng vol 22 3 pp 12231263 2017 articlechoetkiertikul2017 title predicting delay issue due date software project author choetkiertikul morakot dam hoa khanh tran truyen ghose aditya doi 101007s1066401694967 issn 15737616 journal empirical software engineering number 3 page 12231263 publisher empirical software engineering volume 22 year 2017 msr2015 characterization prediction issuerelated risk software project 1 choetkiertikul h k dam tran ghose characterization prediction issuerelated risk software project proceeding 12th working conference mining software repository msr 2015 pp 280291 inproceedingsmorakot2015 title characterization prediction issuerelated risk software project author choetkiertikul morakot dam hoa khanh tran truyen ghose aditya booktitle proceeding 12th ieeeacm working conference mining software repository msr doi 101109msr201533 isbn 9780769555942 issn 21601860 page 280291 publisher ieee url httpieeexploreieeeorgdocument7180087 year 2015 ase2015 predicting delay software project using networked classification 1 choetkiertikul h k dam tran ghose predicting delay software project using networked classification proceeding 30th ieeeacm international conference automated software engineering ase 2015 pp 353364 inproceedingschoetkiertikul2015 title predicting delay software project using networked classification author choetkiertikul morakot dam hoa khanh tran truyen ghose aditya booktitle proceeding 30th ieeeacm international conference automated software engineering ase doi 101109ase201555 isbn 9781509000241 page 353364 year 2015
Python ,"socrata-py
Python SDK for the Socrata Data Management API. Use this library to call into publishing and ETL functionality offered when writing to Socrata datasets.
with open('cool_dataset.csv', 'rb') as file:
    (revision, output) = Socrata(auth).create(
        name = ""cool dataset"",
        description = ""a description""
    ).csv(file)
    
    revision.apply(output_schema = output)

Installation
Example
Using

Boilerplate
Simple usage

Create a new Dataset from a csv, tsv, xls or xlsx file
Create a new Dataset from Pandas
Updating a dataset
Generating a config and using it to update




Advanced usage

Create a revision
Create an upload
Upload a file
Transforming your data
Wait for the transformation to finish
Errors in a transformation
Validating rows
Do the upsert!
Metadata only revisions




Development

Testing
Generating docs
Releasing


Library Docs

Socrata

create
new
using_config


Authorization

live_dangerously


Revisions

create_delete_revision
create_replace_revision
create_update_revision
create_using_config
list
lookup


Revision

apply
create_upload
discard
list_operations
open_in_browser
plan
set_output_schema
source_as_blob
source_from_agent
source_from_dataset
source_from_url
ui_url
update


Sources

create_upload
lookup


Source

add_to_revision
blob
change_parse_option
csv
df
geojson
kml
list_operations
load
open_in_browser
shapefile
tsv
ui_url
wait_for_finish
xls
xlsx


Configs

create
list
lookup


Config

change_parse_option
create_revision
delete
list_operations
update


InputSchema

get_latest_output_schema
latest_output
list_operations
transform


OutputSchema

add_column
build_config
change_column_metadata
change_column_transform
drop_column
list_operations
rows
run
schema_errors
schema_errors_csv
set_row_id
validate_row_id
wait_for_finish


Job

is_complete
list_operations
wait_for_finish





Installation
This only supports python3.
Installation is available through pip. Using a virtualenv is advised. Install
the package by running
pip3 install socrata-py

The only hard dependency is requests which will be installed via pip. Pandas is not required, but creating a dataset from a Pandas dataframe is supported. See below.
Example
Try the command line example with
python -m examples.create 'Police Reports' ~/Desktop/catalog.data.gov/Seattle_Real_Time_Fire_911_Calls.csv 'pete-test.test-socrata.com' --username $SOCRATA_USERNAME --password $SOCRATA_PASSWORD
Using
Boilerplate
# Import some stuff
from socrata.authorization import Authorization
from socrata import Socrata
import os

# Boilerplate...
# Make an auth object
auth = Authorization(
  ""pete-test.test-socrata.com"",
  os.environ['SOCRATA_USERNAME'],
  os.environ['SOCRATA_PASSWORD']
)
Simple usage
Create a new Dataset from a csv, tsv, xls or xlsx file
To create a dataset, you can do this:
with open('cool_dataset.csv', 'rb') as file:
    # Upload + Transform step

    # revision is the *change* to the view in the catalog, which has not yet been applied.
    # output is the OutputSchema, which is a change to data which can be applied via the revision
    (revision, output) = Socrata(auth).create(
        name = ""cool dataset"",
        description = ""a description""
    ).csv(file)

    # Transformation step
    # We want to add some metadata to our column, drop another column, and add a new column which will
    # be filled with values from another column and then transformed
    output = output\
        .change_column_metadata('a_column', 'display_name').to('A Column!')\
        .change_column_metadata('a_column', 'description').to('Here is a description of my A Column')\
        .drop_column('b_column')\
        .add_column('a_column_squared', 'A Column, but times itself', 'to_number(`a_column`) * to_number(`a_column`)', 'this is a column squared')\
        .run()


    # Validation of the results step
    output = output.wait_for_finish()
    # The data has been validated now, and we can access errors that happened during validation. For example, if one of the cells in `a_column` couldn't be converted to a number in the call to `to_number`, that error would be reflected in this error_count
    assert output.attributes['error_count'] == 0

    # If you want, you can get a csv stream of all the errors
    errors = output.schema_errors_csv()
    for line in errors.iter_lines():
        print(line)

    # Update step

    # Apply the revision - this will make it public and available to make
    # visualizations from
    job = revision.apply(output_schema = output)

    # This opens a browser window to your revision, and you will see the progress
    # of the job
    revision.open_in_browser()

    # Application is async - this will block until all the data
    # is in place and readable
    job.wait_for_finish()
Similar to the csv method are the xls, xlsx, and tsv methods, which upload
those files.
There is a blob method as well, which uploads blobby data to the source. This means the data will not be parsed, and will be displayed under ""Files and Documents"" in the catalog once the revision is applied.
Create a new Dataset from Pandas
Datasets can also be created from Pandas DataFrames
import pandas as pd
df = pd.read_csv('socrata-py/test/fixtures/simple.csv')
# Do various Pandas-y changes and modifications, then...
(revision, output) = Socrata(auth).create(
    name = ""Pandas Dataset"",
    description = ""Dataset made from a Pandas Dataframe""
).df(df)

# Same code as above to apply the revision.
Updating a dataset
A Socrata update is actually an upsert. Rows are updated or created based on the row identifier. If the row-identifer doesn't exist, all updates are just appends to the dataset.
A replace truncates the whole dataset and then inserts the new data.
Generating a config and using it to update
# This is how we create our view initially
with open('cool_dataset.csv', 'rb') as file:
    (revision, output) = Socrata(auth).create(
        name = ""cool dataset"",
        description = ""a description""
    ).csv(file)

    revision.apply(output_schema = output)

# This will build a configuration using the same settings (file parsing and
# data transformation rules) that we used to get our output. The action
# that we will take will be ""update"", though it could also be ""replace""
config = output.build_config(""cool-dataset-config"", ""update"")

# Now we need to save our configuration name and view id somewhere so we
# can update the view using our config
configuration_name = ""cool-dataset-config""
view_id = revision.view_id()

# Now later, if we want to use that config to update our view, we just need the view and the configuration_name
socrata = Socrata(auth)
view = socrata.views.lookup(view_id) # View will be the view we are updating with the new data

with open('updated-cool-dataset.csv', 'rb') as my_file:
    (revision, job) = socrata.using_config(
        configuration_name,
        view
    ).csv(my_file)
    print(job) # Our update job is now running
Advanced usage
Create a revision
# This is our socrata object, using the auth variable from above
socrata = Socrata(auth)

# This will make our initial revision, on a view that doesn't yet exist
revision = socrata.new({'name': 'cool dataset'})

# revision is a Revision object, we can print it
print(revision)
Revision({'created_by': {'display_name': 'rozap',
                'email': 'chris.duranti@socrata.com',
                'user_id': 'tugg-ikce'},
 'fourfour': 'ij46-xpxe',
 'id': 346,
 'inserted_at': '2017-02-27T23:05:08.522796',
 'metadata': None,
 'update_seq': 285,
 'upsert_jobs': []})

# We can also access the attributes of the revision
print(revision.attributes['metadata']['name'])
'cool dataset'
Create an upload
# Using that revision, we can create an upload
upload = revision.create_upload('foo.csv')

# And print it
print(upload)
Source({'content_type': None,
 'created_by': {'display_name': 'rozap',
                'email': 'chris.duranti@socrata.com',
                'user_id': 'tugg-ikce'},
 'source_type': {
    'filename': 'foo.csv',
    'type': 'upload'
 },
 'finished_at': None,
 'id': 290,
 'inserted_at': '2017-02-27T23:07:18.309676',
 'schemas': []})
Upload a file
# And using that upload we just created, we can put bytes into it
with open('test/fixtures/simple.csv', 'rb') as f:
    source = upload.csv(f)
Transforming your data
Transforming data consists of going from input data (data exactly as it appeared in the source)
to output data (data as you want it to appear).
Transformation from input data to output data often has problems. You might, for example, have a column
full of numbers, but one row in that column is actually the value hehe! which cannot be transformed into
a number. Rather than failing at each datum which is dirty or wrong, transforming your data allows you to
reconcile these issues.
We might have a dataset called temps.csv that looks like
date, celsius
8-24-2017, 22
8-25-2017, 20
8-26-2017, 23
8-27-2017, hehe!
8-28-2017,
8-29-2017, 21

Suppose we uploaded it in our previous step, like this:
with open('temps.csv', 'rb') as f:
    source = upload.csv(f)
    input_schema = source.get_latest_input_schema()
Our input_schema is the input data exactly as it appeared in the CSV, with all values of type string.
Our output_schema is the output data as it was guessed by Socrata. Guessing may not always be correct, which is why we have import configs to ""lock in"" a schema for automation. We can get the output_schema
like so:
output_schema = input_schema.get_latest_output_schema()
We can now make changes to the schema, like so
new_output_schema = output
    # Change the field_name of date to the_date
    .change_column_metadata('date', 'field_name').to('the_date')\
    # Change the description of the celsius column
    .change_column_metadata('celsius', 'description').to('the temperature in celsius')\
    # Change the display name of the celsius column
    .change_column_metadata('celsius', 'display_name').to('Degrees (Celsius)')\
    # Change the transform of the_date column to to_fixed_timestamp(`date`)
    .change_column_transform('the_date').to('to_fixed_timestamp(`date`)')\
    # Make the celsius column all numbers
    .change_column_transform('celsius').to('to_number(`celsius`)')\
    # Add a new column, which is computed from the `celsius` column
    .add_column('fahrenheit', 'Degrees (Fahrenheit)', '(to_number(`celsius`) * (9 / 5)) + 32', 'the temperature in celsius')\
    .run()
change_column_metadata(column_name, column_attribute) takes the field name used to
identify the column and the column attribute to change (field_name, display_name, description, position)
add_column(field_name, display_name, transform_expression, description) will create a new column
We can also call drop_column(celsius) which will drop the column.
.run() will then make a request and return the new output_schema, or an error if something is invalid.
Transforms can be complex SoQL expressions. Available functions are listed here. You can do lots of stuff with them;
For example, you could change all null values into errors (which won't be imported) by doing
something like
new_output_schema = output
    .change_column_transform('celsius').to('coalesce(to_number(`celsius`), error(""Celsius was null!""))')
    .run()
Or you could add a new column that says if the day was hot or not
new_output_schema = output
    .add_column('is_hot', 'Was the day hot?', 'to_number(`celsius`) >= 23')
    .run()
Or you could geocode a column, given the following CSV
address,city,zip,state
10028 Ravenna Ave NE, Seattle, 98128, WA
1600 Pennsylvania Avenue, Washington DC, 20500, DC
6511 32nd Ave NW, Seattle, 98155, WA

We could transform our first output_schema into a single column dataset, where that
single column is a Point of the address
output = output\
    .add_column('location', 'Incident Location', 'geocode(`address`, `city`, `state`, `zip`)')\
    .drop_column('address')\
    .drop_column('city')\
    .drop_column('state')\
    .drop_column('zip')\
    .run()
Composing these SoQL functions into expressions will allow you to validate, shape, clean and extend your data to make it more useful to the consumer.
Wait for the transformation to finish
Transformations are async, so if you want to wait for it to finish, you can do so

Errors in a transformation
Transformations may have had errors, like in the previous example, we can't convert hehe! to a number. We can see the count of them like this:
print(output_schema.attributes['error_count'])
We can view the detailed errors like this:
errors = output_schema.schema_errors()
We can get a CSV of the errors like this:
csv_stream = output_schema.schema_errors_csv()
Validating rows
We can look at the rows of our schema as well
rows = output_schema.rows(offset = 0, limit = 20)

self.assertEqual(rows, [
    {'b': {'ok': ' bfoo'}},
    {'b': {'ok': ' bfoo'}},
    {'b': {'ok': ' bfoo'}},
    {'b': {'ok': ' bfoo'}}
])
Do the upsert!
# Now we have transformed our data into the shape we want, let's do an upsert
job = revision.apply(output_schema = output_schema)

# This will complete the upsert behind the scenes. If we want to
# re-fetch the current state of the upsert job, we can do so
job = job.show()

# To get the progress
print(job.attributes['log'])
[
    {'details': {'Errors': 0, 'Rows Created': 0, 'Rows Updated': 0, 'By RowIdentifier': 0, 'By SID': 0, 'Rows Deleted': 0}, 'time': '2017-02-28T20:20:59', 'stage': 'upsert_complete'},
    {'details': {'created': 1}, 'time': '2017-02-28T20:20:59', 'stage': 'columns_created'},
    {'details': {'created': 1}, 'time': '2017-02-28T20:20:59', 'stage': 'columns_created'},
    {'details': None, 'time': '2017-02-28T20:20:59', 'stage': 'started'}
]


# So maybe we just want to wait here, printing the progress, until the job is done
job.wait_for_finish(progress = lambda job: print(job.attributes['log']))

# So now if we go look at our original four-four, our data will be there
Metadata only revisions
When there is an existing Socrata view that you'd like to update the metadata of, you can do so by creating a Source which is the Socrata view.
view = socrata.views.lookup('abba-cafe')

revision = view.revisions.create_replace_revision()
source = revision.source_from_dataset()
output_schema = source.get_latest_input_schema().get_latest_output_schema()
new_output_schema = output_schema\
    .change_column_metadata('a', 'description').to('meh')\
    .change_column_metadata('b', 'display_name').to('bbbb')\
    .change_column_metadata('c', 'field_name').to('ccc')\
    .run()


revision.apply(output_schema = new_output_schema)
Development
Testing
Install test deps by running pip install -r requirements.txt. This will install pdoc and pandas which are required to run the tests.
Configuration is set in test/auth.py for tests. It reads the domain, username, and password from environment variables. If you want to run the tests, set those environment variables to something that will work.
If I wanted to run the tests against my local instance, I would run:
SOCRATA_DOMAIN=localhost SOCRATA_USERNAME=$SOCRATA_LOCAL_USER SOCRATA_PASSWORD=$SOCRATA_LOCAL_PASS bin/test
Generating docs
make the docs by running
make docs
Releasing
release to pypi by bumping the version to something reasonable and running
python setup.py sdist upload -r pypi

Note you'll need your .pypirc file in your home directory. For help, read this
Library Docs
Socrata
ArgSpec
    Args: auth

Top level publishing object.
All functions making HTTP calls return a result tuple, where the first element in the
tuple is whether or not the call succeeded, and the second element is the returned
object if it was a success, or a dictionary containing the error response if the call
failed. 2xx responses are considered successes. 4xx and 5xx responses are considered failures.
In the event of a socket hangup, an exception is raised.
create
Shortcut to create a dataset. Returns a Create object,
which contains functions which will create a view, upload
your file, and validate data quality in one step.
To actually place the validated data into a view, you can call .apply()
on the revision
(revision, output_schema) Socrata(auth).create(
    name = ""cool dataset"",
    description = ""a description""
).csv(file)

job = revision.apply(output_schema = output_schema)

Args:
   **kwargs: Arbitrary revision metadata values

Returns:
    result (Revision, OutputSchema): Returns the revision that was created and the
        OutputSchema created from your uploaded file

Examples:
Socrata(auth).create(
    name = ""cool dataset"",
    description = ""a description""
).csv(open('my-file.csv'))
new
ArgSpec
    Args: metadata

Create an empty revision, on a view that doesn't exist yet. The
view will be created for you, and the initial revision will be returned.
Args:
    metadata (dict): Metadata to apply to the revision

Returns:
    Revision

Examples:
    rev = Socrata(auth).new({
        'name': 'hi',
        'description': 'foo!',
        'metadata': {
            'view': 'metadata',
            'anything': 'is allowed here'

        }
    })
using_config
ArgSpec
    Args: config_name, view

Update a dataset, using the configuration that you previously
created, and saved the name of. Takes the config_name parameter
which uniquely identifies the config, and the View object, which can
be obtained from socrata.views.lookup('view-id42')
Args:
    config_name (str): The config name
    view (View): The view to update

Returns:
    result (ConfiguredJob): Returns the ConfiguredJob

Note:
Typical usage would be in a context manager block (as demonstrated in the example
below). In this case, the ConfiguredJob is created and immediately launched by way of
the call to the ConfiguredJob.csv method.
Examples:
    with open('my-file.csv', 'rb') as my_file:
        (rev, job) = p.using_config(name, view).csv(my_file)

Authorization
ArgSpec
    Args: domain, username, password, request_id_prefix
    Defaults: domain=

Manages basic authorization for accessing the socrata API.
This is passed into the Socrata object once, which is the entry
point for all operations.
auth = Authorization(
    ""data.seattle.gov"",
    os.environ['SOCRATA_USERNAME'],
    os.environ['SOCRATA_PASSWORD']
)
publishing = Socrata(auth)

live_dangerously
Disable SSL checking. Note that this should only be used while developing
against a local Socrata instance.
Revisions
ArgSpec
    Args: fourfour, auth

create_delete_revision
ArgSpec
    Args: metadata, permission
    Defaults: metadata={}, permission=public

Create a revision on the view, which when applied, will delete rows of data.
This is an upsert; a row id must be set.
Args:
    metadata (dict): The metadata to change; these changes will be applied when the revision is applied
    permission (string): 'public' or 'private'

Returns:
    Revision The new revision, or an error

Examples:
    view.revisions.create_delete_revision(metadata = {
        'name': 'new dataset name',
        'description': 'description'
    })
create_replace_revision
ArgSpec
    Args: metadata, permission
    Defaults: metadata={}, permission=public

Create a revision on the view, which when applied, will replace the data.
Args:
    metadata (dict): The metadata to change; these changes will be applied when the revision
        is applied
    permission (string): 'public' or 'private'

Returns:
    Revision The new revision, or an error

Examples:
    >>> view.revisions.create_replace_revision(metadata = {'name': 'new dataset name', 'description': 'updated description'})

create_update_revision
ArgSpec
    Args: metadata, permission
    Defaults: metadata={}, permission=public

Create a revision on the view, which when applied, will update the data
rather than replacing it.
This is an upsert; if there is a rowId defined and you have duplicate ID values,
those rows will be updated. Otherwise they will be appended.
Args:
    metadata (dict): The metadata to change; these changes will be applied when the revision is applied
    permission (string): 'public' or 'private'

Returns:
    Revision The new revision, or an error

Examples:
    view.revisions.create_update_revision(metadata = {
        'name': 'new dataset name',
        'description': 'updated description'
    })
create_using_config
ArgSpec
    Args: config

Create a revision for the given dataset.
list
List all the revisions on the view
Returns:
    list[Revision]

lookup
ArgSpec
    Args: revision_seq

Lookup a revision within the view based on the sequence number
Args:
    revision_seq (int): The sequence number of the revision to lookup

Returns:
    Revision The Revision resulting from this API call, or an error

Revision
ArgSpec
    Args: auth, response, parent

A revision is a change to a dataset
apply
ArgSpec
    Args: output_schema

Apply the Revision to the view that it was opened on
Args:
    output_schema (OutputSchema): Optional output schema. If your revision includes
        data changes, this should be included. If it is a metadata only revision,
        then you will not have an output schema, and you do not need to pass anything
        here

Returns:
    Job

Examples:
job = revision.apply(output_schema = my_output_schema)

create_upload
ArgSpec
    Args: filename, parse_options
    Defaults: filename={}

Create an upload within this revision
Args:
    filename (str): The name of the file to upload

Returns:
    Source: Returns the new Source The Source created by this API call, or an error

discard
Discard this open revision.
Returns:
    Revision The closed Revision or an error

list_operations
Get a list of the operations that you can perform on this
object. These map directly onto what's returned from the API
in the links section of each resource
open_in_browser
Open this revision in your browser, this will open a window
plan
Return the list of operations this revision will make when it is applied
Returns:
    dict

set_output_schema
ArgSpec
    Args: output_schema_id

Set the output schema id on the revision. This is what will get applied when
the revision is applied if no ouput schema is explicitly supplied
Args:
    output_schema_id (int): The output schema id

Returns:
    Revision The updated Revision as a result of this API call, or an error

Examples:
    revision = revision.set_output_schema(42)
source_as_blob
ArgSpec
    Args: filename, parse_options
    Defaults: filename={}

Create a source from a file that should remain unparsed
source_from_agent
ArgSpec
    Args: agent_uid, namespace, path, parse_options, parameters
    Defaults: agent_uid={}, namespace={}

Create a source from a connection agent in this revision
source_from_dataset
ArgSpec
    Args: parse_options
    Defaults: parse_options={}

Create a dataset source within this revision
source_from_url
ArgSpec
    Args: url, parse_options
    Defaults: url={}

Create a URL source
Args:
    url (str): The URL to create the dataset from

Returns:
    Source: Returns the new Source The Source created by this API call, or an error

ui_url
This is the URL to the landing page in the UI for this revision
Returns:
    url (str): URL you can paste into a browser to view the revision UI

update
ArgSpec
    Args: body

Set the metadata to be applied to the view
when this revision is applied
Args:
    body (dict): The changes to make to this revision

Returns:
    Revision The updated Revision as a result of this API call, or an error

Examples:
    revision = revision.update({
        'metadata': {
            'name': 'new name',
            'description': 'new description'
        }
    })
Sources
ArgSpec
    Args: auth

create_upload
ArgSpec
    Args: filename

Create a new source. Takes a body param, which must contain a filename
of the file.
Args:
    filename (str): The name of the file you are uploading

Returns:
    Source: Returns the new Source

Examples:
    upload = revision.create_upload('foo.csv')
lookup
ArgSpec
    Args: source_id

Lookup a source
Args:
    source_id (int): The id

Returns:
    Source: Returns the new Source The Source resulting from this API call, or an error

Source
ArgSpec
    Args: auth, response, parent

add_to_revision
ArgSpec
    Args: revision

Associate this Source with the given revision.
blob
ArgSpec
    Args: file_handle

Uploads a Blob dataset. A blob is a file that will not be parsed as a data file,
ie: an image, video, etc.
Returns:
    Source: Returns the new Source

Examples:
    with open('my-blob.jpg', 'rb') as f:
        upload = upload.blob(f)
change_parse_option
ArgSpec
    Args: name

Change a parse option on the source.
If there are not yet bytes uploaded, these parse options will be used
in order to parse the file.
If there are already bytes uploaded, this will trigger a re-parsing of
the file, and consequently a new InputSchema will be created. You can call
source.latest_input() to get the newest one.
Parse options are:
header_count (int): the number of rows considered a header
column_header (int): the one based index of row to use to generate the header
encoding (string): defaults to guessing the encoding, but it can be explicitly set
column_separator (string): For CSVs, this defaults to "","", and for TSVs ""       "", but you can use a custom separator
quote_char (string): Character used to quote values that should be escaped. Defaults to """"""
Args:
    name (string): One of the options above, ie: ""column_separator"" or ""header_count""

Returns:
    change (ParseOptionChange): implements a `.to(value)` function which you call to set the value

For our example, assume we have this dataset
This is my cool dataset
A, B, C
1, 2, 3
4, 5, 6

We want to say that the first 2 rows are headers, and the second of those 2
rows should be used to make the column header. We would do that like so:
Examples:
    source = source            .change_parse_option('header_count').to(2)            .change_parse_option('column_header').to(2)            .run()
csv
ArgSpec
    Args: file_handle

Upload a CSV, returns the new input schema.
Args:
    file_handle: The file handle, as returned by the python function `open()`

    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.
    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.

Returns:
    Source: Returns the new Source

Examples:
    with open('my-file.csv', 'rb') as f:
        upload = upload.csv(f)
df
ArgSpec
    Args: dataframe

Upload a pandas DataFrame, returns the new source.
Args:
    file_handle: The file handle, as returned by the python function `open()`

    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.
    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.

Returns:
    Source: Returns the new Source

Examples:
    import pandas
    df = pandas.read_csv('test/fixtures/simple.csv')
    upload = upload.df(df)
geojson
ArgSpec
    Args: file_handle

Upload a geojson file, returns the new input schema.
Args:
    file_handle: The file handle, as returned by the python function `open()`

    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.
    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.

Returns:
    Source: Returns the new Source

Examples:
    with open('my-geojson-file.geojson', 'rb') as f:
        upload = upload.geojson(f)
kml
ArgSpec
    Args: file_handle

Upload a KML file, returns the new input schema.
Args:
    file_handle: The file handle, as returned by the python function `open()`

    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.
    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.

Returns:
    Source: Returns the new Source

Examples:
    with open('my-kml-file.kml', 'rb') as f:
        upload = upload.kml(f)
list_operations
Get a list of the operations that you can perform on this
object. These map directly onto what's returned from the API
in the links section of each resource
load
Forces the source to load, if it's a view source.
Returns:
    Source: Returns the new Source

open_in_browser
Open this source in your browser, this will open a window
shapefile
ArgSpec
    Args: file_handle

Upload a Shapefile, returns the new input schema.
Args:
    file_handle: The file handle, as returned by the python function `open()`

    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.
    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.

Returns:
    Source: Returns the new Source

Examples:
    with open('my-shapefile-archive.zip', 'rb') as f:
        upload = upload.shapefile(f)
tsv
ArgSpec
    Args: file_handle

Upload a TSV, returns the new input schema.
Args:
    file_handle: The file handle, as returned by the python function `open()`

    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.
    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.

Returns:
    Source: Returns the new Source

Examples:
    with open('my-file.tsv', 'rb') as f:
        upload = upload.tsv(f)
ui_url
This is the URL to the landing page in the UI for the sources
Returns:
    url (str): URL you can paste into a browser to view the source UI

wait_for_finish
ArgSpec
    Args: progress, timeout, sleeptime
    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1

Wait for this dataset to finish transforming and validating. Accepts a progress function
and a timeout.
xls
ArgSpec
    Args: file_handle

Upload an XLS, returns the new input schema
Args:
    file_handle: The file handle, as returned by the python function `open()`

    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.
    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.

Returns:
    Source: Returns the new Source

Examples:
    with open('my-file.xls', 'rb') as f:
        upload = upload.xls(f)
xlsx
ArgSpec
    Args: file_handle

Upload an XLSX, returns the new input schema.
Args:
    file_handle: The file handle, as returned by the python function `open()`

    max_retries (integer): Optional retry limit per chunk in the upload. Defaults to 5.
    backoff_seconds (integer): Optional amount of time to backoff upon a chunk upload failure. Defaults to 2.

Returns:
    Source: Returns the new Source

Examples:
    with open('my-file.xlsx', 'rb') as f:
        upload = upload.xlsx(f)
Configs
ArgSpec
    Args: auth

create
ArgSpec
    Args: name, data_action, parse_options, columns

Create a new ImportConfig. See http://docs.socratapublishing.apiary.io/
ImportConfig section for what is supported in data_action, parse_options,
and columns.
list
List all the ImportConfigs on this domain
lookup
ArgSpec
    Args: name

Obtain a single ImportConfig by name
Config
ArgSpec
    Args: auth, response, parent

change_parse_option
ArgSpec
    Args: name

Change a parse option on the source.
If there are not yet bytes uploaded, these parse options will be used
in order to parse the file.
If there are already bytes uploaded, this will trigger a re-parsing of
the file, and consequently a new InputSchema will be created. You can call
source.latest_input() to get the newest one.
Parse options are:
header_count (int): the number of rows considered a header
column_header (int): the one based index of row to use to generate the header
encoding (string): defaults to guessing the encoding, but it can be explicitly set
column_separator (string): For CSVs, this defaults to "","", and for TSVs ""       "", but you can use a custom separator
quote_char (string): Character used to quote values that should be escaped. Defaults to """"""
Args:
    name (string): One of the options above, ie: ""column_separator"" or ""header_count""

Returns:
    change (ParseOptionChange): implements a `.to(value)` function which you call to set the value

For our example, assume we have this dataset
This is my cool dataset
A, B, C
1, 2, 3
4, 5, 6

We want to say that the first 2 rows are headers, and the second of those 2
rows should be used to make the column header. We would do that like so:
Examples:
    source = source            .change_parse_option('header_count').to(2)            .change_parse_option('column_header').to(2)            .run()
create_revision
ArgSpec
    Args: fourfour

Create a new Revision in the context of this ImportConfig.
Sources that happen in this Revision will take on the values
in this Config.
delete
Delete this ImportConfig. Note that this cannot be undone.
list_operations
Get a list of the operations that you can perform on this
object. These map directly onto what's returned from the API
in the links section of each resource
update
ArgSpec
    Args: body

Mutate this ImportConfig in place. Subsequent revisions opened against this
ImportConfig will take on its new value.
InputSchema
ArgSpec
    Args: auth, response, parent

This represents a schema exactly as it appeared in the source
get_latest_output_schema
Note that this does not make an API request
Returns:
output_schema (OutputSchema): Returns the latest output schema
latest_output
Get the latest (most recently created) OutputSchema
which descends from this InputSchema
Returns:
OutputSchema
list_operations
Get a list of the operations that you can perform on this
object. These map directly onto what's returned from the API
in the links section of each resource
transform
ArgSpec
    Args: body

Transform this InputSchema into an Output. Returns the
new OutputSchema. Note that this call is async - the data
may still be transforming even though the OutputSchema is
returned. See OutputSchema.wait_for_finish to block until
the
OutputSchema
This is data as transformed from an InputSchema
add_column
ArgSpec
    Args: field_name, display_name, transform_expr, description

Add a column
Args:
    field_name (str): The column's field_name, must be unique
    display_name (str): The columns display name
    transform_expr (str): SoQL expression to evaluate and fill the column with data from
    description (str): Optional column description

Returns:
    output_schema (OutputSchema): Returns self for easy chaining

Examples:
new_output_schema = output
    # Add a new column, which is computed from the `celsius` column
    .add_column('fahrenheit', 'Degrees (Fahrenheit)', '(to_number(`celsius`) * (9 / 5)) + 32', 'the temperature in celsius')
    # Add a new column, which is computed from the `celsius` column
    .add_column('kelvin', 'Degrees (Kelvin)', '(to_number(`celsius`) + 273.15')
    .run()
build_config
ArgSpec
    Args: name, data_action

Create a new ImportConfig from this OutputSchema. See the API
docs for what an ImportConfig is and why they're useful
change_column_metadata
ArgSpec
    Args: field_name, attribute

Change the column metadata. This returns a ColumnChange,
which implements a .to function, which takes the new value to change to
Args:
    field_name (str): The column to change
    attribute (str): The attribute of the column to change

Returns:
    change (TransformChange): The transform change, which implements the `.to` function

Examples:
    new_output_schema = output
        # Change the field_name of date to the_date
        .change_column_metadata('date', 'field_name').to('the_date')
        # Change the description of the celsius column
        .change_column_metadata('celsius', 'description').to('the temperature in celsius')
        # Change the display name of the celsius column
        .change_column_metadata('celsius', 'display_name').to('Degrees (Celsius)')
        .run()
change_column_transform
ArgSpec
    Args: field_name

Change the column transform. This returns a TransformChange,
which implements a .to function, which takes a transform expression.
Args:
    field_name (str): The column to change

Returns:
    change (TransformChange): The transform change, which implements the `.to` function

Examples:
    new_output_schema = output
        .change_column_transform('the_date').to('to_fixed_timestamp(`date`)')
        # Make the celsius column all numbers
        .change_column_transform('celsius').to('to_number(`celsius`)')
        # Add a new column, which is computed from the `celsius` column
        .add_column('fahrenheit', 'Degrees (Fahrenheit)', '(to_number(`celsius`) * (9 / 5)) + 32', 'the temperature in celsius')
        .run()
drop_column
ArgSpec
    Args: field_name

Drop the column
Args:
    field_name (str): The column to drop

Returns:
    output_schema (OutputSchema): Returns self for easy chaining

Examples:
    new_output_schema = output
        .drop_column('foo')
        .run()
list_operations
Get a list of the operations that you can perform on this
object. These map directly onto what's returned from the API
in the links section of each resource
rows
ArgSpec
    Args: offset, limit
    Defaults: offset=0, limit=500

Get the rows for this OutputSchema. Acceps offset and limit params
for paging through the data.
run
Run all adds, drops, and column changes.
Returns:
    OutputSchema

Examples:
    new_output_schema = output
        # Change the field_name of date to the_date
        .change_column_metadata('date', 'field_name').to('the_date')
        # Change the description of the celsius column
        .change_column_metadata('celsius', 'description').to('the temperature in celsius')
        # Change the display name of the celsius column
        .change_column_metadata('celsius', 'display_name').to('Degrees (Celsius)')
        # Change the transform of the_date column to to_fixed_timestamp(`date`)
        .change_column_transform('the_date').to('to_fixed_timestamp(`date`)')
        # Make the celsius column all numbers
        .change_column_transform('celsius').to('to_number(`celsius`)')
        # Add a new column, which is computed from the `celsius` column
        .add_column('fahrenheit', 'Degrees (Fahrenheit)', '(to_number(`celsius`) * (9 / 5)) + 32', 'the temperature in celsius')
        .run()
schema_errors
ArgSpec
    Args: offset, limit
    Defaults: offset=0, limit=500

Get the errors that resulted in transforming into this output schema.
Accepts offset and limit params
schema_errors_csv
Get the errors that results in transforming into this output schema
as a CSV stream.
Note that this returns a Reponse, where Reponse
is a python requests Reponse object
set_row_id
ArgSpec
    Args: field_name

Set the row id. Note you must call validate_row_id before doing this.
Args:
    field_name (str): The column to set as the row id

Returns:
    OutputSchema

Examples:
new_output_schema = output.set_row_id('the_id_column')
validate_row_id
ArgSpec
    Args: field_name

Set the row id. Note you must call validate_row_id before doing this.
Args:
    field_name (str): The column to validate as the row id

Returns:
    boolean

wait_for_finish
ArgSpec
    Args: progress, timeout, sleeptime
    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1

Wait for this dataset to finish transforming and validating. Accepts a progress function
and a timeout.
Job
ArgSpec
    Args: auth, response, parent

is_complete
Has this job finished or failed
list_operations
Get a list of the operations that you can perform on this
object. These map directly onto what's returned from the API
in the links section of each resource
wait_for_finish
ArgSpec
    Args: progress, timeout, sleeptime
    Defaults: progress=<function noop at 0x7f34e14fb7b8>, sleeptime=1

Wait for this dataset to finish transforming and validating. Accepts a progress function
and a timeout.
",socratapi python sdk for the socrata data manag api use thi librari to call into publish and etl function offer when write to socrata dataset with opencool_datasetcsv rb as file revis output socrataauthcr name cool dataset descript a descript csvfile revisionapplyoutput_schema output instal exampl use boilerpl simpl usag creat a new dataset from a csv tsv xl or xlsx file creat a new dataset from panda updat a dataset gener a config and use it to updat advanc usag creat a revis creat an upload upload a file transform your data wait for the transform to finish error in a transform valid row do the upsert metadata onli revis develop test gener doc releas librari doc socrata creat new using_config author live_danger revis create_delete_revis create_replace_revis create_update_revis create_using_config list lookup revis appli create_upload discard list_oper open_in_brows plan set_output_schema source_as_blob source_from_ag source_from_dataset source_from_url ui_url updat sourc create_upload lookup sourc add_to_revis blob change_parse_opt csv df geojson kml list_oper load open_in_brows shapefil tsv ui_url wait_for_finish xl xlsx config creat list lookup config change_parse_opt create_revis delet list_oper updat inputschema get_latest_output_schema latest_output list_oper transform outputschema add_column build_config change_column_metadata change_column_transform drop_column list_oper row run schema_error schema_errors_csv set_row_id validate_row_id wait_for_finish job is_complet list_oper wait_for_finish instal thi onli support python3 instal is avail through pip use a virtualenv is advis instal the packag by run pip3 instal socratapi the onli hard depend is request which will be instal via pip panda is not requir but creat a dataset from a panda datafram is support see below exampl tri the command line exampl with python m examplescr polic report desktopcatalogdatagovseattle_real_time_fire_911_callscsv petetesttestsocratacom usernam socrata_usernam password socrata_password use boilerpl import some stuff from socrataauthor import author from socrata import socrata import os boilerpl make an auth object auth author petetesttestsocratacom osenvironsocrata_usernam osenvironsocrata_password simpl usag creat a new dataset from a csv tsv xl or xlsx file to creat a dataset you can do thi with opencool_datasetcsv rb as file upload transform step revis is the chang to the view in the catalog which ha not yet been appli output is the outputschema which is a chang to data which can be appli via the revis revis output socrataauthcr name cool dataset descript a descript csvfile transform step we want to add some metadata to our column drop anoth column and add a new column which will be fill with valu from anoth column and then transform output output change_column_metadataa_column display_nametoa column change_column_metadataa_column descriptiontoher is a descript of my a column drop_columnb_column add_columna_column_squar a column but time itself to_numbera_column to_numbera_column thi is a column squar run valid of the result step output outputwait_for_finish the data ha been valid now and we can access error that happen dure valid for exampl if one of the cell in a_column couldnt be convert to a number in the call to to_numb that error would be reflect in thi error_count assert outputattributeserror_count 0 if you want you can get a csv stream of all the error error outputschema_errors_csv for line in errorsiter_lin printlin updat step appli the revis thi will make it public and avail to make visual from job revisionapplyoutput_schema output thi open a browser window to your revis and you will see the progress of the job revisionopen_in_brows applic is async thi will block until all the data is in place and readabl jobwait_for_finish similar to the csv method are the xl xlsx and tsv method which upload those file there is a blob method as well which upload blobbi data to the sourc thi mean the data will not be pars and will be display under file and document in the catalog onc the revis is appli creat a new dataset from panda dataset can also be creat from panda datafram import panda as pd df pdread_csvsocratapytestfixturessimplecsv do variou pandasi chang and modif then revis output socrataauthcr name panda dataset descript dataset made from a panda datafram dfdf same code as abov to appli the revis updat a dataset a socrata updat is actual an upsert row are updat or creat base on the row identifi if the rowidentif doesnt exist all updat are just append to the dataset a replac truncat the whole dataset and then insert the new data gener a config and use it to updat thi is how we creat our view initi with opencool_datasetcsv rb as file revis output socrataauthcr name cool dataset descript a descript csvfile revisionapplyoutput_schema output thi will build a configur use the same set file pars and data transform rule that we use to get our output the action that we will take will be updat though it could also be replac config outputbuild_configcooldatasetconfig updat now we need to save our configur name and view id somewher so we can updat the view use our config configuration_nam cooldatasetconfig view_id revisionview_id now later if we want to use that config to updat our view we just need the view and the configuration_nam socrata socrataauth view socrataviewslookupview_id view will be the view we are updat with the new data with openupdatedcooldatasetcsv rb as my_fil revis job socratausing_config configuration_nam view csvmy_fil printjob our updat job is now run advanc usag creat a revis thi is our socrata object use the auth variabl from abov socrata socrataauth thi will make our initi revis on a view that doesnt yet exist revis socratanewnam cool dataset revis is a revis object we can print it printrevis revisioncreated_bi display_nam rozap email chrisdurantisocratacom user_id tuggikc fourfour ij46xpx id 346 inserted_at 20170227t230508522796 metadata none update_seq 285 upsert_job we can also access the attribut of the revis printrevisionattributesmetadatanam cool dataset creat an upload use that revis we can creat an upload upload revisioncreate_uploadfoocsv and print it printupload sourcecontent_typ none created_bi display_nam rozap email chrisdurantisocratacom user_id tuggikc source_typ filenam foocsv type upload finished_at none id 290 inserted_at 20170227t230718309676 schema upload a file and use that upload we just creat we can put byte into it with opentestfixturessimplecsv rb as f sourc uploadcsvf transform your data transform data consist of go from input data data exactli as it appear in the sourc to output data data as you want it to appear transform from input data to output data often ha problem you might for exampl have a column full of number but one row in that column is actual the valu hehe which cannot be transform into a number rather than fail at each datum which is dirti or wrong transform your data allow you to reconcil these issu we might have a dataset call tempscsv that look like date celsiu 8242017 22 8252017 20 8262017 23 8272017 hehe 8282017 8292017 21 suppos we upload it in our previou step like thi with opentempscsv rb as f sourc uploadcsvf input_schema sourceget_latest_input_schema our input_schema is the input data exactli as it appear in the csv with all valu of type string our output_schema is the output data as it wa guess by socrata guess may not alway be correct which is whi we have import config to lock in a schema for autom we can get the output_schema like so output_schema input_schemaget_latest_output_schema we can now make chang to the schema like so new_output_schema output chang the field_nam of date to the_dat change_column_metadatad field_nametothe_d chang the descript of the celsiu column change_column_metadatacelsiu descriptiontoth temperatur in celsiu chang the display name of the celsiu column change_column_metadatacelsiu display_nametodegre celsiu chang the transform of the_dat column to to_fixed_timestampd change_column_transformthe_datetoto_fixed_timestampd make the celsiu column all number change_column_transformcelsiustoto_numbercelsiu add a new column which is comput from the celsiu column add_columnfahrenheit degre fahrenheit to_numbercelsiu 9 5 32 the temperatur in celsiu run change_column_metadatacolumn_nam column_attribut take the field name use to identifi the column and the column attribut to chang field_nam display_nam descript posit add_columnfield_nam display_nam transform_express descript will creat a new column we can also call drop_columncelsiu which will drop the column run will then make a request and return the new output_schema or an error if someth is invalid transform can be complex soql express avail function are list here you can do lot of stuff with them for exampl you could chang all null valu into error which wont be import by do someth like new_output_schema output change_column_transformcelsiustocoalesceto_numbercelsiu errorcelsiu wa null run or you could add a new column that say if the day wa hot or not new_output_schema output add_columnis_hot wa the day hot to_numbercelsiu 23 run or you could geocod a column given the follow csv addresscityzipst 10028 ravenna ave ne seattl 98128 wa 1600 pennsylvania avenu washington dc 20500 dc 6511 32nd ave nw seattl 98155 wa we could transform our first output_schema into a singl column dataset where that singl column is a point of the address output output add_columnloc incid locat geocodeaddress citi state zip drop_columnaddress drop_columnc drop_columnst drop_columnzip run compos these soql function into express will allow you to valid shape clean and extend your data to make it more use to the consum wait for the transform to finish transform are async so if you want to wait for it to finish you can do so error in a transform transform may have had error like in the previou exampl we cant convert hehe to a number we can see the count of them like thi printoutput_schemaattributeserror_count we can view the detail error like thi error output_schemaschema_error we can get a csv of the error like thi csv_stream output_schemaschema_errors_csv valid row we can look at the row of our schema as well row output_schemarowsoffset 0 limit 20 selfassertequalrow b ok bfoo b ok bfoo b ok bfoo b ok bfoo do the upsert now we have transform our data into the shape we want let do an upsert job revisionapplyoutput_schema output_schema thi will complet the upsert behind the scene if we want to refetch the current state of the upsert job we can do so job jobshow to get the progress printjobattributeslog detail error 0 row creat 0 row updat 0 by rowidentifi 0 by sid 0 row delet 0 time 20170228t202059 stage upsert_complet detail creat 1 time 20170228t202059 stage columns_cr detail creat 1 time 20170228t202059 stage columns_cr detail none time 20170228t202059 stage start so mayb we just want to wait here print the progress until the job is done jobwait_for_finishprogress lambda job printjobattributeslog so now if we go look at our origin fourfour our data will be there metadata onli revis when there is an exist socrata view that youd like to updat the metadata of you can do so by creat a sourc which is the socrata view view socrataviewslookupabbacaf revis viewrevisionscreate_replace_revis sourc revisionsource_from_dataset output_schema sourceget_latest_input_schemaget_latest_output_schema new_output_schema output_schema change_column_metadataa descriptiontomeh change_column_metadatab display_nametobbbb change_column_metadatac field_nametoccc run revisionapplyoutput_schema new_output_schema develop test instal test dep by run pip instal r requirementstxt thi will instal pdoc and panda which are requir to run the test configur is set in testauthpi for test it read the domain usernam and password from environ variabl if you want to run the test set those environ variabl to someth that will work if i want to run the test against my local instanc i would run socrata_domainlocalhost socrata_usernamesocrata_local_us socrata_passwordsocrata_local_pass bintest gener doc make the doc by run make doc releas releas to pypi by bump the version to someth reason and run python setuppi sdist upload r pypi note youll need your pypirc file in your home directori for help read thi librari doc socrata argspec arg auth top level publish object all function make http call return a result tupl where the first element in the tupl is whether or not the call succeed and the second element is the return object if it wa a success or a dictionari contain the error respons if the call fail 2xx respons are consid success 4xx and 5xx respons are consid failur in the event of a socket hangup an except is rais creat shortcut to creat a dataset return a creat object which contain function which will creat a view upload your file and valid data qualiti in one step to actual place the valid data into a view you can call appli on the revis revis output_schema socrataauthcr name cool dataset descript a descript csvfile job revisionapplyoutput_schema output_schema arg kwarg arbitrari revis metadata valu return result revis outputschema return the revis that wa creat and the outputschema creat from your upload file exampl socrataauthcr name cool dataset descript a descript csvopenmyfilecsv new argspec arg metadata creat an empti revis on a view that doesnt exist yet the view will be creat for you and the initi revis will be return arg metadata dict metadata to appli to the revis return revis exampl rev socrataauthnew name hi descript foo metadata view metadata anyth is allow here using_config argspec arg config_nam view updat a dataset use the configur that you previous creat and save the name of take the config_nam paramet which uniqu identifi the config and the view object which can be obtain from socrataviewslookupviewid42 arg config_nam str the config name view view the view to updat return result configuredjob return the configuredjob note typic usag would be in a context manag block as demonstr in the exampl below in thi case the configuredjob is creat and immedi launch by way of the call to the configuredjobcsv method exampl with openmyfilecsv rb as my_fil rev job pusing_confignam viewcsvmy_fil author argspec arg domain usernam password request_id_prefix default domain manag basic author for access the socrata api thi is pass into the socrata object onc which is the entri point for all oper auth author dataseattlegov osenvironsocrata_usernam osenvironsocrata_password publish socrataauth live_danger disabl ssl check note that thi should onli be use while develop against a local socrata instanc revis argspec arg fourfour auth create_delete_revis argspec arg metadata permiss default metadata permissionpubl creat a revis on the view which when appli will delet row of data thi is an upsert a row id must be set arg metadata dict the metadata to chang these chang will be appli when the revis is appli permiss string public or privat return revis the new revis or an error exampl viewrevisionscreate_delete_revisionmetadata name new dataset name descript descript create_replace_revis argspec arg metadata permiss default metadata permissionpubl creat a revis on the view which when appli will replac the data arg metadata dict the metadata to chang these chang will be appli when the revis is appli permiss string public or privat return revis the new revis or an error exampl viewrevisionscreate_replace_revisionmetadata name new dataset name descript updat descript create_update_revis argspec arg metadata permiss default metadata permissionpubl creat a revis on the view which when appli will updat the data rather than replac it thi is an upsert if there is a rowid defin and you have duplic id valu those row will be updat otherwis they will be append arg metadata dict the metadata to chang these chang will be appli when the revis is appli permiss string public or privat return revis the new revis or an error exampl viewrevisionscreate_update_revisionmetadata name new dataset name descript updat descript create_using_config argspec arg config creat a revis for the given dataset list list all the revis on the view return listrevis lookup argspec arg revision_seq lookup a revis within the view base on the sequenc number arg revision_seq int the sequenc number of the revis to lookup return revis the revis result from thi api call or an error revis argspec arg auth respons parent a revis is a chang to a dataset appli argspec arg output_schema appli the revis to the view that it wa open on arg output_schema outputschema option output schema if your revis includ data chang thi should be includ if it is a metadata onli revis then you will not have an output schema and you do not need to pass anyth here return job exampl job revisionapplyoutput_schema my_output_schema create_upload argspec arg filenam parse_opt default filenam creat an upload within thi revis arg filenam str the name of the file to upload return sourc return the new sourc the sourc creat by thi api call or an error discard discard thi open revis return revis the close revis or an error list_oper get a list of the oper that you can perform on thi object these map directli onto what return from the api in the link section of each resourc open_in_brows open thi revis in your browser thi will open a window plan return the list of oper thi revis will make when it is appli return dict set_output_schema argspec arg output_schema_id set the output schema id on the revis thi is what will get appli when the revis is appli if no ouput schema is explicitli suppli arg output_schema_id int the output schema id return revis the updat revis as a result of thi api call or an error exampl revis revisionset_output_schema42 source_as_blob argspec arg filenam parse_opt default filenam creat a sourc from a file that should remain unpars source_from_ag argspec arg agent_uid namespac path parse_opt paramet default agent_uid namespac creat a sourc from a connect agent in thi revis source_from_dataset argspec arg parse_opt default parse_opt creat a dataset sourc within thi revis source_from_url argspec arg url parse_opt default url creat a url sourc arg url str the url to creat the dataset from return sourc return the new sourc the sourc creat by thi api call or an error ui_url thi is the url to the land page in the ui for thi revis return url str url you can past into a browser to view the revis ui updat argspec arg bodi set the metadata to be appli to the view when thi revis is appli arg bodi dict the chang to make to thi revis return revis the updat revis as a result of thi api call or an error exampl revis revisionupd metadata name new name descript new descript sourc argspec arg auth create_upload argspec arg filenam creat a new sourc take a bodi param which must contain a filenam of the file arg filenam str the name of the file you are upload return sourc return the new sourc exampl upload revisioncreate_uploadfoocsv lookup argspec arg source_id lookup a sourc arg source_id int the id return sourc return the new sourc the sourc result from thi api call or an error sourc argspec arg auth respons parent add_to_revis argspec arg revis associ thi sourc with the given revis blob argspec arg file_handl upload a blob dataset a blob is a file that will not be pars as a data file ie an imag video etc return sourc return the new sourc exampl with openmyblobjpg rb as f upload uploadblobf change_parse_opt argspec arg name chang a pars option on the sourc if there are not yet byte upload these pars option will be use in order to pars the file if there are alreadi byte upload thi will trigger a repars of the file and consequ a new inputschema will be creat you can call sourcelatest_input to get the newest one pars option are header_count int the number of row consid a header column_head int the one base index of row to use to gener the header encod string default to guess the encod but it can be explicitli set column_separ string for csv thi default to and for tsv but you can use a custom separ quote_char string charact use to quot valu that should be escap default to arg name string one of the option abov ie column_separ or header_count return chang parseoptionchang implement a tovalu function which you call to set the valu for our exampl assum we have thi dataset thi is my cool dataset a b c 1 2 3 4 5 6 we want to say that the first 2 row are header and the second of those 2 row should be use to make the column header we would do that like so exampl sourc sourc change_parse_optionheader_countto2 change_parse_optioncolumn_headerto2 run csv argspec arg file_handl upload a csv return the new input schema arg file_handl the file handl as return by the python function open max_retri integ option retri limit per chunk in the upload default to 5 backoff_second integ option amount of time to backoff upon a chunk upload failur default to 2 return sourc return the new sourc exampl with openmyfilecsv rb as f upload uploadcsvf df argspec arg datafram upload a panda datafram return the new sourc arg file_handl the file handl as return by the python function open max_retri integ option retri limit per chunk in the upload default to 5 backoff_second integ option amount of time to backoff upon a chunk upload failur default to 2 return sourc return the new sourc exampl import panda df pandasread_csvtestfixturessimplecsv upload uploaddfdf geojson argspec arg file_handl upload a geojson file return the new input schema arg file_handl the file handl as return by the python function open max_retri integ option retri limit per chunk in the upload default to 5 backoff_second integ option amount of time to backoff upon a chunk upload failur default to 2 return sourc return the new sourc exampl with openmygeojsonfilegeojson rb as f upload uploadgeojsonf kml argspec arg file_handl upload a kml file return the new input schema arg file_handl the file handl as return by the python function open max_retri integ option retri limit per chunk in the upload default to 5 backoff_second integ option amount of time to backoff upon a chunk upload failur default to 2 return sourc return the new sourc exampl with openmykmlfilekml rb as f upload uploadkmlf list_oper get a list of the oper that you can perform on thi object these map directli onto what return from the api in the link section of each resourc load forc the sourc to load if it a view sourc return sourc return the new sourc open_in_brows open thi sourc in your browser thi will open a window shapefil argspec arg file_handl upload a shapefil return the new input schema arg file_handl the file handl as return by the python function open max_retri integ option retri limit per chunk in the upload default to 5 backoff_second integ option amount of time to backoff upon a chunk upload failur default to 2 return sourc return the new sourc exampl with openmyshapefilearchivezip rb as f upload uploadshapefilef tsv argspec arg file_handl upload a tsv return the new input schema arg file_handl the file handl as return by the python function open max_retri integ option retri limit per chunk in the upload default to 5 backoff_second integ option amount of time to backoff upon a chunk upload failur default to 2 return sourc return the new sourc exampl with openmyfiletsv rb as f upload uploadtsvf ui_url thi is the url to the land page in the ui for the sourc return url str url you can past into a browser to view the sourc ui wait_for_finish argspec arg progress timeout sleeptim default progressfunct noop at 0x7f34e14fb7b8 sleeptime1 wait for thi dataset to finish transform and valid accept a progress function and a timeout xl argspec arg file_handl upload an xl return the new input schema arg file_handl the file handl as return by the python function open max_retri integ option retri limit per chunk in the upload default to 5 backoff_second integ option amount of time to backoff upon a chunk upload failur default to 2 return sourc return the new sourc exampl with openmyfilexl rb as f upload uploadxlsf xlsx argspec arg file_handl upload an xlsx return the new input schema arg file_handl the file handl as return by the python function open max_retri integ option retri limit per chunk in the upload default to 5 backoff_second integ option amount of time to backoff upon a chunk upload failur default to 2 return sourc return the new sourc exampl with openmyfilexlsx rb as f upload uploadxlsxf config argspec arg auth creat argspec arg name data_act parse_opt column creat a new importconfig see httpdocssocratapublishingapiaryio importconfig section for what is support in data_act parse_opt and column list list all the importconfig on thi domain lookup argspec arg name obtain a singl importconfig by name config argspec arg auth respons parent change_parse_opt argspec arg name chang a pars option on the sourc if there are not yet byte upload these pars option will be use in order to pars the file if there are alreadi byte upload thi will trigger a repars of the file and consequ a new inputschema will be creat you can call sourcelatest_input to get the newest one pars option are header_count int the number of row consid a header column_head int the one base index of row to use to gener the header encod string default to guess the encod but it can be explicitli set column_separ string for csv thi default to and for tsv but you can use a custom separ quote_char string charact use to quot valu that should be escap default to arg name string one of the option abov ie column_separ or header_count return chang parseoptionchang implement a tovalu function which you call to set the valu for our exampl assum we have thi dataset thi is my cool dataset a b c 1 2 3 4 5 6 we want to say that the first 2 row are header and the second of those 2 row should be use to make the column header we would do that like so exampl sourc sourc change_parse_optionheader_countto2 change_parse_optioncolumn_headerto2 run create_revis argspec arg fourfour creat a new revis in the context of thi importconfig sourc that happen in thi revis will take on the valu in thi config delet delet thi importconfig note that thi cannot be undon list_oper get a list of the oper that you can perform on thi object these map directli onto what return from the api in the link section of each resourc updat argspec arg bodi mutat thi importconfig in place subsequ revis open against thi importconfig will take on it new valu inputschema argspec arg auth respons parent thi repres a schema exactli as it appear in the sourc get_latest_output_schema note that thi doe not make an api request return output_schema outputschema return the latest output schema latest_output get the latest most recent creat outputschema which descend from thi inputschema return outputschema list_oper get a list of the oper that you can perform on thi object these map directli onto what return from the api in the link section of each resourc transform argspec arg bodi transform thi inputschema into an output return the new outputschema note that thi call is async the data may still be transform even though the outputschema is return see outputschemawait_for_finish to block until the outputschema thi is data as transform from an inputschema add_column argspec arg field_nam display_nam transform_expr descript add a column arg field_nam str the column field_nam must be uniqu display_nam str the column display name transform_expr str soql express to evalu and fill the column with data from descript str option column descript return output_schema outputschema return self for easi chain exampl new_output_schema output add a new column which is comput from the celsiu column add_columnfahrenheit degre fahrenheit to_numbercelsiu 9 5 32 the temperatur in celsiu add a new column which is comput from the celsiu column add_columnkelvin degre kelvin to_numbercelsiu 27315 run build_config argspec arg name data_act creat a new importconfig from thi outputschema see the api doc for what an importconfig is and whi theyr use change_column_metadata argspec arg field_nam attribut chang the column metadata thi return a columnchang which implement a to function which take the new valu to chang to arg field_nam str the column to chang attribut str the attribut of the column to chang return chang transformchang the transform chang which implement the to function exampl new_output_schema output chang the field_nam of date to the_dat change_column_metadatad field_nametothe_d chang the descript of the celsiu column change_column_metadatacelsiu descriptiontoth temperatur in celsiu chang the display name of the celsiu column change_column_metadatacelsiu display_nametodegre celsiu run change_column_transform argspec arg field_nam chang the column transform thi return a transformchang which implement a to function which take a transform express arg field_nam str the column to chang return chang transformchang the transform chang which implement the to function exampl new_output_schema output change_column_transformthe_datetoto_fixed_timestampd make the celsiu column all number change_column_transformcelsiustoto_numbercelsiu add a new column which is comput from the celsiu column add_columnfahrenheit degre fahrenheit to_numbercelsiu 9 5 32 the temperatur in celsiu run drop_column argspec arg field_nam drop the column arg field_nam str the column to drop return output_schema outputschema return self for easi chain exampl new_output_schema output drop_columnfoo run list_oper get a list of the oper that you can perform on thi object these map directli onto what return from the api in the link section of each resourc row argspec arg offset limit default offset0 limit500 get the row for thi outputschema accep offset and limit param for page through the data run run all add drop and column chang return outputschema exampl new_output_schema output chang the field_nam of date to the_dat change_column_metadatad field_nametothe_d chang the descript of the celsiu column change_column_metadatacelsiu descriptiontoth temperatur in celsiu chang the display name of the celsiu column change_column_metadatacelsiu display_nametodegre celsiu chang the transform of the_dat column to to_fixed_timestampd change_column_transformthe_datetoto_fixed_timestampd make the celsiu column all number change_column_transformcelsiustoto_numbercelsiu add a new column which is comput from the celsiu column add_columnfahrenheit degre fahrenheit to_numbercelsiu 9 5 32 the temperatur in celsiu run schema_error argspec arg offset limit default offset0 limit500 get the error that result in transform into thi output schema accept offset and limit param schema_errors_csv get the error that result in transform into thi output schema as a csv stream note that thi return a repons where repons is a python request repons object set_row_id argspec arg field_nam set the row id note you must call validate_row_id befor do thi arg field_nam str the column to set as the row id return outputschema exampl new_output_schema outputset_row_idthe_id_column validate_row_id argspec arg field_nam set the row id note you must call validate_row_id befor do thi arg field_nam str the column to valid as the row id return boolean wait_for_finish argspec arg progress timeout sleeptim default progressfunct noop at 0x7f34e14fb7b8 sleeptime1 wait for thi dataset to finish transform and valid accept a progress function and a timeout job argspec arg auth respons parent is_complet ha thi job finish or fail list_oper get a list of the oper that you can perform on thi object these map directli onto what return from the api in the link section of each resourc wait_for_finish argspec arg progress timeout sleeptim default progressfunct noop at 0x7f34e14fb7b8 sleeptime1 wait for thi dataset to finish transform and valid accept a progress function and a timeout,socratapy python sdk for the socrata data management api use this library to call into publishing and etl functionality offered when writing to socrata datasets with opencool_datasetcsv rb a file revision output socrataauthcreate name cool dataset description a description csvfile revisionapplyoutput_schema output installation example using boilerplate simple usage create a new dataset from a csv tsv xl or xlsx file create a new dataset from panda updating a dataset generating a config and using it to update advanced usage create a revision create an upload upload a file transforming your data wait for the transformation to finish error in a transformation validating row do the upsert metadata only revision development testing generating doc releasing library doc socrata create new using_config authorization live_dangerously revision create_delete_revision create_replace_revision create_update_revision create_using_config list lookup revision apply create_upload discard list_operations open_in_browser plan set_output_schema source_as_blob source_from_agent source_from_dataset source_from_url ui_url update source create_upload lookup source add_to_revision blob change_parse_option csv df geojson kml list_operations load open_in_browser shapefile tsv ui_url wait_for_finish xl xlsx configs create list lookup config change_parse_option create_revision delete list_operations update inputschema get_latest_output_schema latest_output list_operations transform outputschema add_column build_config change_column_metadata change_column_transform drop_column list_operations row run schema_errors schema_errors_csv set_row_id validate_row_id wait_for_finish job is_complete list_operations wait_for_finish installation this only support python3 installation is available through pip using a virtualenv is advised install the package by running pip3 install socratapy the only hard dependency is request which will be installed via pip panda is not required but creating a dataset from a panda dataframe is supported see below example try the command line example with python m examplescreate police report desktopcatalogdatagovseattle_real_time_fire_911_callscsv petetesttestsocratacom username socrata_username password socrata_password using boilerplate import some stuff from socrataauthorization import authorization from socrata import socrata import o boilerplate make an auth object auth authorization petetesttestsocratacom osenvironsocrata_username osenvironsocrata_password simple usage create a new dataset from a csv tsv xl or xlsx file to create a dataset you can do this with opencool_datasetcsv rb a file upload transform step revision is the change to the view in the catalog which ha not yet been applied output is the outputschema which is a change to data which can be applied via the revision revision output socrataauthcreate name cool dataset description a description csvfile transformation step we want to add some metadata to our column drop another column and add a new column which will be filled with value from another column and then transformed output output change_column_metadataa_column display_nametoa column change_column_metadataa_column descriptiontohere is a description of my a column drop_columnb_column add_columna_column_squared a column but time itself to_numbera_column to_numbera_column this is a column squared run validation of the result step output outputwait_for_finish the data ha been validated now and we can access error that happened during validation for example if one of the cell in a_column couldnt be converted to a number in the call to to_number that error would be reflected in this error_count assert outputattributeserror_count 0 if you want you can get a csv stream of all the error error outputschema_errors_csv for line in errorsiter_lines printline update step apply the revision this will make it public and available to make visualization from job revisionapplyoutput_schema output this open a browser window to your revision and you will see the progress of the job revisionopen_in_browser application is async this will block until all the data is in place and readable jobwait_for_finish similar to the csv method are the xl xlsx and tsv method which upload those file there is a blob method a well which uploads blobby data to the source this mean the data will not be parsed and will be displayed under file and document in the catalog once the revision is applied create a new dataset from panda datasets can also be created from panda dataframes import panda a pd df pdread_csvsocratapytestfixturessimplecsv do various pandasy change and modification then revision output socrataauthcreate name panda dataset description dataset made from a panda dataframe dfdf same code a above to apply the revision updating a dataset a socrata update is actually an upsert row are updated or created based on the row identifier if the rowidentifer doesnt exist all update are just appends to the dataset a replace truncates the whole dataset and then insert the new data generating a config and using it to update this is how we create our view initially with opencool_datasetcsv rb a file revision output socrataauthcreate name cool dataset description a description csvfile revisionapplyoutput_schema output this will build a configuration using the same setting file parsing and data transformation rule that we used to get our output the action that we will take will be update though it could also be replace config outputbuild_configcooldatasetconfig update now we need to save our configuration name and view id somewhere so we can update the view using our config configuration_name cooldatasetconfig view_id revisionview_id now later if we want to use that config to update our view we just need the view and the configuration_name socrata socrataauth view socrataviewslookupview_id view will be the view we are updating with the new data with openupdatedcooldatasetcsv rb a my_file revision job socratausing_config configuration_name view csvmy_file printjob our update job is now running advanced usage create a revision this is our socrata object using the auth variable from above socrata socrataauth this will make our initial revision on a view that doesnt yet exist revision socratanewname cool dataset revision is a revision object we can print it printrevision revisioncreated_by display_name rozap email chrisdurantisocratacom user_id tuggikce fourfour ij46xpxe id 346 inserted_at 20170227t230508522796 metadata none update_seq 285 upsert_jobs we can also access the attribute of the revision printrevisionattributesmetadataname cool dataset create an upload using that revision we can create an upload upload revisioncreate_uploadfoocsv and print it printupload sourcecontent_type none created_by display_name rozap email chrisdurantisocratacom user_id tuggikce source_type filename foocsv type upload finished_at none id 290 inserted_at 20170227t230718309676 schema upload a file and using that upload we just created we can put byte into it with opentestfixturessimplecsv rb a f source uploadcsvf transforming your data transforming data consists of going from input data data exactly a it appeared in the source to output data data a you want it to appear transformation from input data to output data often ha problem you might for example have a column full of number but one row in that column is actually the value hehe which cannot be transformed into a number rather than failing at each datum which is dirty or wrong transforming your data allows you to reconcile these issue we might have a dataset called tempscsv that look like date celsius 8242017 22 8252017 20 8262017 23 8272017 hehe 8282017 8292017 21 suppose we uploaded it in our previous step like this with opentempscsv rb a f source uploadcsvf input_schema sourceget_latest_input_schema our input_schema is the input data exactly a it appeared in the csv with all value of type string our output_schema is the output data a it wa guessed by socrata guessing may not always be correct which is why we have import configs to lock in a schema for automation we can get the output_schema like so output_schema input_schemaget_latest_output_schema we can now make change to the schema like so new_output_schema output change the field_name of date to the_date change_column_metadatadate field_nametothe_date change the description of the celsius column change_column_metadatacelsius descriptiontothe temperature in celsius change the display name of the celsius column change_column_metadatacelsius display_nametodegrees celsius change the transform of the_date column to to_fixed_timestampdate change_column_transformthe_datetoto_fixed_timestampdate make the celsius column all number change_column_transformcelsiustoto_numbercelsius add a new column which is computed from the celsius column add_columnfahrenheit degree fahrenheit to_numbercelsius 9 5 32 the temperature in celsius run change_column_metadatacolumn_name column_attribute take the field name used to identify the column and the column attribute to change field_name display_name description position add_columnfield_name display_name transform_expression description will create a new column we can also call drop_columncelsius which will drop the column run will then make a request and return the new output_schema or an error if something is invalid transforms can be complex soql expression available function are listed here you can do lot of stuff with them for example you could change all null value into error which wont be imported by doing something like new_output_schema output change_column_transformcelsiustocoalesceto_numbercelsius errorcelsius wa null run or you could add a new column that say if the day wa hot or not new_output_schema output add_columnis_hot wa the day hot to_numbercelsius 23 run or you could geocode a column given the following csv addresscityzipstate 10028 ravenna ave ne seattle 98128 wa 1600 pennsylvania avenue washington dc 20500 dc 6511 32nd ave nw seattle 98155 wa we could transform our first output_schema into a single column dataset where that single column is a point of the address output output add_columnlocation incident location geocodeaddress city state zip drop_columnaddress drop_columncity drop_columnstate drop_columnzip run composing these soql function into expression will allow you to validate shape clean and extend your data to make it more useful to the consumer wait for the transformation to finish transformation are async so if you want to wait for it to finish you can do so error in a transformation transformation may have had error like in the previous example we cant convert hehe to a number we can see the count of them like this printoutput_schemaattributeserror_count we can view the detailed error like this error output_schemaschema_errors we can get a csv of the error like this csv_stream output_schemaschema_errors_csv validating row we can look at the row of our schema a well row output_schemarowsoffset 0 limit 20 selfassertequalrows b ok bfoo b ok bfoo b ok bfoo b ok bfoo do the upsert now we have transformed our data into the shape we want let do an upsert job revisionapplyoutput_schema output_schema this will complete the upsert behind the scene if we want to refetch the current state of the upsert job we can do so job jobshow to get the progress printjobattributeslog detail error 0 row created 0 row updated 0 by rowidentifier 0 by sid 0 row deleted 0 time 20170228t202059 stage upsert_complete detail created 1 time 20170228t202059 stage columns_created detail created 1 time 20170228t202059 stage columns_created detail none time 20170228t202059 stage started so maybe we just want to wait here printing the progress until the job is done jobwait_for_finishprogress lambda job printjobattributeslog so now if we go look at our original fourfour our data will be there metadata only revision when there is an existing socrata view that youd like to update the metadata of you can do so by creating a source which is the socrata view view socrataviewslookupabbacafe revision viewrevisionscreate_replace_revision source revisionsource_from_dataset output_schema sourceget_latest_input_schemaget_latest_output_schema new_output_schema output_schema change_column_metadataa descriptiontomeh change_column_metadatab display_nametobbbb change_column_metadatac field_nametoccc run revisionapplyoutput_schema new_output_schema development testing install test deps by running pip install r requirementstxt this will install pdoc and panda which are required to run the test configuration is set in testauthpy for test it read the domain username and password from environment variable if you want to run the test set those environment variable to something that will work if i wanted to run the test against my local instance i would run socrata_domainlocalhost socrata_usernamesocrata_local_user socrata_passwordsocrata_local_pass bintest generating doc make the doc by running make doc releasing release to pypi by bumping the version to something reasonable and running python setuppy sdist upload r pypi note youll need your pypirc file in your home directory for help read this library doc socrata argspec args auth top level publishing object all function making http call return a result tuple where the first element in the tuple is whether or not the call succeeded and the second element is the returned object if it wa a success or a dictionary containing the error response if the call failed 2xx response are considered success 4xx and 5xx response are considered failure in the event of a socket hangup an exception is raised create shortcut to create a dataset return a create object which contains function which will create a view upload your file and validate data quality in one step to actually place the validated data into a view you can call apply on the revision revision output_schema socrataauthcreate name cool dataset description a description csvfile job revisionapplyoutput_schema output_schema args kwargs arbitrary revision metadata value return result revision outputschema return the revision that wa created and the outputschema created from your uploaded file example socrataauthcreate name cool dataset description a description csvopenmyfilecsv new argspec args metadata create an empty revision on a view that doesnt exist yet the view will be created for you and the initial revision will be returned args metadata dict metadata to apply to the revision return revision example rev socrataauthnew name hi description foo metadata view metadata anything is allowed here using_config argspec args config_name view update a dataset using the configuration that you previously created and saved the name of take the config_name parameter which uniquely identifies the config and the view object which can be obtained from socrataviewslookupviewid42 args config_name str the config name view view the view to update return result configuredjob return the configuredjob note typical usage would be in a context manager block a demonstrated in the example below in this case the configuredjob is created and immediately launched by way of the call to the configuredjobcsv method example with openmyfilecsv rb a my_file rev job pusing_configname viewcsvmy_file authorization argspec args domain username password request_id_prefix default domain manages basic authorization for accessing the socrata api this is passed into the socrata object once which is the entry point for all operation auth authorization dataseattlegov osenvironsocrata_username osenvironsocrata_password publishing socrataauth live_dangerously disable ssl checking note that this should only be used while developing against a local socrata instance revision argspec args fourfour auth create_delete_revision argspec args metadata permission default metadata permissionpublic create a revision on the view which when applied will delete row of data this is an upsert a row id must be set args metadata dict the metadata to change these change will be applied when the revision is applied permission string public or private return revision the new revision or an error example viewrevisionscreate_delete_revisionmetadata name new dataset name description description create_replace_revision argspec args metadata permission default metadata permissionpublic create a revision on the view which when applied will replace the data args metadata dict the metadata to change these change will be applied when the revision is applied permission string public or private return revision the new revision or an error example viewrevisionscreate_replace_revisionmetadata name new dataset name description updated description create_update_revision argspec args metadata permission default metadata permissionpublic create a revision on the view which when applied will update the data rather than replacing it this is an upsert if there is a rowid defined and you have duplicate id value those row will be updated otherwise they will be appended args metadata dict the metadata to change these change will be applied when the revision is applied permission string public or private return revision the new revision or an error example viewrevisionscreate_update_revisionmetadata name new dataset name description updated description create_using_config argspec args config create a revision for the given dataset list list all the revision on the view return listrevision lookup argspec args revision_seq lookup a revision within the view based on the sequence number args revision_seq int the sequence number of the revision to lookup return revision the revision resulting from this api call or an error revision argspec args auth response parent a revision is a change to a dataset apply argspec args output_schema apply the revision to the view that it wa opened on args output_schema outputschema optional output schema if your revision includes data change this should be included if it is a metadata only revision then you will not have an output schema and you do not need to pas anything here return job example job revisionapplyoutput_schema my_output_schema create_upload argspec args filename parse_options default filename create an upload within this revision args filename str the name of the file to upload return source return the new source the source created by this api call or an error discard discard this open revision return revision the closed revision or an error list_operations get a list of the operation that you can perform on this object these map directly onto whats returned from the api in the link section of each resource open_in_browser open this revision in your browser this will open a window plan return the list of operation this revision will make when it is applied return dict set_output_schema argspec args output_schema_id set the output schema id on the revision this is what will get applied when the revision is applied if no ouput schema is explicitly supplied args output_schema_id int the output schema id return revision the updated revision a a result of this api call or an error example revision revisionset_output_schema42 source_as_blob argspec args filename parse_options default filename create a source from a file that should remain unparsed source_from_agent argspec args agent_uid namespace path parse_options parameter default agent_uid namespace create a source from a connection agent in this revision source_from_dataset argspec args parse_options default parse_options create a dataset source within this revision source_from_url argspec args url parse_options default url create a url source args url str the url to create the dataset from return source return the new source the source created by this api call or an error ui_url this is the url to the landing page in the ui for this revision return url str url you can paste into a browser to view the revision ui update argspec args body set the metadata to be applied to the view when this revision is applied args body dict the change to make to this revision return revision the updated revision a a result of this api call or an error example revision revisionupdate metadata name new name description new description source argspec args auth create_upload argspec args filename create a new source take a body param which must contain a filename of the file args filename str the name of the file you are uploading return source return the new source example upload revisioncreate_uploadfoocsv lookup argspec args source_id lookup a source args source_id int the id return source return the new source the source resulting from this api call or an error source argspec args auth response parent add_to_revision argspec args revision associate this source with the given revision blob argspec args file_handle uploads a blob dataset a blob is a file that will not be parsed a a data file ie an image video etc return source return the new source example with openmyblobjpg rb a f upload uploadblobf change_parse_option argspec args name change a parse option on the source if there are not yet byte uploaded these parse option will be used in order to parse the file if there are already byte uploaded this will trigger a reparsing of the file and consequently a new inputschema will be created you can call sourcelatest_input to get the newest one parse option are header_count int the number of row considered a header column_header int the one based index of row to use to generate the header encoding string default to guessing the encoding but it can be explicitly set column_separator string for csvs this default to and for tsvs but you can use a custom separator quote_char string character used to quote value that should be escaped default to args name string one of the option above ie column_separator or header_count return change parseoptionchange implement a tovalue function which you call to set the value for our example assume we have this dataset this is my cool dataset a b c 1 2 3 4 5 6 we want to say that the first 2 row are header and the second of those 2 row should be used to make the column header we would do that like so example source source change_parse_optionheader_countto2 change_parse_optioncolumn_headerto2 run csv argspec args file_handle upload a csv return the new input schema args file_handle the file handle a returned by the python function open max_retries integer optional retry limit per chunk in the upload default to 5 backoff_seconds integer optional amount of time to backoff upon a chunk upload failure default to 2 return source return the new source example with openmyfilecsv rb a f upload uploadcsvf df argspec args dataframe upload a panda dataframe return the new source args file_handle the file handle a returned by the python function open max_retries integer optional retry limit per chunk in the upload default to 5 backoff_seconds integer optional amount of time to backoff upon a chunk upload failure default to 2 return source return the new source example import panda df pandasread_csvtestfixturessimplecsv upload uploaddfdf geojson argspec args file_handle upload a geojson file return the new input schema args file_handle the file handle a returned by the python function open max_retries integer optional retry limit per chunk in the upload default to 5 backoff_seconds integer optional amount of time to backoff upon a chunk upload failure default to 2 return source return the new source example with openmygeojsonfilegeojson rb a f upload uploadgeojsonf kml argspec args file_handle upload a kml file return the new input schema args file_handle the file handle a returned by the python function open max_retries integer optional retry limit per chunk in the upload default to 5 backoff_seconds integer optional amount of time to backoff upon a chunk upload failure default to 2 return source return the new source example with openmykmlfilekml rb a f upload uploadkmlf list_operations get a list of the operation that you can perform on this object these map directly onto whats returned from the api in the link section of each resource load force the source to load if it a view source return source return the new source open_in_browser open this source in your browser this will open a window shapefile argspec args file_handle upload a shapefile return the new input schema args file_handle the file handle a returned by the python function open max_retries integer optional retry limit per chunk in the upload default to 5 backoff_seconds integer optional amount of time to backoff upon a chunk upload failure default to 2 return source return the new source example with openmyshapefilearchivezip rb a f upload uploadshapefilef tsv argspec args file_handle upload a tsv return the new input schema args file_handle the file handle a returned by the python function open max_retries integer optional retry limit per chunk in the upload default to 5 backoff_seconds integer optional amount of time to backoff upon a chunk upload failure default to 2 return source return the new source example with openmyfiletsv rb a f upload uploadtsvf ui_url this is the url to the landing page in the ui for the source return url str url you can paste into a browser to view the source ui wait_for_finish argspec args progress timeout sleeptime default progressfunction noop at 0x7f34e14fb7b8 sleeptime1 wait for this dataset to finish transforming and validating accepts a progress function and a timeout xl argspec args file_handle upload an xl return the new input schema args file_handle the file handle a returned by the python function open max_retries integer optional retry limit per chunk in the upload default to 5 backoff_seconds integer optional amount of time to backoff upon a chunk upload failure default to 2 return source return the new source example with openmyfilexls rb a f upload uploadxlsf xlsx argspec args file_handle upload an xlsx return the new input schema args file_handle the file handle a returned by the python function open max_retries integer optional retry limit per chunk in the upload default to 5 backoff_seconds integer optional amount of time to backoff upon a chunk upload failure default to 2 return source return the new source example with openmyfilexlsx rb a f upload uploadxlsxf configs argspec args auth create argspec args name data_action parse_options column create a new importconfig see httpdocssocratapublishingapiaryio importconfig section for what is supported in data_action parse_options and column list list all the importconfigs on this domain lookup argspec args name obtain a single importconfig by name config argspec args auth response parent change_parse_option argspec args name change a parse option on the source if there are not yet byte uploaded these parse option will be used in order to parse the file if there are already byte uploaded this will trigger a reparsing of the file and consequently a new inputschema will be created you can call sourcelatest_input to get the newest one parse option are header_count int the number of row considered a header column_header int the one based index of row to use to generate the header encoding string default to guessing the encoding but it can be explicitly set column_separator string for csvs this default to and for tsvs but you can use a custom separator quote_char string character used to quote value that should be escaped default to args name string one of the option above ie column_separator or header_count return change parseoptionchange implement a tovalue function which you call to set the value for our example assume we have this dataset this is my cool dataset a b c 1 2 3 4 5 6 we want to say that the first 2 row are header and the second of those 2 row should be used to make the column header we would do that like so example source source change_parse_optionheader_countto2 change_parse_optioncolumn_headerto2 run create_revision argspec args fourfour create a new revision in the context of this importconfig source that happen in this revision will take on the value in this config delete delete this importconfig note that this cannot be undone list_operations get a list of the operation that you can perform on this object these map directly onto whats returned from the api in the link section of each resource update argspec args body mutate this importconfig in place subsequent revision opened against this importconfig will take on it new value inputschema argspec args auth response parent this represents a schema exactly a it appeared in the source get_latest_output_schema note that this doe not make an api request return output_schema outputschema return the latest output schema latest_output get the latest most recently created outputschema which descends from this inputschema return outputschema list_operations get a list of the operation that you can perform on this object these map directly onto whats returned from the api in the link section of each resource transform argspec args body transform this inputschema into an output return the new outputschema note that this call is async the data may still be transforming even though the outputschema is returned see outputschemawait_for_finish to block until the outputschema this is data a transformed from an inputschema add_column argspec args field_name display_name transform_expr description add a column args field_name str the column field_name must be unique display_name str the column display name transform_expr str soql expression to evaluate and fill the column with data from description str optional column description return output_schema outputschema return self for easy chaining example new_output_schema output add a new column which is computed from the celsius column add_columnfahrenheit degree fahrenheit to_numbercelsius 9 5 32 the temperature in celsius add a new column which is computed from the celsius column add_columnkelvin degree kelvin to_numbercelsius 27315 run build_config argspec args name data_action create a new importconfig from this outputschema see the api doc for what an importconfig is and why theyre useful change_column_metadata argspec args field_name attribute change the column metadata this return a columnchange which implement a to function which take the new value to change to args field_name str the column to change attribute str the attribute of the column to change return change transformchange the transform change which implement the to function example new_output_schema output change the field_name of date to the_date change_column_metadatadate field_nametothe_date change the description of the celsius column change_column_metadatacelsius descriptiontothe temperature in celsius change the display name of the celsius column change_column_metadatacelsius display_nametodegrees celsius run change_column_transform argspec args field_name change the column transform this return a transformchange which implement a to function which take a transform expression args field_name str the column to change return change transformchange the transform change which implement the to function example new_output_schema output change_column_transformthe_datetoto_fixed_timestampdate make the celsius column all number change_column_transformcelsiustoto_numbercelsius add a new column which is computed from the celsius column add_columnfahrenheit degree fahrenheit to_numbercelsius 9 5 32 the temperature in celsius run drop_column argspec args field_name drop the column args field_name str the column to drop return output_schema outputschema return self for easy chaining example new_output_schema output drop_columnfoo run list_operations get a list of the operation that you can perform on this object these map directly onto whats returned from the api in the link section of each resource row argspec args offset limit default offset0 limit500 get the row for this outputschema acceps offset and limit params for paging through the data run run all add drop and column change return outputschema example new_output_schema output change the field_name of date to the_date change_column_metadatadate field_nametothe_date change the description of the celsius column change_column_metadatacelsius descriptiontothe temperature in celsius change the display name of the celsius column change_column_metadatacelsius display_nametodegrees celsius change the transform of the_date column to to_fixed_timestampdate change_column_transformthe_datetoto_fixed_timestampdate make the celsius column all number change_column_transformcelsiustoto_numbercelsius add a new column which is computed from the celsius column add_columnfahrenheit degree fahrenheit to_numbercelsius 9 5 32 the temperature in celsius run schema_errors argspec args offset limit default offset0 limit500 get the error that resulted in transforming into this output schema accepts offset and limit params schema_errors_csv get the error that result in transforming into this output schema a a csv stream note that this return a reponse where reponse is a python request reponse object set_row_id argspec args field_name set the row id note you must call validate_row_id before doing this args field_name str the column to set a the row id return outputschema example new_output_schema outputset_row_idthe_id_column validate_row_id argspec args field_name set the row id note you must call validate_row_id before doing this args field_name str the column to validate a the row id return boolean wait_for_finish argspec args progress timeout sleeptime default progressfunction noop at 0x7f34e14fb7b8 sleeptime1 wait for this dataset to finish transforming and validating accepts a progress function and a timeout job argspec args auth response parent is_complete ha this job finished or failed list_operations get a list of the operation that you can perform on this object these map directly onto whats returned from the api in the link section of each resource wait_for_finish argspec args progress timeout sleeptime default progressfunction noop at 0x7f34e14fb7b8 sleeptime1 wait for this dataset to finish transforming and validating accepts a progress function and a timeout,socratapy python sdk socrata data management api use library call publishing etl functionality offered writing socrata datasets opencool_datasetcsv rb file revision output socrataauthcreate name cool dataset description description csvfile revisionapplyoutput_schema output installation example using boilerplate simple usage create new dataset csv tsv xl xlsx file create new dataset panda updating dataset generating config using update advanced usage create revision create upload upload file transforming data wait transformation finish error transformation validating row upsert metadata revision development testing generating doc releasing library doc socrata create new using_config authorization live_dangerously revision create_delete_revision create_replace_revision create_update_revision create_using_config list lookup revision apply create_upload discard list_operations open_in_browser plan set_output_schema source_as_blob source_from_agent source_from_dataset source_from_url ui_url update source create_upload lookup source add_to_revision blob change_parse_option csv df geojson kml list_operations load open_in_browser shapefile tsv ui_url wait_for_finish xl xlsx configs create list lookup config change_parse_option create_revision delete list_operations update inputschema get_latest_output_schema latest_output list_operations transform outputschema add_column build_config change_column_metadata change_column_transform drop_column list_operations row run schema_errors schema_errors_csv set_row_id validate_row_id wait_for_finish job is_complete list_operations wait_for_finish installation support python3 installation available pip using virtualenv advised install package running pip3 install socratapy hard dependency request installed via pip panda required creating dataset panda dataframe supported see example try command line example python examplescreate police report desktopcatalogdatagovseattle_real_time_fire_911_callscsv petetesttestsocratacom username socrata_username password socrata_password using boilerplate import stuff socrataauthorization import authorization socrata import socrata import o boilerplate make auth object auth authorization petetesttestsocratacom osenvironsocrata_username osenvironsocrata_password simple usage create new dataset csv tsv xl xlsx file create dataset opencool_datasetcsv rb file upload transform step revision change view catalog yet applied output outputschema change data applied via revision revision output socrataauthcreate name cool dataset description description csvfile transformation step want add metadata column drop another column add new column filled value another column transformed output output change_column_metadataa_column display_nametoa column change_column_metadataa_column descriptiontohere description column drop_columnb_column add_columna_column_squared column time to_numbera_column to_numbera_column column squared run validation result step output outputwait_for_finish data validated access error happened validation example one cell a_column couldnt converted number call to_number error would reflected error_count assert outputattributeserror_count 0 want get csv stream error error outputschema_errors_csv line errorsiter_lines printline update step apply revision make public available make visualization job revisionapplyoutput_schema output open browser window revision see progress job revisionopen_in_browser application async block data place readable jobwait_for_finish similar csv method xl xlsx tsv method upload file blob method well uploads blobby data source mean data parsed displayed file document catalog revision applied create new dataset panda datasets also created panda dataframes import panda pd df pdread_csvsocratapytestfixturessimplecsv various pandasy change modification revision output socrataauthcreate name panda dataset description dataset made panda dataframe dfdf code apply revision updating dataset socrata update actually upsert row updated created based row identifier rowidentifer doesnt exist update appends dataset replace truncates whole dataset insert new data generating config using update create view initially opencool_datasetcsv rb file revision output socrataauthcreate name cool dataset description description csvfile revisionapplyoutput_schema output build configuration using setting file parsing data transformation rule used get output action take update though could also replace config outputbuild_configcooldatasetconfig update need save configuration name view id somewhere update view using config configuration_name cooldatasetconfig view_id revisionview_id later want use config update view need view configuration_name socrata socrataauth view socrataviewslookupview_id view view updating new data openupdatedcooldatasetcsv rb my_file revision job socratausing_config configuration_name view csvmy_file printjob update job running advanced usage create revision socrata object using auth variable socrata socrataauth make initial revision view doesnt yet exist revision socratanewname cool dataset revision revision object print printrevision revisioncreated_by display_name rozap email chrisdurantisocratacom user_id tuggikce fourfour ij46xpxe id 346 inserted_at 20170227t230508522796 metadata none update_seq 285 upsert_jobs also access attribute revision printrevisionattributesmetadataname cool dataset create upload using revision create upload upload revisioncreate_uploadfoocsv print printupload sourcecontent_type none created_by display_name rozap email chrisdurantisocratacom user_id tuggikce source_type filename foocsv type upload finished_at none id 290 inserted_at 20170227t230718309676 schema upload file using upload created put byte opentestfixturessimplecsv rb f source uploadcsvf transforming data transforming data consists going input data data exactly appeared source output data data want appear transformation input data output data often problem might example column full number one row column actually value hehe cannot transformed number rather failing datum dirty wrong transforming data allows reconcile issue might dataset called tempscsv look like date celsius 8242017 22 8252017 20 8262017 23 8272017 hehe 8282017 8292017 21 suppose uploaded previous step like opentempscsv rb f source uploadcsvf input_schema sourceget_latest_input_schema input_schema input data exactly appeared csv value type string output_schema output data guessed socrata guessing may always correct import configs lock schema automation get output_schema like output_schema input_schemaget_latest_output_schema make change schema like new_output_schema output change field_name date the_date change_column_metadatadate field_nametothe_date change description celsius column change_column_metadatacelsius descriptiontothe temperature celsius change display name celsius column change_column_metadatacelsius display_nametodegrees celsius change transform the_date column to_fixed_timestampdate change_column_transformthe_datetoto_fixed_timestampdate make celsius column number change_column_transformcelsiustoto_numbercelsius add new column computed celsius column add_columnfahrenheit degree fahrenheit to_numbercelsius 9 5 32 temperature celsius run change_column_metadatacolumn_name column_attribute take field name used identify column column attribute change field_name display_name description position add_columnfield_name display_name transform_expression description create new column also call drop_columncelsius drop column run make request return new output_schema error something invalid transforms complex soql expression available function listed lot stuff example could change null value error wont imported something like new_output_schema output change_column_transformcelsiustocoalesceto_numbercelsius errorcelsius null run could add new column say day hot new_output_schema output add_columnis_hot day hot to_numbercelsius 23 run could geocode column given following csv addresscityzipstate 10028 ravenna ave ne seattle 98128 wa 1600 pennsylvania avenue washington dc 20500 dc 6511 32nd ave nw seattle 98155 wa could transform first output_schema single column dataset single column point address output output add_columnlocation incident location geocodeaddress city state zip drop_columnaddress drop_columncity drop_columnstate drop_columnzip run composing soql function expression allow validate shape clean extend data make useful consumer wait transformation finish transformation async want wait finish error transformation transformation may error like previous example cant convert hehe number see count like printoutput_schemaattributeserror_count view detailed error like error output_schemaschema_errors get csv error like csv_stream output_schemaschema_errors_csv validating row look row schema well row output_schemarowsoffset 0 limit 20 selfassertequalrows b ok bfoo b ok bfoo b ok bfoo b ok bfoo upsert transformed data shape want let upsert job revisionapplyoutput_schema output_schema complete upsert behind scene want refetch current state upsert job job jobshow get progress printjobattributeslog detail error 0 row created 0 row updated 0 rowidentifier 0 sid 0 row deleted 0 time 20170228t202059 stage upsert_complete detail created 1 time 20170228t202059 stage columns_created detail created 1 time 20170228t202059 stage columns_created detail none time 20170228t202059 stage started maybe want wait printing progress job done jobwait_for_finishprogress lambda job printjobattributeslog go look original fourfour data metadata revision existing socrata view youd like update metadata creating source socrata view view socrataviewslookupabbacafe revision viewrevisionscreate_replace_revision source revisionsource_from_dataset output_schema sourceget_latest_input_schemaget_latest_output_schema new_output_schema output_schema change_column_metadataa descriptiontomeh change_column_metadatab display_nametobbbb change_column_metadatac field_nametoccc run revisionapplyoutput_schema new_output_schema development testing install test deps running pip install r requirementstxt install pdoc panda required run test configuration set testauthpy test read domain username password environment variable want run test set environment variable something work wanted run test local instance would run socrata_domainlocalhost socrata_usernamesocrata_local_user socrata_passwordsocrata_local_pass bintest generating doc make doc running make doc releasing release pypi bumping version something reasonable running python setuppy sdist upload r pypi note youll need pypirc file home directory help read library doc socrata argspec args auth top level publishing object function making http call return result tuple first element tuple whether call succeeded second element returned object success dictionary containing error response call failed 2xx response considered success 4xx 5xx response considered failure event socket hangup exception raised create shortcut create dataset return create object contains function create view upload file validate data quality one step actually place validated data view call apply revision revision output_schema socrataauthcreate name cool dataset description description csvfile job revisionapplyoutput_schema output_schema args kwargs arbitrary revision metadata value return result revision outputschema return revision created outputschema created uploaded file example socrataauthcreate name cool dataset description description csvopenmyfilecsv new argspec args metadata create empty revision view doesnt exist yet view created initial revision returned args metadata dict metadata apply revision return revision example rev socrataauthnew name hi description foo metadata view metadata anything allowed using_config argspec args config_name view update dataset using configuration previously created saved name take config_name parameter uniquely identifies config view object obtained socrataviewslookupviewid42 args config_name str config name view view view update return result configuredjob return configuredjob note typical usage would context manager block demonstrated example case configuredjob created immediately launched way call configuredjobcsv method example openmyfilecsv rb my_file rev job pusing_configname viewcsvmy_file authorization argspec args domain username password request_id_prefix default domain manages basic authorization accessing socrata api passed socrata object entry point operation auth authorization dataseattlegov osenvironsocrata_username osenvironsocrata_password publishing socrataauth live_dangerously disable ssl checking note used developing local socrata instance revision argspec args fourfour auth create_delete_revision argspec args metadata permission default metadata permissionpublic create revision view applied delete row data upsert row id must set args metadata dict metadata change change applied revision applied permission string public private return revision new revision error example viewrevisionscreate_delete_revisionmetadata name new dataset name description description create_replace_revision argspec args metadata permission default metadata permissionpublic create revision view applied replace data args metadata dict metadata change change applied revision applied permission string public private return revision new revision error example viewrevisionscreate_replace_revisionmetadata name new dataset name description updated description create_update_revision argspec args metadata permission default metadata permissionpublic create revision view applied update data rather replacing upsert rowid defined duplicate id value row updated otherwise appended args metadata dict metadata change change applied revision applied permission string public private return revision new revision error example viewrevisionscreate_update_revisionmetadata name new dataset name description updated description create_using_config argspec args config create revision given dataset list list revision view return listrevision lookup argspec args revision_seq lookup revision within view based sequence number args revision_seq int sequence number revision lookup return revision revision resulting api call error revision argspec args auth response parent revision change dataset apply argspec args output_schema apply revision view opened args output_schema outputschema optional output schema revision includes data change included metadata revision output schema need pas anything return job example job revisionapplyoutput_schema my_output_schema create_upload argspec args filename parse_options default filename create upload within revision args filename str name file upload return source return new source source created api call error discard discard open revision return revision closed revision error list_operations get list operation perform object map directly onto whats returned api link section resource open_in_browser open revision browser open window plan return list operation revision make applied return dict set_output_schema argspec args output_schema_id set output schema id revision get applied revision applied ouput schema explicitly supplied args output_schema_id int output schema id return revision updated revision result api call error example revision revisionset_output_schema42 source_as_blob argspec args filename parse_options default filename create source file remain unparsed source_from_agent argspec args agent_uid namespace path parse_options parameter default agent_uid namespace create source connection agent revision source_from_dataset argspec args parse_options default parse_options create dataset source within revision source_from_url argspec args url parse_options default url create url source args url str url create dataset return source return new source source created api call error ui_url url landing page ui revision return url str url paste browser view revision ui update argspec args body set metadata applied view revision applied args body dict change make revision return revision updated revision result api call error example revision revisionupdate metadata name new name description new description source argspec args auth create_upload argspec args filename create new source take body param must contain filename file args filename str name file uploading return source return new source example upload revisioncreate_uploadfoocsv lookup argspec args source_id lookup source args source_id int id return source return new source source resulting api call error source argspec args auth response parent add_to_revision argspec args revision associate source given revision blob argspec args file_handle uploads blob dataset blob file parsed data file ie image video etc return source return new source example openmyblobjpg rb f upload uploadblobf change_parse_option argspec args name change parse option source yet byte uploaded parse option used order parse file already byte uploaded trigger reparsing file consequently new inputschema created call sourcelatest_input get newest one parse option header_count int number row considered header column_header int one based index row use generate header encoding string default guessing encoding explicitly set column_separator string csvs default tsvs use custom separator quote_char string character used quote value escaped default args name string one option ie column_separator header_count return change parseoptionchange implement tovalue function call set value example assume dataset cool dataset b c 1 2 3 4 5 6 want say first 2 row header second 2 row used make column header would like example source source change_parse_optionheader_countto2 change_parse_optioncolumn_headerto2 run csv argspec args file_handle upload csv return new input schema args file_handle file handle returned python function open max_retries integer optional retry limit per chunk upload default 5 backoff_seconds integer optional amount time backoff upon chunk upload failure default 2 return source return new source example openmyfilecsv rb f upload uploadcsvf df argspec args dataframe upload panda dataframe return new source args file_handle file handle returned python function open max_retries integer optional retry limit per chunk upload default 5 backoff_seconds integer optional amount time backoff upon chunk upload failure default 2 return source return new source example import panda df pandasread_csvtestfixturessimplecsv upload uploaddfdf geojson argspec args file_handle upload geojson file return new input schema args file_handle file handle returned python function open max_retries integer optional retry limit per chunk upload default 5 backoff_seconds integer optional amount time backoff upon chunk upload failure default 2 return source return new source example openmygeojsonfilegeojson rb f upload uploadgeojsonf kml argspec args file_handle upload kml file return new input schema args file_handle file handle returned python function open max_retries integer optional retry limit per chunk upload default 5 backoff_seconds integer optional amount time backoff upon chunk upload failure default 2 return source return new source example openmykmlfilekml rb f upload uploadkmlf list_operations get list operation perform object map directly onto whats returned api link section resource load force source load view source return source return new source open_in_browser open source browser open window shapefile argspec args file_handle upload shapefile return new input schema args file_handle file handle returned python function open max_retries integer optional retry limit per chunk upload default 5 backoff_seconds integer optional amount time backoff upon chunk upload failure default 2 return source return new source example openmyshapefilearchivezip rb f upload uploadshapefilef tsv argspec args file_handle upload tsv return new input schema args file_handle file handle returned python function open max_retries integer optional retry limit per chunk upload default 5 backoff_seconds integer optional amount time backoff upon chunk upload failure default 2 return source return new source example openmyfiletsv rb f upload uploadtsvf ui_url url landing page ui source return url str url paste browser view source ui wait_for_finish argspec args progress timeout sleeptime default progressfunction noop 0x7f34e14fb7b8 sleeptime1 wait dataset finish transforming validating accepts progress function timeout xl argspec args file_handle upload xl return new input schema args file_handle file handle returned python function open max_retries integer optional retry limit per chunk upload default 5 backoff_seconds integer optional amount time backoff upon chunk upload failure default 2 return source return new source example openmyfilexls rb f upload uploadxlsf xlsx argspec args file_handle upload xlsx return new input schema args file_handle file handle returned python function open max_retries integer optional retry limit per chunk upload default 5 backoff_seconds integer optional amount time backoff upon chunk upload failure default 2 return source return new source example openmyfilexlsx rb f upload uploadxlsxf configs argspec args auth create argspec args name data_action parse_options column create new importconfig see httpdocssocratapublishingapiaryio importconfig section supported data_action parse_options column list list importconfigs domain lookup argspec args name obtain single importconfig name config argspec args auth response parent change_parse_option argspec args name change parse option source yet byte uploaded parse option used order parse file already byte uploaded trigger reparsing file consequently new inputschema created call sourcelatest_input get newest one parse option header_count int number row considered header column_header int one based index row use generate header encoding string default guessing encoding explicitly set column_separator string csvs default tsvs use custom separator quote_char string character used quote value escaped default args name string one option ie column_separator header_count return change parseoptionchange implement tovalue function call set value example assume dataset cool dataset b c 1 2 3 4 5 6 want say first 2 row header second 2 row used make column header would like example source source change_parse_optionheader_countto2 change_parse_optioncolumn_headerto2 run create_revision argspec args fourfour create new revision context importconfig source happen revision take value config delete delete importconfig note cannot undone list_operations get list operation perform object map directly onto whats returned api link section resource update argspec args body mutate importconfig place subsequent revision opened importconfig take new value inputschema argspec args auth response parent represents schema exactly appeared source get_latest_output_schema note make api request return output_schema outputschema return latest output schema latest_output get latest recently created outputschema descends inputschema return outputschema list_operations get list operation perform object map directly onto whats returned api link section resource transform argspec args body transform inputschema output return new outputschema note call async data may still transforming even though outputschema returned see outputschemawait_for_finish block outputschema data transformed inputschema add_column argspec args field_name display_name transform_expr description add column args field_name str column field_name must unique display_name str column display name transform_expr str soql expression evaluate fill column data description str optional column description return output_schema outputschema return self easy chaining example new_output_schema output add new column computed celsius column add_columnfahrenheit degree fahrenheit to_numbercelsius 9 5 32 temperature celsius add new column computed celsius column add_columnkelvin degree kelvin to_numbercelsius 27315 run build_config argspec args name data_action create new importconfig outputschema see api doc importconfig theyre useful change_column_metadata argspec args field_name attribute change column metadata return columnchange implement function take new value change args field_name str column change attribute str attribute column change return change transformchange transform change implement function example new_output_schema output change field_name date the_date change_column_metadatadate field_nametothe_date change description celsius column change_column_metadatacelsius descriptiontothe temperature celsius change display name celsius column change_column_metadatacelsius display_nametodegrees celsius run change_column_transform argspec args field_name change column transform return transformchange implement function take transform expression args field_name str column change return change transformchange transform change implement function example new_output_schema output change_column_transformthe_datetoto_fixed_timestampdate make celsius column number change_column_transformcelsiustoto_numbercelsius add new column computed celsius column add_columnfahrenheit degree fahrenheit to_numbercelsius 9 5 32 temperature celsius run drop_column argspec args field_name drop column args field_name str column drop return output_schema outputschema return self easy chaining example new_output_schema output drop_columnfoo run list_operations get list operation perform object map directly onto whats returned api link section resource row argspec args offset limit default offset0 limit500 get row outputschema acceps offset limit params paging data run run add drop column change return outputschema example new_output_schema output change field_name date the_date change_column_metadatadate field_nametothe_date change description celsius column change_column_metadatacelsius descriptiontothe temperature celsius change display name celsius column change_column_metadatacelsius display_nametodegrees celsius change transform the_date column to_fixed_timestampdate change_column_transformthe_datetoto_fixed_timestampdate make celsius column number change_column_transformcelsiustoto_numbercelsius add new column computed celsius column add_columnfahrenheit degree fahrenheit to_numbercelsius 9 5 32 temperature celsius run schema_errors argspec args offset limit default offset0 limit500 get error resulted transforming output schema accepts offset limit params schema_errors_csv get error result transforming output schema csv stream note return reponse reponse python request reponse object set_row_id argspec args field_name set row id note must call validate_row_id args field_name str column set row id return outputschema example new_output_schema outputset_row_idthe_id_column validate_row_id argspec args field_name set row id note must call validate_row_id args field_name str column validate row id return boolean wait_for_finish argspec args progress timeout sleeptime default progressfunction noop 0x7f34e14fb7b8 sleeptime1 wait dataset finish transforming validating accepts progress function timeout job argspec args auth response parent is_complete job finished failed list_operations get list operation perform object map directly onto whats returned api link section resource wait_for_finish argspec args progress timeout sleeptime default progressfunction noop 0x7f34e14fb7b8 sleeptime1 wait dataset finish transforming validating accepts progress function timeout
Python ,"Data Engineering Project
  
Data Engineering Project is an implementation of the data pipeline which consumes the latest news from RSS Feeds and makes them available for users via handy API.
The pipeline infrastructure is built using popular, open-source projects.
Access the latest news and headlines in one place. 💪
Table of Contents

Architecture diagram
How it works

Data scraping
Data flow
Data access


Prerequisites
Running project
Testing
API service
References
Contributions
License
Contact

Architecture diagram

How it works
Data Scraping
Airflow DAG is responsible for the execution of Python scraping modules.
It runs periodically every X minutes producing micro-batches.


First task updates proxypool. Using proxies in combination with rotating user agents can help get scrapers past most of the anti-scraping measures and prevent being detected as a scraper.


Second task extracts news from RSS feeds provided in the configuration file, validates the quality and sends data into Kafka topic A. The extraction process is using validated proxies from proxypool.


Data flow

Kafka Connect Mongo Sink consumes data from Kafka topic A and stores news in MongoDB using upsert functionality based on _id field.
Debezium MongoDB Source tracks a MongoDB replica set for document changes in databases and collections, recording those changes as events in Kafka topic B.
Kafka Connect Elasticsearch Sink consumes data from Kafka topic B and upserts news in Elasticsearch. Data replicated between topics A and B ensures MongoDB and ElasticSearch synchronization. Command Query Responsibility Segregation (CQRS) pattern allows the use of separate models for updating and reading information.
Kafka Connect S3-Minio Sink consumes records from Kafka topic B and stores them in MinIO (high-performance object storage) to ensure data persistency.

Data access

Data gathered by previous steps can be easily accessed in API service  using public endpoints.

Prerequisites
Software required to run the project. Install:

Docker
Python 3.8+ (pip)
docker-compose

Running project
Script manage.sh - wrapper for docker-compose works as a managing tool.

Build project infrastructure

./manage.sh up

Stop project infrastructure

./manage.sh stop

Delete project infrastructure

./manage.sh down
Testing
Script run_tests.sh executes unit tests against Airflow scraping modules and Django Rest Framework applications.
./run_tests.sh
API service
Read detailed documentation on how to interact with data collected by pipeline using search endpoints.
Example searches:

see all news

http://0.0.0.0:5000/api/v1/news/ 


add search_fields title and description, see all of the news containing the Robert Lewandowski phrase

http://0.0.0.0:5000/api/v1/news/?search=Robert%20Lewandowski 


find news containing the Lewandowski phrase in their titles

http://0.0.0.0:5000/api/v1/news/?search=title|Lewandowski 


see all of the polish news containing the Lewandowski phrase

http://0.0.0.0:5000/api/v1/news/?search=lewandowski&language=pl

References
Inspired by following codes, articles and videos:

How we launched a data product in 60 days with AWS
Toruń JUG #55 - ""Kafka Connect - szwajcarski scyzoryk w rękach inżyniera?"" - Mariusz Strzelecki
Kafka Elasticsearch Sink Connector and the Power of Single Message Transformations
Docker Tips and Tricks with Kafka Connect, ksqlDB, and Kafka

Contributions
Contributions are what makes the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.

Fork the Project
Create your Feature Branch (git checkout -b feature/AmazingFeature)
Commit your Changes (git commit -m 'Add some AmazingFeature')
Push to the Branch (git push origin feature/AmazingFeature)
Open a Pull Request

License
Distributed under the MIT License. See LICENSE for more information.
Contact
Please feel free to contact me if you have any questions.
Damian Kliś @DamianKlis
",data engin project data engin project is an implement of the data pipelin which consum the latest news from rss feed and make them avail for user via handi api the pipelin infrastructur is built use popular opensourc project access the latest news and headlin in one place tabl of content architectur diagram how it work data scrape data flow data access prerequisit run project test api servic refer contribut licens contact architectur diagram how it work data scrape airflow dag is respons for the execut of python scrape modul it run period everi x minut produc microbatch first task updat proxypool use proxi in combin with rotat user agent can help get scraper past most of the antiscrap measur and prevent be detect as a scraper second task extract news from rss feed provid in the configur file valid the qualiti and send data into kafka topic a the extract process is use valid proxi from proxypool data flow kafka connect mongo sink consum data from kafka topic a and store news in mongodb use upsert function base on _id field debezium mongodb sourc track a mongodb replica set for document chang in databas and collect record those chang as event in kafka topic b kafka connect elasticsearch sink consum data from kafka topic b and upsert news in elasticsearch data replic between topic a and b ensur mongodb and elasticsearch synchron command queri respons segreg cqr pattern allow the use of separ model for updat and read inform kafka connect s3minio sink consum record from kafka topic b and store them in minio highperform object storag to ensur data persist data access data gather by previou step can be easili access in api servic use public endpoint prerequisit softwar requir to run the project instal docker python 38 pip dockercompos run project script managesh wrapper for dockercompos work as a manag tool build project infrastructur managesh up stop project infrastructur managesh stop delet project infrastructur managesh down test script run_testssh execut unit test against airflow scrape modul and django rest framework applic run_testssh api servic read detail document on how to interact with data collect by pipelin use search endpoint exampl search see all news http00005000apiv1new add search_field titl and descript see all of the news contain the robert lewandowski phrase http00005000apiv1newssearchrobert20lewandowski find news contain the lewandowski phrase in their titl http00005000apiv1newssearchtitlelewandowski see all of the polish news contain the lewandowski phrase http00005000apiv1newssearchlewandowskilanguagepl refer inspir by follow code articl and video how we launch a data product in 60 day with aw toru jug 55 kafka connect szwajcarski scyzoryk w rkach inyniera mariusz strzelecki kafka elasticsearch sink connector and the power of singl messag transform docker tip and trick with kafka connect ksqldb and kafka contribut contribut are what make the opensourc commun such an amaz place to learn inspir and creat ani contribut you make are greatli appreci fork the project creat your featur branch git checkout b featureamazingfeatur commit your chang git commit m add some amazingfeatur push to the branch git push origin featureamazingfeatur open a pull request licens distribut under the mit licens see licens for more inform contact pleas feel free to contact me if you have ani question damian kli damiankli,data engineering project data engineering project is an implementation of the data pipeline which consumes the latest news from r feed and make them available for user via handy api the pipeline infrastructure is built using popular opensource project access the latest news and headline in one place table of content architecture diagram how it work data scraping data flow data access prerequisite running project testing api service reference contribution license contact architecture diagram how it work data scraping airflow dag is responsible for the execution of python scraping module it run periodically every x minute producing microbatches first task update proxypool using proxy in combination with rotating user agent can help get scraper past most of the antiscraping measure and prevent being detected a a scraper second task extract news from r feed provided in the configuration file validates the quality and sends data into kafka topic a the extraction process is using validated proxy from proxypool data flow kafka connect mongo sink consumes data from kafka topic a and store news in mongodb using upsert functionality based on _id field debezium mongodb source track a mongodb replica set for document change in database and collection recording those change a event in kafka topic b kafka connect elasticsearch sink consumes data from kafka topic b and upserts news in elasticsearch data replicated between topic a and b ensures mongodb and elasticsearch synchronization command query responsibility segregation cqrs pattern allows the use of separate model for updating and reading information kafka connect s3minio sink consumes record from kafka topic b and store them in minio highperformance object storage to ensure data persistency data access data gathered by previous step can be easily accessed in api service using public endpoint prerequisite software required to run the project install docker python 38 pip dockercompose running project script managesh wrapper for dockercompose work a a managing tool build project infrastructure managesh up stop project infrastructure managesh stop delete project infrastructure managesh down testing script run_testssh executes unit test against airflow scraping module and django rest framework application run_testssh api service read detailed documentation on how to interact with data collected by pipeline using search endpoint example search see all news http00005000apiv1news add search_fields title and description see all of the news containing the robert lewandowski phrase http00005000apiv1newssearchrobert20lewandowski find news containing the lewandowski phrase in their title http00005000apiv1newssearchtitlelewandowski see all of the polish news containing the lewandowski phrase http00005000apiv1newssearchlewandowskilanguagepl reference inspired by following code article and video how we launched a data product in 60 day with aws toru jug 55 kafka connect szwajcarski scyzoryk w rkach inyniera mariusz strzelecki kafka elasticsearch sink connector and the power of single message transformation docker tip and trick with kafka connect ksqldb and kafka contribution contribution are what make the opensource community such an amazing place to learn inspire and create any contribution you make are greatly appreciated fork the project create your feature branch git checkout b featureamazingfeature commit your change git commit m add some amazingfeature push to the branch git push origin featureamazingfeature open a pull request license distributed under the mit license see license for more information contact please feel free to contact me if you have any question damian kli damianklis,data engineering project data engineering project implementation data pipeline consumes latest news r feed make available user via handy api pipeline infrastructure built using popular opensource project access latest news headline one place table content architecture diagram work data scraping data flow data access prerequisite running project testing api service reference contribution license contact architecture diagram work data scraping airflow dag responsible execution python scraping module run periodically every x minute producing microbatches first task update proxypool using proxy combination rotating user agent help get scraper past antiscraping measure prevent detected scraper second task extract news r feed provided configuration file validates quality sends data kafka topic extraction process using validated proxy proxypool data flow kafka connect mongo sink consumes data kafka topic store news mongodb using upsert functionality based _id field debezium mongodb source track mongodb replica set document change database collection recording change event kafka topic b kafka connect elasticsearch sink consumes data kafka topic b upserts news elasticsearch data replicated topic b ensures mongodb elasticsearch synchronization command query responsibility segregation cqrs pattern allows use separate model updating reading information kafka connect s3minio sink consumes record kafka topic b store minio highperformance object storage ensure data persistency data access data gathered previous step easily accessed api service using public endpoint prerequisite software required run project install docker python 38 pip dockercompose running project script managesh wrapper dockercompose work managing tool build project infrastructure managesh stop project infrastructure managesh stop delete project infrastructure managesh testing script run_testssh executes unit test airflow scraping module django rest framework application run_testssh api service read detailed documentation interact data collected pipeline using search endpoint example search see news http00005000apiv1news add search_fields title description see news containing robert lewandowski phrase http00005000apiv1newssearchrobert20lewandowski find news containing lewandowski phrase title http00005000apiv1newssearchtitlelewandowski see polish news containing lewandowski phrase http00005000apiv1newssearchlewandowskilanguagepl reference inspired following code article video launched data product 60 day aws toru jug 55 kafka connect szwajcarski scyzoryk w rkach inyniera mariusz strzelecki kafka elasticsearch sink connector power single message transformation docker tip trick kafka connect ksqldb kafka contribution contribution make opensource community amazing place learn inspire create contribution make greatly appreciated fork project create feature branch git checkout b featureamazingfeature commit change git commit add amazingfeature push branch git push origin featureamazingfeature open pull request license distributed mit license see license information contact please feel free contact question damian kli damianklis
Python ,"Data Enginner's Essential Commands

Linux: Link
Python: Link
PySpark: Link
AWS: Link

EKS: Link
EMR: Link
S3: Link


Terraform: Link
Git: Link
Helm: Link

Jupyterhub: Link




Want to contribute?


The commands should not be copy-pasted from any source in bulk.
Only add those commands that you use more frequently but may be unknown to other developers.

Example: pwd, ls e.t.c., are not allowed

Follow the structure and don't forget to embed any reference links either in heading or command description.

Put it inside a directory if applicable
Give a proper heading
Use markdown script for block code or inline code to embed commands


If the command heading is not sufficient to explain the uses, give 1 liner explanation with an example.
I would be happy to accept your pull request even if you add one good command than adding ten not so good commands.



",data enginn essenti command linux link python link pyspark link aw link ek link emr link s3 link terraform link git link helm link jupyterhub link want to contribut the command should not be copypast from ani sourc in bulk onli add those command that you use more frequent but may be unknown to other develop exampl pwd ls etc are not allow follow the structur and dont forget to emb ani refer link either in head or command descript put it insid a directori if applic give a proper head use markdown script for block code or inlin code to emb command if the command head is not suffici to explain the use give 1 liner explan with an exampl i would be happi to accept your pull request even if you add one good command than ad ten not so good command,data enginners essential command linux link python link pyspark link aws link eks link emr link s3 link terraform link git link helm link jupyterhub link want to contribute the command should not be copypasted from any source in bulk only add those command that you use more frequently but may be unknown to other developer example pwd l etc are not allowed follow the structure and dont forget to embed any reference link either in heading or command description put it inside a directory if applicable give a proper heading use markdown script for block code or inline code to embed command if the command heading is not sufficient to explain the us give 1 liner explanation with an example i would be happy to accept your pull request even if you add one good command than adding ten not so good command,data enginners essential command linux link python link pyspark link aws link eks link emr link s3 link terraform link git link helm link jupyterhub link want contribute command copypasted source bulk add command use frequently may unknown developer example pwd l etc allowed follow structure dont forget embed reference link either heading command description put inside directory applicable give proper heading use markdown script block code inline code embed command command heading sufficient explain us give 1 liner explanation example would happy accept pull request even add one good command adding ten good command
Python ,"Data Engineering Projects

Project 1: Data Modeling with Postgres
In this project, we apply Data Modeling with Postgres and build an ETL pipeline using Python. A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Currently, they are collecting data in json format and the analytics team is particularly interested in understanding what songs users are listening to.
Link: Data_Modeling_with_Postgres
Project 2: Data Modeling with Cassandra
In this project, we apply Data Modeling with Cassandra and build an ETL pipeline using Python. We will build a Data Model around our queries that we want to get answers for.
For our use case we want below answers:

Get details of a song that was herad on the music app history during a particular session.
Get songs played by a user during particular session on music app.
Get all users from the music app history who listened to a particular song.

Link : Data_Modeling_with_Apache_Cassandra
Project 3: Data Warehouse
In this project, we apply the Data Warehouse architectures we learnt and build a Data Warehouse on AWS cloud. We build an ETL pipeline to extract and transform data stored in json format in s3 buckets and move the data to Warehouse hosted on Amazon Redshift.
Use Redshift IaC script - Redshift_IaC_README
Link  - Data_Warehouse
Project 4: Data Lake
In this project, we will build a Data Lake on AWS cloud using Spark and AWS EMR cluster. The data lake will serve as a Single Source of Truth for the Analytics Platform. We will write spark jobs to perform ELT operations that picks data from landing zone on S3 and transform and stores data on the S3 processed zone.
Link: Data_Lake
Project 5: Data Pipelines with Airflow
In this project, we will orchestrate our Data Pipeline workflow using an open-source Apache project called Apache Airflow. We will schedule our ETL jobs in Airflow, create project related custom plugins and operators and automate the pipeline execution.
Link:  Airflow_Data_Pipelines
CAPSTONE PROJECT
Udacity provides their own crafted Capstone project with dataset that include data on immigration to the United States, and supplementary datasets that include data on airport codes, U.S. city demographics, and temperature data.
I worked on my own open-ended project. Here is the link - goodreads_etl_pipeline
",data engin project project 1 data model with postgr in thi project we appli data model with postgr and build an etl pipelin use python a startup want to analyz the data theyv been collect on song and user activ on their new music stream app current they are collect data in json format and the analyt team is particularli interest in understand what song user are listen to link data_modeling_with_postgr project 2 data model with cassandra in thi project we appli data model with cassandra and build an etl pipelin use python we will build a data model around our queri that we want to get answer for for our use case we want below answer get detail of a song that wa herad on the music app histori dure a particular session get song play by a user dure particular session on music app get all user from the music app histori who listen to a particular song link data_modeling_with_apache_cassandra project 3 data warehous in thi project we appli the data warehous architectur we learnt and build a data warehous on aw cloud we build an etl pipelin to extract and transform data store in json format in s3 bucket and move the data to warehous host on amazon redshift use redshift iac script redshift_iac_readm link data_warehous project 4 data lake in thi project we will build a data lake on aw cloud use spark and aw emr cluster the data lake will serv as a singl sourc of truth for the analyt platform we will write spark job to perform elt oper that pick data from land zone on s3 and transform and store data on the s3 process zone link data_lak project 5 data pipelin with airflow in thi project we will orchestr our data pipelin workflow use an opensourc apach project call apach airflow we will schedul our etl job in airflow creat project relat custom plugin and oper and autom the pipelin execut link airflow_data_pipelin capston project udac provid their own craft capston project with dataset that includ data on immigr to the unit state and supplementari dataset that includ data on airport code us citi demograph and temperatur data i work on my own openend project here is the link goodreads_etl_pipelin,data engineering project project 1 data modeling with postgres in this project we apply data modeling with postgres and build an etl pipeline using python a startup want to analyze the data theyve been collecting on song and user activity on their new music streaming app currently they are collecting data in json format and the analytics team is particularly interested in understanding what song user are listening to link data_modeling_with_postgres project 2 data modeling with cassandra in this project we apply data modeling with cassandra and build an etl pipeline using python we will build a data model around our query that we want to get answer for for our use case we want below answer get detail of a song that wa herad on the music app history during a particular session get song played by a user during particular session on music app get all user from the music app history who listened to a particular song link data_modeling_with_apache_cassandra project 3 data warehouse in this project we apply the data warehouse architecture we learnt and build a data warehouse on aws cloud we build an etl pipeline to extract and transform data stored in json format in s3 bucket and move the data to warehouse hosted on amazon redshift use redshift iac script redshift_iac_readme link data_warehouse project 4 data lake in this project we will build a data lake on aws cloud using spark and aws emr cluster the data lake will serve a a single source of truth for the analytics platform we will write spark job to perform elt operation that pick data from landing zone on s3 and transform and store data on the s3 processed zone link data_lake project 5 data pipeline with airflow in this project we will orchestrate our data pipeline workflow using an opensource apache project called apache airflow we will schedule our etl job in airflow create project related custom plugins and operator and automate the pipeline execution link airflow_data_pipelines capstone project udacity provides their own crafted capstone project with dataset that include data on immigration to the united state and supplementary datasets that include data on airport code u city demographic and temperature data i worked on my own openended project here is the link goodreads_etl_pipeline,data engineering project project 1 data modeling postgres project apply data modeling postgres build etl pipeline using python startup want analyze data theyve collecting song user activity new music streaming app currently collecting data json format analytics team particularly interested understanding song user listening link data_modeling_with_postgres project 2 data modeling cassandra project apply data modeling cassandra build etl pipeline using python build data model around query want get answer use case want answer get detail song herad music app history particular session get song played user particular session music app get user music app history listened particular song link data_modeling_with_apache_cassandra project 3 data warehouse project apply data warehouse architecture learnt build data warehouse aws cloud build etl pipeline extract transform data stored json format s3 bucket move data warehouse hosted amazon redshift use redshift iac script redshift_iac_readme link data_warehouse project 4 data lake project build data lake aws cloud using spark aws emr cluster data lake serve single source truth analytics platform write spark job perform elt operation pick data landing zone s3 transform store data s3 processed zone link data_lake project 5 data pipeline airflow project orchestrate data pipeline workflow using opensource apache project called apache airflow schedule etl job airflow create project related custom plugins operator automate pipeline execution link airflow_data_pipelines capstone project udacity provides crafted capstone project dataset include data immigration united state supplementary datasets include data airport code u city demographic temperature data worked openended project link goodreads_etl_pipeline
Python ,"NEW LIST 2020 - 2021: Machine-Learning / Deep-Learning / AI -Tutorials
Hi - Thanks for dropping by!

I will be updating this tutorials site on a daily basis adding all relevant topcis, including latest researches papers from internet such as arxiv.org, BIORXIV - Specifically Neuroscience to name a few. 

More importantly the applications of ML/DL/AI into industry areas such as Transportation, Medicine/Healthcare etc. will be something I'll watch with keen interest and would love to share the same with you.

Finally, it is YOUR help I will seek to make it more useful and less boring, so please do suggest/comment/contribute!



Index


deep-learning

UBER | Pyro
Netflix | VectorFlow
PyTorch
tensorflow
theano
keras
caffe
Torch/Lua
MXNET



scikit-learn


statistical-inference-scipy


pandas


matplotlib


numpy


python-data


kaggle-and-business-analyses


spark


mapreduce-python


amazon web services


command lines


misc


notebook-installation


Curated list of Deep Learning / AI blogs


credits


contributing


contact-info


license


deep-learning
IPython Notebook(s) and other programming tools such as Torch/Lua/D lang in demonstrating deep learning functionality.
uber-pyro-probabalistic-tutorials



Additional PyRo tutorials:

pyro-examples/full examples
pyro-examples/Variational Autoencoders
pyro-examples/Bayesian Regression
pyro-examples/Deep Markov Model
pyro-examples/AIR(Attend Infer Repeat)
pyro-examples/Semi-Supervised VE
pyro-examples/GMM
pyro-examples/Gaussian Process
pyro-examples/Bayesian Optimization
Full Pyro Code

netflix-vectorflow-tutorials




MNIST Example, running with Dlang

pytorch-tutorials






Level
Description




Beginners/Zakizhou
Learning the basics of PyTorch from Facebook.


Intermedia/Quanvuong
Learning the intermediate stuff about PyTorch of from Facebook.


Advanced/Chsasank
Learning the advanced stuff about PyTorch of from Facebook.


Learning PyTorch by Examples - Numpy, Tensors and Autograd
At its core, PyTorch provides two main features an n-dimensional Tensor, similar to numpy but can run on GPUs AND automatic differentiation for building and training neural networks.


PyTorch - Getting to know autograd.Variable, Gradient, Neural Network
Here we start with ultimate basics of Tensors, wrap a Tensor with Variable module, play with nn.Module and implement forward and backward function.



tensor-flow-tutorials




Additional TensorFlow tutorials:

pkmital/tensorflow_tutorials
nlintz/TensorFlow-Tutorials
alrojo/tensorflow-tutorial
BinRoot/TensorFlow-Book




Notebook
Description




tsf-basics
Learn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google.


tsf-linear
Implement linear regression in TensorFlow.


tsf-logistic
Implement logistic regression in TensorFlow.


tsf-nn
Implement nearest neighboars in TensorFlow.


tsf-alex
Implement AlexNet in TensorFlow.


tsf-cnn
Implement convolutional neural networks in TensorFlow.


tsf-mlp
Implement multilayer perceptrons in TensorFlow.


tsf-rnn
Implement recurrent neural networks in TensorFlow.


tsf-gpu
Learn about basic multi-GPU computation in TensorFlow.


tsf-gviz
Learn about graph visualization in TensorFlow.


tsf-lviz
Learn about loss visualization in TensorFlow.



tensor-flow-exercises



Notebook
Description




tsf-not-mnist
Learn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow.


tsf-fully-connected
Progressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow.


tsf-regularization
Explore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow.


tsf-convolutions
Create convolutional neural networks in TensorFlow.


tsf-word2vec
Train a skip-gram model over Text8 data in TensorFlow.


tsf-lstm
Train a LSTM character model over Text8 data in TensorFlow.







theano-tutorials



Notebook
Description




theano-intro
Intro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.


theano-scan
Learn scans, a mechanism to perform loops in a Theano graph.


theano-logistic
Implement logistic regression in Theano.


theano-rnn
Implement recurrent neural networks in Theano.


theano-mlp
Implement multilayer perceptrons in Theano.







keras-tutorials



Notebook
Description




keras
Keras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano.


setup
Learn about the tutorial goals and how to set up your Keras environment.


intro-deep-learning-ann
Get an intro to deep learning with Keras and Artificial Neural Networks (ANN).


Perceptrons and Adaline
Implement Peceptron and adaptive linear neurons.


MLP and MNIST Data
Classifying handwritten digits,implement MLP, train and debug ANN


theano
Learn about Theano by working with weights matrices and gradients.


keras-otto
Learn about Keras by looking at the Kaggle Otto challenge.


ann-mnist
Review a simple implementation of ANN for MNIST using Keras.


conv-nets
Learn about Convolutional Neural Networks (CNNs) with Keras.


conv-net-1
Recognize handwritten digits from MNIST using Keras - Part 1.


conv-net-2
Recognize handwritten digits from MNIST using Keras - Part 2.


keras-models
Use pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras.


auto-encoders
Learn about Autoencoders with Keras.


rnn-lstm
Learn about Recurrent Neural Networks (RNNs) with Keras.


lstm-sentence-gen
Learn about RNNs using Long Short Term Memory (LSTM) networks with Keras.


nlp-deep-learning
Learn about NLP using ANN (Artificial Neural Networks.


hyperparamter-tuning
Hyperparamters tuning using keras-wrapper.scikit-learn



deep-learning-misc



Notebook
Description




deep-dream
Caffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images.







scikit-learn
IPython Notebook(s) demonstrating scikit-learn functionality.



Notebook
Description




intro
Intro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.


knn
Implement k-nearest neighbors in scikit-learn.


linear-reg
Implement linear regression in scikit-learn.


svm
Implement support vector machine classifiers with and without kernels in scikit-learn.


random-forest
Implement random forest classifiers and regressors in scikit-learn.


k-means
Implement k-means clustering in scikit-learn.


pca
Implement principal component analysis in scikit-learn.


gmm
Implement Gaussian mixture models in scikit-learn.


validation
Implement validation and model selection in scikit-learn.







statistical-inference-scipy
IPython Notebook(s) demonstrating statistical inference with SciPy functionality.



Notebook
Description




scipy
SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.


effect-size
Explore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States.


sampling
Explore random sampling by analyzing the average weight of men and women in the United States using BRFSS data.


hypothesis
Explore hypothesis testing by analyzing the difference of first-born babies compared with others.







pandas
IPython Notebook(s) demonstrating pandas functionality.



Notebook
Description




pandas
Software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series.


github-data-wrangling
Learn how to load, clean, merge, and feature engineer by analyzing GitHub data from the Viz repo.


Introduction-to-Pandas
Introduction to Pandas.


Introducing-Pandas-Objects
Learn about Pandas objects.


Data Indexing and Selection
Learn about data indexing and selection in Pandas.


Operations-in-Pandas
Learn about operating on data in Pandas.


Missing-Values
Learn about handling missing data in Pandas.


Hierarchical-Indexing
Learn about hierarchical indexing in Pandas.


Concat-And-Append
Learn about combining datasets: concat and append in Pandas.


Merge-and-Join
Learn about combining datasets: merge and join in Pandas.


Aggregation-and-Grouping
Learn about aggregation and grouping in Pandas.


Pivot-Tables
Learn about pivot tables in Pandas.


Working-With-Strings
Learn about vectorized string operations in Pandas.


Working-with-Time-Series
Learn about working with time series in pandas.


Performance-Eval-and-Query
Learn about high-performance Pandas: eval() and query() in Pandas.







matplotlib
IPython Notebook(s) demonstrating matplotlib functionality.



Notebook
Description




matplotlib
Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.


matplotlib-applied
Apply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots.


Introduction-To-Matplotlib
Introduction to Matplotlib.


Simple-Line-Plots
Learn about simple line plots in Matplotlib.


Simple-Scatter-Plots
Learn about simple scatter plots in Matplotlib.


Errorbars.ipynb
Learn about visualizing errors in Matplotlib.


Density-and-Contour-Plots
Learn about density and contour plots in Matplotlib.


Histograms-and-Binnings
Learn about histograms, binnings, and density in Matplotlib.


Customizing-Legends
Learn about customizing plot legends in Matplotlib.


Customizing-Colorbars
Learn about customizing colorbars in Matplotlib.


Multiple-Subplots
Learn about multiple subplots in Matplotlib.


Text-and-Annotation
Learn about text and annotation in Matplotlib.


Customizing-Ticks
Learn about customizing ticks in Matplotlib.


Settings-and-Stylesheets
Learn about customizing Matplotlib: configurations and stylesheets.


Three-Dimensional-Plotting
Learn about three-dimensional plotting in Matplotlib.


Geographic-Data-With-Basemap
Learn about geographic data with basemap in Matplotlib.


Visualization-With-Seaborn
Learn about visualization with Seaborn.







numpy
IPython Notebook(s) demonstrating NumPy functionality.



Notebook
Description




numpy
Adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.


Introduction-to-NumPy
Introduction to NumPy.


Understanding-Data-Types
Learn about data types in Python.


The-Basics-Of-NumPy-Arrays
Learn about the basics of NumPy arrays.


Computation-on-arrays-ufuncs
Learn about computations on NumPy arrays: universal functions.


Computation-on-arrays-aggregates
Learn about aggregations: min, max, and everything in between in NumPy.


Computation-on-arrays-broadcasting
Learn about computation on arrays: broadcasting in NumPy.


Boolean-Arrays-and-Masks
Learn about comparisons, masks, and boolean logic in NumPy.


Fancy-Indexing
Learn about fancy indexing in NumPy.


Sorting
Learn about sorting arrays in NumPy.


Structured-Data-NumPy
Learn about structured data: NumPy's structured arrays.







python-data
IPython Notebook(s) demonstrating Python functionality geared towards data analysis.



Notebook
Description




data structures
Learn Python basics with tuples, lists, dicts, sets.


data structure utilities
Learn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions.


functions
Learn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools.


datetime
Learn how to work with Python dates and times: datetime, strftime, strptime, timedelta.


logging
Learn about Python logging with RotatingFileHandler and TimedRotatingFileHandler.


pdb
Learn how to debug in Python with the interactive source code debugger.


unit tests
Learn how to test in Python with Nose unit tests.







kaggle-and-business-analyses
IPython Notebook(s) used in kaggle competitions and business analyses.



Notebook
Description




titanic
Predict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning.


churn-analysis
Predict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.







spark
IPython Notebook(s) demonstrating spark and HDFS functionality.



Notebook
Description




spark
In-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms.


hdfs
Reliably stores very large files across machines in a large cluster.







mapreduce-python
IPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.



Notebook
Description




mapreduce-python
Runs MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and mrjob config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  Disco is another python-based alternative.







aws
IPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.
Also check out:

SAWS: A Supercharged AWS command line interface (CLI).
Awesome AWS: A curated list of libraries, open source repos, guides, blogs, and other resources.




Notebook
Description




boto
Official AWS SDK for Python.


s3cmd
Interacts with S3 through the command line.


s3distcp
Combines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster.


s3-parallel-put
Uploads multiple files to S3 in parallel.


redshift
Acts as a fast data warehouse built on top of technology from massive parallel processing (MPP).


kinesis
Streams data in real time with the ability to process thousands of data streams per second.


lambda
Runs code in response to events, automatically managing compute resources.







commands
IPython Notebook(s) demonstrating various command lines for Linux, Git, etc.



Notebook
Description




linux
Unix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.


anaconda
Distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment.


ipython notebook
Web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document.


git
Distributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows.


ruby
Used to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages.


jekyll
Simple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server.


pelican
Python-based alternative to Jekyll.


django
High-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include Pyramid, Flask, Tornado, and Bottle.



misc
IPython Notebook(s) demonstrating miscellaneous functionality.



Notebook
Description




regex
Regular expression cheat sheet useful in data wrangling.


algorithmia
Algorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.



notebook-installation
anaconda
Anaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.
Follow instructions to install Anaconda or the more lightweight miniconda.
dev-setup
For detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the dev-setup repo.
running-notebooks
Note: If you intend to learn the hard way (preferred method)then I'd strongly advice to write as much code as you can yourself and not just run pre-written code. If you still want to test it, then do the following:
To view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found here.
$ git clone https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials.git
$ cd Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials
$ jupyter notebook

Notebooks tested with Python 3.7+
curated-list-of-deeplearning-blogs

A Blog From a Human-engineer-being http://www.erogol.com/ (RSS)
Aakash Japi http://aakashjapi.com/ (RSS)
Adit Deshpande https://adeshpande3.github.io/ (RSS)
Advanced Analytics & R http://advanceddataanalytics.net/ (RSS)
Adventures in Data Land http://blog.smola.org (RSS)
Agile Data Science http://blog.sense.io/ (RSS)
Ahmed El Deeb https://medium.com/@D33B (RSS)
Airbnb Data blog http://nerds.airbnb.com/data/ (RSS)
Alex Castrounis | InnoArchiTech http://www.innoarchitech.com/ (RSS)
Alex Perrier http://alexperrier.github.io/ (RSS)
Algobeans | Data Analytics Tutorials & Experiments for the Layman https://algobeans.com (RSS)
Amazon AWS AI Blog https://aws.amazon.com/blogs/ai/ (RSS)
Analytics Vidhya http://www.analyticsvidhya.com/blog/ (RSS)
Analytics and Visualization in Big Data @ Sicara https://blog.sicara.com (RSS)
Andreas Müller http://peekaboo-vision.blogspot.com/ (RSS)
Andrej Karpathy blog http://karpathy.github.io/ (RSS)
Andrew Brooks http://brooksandrew.github.io/simpleblog/ (RSS)
Andrey Kurenkov http://www.andreykurenkov.com/writing/ (RSS)
Anton Lebedevich's Blog http://mabrek.github.io/ (RSS)
Arthur Juliani https://medium.com/@awjuliani (RSS)
Audun M. Øygard http://www.auduno.com/ (RSS)
Avi Singh https://avisingh599.github.io/ (RSS)
Beautiful Data http://beautifuldata.net/ (RSS)
Beckerfuffle http://mdbecker.github.io/ (RSS)
Becoming A Data Scientist http://www.becomingadatascientist.com/ (RSS)
Ben Bolte's Blog http://benjaminbolte.com/ml/ (RSS)
Ben Frederickson http://www.benfrederickson.com/blog/ (RSS)
Berkeley AI Research http://bair.berkeley.edu/blog/ (RSS)
Big-Ish Data http://bigishdata.com/ (RSS)
Blog on neural networks http://yerevann.github.io/ (RSS)
Blogistic RegressionAbout Projects http://d10genes.github.io/blog/ (RSS)
blogR | R tips and tricks from a scientist https://drsimonj.svbtle.com/ (RSS)
Brain of mat kelcey http://matpalm.com/blog/ (RSS)
Brilliantly wrong thoughts on science and programming https://arogozhnikov.github.io/ (RSS)
Bugra Akyildiz http://bugra.github.io/ (RSS)
Building Babylon https://building-babylon.net/ (RSS)
Carl Shan http://carlshan.com/ (RSS)
Chris Stucchio https://www.chrisstucchio.com/blog/index.html (RSS)
Christophe Bourguignat https://medium.com/@chris_bour (RSS)
Christopher Nguyen https://medium.com/@ctn (RSS)
Cloudera Data Science Posts http://blog.cloudera.com/blog/category/data-science/ (RSS)
colah's blog http://colah.github.io/archive.html (RSS)
Cortana Intelligence and Machine Learning Blog https://blogs.technet.microsoft.com/machinelearning/ (RSS)
Daniel Forsyth http://www.danielforsyth.me/ (RSS)
Daniel Homola http://danielhomola.com/category/blog/ (RSS)
Daniel Nee http://danielnee.com (RSS)
Data Based Inventions http://datalab.lu/ (RSS)
Data Blogger https://www.data-blogger.com/ (RSS)
Data Labs http://blog.insightdatalabs.com/ (RSS)
Data Meets Media http://datameetsmedia.com/ (RSS)
Data Miners Blog http://blog.data-miners.com/ (RSS)
Data Mining Research http://www.dataminingblog.com/ (RSS)
Data Mining: Text Mining, Visualization and Social Media http://datamining.typepad.com/data_mining/ (RSS)
Data Piques http://blog.ethanrosenthal.com/ (RSS)
Data School http://www.dataschool.io/ (RSS)
Data Science 101 http://101.datascience.community/ (RSS)
Data Science @ Facebook https://research.facebook.com/blog/datascience/ (RSS)
Data Science Insights http://www.datasciencebowl.com/data-science-insights/ (RSS)
Data Science Tutorials https://codementor.io/data-science/tutorial (RSS)
Data Science Vademecum http://datasciencevademecum.wordpress.com/ (RSS)
Dataaspirant http://dataaspirant.com/ (RSS)
Dataclysm http://blog.okcupid.com/ (RSS)
DataGenetics http://datagenetics.com/blog.html (RSS)
Dataiku https://www.dataiku.com/blog/ (RSS)
DataKind http://www.datakind.org/blog (RSS)
DataLook http://blog.datalook.io/ (RSS)
Datanice https://datanice.wordpress.com/ (RSS)
Dataquest Blog https://www.dataquest.io/blog/ (RSS)
DataRobot http://www.datarobot.com/blog/ (RSS)
Datascope http://datascopeanalytics.com/blog (RSS)
DatasFrame http://tomaugspurger.github.io/ (RSS)
David Mimno http://www.mimno.org/ (RSS)
Dayne Batten http://daynebatten.com (RSS)
Deep Learning http://deeplearning.net/blog/ (RSS)
Deepdish http://deepdish.io/ (RSS)
Delip Rao http://deliprao.com/ (RSS)
DENNY'S BLOG http://blog.dennybritz.com/ (RSS)
Dimensionless https://dimensionless.in/blog/ (RSS)
Distill http://distill.pub/ (RSS)
District Data Labs http://districtdatalabs.silvrback.com/ (RSS)
Diving into data https://blog.datadive.net/ (RSS)
Domino Data Lab's blog http://blog.dominodatalab.com/ (RSS)
Dr. Randal S. Olson http://www.randalolson.com/blog/ (RSS)
Drew Conway https://medium.com/@drewconway (RSS)
Dustin Tran http://dustintran.com/blog/ (RSS)
Eder Santana https://edersantana.github.io/blog.html (RSS)
Edwin Chen http://blog.echen.me (RSS)
EFavDB http://efavdb.com/ (RSS)
Emilio Ferrara, Ph.D.  http://www.emilio.ferrara.name/ (RSS)
Entrepreneurial Geekiness http://ianozsvald.com/ (RSS)
Eric Jonas http://ericjonas.com/archives.html (RSS)
Eric Siegel http://www.predictiveanalyticsworld.com/blog (RSS)
Erik Bern http://erikbern.com (RSS)
ERIN SHELLMAN http://www.erinshellman.com/ (RSS)
Eugenio Culurciello http://culurciello.github.io/ (RSS)
Fabian Pedregosa http://fa.bianp.net/ (RSS)
Fast Forward Labs http://blog.fastforwardlabs.com/ (RSS)
FastML http://fastml.com/ (RSS)
Florian Hartl http://florianhartl.com/ (RSS)
FlowingData http://flowingdata.com/ (RSS)
Full Stack ML http://fullstackml.com/ (RSS)
GAB41 http://www.lab41.org/gab41/ (RSS)
Garbled Notes http://www.chioka.in/ (RSS)
Greg Reda http://www.gregreda.com/blog/ (RSS)
Hyon S Chu https://medium.com/@adailyventure (RSS)
i am trask http://iamtrask.github.io/ (RSS)
I Quant NY http://iquantny.tumblr.com/ (RSS)
inFERENCe http://www.inference.vc/ (RSS)
Insight Data Science https://blog.insightdatascience.com/ (RSS)
INSPIRATION INFORMATION http://myinspirationinformation.com/ (RSS)
Ira Korshunova http://irakorshunova.github.io/ (RSS)
I’m a bandit https://blogs.princeton.edu/imabandit/ (RSS)
Jason Toy http://www.jtoy.net/ (RSS)
Jeremy D. Jackson, PhD http://www.jeremydjacksonphd.com/ (RSS)
Jesse Steinweg-Woods https://jessesw.com/ (RSS)
Joe Cauteruccio http://www.joecjr.com/ (RSS)
John Myles White http://www.johnmyleswhite.com/ (RSS)
John's Soapbox http://joschu.github.io/ (RSS)
Jonas Degrave http://317070.github.io/ (RSS)
Joy Of Data http://www.joyofdata.de/blog/ (RSS)
Julia Evans http://jvns.ca/ (RSS)
KDnuggets http://www.kdnuggets.com/ (RSS)
Keeping Up With The Latest Techniques http://colinpriest.com/ (RSS)
Kenny Bastani http://www.kennybastani.com/ (RSS)
Kevin Davenport http://kldavenport.com/ (RSS)
kevin frans http://kvfrans.com/ (RSS)
korbonits | Math ∩ Data http://korbonits.github.io/ (RSS)
Large Scale Machine Learning  http://bickson.blogspot.com/ (RSS)
LATERAL BLOG https://blog.lateral.io/ (RSS)
Lazy Programmer http://lazyprogrammer.me/ (RSS)
Learn Analytics Here https://learnanalyticshere.wordpress.com/ (RSS)
LearnDataSci http://www.learndatasci.com/ (RSS)
Learning With Data http://learningwithdata.com/ (RSS)
Life, Language, Learning http://daoudclarke.github.io/ (RSS)
Locke Data https://itsalocke.com/blog/ (RSS)
Louis Dorard http://www.louisdorard.com/blog/ (RSS)
M.E.Driscoll http://medriscoll.com/ (RSS)
Machinalis http://www.machinalis.com/blog (RSS)
Machine Learning (Theory) http://hunch.net/ (RSS)
Machine Learning and Data Science http://alexhwoods.com/blog/ (RSS)
Machine Learning https://charlesmartin14.wordpress.com/ (RSS)
Machine Learning Mastery http://machinelearningmastery.com/blog/ (RSS)
Machine Learning Blogs https://machinelearningblogs.com/ (RSS)
Machine Learning, etc http://yaroslavvb.blogspot.com (RSS)
Machine Learning, Maths and Physics https://mlopezm.wordpress.com/ (RSS)
Machine Learning Flashcards https://machinelearningflashcards.com/ $10, but a nicely illustrated set of 300 flash cards
Machined Learnings http://www.machinedlearnings.com/ (RSS)
MAPPING BABEL https://jack-clark.net/ (RSS)
MAPR Blog https://www.mapr.com/blog (RSS)
MAREK REI http://www.marekrei.com/blog/ (RSS)
MARGINALLY INTERESTING http://blog.mikiobraun.de/ (RSS)
Math ∩ Programming http://jeremykun.com/ (RSS)
Matthew Rocklin http://matthewrocklin.com/blog/ (RSS)
Melody Wolk http://melodywolk.com/projects/ (RSS)
Mic Farris http://www.micfarris.com/ (RSS)
Mike Tyka http://mtyka.github.io/ (RSS)
minimaxir | Max Woolf's Blog http://minimaxir.com/ (RSS)
Mirror Image https://mirror2image.wordpress.com/ (RSS)
Mitch Crowe http://www.dataphoric.com/ (RSS)
MLWave http://mlwave.com/ (RSS)
MLWhiz http://mlwhiz.com/ (RSS)
Models are illuminating and wrong https://peadarcoyle.wordpress.com/ (RSS)
Moody Rd http://blog.mrtz.org/ (RSS)
Moonshots http://jxieeducation.com/ (RSS)
Mourad Mourafiq http://mourafiq.com/ (RSS)
My thoughts on Data science, predictive analytics, Python http://shahramabyari.com/ (RSS)
Natural language processing blog http://nlpers.blogspot.fr/ (RSS)
Neil Lawrence http://inverseprobability.com/blog.html (RSS)
NLP and Deep Learning enthusiast http://camron.xyz/ (RSS)
no free hunch http://blog.kaggle.com/ (RSS)
Nuit Blanche http://nuit-blanche.blogspot.com/ (RSS)
Number 2147483647 https://no2147483647.wordpress.com/ (RSS)
On Machine Intelligence https://aimatters.wordpress.com/ (RSS)
Opiate for the masses Data is our religion. http://opiateforthemass.es/ (RSS)
p-value.info http://www.p-value.info/ (RSS)
Pete Warden's blog http://petewarden.com/ (RSS)
Plotly Blog http://blog.plot.ly/ (RSS)
Probably Overthinking It http://allendowney.blogspot.ca/ (RSS)
Prooffreader.com http://www.prooffreader.com (RSS)
ProoffreaderPlus http://prooffreaderplus.blogspot.ca/ (RSS)
Publishable Stuff http://www.sumsar.net/ (RSS)
PyImageSearch http://www.pyimagesearch.com/ (RSS)
Pythonic Perambulations https://jakevdp.github.io/ (RSS)
quintuitive http://quintuitive.com/ (RSS)
R and Data Mining https://rdatamining.wordpress.com/ (RSS)
R-bloggers http://www.r-bloggers.com/ (RSS)
R2RT http://r2rt.com/ (RSS)
Ramiro Gómez http://ramiro.org/notebooks/ (RSS)
Random notes on Computer Science, Mathematics and Software Engineering http://barmaley-exe.github.io/ (RSS)
Randy Zwitch http://randyzwitch.com/ (RSS)
RaRe Technologies http://rare-technologies.com/blog/ (RSS)
Rayli.Net http://rayli.net/blog/ (RSS)
Revolutions http://blog.revolutionanalytics.com/ (RSS)
Rinu Boney http://rinuboney.github.io/ (RSS)
RNDuja Blog http://rnduja.github.io/ (RSS)
Robert Chang https://medium.com/@rchang (RSS)
Rocket-Powered Data Science http://rocketdatascience.org (RSS)
Sachin Joglekar's blog https://codesachin.wordpress.com/ (RSS)
samim https://medium.com/@samim (RSS)
Sean J. Taylor http://seanjtaylor.com/ (RSS)
Sebastian Raschka http://sebastianraschka.com/blog/index.html (RSS)
Sebastian Ruder http://sebastianruder.com/ (RSS)
Sebastian's slow blog http://www.nowozin.net/sebastian/blog/ (RSS)
SFL Scientific Blog https://sflscientific.com/blog/ (RSS)
Shakir's Machine Learning Blog http://blog.shakirm.com/ (RSS)
Simply Statistics http://simplystatistics.org (RSS)
Springboard Blog http://springboard.com/blog
Startup.ML Blog http://startup.ml/blog (RSS)
Statistical Modeling, Causal Inference, and Social Science http://andrewgelman.com/ (RSS)
Stigler Diet http://stiglerdiet.com/ (RSS)
Stitch Fix Tech Blog http://multithreaded.stitchfix.com/blog/ (RSS)
Stochastic R&D Notes http://arseny.info/ (RSS)
Storytelling with Statistics on Quora http://datastories.quora.com/ (RSS)
StreamHacker http://streamhacker.com/ (RSS)
Subconscious Musings http://blogs.sas.com/content/subconsciousmusings/ (RSS)
Swan Intelligence http://swanintelligence.com/ (RSS)
TechnoCalifornia http://technocalifornia.blogspot.se/ (RSS)
TEXT ANALYSIS BLOG | AYLIEN http://blog.aylien.com/ (RSS)
The Angry Statistician http://angrystatistician.blogspot.com/ (RSS)
The Clever Machine https://theclevermachine.wordpress.com/ (RSS)
The Data Camp Blog https://www.datacamp.com/community/blog (RSS)
The Data Incubator http://blog.thedataincubator.com/ (RSS)
The Data Science Lab https://datasciencelab.wordpress.com/ (RSS)
THE ETZ-FILES http://alexanderetz.com/ (RSS)
The Science of Data http://www.martingoodson.com (RSS)
The Shape of Data https://shapeofdata.wordpress.com (RSS)
The unofficial Google data science Blog http://www.unofficialgoogledatascience.com/ (RSS)
Tim Dettmers http://timdettmers.com/ (RSS)
Tombone's Computer Vision Blog http://www.computervisionblog.com/ (RSS)
Tommy Blanchard http://tommyblanchard.com/category/projects (RSS)
Trevor Stephens http://trevorstephens.com/ (RSS)
Trey Causey http://treycausey.com/ (RSS)
UW Data Science Blog http://datasciencedegree.wisconsin.edu/blog/ (RSS)
Wellecks http://wellecks.wordpress.com/ (RSS)
Wes McKinney http://wesmckinney.com/archives.html (RSS)
While My MCMC Gently Samples http://twiecki.github.io/ (RSS)
WildML http://www.wildml.com/ (RSS)
Will do stuff for stuff http://rinzewind.org/blog-en (RSS)
Will wolf http://willwolf.io/ (RSS)
WILL'S NOISE http://www.willmcginnis.com/ (RSS)
William Lyon http://www.lyonwj.com/ (RSS)
Win-Vector Blog http://www.win-vector.com/blog/ (RSS)
Yanir Seroussi http://yanirseroussi.com/ (RSS)
Zac Stewart http://zacstewart.com/ (RSS)
ŷhat http://blog.yhat.com/ (RSS)
ℚuantitative √ourney http://outlace.com/ (RSS)
大トロ http://blog.otoro.net/ (RSS)

credits

Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython by Wes McKinney
PyCon 2015 Scikit-learn Tutorial by Jake VanderPlas
Python Data Science Handbook by Jake VanderPlas
Parallel Machine Learning with scikit-learn and IPython by Olivier Grisel
Statistical Interference Using Computational Methods in Python by Allen Downey
TensorFlow Examples by Aymeric Damien
TensorFlow Tutorials by Parag K Mital
TensorFlow Tutorials by Nathan Lintz
TensorFlow Tutorials by Alexander R Johansen
TensorFlow Book by Nishant Shukla
Summer School 2015 by mila-udem
Keras tutorials by Valerio Maggio
Kaggle
Yhat Blog

contributing
Contributions are welcome!  For bug reports or requests please submit an issue.
contact-info
Feel free to contact me to discuss any issues, questions, or comments.

Email: tarry.singh@gmail.com
Twitter: @tarrysingh
GitHub: tarrysingh
LinkedIn: Tarry Singh
Website: tarrysingh.com
Medium: tarry@Medium
Quora : Answers from Tarry on Quora

license
This repository contains a variety of content; some developed by Tarry Singh and some from third-parties and a lot will be maintained by me. The third-party content is distributed under the license provided by those parties.
The content was originally developed by Donne Martin is distributed under the following license. I will be maintaining and revamping it by adding PyTorch, Torch/Lua, MXNET and much more:
I am providing code and resources in this repository to you under an open source license.
Copyright 2017 Tarry Singh

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",new list 2020 2021 machinelearn deeplearn ai tutori hi thank for drop by i will be updat thi tutori site on a daili basi ad all relev topci includ latest research paper from internet such as arxivorg biorxiv specif neurosci to name a few more importantli the applic of mldlai into industri area such as transport medicinehealthcar etc will be someth ill watch with keen interest and would love to share the same with you final it is your help i will seek to make it more use and less bore so pleas do suggestcommentcontribut index deeplearn uber pyro netflix vectorflow pytorch tensorflow theano kera caff torchlua mxnet scikitlearn statisticalinferencescipi panda matplotlib numpi pythondata kaggleandbusinessanalys spark mapreducepython amazon web servic command line misc notebookinstal curat list of deep learn ai blog credit contribut contactinfo licens deeplearn ipython notebook and other program tool such as torchluad lang in demonstr deep learn function uberpyroprobabalistictutori addit pyro tutori pyroexamplesful exampl pyroexamplesvari autoencod pyroexamplesbayesian regress pyroexamplesdeep markov model pyroexamplesairattend infer repeat pyroexamplessemisupervis ve pyroexamplesgmm pyroexamplesgaussian process pyroexamplesbayesian optim full pyro code netflixvectorflowtutori mnist exampl run with dlang pytorchtutori level descript beginnerszakizh learn the basic of pytorch from facebook intermediaquanvuong learn the intermedi stuff about pytorch of from facebook advancedchsasank learn the advanc stuff about pytorch of from facebook learn pytorch by exampl numpi tensor and autograd at it core pytorch provid two main featur an ndimension tensor similar to numpi but can run on gpu and automat differenti for build and train neural network pytorch get to know autogradvari gradient neural network here we start with ultim basic of tensor wrap a tensor with variabl modul play with nnmodul and implement forward and backward function tensorflowtutori addit tensorflow tutori pkmitaltensorflow_tutori nlintztensorflowtutori alrojotensorflowtutori binroottensorflowbook notebook descript tsfbasic learn basic oper in tensorflow a librari for variou kind of perceptu and languag understand task from googl tsflinear implement linear regress in tensorflow tsflogist implement logist regress in tensorflow tsfnn implement nearest neighboar in tensorflow tsfalex implement alexnet in tensorflow tsfcnn implement convolut neural network in tensorflow tsfmlp implement multilay perceptron in tensorflow tsfrnn implement recurr neural network in tensorflow tsfgpu learn about basic multigpu comput in tensorflow tsfgviz learn about graph visual in tensorflow tsflviz learn about loss visual in tensorflow tensorflowexercis notebook descript tsfnotmnist learn simpl data curat by creat a pickl with format dataset for train develop and test in tensorflow tsffullyconnect progress train deeper and more accur model use logist regress and neural network in tensorflow tsfregular explor regular techniqu by train fulli connect network to classifi notmnist charact in tensorflow tsfconvolut creat convolut neural network in tensorflow tsfword2vec train a skipgram model over text8 data in tensorflow tsflstm train a lstm charact model over text8 data in tensorflow theanotutori notebook descript theanointro intro to theano which allow you to defin optim and evalu mathemat express involv multidimension array effici it can use gpu and perform effici symbol differenti theanoscan learn scan a mechan to perform loop in a theano graph theanologist implement logist regress in theano theanornn implement recurr neural network in theano theanomlp implement multilay perceptron in theano kerastutori notebook descript kera kera is an open sourc neural network librari written in python it is capabl of run on top of either tensorflow or theano setup learn about the tutori goal and how to set up your kera environ introdeeplearningann get an intro to deep learn with kera and artifici neural network ann perceptron and adalin implement peceptron and adapt linear neuron mlp and mnist data classifi handwritten digitsimpl mlp train and debug ann theano learn about theano by work with weight matric and gradient kerasotto learn about kera by look at the kaggl otto challeng annmnist review a simpl implement of ann for mnist use kera convnet learn about convolut neural network cnn with kera convnet1 recogn handwritten digit from mnist use kera part 1 convnet2 recogn handwritten digit from mnist use kera part 2 kerasmodel use pretrain model such as vgg16 vgg19 resnet50 and incept v3 with kera autoencod learn about autoencod with kera rnnlstm learn about recurr neural network rnn with kera lstmsentencegen learn about rnn use long short term memori lstm network with kera nlpdeeplearn learn about nlp use ann artifici neural network hyperparamtertun hyperparamt tune use keraswrapperscikitlearn deeplearningmisc notebook descript deepdream caffebas comput vision program which use a convolut neural network to find and enhanc pattern in imag scikitlearn ipython notebook demonstr scikitlearn function notebook descript intro intro notebook to scikitlearn scikitlearn add python support for larg multidimension array and matric along with a larg librari of highlevel mathemat function to oper on these array knn implement knearest neighbor in scikitlearn linearreg implement linear regress in scikitlearn svm implement support vector machin classifi with and without kernel in scikitlearn randomforest implement random forest classifi and regressor in scikitlearn kmean implement kmean cluster in scikitlearn pca implement princip compon analysi in scikitlearn gmm implement gaussian mixtur model in scikitlearn valid implement valid and model select in scikitlearn statisticalinferencescipi ipython notebook demonstr statist infer with scipi function notebook descript scipi scipi is a collect of mathemat algorithm and conveni function built on the numpi extens of python it add signific power to the interact python session by provid the user with highlevel command and class for manipul and visual data effects explor statist that quantifi effect size by analyz the differ in height between men and women use data from the behavior risk factor surveil system brfss to estim the mean and standard deviat of height for adult women and men in the unit state sampl explor random sampl by analyz the averag weight of men and women in the unit state use brfss data hypothesi explor hypothesi test by analyz the differ of firstborn babi compar with other panda ipython notebook demonstr panda function notebook descript panda softwar librari written for data manipul and analysi in python offer data structur and oper for manipul numer tabl and time seri githubdatawrangl learn how to load clean merg and featur engin by analyz github data from the viz repo introductiontopanda introduct to panda introducingpandasobject learn about panda object data index and select learn about data index and select in panda operationsinpanda learn about oper on data in panda missingvalu learn about handl miss data in panda hierarchicalindex learn about hierarch index in panda concatandappend learn about combin dataset concat and append in panda mergeandjoin learn about combin dataset merg and join in panda aggregationandgroup learn about aggreg and group in panda pivott learn about pivot tabl in panda workingwithstr learn about vector string oper in panda workingwithtimeseri learn about work with time seri in panda performanceevalandqueri learn about highperform panda eval and queri in panda matplotlib ipython notebook demonstr matplotlib function notebook descript matplotlib python 2d plot librari which produc public qualiti figur in a varieti of hardcopi format and interact environ across platform matplotlibappli appli matplotlib visual to kaggl competit for exploratori data analysi learn how to creat bar plot histogram subplot2grid normal plot scatter plot subplot and kernel densiti estim plot introductiontomatplotlib introduct to matplotlib simplelineplot learn about simpl line plot in matplotlib simplescatterplot learn about simpl scatter plot in matplotlib errorbarsipynb learn about visual error in matplotlib densityandcontourplot learn about densiti and contour plot in matplotlib histogramsandbin learn about histogram bin and densiti in matplotlib customizinglegend learn about custom plot legend in matplotlib customizingcolorbar learn about custom colorbar in matplotlib multiplesubplot learn about multipl subplot in matplotlib textandannot learn about text and annot in matplotlib customizingtick learn about custom tick in matplotlib settingsandstylesheet learn about custom matplotlib configur and stylesheet threedimensionalplot learn about threedimension plot in matplotlib geographicdatawithbasemap learn about geograph data with basemap in matplotlib visualizationwithseaborn learn about visual with seaborn numpi ipython notebook demonstr numpi function notebook descript numpi add python support for larg multidimension array and matric along with a larg librari of highlevel mathemat function to oper on these array introductiontonumpi introduct to numpi understandingdatatyp learn about data type in python thebasicsofnumpyarray learn about the basic of numpi array computationonarraysufunc learn about comput on numpi array univers function computationonarraysaggreg learn about aggreg min max and everyth in between in numpi computationonarraysbroadcast learn about comput on array broadcast in numpi booleanarraysandmask learn about comparison mask and boolean logic in numpi fancyindex learn about fanci index in numpi sort learn about sort array in numpi structureddatanumpi learn about structur data numpi structur array pythondata ipython notebook demonstr python function gear toward data analysi notebook descript data structur learn python basic with tupl list dict set data structur util learn python oper such as slice rang xrang bisect sort sort revers enumer zip list comprehens function learn about more advanc python featur function as object lambda function closur arg kwarg curri gener gener express itertool datetim learn how to work with python date and time datetim strftime strptime timedelta log learn about python log with rotatingfilehandl and timedrotatingfilehandl pdb learn how to debug in python with the interact sourc code debugg unit test learn how to test in python with nose unit test kaggleandbusinessanalys ipython notebook use in kaggl competit and busi analys notebook descript titan predict surviv on the titan learn data clean exploratori data analysi and machin learn churnanalysi predict custom churn exercis logist regress gradient boost classif support vector machin random forest and knearestneighbor includ discuss of confus matric roc plot featur import predict probabl and calibrationdescrimin spark ipython notebook demonstr spark and hdf function notebook descript spark inmemori cluster comput framework up to 100 time faster for certain applic and is well suit for machin learn algorithm hdf reliabl store veri larg file across machin in a larg cluster mapreducepython ipython notebook demonstr hadoop mapreduc with mrjob function notebook descript mapreducepython run mapreduc job in python execut job local or on hadoop cluster demonstr hadoop stream in python code with unit test and mrjob config file to analyz amazon s3 bucket log on elast mapreduc disco is anoth pythonbas altern aw ipython notebook demonstr amazon web servic aw and aw tool function also check out saw a supercharg aw command line interfac cli awesom aw a curat list of librari open sourc repo guid blog and other resourc notebook descript boto offici aw sdk for python s3cmd interact with s3 through the command line s3distcp combin smaller file and aggreg them togeth by take in a pattern and target file s3distcp can also be use to transfer larg volum of data from s3 to your hadoop cluster s3parallelput upload multipl file to s3 in parallel redshift act as a fast data warehous built on top of technolog from massiv parallel process mpp kinesi stream data in real time with the abil to process thousand of data stream per second lambda run code in respons to event automat manag comput resourc command ipython notebook demonstr variou command line for linux git etc notebook descript linux unixlik and mostli posixcompli comput oper system disk usag split file grep sed curl view run process termin syntax highlight and vim anaconda distribut of the python program languag for largescal data process predict analyt and scientif comput that aim to simplifi packag manag and deploy ipython notebook webbas interact comput environ where you can combin code execut text mathemat plot and rich media into a singl document git distribut revis control system with an emphasi on speed data integr and support for distribut nonlinear workflow rubi use to interact with the aw command line and for jekyl a blog framework that can be host on github page jekyl simpl blogawar static site gener for person project or organ site render markdown or textil and liquid templat and produc a complet static websit readi to be serv by apach http server nginx or anoth web server pelican pythonbas altern to jekyl django highlevel python web framework that encourag rapid develop and clean pragmat design it can be use to share reportsanalys and for blog lighterweight altern includ pyramid flask tornado and bottl misc ipython notebook demonstr miscellan function notebook descript regex regular express cheat sheet use in data wrangl algorithmia algorithmia is a marketplac for algorithm thi notebook showcas 4 differ algorithm face detect content summar latent dirichlet alloc and optic charact recognit notebookinstal anaconda anaconda is a free distribut of the python program languag for largescal data process predict analyt and scientif comput that aim to simplifi packag manag and deploy follow instruct to instal anaconda or the more lightweight miniconda devsetup for detail instruct script and tool to set up your develop environ for data analysi check out the devsetup repo runningnotebook note if you intend to learn the hard way prefer methodthen id strongli advic to write as much code as you can yourself and not just run prewritten code if you still want to test it then do the follow to view interact content or to modifi element within the ipython notebook you must first clone or download the repositori then run the notebook more inform on ipython notebook can be found here git clone httpsgithubcomtarrysinghartificialintelligencedeeplearningmachinelearningtutorialsgit cd artificialintelligencedeeplearningmachinelearningtutori jupyt notebook notebook test with python 37 curatedlistofdeeplearningblog a blog from a humanengineerb httpwwwerogolcom rss aakash japi httpaakashjapicom rss adit deshpand httpsadeshpande3githubio rss advanc analyt r httpadvanceddataanalyticsnet rss adventur in data land httpblogsmolaorg rss agil data scienc httpblogsenseio rss ahm el deeb httpsmediumcomd33b rss airbnb data blog httpnerdsairbnbcomdata rss alex castrouni innoarchitech httpwwwinnoarchitechcom rss alex perrier httpalexperriergithubio rss algobean data analyt tutori experi for the layman httpsalgobeanscom rss amazon aw ai blog httpsawsamazoncomblogsai rss analyt vidhya httpwwwanalyticsvidhyacomblog rss analyt and visual in big data sicara httpsblogsicaracom rss andrea mller httppeekaboovisionblogspotcom rss andrej karpathi blog httpkarpathygithubio rss andrew brook httpbrooksandrewgithubiosimpleblog rss andrey kurenkov httpwwwandreykurenkovcomwrit rss anton lebedevich blog httpmabrekgithubio rss arthur juliani httpsmediumcomawjuliani rss audun m ygard httpwwwaudunocom rss avi singh httpsavisingh599githubio rss beauti data httpbeautifuldatanet rss beckerfuffl httpmdbeckergithubio rss becom a data scientist httpwwwbecomingadatascientistcom rss ben bolt blog httpbenjaminboltecomml rss ben frederickson httpwwwbenfredericksoncomblog rss berkeley ai research httpbairberkeleyedublog rss bigish data httpbigishdatacom rss blog on neural network httpyerevanngithubio rss blogist regressionabout project httpd10genesgithubioblog rss blogr r tip and trick from a scientist httpsdrsimonjsvbtlecom rss brain of mat kelcey httpmatpalmcomblog rss brilliantli wrong thought on scienc and program httpsarogozhnikovgithubio rss bugra akyildiz httpbugragithubio rss build babylon httpsbuildingbabylonnet rss carl shan httpcarlshancom rss chri stucchio httpswwwchrisstucchiocomblogindexhtml rss christoph bourguignat httpsmediumcomchris_bour rss christoph nguyen httpsmediumcomctn rss cloudera data scienc post httpblogclouderacomblogcategorydatasci rss colah blog httpcolahgithubioarchivehtml rss cortana intellig and machin learn blog httpsblogstechnetmicrosoftcommachinelearn rss daniel forsyth httpwwwdanielforsythm rss daniel homola httpdanielhomolacomcategoryblog rss daniel nee httpdanielneecom rss data base invent httpdatalablu rss data blogger httpswwwdatabloggercom rss data lab httpbloginsightdatalabscom rss data meet media httpdatameetsmediacom rss data miner blog httpblogdataminerscom rss data mine research httpwwwdataminingblogcom rss data mine text mine visual and social media httpdataminingtypepadcomdata_min rss data piqu httpblogethanrosenthalcom rss data school httpwwwdataschoolio rss data scienc 101 http101datasciencecommun rss data scienc facebook httpsresearchfacebookcomblogdatasci rss data scienc insight httpwwwdatasciencebowlcomdatascienceinsight rss data scienc tutori httpscodementoriodatasciencetutori rss data scienc vademecum httpdatasciencevademecumwordpresscom rss dataaspir httpdataaspirantcom rss dataclysm httpblogokcupidcom rss datagenet httpdatageneticscombloghtml rss dataiku httpswwwdataikucomblog rss datakind httpwwwdatakindorgblog rss datalook httpblogdatalookio rss datanic httpsdatanicewordpresscom rss dataquest blog httpswwwdataquestioblog rss datarobot httpwwwdatarobotcomblog rss datascop httpdatascopeanalyticscomblog rss datasfram httptomaugspurgergithubio rss david mimno httpwwwmimnoorg rss dayn batten httpdaynebattencom rss deep learn httpdeeplearningnetblog rss deepdish httpdeepdishio rss delip rao httpdelipraocom rss denni blog httpblogdennybritzcom rss dimensionless httpsdimensionlessinblog rss distil httpdistillpub rss district data lab httpdistrictdatalabssilvrbackcom rss dive into data httpsblogdatadivenet rss domino data lab blog httpblogdominodatalabcom rss dr randal s olson httpwwwrandalolsoncomblog rss drew conway httpsmediumcomdrewconway rss dustin tran httpdustintrancomblog rss eder santana httpsedersantanagithubiobloghtml rss edwin chen httpblogechenm rss efavdb httpefavdbcom rss emilio ferrara phd httpwwwemilioferraranam rss entrepreneuri geeki httpianozsvaldcom rss eric jona httpericjonascomarchiveshtml rss eric siegel httpwwwpredictiveanalyticsworldcomblog rss erik bern httperikberncom rss erin shellman httpwwwerinshellmancom rss eugenio culurciello httpculurciellogithubio rss fabian pedregosa httpfabianpnet rss fast forward lab httpblogfastforwardlabscom rss fastml httpfastmlcom rss florian hartl httpflorianhartlcom rss flowingdata httpflowingdatacom rss full stack ml httpfullstackmlcom rss gab41 httpwwwlab41orggab41 rss garbl note httpwwwchiokain rss greg reda httpwwwgregredacomblog rss hyon s chu httpsmediumcomadailyventur rss i am trask httpiamtraskgithubio rss i quant ny httpiquantnytumblrcom rss infer httpwwwinferencevc rss insight data scienc httpsbloginsightdatasciencecom rss inspir inform httpmyinspirationinformationcom rss ira korshunova httpirakorshunovagithubio rss im a bandit httpsblogsprincetoneduimabandit rss jason toy httpwwwjtoynet rss jeremi d jackson phd httpwwwjeremydjacksonphdcom rss jess steinwegwood httpsjesseswcom rss joe cauteruccio httpwwwjoecjrcom rss john myle white httpwwwjohnmyleswhitecom rss john soapbox httpjoschugithubio rss jona degrav http317070githubio rss joy of data httpwwwjoyofdatadeblog rss julia evan httpjvnsca rss kdnugget httpwwwkdnuggetscom rss keep up with the latest techniqu httpcolinpriestcom rss kenni bastani httpwwwkennybastanicom rss kevin davenport httpkldavenportcom rss kevin fran httpkvfranscom rss korbonit math data httpkorbonitsgithubio rss larg scale machin learn httpbicksonblogspotcom rss later blog httpsbloglateralio rss lazi programm httplazyprogrammerm rss learn analyt here httpslearnanalyticsherewordpresscom rss learndatasci httpwwwlearndatascicom rss learn with data httplearningwithdatacom rss life languag learn httpdaoudclarkegithubio rss lock data httpsitsalockecomblog rss loui dorard httpwwwlouisdorardcomblog rss medriscol httpmedriscollcom rss machinali httpwwwmachinaliscomblog rss machin learn theori httphunchnet rss machin learn and data scienc httpalexhwoodscomblog rss machin learn httpscharlesmartin14wordpresscom rss machin learn masteri httpmachinelearningmasterycomblog rss machin learn blog httpsmachinelearningblogscom rss machin learn etc httpyaroslavvbblogspotcom rss machin learn math and physic httpsmlopezmwordpresscom rss machin learn flashcard httpsmachinelearningflashcardscom 10 but a nice illustr set of 300 flash card machin learn httpwwwmachinedlearningscom rss map babel httpsjackclarknet rss mapr blog httpswwwmaprcomblog rss marek rei httpwwwmarekreicomblog rss margin interest httpblogmikiobraund rss math program httpjeremykuncom rss matthew rocklin httpmatthewrocklincomblog rss melodi wolk httpmelodywolkcomproject rss mic farri httpwwwmicfarriscom rss mike tyka httpmtykagithubio rss minimaxir max woolf blog httpminimaxircom rss mirror imag httpsmirror2imagewordpresscom rss mitch crow httpwwwdataphoriccom rss mlwave httpmlwavecom rss mlwhiz httpmlwhizcom rss model are illumin and wrong httpspeadarcoylewordpresscom rss moodi rd httpblogmrtzorg rss moonshot httpjxieeducationcom rss mourad mourafiq httpmourafiqcom rss my thought on data scienc predict analyt python httpshahramabyaricom rss natur languag process blog httpnlpersblogspotfr rss neil lawrenc httpinverseprobabilitycombloghtml rss nlp and deep learn enthusiast httpcamronxyz rss no free hunch httpblogkagglecom rss nuit blanch httpnuitblancheblogspotcom rss number 2147483647 httpsno2147483647wordpresscom rss on machin intellig httpsaimatterswordpresscom rss opiat for the mass data is our religion httpopiateforthemass rss pvalueinfo httpwwwpvalueinfo rss pete warden blog httppetewardencom rss plotli blog httpblogplotli rss probabl overthink it httpallendowneyblogspotca rss prooffreadercom httpwwwprooffreadercom rss prooffreaderplu httpprooffreaderplusblogspotca rss publish stuff httpwwwsumsarnet rss pyimagesearch httpwwwpyimagesearchcom rss python perambul httpsjakevdpgithubio rss quintuit httpquintuitivecom rss r and data mine httpsrdataminingwordpresscom rss rblogger httpwwwrbloggerscom rss r2rt httpr2rtcom rss ramiro gmez httpramiroorgnotebook rss random note on comput scienc mathemat and softwar engin httpbarmaleyexegithubio rss randi zwitch httprandyzwitchcom rss rare technolog httpraretechnologiescomblog rss raylinet httpraylinetblog rss revolut httpblogrevolutionanalyticscom rss rinu boney httprinuboneygithubio rss rnduja blog httprndujagithubio rss robert chang httpsmediumcomrchang rss rocketpow data scienc httprocketdatascienceorg rss sachin joglekar blog httpscodesachinwordpresscom rss samim httpsmediumcomsamim rss sean j taylor httpseanjtaylorcom rss sebastian raschka httpsebastianraschkacomblogindexhtml rss sebastian ruder httpsebastianrudercom rss sebastian slow blog httpwwwnowozinnetsebastianblog rss sfl scientif blog httpssflscientificcomblog rss shakir machin learn blog httpblogshakirmcom rss simpli statist httpsimplystatisticsorg rss springboard blog httpspringboardcomblog startupml blog httpstartupmlblog rss statist model causal infer and social scienc httpandrewgelmancom rss stigler diet httpstiglerdietcom rss stitch fix tech blog httpmultithreadedstitchfixcomblog rss stochast rd note httparsenyinfo rss storytel with statist on quora httpdatastoriesquoracom rss streamhack httpstreamhackercom rss subconsci muse httpblogssascomcontentsubconsciousmus rss swan intellig httpswanintelligencecom rss technocalifornia httptechnocaliforniablogspots rss text analysi blog aylien httpblogayliencom rss the angri statistician httpangrystatisticianblogspotcom rss the clever machin httpstheclevermachinewordpresscom rss the data camp blog httpswwwdatacampcomcommunityblog rss the data incub httpblogthedataincubatorcom rss the data scienc lab httpsdatasciencelabwordpresscom rss the etzfil httpalexanderetzcom rss the scienc of data httpwwwmartingoodsoncom rss the shape of data httpsshapeofdatawordpresscom rss the unoffici googl data scienc blog httpwwwunofficialgoogledatasciencecom rss tim dettmer httptimdettmerscom rss tombon comput vision blog httpwwwcomputervisionblogcom rss tommi blanchard httptommyblanchardcomcategoryproject rss trevor stephen httptrevorstephenscom rss trey causey httptreycauseycom rss uw data scienc blog httpdatasciencedegreewisconsinedublog rss welleck httpwelleckswordpresscom rss we mckinney httpwesmckinneycomarchiveshtml rss while my mcmc gentli sampl httptwieckigithubio rss wildml httpwwwwildmlcom rss will do stuff for stuff httprinzewindorgblogen rss will wolf httpwillwolfio rss will nois httpwwwwillmcginniscom rss william lyon httpwwwlyonwjcom rss winvector blog httpwwwwinvectorcomblog rss yanir seroussi httpyanirseroussicom rss zac stewart httpzacstewartcom rss hat httpblogyhatcom rss quantit ourney httpoutlacecom rss httpblogotoronet rss credit python for data analysi data wrangl with panda numpi and ipython by we mckinney pycon 2015 scikitlearn tutori by jake vanderpla python data scienc handbook by jake vanderpla parallel machin learn with scikitlearn and ipython by olivi grisel statist interfer use comput method in python by allen downey tensorflow exampl by aymer damien tensorflow tutori by parag k mital tensorflow tutori by nathan lintz tensorflow tutori by alexand r johansen tensorflow book by nishant shukla summer school 2015 by milaudem kera tutori by valerio maggio kaggl yhat blog contribut contribut are welcom for bug report or request pleas submit an issu contactinfo feel free to contact me to discuss ani issu question or comment email tarrysinghgmailcom twitter tarrysingh github tarrysingh linkedin tarri singh websit tarrysinghcom medium tarrymedium quora answer from tarri on quora licens thi repositori contain a varieti of content some develop by tarri singh and some from thirdparti and a lot will be maintain by me the thirdparti content is distribut under the licens provid by those parti the content wa origin develop by donn martin is distribut under the follow licens i will be maintain and revamp it by ad pytorch torchlua mxnet and much more i am provid code and resourc in thi repositori to you under an open sourc licens copyright 2017 tarri singh licens under the apach licens version 20 the licens you may not use thi file except in complianc with the licens you may obtain a copi of the licens at httpwwwapacheorglicenseslicense20 unless requir by applic law or agre to in write softwar distribut under the licens is distribut on an as is basi without warranti or condit of ani kind either express or impli see the licens for the specif languag govern permiss and limit under the licens,new list 2020 2021 machinelearning deeplearning ai tutorial hi thanks for dropping by i will be updating this tutorial site on a daily basis adding all relevant topcis including latest research paper from internet such a arxivorg biorxiv specifically neuroscience to name a few more importantly the application of mldlai into industry area such a transportation medicinehealthcare etc will be something ill watch with keen interest and would love to share the same with you finally it is your help i will seek to make it more useful and le boring so please do suggestcommentcontribute index deeplearning uber pyro netflix vectorflow pytorch tensorflow theano kera caffe torchlua mxnet scikitlearn statisticalinferencescipy panda matplotlib numpy pythondata kaggleandbusinessanalyses spark mapreducepython amazon web service command line misc notebookinstallation curated list of deep learning ai blog credit contributing contactinfo license deeplearning ipython notebook and other programming tool such a torchluad lang in demonstrating deep learning functionality uberpyroprobabalistictutorials additional pyro tutorial pyroexamplesfull example pyroexamplesvariational autoencoders pyroexamplesbayesian regression pyroexamplesdeep markov model pyroexamplesairattend infer repeat pyroexamplessemisupervised ve pyroexamplesgmm pyroexamplesgaussian process pyroexamplesbayesian optimization full pyro code netflixvectorflowtutorials mnist example running with dlang pytorchtutorials level description beginnerszakizhou learning the basic of pytorch from facebook intermediaquanvuong learning the intermediate stuff about pytorch of from facebook advancedchsasank learning the advanced stuff about pytorch of from facebook learning pytorch by example numpy tensor and autograd at it core pytorch provides two main feature an ndimensional tensor similar to numpy but can run on gpus and automatic differentiation for building and training neural network pytorch getting to know autogradvariable gradient neural network here we start with ultimate basic of tensor wrap a tensor with variable module play with nnmodule and implement forward and backward function tensorflowtutorials additional tensorflow tutorial pkmitaltensorflow_tutorials nlintztensorflowtutorials alrojotensorflowtutorial binroottensorflowbook notebook description tsfbasics learn basic operation in tensorflow a library for various kind of perceptual and language understanding task from google tsflinear implement linear regression in tensorflow tsflogistic implement logistic regression in tensorflow tsfnn implement nearest neighboars in tensorflow tsfalex implement alexnet in tensorflow tsfcnn implement convolutional neural network in tensorflow tsfmlp implement multilayer perceptrons in tensorflow tsfrnn implement recurrent neural network in tensorflow tsfgpu learn about basic multigpu computation in tensorflow tsfgviz learn about graph visualization in tensorflow tsflviz learn about loss visualization in tensorflow tensorflowexercises notebook description tsfnotmnist learn simple data curation by creating a pickle with formatted datasets for training development and testing in tensorflow tsffullyconnected progressively train deeper and more accurate model using logistic regression and neural network in tensorflow tsfregularization explore regularization technique by training fully connected network to classify notmnist character in tensorflow tsfconvolutions create convolutional neural network in tensorflow tsfword2vec train a skipgram model over text8 data in tensorflow tsflstm train a lstm character model over text8 data in tensorflow theanotutorials notebook description theanointro intro to theano which allows you to define optimize and evaluate mathematical expression involving multidimensional array efficiently it can use gpus and perform efficient symbolic differentiation theanoscan learn scan a mechanism to perform loop in a theano graph theanologistic implement logistic regression in theano theanornn implement recurrent neural network in theano theanomlp implement multilayer perceptrons in theano kerastutorials notebook description kera kera is an open source neural network library written in python it is capable of running on top of either tensorflow or theano setup learn about the tutorial goal and how to set up your kera environment introdeeplearningann get an intro to deep learning with kera and artificial neural network ann perceptrons and adaline implement peceptron and adaptive linear neuron mlp and mnist data classifying handwritten digitsimplement mlp train and debug ann theano learn about theano by working with weight matrix and gradient kerasotto learn about kera by looking at the kaggle otto challenge annmnist review a simple implementation of ann for mnist using kera convnets learn about convolutional neural network cnns with kera convnet1 recognize handwritten digit from mnist using kera part 1 convnet2 recognize handwritten digit from mnist using kera part 2 kerasmodels use pretrained model such a vgg16 vgg19 resnet50 and inception v3 with kera autoencoders learn about autoencoders with kera rnnlstm learn about recurrent neural network rnns with kera lstmsentencegen learn about rnns using long short term memory lstm network with kera nlpdeeplearning learn about nlp using ann artificial neural network hyperparamtertuning hyperparamters tuning using keraswrapperscikitlearn deeplearningmisc notebook description deepdream caffebased computer vision program which us a convolutional neural network to find and enhance pattern in image scikitlearn ipython notebook demonstrating scikitlearn functionality notebook description intro intro notebook to scikitlearn scikitlearn add python support for large multidimensional array and matrix along with a large library of highlevel mathematical function to operate on these array knn implement knearest neighbor in scikitlearn linearreg implement linear regression in scikitlearn svm implement support vector machine classifier with and without kernel in scikitlearn randomforest implement random forest classifier and regressors in scikitlearn kmeans implement kmeans clustering in scikitlearn pca implement principal component analysis in scikitlearn gmm implement gaussian mixture model in scikitlearn validation implement validation and model selection in scikitlearn statisticalinferencescipy ipython notebook demonstrating statistical inference with scipy functionality notebook description scipy scipy is a collection of mathematical algorithm and convenience function built on the numpy extension of python it add significant power to the interactive python session by providing the user with highlevel command and class for manipulating and visualizing data effectsize explore statistic that quantify effect size by analyzing the difference in height between men and woman us data from the behavioral risk factor surveillance system brfss to estimate the mean and standard deviation of height for adult woman and men in the united state sampling explore random sampling by analyzing the average weight of men and woman in the united state using brfss data hypothesis explore hypothesis testing by analyzing the difference of firstborn baby compared with others panda ipython notebook demonstrating panda functionality notebook description panda software library written for data manipulation and analysis in python offer data structure and operation for manipulating numerical table and time series githubdatawrangling learn how to load clean merge and feature engineer by analyzing github data from the viz repo introductiontopandas introduction to panda introducingpandasobjects learn about panda object data indexing and selection learn about data indexing and selection in panda operationsinpandas learn about operating on data in panda missingvalues learn about handling missing data in panda hierarchicalindexing learn about hierarchical indexing in panda concatandappend learn about combining datasets concat and append in panda mergeandjoin learn about combining datasets merge and join in panda aggregationandgrouping learn about aggregation and grouping in panda pivottables learn about pivot table in panda workingwithstrings learn about vectorized string operation in panda workingwithtimeseries learn about working with time series in panda performanceevalandquery learn about highperformance panda eval and query in panda matplotlib ipython notebook demonstrating matplotlib functionality notebook description matplotlib python 2d plotting library which produce publication quality figure in a variety of hardcopy format and interactive environment across platform matplotlibapplied apply matplotlib visualization to kaggle competition for exploratory data analysis learn how to create bar plot histogram subplot2grid normalized plot scatter plot subplots and kernel density estimation plot introductiontomatplotlib introduction to matplotlib simplelineplots learn about simple line plot in matplotlib simplescatterplots learn about simple scatter plot in matplotlib errorbarsipynb learn about visualizing error in matplotlib densityandcontourplots learn about density and contour plot in matplotlib histogramsandbinnings learn about histogram binnings and density in matplotlib customizinglegends learn about customizing plot legend in matplotlib customizingcolorbars learn about customizing colorbars in matplotlib multiplesubplots learn about multiple subplots in matplotlib textandannotation learn about text and annotation in matplotlib customizingticks learn about customizing tick in matplotlib settingsandstylesheets learn about customizing matplotlib configuration and stylesheets threedimensionalplotting learn about threedimensional plotting in matplotlib geographicdatawithbasemap learn about geographic data with basemap in matplotlib visualizationwithseaborn learn about visualization with seaborn numpy ipython notebook demonstrating numpy functionality notebook description numpy add python support for large multidimensional array and matrix along with a large library of highlevel mathematical function to operate on these array introductiontonumpy introduction to numpy understandingdatatypes learn about data type in python thebasicsofnumpyarrays learn about the basic of numpy array computationonarraysufuncs learn about computation on numpy array universal function computationonarraysaggregates learn about aggregation min max and everything in between in numpy computationonarraysbroadcasting learn about computation on array broadcasting in numpy booleanarraysandmasks learn about comparison mask and boolean logic in numpy fancyindexing learn about fancy indexing in numpy sorting learn about sorting array in numpy structureddatanumpy learn about structured data numpys structured array pythondata ipython notebook demonstrating python functionality geared towards data analysis notebook description data structure learn python basic with tuples list dicts set data structure utility learn python operation such a slice range xrange bisect sort sorted reversed enumerate zip list comprehension function learn about more advanced python feature function a object lambda function closure args kwargs currying generator generator expression itertools datetime learn how to work with python date and time datetime strftime strptime timedelta logging learn about python logging with rotatingfilehandler and timedrotatingfilehandler pdb learn how to debug in python with the interactive source code debugger unit test learn how to test in python with nose unit test kaggleandbusinessanalyses ipython notebook used in kaggle competition and business analysis notebook description titanic predict survival on the titanic learn data cleaning exploratory data analysis and machine learning churnanalysis predict customer churn exercise logistic regression gradient boosting classifers support vector machine random forest and knearestneighbors includes discussion of confusion matrix roc plot feature importance prediction probability and calibrationdescrimination spark ipython notebook demonstrating spark and hdfs functionality notebook description spark inmemory cluster computing framework up to 100 time faster for certain application and is well suited for machine learning algorithm hdfs reliably store very large file across machine in a large cluster mapreducepython ipython notebook demonstrating hadoop mapreduce with mrjob functionality notebook description mapreducepython run mapreduce job in python executing job locally or on hadoop cluster demonstrates hadoop streaming in python code with unit test and mrjob config file to analyze amazon s3 bucket log on elastic mapreduce disco is another pythonbased alternative aws ipython notebook demonstrating amazon web service aws and aws tool functionality also check out saw a supercharged aws command line interface cli awesome aws a curated list of library open source repos guide blog and other resource notebook description boto official aws sdk for python s3cmd interacts with s3 through the command line s3distcp combine smaller file and aggregate them together by taking in a pattern and target file s3distcp can also be used to transfer large volume of data from s3 to your hadoop cluster s3parallelput uploads multiple file to s3 in parallel redshift act a a fast data warehouse built on top of technology from massive parallel processing mpp kinesis stream data in real time with the ability to process thousand of data stream per second lambda run code in response to event automatically managing compute resource command ipython notebook demonstrating various command line for linux git etc notebook description linux unixlike and mostly posixcompliant computer operating system disk usage splitting file grep sed curl viewing running process terminal syntax highlighting and vim anaconda distribution of the python programming language for largescale data processing predictive analytics and scientific computing that aim to simplify package management and deployment ipython notebook webbased interactive computational environment where you can combine code execution text mathematics plot and rich medium into a single document git distributed revision control system with an emphasis on speed data integrity and support for distributed nonlinear workflow ruby used to interact with the aws command line and for jekyll a blog framework that can be hosted on github page jekyll simple blogaware static site generator for personal project or organization site render markdown or textile and liquid template and produce a complete static website ready to be served by apache http server nginx or another web server pelican pythonbased alternative to jekyll django highlevel python web framework that encourages rapid development and clean pragmatic design it can be useful to share reportsanalyses and for blogging lighterweight alternative include pyramid flask tornado and bottle misc ipython notebook demonstrating miscellaneous functionality notebook description regex regular expression cheat sheet useful in data wrangling algorithmia algorithmia is a marketplace for algorithm this notebook showcase 4 different algorithm face detection content summarizer latent dirichlet allocation and optical character recognition notebookinstallation anaconda anaconda is a free distribution of the python programming language for largescale data processing predictive analytics and scientific computing that aim to simplify package management and deployment follow instruction to install anaconda or the more lightweight miniconda devsetup for detailed instruction script and tool to set up your development environment for data analysis check out the devsetup repo runningnotebooks note if you intend to learn the hard way preferred methodthen id strongly advice to write a much code a you can yourself and not just run prewritten code if you still want to test it then do the following to view interactive content or to modify element within the ipython notebook you must first clone or download the repository then run the notebook more information on ipython notebook can be found here git clone httpsgithubcomtarrysinghartificialintelligencedeeplearningmachinelearningtutorialsgit cd artificialintelligencedeeplearningmachinelearningtutorials jupyter notebook notebook tested with python 37 curatedlistofdeeplearningblogs a blog from a humanengineerbeing httpwwwerogolcom r aakash japi httpaakashjapicom r adit deshpande httpsadeshpande3githubio r advanced analytics r httpadvanceddataanalyticsnet r adventure in data land httpblogsmolaorg r agile data science httpblogsenseio r ahmed el deeb httpsmediumcomd33b r airbnb data blog httpnerdsairbnbcomdata r alex castrounis innoarchitech httpwwwinnoarchitechcom r alex perrier httpalexperriergithubio r algobeans data analytics tutorial experiment for the layman httpsalgobeanscom r amazon aws ai blog httpsawsamazoncomblogsai r analytics vidhya httpwwwanalyticsvidhyacomblog r analytics and visualization in big data sicara httpsblogsicaracom r andreas mller httppeekaboovisionblogspotcom r andrej karpathy blog httpkarpathygithubio r andrew brook httpbrooksandrewgithubiosimpleblog r andrey kurenkov httpwwwandreykurenkovcomwriting r anton lebedevichs blog httpmabrekgithubio r arthur juliani httpsmediumcomawjuliani r audun m ygard httpwwwaudunocom r avi singh httpsavisingh599githubio r beautiful data httpbeautifuldatanet r beckerfuffle httpmdbeckergithubio r becoming a data scientist httpwwwbecomingadatascientistcom r ben boltes blog httpbenjaminboltecomml r ben frederickson httpwwwbenfredericksoncomblog r berkeley ai research httpbairberkeleyedublog r bigish data httpbigishdatacom r blog on neural network httpyerevanngithubio r blogistic regressionabout project httpd10genesgithubioblog r blogr r tip and trick from a scientist httpsdrsimonjsvbtlecom r brain of mat kelcey httpmatpalmcomblog r brilliantly wrong thought on science and programming httpsarogozhnikovgithubio r bugra akyildiz httpbugragithubio r building babylon httpsbuildingbabylonnet r carl shan httpcarlshancom r chris stucchio httpswwwchrisstucchiocomblogindexhtml r christophe bourguignat httpsmediumcomchris_bour r christopher nguyen httpsmediumcomctn r cloudera data science post httpblogclouderacomblogcategorydatascience r colahs blog httpcolahgithubioarchivehtml r cortana intelligence and machine learning blog httpsblogstechnetmicrosoftcommachinelearning r daniel forsyth httpwwwdanielforsythme r daniel homola httpdanielhomolacomcategoryblog r daniel nee httpdanielneecom r data based invention httpdatalablu r data blogger httpswwwdatabloggercom r data lab httpbloginsightdatalabscom r data meet medium httpdatameetsmediacom r data miner blog httpblogdataminerscom r data mining research httpwwwdataminingblogcom r data mining text mining visualization and social medium httpdataminingtypepadcomdata_mining r data pique httpblogethanrosenthalcom r data school httpwwwdataschoolio r data science 101 http101datasciencecommunity r data science facebook httpsresearchfacebookcomblogdatascience r data science insight httpwwwdatasciencebowlcomdatascienceinsights r data science tutorial httpscodementoriodatasciencetutorial r data science vademecum httpdatasciencevademecumwordpresscom r dataaspirant httpdataaspirantcom r dataclysm httpblogokcupidcom r datagenetics httpdatageneticscombloghtml r dataiku httpswwwdataikucomblog r datakind httpwwwdatakindorgblog r datalook httpblogdatalookio r datanice httpsdatanicewordpresscom r dataquest blog httpswwwdataquestioblog r datarobot httpwwwdatarobotcomblog r datascope httpdatascopeanalyticscomblog r datasframe httptomaugspurgergithubio r david mimno httpwwwmimnoorg r dayne batten httpdaynebattencom r deep learning httpdeeplearningnetblog r deepdish httpdeepdishio r delip rao httpdelipraocom r dennys blog httpblogdennybritzcom r dimensionless httpsdimensionlessinblog r distill httpdistillpub r district data lab httpdistrictdatalabssilvrbackcom r diving into data httpsblogdatadivenet r domino data lab blog httpblogdominodatalabcom r dr randal s olson httpwwwrandalolsoncomblog r drew conway httpsmediumcomdrewconway r dustin tran httpdustintrancomblog r eder santana httpsedersantanagithubiobloghtml r edwin chen httpblogechenme r efavdb httpefavdbcom r emilio ferrara phd httpwwwemilioferraraname r entrepreneurial geekiness httpianozsvaldcom r eric jonas httpericjonascomarchiveshtml r eric siegel httpwwwpredictiveanalyticsworldcomblog r erik bern httperikberncom r erin shellman httpwwwerinshellmancom r eugenio culurciello httpculurciellogithubio r fabian pedregosa httpfabianpnet r fast forward lab httpblogfastforwardlabscom r fastml httpfastmlcom r florian hartl httpflorianhartlcom r flowingdata httpflowingdatacom r full stack ml httpfullstackmlcom r gab41 httpwwwlab41orggab41 r garbled note httpwwwchiokain r greg reda httpwwwgregredacomblog r hyon s chu httpsmediumcomadailyventure r i am trask httpiamtraskgithubio r i quant ny httpiquantnytumblrcom r inference httpwwwinferencevc r insight data science httpsbloginsightdatasciencecom r inspiration information httpmyinspirationinformationcom r ira korshunova httpirakorshunovagithubio r im a bandit httpsblogsprincetoneduimabandit r jason toy httpwwwjtoynet r jeremy d jackson phd httpwwwjeremydjacksonphdcom r jesse steinwegwoods httpsjesseswcom r joe cauteruccio httpwwwjoecjrcom r john myles white httpwwwjohnmyleswhitecom r john soapbox httpjoschugithubio r jonas degrave http317070githubio r joy of data httpwwwjoyofdatadeblog r julia evans httpjvnsca r kdnuggets httpwwwkdnuggetscom r keeping up with the latest technique httpcolinpriestcom r kenny bastani httpwwwkennybastanicom r kevin davenport httpkldavenportcom r kevin frans httpkvfranscom r korbonits math data httpkorbonitsgithubio r large scale machine learning httpbicksonblogspotcom r lateral blog httpsbloglateralio r lazy programmer httplazyprogrammerme r learn analytics here httpslearnanalyticsherewordpresscom r learndatasci httpwwwlearndatascicom r learning with data httplearningwithdatacom r life language learning httpdaoudclarkegithubio r locke data httpsitsalockecomblog r louis dorard httpwwwlouisdorardcomblog r medriscoll httpmedriscollcom r machinalis httpwwwmachinaliscomblog r machine learning theory httphunchnet r machine learning and data science httpalexhwoodscomblog r machine learning httpscharlesmartin14wordpresscom r machine learning mastery httpmachinelearningmasterycomblog r machine learning blog httpsmachinelearningblogscom r machine learning etc httpyaroslavvbblogspotcom r machine learning math and physic httpsmlopezmwordpresscom r machine learning flashcard httpsmachinelearningflashcardscom 10 but a nicely illustrated set of 300 flash card machined learning httpwwwmachinedlearningscom r mapping babel httpsjackclarknet r mapr blog httpswwwmaprcomblog r marek rei httpwwwmarekreicomblog r marginally interesting httpblogmikiobraunde r math programming httpjeremykuncom r matthew rocklin httpmatthewrocklincomblog r melody wolk httpmelodywolkcomprojects r mic farris httpwwwmicfarriscom r mike tyka httpmtykagithubio r minimaxir max woolf blog httpminimaxircom r mirror image httpsmirror2imagewordpresscom r mitch crowe httpwwwdataphoriccom r mlwave httpmlwavecom r mlwhiz httpmlwhizcom r model are illuminating and wrong httpspeadarcoylewordpresscom r moody rd httpblogmrtzorg r moonshots httpjxieeducationcom r mourad mourafiq httpmourafiqcom r my thought on data science predictive analytics python httpshahramabyaricom r natural language processing blog httpnlpersblogspotfr r neil lawrence httpinverseprobabilitycombloghtml r nlp and deep learning enthusiast httpcamronxyz r no free hunch httpblogkagglecom r nuit blanche httpnuitblancheblogspotcom r number 2147483647 httpsno2147483647wordpresscom r on machine intelligence httpsaimatterswordpresscom r opiate for the mass data is our religion httpopiateforthemasses r pvalueinfo httpwwwpvalueinfo r pete warden blog httppetewardencom r plotly blog httpblogplotly r probably overthinking it httpallendowneyblogspotca r prooffreadercom httpwwwprooffreadercom r prooffreaderplus httpprooffreaderplusblogspotca r publishable stuff httpwwwsumsarnet r pyimagesearch httpwwwpyimagesearchcom r pythonic perambulation httpsjakevdpgithubio r quintuitive httpquintuitivecom r r and data mining httpsrdataminingwordpresscom r rbloggers httpwwwrbloggerscom r r2rt httpr2rtcom r ramiro gmez httpramiroorgnotebooks r random note on computer science mathematics and software engineering httpbarmaleyexegithubio r randy zwitch httprandyzwitchcom r rare technology httpraretechnologiescomblog r raylinet httpraylinetblog r revolution httpblogrevolutionanalyticscom r rinu boney httprinuboneygithubio r rnduja blog httprndujagithubio r robert chang httpsmediumcomrchang r rocketpowered data science httprocketdatascienceorg r sachin joglekars blog httpscodesachinwordpresscom r samim httpsmediumcomsamim r sean j taylor httpseanjtaylorcom r sebastian raschka httpsebastianraschkacomblogindexhtml r sebastian ruder httpsebastianrudercom r sebastians slow blog httpwwwnowozinnetsebastianblog r sfl scientific blog httpssflscientificcomblog r shakirs machine learning blog httpblogshakirmcom r simply statistic httpsimplystatisticsorg r springboard blog httpspringboardcomblog startupml blog httpstartupmlblog r statistical modeling causal inference and social science httpandrewgelmancom r stigler diet httpstiglerdietcom r stitch fix tech blog httpmultithreadedstitchfixcomblog r stochastic rd note httparsenyinfo r storytelling with statistic on quora httpdatastoriesquoracom r streamhacker httpstreamhackercom r subconscious musing httpblogssascomcontentsubconsciousmusings r swan intelligence httpswanintelligencecom r technocalifornia httptechnocaliforniablogspotse r text analysis blog aylien httpblogayliencom r the angry statistician httpangrystatisticianblogspotcom r the clever machine httpstheclevermachinewordpresscom r the data camp blog httpswwwdatacampcomcommunityblog r the data incubator httpblogthedataincubatorcom r the data science lab httpsdatasciencelabwordpresscom r the etzfiles httpalexanderetzcom r the science of data httpwwwmartingoodsoncom r the shape of data httpsshapeofdatawordpresscom r the unofficial google data science blog httpwwwunofficialgoogledatasciencecom r tim dettmers httptimdettmerscom r tombones computer vision blog httpwwwcomputervisionblogcom r tommy blanchard httptommyblanchardcomcategoryprojects r trevor stephen httptrevorstephenscom r trey causey httptreycauseycom r uw data science blog httpdatasciencedegreewisconsinedublog r wellecks httpwelleckswordpresscom r wes mckinney httpwesmckinneycomarchiveshtml r while my mcmc gently sample httptwieckigithubio r wildml httpwwwwildmlcom r will do stuff for stuff httprinzewindorgblogen r will wolf httpwillwolfio r will noise httpwwwwillmcginniscom r william lyon httpwwwlyonwjcom r winvector blog httpwwwwinvectorcomblog r yanir seroussi httpyanirseroussicom r zac stewart httpzacstewartcom r hat httpblogyhatcom r quantitative ourney httpoutlacecom r httpblogotoronet r credit python for data analysis data wrangling with panda numpy and ipython by wes mckinney pycon 2015 scikitlearn tutorial by jake vanderplas python data science handbook by jake vanderplas parallel machine learning with scikitlearn and ipython by olivier grisel statistical interference using computational method in python by allen downey tensorflow example by aymeric damien tensorflow tutorial by parag k mital tensorflow tutorial by nathan lintz tensorflow tutorial by alexander r johansen tensorflow book by nishant shukla summer school 2015 by milaudem kera tutorial by valerio maggio kaggle yhat blog contributing contribution are welcome for bug report or request please submit an issue contactinfo feel free to contact me to discus any issue question or comment email tarrysinghgmailcom twitter tarrysingh github tarrysingh linkedin tarry singh website tarrysinghcom medium tarrymedium quora answer from tarry on quora license this repository contains a variety of content some developed by tarry singh and some from thirdparties and a lot will be maintained by me the thirdparty content is distributed under the license provided by those party the content wa originally developed by donne martin is distributed under the following license i will be maintaining and revamping it by adding pytorch torchlua mxnet and much more i am providing code and resource in this repository to you under an open source license copyright 2017 tarry singh licensed under the apache license version 20 the license you may not use this file except in compliance with the license you may obtain a copy of the license at httpwwwapacheorglicenseslicense20 unless required by applicable law or agreed to in writing software distributed under the license is distributed on an a is basis without warranty or condition of any kind either express or implied see the license for the specific language governing permission and limitation under the license,new list 2020 2021 machinelearning deeplearning ai tutorial hi thanks dropping updating tutorial site daily basis adding relevant topcis including latest research paper internet arxivorg biorxiv specifically neuroscience name importantly application mldlai industry area transportation medicinehealthcare etc something ill watch keen interest would love share finally help seek make useful le boring please suggestcommentcontribute index deeplearning uber pyro netflix vectorflow pytorch tensorflow theano kera caffe torchlua mxnet scikitlearn statisticalinferencescipy panda matplotlib numpy pythondata kaggleandbusinessanalyses spark mapreducepython amazon web service command line misc notebookinstallation curated list deep learning ai blog credit contributing contactinfo license deeplearning ipython notebook programming tool torchluad lang demonstrating deep learning functionality uberpyroprobabalistictutorials additional pyro tutorial pyroexamplesfull example pyroexamplesvariational autoencoders pyroexamplesbayesian regression pyroexamplesdeep markov model pyroexamplesairattend infer repeat pyroexamplessemisupervised pyroexamplesgmm pyroexamplesgaussian process pyroexamplesbayesian optimization full pyro code netflixvectorflowtutorials mnist example running dlang pytorchtutorials level description beginnerszakizhou learning basic pytorch facebook intermediaquanvuong learning intermediate stuff pytorch facebook advancedchsasank learning advanced stuff pytorch facebook learning pytorch example numpy tensor autograd core pytorch provides two main feature ndimensional tensor similar numpy run gpus automatic differentiation building training neural network pytorch getting know autogradvariable gradient neural network start ultimate basic tensor wrap tensor variable module play nnmodule implement forward backward function tensorflowtutorials additional tensorflow tutorial pkmitaltensorflow_tutorials nlintztensorflowtutorials alrojotensorflowtutorial binroottensorflowbook notebook description tsfbasics learn basic operation tensorflow library various kind perceptual language understanding task google tsflinear implement linear regression tensorflow tsflogistic implement logistic regression tensorflow tsfnn implement nearest neighboars tensorflow tsfalex implement alexnet tensorflow tsfcnn implement convolutional neural network tensorflow tsfmlp implement multilayer perceptrons tensorflow tsfrnn implement recurrent neural network tensorflow tsfgpu learn basic multigpu computation tensorflow tsfgviz learn graph visualization tensorflow tsflviz learn loss visualization tensorflow tensorflowexercises notebook description tsfnotmnist learn simple data curation creating pickle formatted datasets training development testing tensorflow tsffullyconnected progressively train deeper accurate model using logistic regression neural network tensorflow tsfregularization explore regularization technique training fully connected network classify notmnist character tensorflow tsfconvolutions create convolutional neural network tensorflow tsfword2vec train skipgram model text8 data tensorflow tsflstm train lstm character model text8 data tensorflow theanotutorials notebook description theanointro intro theano allows define optimize evaluate mathematical expression involving multidimensional array efficiently use gpus perform efficient symbolic differentiation theanoscan learn scan mechanism perform loop theano graph theanologistic implement logistic regression theano theanornn implement recurrent neural network theano theanomlp implement multilayer perceptrons theano kerastutorials notebook description kera kera open source neural network library written python capable running top either tensorflow theano setup learn tutorial goal set kera environment introdeeplearningann get intro deep learning kera artificial neural network ann perceptrons adaline implement peceptron adaptive linear neuron mlp mnist data classifying handwritten digitsimplement mlp train debug ann theano learn theano working weight matrix gradient kerasotto learn kera looking kaggle otto challenge annmnist review simple implementation ann mnist using kera convnets learn convolutional neural network cnns kera convnet1 recognize handwritten digit mnist using kera part 1 convnet2 recognize handwritten digit mnist using kera part 2 kerasmodels use pretrained model vgg16 vgg19 resnet50 inception v3 kera autoencoders learn autoencoders kera rnnlstm learn recurrent neural network rnns kera lstmsentencegen learn rnns using long short term memory lstm network kera nlpdeeplearning learn nlp using ann artificial neural network hyperparamtertuning hyperparamters tuning using keraswrapperscikitlearn deeplearningmisc notebook description deepdream caffebased computer vision program us convolutional neural network find enhance pattern image scikitlearn ipython notebook demonstrating scikitlearn functionality notebook description intro intro notebook scikitlearn scikitlearn add python support large multidimensional array matrix along large library highlevel mathematical function operate array knn implement knearest neighbor scikitlearn linearreg implement linear regression scikitlearn svm implement support vector machine classifier without kernel scikitlearn randomforest implement random forest classifier regressors scikitlearn kmeans implement kmeans clustering scikitlearn pca implement principal component analysis scikitlearn gmm implement gaussian mixture model scikitlearn validation implement validation model selection scikitlearn statisticalinferencescipy ipython notebook demonstrating statistical inference scipy functionality notebook description scipy scipy collection mathematical algorithm convenience function built numpy extension python add significant power interactive python session providing user highlevel command class manipulating visualizing data effectsize explore statistic quantify effect size analyzing difference height men woman us data behavioral risk factor surveillance system brfss estimate mean standard deviation height adult woman men united state sampling explore random sampling analyzing average weight men woman united state using brfss data hypothesis explore hypothesis testing analyzing difference firstborn baby compared others panda ipython notebook demonstrating panda functionality notebook description panda software library written data manipulation analysis python offer data structure operation manipulating numerical table time series githubdatawrangling learn load clean merge feature engineer analyzing github data viz repo introductiontopandas introduction panda introducingpandasobjects learn panda object data indexing selection learn data indexing selection panda operationsinpandas learn operating data panda missingvalues learn handling missing data panda hierarchicalindexing learn hierarchical indexing panda concatandappend learn combining datasets concat append panda mergeandjoin learn combining datasets merge join panda aggregationandgrouping learn aggregation grouping panda pivottables learn pivot table panda workingwithstrings learn vectorized string operation panda workingwithtimeseries learn working time series panda performanceevalandquery learn highperformance panda eval query panda matplotlib ipython notebook demonstrating matplotlib functionality notebook description matplotlib python 2d plotting library produce publication quality figure variety hardcopy format interactive environment across platform matplotlibapplied apply matplotlib visualization kaggle competition exploratory data analysis learn create bar plot histogram subplot2grid normalized plot scatter plot subplots kernel density estimation plot introductiontomatplotlib introduction matplotlib simplelineplots learn simple line plot matplotlib simplescatterplots learn simple scatter plot matplotlib errorbarsipynb learn visualizing error matplotlib densityandcontourplots learn density contour plot matplotlib histogramsandbinnings learn histogram binnings density matplotlib customizinglegends learn customizing plot legend matplotlib customizingcolorbars learn customizing colorbars matplotlib multiplesubplots learn multiple subplots matplotlib textandannotation learn text annotation matplotlib customizingticks learn customizing tick matplotlib settingsandstylesheets learn customizing matplotlib configuration stylesheets threedimensionalplotting learn threedimensional plotting matplotlib geographicdatawithbasemap learn geographic data basemap matplotlib visualizationwithseaborn learn visualization seaborn numpy ipython notebook demonstrating numpy functionality notebook description numpy add python support large multidimensional array matrix along large library highlevel mathematical function operate array introductiontonumpy introduction numpy understandingdatatypes learn data type python thebasicsofnumpyarrays learn basic numpy array computationonarraysufuncs learn computation numpy array universal function computationonarraysaggregates learn aggregation min max everything numpy computationonarraysbroadcasting learn computation array broadcasting numpy booleanarraysandmasks learn comparison mask boolean logic numpy fancyindexing learn fancy indexing numpy sorting learn sorting array numpy structureddatanumpy learn structured data numpys structured array pythondata ipython notebook demonstrating python functionality geared towards data analysis notebook description data structure learn python basic tuples list dicts set data structure utility learn python operation slice range xrange bisect sort sorted reversed enumerate zip list comprehension function learn advanced python feature function object lambda function closure args kwargs currying generator generator expression itertools datetime learn work python date time datetime strftime strptime timedelta logging learn python logging rotatingfilehandler timedrotatingfilehandler pdb learn debug python interactive source code debugger unit test learn test python nose unit test kaggleandbusinessanalyses ipython notebook used kaggle competition business analysis notebook description titanic predict survival titanic learn data cleaning exploratory data analysis machine learning churnanalysis predict customer churn exercise logistic regression gradient boosting classifers support vector machine random forest knearestneighbors includes discussion confusion matrix roc plot feature importance prediction probability calibrationdescrimination spark ipython notebook demonstrating spark hdfs functionality notebook description spark inmemory cluster computing framework 100 time faster certain application well suited machine learning algorithm hdfs reliably store large file across machine large cluster mapreducepython ipython notebook demonstrating hadoop mapreduce mrjob functionality notebook description mapreducepython run mapreduce job python executing job locally hadoop cluster demonstrates hadoop streaming python code unit test mrjob config file analyze amazon s3 bucket log elastic mapreduce disco another pythonbased alternative aws ipython notebook demonstrating amazon web service aws aws tool functionality also check saw supercharged aws command line interface cli awesome aws curated list library open source repos guide blog resource notebook description boto official aws sdk python s3cmd interacts s3 command line s3distcp combine smaller file aggregate together taking pattern target file s3distcp also used transfer large volume data s3 hadoop cluster s3parallelput uploads multiple file s3 parallel redshift act fast data warehouse built top technology massive parallel processing mpp kinesis stream data real time ability process thousand data stream per second lambda run code response event automatically managing compute resource command ipython notebook demonstrating various command line linux git etc notebook description linux unixlike mostly posixcompliant computer operating system disk usage splitting file grep sed curl viewing running process terminal syntax highlighting vim anaconda distribution python programming language largescale data processing predictive analytics scientific computing aim simplify package management deployment ipython notebook webbased interactive computational environment combine code execution text mathematics plot rich medium single document git distributed revision control system emphasis speed data integrity support distributed nonlinear workflow ruby used interact aws command line jekyll blog framework hosted github page jekyll simple blogaware static site generator personal project organization site render markdown textile liquid template produce complete static website ready served apache http server nginx another web server pelican pythonbased alternative jekyll django highlevel python web framework encourages rapid development clean pragmatic design useful share reportsanalyses blogging lighterweight alternative include pyramid flask tornado bottle misc ipython notebook demonstrating miscellaneous functionality notebook description regex regular expression cheat sheet useful data wrangling algorithmia algorithmia marketplace algorithm notebook showcase 4 different algorithm face detection content summarizer latent dirichlet allocation optical character recognition notebookinstallation anaconda anaconda free distribution python programming language largescale data processing predictive analytics scientific computing aim simplify package management deployment follow instruction install anaconda lightweight miniconda devsetup detailed instruction script tool set development environment data analysis check devsetup repo runningnotebooks note intend learn hard way preferred methodthen id strongly advice write much code run prewritten code still want test following view interactive content modify element within ipython notebook must first clone download repository run notebook information ipython notebook found git clone httpsgithubcomtarrysinghartificialintelligencedeeplearningmachinelearningtutorialsgit cd artificialintelligencedeeplearningmachinelearningtutorials jupyter notebook notebook tested python 37 curatedlistofdeeplearningblogs blog humanengineerbeing httpwwwerogolcom r aakash japi httpaakashjapicom r adit deshpande httpsadeshpande3githubio r advanced analytics r httpadvanceddataanalyticsnet r adventure data land httpblogsmolaorg r agile data science httpblogsenseio r ahmed el deeb httpsmediumcomd33b r airbnb data blog httpnerdsairbnbcomdata r alex castrounis innoarchitech httpwwwinnoarchitechcom r alex perrier httpalexperriergithubio r algobeans data analytics tutorial experiment layman httpsalgobeanscom r amazon aws ai blog httpsawsamazoncomblogsai r analytics vidhya httpwwwanalyticsvidhyacomblog r analytics visualization big data sicara httpsblogsicaracom r andreas mller httppeekaboovisionblogspotcom r andrej karpathy blog httpkarpathygithubio r andrew brook httpbrooksandrewgithubiosimpleblog r andrey kurenkov httpwwwandreykurenkovcomwriting r anton lebedevichs blog httpmabrekgithubio r arthur juliani httpsmediumcomawjuliani r audun ygard httpwwwaudunocom r avi singh httpsavisingh599githubio r beautiful data httpbeautifuldatanet r beckerfuffle httpmdbeckergithubio r becoming data scientist httpwwwbecomingadatascientistcom r ben boltes blog httpbenjaminboltecomml r ben frederickson httpwwwbenfredericksoncomblog r berkeley ai research httpbairberkeleyedublog r bigish data httpbigishdatacom r blog neural network httpyerevanngithubio r blogistic regressionabout project httpd10genesgithubioblog r blogr r tip trick scientist httpsdrsimonjsvbtlecom r brain mat kelcey httpmatpalmcomblog r brilliantly wrong thought science programming httpsarogozhnikovgithubio r bugra akyildiz httpbugragithubio r building babylon httpsbuildingbabylonnet r carl httpcarlshancom r chris stucchio httpswwwchrisstucchiocomblogindexhtml r christophe bourguignat httpsmediumcomchris_bour r christopher nguyen httpsmediumcomctn r cloudera data science post httpblogclouderacomblogcategorydatascience r colahs blog httpcolahgithubioarchivehtml r cortana intelligence machine learning blog httpsblogstechnetmicrosoftcommachinelearning r daniel forsyth httpwwwdanielforsythme r daniel homola httpdanielhomolacomcategoryblog r daniel nee httpdanielneecom r data based invention httpdatalablu r data blogger httpswwwdatabloggercom r data lab httpbloginsightdatalabscom r data meet medium httpdatameetsmediacom r data miner blog httpblogdataminerscom r data mining research httpwwwdataminingblogcom r data mining text mining visualization social medium httpdataminingtypepadcomdata_mining r data pique httpblogethanrosenthalcom r data school httpwwwdataschoolio r data science 101 http101datasciencecommunity r data science facebook httpsresearchfacebookcomblogdatascience r data science insight httpwwwdatasciencebowlcomdatascienceinsights r data science tutorial httpscodementoriodatasciencetutorial r data science vademecum httpdatasciencevademecumwordpresscom r dataaspirant httpdataaspirantcom r dataclysm httpblogokcupidcom r datagenetics httpdatageneticscombloghtml r dataiku httpswwwdataikucomblog r datakind httpwwwdatakindorgblog r datalook httpblogdatalookio r datanice httpsdatanicewordpresscom r dataquest blog httpswwwdataquestioblog r datarobot httpwwwdatarobotcomblog r datascope httpdatascopeanalyticscomblog r datasframe httptomaugspurgergithubio r david mimno httpwwwmimnoorg r dayne batten httpdaynebattencom r deep learning httpdeeplearningnetblog r deepdish httpdeepdishio r delip rao httpdelipraocom r dennys blog httpblogdennybritzcom r dimensionless httpsdimensionlessinblog r distill httpdistillpub r district data lab httpdistrictdatalabssilvrbackcom r diving data httpsblogdatadivenet r domino data lab blog httpblogdominodatalabcom r dr randal olson httpwwwrandalolsoncomblog r drew conway httpsmediumcomdrewconway r dustin tran httpdustintrancomblog r eder santana httpsedersantanagithubiobloghtml r edwin chen httpblogechenme r efavdb httpefavdbcom r emilio ferrara phd httpwwwemilioferraraname r entrepreneurial geekiness httpianozsvaldcom r eric jonas httpericjonascomarchiveshtml r eric siegel httpwwwpredictiveanalyticsworldcomblog r erik bern httperikberncom r erin shellman httpwwwerinshellmancom r eugenio culurciello httpculurciellogithubio r fabian pedregosa httpfabianpnet r fast forward lab httpblogfastforwardlabscom r fastml httpfastmlcom r florian hartl httpflorianhartlcom r flowingdata httpflowingdatacom r full stack ml httpfullstackmlcom r gab41 httpwwwlab41orggab41 r garbled note httpwwwchiokain r greg reda httpwwwgregredacomblog r hyon chu httpsmediumcomadailyventure r trask httpiamtraskgithubio r quant ny httpiquantnytumblrcom r inference httpwwwinferencevc r insight data science httpsbloginsightdatasciencecom r inspiration information httpmyinspirationinformationcom r ira korshunova httpirakorshunovagithubio r im bandit httpsblogsprincetoneduimabandit r jason toy httpwwwjtoynet r jeremy jackson phd httpwwwjeremydjacksonphdcom r jesse steinwegwoods httpsjesseswcom r joe cauteruccio httpwwwjoecjrcom r john myles white httpwwwjohnmyleswhitecom r john soapbox httpjoschugithubio r jonas degrave http317070githubio r joy data httpwwwjoyofdatadeblog r julia evans httpjvnsca r kdnuggets httpwwwkdnuggetscom r keeping latest technique httpcolinpriestcom r kenny bastani httpwwwkennybastanicom r kevin davenport httpkldavenportcom r kevin frans httpkvfranscom r korbonits math data httpkorbonitsgithubio r large scale machine learning httpbicksonblogspotcom r lateral blog httpsbloglateralio r lazy programmer httplazyprogrammerme r learn analytics httpslearnanalyticsherewordpresscom r learndatasci httpwwwlearndatascicom r learning data httplearningwithdatacom r life language learning httpdaoudclarkegithubio r locke data httpsitsalockecomblog r louis dorard httpwwwlouisdorardcomblog r medriscoll httpmedriscollcom r machinalis httpwwwmachinaliscomblog r machine learning theory httphunchnet r machine learning data science httpalexhwoodscomblog r machine learning httpscharlesmartin14wordpresscom r machine learning mastery httpmachinelearningmasterycomblog r machine learning blog httpsmachinelearningblogscom r machine learning etc httpyaroslavvbblogspotcom r machine learning math physic httpsmlopezmwordpresscom r machine learning flashcard httpsmachinelearningflashcardscom 10 nicely illustrated set 300 flash card machined learning httpwwwmachinedlearningscom r mapping babel httpsjackclarknet r mapr blog httpswwwmaprcomblog r marek rei httpwwwmarekreicomblog r marginally interesting httpblogmikiobraunde r math programming httpjeremykuncom r matthew rocklin httpmatthewrocklincomblog r melody wolk httpmelodywolkcomprojects r mic farris httpwwwmicfarriscom r mike tyka httpmtykagithubio r minimaxir max woolf blog httpminimaxircom r mirror image httpsmirror2imagewordpresscom r mitch crowe httpwwwdataphoriccom r mlwave httpmlwavecom r mlwhiz httpmlwhizcom r model illuminating wrong httpspeadarcoylewordpresscom r moody rd httpblogmrtzorg r moonshots httpjxieeducationcom r mourad mourafiq httpmourafiqcom r thought data science predictive analytics python httpshahramabyaricom r natural language processing blog httpnlpersblogspotfr r neil lawrence httpinverseprobabilitycombloghtml r nlp deep learning enthusiast httpcamronxyz r free hunch httpblogkagglecom r nuit blanche httpnuitblancheblogspotcom r number 2147483647 httpsno2147483647wordpresscom r machine intelligence httpsaimatterswordpresscom r opiate mass data religion httpopiateforthemasses r pvalueinfo httpwwwpvalueinfo r pete warden blog httppetewardencom r plotly blog httpblogplotly r probably overthinking httpallendowneyblogspotca r prooffreadercom httpwwwprooffreadercom r prooffreaderplus httpprooffreaderplusblogspotca r publishable stuff httpwwwsumsarnet r pyimagesearch httpwwwpyimagesearchcom r pythonic perambulation httpsjakevdpgithubio r quintuitive httpquintuitivecom r r data mining httpsrdataminingwordpresscom r rbloggers httpwwwrbloggerscom r r2rt httpr2rtcom r ramiro gmez httpramiroorgnotebooks r random note computer science mathematics software engineering httpbarmaleyexegithubio r randy zwitch httprandyzwitchcom r rare technology httpraretechnologiescomblog r raylinet httpraylinetblog r revolution httpblogrevolutionanalyticscom r rinu boney httprinuboneygithubio r rnduja blog httprndujagithubio r robert chang httpsmediumcomrchang r rocketpowered data science httprocketdatascienceorg r sachin joglekars blog httpscodesachinwordpresscom r samim httpsmediumcomsamim r sean j taylor httpseanjtaylorcom r sebastian raschka httpsebastianraschkacomblogindexhtml r sebastian ruder httpsebastianrudercom r sebastians slow blog httpwwwnowozinnetsebastianblog r sfl scientific blog httpssflscientificcomblog r shakirs machine learning blog httpblogshakirmcom r simply statistic httpsimplystatisticsorg r springboard blog httpspringboardcomblog startupml blog httpstartupmlblog r statistical modeling causal inference social science httpandrewgelmancom r stigler diet httpstiglerdietcom r stitch fix tech blog httpmultithreadedstitchfixcomblog r stochastic rd note httparsenyinfo r storytelling statistic quora httpdatastoriesquoracom r streamhacker httpstreamhackercom r subconscious musing httpblogssascomcontentsubconsciousmusings r swan intelligence httpswanintelligencecom r technocalifornia httptechnocaliforniablogspotse r text analysis blog aylien httpblogayliencom r angry statistician httpangrystatisticianblogspotcom r clever machine httpstheclevermachinewordpresscom r data camp blog httpswwwdatacampcomcommunityblog r data incubator httpblogthedataincubatorcom r data science lab httpsdatasciencelabwordpresscom r etzfiles httpalexanderetzcom r science data httpwwwmartingoodsoncom r shape data httpsshapeofdatawordpresscom r unofficial google data science blog httpwwwunofficialgoogledatasciencecom r tim dettmers httptimdettmerscom r tombones computer vision blog httpwwwcomputervisionblogcom r tommy blanchard httptommyblanchardcomcategoryprojects r trevor stephen httptrevorstephenscom r trey causey httptreycauseycom r uw data science blog httpdatasciencedegreewisconsinedublog r wellecks httpwelleckswordpresscom r wes mckinney httpwesmckinneycomarchiveshtml r mcmc gently sample httptwieckigithubio r wildml httpwwwwildmlcom r stuff stuff httprinzewindorgblogen r wolf httpwillwolfio r will noise httpwwwwillmcginniscom r william lyon httpwwwlyonwjcom r winvector blog httpwwwwinvectorcomblog r yanir seroussi httpyanirseroussicom r zac stewart httpzacstewartcom r hat httpblogyhatcom r quantitative ourney httpoutlacecom r httpblogotoronet r credit python data analysis data wrangling panda numpy ipython wes mckinney pycon 2015 scikitlearn tutorial jake vanderplas python data science handbook jake vanderplas parallel machine learning scikitlearn ipython olivier grisel statistical interference using computational method python allen downey tensorflow example aymeric damien tensorflow tutorial parag k mital tensorflow tutorial nathan lintz tensorflow tutorial alexander r johansen tensorflow book nishant shukla summer school 2015 milaudem kera tutorial valerio maggio kaggle yhat blog contributing contribution welcome bug report request please submit issue contactinfo feel free contact discus issue question comment email tarrysinghgmailcom twitter tarrysingh github tarrysingh linkedin tarry singh website tarrysinghcom medium tarrymedium quora answer tarry quora license repository contains variety content developed tarry singh thirdparties lot maintained thirdparty content distributed license provided party content originally developed donne martin distributed following license maintaining revamping adding pytorch torchlua mxnet much providing code resource repository open source license copyright 2017 tarry singh licensed apache license version 20 license may use file except compliance license may obtain copy license httpwwwapacheorglicenseslicense20 unless required applicable law agreed writing software distributed license distributed basis without warranty condition kind either express implied see license specific language governing permission limitation license
Python ,"Artificial Intelligence Nanodegree Program Resources
Classroom Exercises
1. Constraint Satisfaction Problems
In this exercise you will explore Constraint Satisfaction Problems in a Jupyter notebook and use a CSP solver to solve a variety of problems.
Read more here
2. Classical Search for PacMan (only in classroom)
Please DO NOT publish your work on this exercise.
In this exercise you will teach Pac-Man to search his world to complete the following tasks:

find a single obstacle
find multiple obstacles
find the fastest way to eat all the food in the map

3. Local Search Optimization
In this exercise, you'll implement several local search algorithms and test them on the Traveling Salesman Problem (TSP) between a few dozen US state capitals.
Projects
1. Sudoku Solver
In this project, you will extend the Sudoku-solving agent developed in the classroom lectures to solve diagonal Sudoku puzzles and implement a new constraint strategy called ""naked twins"". A diagonal Sudoku puzzle is identical to traditional Sudoku puzzles with the added constraint that the boxes on the two main diagonals of the board must also contain the digits 1-9 in each cell (just like the rows, columns, and 3x3 blocks).
Read more here
2. Classical Planning
This project is split between implementation and analysis. First you will combine symbolic logic and classical search to implement an agent that performs progression search to solve planning problems. Then you will experiment with different search algorithms and heuristics, and use the results to answer questions about designing planning systems.
Read more here
3. Game Playing
In this project you will choose an experiment with adversarial game-playing techniques like minimax, Monte Carlo tree search, opening books, and more. Your goal will be to build and evaluate the performance of your agent in a finite deterministic two player game of perfect information called Isolation.
Read more here
4. Part of Speech Tagger
In this notebook, you'll use the Pomegranate library to build a hidden Markov model for part of speech tagging with a universal tagset. Hidden Markov models have been able to achieve >96% tag accuracy with larger tagsets on realistic text corpora. Hidden Markov models have also been used for speech recognition and speech generation, machine translation, gene recognition for bioinformatics, and human gesture recognition for computer vision, and more.
Read more here
",artifici intellig nanodegre program resourc classroom exercis 1 constraint satisfact problem in thi exercis you will explor constraint satisfact problem in a jupyt notebook and use a csp solver to solv a varieti of problem read more here 2 classic search for pacman onli in classroom pleas do not publish your work on thi exercis in thi exercis you will teach pacman to search hi world to complet the follow task find a singl obstacl find multipl obstacl find the fastest way to eat all the food in the map 3 local search optim in thi exercis youll implement sever local search algorithm and test them on the travel salesman problem tsp between a few dozen us state capit project 1 sudoku solver in thi project you will extend the sudokusolv agent develop in the classroom lectur to solv diagon sudoku puzzl and implement a new constraint strategi call nake twin a diagon sudoku puzzl is ident to tradit sudoku puzzl with the ad constraint that the box on the two main diagon of the board must also contain the digit 19 in each cell just like the row column and 3x3 block read more here 2 classic plan thi project is split between implement and analysi first you will combin symbol logic and classic search to implement an agent that perform progress search to solv plan problem then you will experi with differ search algorithm and heurist and use the result to answer question about design plan system read more here 3 game play in thi project you will choos an experi with adversari gameplay techniqu like minimax mont carlo tree search open book and more your goal will be to build and evalu the perform of your agent in a finit determinist two player game of perfect inform call isol read more here 4 part of speech tagger in thi notebook youll use the pomegran librari to build a hidden markov model for part of speech tag with a univers tagset hidden markov model have been abl to achiev 96 tag accuraci with larger tagset on realist text corpora hidden markov model have also been use for speech recognit and speech gener machin translat gene recognit for bioinformat and human gestur recognit for comput vision and more read more here,artificial intelligence nanodegree program resource classroom exercise 1 constraint satisfaction problem in this exercise you will explore constraint satisfaction problem in a jupyter notebook and use a csp solver to solve a variety of problem read more here 2 classical search for pacman only in classroom please do not publish your work on this exercise in this exercise you will teach pacman to search his world to complete the following task find a single obstacle find multiple obstacle find the fastest way to eat all the food in the map 3 local search optimization in this exercise youll implement several local search algorithm and test them on the traveling salesman problem tsp between a few dozen u state capital project 1 sudoku solver in this project you will extend the sudokusolving agent developed in the classroom lecture to solve diagonal sudoku puzzle and implement a new constraint strategy called naked twin a diagonal sudoku puzzle is identical to traditional sudoku puzzle with the added constraint that the box on the two main diagonal of the board must also contain the digit 19 in each cell just like the row column and 3x3 block read more here 2 classical planning this project is split between implementation and analysis first you will combine symbolic logic and classical search to implement an agent that performs progression search to solve planning problem then you will experiment with different search algorithm and heuristic and use the result to answer question about designing planning system read more here 3 game playing in this project you will choose an experiment with adversarial gameplaying technique like minimax monte carlo tree search opening book and more your goal will be to build and evaluate the performance of your agent in a finite deterministic two player game of perfect information called isolation read more here 4 part of speech tagger in this notebook youll use the pomegranate library to build a hidden markov model for part of speech tagging with a universal tagset hidden markov model have been able to achieve 96 tag accuracy with larger tagsets on realistic text corpus hidden markov model have also been used for speech recognition and speech generation machine translation gene recognition for bioinformatics and human gesture recognition for computer vision and more read more here,artificial intelligence nanodegree program resource classroom exercise 1 constraint satisfaction problem exercise explore constraint satisfaction problem jupyter notebook use csp solver solve variety problem read 2 classical search pacman classroom please publish work exercise exercise teach pacman search world complete following task find single obstacle find multiple obstacle find fastest way eat food map 3 local search optimization exercise youll implement several local search algorithm test traveling salesman problem tsp dozen u state capital project 1 sudoku solver project extend sudokusolving agent developed classroom lecture solve diagonal sudoku puzzle implement new constraint strategy called naked twin diagonal sudoku puzzle identical traditional sudoku puzzle added constraint box two main diagonal board must also contain digit 19 cell like row column 3x3 block read 2 classical planning project split implementation analysis first combine symbolic logic classical search implement agent performs progression search solve planning problem experiment different search algorithm heuristic use result answer question designing planning system read 3 game playing project choose experiment adversarial gameplaying technique like minimax monte carlo tree search opening book goal build evaluate performance agent finite deterministic two player game perfect information called isolation read 4 part speech tagger notebook youll use pomegranate library build hidden markov model part speech tagging universal tagset hidden markov model able achieve 96 tag accuracy larger tagsets realistic text corpus hidden markov model also used speech recognition speech generation machine translation gene recognition bioinformatics human gesture recognition computer vision read
Python ,"   
 


Mycroft
Mycroft is a hackable open source voice assistant.
Table of Contents

Table of Contents
Getting Started
Running Mycroft
Using Mycroft

Home Device and Account Manager
Skills


Behind the scenes

Pairing Information
Configuration
Using Mycroft Without Home
API Key Services
Using Mycroft behind a proxy

Using Mycroft behind a proxy without authentication
Using Mycroft behind an authenticated proxy




Getting Involved
Links

Getting Started
First, get the code on your system!  The simplest method is via git (git installation instructions):

cd ~/
git clone https://github.com/MycroftAI/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

This script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.
NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.
Running Mycroft
Mycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:

cd ~/mycroft-core
./start-mycroft.sh debug

The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.
The background services can be stopped as a group with:

./stop-mycroft.sh

Using Mycroft
Home Device and Account Manager
Mycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai
By default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can enter into the pairing page within the Mycroft Home site.
Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.
Skills
Mycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!
Behind the scenes
Pairing Information
Pairing information generated by registering with Home is stored in:
~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!
Configuration
Mycroft configuration consists of 4 possible locations:

mycroft-core/mycroft/configuration/mycroft.conf(Defaults)
Mycroft Home (Remote)
/etc/mycroft/mycroft.conf(Machine)
$HOME/.mycroft/mycroft.conf(User)

When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.
Using Mycroft Without Home
If you do not wish to use the Mycroft Home service, before starting Mycroft for the first time, create $HOME/.mycroft/mycroft.conf with the following contents:
{
  ""skills"": {
    ""blacklisted_skills"": [
      ""mycroft-configuration.mycroftai"",
      ""mycroft-pairing.mycroftai""
    ]
  }
}

Mycroft will then be unable to perform speech-to-text conversion, so you'll need to set that up as well, using one of the STT engines Mycroft supports.
You may insert your own API keys into the configuration files listed above in Configuration.  For example, to insert the API key for the Weather skill, create a new JSON key in the configuration file like so:
{
  // other configuration settings...
  //
  ""WeatherSkill"": {
    ""api_key"": ""<insert your API key here>""
  }
}

API Key Services
These are the keys currently used in Mycroft Core:

STT API, Google STT, Google Cloud Speech
Weather Skill API, OpenWeatherMap
Wolfram-Alpha Skill

Using Mycroft behind a proxy
Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.
If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.
NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.
Using Mycroft behind a proxy without authentication
If you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxy
If  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting Involved
This is an open source project and we would love your help. We have prepared a contributing guide to help you get started.
If this is your first PR or you're not sure where to get started,
say hi in Mycroft Chat and a team member would be happy to mentor you.
Join the Mycroft Forum for questions and answers.
Links

Creating a Skill
Documentation
Skill Writer API Docs
Release Notes
Mycroft Chat
Mycroft Forum
Mycroft Blog

",mycroft mycroft is a hackabl open sourc voic assist tabl of content tabl of content get start run mycroft use mycroft home devic and account manag skill behind the scene pair inform configur use mycroft without home api key servic use mycroft behind a proxi use mycroft behind a proxi without authent use mycroft behind an authent proxi get involv link get start first get the code on your system the simplest method is via git git instal instruct cd git clone httpsgithubcommycroftaimycroftcoregit cd mycroftcor bash dev_setupsh thi script set up depend and a virtualenv if run in an environ besid ubuntudebian arch or fedora you may need to manual instal packag as instruct by dev_setupsh note the default branch for thi repositori is dev which should be consid a workinprogress if you want to clone a more stabl version switch over to the master branch run mycroft mycroft provid startmycroftsh to perform common task thi script use a virtualenv creat by dev_setupsh assum you instal mycroftcor in your home directori run cd mycroftcor startmycroftsh debug the debug command will start the background servic microphon listen skill messagebu and audio subsystem as well as bring up a textbas command line interfac cli you can use to interact with mycroft and see the content of the variou log altern you can run startmycroftsh all to begin the servic without the command line interfac later you can bring up the cli use startmycroftsh cli the background servic can be stop as a group with stopmycroftsh use mycroft home devic and account manag mycroft ai inc maintain a devic and account manag system known as mycroft home develop may sign up at httpshomemycroftai by default mycroftcor is configur to use home by say hey mycroft pair my devic or ani other request verbal request you will be inform that your devic need to be pair mycroft will speak a 6digit code which you can enter into the pair page within the mycroft home site onc pair your unit will use mycroft api key for servic such as speechtotext stt weather and variou other skill skill mycroft is noth without skill there are a hand of default skill that are download automat to your optmycroftskil directori but most need to be instal explicitli see the skill repo to discov skill made by other and pleas share your own interest work behind the scene pair inform pair inform gener by regist with home is store in mycroftidentityidentity2json do not share thi with other configur mycroft configur consist of 4 possibl locat mycroftcoremycroftconfigurationmycroftconfdefault mycroft home remot etcmycroftmycroftconfmachin homemycroftmycroftconfus when the configur loader start it look in these locat in thi order and load all configur key that exist in multipl configur file will be overridden by the last file to contain the valu thi process result in a minim amount be written for a specif devic and user without modifi default distribut file use mycroft without home if you do not wish to use the mycroft home servic befor start mycroft for the first time creat homemycroftmycroftconf with the follow content skill blacklisted_skil mycroftconfigurationmycroftai mycroftpairingmycroftai mycroft will then be unabl to perform speechtotext convers so youll need to set that up as well use one of the stt engin mycroft support you may insert your own api key into the configur file list abov in configur for exampl to insert the api key for the weather skill creat a new json key in the configur file like so other configur set weatherskil api_key insert your api key here api key servic these are the key current use in mycroft core stt api googl stt googl cloud speech weather skill api openweathermap wolframalpha skill use mycroft behind a proxi mani school univers and workplac run a proxi on their network if you need to type in a usernam and password to access the extern internet then you are like behind a proxi if you plan to use mycroft behind a proxi then you will need to do an addit configur step note in order to complet thi step you will need to know the hostnam and port for the proxi server your network administr will be abl to provid these detail your network administr may want inform on what type of traffic mycroft will be use we use http traffic on port 443 primarili for access restbas api use mycroft behind a proxi without authent if you are use mycroft behind a proxi without authent add the follow environ variabl chang the proxy_hostnamecom and proxy_port for the valu for your network these command are execut from the linux command line interfac cli export http_proxyhttpproxy_hostnamecomproxy_port export https_porthttpproxy_hostnamecomproxy_port export no_proxylocalhost127001localaddresslocaldomaincom00001 use mycroft behind an authent proxi if you are behind a proxi which requir authent add the follow environ variabl chang the proxy_hostnamecom and proxy_port for the valu for your network these command are execut from the linux command line interfac cli export http_proxyhttpuserpasswordproxy_hostnamecomproxy_port export https_porthttpuserpasswordproxy_hostnamecomproxy_port export no_proxylocalhost127001localaddresslocaldomaincom00001 get involv thi is an open sourc project and we would love your help we have prepar a contribut guid to help you get start if thi is your first pr or your not sure where to get start say hi in mycroft chat and a team member would be happi to mentor you join the mycroft forum for question and answer link creat a skill document skill writer api doc releas note mycroft chat mycroft forum mycroft blog,mycroft mycroft is a hackable open source voice assistant table of content table of content getting started running mycroft using mycroft home device and account manager skill behind the scene pairing information configuration using mycroft without home api key service using mycroft behind a proxy using mycroft behind a proxy without authentication using mycroft behind an authenticated proxy getting involved link getting started first get the code on your system the simplest method is via git git installation instruction cd git clone httpsgithubcommycroftaimycroftcoregit cd mycroftcore bash dev_setupsh this script set up dependency and a virtualenv if running in an environment besides ubuntudebian arch or fedora you may need to manually install package a instructed by dev_setupsh note the default branch for this repository is dev which should be considered a workinprogress if you want to clone a more stable version switch over to the master branch running mycroft mycroft provides startmycroftsh to perform common task this script us a virtualenv created by dev_setupsh assuming you installed mycroftcore in your home directory run cd mycroftcore startmycroftsh debug the debug command will start the background service microphone listener skill messagebus and audio subsystem a well a bringing up a textbased command line interface cli you can use to interact with mycroft and see the content of the various log alternatively you can run startmycroftsh all to begin the service without the command line interface later you can bring up the cli using startmycroftsh cli the background service can be stopped a a group with stopmycroftsh using mycroft home device and account manager mycroft ai inc maintains a device and account management system known a mycroft home developer may sign up at httpshomemycroftai by default mycroftcore is configured to use home by saying hey mycroft pair my device or any other request verbal request you will be informed that your device need to be paired mycroft will speak a 6digit code which you can enter into the pairing page within the mycroft home site once paired your unit will use mycroft api key for service such a speechtotext stt weather and various other skill skill mycroft is nothing without skill there are a handful of default skill that are downloaded automatically to your optmycroftskills directory but most need to be installed explicitly see the skill repo to discover skill made by others and please share your own interesting work behind the scene pairing information pairing information generated by registering with home is stored in mycroftidentityidentity2json do not share this with others configuration mycroft configuration consists of 4 possible location mycroftcoremycroftconfigurationmycroftconfdefaults mycroft home remote etcmycroftmycroftconfmachine homemycroftmycroftconfuser when the configuration loader start it look in these location in this order and load all configuration key that exist in multiple configuration file will be overridden by the last file to contain the value this process result in a minimal amount being written for a specific device and user without modifying default distribution file using mycroft without home if you do not wish to use the mycroft home service before starting mycroft for the first time create homemycroftmycroftconf with the following content skill blacklisted_skills mycroftconfigurationmycroftai mycroftpairingmycroftai mycroft will then be unable to perform speechtotext conversion so youll need to set that up a well using one of the stt engine mycroft support you may insert your own api key into the configuration file listed above in configuration for example to insert the api key for the weather skill create a new json key in the configuration file like so other configuration setting weatherskill api_key insert your api key here api key service these are the key currently used in mycroft core stt api google stt google cloud speech weather skill api openweathermap wolframalpha skill using mycroft behind a proxy many school university and workplace run a proxy on their network if you need to type in a username and password to access the external internet then you are likely behind a proxy if you plan to use mycroft behind a proxy then you will need to do an additional configuration step note in order to complete this step you will need to know the hostname and port for the proxy server your network administrator will be able to provide these detail your network administrator may want information on what type of traffic mycroft will be using we use http traffic on port 443 primarily for accessing restbased apis using mycroft behind a proxy without authentication if you are using mycroft behind a proxy without authentication add the following environment variable changing the proxy_hostnamecom and proxy_port for the value for your network these command are executed from the linux command line interface cli export http_proxyhttpproxy_hostnamecomproxy_port export https_porthttpproxy_hostnamecomproxy_port export no_proxylocalhost127001localaddresslocaldomaincom00001 using mycroft behind an authenticated proxy if you are behind a proxy which requires authentication add the following environment variable changing the proxy_hostnamecom and proxy_port for the value for your network these command are executed from the linux command line interface cli export http_proxyhttpuserpasswordproxy_hostnamecomproxy_port export https_porthttpuserpasswordproxy_hostnamecomproxy_port export no_proxylocalhost127001localaddresslocaldomaincom00001 getting involved this is an open source project and we would love your help we have prepared a contributing guide to help you get started if this is your first pr or youre not sure where to get started say hi in mycroft chat and a team member would be happy to mentor you join the mycroft forum for question and answer link creating a skill documentation skill writer api doc release note mycroft chat mycroft forum mycroft blog,mycroft mycroft hackable open source voice assistant table content table content getting started running mycroft using mycroft home device account manager skill behind scene pairing information configuration using mycroft without home api key service using mycroft behind proxy using mycroft behind proxy without authentication using mycroft behind authenticated proxy getting involved link getting started first get code system simplest method via git git installation instruction cd git clone httpsgithubcommycroftaimycroftcoregit cd mycroftcore bash dev_setupsh script set dependency virtualenv running environment besides ubuntudebian arch fedora may need manually install package instructed dev_setupsh note default branch repository dev considered workinprogress want clone stable version switch master branch running mycroft mycroft provides startmycroftsh perform common task script us virtualenv created dev_setupsh assuming installed mycroftcore home directory run cd mycroftcore startmycroftsh debug debug command start background service microphone listener skill messagebus audio subsystem well bringing textbased command line interface cli use interact mycroft see content various log alternatively run startmycroftsh begin service without command line interface later bring cli using startmycroftsh cli background service stopped group stopmycroftsh using mycroft home device account manager mycroft ai inc maintains device account management system known mycroft home developer may sign httpshomemycroftai default mycroftcore configured use home saying hey mycroft pair device request verbal request informed device need paired mycroft speak 6digit code enter pairing page within mycroft home site paired unit use mycroft api key service speechtotext stt weather various skill skill mycroft nothing without skill handful default skill downloaded automatically optmycroftskills directory need installed explicitly see skill repo discover skill made others please share interesting work behind scene pairing information pairing information generated registering home stored mycroftidentityidentity2json share others configuration mycroft configuration consists 4 possible location mycroftcoremycroftconfigurationmycroftconfdefaults mycroft home remote etcmycroftmycroftconfmachine homemycroftmycroftconfuser configuration loader start look location order load configuration key exist multiple configuration file overridden last file contain value process result minimal amount written specific device user without modifying default distribution file using mycroft without home wish use mycroft home service starting mycroft first time create homemycroftmycroftconf following content skill blacklisted_skills mycroftconfigurationmycroftai mycroftpairingmycroftai mycroft unable perform speechtotext conversion youll need set well using one stt engine mycroft support may insert api key configuration file listed configuration example insert api key weather skill create new json key configuration file like configuration setting weatherskill api_key insert api key api key service key currently used mycroft core stt api google stt google cloud speech weather skill api openweathermap wolframalpha skill using mycroft behind proxy many school university workplace run proxy network need type username password access external internet likely behind proxy plan use mycroft behind proxy need additional configuration step note order complete step need know hostname port proxy server network administrator able provide detail network administrator may want information type traffic mycroft using use http traffic port 443 primarily accessing restbased apis using mycroft behind proxy without authentication using mycroft behind proxy without authentication add following environment variable changing proxy_hostnamecom proxy_port value network command executed linux command line interface cli export http_proxyhttpproxy_hostnamecomproxy_port export https_porthttpproxy_hostnamecomproxy_port export no_proxylocalhost127001localaddresslocaldomaincom00001 using mycroft behind authenticated proxy behind proxy requires authentication add following environment variable changing proxy_hostnamecom proxy_port value network command executed linux command line interface cli export http_proxyhttpuserpasswordproxy_hostnamecomproxy_port export https_porthttpuserpasswordproxy_hostnamecomproxy_port export no_proxylocalhost127001localaddresslocaldomaincom00001 getting involved open source project would love help prepared contributing guide help get started first pr youre sure get started say hi mycroft chat team member would happy mentor join mycroft forum question answer link creating skill documentation skill writer api doc release note mycroft chat mycroft forum mycroft blog
Python ,"Artificial Intelligence with Python
This is the code repository for Artificial Intelligence with Python, published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.
About the Book
During the course of this book, you will find out how to make informed decisions about what algorithms to use in a given context. Starting from the basics of Artificial Intelligence, you will learn how to develop various building blocks using different data mining techniques. You will see how to implement different algorithms to get the best possible results, and will understand how to apply them to real-world scenarios. If you want to add an intelligence layer to any application that’s based on images, text, stock market, or some other form of data, this exciting book on Artificial Intelligence will definitely be your guide!
##Instructions and Navigation
All of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter02.
The code will look like the following:
A block of code is set as follows:

[default]
exten => s,1,Dial(Zap/1|30)
exten => s,2,Voicemail(u100)
exten => s,102,Voicemail(b100)
exten => i,1,Voicemail(s0)

This book is focused on artificial intelligence in Python as opposed to the Python itself. We have used Python 3 to build various applications. We focus on how to utilize various Python libraries in the best possible way to build real world applications. In that spirit, we have tried to keep all of the code as friendly and readable as possible. We feel that this will enable our readers to easily understand the code and readily use it in different scenarios.
Related Products


Deep Learning with Python [Video]


Learning IPython for Interactive Computing and Data Visualization


Python High Performance Programming


Suggestions and Feedback
Click here if you have any feedback or suggestions.
",artifici intellig with python thi is the code repositori for artifici intellig with python publish by packt it contain all the support project file necessari to work through the book from start to finish about the book dure the cours of thi book you will find out how to make inform decis about what algorithm to use in a given context start from the basic of artifici intellig you will learn how to develop variou build block use differ data mine techniqu you will see how to implement differ algorithm to get the best possibl result and will understand how to appli them to realworld scenario if you want to add an intellig layer to ani applic that base on imag text stock market or some other form of data thi excit book on artifici intellig will definit be your guid instruct and navig all of the code is organ into folder each folder start with a number follow by the applic name for exampl chapter02 the code will look like the follow a block of code is set as follow default exten s1dialzap130 exten s2voicemailu100 exten s102voicemailb100 exten i1voicemails0 thi book is focus on artifici intellig in python as oppos to the python itself we have use python 3 to build variou applic we focu on how to util variou python librari in the best possibl way to build real world applic in that spirit we have tri to keep all of the code as friendli and readabl as possibl we feel that thi will enabl our reader to easili understand the code and readili use it in differ scenario relat product deep learn with python video learn ipython for interact comput and data visual python high perform program suggest and feedback click here if you have ani feedback or suggest,artificial intelligence with python this is the code repository for artificial intelligence with python published by packt it contains all the supporting project file necessary to work through the book from start to finish about the book during the course of this book you will find out how to make informed decision about what algorithm to use in a given context starting from the basic of artificial intelligence you will learn how to develop various building block using different data mining technique you will see how to implement different algorithm to get the best possible result and will understand how to apply them to realworld scenario if you want to add an intelligence layer to any application thats based on image text stock market or some other form of data this exciting book on artificial intelligence will definitely be your guide instruction and navigation all of the code is organized into folder each folder start with a number followed by the application name for example chapter02 the code will look like the following a block of code is set a follows default exten s1dialzap130 exten s2voicemailu100 exten s102voicemailb100 exten i1voicemails0 this book is focused on artificial intelligence in python a opposed to the python itself we have used python 3 to build various application we focus on how to utilize various python library in the best possible way to build real world application in that spirit we have tried to keep all of the code a friendly and readable a possible we feel that this will enable our reader to easily understand the code and readily use it in different scenario related product deep learning with python video learning ipython for interactive computing and data visualization python high performance programming suggestion and feedback click here if you have any feedback or suggestion,artificial intelligence python code repository artificial intelligence python published packt contains supporting project file necessary work book start finish book course book find make informed decision algorithm use given context starting basic artificial intelligence learn develop various building block using different data mining technique see implement different algorithm get best possible result understand apply realworld scenario want add intelligence layer application thats based image text stock market form data exciting book artificial intelligence definitely guide instruction navigation code organized folder folder start number followed application name example chapter02 code look like following block code set follows default exten s1dialzap130 exten s2voicemailu100 exten s102voicemailb100 exten i1voicemails0 book focused artificial intelligence python opposed python used python 3 build various application focus utilize various python library best possible way build real world application spirit tried keep code friendly readable possible feel enable reader easily understand code readily use different scenario related product deep learning python video learning ipython interactive computing data visualization python high performance programming suggestion feedback click feedback suggestion
Python ,"Artificial Intelligence with Python
This is the code repository for Artificial Intelligence with Python, published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.
About the Book
During the course of this book, you will find out how to make informed decisions about what algorithms to use in a given context. Starting from the basics of Artificial Intelligence, you will learn how to develop various building blocks using different data mining techniques. You will see how to implement different algorithms to get the best possible results, and will understand how to apply them to real-world scenarios. If you want to add an intelligence layer to any application that’s based on images, text, stock market, or some other form of data, this exciting book on Artificial Intelligence will definitely be your guide!
##Instructions and Navigation
All of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter02.
The code will look like the following:
A block of code is set as follows:

[default]
exten => s,1,Dial(Zap/1|30)
exten => s,2,Voicemail(u100)
exten => s,102,Voicemail(b100)
exten => i,1,Voicemail(s0)

This book is focused on artificial intelligence in Python as opposed to the Python itself. We have used Python 3 to build various applications. We focus on how to utilize various Python libraries in the best possible way to build real world applications. In that spirit, we have tried to keep all of the code as friendly and readable as possible. We feel that this will enable our readers to easily understand the code and readily use it in different scenarios.
Related Products


Deep Learning with Python [Video]


Learning IPython for Interactive Computing and Data Visualization


Python High Performance Programming


Suggestions and Feedback
Click here if you have any feedback or suggestions.
",artifici intellig with python thi is the code repositori for artifici intellig with python publish by packt it contain all the support project file necessari to work through the book from start to finish about the book dure the cours of thi book you will find out how to make inform decis about what algorithm to use in a given context start from the basic of artifici intellig you will learn how to develop variou build block use differ data mine techniqu you will see how to implement differ algorithm to get the best possibl result and will understand how to appli them to realworld scenario if you want to add an intellig layer to ani applic that base on imag text stock market or some other form of data thi excit book on artifici intellig will definit be your guid instruct and navig all of the code is organ into folder each folder start with a number follow by the applic name for exampl chapter02 the code will look like the follow a block of code is set as follow default exten s1dialzap130 exten s2voicemailu100 exten s102voicemailb100 exten i1voicemails0 thi book is focus on artifici intellig in python as oppos to the python itself we have use python 3 to build variou applic we focu on how to util variou python librari in the best possibl way to build real world applic in that spirit we have tri to keep all of the code as friendli and readabl as possibl we feel that thi will enabl our reader to easili understand the code and readili use it in differ scenario relat product deep learn with python video learn ipython for interact comput and data visual python high perform program suggest and feedback click here if you have ani feedback or suggest,artificial intelligence with python this is the code repository for artificial intelligence with python published by packt it contains all the supporting project file necessary to work through the book from start to finish about the book during the course of this book you will find out how to make informed decision about what algorithm to use in a given context starting from the basic of artificial intelligence you will learn how to develop various building block using different data mining technique you will see how to implement different algorithm to get the best possible result and will understand how to apply them to realworld scenario if you want to add an intelligence layer to any application thats based on image text stock market or some other form of data this exciting book on artificial intelligence will definitely be your guide instruction and navigation all of the code is organized into folder each folder start with a number followed by the application name for example chapter02 the code will look like the following a block of code is set a follows default exten s1dialzap130 exten s2voicemailu100 exten s102voicemailb100 exten i1voicemails0 this book is focused on artificial intelligence in python a opposed to the python itself we have used python 3 to build various application we focus on how to utilize various python library in the best possible way to build real world application in that spirit we have tried to keep all of the code a friendly and readable a possible we feel that this will enable our reader to easily understand the code and readily use it in different scenario related product deep learning with python video learning ipython for interactive computing and data visualization python high performance programming suggestion and feedback click here if you have any feedback or suggestion,artificial intelligence python code repository artificial intelligence python published packt contains supporting project file necessary work book start finish book course book find make informed decision algorithm use given context starting basic artificial intelligence learn develop various building block using different data mining technique see implement different algorithm get best possible result understand apply realworld scenario want add intelligence layer application thats based image text stock market form data exciting book artificial intelligence definitely guide instruction navigation code organized folder folder start number followed application name example chapter02 code look like following block code set follows default exten s1dialzap130 exten s2voicemailu100 exten s102voicemailb100 exten i1voicemails0 book focused artificial intelligence python opposed python used python 3 build various application focus utilize various python library best possible way build real world application spirit tried keep code friendly readable possible feel enable reader easily understand code readily use different scenario related product deep learning python video learning ipython interactive computing data visualization python high performance programming suggestion feedback click feedback suggestion
Python ,"
Simple AI
Project home: http://github.com/simpleai-team/simpleai
This lib implements many of the artificial intelligence algorithms described on the book ""Artificial Intelligence, a Modern Approach"", from Stuart Russel and Peter Norvig. We strongly recommend you to read the book, or at least the introductory chapters and the ones related to the components you want to use, because we won't explain the algorithms here.
This implementation takes some of the ideas from the Norvig's implementation (the aima-python lib), but it's made with a more ""pythonic"" approach, and more emphasis on creating a stable, modern, and maintainable version. We are testing the majority of the lib, it's available via pip install, has a standard repo and lib architecture, well documented, respects the python pep8 guidelines, provides only working code (no placeholders for future things), etc. Even the internal code is written with readability in mind, not only the external API.
At this moment, the implementation includes:


Search

Traditional search algorithms (not informed and informed)
Local Search algorithms
Constraint Satisfaction Problems algorithms
Interactive execution viewers for search algorithms (web-based and terminal-based)





Machine Learning

Statistical Classification






Installation
Just get it:
pip install simpleai

And if you want to use the interactive search viewers, also install:
pip install pydot flask

You will need to have pip installed on your system. On linux install the
python-pip package, on windows follow this.
Also, if you are on linux and not working with a virtualenv, remember to use
sudo for both commands (sudo pip install ...).

Examples
Simple AI allows you to define problems and look for the solution with
different strategies. Another samples are in the samples directory, but
here is an easy one.
This problem tries to create the string ""HELLO WORLD"" using the A* algorithm:
from simpleai.search import SearchProblem, astar

GOAL = 'HELLO WORLD'


class HelloProblem(SearchProblem):
    def actions(self, state):
        if len(state) < len(GOAL):
            return list(' ABCDEFGHIJKLMNOPQRSTUVWXYZ')
        else:
            return []

    def result(self, state, action):
        return state + action

    def is_goal(self, state):
        return state == GOAL

    def heuristic(self, state):
        # how far are we from the goal?
        wrong = sum([1 if state[i] != GOAL[i] else 0
                    for i in range(len(state))])
        missing = len(GOAL) - len(state)
        return wrong + missing

problem = HelloProblem(initial_state='')
result = astar(problem)

print(result.state)
print(result.path())

More detailed documentation
You can read the docs online here. Or for offline access, you can clone the project code repository and read them from the docs folder.

Help and discussion
Join us at the Simple AI google group.

Authors

Many people you can find on the contributors section.
Special acknowledgements to Machinalis for the time provided to work on this project. Machinalis also works on some other very interesting projects, like Quepy and more.

",simpl ai project home httpgithubcomsimpleaiteamsimpleai thi lib implement mani of the artifici intellig algorithm describ on the book artifici intellig a modern approach from stuart russel and peter norvig we strongli recommend you to read the book or at least the introductori chapter and the one relat to the compon you want to use becaus we wont explain the algorithm here thi implement take some of the idea from the norvig implement the aimapython lib but it made with a more python approach and more emphasi on creat a stabl modern and maintain version we are test the major of the lib it avail via pip instal ha a standard repo and lib architectur well document respect the python pep8 guidelin provid onli work code no placehold for futur thing etc even the intern code is written with readabl in mind not onli the extern api at thi moment the implement includ search tradit search algorithm not inform and inform local search algorithm constraint satisfact problem algorithm interact execut viewer for search algorithm webbas and terminalbas machin learn statist classif instal just get it pip instal simpleai and if you want to use the interact search viewer also instal pip instal pydot flask you will need to have pip instal on your system on linux instal the pythonpip packag on window follow thi also if you are on linux and not work with a virtualenv rememb to use sudo for both command sudo pip instal exampl simpl ai allow you to defin problem and look for the solut with differ strategi anoth sampl are in the sampl directori but here is an easi one thi problem tri to creat the string hello world use the a algorithm from simpleaisearch import searchproblem astar goal hello world class helloproblemsearchproblem def actionsself state if lenstat lengoal return list abcdefghijklmnopqrstuvwxyz els return def resultself state action return state action def is_goalself state return state goal def heuristicself state how far are we from the goal wrong sum1 if statei goali els 0 for i in rangelenst miss lengoal lenstat return wrong miss problem helloprobleminitial_st result astarproblem printresultst printresultpath more detail document you can read the doc onlin here or for offlin access you can clone the project code repositori and read them from the doc folder help and discuss join us at the simpl ai googl group author mani peopl you can find on the contributor section special acknowledg to machinali for the time provid to work on thi project machinali also work on some other veri interest project like quepi and more,simple ai project home httpgithubcomsimpleaiteamsimpleai this lib implement many of the artificial intelligence algorithm described on the book artificial intelligence a modern approach from stuart russel and peter norvig we strongly recommend you to read the book or at least the introductory chapter and the one related to the component you want to use because we wont explain the algorithm here this implementation take some of the idea from the norvigs implementation the aimapython lib but it made with a more pythonic approach and more emphasis on creating a stable modern and maintainable version we are testing the majority of the lib it available via pip install ha a standard repo and lib architecture well documented respect the python pep8 guideline provides only working code no placeholder for future thing etc even the internal code is written with readability in mind not only the external api at this moment the implementation includes search traditional search algorithm not informed and informed local search algorithm constraint satisfaction problem algorithm interactive execution viewer for search algorithm webbased and terminalbased machine learning statistical classification installation just get it pip install simpleai and if you want to use the interactive search viewer also install pip install pydot flask you will need to have pip installed on your system on linux install the pythonpip package on window follow this also if you are on linux and not working with a virtualenv remember to use sudo for both command sudo pip install example simple ai allows you to define problem and look for the solution with different strategy another sample are in the sample directory but here is an easy one this problem try to create the string hello world using the a algorithm from simpleaisearch import searchproblem astar goal hello world class helloproblemsearchproblem def actionsself state if lenstate lengoal return list abcdefghijklmnopqrstuvwxyz else return def resultself state action return state action def is_goalself state return state goal def heuristicself state how far are we from the goal wrong sum1 if statei goali else 0 for i in rangelenstate missing lengoal lenstate return wrong missing problem helloprobleminitial_state result astarproblem printresultstate printresultpath more detailed documentation you can read the doc online here or for offline access you can clone the project code repository and read them from the doc folder help and discussion join u at the simple ai google group author many people you can find on the contributor section special acknowledgement to machinalis for the time provided to work on this project machinalis also work on some other very interesting project like quepy and more,simple ai project home httpgithubcomsimpleaiteamsimpleai lib implement many artificial intelligence algorithm described book artificial intelligence modern approach stuart russel peter norvig strongly recommend read book least introductory chapter one related component want use wont explain algorithm implementation take idea norvigs implementation aimapython lib made pythonic approach emphasis creating stable modern maintainable version testing majority lib available via pip install standard repo lib architecture well documented respect python pep8 guideline provides working code placeholder future thing etc even internal code written readability mind external api moment implementation includes search traditional search algorithm informed informed local search algorithm constraint satisfaction problem algorithm interactive execution viewer search algorithm webbased terminalbased machine learning statistical classification installation get pip install simpleai want use interactive search viewer also install pip install pydot flask need pip installed system linux install pythonpip package window follow also linux working virtualenv remember use sudo command sudo pip install example simple ai allows define problem look solution different strategy another sample sample directory easy one problem try create string hello world using algorithm simpleaisearch import searchproblem astar goal hello world class helloproblemsearchproblem def actionsself state lenstate lengoal return list abcdefghijklmnopqrstuvwxyz else return def resultself state action return state action def is_goalself state return state goal def heuristicself state far goal wrong sum1 statei goali else 0 rangelenstate missing lengoal lenstate return wrong missing problem helloprobleminitial_state result astarproblem printresultstate printresultpath detailed documentation read doc online offline access clone project code repository read doc folder help discussion join u simple ai google group author many people find contributor section special acknowledgement machinalis time provided work project machinalis also work interesting project like quepy
Python ,"Snake
  
The project focuses on the artificial intelligence of the Snake game. The snake's goal is to eat the food continuously and fill the map with its bodies as soon as possible. Originally, the project was written in C++. It has now been rewritten in Python for a user-friendly GUI and the simplicity in algorithm implementations.
Algorithms >
Experiments
We use two metrics to evaluate the performance of an AI:

Average Length: Average length the snake has grown to (max: 64).
Average Steps: Average steps the snake has moved.

Test results (averaged over 1000 episodes):



Solver
Demo (optimal)
Average Length
Average Steps




Hamilton

63.93
717.83


Greedy

60.15
904.56


DQN(experimental)

24.44
131.69



Installation
Requirements: Python 3.5+ (64-bit) with Tkinter installed.
$ pip3 install -r requirements.txt

# Use -h for more details
$ python3 run.py [-h]

Run unit tests:
$ python3 -m pytest -v

License
See the LICENSE file for license rights and limitations.
",snake the project focus on the artifici intellig of the snake game the snake goal is to eat the food continu and fill the map with it bodi as soon as possibl origin the project wa written in c it ha now been rewritten in python for a userfriendli gui and the simplic in algorithm implement algorithm experi we use two metric to evalu the perform of an ai averag length averag length the snake ha grown to max 64 averag step averag step the snake ha move test result averag over 1000 episod solver demo optim averag length averag step hamilton 6393 71783 greedi 6015 90456 dqnexperiment 2444 13169 instal requir python 35 64bit with tkinter instal pip3 instal r requirementstxt use h for more detail python3 runpi h run unit test python3 m pytest v licens see the licens file for licens right and limit,snake the project focus on the artificial intelligence of the snake game the snake goal is to eat the food continuously and fill the map with it body a soon a possible originally the project wa written in c it ha now been rewritten in python for a userfriendly gui and the simplicity in algorithm implementation algorithm experiment we use two metric to evaluate the performance of an ai average length average length the snake ha grown to max 64 average step average step the snake ha moved test result averaged over 1000 episode solver demo optimal average length average step hamilton 6393 71783 greedy 6015 90456 dqnexperimental 2444 13169 installation requirement python 35 64bit with tkinter installed pip3 install r requirementstxt use h for more detail python3 runpy h run unit test python3 m pytest v license see the license file for license right and limitation,snake project focus artificial intelligence snake game snake goal eat food continuously fill map body soon possible originally project written c rewritten python userfriendly gui simplicity algorithm implementation algorithm experiment use two metric evaluate performance ai average length average length snake grown max 64 average step average step snake moved test result averaged 1000 episode solver demo optimal average length average step hamilton 6393 71783 greedy 6015 90456 dqnexperimental 2444 13169 installation requirement python 35 64bit tkinter installed pip3 install r requirementstxt use h detail python3 runpy h run unit test python3 pytest v license see license file license right limitation
Python ,"





Non-tech crash course into Operação Serenata de Amor
Tech crash course into Operação Serenata de Amor
Contributing with code and tech skills
Supporting
Acknowledgments

Non-tech crash course into Operação Serenata de Amor
What
Serenata de Amor is an open project using artificial intelligence for social control of public administration.
Who
We are a group of people who believes in power to the people motto. We are also part of the Data Science for Civic Innovation Programme from Open Knowledge Brasil.
Among founders and long-term members, we can list a group of eight people – plus numerous contributors from the open source and open knowledge communities:  Tatiana Balachova, Felipe Cabral, Eduardo Cuducos,  Irio Musskopf, Bruno Pazzim, Ana Schwendler, Jessica Temporal, Yasodara Córdova and Pedro Vilanova.
How
Similar to organizations like Google, Facebook, and Netflix, we use technology to track government spendings and make open data accessible for everyone. We started looking into data from the Chamber of Deputies (Brazilian lower house) but we expanded to the Federal Senate (Brazilian upper house) and to municipalities.
When
Irio had the main ideas for the project in early 2016. For a few months, he experimented and gathered people around the project. September, 2016 marks the launching of our first crowd funding. Since then, we have been creating open source technological products and tools, as well as high quality content on civic tech on our Facebook and Medium.
Where
We have no non-virtual headquarters, but we work remotely everyday. Most of our ideas are crafted to work in any country that offers open data, but our main implementations focus in Brazil.
Why
Empowering citizens with data is important: people talk about smart cities, surveillance and privacy. We prefer to focus on smart citizens, accountability and open knowledge.
Tech crash course into Operação Serenata de Amor
What
Serenata de Amor develops open source tools to make it easy for people to use open data. The focus is to gather relevant insights and share them in an accessible interface. Through this interface, we invite citizens to dialogue with politicians, state and government about public spendings.
Who
Serenata's main role is played by Rosie: she is an artificial intelligence who analyzes Brazilian congresspeople expenses while they are in office. Rosie can find suspicious spendings and engage citizens in the discussion about these findings. She's on Twitter.
To allow people to visualize and make sense of data Rosie generates, we have created Jarbas. On this website, users can browse congresspeople expenses and get details about each of the suspicions. It is the starting point to validate a suspicion.
How
We have two main repositories on GitHub. This is the main repo and hosts Rosie and Jarbas. In addition, we have the toolbox - a pip installable package. Yet there are experimental notebooks maintained by the community and our static webpage.
When
Despite all these players acting together, the core part of the job is ran manually from time to time. The only part that is always online is Jarbas – freely serving a wide range of information about public expenditure 24/7.
Roughly once a month, we manually run Rosie and update Jarbas. A few times per year, we upload versioned datasets accessible via the toolbox – but we encourage you to use the toolbox to generate fresh datasets whenever you need.
Where
Jarbas is running in Digital Ocean droplets, and deployed using the Docker Cloud architecture.
Why
The answer to most technical why questions is because that is what we had in the past and enabled us to deliver fast. We acknowledge that this is not the best stack ever, but it has brought us here.
Contributing with code and tech skills
Make sure you have read the Tech crash course on this page. Next, check out our contributing guide.
Supporting

Join our recurring crowd funding campaign on Apoia.se
Donate via Bitcoin to 1Gbvfjmjvur7qwbwNFdPSNDgx66KSdVB5b
Follow, share and interact with us on Facebook
Follow, retweet and join Rosie on Twitter to interact with your favourite congresspeople

Acknowledgments
 
",nontech crash cours into operao serenata de amor tech crash cours into operao serenata de amor contribut with code and tech skill support acknowledg nontech crash cours into operao serenata de amor what serenata de amor is an open project use artifici intellig for social control of public administr who we are a group of peopl who believ in power to the peopl motto we are also part of the data scienc for civic innov programm from open knowledg brasil among founder and longterm member we can list a group of eight peopl plu numer contributor from the open sourc and open knowledg commun tatiana balachova felip cabral eduardo cuduco irio musskopf bruno pazzim ana schwendler jessica tempor yasodara crdova and pedro vilanova how similar to organ like googl facebook and netflix we use technolog to track govern spend and make open data access for everyon we start look into data from the chamber of deputi brazilian lower hous but we expand to the feder senat brazilian upper hous and to municip when irio had the main idea for the project in earli 2016 for a few month he experi and gather peopl around the project septemb 2016 mark the launch of our first crowd fund sinc then we have been creat open sourc technolog product and tool as well as high qualiti content on civic tech on our facebook and medium where we have no nonvirtu headquart but we work remot everyday most of our idea are craft to work in ani countri that offer open data but our main implement focu in brazil whi empow citizen with data is import peopl talk about smart citi surveil and privaci we prefer to focu on smart citizen account and open knowledg tech crash cours into operao serenata de amor what serenata de amor develop open sourc tool to make it easi for peopl to use open data the focu is to gather relev insight and share them in an access interfac through thi interfac we invit citizen to dialogu with politician state and govern about public spend who serenata main role is play by rosi she is an artifici intellig who analyz brazilian congresspeopl expens while they are in offic rosi can find suspici spend and engag citizen in the discuss about these find she on twitter to allow peopl to visual and make sens of data rosi gener we have creat jarba on thi websit user can brows congresspeopl expens and get detail about each of the suspicion it is the start point to valid a suspicion how we have two main repositori on github thi is the main repo and host rosi and jarba in addit we have the toolbox a pip instal packag yet there are experiment notebook maintain by the commun and our static webpag when despit all these player act togeth the core part of the job is ran manual from time to time the onli part that is alway onlin is jarba freeli serv a wide rang of inform about public expenditur 247 roughli onc a month we manual run rosi and updat jarba a few time per year we upload version dataset access via the toolbox but we encourag you to use the toolbox to gener fresh dataset whenev you need where jarba is run in digit ocean droplet and deploy use the docker cloud architectur whi the answer to most technic whi question is becaus that is what we had in the past and enabl us to deliv fast we acknowledg that thi is not the best stack ever but it ha brought us here contribut with code and tech skill make sure you have read the tech crash cours on thi page next check out our contribut guid support join our recur crowd fund campaign on apoias donat via bitcoin to 1gbvfjmjvur7qwbwnfdpsndgx66ksdvb5b follow share and interact with us on facebook follow retweet and join rosi on twitter to interact with your favourit congresspeopl acknowledg,nontech crash course into operao serenata de amor tech crash course into operao serenata de amor contributing with code and tech skill supporting acknowledgment nontech crash course into operao serenata de amor what serenata de amor is an open project using artificial intelligence for social control of public administration who we are a group of people who belief in power to the people motto we are also part of the data science for civic innovation programme from open knowledge brasil among founder and longterm member we can list a group of eight people plus numerous contributor from the open source and open knowledge community tatiana balachova felipe cabral eduardo cuducos irio musskopf bruno pazzim ana schwendler jessica temporal yasodara crdova and pedro vilanova how similar to organization like google facebook and netflix we use technology to track government spending and make open data accessible for everyone we started looking into data from the chamber of deputy brazilian lower house but we expanded to the federal senate brazilian upper house and to municipality when irio had the main idea for the project in early 2016 for a few month he experimented and gathered people around the project september 2016 mark the launching of our first crowd funding since then we have been creating open source technological product and tool a well a high quality content on civic tech on our facebook and medium where we have no nonvirtual headquarters but we work remotely everyday most of our idea are crafted to work in any country that offer open data but our main implementation focus in brazil why empowering citizen with data is important people talk about smart city surveillance and privacy we prefer to focus on smart citizen accountability and open knowledge tech crash course into operao serenata de amor what serenata de amor develops open source tool to make it easy for people to use open data the focus is to gather relevant insight and share them in an accessible interface through this interface we invite citizen to dialogue with politician state and government about public spending who serenatas main role is played by rosie she is an artificial intelligence who analyzes brazilian congresspeople expense while they are in office rosie can find suspicious spending and engage citizen in the discussion about these finding shes on twitter to allow people to visualize and make sense of data rosie generates we have created jarbas on this website user can browse congresspeople expense and get detail about each of the suspicion it is the starting point to validate a suspicion how we have two main repository on github this is the main repo and host rosie and jarbas in addition we have the toolbox a pip installable package yet there are experimental notebook maintained by the community and our static webpage when despite all these player acting together the core part of the job is ran manually from time to time the only part that is always online is jarbas freely serving a wide range of information about public expenditure 247 roughly once a month we manually run rosie and update jarbas a few time per year we upload versioned datasets accessible via the toolbox but we encourage you to use the toolbox to generate fresh datasets whenever you need where jarbas is running in digital ocean droplet and deployed using the docker cloud architecture why the answer to most technical why question is because that is what we had in the past and enabled u to deliver fast we acknowledge that this is not the best stack ever but it ha brought u here contributing with code and tech skill make sure you have read the tech crash course on this page next check out our contributing guide supporting join our recurring crowd funding campaign on apoiase donate via bitcoin to 1gbvfjmjvur7qwbwnfdpsndgx66ksdvb5b follow share and interact with u on facebook follow retweet and join rosie on twitter to interact with your favourite congresspeople acknowledgment,nontech crash course operao serenata de amor tech crash course operao serenata de amor contributing code tech skill supporting acknowledgment nontech crash course operao serenata de amor serenata de amor open project using artificial intelligence social control public administration group people belief power people motto also part data science civic innovation programme open knowledge brasil among founder longterm member list group eight people plus numerous contributor open source open knowledge community tatiana balachova felipe cabral eduardo cuducos irio musskopf bruno pazzim ana schwendler jessica temporal yasodara crdova pedro vilanova similar organization like google facebook netflix use technology track government spending make open data accessible everyone started looking data chamber deputy brazilian lower house expanded federal senate brazilian upper house municipality irio main idea project early 2016 month experimented gathered people around project september 2016 mark launching first crowd funding since creating open source technological product tool well high quality content civic tech facebook medium nonvirtual headquarters work remotely everyday idea crafted work country offer open data main implementation focus brazil empowering citizen data important people talk smart city surveillance privacy prefer focus smart citizen accountability open knowledge tech crash course operao serenata de amor serenata de amor develops open source tool make easy people use open data focus gather relevant insight share accessible interface interface invite citizen dialogue politician state government public spending serenatas main role played rosie artificial intelligence analyzes brazilian congresspeople expense office rosie find suspicious spending engage citizen discussion finding shes twitter allow people visualize make sense data rosie generates created jarbas website user browse congresspeople expense get detail suspicion starting point validate suspicion two main repository github main repo host rosie jarbas addition toolbox pip installable package yet experimental notebook maintained community static webpage despite player acting together core part job ran manually time time part always online jarbas freely serving wide range information public expenditure 247 roughly month manually run rosie update jarbas time per year upload versioned datasets accessible via toolbox encourage use toolbox generate fresh datasets whenever need jarbas running digital ocean droplet deployed using docker cloud architecture answer technical question past enabled u deliver fast acknowledge best stack ever brought u contributing code tech skill make sure read tech crash course page next check contributing guide supporting join recurring crowd funding campaign apoiase donate via bitcoin 1gbvfjmjvur7qwbwnfdpsndgx66ksdvb5b follow share interact u facebook follow retweet join rosie twitter interact favourite congresspeople acknowledgment
Python ,"easyAI
EasyAI (full documentation here) is a pure-Python artificial intelligence framework for two-players abstract games such as Tic Tac Toe, Connect 4, Reversi, etc.
It makes it easy to define the mechanisms of a game, and play against the computer or solve the game.
Under the hood, the AI is a Negamax algorithm with alpha-beta pruning and transposition tables as described on Wikipedia.

Installation
If you have pip installed, type this in a terminal
sudo pip install easyAI

Otherwise, dowload the source code (for instance on Github), unzip everything into one folder and in this folder, in a terminal, type
sudo python setup.py install

Additionnally you will need to install Numpy to be able to run some of the examples.

A quick example
Let us define the rules of a game and start a match against the AI:
from easyAI import TwoPlayersGame, Human_Player, AI_Player, Negamax

class GameOfBones( TwoPlayersGame ):
    """""" In turn, the players remove one, two or three bones from a
    pile of bones. The player who removes the last bone loses. """"""

    def __init__(self, players):
        self.players = players
        self.pile = 20 # start with 20 bones in the pile
        self.nplayer = 1 # player 1 starts

    def possible_moves(self): return ['1','2','3']
    def make_move(self,move): self.pile -= int(move) # remove bones.
    def win(self): return self.pile<=0 # opponent took the last bone ?
    def is_over(self): return self.win() # Game stops when someone wins.
    def show(self): print (""%d bones left in the pile"" % self.pile)
    def scoring(self): return 100 if game.win() else 0 # For the AI

# Start a match (and store the history of moves when it ends)
ai = Negamax(13) # The AI will think 13 moves in advance
game = GameOfBones( [ Human_Player(), AI_Player(ai) ] )
history = game.play()
Result:
20 bones left in the pile

Player 1 what do you play ? 3

Move #1: player 1 plays 3 :
17 bones left in the pile

Move #2: player 2 plays 1 :
16 bones left in the pile

Player 1 what do you play ?


Solving the game
Let us now solve the game:
from easyAI import id_solve
r,d,m = id_solve(GameOfBones, ai_depths=range(2,20), win_score=100)
We obtain r=1, meaning that if both players play perfectly, the first player to play can always win (-1 would have meant always lose), d=10, which means that the wins will be in ten moves (i.e. 5 moves per player) or less, and m='3', which indicates that the first player's first move should be '3'.
These computations can be sped up using a transposition table which will store the situations encountered and the best moves for each:
tt = TT()
GameOfBones.ttentry = lambda game : game.pile # key for the table
r,d,m = id_solve(GameOfBones, range(2,20), win_score=100, tt=tt)
After these lines are run the variable tt contains a transposition table storing the possible situations (here, the possible sizes of the pile) and the optimal moves to perform. With tt you can play perfectly without thinking:
.. code:: python


game = GameOfBones( [  AI_Player( tt ), Human_Player() ] )
game.play() # you will always lose this game :)

Contribute !
EasyAI is an open source software originally written by Zulko and released under the MIT licence. It could do with some improvements, so if your are a Python/AI guru maybe you can contribute through Github . Some ideas: AI algos for incomplete information games, better game solving strategies, (efficient) use of databases to store moves,  AI algorithms using parallelisation.
For troubleshooting and bug reports, the best for now is to ask on Github.

Maintainers

Zulko (owner)
JohnAD

",easyai easyai full document here is a purepython artifici intellig framework for twoplay abstract game such as tic tac toe connect 4 reversi etc it make it easi to defin the mechan of a game and play against the comput or solv the game under the hood the ai is a negamax algorithm with alphabeta prune and transposit tabl as describ on wikipedia instal if you have pip instal type thi in a termin sudo pip instal easyai otherwis dowload the sourc code for instanc on github unzip everyth into one folder and in thi folder in a termin type sudo python setuppi instal additionn you will need to instal numpi to be abl to run some of the exampl a quick exampl let us defin the rule of a game and start a match against the ai from easyai import twoplayersgam human_play ai_play negamax class gameofbon twoplayersgam in turn the player remov one two or three bone from a pile of bone the player who remov the last bone lose def __init__self player selfplay player selfpil 20 start with 20 bone in the pile selfnplay 1 player 1 start def possible_movesself return 123 def make_moveselfmov selfpil intmov remov bone def winself return selfpile0 oppon took the last bone def is_overself return selfwin game stop when someon win def showself print d bone left in the pile selfpil def scoringself return 100 if gamewin els 0 for the ai start a match and store the histori of move when it end ai negamax13 the ai will think 13 move in advanc game gameofbon human_play ai_playerai histori gameplay result 20 bone left in the pile player 1 what do you play 3 move 1 player 1 play 3 17 bone left in the pile move 2 player 2 play 1 16 bone left in the pile player 1 what do you play solv the game let us now solv the game from easyai import id_solv rdm id_solvegameofbon ai_depthsrange220 win_score100 we obtain r1 mean that if both player play perfectli the first player to play can alway win 1 would have meant alway lose d10 which mean that the win will be in ten move ie 5 move per player or less and m3 which indic that the first player first move should be 3 these comput can be sped up use a transposit tabl which will store the situat encount and the best move for each tt tt gameofbonesttentri lambda game gamepil key for the tabl rdm id_solvegameofbon range220 win_score100 tttt after these line are run the variabl tt contain a transposit tabl store the possibl situat here the possibl size of the pile and the optim move to perform with tt you can play perfectli without think code python game gameofbon ai_play tt human_play gameplay you will alway lose thi game contribut easyai is an open sourc softwar origin written by zulko and releas under the mit licenc it could do with some improv so if your are a pythonai guru mayb you can contribut through github some idea ai algo for incomplet inform game better game solv strategi effici use of databas to store move ai algorithm use parallelis for troubleshoot and bug report the best for now is to ask on github maintain zulko owner johnad,easyai easyai full documentation here is a purepython artificial intelligence framework for twoplayers abstract game such a tic tac toe connect 4 reversi etc it make it easy to define the mechanism of a game and play against the computer or solve the game under the hood the ai is a negamax algorithm with alphabeta pruning and transposition table a described on wikipedia installation if you have pip installed type this in a terminal sudo pip install easyai otherwise dowload the source code for instance on github unzip everything into one folder and in this folder in a terminal type sudo python setuppy install additionnally you will need to install numpy to be able to run some of the example a quick example let u define the rule of a game and start a match against the ai from easyai import twoplayersgame human_player ai_player negamax class gameofbones twoplayersgame in turn the player remove one two or three bone from a pile of bone the player who remove the last bone loses def __init__self player selfplayers player selfpile 20 start with 20 bone in the pile selfnplayer 1 player 1 start def possible_movesself return 123 def make_moveselfmove selfpile intmove remove bone def winself return selfpile0 opponent took the last bone def is_overself return selfwin game stop when someone win def showself print d bone left in the pile selfpile def scoringself return 100 if gamewin else 0 for the ai start a match and store the history of move when it end ai negamax13 the ai will think 13 move in advance game gameofbones human_player ai_playerai history gameplay result 20 bone left in the pile player 1 what do you play 3 move 1 player 1 play 3 17 bone left in the pile move 2 player 2 play 1 16 bone left in the pile player 1 what do you play solving the game let u now solve the game from easyai import id_solve rdm id_solvegameofbones ai_depthsrange220 win_score100 we obtain r1 meaning that if both player play perfectly the first player to play can always win 1 would have meant always lose d10 which mean that the win will be in ten move ie 5 move per player or le and m3 which indicates that the first player first move should be 3 these computation can be sped up using a transposition table which will store the situation encountered and the best move for each tt tt gameofbonesttentry lambda game gamepile key for the table rdm id_solvegameofbones range220 win_score100 tttt after these line are run the variable tt contains a transposition table storing the possible situation here the possible size of the pile and the optimal move to perform with tt you can play perfectly without thinking code python game gameofbones ai_player tt human_player gameplay you will always lose this game contribute easyai is an open source software originally written by zulko and released under the mit licence it could do with some improvement so if your are a pythonai guru maybe you can contribute through github some idea ai algos for incomplete information game better game solving strategy efficient use of database to store move ai algorithm using parallelisation for troubleshooting and bug report the best for now is to ask on github maintainer zulko owner johnad,easyai easyai full documentation purepython artificial intelligence framework twoplayers abstract game tic tac toe connect 4 reversi etc make easy define mechanism game play computer solve game hood ai negamax algorithm alphabeta pruning transposition table described wikipedia installation pip installed type terminal sudo pip install easyai otherwise dowload source code instance github unzip everything one folder folder terminal type sudo python setuppy install additionnally need install numpy able run example quick example let u define rule game start match ai easyai import twoplayersgame human_player ai_player negamax class gameofbones twoplayersgame turn player remove one two three bone pile bone player remove last bone loses def __init__self player selfplayers player selfpile 20 start 20 bone pile selfnplayer 1 player 1 start def possible_movesself return 123 def make_moveselfmove selfpile intmove remove bone def winself return selfpile0 opponent took last bone def is_overself return selfwin game stop someone win def showself print bone left pile selfpile def scoringself return 100 gamewin else 0 ai start match store history move end ai negamax13 ai think 13 move advance game gameofbones human_player ai_playerai history gameplay result 20 bone left pile player 1 play 3 move 1 player 1 play 3 17 bone left pile move 2 player 2 play 1 16 bone left pile player 1 play solving game let u solve game easyai import id_solve rdm id_solvegameofbones ai_depthsrange220 win_score100 obtain r1 meaning player play perfectly first player play always win 1 would meant always lose d10 mean win ten move ie 5 move per player le m3 indicates first player first move 3 computation sped using transposition table store situation encountered best move tt tt gameofbonesttentry lambda game gamepile key table rdm id_solvegameofbones range220 win_score100 tttt line run variable tt contains transposition table storing possible situation possible size pile optimal move perform tt play perfectly without thinking code python game gameofbones ai_player tt human_player gameplay always lose game contribute easyai open source software originally written zulko released mit licence could improvement pythonai guru maybe contribute github idea ai algos incomplete information game better game solving strategy efficient use database store move ai algorithm using parallelisation troubleshooting bug report best ask github maintainer zulko owner johnad
Python ,"说明
学习人工智能过程中的一些模式识别作业
",,,
Python ,"PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
Code accompanying CVPR'20 paper of the same title. Paper link: https://arxiv.org/pdf/2003.03808.pdf
NOTE
We have noticed a lot of concern that PULSE will be used to identify individuals whose faces have been blurred out. We want to emphasize that this is impossible - PULSE makes imaginary faces of people who do not exist, which should not be confused for real people. It will not help identify or reconstruct the original image.
We also want to address concerns of bias in PULSE. We have now included a new section in the paper and an accompanying model card directly addressing this bias.




Table of Contents

PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
Table of Contents

What does it do?
Usage

Prereqs
Data
Applying PULSE





What does it do?
Given a low-resolution input image, PULSE searches the outputs of a generative model (here, StyleGAN) for high-resolution images that are perceptually realistic and downscale correctly.

Usage
The main file of interest for applying PULSE is run.py. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.
Prereqs
You will need to install cmake first (required for dlib, which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux and Windows. For the full set of required Python packages, create a Conda environment from the provided YAML, e.g.
conda create -f pulse.yml 

or (Anaconda on Windows):
conda env create -n pulse -f pulse.yml
conda activate pulse

In some environments (e.g. on Windows), you may have to edit the pulse.yml to remove the version specific hash on each dependency and remove any dependency that still throws an error after running conda env create... (such as readline)
dependencies
  - blas=1.0=mkl
  ...

to
dependencies
  - blas=1.0
 ...

Finally, you will need an internet connection the first time you run the code as it will automatically download the relevant pretrained model from Google Drive (if it has already been downloaded, it will use the local copy). In the event that the public Google Drive is out of capacity, add the files to your own Google Drive instead; get the share URL and replace the ID in the https://drive.google.com/uc?=ID links in align_face.py and PULSE.py with the new file ids from the share URL given by your own Drive file.
Data
By default, input data for run.py should be placed in ./input/ (though this can be modified). However, this assumes faces have already been aligned and downscaled. If you have data that is not already in this form, place it in realpics and run align_face.py which will automatically do this for you. (Again, all directories can be changed by command line arguments if more convenient.) You will at this stage pic a downscaling factor.
Note that if your data begins at a low resolution already, downscaling it further will retain very little information. In this case, you may wish to bicubically upsample (usually, to 1024x1024) and allow align_face.py to downscale for you.
Applying PULSE
Once your data is appropriately formatted, all you need to do is
python run.py

Enjoy!
",puls selfsupervis photo upsampl via latent space explor of gener model code accompani cvpr20 paper of the same titl paper link httpsarxivorgpdf200303808pdf note we have notic a lot of concern that puls will be use to identifi individu whose face have been blur out we want to emphas that thi is imposs puls make imaginari face of peopl who do not exist which should not be confus for real peopl it will not help identifi or reconstruct the origin imag we also want to address concern of bia in puls we have now includ a new section in the paper and an accompani model card directli address thi bia tabl of content puls selfsupervis photo upsampl via latent space explor of gener model tabl of content what doe it do usag prereq data appli puls what doe it do given a lowresolut input imag puls search the output of a gener model here stylegan for highresolut imag that are perceptu realist and downscal correctli usag the main file of interest for appli puls is runpi a full list of argument with descript can be found in that file here we describ those relev to get start prereq you will need to instal cmake first requir for dlib which is use for face align current the code onli work with cuda instal and therefor requir an appropri gpu and ha been test on linux and window for the full set of requir python packag creat a conda environ from the provid yaml eg conda creat f pulseyml or anaconda on window conda env creat n puls f pulseyml conda activ puls in some environ eg on window you may have to edit the pulseyml to remov the version specif hash on each depend and remov ani depend that still throw an error after run conda env creat such as readlin depend blas10mkl to depend blas10 final you will need an internet connect the first time you run the code as it will automat download the relev pretrain model from googl drive if it ha alreadi been download it will use the local copi in the event that the public googl drive is out of capac add the file to your own googl drive instead get the share url and replac the id in the httpsdrivegooglecomucid link in align_facepi and pulsepi with the new file id from the share url given by your own drive file data by default input data for runpi should be place in input though thi can be modifi howev thi assum face have alreadi been align and downscal if you have data that is not alreadi in thi form place it in realpic and run align_facepi which will automat do thi for you again all directori can be chang by command line argument if more conveni you will at thi stage pic a downscal factor note that if your data begin at a low resolut alreadi downscal it further will retain veri littl inform in thi case you may wish to bicub upsampl usual to 1024x1024 and allow align_facepi to downscal for you appli puls onc your data is appropri format all you need to do is python runpi enjoy,pulse selfsupervised photo upsampling via latent space exploration of generative model code accompanying cvpr20 paper of the same title paper link httpsarxivorgpdf200303808pdf note we have noticed a lot of concern that pulse will be used to identify individual whose face have been blurred out we want to emphasize that this is impossible pulse make imaginary face of people who do not exist which should not be confused for real people it will not help identify or reconstruct the original image we also want to address concern of bias in pulse we have now included a new section in the paper and an accompanying model card directly addressing this bias table of content pulse selfsupervised photo upsampling via latent space exploration of generative model table of content what doe it do usage prereqs data applying pulse what doe it do given a lowresolution input image pulse search the output of a generative model here stylegan for highresolution image that are perceptually realistic and downscale correctly usage the main file of interest for applying pulse is runpy a full list of argument with description can be found in that file here we describe those relevant to getting started prereqs you will need to install cmake first required for dlib which is used for face alignment currently the code only work with cuda installed and therefore requires an appropriate gpu and ha been tested on linux and window for the full set of required python package create a conda environment from the provided yaml eg conda create f pulseyml or anaconda on window conda env create n pulse f pulseyml conda activate pulse in some environment eg on window you may have to edit the pulseyml to remove the version specific hash on each dependency and remove any dependency that still throw an error after running conda env create such a readline dependency blas10mkl to dependency blas10 finally you will need an internet connection the first time you run the code a it will automatically download the relevant pretrained model from google drive if it ha already been downloaded it will use the local copy in the event that the public google drive is out of capacity add the file to your own google drive instead get the share url and replace the id in the httpsdrivegooglecomucid link in align_facepy and pulsepy with the new file id from the share url given by your own drive file data by default input data for runpy should be placed in input though this can be modified however this assumes face have already been aligned and downscaled if you have data that is not already in this form place it in realpics and run align_facepy which will automatically do this for you again all directory can be changed by command line argument if more convenient you will at this stage pic a downscaling factor note that if your data begin at a low resolution already downscaling it further will retain very little information in this case you may wish to bicubically upsample usually to 1024x1024 and allow align_facepy to downscale for you applying pulse once your data is appropriately formatted all you need to do is python runpy enjoy,pulse selfsupervised photo upsampling via latent space exploration generative model code accompanying cvpr20 paper title paper link httpsarxivorgpdf200303808pdf note noticed lot concern pulse used identify individual whose face blurred want emphasize impossible pulse make imaginary face people exist confused real people help identify reconstruct original image also want address concern bias pulse included new section paper accompanying model card directly addressing bias table content pulse selfsupervised photo upsampling via latent space exploration generative model table content usage prereqs data applying pulse given lowresolution input image pulse search output generative model stylegan highresolution image perceptually realistic downscale correctly usage main file interest applying pulse runpy full list argument description found file describe relevant getting started prereqs need install cmake first required dlib used face alignment currently code work cuda installed therefore requires appropriate gpu tested linux window full set required python package create conda environment provided yaml eg conda create f pulseyml anaconda window conda env create n pulse f pulseyml conda activate pulse environment eg window may edit pulseyml remove version specific hash dependency remove dependency still throw error running conda env create readline dependency blas10mkl dependency blas10 finally need internet connection first time run code automatically download relevant pretrained model google drive already downloaded use local copy event public google drive capacity add file google drive instead get share url replace id httpsdrivegooglecomucid link align_facepy pulsepy new file id share url given drive file data default input data runpy placed input though modified however assumes face already aligned downscaled data already form place realpics run align_facepy automatically directory changed command line argument convenient stage pic downscaling factor note data begin low resolution already downscaling retain little information case may wish bicubically upsample usually 1024x1024 allow align_facepy downscale applying pulse data appropriately formatted need python runpy enjoy
Python ,"This repository was originally at https://github.com/aiforearth/SpaceNetExploration. For other Microsoft AI for Earth repositories, search for the topic #aiforearth on GitHub or visit them here.
Building Footprint Extraction
Overview
This repository contains a walkthrough demonstrating how to perform semantic segmentation using convolutional neural networks (CNNs) on satellite images to extract the footprints of buildings. We show how to carry out the procedure on an Azure Deep Learning Virtual Machine (DLVM), which are GPU-enabled and have all major frameworks pre-installed so you can start model training straight-away. We use a subset of the data and labels from the SpaceNet Challenge, an online repository of freely available satellite imagery released to encourage the application of machine learning to geospatial data.
The blog post that first announced this sample project is here on the Azure Blog.
Data
SpaceNet Building Footprint Extraction Dataset
The code in this repository was developed for training a semantic segmentation model (currently two variants of the U-Net are implemented) on the Vegas set of the SpaceNet building footprint extraction data. This makes the sample code clearer, but it can be easily extended to take in training data from the four other locations.
The organizers release a portion of this data as training data and the rest are held out for the purpose of the competitions they hold. For the experiments discussed here, we split the official training set 70:15:15 into our own training, validation and test sets. These are 39 GB in size as raw images in TIFF format with labels.
Generate Input from Raw Data
Instruction for downloading the SpaceNet data can be found on their website. The authors provide a set of utilities to convert the raw images to a format that semantic segmentation models can take as input. The utilities are in this repo. Most of the functionalities you will need are in the python folder. Please read their instructions on the repo's README to understand all the tools and parameters available. After using python/createDataSpaceNet.py from the utilities repo to process the raw data, the input image and its label look like the following:

Environment Setup
Provision an Azure Deep Learning Virtual Machine
You could train your models on a Deep Learning Virtual Machine (DLVM) on Azure to get started quickly, where all the major deep learning frameworks, including PyTorch used in this repo, are installed and ready to use. These VMs are configured specifically for use with GPUs. Instructions for provisioning can be found here. The code here has been used on a Ubuntu Linux DLVM, but you should be able to use it on a Windows DLVM with minor modifications to the commands such as those setting environment variable values. The commands on this page are for running in a Linux shell.
Additional Packages to Install
There are two additional packages for the polygonization of the result of the CNN model so that our results can be compared to the original labels, which are expressed in a polygon data type. You can install these using pip:
pip install rasterio
pip install shapely

Data Storage Options
For quick experimentations you could download your data to the OS disk, but this makes data transfer and sharing costly when you scale out.
There are several options for storing the data while you perform computation on them in Azure. Here's a piece of documentation to guide you through choosing among these, and here are the pricing information.
If you are not planning on training models distributedly across several machines, you could attach a data disk to your VM. See instructions on attaching a data disk to a Linux VM. You can later re-attach this data disk to a more powerful VM, but it can only be attached to one machine at a time.
For both Azure Blob Storage and File Share, you can browse the files stored from any computer using the Storage Explorer desktop app. Both blob storage containers and file shares can be mounted on your VM so you can use them as if they were local disks. See instructions for mounting blob storage and file shares. Note however that such file systems have different performance for writing and deleting files than local file systems. Please refer to Azure Storage performance targets for more information.
Model Training
We tackle the problem of outlining building footprints in satellite images by applying a semantic segmentation model to first classify each pixel as background, building, or boundary of buildings. The U-Net is used for this task. There are two variants of the U-Net implemented in the models directory, differing by the sizes of filters used. The baseline U-Net is a similar version as used by the winner of the SpaceNet Building Footprint competition XD_XD. We referenced several open source implementations, noted in the relevant files.
Code for training the model is in the pipeline directory. The training script is train.py and all the paths to input/output, parameters and other arguments are specified in train_config.py, which you can modify and experiment with. The default configuration has total_epochs set to 15 to run training for 15 epochs, which takes about an hour in total on a VM with a P100 GPU (SKU NC6s_v2 on Azure). For the sample image above, the result of the segmentation model is as follows at epoch 3, 5, 7 and 10:

Generate Polygons of the Building Footprints
Standard graphics techniques are used to convert contiguous blobs of building pixels identified by the segmentation model, using libraries Rasterio and Shapely. The script pipeline/polygonize.py performs this procedure, and you can change various parameters in polygonize_config.py in the same directory. The most important parameter influencing the performance of the model is min_polygon_area, which is the area in squared pixels below which blobs of building pixels are discarded, reducing the noise in our results. Increasing this threshold decreases the number of false positive footprint proposals.
Evaluation
The evaluation metric used by the SpaceNet Challenge is the F1 score, where a footprint proposal is counted as a true positive if its intersection over union (IoU) with the ground truth polygon is above 0.5.
You can of course employ your own metric to suit your application, but if you would like to use the SpaceNet utilities to compute the F1 score based on polygons of building footprints, you need to first combine the annotations for each image in geojson format into a csv with python/createCSVFromGEOJSON.py from the utilities repo. In the root directory of utilities, run
python python/createCSVFromGEOJSON.py -imgDir /tutorial_data/val/RGB-PanSharpen -geoDir /tutorial_data/val/geojson/buildings -o ground_truth.csv --CreateProposalFile

Then you can use python/evaluateScene.py to compute the F1 score, giving the ground truth csv produced from the last command and the csv output proposals.csv produced by pipeline/polygonize.py in this repo:
python python/evaluateScene.py ground_truth.csv proposal.csv

Related Materials
Bing team's announcement that they released a large quantity of building footprints in the US in support of the Open Street Map community, and article briefly describing their method of extracting them.
Very helpful blog post and code on road extraction from satellite images by Jeff Wen on a different dataset. We also took inspiration in structuring the training pipeline from this repo.
SpaceNet road extraction challenge.
Tutorial on pixel-level land cover classification using semantic segmentation in CNTK on Azure.
",thi repositori wa origin at httpsgithubcomaiforearthspacenetexplor for other microsoft ai for earth repositori search for the topic aiforearth on github or visit them here build footprint extract overview thi repositori contain a walkthrough demonstr how to perform semant segment use convolut neural network cnn on satellit imag to extract the footprint of build we show how to carri out the procedur on an azur deep learn virtual machin dlvm which are gpuenabl and have all major framework preinstal so you can start model train straightaway we use a subset of the data and label from the spacenet challeng an onlin repositori of freeli avail satellit imageri releas to encourag the applic of machin learn to geospati data the blog post that first announc thi sampl project is here on the azur blog data spacenet build footprint extract dataset the code in thi repositori wa develop for train a semant segment model current two variant of the unet are implement on the vega set of the spacenet build footprint extract data thi make the sampl code clearer but it can be easili extend to take in train data from the four other locat the organ releas a portion of thi data as train data and the rest are held out for the purpos of the competit they hold for the experi discuss here we split the offici train set 701515 into our own train valid and test set these are 39 gb in size as raw imag in tiff format with label gener input from raw data instruct for download the spacenet data can be found on their websit the author provid a set of util to convert the raw imag to a format that semant segment model can take as input the util are in thi repo most of the function you will need are in the python folder pleas read their instruct on the repo readm to understand all the tool and paramet avail after use pythoncreatedataspacenetpi from the util repo to process the raw data the input imag and it label look like the follow environ setup provis an azur deep learn virtual machin you could train your model on a deep learn virtual machin dlvm on azur to get start quickli where all the major deep learn framework includ pytorch use in thi repo are instal and readi to use these vm are configur specif for use with gpu instruct for provis can be found here the code here ha been use on a ubuntu linux dlvm but you should be abl to use it on a window dlvm with minor modif to the command such as those set environ variabl valu the command on thi page are for run in a linux shell addit packag to instal there are two addit packag for the polygon of the result of the cnn model so that our result can be compar to the origin label which are express in a polygon data type you can instal these use pip pip instal rasterio pip instal shape data storag option for quick experiment you could download your data to the os disk but thi make data transfer and share costli when you scale out there are sever option for store the data while you perform comput on them in azur here a piec of document to guid you through choos among these and here are the price inform if you are not plan on train model distributedli across sever machin you could attach a data disk to your vm see instruct on attach a data disk to a linux vm you can later reattach thi data disk to a more power vm but it can onli be attach to one machin at a time for both azur blob storag and file share you can brows the file store from ani comput use the storag explor desktop app both blob storag contain and file share can be mount on your vm so you can use them as if they were local disk see instruct for mount blob storag and file share note howev that such file system have differ perform for write and delet file than local file system pleas refer to azur storag perform target for more inform model train we tackl the problem of outlin build footprint in satellit imag by appli a semant segment model to first classifi each pixel as background build or boundari of build the unet is use for thi task there are two variant of the unet implement in the model directori differ by the size of filter use the baselin unet is a similar version as use by the winner of the spacenet build footprint competit xd_xd we referenc sever open sourc implement note in the relev file code for train the model is in the pipelin directori the train script is trainpi and all the path to inputoutput paramet and other argument are specifi in train_configpi which you can modifi and experi with the default configur ha total_epoch set to 15 to run train for 15 epoch which take about an hour in total on a vm with a p100 gpu sku nc6s_v2 on azur for the sampl imag abov the result of the segment model is as follow at epoch 3 5 7 and 10 gener polygon of the build footprint standard graphic techniqu are use to convert contigu blob of build pixel identifi by the segment model use librari rasterio and shape the script pipelinepolygonizepi perform thi procedur and you can chang variou paramet in polygonize_configpi in the same directori the most import paramet influenc the perform of the model is min_polygon_area which is the area in squar pixel below which blob of build pixel are discard reduc the nois in our result increas thi threshold decreas the number of fals posit footprint propos evalu the evalu metric use by the spacenet challeng is the f1 score where a footprint propos is count as a true posit if it intersect over union iou with the ground truth polygon is abov 05 you can of cours employ your own metric to suit your applic but if you would like to use the spacenet util to comput the f1 score base on polygon of build footprint you need to first combin the annot for each imag in geojson format into a csv with pythoncreatecsvfromgeojsonpi from the util repo in the root directori of util run python pythoncreatecsvfromgeojsonpi imgdir tutorial_datavalrgbpansharpen geodir tutorial_datavalgeojsonbuild o ground_truthcsv createproposalfil then you can use pythonevaluatescenepi to comput the f1 score give the ground truth csv produc from the last command and the csv output proposalscsv produc by pipelinepolygonizepi in thi repo python pythonevaluatescenepi ground_truthcsv proposalcsv relat materi bing team announc that they releas a larg quantiti of build footprint in the us in support of the open street map commun and articl briefli describ their method of extract them veri help blog post and code on road extract from satellit imag by jeff wen on a differ dataset we also took inspir in structur the train pipelin from thi repo spacenet road extract challeng tutori on pixellevel land cover classif use semant segment in cntk on azur,this repository wa originally at httpsgithubcomaiforearthspacenetexploration for other microsoft ai for earth repository search for the topic aiforearth on github or visit them here building footprint extraction overview this repository contains a walkthrough demonstrating how to perform semantic segmentation using convolutional neural network cnns on satellite image to extract the footprint of building we show how to carry out the procedure on an azure deep learning virtual machine dlvm which are gpuenabled and have all major framework preinstalled so you can start model training straightaway we use a subset of the data and label from the spacenet challenge an online repository of freely available satellite imagery released to encourage the application of machine learning to geospatial data the blog post that first announced this sample project is here on the azure blog data spacenet building footprint extraction dataset the code in this repository wa developed for training a semantic segmentation model currently two variant of the unet are implemented on the vega set of the spacenet building footprint extraction data this make the sample code clearer but it can be easily extended to take in training data from the four other location the organizer release a portion of this data a training data and the rest are held out for the purpose of the competition they hold for the experiment discussed here we split the official training set 701515 into our own training validation and test set these are 39 gb in size a raw image in tiff format with label generate input from raw data instruction for downloading the spacenet data can be found on their website the author provide a set of utility to convert the raw image to a format that semantic segmentation model can take a input the utility are in this repo most of the functionality you will need are in the python folder please read their instruction on the repos readme to understand all the tool and parameter available after using pythoncreatedataspacenetpy from the utility repo to process the raw data the input image and it label look like the following environment setup provision an azure deep learning virtual machine you could train your model on a deep learning virtual machine dlvm on azure to get started quickly where all the major deep learning framework including pytorch used in this repo are installed and ready to use these vms are configured specifically for use with gpus instruction for provisioning can be found here the code here ha been used on a ubuntu linux dlvm but you should be able to use it on a window dlvm with minor modification to the command such a those setting environment variable value the command on this page are for running in a linux shell additional package to install there are two additional package for the polygonization of the result of the cnn model so that our result can be compared to the original label which are expressed in a polygon data type you can install these using pip pip install rasterio pip install shapely data storage option for quick experimentation you could download your data to the o disk but this make data transfer and sharing costly when you scale out there are several option for storing the data while you perform computation on them in azure here a piece of documentation to guide you through choosing among these and here are the pricing information if you are not planning on training model distributedly across several machine you could attach a data disk to your vm see instruction on attaching a data disk to a linux vm you can later reattach this data disk to a more powerful vm but it can only be attached to one machine at a time for both azure blob storage and file share you can browse the file stored from any computer using the storage explorer desktop app both blob storage container and file share can be mounted on your vm so you can use them a if they were local disk see instruction for mounting blob storage and file share note however that such file system have different performance for writing and deleting file than local file system please refer to azure storage performance target for more information model training we tackle the problem of outlining building footprint in satellite image by applying a semantic segmentation model to first classify each pixel a background building or boundary of building the unet is used for this task there are two variant of the unet implemented in the model directory differing by the size of filter used the baseline unet is a similar version a used by the winner of the spacenet building footprint competition xd_xd we referenced several open source implementation noted in the relevant file code for training the model is in the pipeline directory the training script is trainpy and all the path to inputoutput parameter and other argument are specified in train_configpy which you can modify and experiment with the default configuration ha total_epochs set to 15 to run training for 15 epoch which take about an hour in total on a vm with a p100 gpu sku nc6s_v2 on azure for the sample image above the result of the segmentation model is a follows at epoch 3 5 7 and 10 generate polygon of the building footprint standard graphic technique are used to convert contiguous blob of building pixel identified by the segmentation model using library rasterio and shapely the script pipelinepolygonizepy performs this procedure and you can change various parameter in polygonize_configpy in the same directory the most important parameter influencing the performance of the model is min_polygon_area which is the area in squared pixel below which blob of building pixel are discarded reducing the noise in our result increasing this threshold decrease the number of false positive footprint proposal evaluation the evaluation metric used by the spacenet challenge is the f1 score where a footprint proposal is counted a a true positive if it intersection over union iou with the ground truth polygon is above 05 you can of course employ your own metric to suit your application but if you would like to use the spacenet utility to compute the f1 score based on polygon of building footprint you need to first combine the annotation for each image in geojson format into a csv with pythoncreatecsvfromgeojsonpy from the utility repo in the root directory of utility run python pythoncreatecsvfromgeojsonpy imgdir tutorial_datavalrgbpansharpen geodir tutorial_datavalgeojsonbuildings o ground_truthcsv createproposalfile then you can use pythonevaluatescenepy to compute the f1 score giving the ground truth csv produced from the last command and the csv output proposalscsv produced by pipelinepolygonizepy in this repo python pythonevaluatescenepy ground_truthcsv proposalcsv related material bing team announcement that they released a large quantity of building footprint in the u in support of the open street map community and article briefly describing their method of extracting them very helpful blog post and code on road extraction from satellite image by jeff wen on a different dataset we also took inspiration in structuring the training pipeline from this repo spacenet road extraction challenge tutorial on pixellevel land cover classification using semantic segmentation in cntk on azure,repository originally httpsgithubcomaiforearthspacenetexploration microsoft ai earth repository search topic aiforearth github visit building footprint extraction overview repository contains walkthrough demonstrating perform semantic segmentation using convolutional neural network cnns satellite image extract footprint building show carry procedure azure deep learning virtual machine dlvm gpuenabled major framework preinstalled start model training straightaway use subset data label spacenet challenge online repository freely available satellite imagery released encourage application machine learning geospatial data blog post first announced sample project azure blog data spacenet building footprint extraction dataset code repository developed training semantic segmentation model currently two variant unet implemented vega set spacenet building footprint extraction data make sample code clearer easily extended take training data four location organizer release portion data training data rest held purpose competition hold experiment discussed split official training set 701515 training validation test set 39 gb size raw image tiff format label generate input raw data instruction downloading spacenet data found website author provide set utility convert raw image format semantic segmentation model take input utility repo functionality need python folder please read instruction repos readme understand tool parameter available using pythoncreatedataspacenetpy utility repo process raw data input image label look like following environment setup provision azure deep learning virtual machine could train model deep learning virtual machine dlvm azure get started quickly major deep learning framework including pytorch used repo installed ready use vms configured specifically use gpus instruction provisioning found code used ubuntu linux dlvm able use window dlvm minor modification command setting environment variable value command page running linux shell additional package install two additional package polygonization result cnn model result compared original label expressed polygon data type install using pip pip install rasterio pip install shapely data storage option quick experimentation could download data o disk make data transfer sharing costly scale several option storing data perform computation azure here piece documentation guide choosing among pricing information planning training model distributedly across several machine could attach data disk vm see instruction attaching data disk linux vm later reattach data disk powerful vm attached one machine time azure blob storage file share browse file stored computer using storage explorer desktop app blob storage container file share mounted vm use local disk see instruction mounting blob storage file share note however file system different performance writing deleting file local file system please refer azure storage performance target information model training tackle problem outlining building footprint satellite image applying semantic segmentation model first classify pixel background building boundary building unet used task two variant unet implemented model directory differing size filter used baseline unet similar version used winner spacenet building footprint competition xd_xd referenced several open source implementation noted relevant file code training model pipeline directory training script trainpy path inputoutput parameter argument specified train_configpy modify experiment default configuration total_epochs set 15 run training 15 epoch take hour total vm p100 gpu sku nc6s_v2 azure sample image result segmentation model follows epoch 3 5 7 10 generate polygon building footprint standard graphic technique used convert contiguous blob building pixel identified segmentation model using library rasterio shapely script pipelinepolygonizepy performs procedure change various parameter polygonize_configpy directory important parameter influencing performance model min_polygon_area area squared pixel blob building pixel discarded reducing noise result increasing threshold decrease number false positive footprint proposal evaluation evaluation metric used spacenet challenge f1 score footprint proposal counted true positive intersection union iou ground truth polygon 05 course employ metric suit application would like use spacenet utility compute f1 score based polygon building footprint need first combine annotation image geojson format csv pythoncreatecsvfromgeojsonpy utility repo root directory utility run python pythoncreatecsvfromgeojsonpy imgdir tutorial_datavalrgbpansharpen geodir tutorial_datavalgeojsonbuildings ground_truthcsv createproposalfile use pythonevaluatescenepy compute f1 score giving ground truth csv produced last command csv output proposalscsv produced pipelinepolygonizepy repo python pythonevaluatescenepy ground_truthcsv proposalcsv related material bing team announcement released large quantity building footprint u support open street map community article briefly describing method extracting helpful blog post code road extraction satellite image jeff wen different dataset also took inspiration structuring training pipeline repo spacenet road extraction challenge tutorial pixellevel land cover classification using semantic segmentation cntk azure
Python ,"



Cosmonium is a 3D astronomy and space exploration program. With Cosmonium you can navigate in our solar system and discover all the planets and their moons. You can also visit the neighboring stars and discover the true size of our galaxy and the Universe.
Cosmonium supports (or will support) the creation of fictional planets, stellar systems nebulaes, ... using procedural generation.
Cosmonium also already supports some Celestia addons (though CMOD and CelX are not yet supported).
Requirements
Cosmonium runs on Windows (Vista or above), Linux (CentOS 5, Ubuntu 14 or above) or macOS (mac0S 10.9 or above)
with a graphic card supporting OpenGL 2.1 or better (OpenGL 4.5 is recommended) and at least 512MB of disk
(up to 4GB if the HD and UHD textures are installed).
Installation
Download the installer or package for your platform from the download page and see the [[Installation]] page
The package contains only low resolution textures, see here to install extra HD and UHD textures.
Screenshots
See in the Wiki some screenshots of the application with views of
Saturn,
Jupiter,
Mars,
the Moon,
procedural planets, ...

Launch
Simply starts cosmonium from your application menu or from the cosmonium folder. See also the installation page for more options.
User interface
Cosmonium user interface is still heavily based on Celestia, most of the command and keyboard shortcuts work the same.
Go to First steps to have an explanation of the basic command or see the Control page for an exhaustive list.
Full documentation
Cosmonium is still in its infancy, but it is already usable to explore all the planets and the moons of our solar system, all the neighbor or visible stars and much more.
It also support custom content and addons, either as Cosmonium or Celestia addons.
The full documentation is available in the Wiki
Bugs
If you encounter any problem to install or run Cosmonium, please don't hesitate to fill a bug report in the issue tracker here on Github.
License
Cosmonium is (C) 2018-2020 Laurent Deru.
This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details, which you should have received along with this program. If not, request a copy from: Free Software Foundation, Inc. 59 Temple Place - Suite 330 Boston, MA 02111-1307 USA.
Cosmonium uses several third-party libraries which are subject to their own licenses,  see Third-Party.md for the complete list.
Cosmonium data (textures, models, orbital elements,..) come from many sources. Their respective copyright holder, license and reference are available in the info panel of the displayed object and in the related yaml file.
Powered by


",cosmonium is a 3d astronomi and space explor program with cosmonium you can navig in our solar system and discov all the planet and their moon you can also visit the neighbor star and discov the true size of our galaxi and the univers cosmonium support or will support the creation of fiction planet stellar system nebula use procedur gener cosmonium also alreadi support some celestia addon though cmod and celx are not yet support requir cosmonium run on window vista or abov linux cento 5 ubuntu 14 or abov or maco mac0 109 or abov with a graphic card support opengl 21 or better opengl 45 is recommend and at least 512mb of disk up to 4gb if the hd and uhd textur are instal instal download the instal or packag for your platform from the download page and see the instal page the packag contain onli low resolut textur see here to instal extra hd and uhd textur screenshot see in the wiki some screenshot of the applic with view of saturn jupit mar the moon procedur planet launch simpli start cosmonium from your applic menu or from the cosmonium folder see also the instal page for more option user interfac cosmonium user interfac is still heavili base on celestia most of the command and keyboard shortcut work the same go to first step to have an explan of the basic command or see the control page for an exhaust list full document cosmonium is still in it infanc but it is alreadi usabl to explor all the planet and the moon of our solar system all the neighbor or visibl star and much more it also support custom content and addon either as cosmonium or celestia addon the full document is avail in the wiki bug if you encount ani problem to instal or run cosmonium pleas dont hesit to fill a bug report in the issu tracker here on github licens cosmonium is c 20182020 laurent deru thi program is free softwar you can redistribut it andor modifi it under the term of the gnu gener public licens as publish by the free softwar foundat either version 3 of the licens or at your option ani later version thi program is distribut in the hope that it will be use but without ani warranti without even the impli warranti of merchant or fit for a particular purpos see the gnu gener public licens for more detail which you should have receiv along with thi program if not request a copi from free softwar foundat inc 59 templ place suit 330 boston ma 021111307 usa cosmonium use sever thirdparti librari which are subject to their own licens see thirdpartymd for the complet list cosmonium data textur model orbit element come from mani sourc their respect copyright holder licens and refer are avail in the info panel of the display object and in the relat yaml file power by,cosmonium is a 3d astronomy and space exploration program with cosmonium you can navigate in our solar system and discover all the planet and their moon you can also visit the neighboring star and discover the true size of our galaxy and the universe cosmonium support or will support the creation of fictional planet stellar system nebulaes using procedural generation cosmonium also already support some celestia addons though cmod and celx are not yet supported requirement cosmonium run on window vista or above linux centos 5 ubuntu 14 or above or macos mac0s 109 or above with a graphic card supporting opengl 21 or better opengl 45 is recommended and at least 512mb of disk up to 4gb if the hd and uhd texture are installed installation download the installer or package for your platform from the download page and see the installation page the package contains only low resolution texture see here to install extra hd and uhd texture screenshots see in the wiki some screenshots of the application with view of saturn jupiter mar the moon procedural planet launch simply start cosmonium from your application menu or from the cosmonium folder see also the installation page for more option user interface cosmonium user interface is still heavily based on celestia most of the command and keyboard shortcut work the same go to first step to have an explanation of the basic command or see the control page for an exhaustive list full documentation cosmonium is still in it infancy but it is already usable to explore all the planet and the moon of our solar system all the neighbor or visible star and much more it also support custom content and addons either a cosmonium or celestia addons the full documentation is available in the wiki bug if you encounter any problem to install or run cosmonium please dont hesitate to fill a bug report in the issue tracker here on github license cosmonium is c 20182020 laurent deru this program is free software you can redistribute it andor modify it under the term of the gnu general public license a published by the free software foundation either version 3 of the license or at your option any later version this program is distributed in the hope that it will be useful but without any warranty without even the implied warranty of merchantability or fitness for a particular purpose see the gnu general public license for more detail which you should have received along with this program if not request a copy from free software foundation inc 59 temple place suite 330 boston ma 021111307 usa cosmonium us several thirdparty library which are subject to their own license see thirdpartymd for the complete list cosmonium data texture model orbital element come from many source their respective copyright holder license and reference are available in the info panel of the displayed object and in the related yaml file powered by,cosmonium 3d astronomy space exploration program cosmonium navigate solar system discover planet moon also visit neighboring star discover true size galaxy universe cosmonium support support creation fictional planet stellar system nebulaes using procedural generation cosmonium also already support celestia addons though cmod celx yet supported requirement cosmonium run window vista linux centos 5 ubuntu 14 macos mac0s 109 graphic card supporting opengl 21 better opengl 45 recommended least 512mb disk 4gb hd uhd texture installed installation download installer package platform download page see installation page package contains low resolution texture see install extra hd uhd texture screenshots see wiki screenshots application view saturn jupiter mar moon procedural planet launch simply start cosmonium application menu cosmonium folder see also installation page option user interface cosmonium user interface still heavily based celestia command keyboard shortcut work go first step explanation basic command see control page exhaustive list full documentation cosmonium still infancy already usable explore planet moon solar system neighbor visible star much also support custom content addons either cosmonium celestia addons full documentation available wiki bug encounter problem install run cosmonium please dont hesitate fill bug report issue tracker github license cosmonium c 20182020 laurent deru program free software redistribute andor modify term gnu general public license published free software foundation either version 3 license option later version program distributed hope useful without warranty without even implied warranty merchantability fitness particular purpose see gnu general public license detail received along program request copy free software foundation inc 59 temple place suite 330 boston 021111307 usa cosmonium us several thirdparty library subject license see thirdpartymd complete list cosmonium data texture model orbital element come many source respective copyright holder license reference available info panel displayed object related yaml file powered
Python ,"SpaceNet Buildings Exploration
Transform SpaceNet geojson buidling labels data into raster masks.
Download data via:
aws s3api get-object --bucket spacenet-dataset \
--key AOI_1_Rio/processedData/processedBuildingLabels.tar.gz \
--request-payer requester processedBuildingLabels.tar.gz

Download spacenet utilities from:
https://github.com/SpaceNetChallenge/utilities/tree/master/python/spaceNet
For further details, see:
https://medium.com/the-downlinq/getting-started-with-spacenet-data-827fd2ec9f53
Example outputs are included in the example_outputs directory

",spacenet build explor transform spacenet geojson buidl label data into raster mask download data via aw s3api getobject bucket spacenetdataset key aoi_1_rioprocesseddataprocessedbuildinglabelstargz requestpay request processedbuildinglabelstargz download spacenet util from httpsgithubcomspacenetchallengeutilitiestreemasterpythonspacenet for further detail see httpsmediumcomthedownlinqgettingstartedwithspacenetdata827fd2ec9f53 exampl output are includ in the example_output directori,spacenet building exploration transform spacenet geojson buidling label data into raster mask download data via aws s3api getobject bucket spacenetdataset key aoi_1_rioprocesseddataprocessedbuildinglabelstargz requestpayer requester processedbuildinglabelstargz download spacenet utility from httpsgithubcomspacenetchallengeutilitiestreemasterpythonspacenet for further detail see httpsmediumcomthedownlinqgettingstartedwithspacenetdata827fd2ec9f53 example output are included in the example_outputs directory,spacenet building exploration transform spacenet geojson buidling label data raster mask download data via aws s3api getobject bucket spacenetdataset key aoi_1_rioprocesseddataprocessedbuildinglabelstargz requestpayer requester processedbuildinglabelstargz download spacenet utility httpsgithubcomspacenetchallengeutilitiestreemasterpythonspacenet detail see httpsmediumcomthedownlinqgettingstartedwithspacenetdata827fd2ec9f53 example output included example_outputs directory
Python ,"pypet






The new python parameter exploration toolkit:
pypet manages exploration of the parameter space
of any numerical simulation in python,
thereby storing your data into HDF5 files for you.
Moreover, pypet offers a new data container which
lets you access all your parameters and results
from a single source. Data I/O of your simulations and
analyses becomes a piece of cake!
Requirements
Python 3.6, 3.7 or 3.8 and


tables >=  3.5.0


pandas >= 1.0.0


numpy >= 1.16.0


scipy >= 1.3.0


HDF5 >= 1.10.0


There are also some optional packages that you can but do not have to install.
If you want to combine pypet with SCOOP you need

scoop >= 0.7.1

For git integration you additionally need

GitPython >= 3.1.3

To utilize the cap feature for multiprocessing you need

psutil >= 5.7.0

To utilize the continuing of crashed trajectories you need

dill >= 0.3.1

Automatic Sumatra records are supported for

Sumatra >= 0.7.1

Python 2.7
This release no longer supports Python 2.7.
If you are still using Python 2.7, you need to
use the pypet legacy version 0.3.0 (https://pypi.python.org/pypi/pypet/0.3.0).
What is pypet all about?
Whenever you do numerical simulations in science, you come across two major challenges.
First, you need some way to save your data. Secondly, you extensively explore the parameter space.
In order to accomplish both you write some hacky I/O functionality to get it done the quick and
dirty way. This means storing stuff into text files, as MATLAB m-files,
or whatever comes in handy.
After a while and many simulations later, you want to look back at some of your very
first results. But because of unforeseen circumstances, you changed a lot of your code.
As a consequence, you can no longer use your old data, but you need to write a hacky
converter to format your previous results to your new needs.
The more complexity you add to your simulations, the worse it gets, and you spend way
too much time formatting your data than doing science.
Indeed, this was a situation I was confronted with pretty soon at the beginning of my PhD.
So this project was born. I wanted to tackle the I/O problems more generally and produce code
that was not specific to my current simulations, but I could also use for future scientific
projects right out of the box.
The python parameter exploration toolkit (pypet) provides a framework to define parameters
that you need to run your simulations. You can actively explore these by following a
trajectory through the space spanned by the parameters.
And finally, you can get your results together and store everything appropriately to disk.
The storage format of choice is HDF5 (http://www.hdfgroup.org/HDF5/) via PyTables
(http://www.pytables.org/).
Package Organization
This project encompasses these core modules:


The pypet.environment module for handling the running of simulations


The pypet.trajectory module for managing the parameters and results,
and providing a way to explore your parameter space. Somewhat related is also the
pypet.naturalnaming module, that provides functionality to access and put data into
the trajectory.


The pypet.parameters module including containers for parameters and results


The pypet.storageservice for saving your data to disk


Install
If you don't have all prerequisites (numpy, scipy, tables, pandas) install them first.
These are standard python packages, so chances are high that they are already installed.
By the way, in case you use the python package manager pip
you can list all installed packages with pip freeze.
Next, simply install pypet via pip install pypet
Or
The package release can also be found on https://pypi.python.org/pypi/pypet. Download, unpack
and python setup.py install it.
Or
In case you use Windows, you have to download the tar file from https://pypi.python.org/pypi/pypet
and unzip it. Next, open a windows terminal
and navigate to your unpacked pypet files to the folder containing the setup.py file.
As above run from the terminal python setup.py install.
Documentation and Support
Documentation can be found on http://pypet.readthedocs.org/.
There is a Google Groups mailing list for support: https://groups.google.com/forum/?hl=de#!forum/pypet
If you have any further questions feel free to contact me at robert.meyer (at) ni.tu-berlin.de.
Main Features


Novel tree container Trajectory, for handling and managing of
parameters and results of numerical simulations


Group your parameters and results into meaningful categories


Access data via natural naming, e.g. traj.parameters.traffic.ncars


Automatic storage of simulation data into HDF5 files via PyTables


Support for many different data formats


python native data types: bool, int, long, float, str, complex


list, tuple, dict


Numpy arrays and matrices


Scipy sparse matrices


pandas DataFrames (http://pandas.pydata.org/)


BRIAN2 quantities and monitors (http://briansimulator.org/)




Easily extendable to other data formats!


Exploration of the parameter space of your simulations


Merging of trajectories residing in the same space


Support for multiprocessing, pypet can run your simulations in parallel


Analyse your data on-the-fly during multiprocessing


Adaptively explore tha parameter space combining pypet with optimization
tools like the evolutionary algorithms framework DEAP (http://deap.readthedocs.org/en/)


Dynamic Loading, load only the parts of your data you currently need


Resume a crashed or halted simulation


Annotate your parameters, results and groups


Git Integration, let pypet make automatic commits of your codebase


Sumatra Integration, let pypet add your simulations to the electronic lab notebook tool
Sumatra (http://neuralensemble.org/sumatra/)


pypet can be used on computing clusters or multiple servers at once if it is combined with
SCOOP (http://scoop.readthedocs.org/)


Quick Working Example
The best way to show how stuff works is by giving examples. I will start right away with a
very simple code snippet.
Well, what we have in mind is some sort of numerical simulation. For now we will keep it simple,
let's say we need to simulate the multiplication of 2 values, i.e. z=x*y.
We have two objectives, a) we want to store results of this simulation z and
b) we want to explore the parameter space and try different values of x and y.
Let's take a look at the snippet at once:
from pypet import Environment, cartesian_product

def multiply(traj):
    """"""Example of a sophisticated simulation that involves multiplying two values.

    :param traj:

        Trajectory containing the parameters in a particular combination,
        it also serves as a container for results.

    """"""
    z=traj.x * traj.y
    traj.f_add_result('z',z, comment='I am the product of two values!')

# Create an environment that handles running our simulation
env = Environment(trajectory='Multiplication',filename='./HDF/example_01.hdf5',
                    file_title='Example_01',
                    comment = 'I am the first example!')

# Get the trajectory from the environment
traj = env.trajectory

# Add both parameters
traj.f_add_parameter('x', 1.0, comment='Im the first dimension!')
traj.f_add_parameter('y', 1.0, comment='Im the second dimension!')

# Explore the parameters with a cartesian product
traj.f_explore(cartesian_product({'x':[1.0,2.0,3.0,4.0], 'y':[6.0,7.0,8.0]}))

# Run the simulation with all parameter combinations
env.run(multiply)
And now let's go through it one by one. At first we have a job to do, that is multiplying two
values:
def multiply(traj):
    """"""Example of a sophisticated simulation that involves multiplying two values.

    :param traj:

        Trajectory containing the parameters in a particular combination,
        it also serves as a container for results.

    """"""
    z=traj.x * traj.y
    traj.f_add_result('z',z, comment='I am the product of two values!')
This is our simulation function multiply. The function uses a so called trajectory
container which manages our parameters. We can access the parameters simply by natural naming,
as seen above via traj.x and traj.y. The value of z is simply added as a result
to the traj object.
After the definition of the job that we want to simulate, we create an environment which
will run the simulation.
# Create an environment that handles running our simulation
env = Environment(trajectory='Multiplication',filename='./HDF/example_01.hdf5',
                    file_title='Example_01',
                    comment = 'I am the first example!')
The environment uses some parameters here, that is the name of the new trajectory, a filename to
store the trajectory into, the title of the file, and a comment that is added to the trajectory.
There are more options available like the number of processors for multiprocessing or
how verbose the final HDF5 file is supposed to be.
Check out the documentation (http://pypet.readthedocs.org/) if you want to know more.
The environment will automatically generate a trajectory for us which we can access via:
# Get the trajectory from the environment
traj = env.trajectory
Now we need to populate our trajectory with our parameters. They are added with the default values
of x=y=1.0.
# Add both parameters
traj.f_add_parameter('x', 1.0, comment='Im the first dimension!')
traj.f_add_parameter('y', 1.0, comment='Im the second dimension!')
Well, calculating 1.0 * 1.0 is quite boring, we want to figure out more products, that is
the results of the cartesian product set {1.0,2.0,3.0,4.0} x {6.0,7.0,8.0}.
Therefore, we use f_explore in combination with the builder function
cartesian_product.
# Explore the parameters with a cartesian product
traj.f_explore(cartesian_product({'x':[1.0,2.0,3.0,4.0], 'y':[6.0,7.0,8.0]}))
Finally, we need to tell the environment to run our job multiply with all parameter
combinations.
# Run the simulation with all parameter combinations
env.run(multiply)
And that's it. The environment will evoke the function multiply now 12 times with
all parameter combinations. Every time it will pass a traj container with another one of these
12 combinations of different x and y values to calculate the value of z.
Moreover, the environment and the storage service will have taken care about the storage
of our trajectory  - including the results we have computed - into an HDF5 file.
So have fun using this tool!
Cheers,
Robert
Miscellaneous
Acknowledgements


Thanks to Robert Pröpper and Philipp Meier for answering all my Python questions
You might want to check out their SpykeViewer (https://github.com/rproepp/spykeviewer)
tool for visualization of MEA recordings and NEO (http://pythonhosted.org/neo) data


Thanks to Owen Mackwood for his SNEP toolbox which provided the initial ideas
for this project


Thanks to Mehmet Nevvaf Timur for his work on the SCOOP integration and the 'NETQUEUE' feature


Thanks to Henri Bunting for his work on the BRIAN2 subpackage


Thanks to the BCCN Berlin (http://www.bccn-berlin.de),
the Research Training Group GRK 1589/1, and the
Neural Information Processing Group ( http://www.ni.tu-berlin.de) for support


Tests
Tests can be found in pypet/tests.
Note that they involve heavy file I/O and you need privileges
to write files to a temporary folder.
The tests suite will make use of the tempfile.gettempdir() function to
create such a temporary folder.
Each test module can be run individually, for instance $ python trajectory_test.py.
You can run all tests with $ python all_tests.py which can also be found under
pypet/tests.
You can pass additional arguments as $ python all_tests.py -k --folder=myfolder/
with -k to keep the HDF5 and log files created by the tests
(if you want to inspect them, otherwise they will be deleted after the completed tests),
and --folder= to specify a folder where to store the HDF5 files instead of the temporary one.
If the folder cannot be created, the program defaults to tempfile.gettempdir().
Running all tests can take up to 20 minutes. The test suite encompasses more than 1000 tests
and has a code coverage of about 90%!
Moreover, pypet is constantly tested with Python 3.7 and 3.8 for Linux using
Travis-CI. Testing for Windows platforms is performed via Appveyor.
The source code is available at https://github.com/SmokinCaterpillar/pypet/.
License
BSD, please read LICENSE file.
Legal Notice
pypet was created by Robert Meyer at the Neural Information Processing Group (TU Berlin),
supported by the Research Training Group GRK 1589/1.
Contact
robert.meyer (at) alcemy.tech
alcemy GmbH
Choriner Str. 83
10119 Berlin, Germany
",pypet the new python paramet explor toolkit pypet manag explor of the paramet space of ani numer simul in python therebi store your data into hdf5 file for you moreov pypet offer a new data contain which let you access all your paramet and result from a singl sourc data io of your simul and analys becom a piec of cake requir python 36 37 or 38 and tabl 350 panda 100 numpi 1160 scipi 130 hdf5 1100 there are also some option packag that you can but do not have to instal if you want to combin pypet with scoop you need scoop 071 for git integr you addit need gitpython 313 to util the cap featur for multiprocess you need psutil 570 to util the continu of crash trajectori you need dill 031 automat sumatra record are support for sumatra 071 python 27 thi releas no longer support python 27 if you are still use python 27 you need to use the pypet legaci version 030 httpspypipythonorgpypipypet030 what is pypet all about whenev you do numer simul in scienc you come across two major challeng first you need some way to save your data secondli you extens explor the paramet space in order to accomplish both you write some hacki io function to get it done the quick and dirti way thi mean store stuff into text file as matlab mfile or whatev come in handi after a while and mani simul later you want to look back at some of your veri first result but becaus of unforeseen circumst you chang a lot of your code as a consequ you can no longer use your old data but you need to write a hacki convert to format your previou result to your new need the more complex you add to your simul the wors it get and you spend way too much time format your data than do scienc inde thi wa a situat i wa confront with pretti soon at the begin of my phd so thi project wa born i want to tackl the io problem more gener and produc code that wa not specif to my current simul but i could also use for futur scientif project right out of the box the python paramet explor toolkit pypet provid a framework to defin paramet that you need to run your simul you can activ explor these by follow a trajectori through the space span by the paramet and final you can get your result togeth and store everyth appropri to disk the storag format of choic is hdf5 httpwwwhdfgrouporghdf5 via pytabl httpwwwpytablesorg packag organ thi project encompass these core modul the pypetenviron modul for handl the run of simul the pypettrajectori modul for manag the paramet and result and provid a way to explor your paramet space somewhat relat is also the pypetnaturalnam modul that provid function to access and put data into the trajectori the pypetparamet modul includ contain for paramet and result the pypetstorageservic for save your data to disk instal if you dont have all prerequisit numpi scipi tabl panda instal them first these are standard python packag so chanc are high that they are alreadi instal by the way in case you use the python packag manag pip you can list all instal packag with pip freez next simpli instal pypet via pip instal pypet or the packag releas can also be found on httpspypipythonorgpypipypet download unpack and python setuppi instal it or in case you use window you have to download the tar file from httpspypipythonorgpypipypet and unzip it next open a window termin and navig to your unpack pypet file to the folder contain the setuppi file as abov run from the termin python setuppi instal document and support document can be found on httppypetreadthedocsorg there is a googl group mail list for support httpsgroupsgooglecomforumhldeforumpypet if you have ani further question feel free to contact me at robertmey at nituberlind main featur novel tree contain trajectori for handl and manag of paramet and result of numer simul group your paramet and result into meaning categori access data via natur name eg trajparameterstrafficncar automat storag of simul data into hdf5 file via pytabl support for mani differ data format python nativ data type bool int long float str complex list tupl dict numpi array and matric scipi spars matric panda datafram httppandaspydataorg brian2 quantiti and monitor httpbriansimulatororg easili extend to other data format explor of the paramet space of your simul merg of trajectori resid in the same space support for multiprocess pypet can run your simul in parallel analys your data onthefli dure multiprocess adapt explor tha paramet space combin pypet with optim tool like the evolutionari algorithm framework deap httpdeapreadthedocsorgen dynam load load onli the part of your data you current need resum a crash or halt simul annot your paramet result and group git integr let pypet make automat commit of your codebas sumatra integr let pypet add your simul to the electron lab notebook tool sumatra httpneuralensembleorgsumatra pypet can be use on comput cluster or multipl server at onc if it is combin with scoop httpscoopreadthedocsorg quick work exampl the best way to show how stuff work is by give exampl i will start right away with a veri simpl code snippet well what we have in mind is some sort of numer simul for now we will keep it simpl let say we need to simul the multipl of 2 valu ie zxi we have two object a we want to store result of thi simul z and b we want to explor the paramet space and tri differ valu of x and y let take a look at the snippet at onc from pypet import environ cartesian_product def multiplytraj exampl of a sophist simul that involv multipli two valu param traj trajectori contain the paramet in a particular combin it also serv as a contain for result ztrajx traji trajf_add_resultzz commenti am the product of two valu creat an environ that handl run our simul env environmenttrajectorymultiplicationfilenamehdfexample_01hdf5 file_titleexample_01 comment i am the first exampl get the trajectori from the environ traj envtrajectori add both paramet trajf_add_parameterx 10 commentim the first dimens trajf_add_parameteri 10 commentim the second dimens explor the paramet with a cartesian product trajf_explorecartesian_productx10203040 y607080 run the simul with all paramet combin envrunmultipli and now let go through it one by one at first we have a job to do that is multipli two valu def multiplytraj exampl of a sophist simul that involv multipli two valu param traj trajectori contain the paramet in a particular combin it also serv as a contain for result ztrajx traji trajf_add_resultzz commenti am the product of two valu thi is our simul function multipli the function use a so call trajectori contain which manag our paramet we can access the paramet simpli by natur name as seen abov via trajx and traji the valu of z is simpli ad as a result to the traj object after the definit of the job that we want to simul we creat an environ which will run the simul creat an environ that handl run our simul env environmenttrajectorymultiplicationfilenamehdfexample_01hdf5 file_titleexample_01 comment i am the first exampl the environ use some paramet here that is the name of the new trajectori a filenam to store the trajectori into the titl of the file and a comment that is ad to the trajectori there are more option avail like the number of processor for multiprocess or how verbos the final hdf5 file is suppos to be check out the document httppypetreadthedocsorg if you want to know more the environ will automat gener a trajectori for us which we can access via get the trajectori from the environ traj envtrajectori now we need to popul our trajectori with our paramet they are ad with the default valu of xy10 add both paramet trajf_add_parameterx 10 commentim the first dimens trajf_add_parameteri 10 commentim the second dimens well calcul 10 10 is quit bore we want to figur out more product that is the result of the cartesian product set 10203040 x 607080 therefor we use f_explor in combin with the builder function cartesian_product explor the paramet with a cartesian product trajf_explorecartesian_productx10203040 y607080 final we need to tell the environ to run our job multipli with all paramet combin run the simul with all paramet combin envrunmultipli and that it the environ will evok the function multipli now 12 time with all paramet combin everi time it will pass a traj contain with anoth one of these 12 combin of differ x and y valu to calcul the valu of z moreov the environ and the storag servic will have taken care about the storag of our trajectori includ the result we have comput into an hdf5 file so have fun use thi tool cheer robert miscellan acknowledg thank to robert prpper and philipp meier for answer all my python question you might want to check out their spykeview httpsgithubcomrproeppspykeview tool for visual of mea record and neo httppythonhostedorgneo data thank to owen mackwood for hi snep toolbox which provid the initi idea for thi project thank to mehmet nevvaf timur for hi work on the scoop integr and the netqueu featur thank to henri bunt for hi work on the brian2 subpackag thank to the bccn berlin httpwwwbccnberlind the research train group grk 15891 and the neural inform process group httpwwwnituberlind for support test test can be found in pypettest note that they involv heavi file io and you need privileg to write file to a temporari folder the test suit will make use of the tempfilegettempdir function to creat such a temporari folder each test modul can be run individu for instanc python trajectory_testpi you can run all test with python all_testspi which can also be found under pypettest you can pass addit argument as python all_testspi k foldermyfold with k to keep the hdf5 and log file creat by the test if you want to inspect them otherwis they will be delet after the complet test and folder to specifi a folder where to store the hdf5 file instead of the temporari one if the folder cannot be creat the program default to tempfilegettempdir run all test can take up to 20 minut the test suit encompass more than 1000 test and ha a code coverag of about 90 moreov pypet is constantli test with python 37 and 38 for linux use travisci test for window platform is perform via appveyor the sourc code is avail at httpsgithubcomsmokincaterpillarpypet licens bsd pleas read licens file legal notic pypet wa creat by robert meyer at the neural inform process group tu berlin support by the research train group grk 15891 contact robertmey at alcemytech alcemi gmbh chorin str 83 10119 berlin germani,pypet the new python parameter exploration toolkit pypet manages exploration of the parameter space of any numerical simulation in python thereby storing your data into hdf5 file for you moreover pypet offer a new data container which let you access all your parameter and result from a single source data io of your simulation and analysis becomes a piece of cake requirement python 36 37 or 38 and table 350 panda 100 numpy 1160 scipy 130 hdf5 1100 there are also some optional package that you can but do not have to install if you want to combine pypet with scoop you need scoop 071 for git integration you additionally need gitpython 313 to utilize the cap feature for multiprocessing you need psutil 570 to utilize the continuing of crashed trajectory you need dill 031 automatic sumatra record are supported for sumatra 071 python 27 this release no longer support python 27 if you are still using python 27 you need to use the pypet legacy version 030 httpspypipythonorgpypipypet030 what is pypet all about whenever you do numerical simulation in science you come across two major challenge first you need some way to save your data secondly you extensively explore the parameter space in order to accomplish both you write some hacky io functionality to get it done the quick and dirty way this mean storing stuff into text file a matlab mfiles or whatever come in handy after a while and many simulation later you want to look back at some of your very first result but because of unforeseen circumstance you changed a lot of your code a a consequence you can no longer use your old data but you need to write a hacky converter to format your previous result to your new need the more complexity you add to your simulation the worse it get and you spend way too much time formatting your data than doing science indeed this wa a situation i wa confronted with pretty soon at the beginning of my phd so this project wa born i wanted to tackle the io problem more generally and produce code that wa not specific to my current simulation but i could also use for future scientific project right out of the box the python parameter exploration toolkit pypet provides a framework to define parameter that you need to run your simulation you can actively explore these by following a trajectory through the space spanned by the parameter and finally you can get your result together and store everything appropriately to disk the storage format of choice is hdf5 httpwwwhdfgrouporghdf5 via pytables httpwwwpytablesorg package organization this project encompasses these core module the pypetenvironment module for handling the running of simulation the pypettrajectory module for managing the parameter and result and providing a way to explore your parameter space somewhat related is also the pypetnaturalnaming module that provides functionality to access and put data into the trajectory the pypetparameters module including container for parameter and result the pypetstorageservice for saving your data to disk install if you dont have all prerequisite numpy scipy table panda install them first these are standard python package so chance are high that they are already installed by the way in case you use the python package manager pip you can list all installed package with pip freeze next simply install pypet via pip install pypet or the package release can also be found on httpspypipythonorgpypipypet download unpack and python setuppy install it or in case you use window you have to download the tar file from httpspypipythonorgpypipypet and unzip it next open a window terminal and navigate to your unpacked pypet file to the folder containing the setuppy file a above run from the terminal python setuppy install documentation and support documentation can be found on httppypetreadthedocsorg there is a google group mailing list for support httpsgroupsgooglecomforumhldeforumpypet if you have any further question feel free to contact me at robertmeyer at nituberlinde main feature novel tree container trajectory for handling and managing of parameter and result of numerical simulation group your parameter and result into meaningful category access data via natural naming eg trajparameterstrafficncars automatic storage of simulation data into hdf5 file via pytables support for many different data format python native data type bool int long float str complex list tuple dict numpy array and matrix scipy sparse matrix panda dataframes httppandaspydataorg brian2 quantity and monitor httpbriansimulatororg easily extendable to other data format exploration of the parameter space of your simulation merging of trajectory residing in the same space support for multiprocessing pypet can run your simulation in parallel analyse your data onthefly during multiprocessing adaptively explore tha parameter space combining pypet with optimization tool like the evolutionary algorithm framework deap httpdeapreadthedocsorgen dynamic loading load only the part of your data you currently need resume a crashed or halted simulation annotate your parameter result and group git integration let pypet make automatic commits of your codebase sumatra integration let pypet add your simulation to the electronic lab notebook tool sumatra httpneuralensembleorgsumatra pypet can be used on computing cluster or multiple server at once if it is combined with scoop httpscoopreadthedocsorg quick working example the best way to show how stuff work is by giving example i will start right away with a very simple code snippet well what we have in mind is some sort of numerical simulation for now we will keep it simple let say we need to simulate the multiplication of 2 value ie zxy we have two objective a we want to store result of this simulation z and b we want to explore the parameter space and try different value of x and y let take a look at the snippet at once from pypet import environment cartesian_product def multiplytraj example of a sophisticated simulation that involves multiplying two value param traj trajectory containing the parameter in a particular combination it also serf a a container for result ztrajx trajy trajf_add_resultzz commenti am the product of two value create an environment that handle running our simulation env environmenttrajectorymultiplicationfilenamehdfexample_01hdf5 file_titleexample_01 comment i am the first example get the trajectory from the environment traj envtrajectory add both parameter trajf_add_parameterx 10 commentim the first dimension trajf_add_parametery 10 commentim the second dimension explore the parameter with a cartesian product trajf_explorecartesian_productx10203040 y607080 run the simulation with all parameter combination envrunmultiply and now let go through it one by one at first we have a job to do that is multiplying two value def multiplytraj example of a sophisticated simulation that involves multiplying two value param traj trajectory containing the parameter in a particular combination it also serf a a container for result ztrajx trajy trajf_add_resultzz commenti am the product of two value this is our simulation function multiply the function us a so called trajectory container which manages our parameter we can access the parameter simply by natural naming a seen above via trajx and trajy the value of z is simply added a a result to the traj object after the definition of the job that we want to simulate we create an environment which will run the simulation create an environment that handle running our simulation env environmenttrajectorymultiplicationfilenamehdfexample_01hdf5 file_titleexample_01 comment i am the first example the environment us some parameter here that is the name of the new trajectory a filename to store the trajectory into the title of the file and a comment that is added to the trajectory there are more option available like the number of processor for multiprocessing or how verbose the final hdf5 file is supposed to be check out the documentation httppypetreadthedocsorg if you want to know more the environment will automatically generate a trajectory for u which we can access via get the trajectory from the environment traj envtrajectory now we need to populate our trajectory with our parameter they are added with the default value of xy10 add both parameter trajf_add_parameterx 10 commentim the first dimension trajf_add_parametery 10 commentim the second dimension well calculating 10 10 is quite boring we want to figure out more product that is the result of the cartesian product set 10203040 x 607080 therefore we use f_explore in combination with the builder function cartesian_product explore the parameter with a cartesian product trajf_explorecartesian_productx10203040 y607080 finally we need to tell the environment to run our job multiply with all parameter combination run the simulation with all parameter combination envrunmultiply and thats it the environment will evoke the function multiply now 12 time with all parameter combination every time it will pas a traj container with another one of these 12 combination of different x and y value to calculate the value of z moreover the environment and the storage service will have taken care about the storage of our trajectory including the result we have computed into an hdf5 file so have fun using this tool cheer robert miscellaneous acknowledgement thanks to robert prpper and philipp meier for answering all my python question you might want to check out their spykeviewer httpsgithubcomrproeppspykeviewer tool for visualization of mea recording and neo httppythonhostedorgneo data thanks to owen mackwood for his snep toolbox which provided the initial idea for this project thanks to mehmet nevvaf timur for his work on the scoop integration and the netqueue feature thanks to henri bunting for his work on the brian2 subpackage thanks to the bccn berlin httpwwwbccnberlinde the research training group grk 15891 and the neural information processing group httpwwwnituberlinde for support test test can be found in pypettests note that they involve heavy file io and you need privilege to write file to a temporary folder the test suite will make use of the tempfilegettempdir function to create such a temporary folder each test module can be run individually for instance python trajectory_testpy you can run all test with python all_testspy which can also be found under pypettests you can pas additional argument a python all_testspy k foldermyfolder with k to keep the hdf5 and log file created by the test if you want to inspect them otherwise they will be deleted after the completed test and folder to specify a folder where to store the hdf5 file instead of the temporary one if the folder cannot be created the program default to tempfilegettempdir running all test can take up to 20 minute the test suite encompasses more than 1000 test and ha a code coverage of about 90 moreover pypet is constantly tested with python 37 and 38 for linux using travisci testing for window platform is performed via appveyor the source code is available at httpsgithubcomsmokincaterpillarpypet license bsd please read license file legal notice pypet wa created by robert meyer at the neural information processing group tu berlin supported by the research training group grk 15891 contact robertmeyer at alcemytech alcemy gmbh choriner str 83 10119 berlin germany,pypet new python parameter exploration toolkit pypet manages exploration parameter space numerical simulation python thereby storing data hdf5 file moreover pypet offer new data container let access parameter result single source data io simulation analysis becomes piece cake requirement python 36 37 38 table 350 panda 100 numpy 1160 scipy 130 hdf5 1100 also optional package install want combine pypet scoop need scoop 071 git integration additionally need gitpython 313 utilize cap feature multiprocessing need psutil 570 utilize continuing crashed trajectory need dill 031 automatic sumatra record supported sumatra 071 python 27 release longer support python 27 still using python 27 need use pypet legacy version 030 httpspypipythonorgpypipypet030 pypet whenever numerical simulation science come across two major challenge first need way save data secondly extensively explore parameter space order accomplish write hacky io functionality get done quick dirty way mean storing stuff text file matlab mfiles whatever come handy many simulation later want look back first result unforeseen circumstance changed lot code consequence longer use old data need write hacky converter format previous result new need complexity add simulation worse get spend way much time formatting data science indeed situation confronted pretty soon beginning phd project born wanted tackle io problem generally produce code specific current simulation could also use future scientific project right box python parameter exploration toolkit pypet provides framework define parameter need run simulation actively explore following trajectory space spanned parameter finally get result together store everything appropriately disk storage format choice hdf5 httpwwwhdfgrouporghdf5 via pytables httpwwwpytablesorg package organization project encompasses core module pypetenvironment module handling running simulation pypettrajectory module managing parameter result providing way explore parameter space somewhat related also pypetnaturalnaming module provides functionality access put data trajectory pypetparameters module including container parameter result pypetstorageservice saving data disk install dont prerequisite numpy scipy table panda install first standard python package chance high already installed way case use python package manager pip list installed package pip freeze next simply install pypet via pip install pypet package release also found httpspypipythonorgpypipypet download unpack python setuppy install case use window download tar file httpspypipythonorgpypipypet unzip next open window terminal navigate unpacked pypet file folder containing setuppy file run terminal python setuppy install documentation support documentation found httppypetreadthedocsorg google group mailing list support httpsgroupsgooglecomforumhldeforumpypet question feel free contact robertmeyer nituberlinde main feature novel tree container trajectory handling managing parameter result numerical simulation group parameter result meaningful category access data via natural naming eg trajparameterstrafficncars automatic storage simulation data hdf5 file via pytables support many different data format python native data type bool int long float str complex list tuple dict numpy array matrix scipy sparse matrix panda dataframes httppandaspydataorg brian2 quantity monitor httpbriansimulatororg easily extendable data format exploration parameter space simulation merging trajectory residing space support multiprocessing pypet run simulation parallel analyse data onthefly multiprocessing adaptively explore tha parameter space combining pypet optimization tool like evolutionary algorithm framework deap httpdeapreadthedocsorgen dynamic loading load part data currently need resume crashed halted simulation annotate parameter result group git integration let pypet make automatic commits codebase sumatra integration let pypet add simulation electronic lab notebook tool sumatra httpneuralensembleorgsumatra pypet used computing cluster multiple server combined scoop httpscoopreadthedocsorg quick working example best way show stuff work giving example start right away simple code snippet well mind sort numerical simulation keep simple let say need simulate multiplication 2 value ie zxy two objective want store result simulation z b want explore parameter space try different value x let take look snippet pypet import environment cartesian_product def multiplytraj example sophisticated simulation involves multiplying two value param traj trajectory containing parameter particular combination also serf container result ztrajx trajy trajf_add_resultzz commenti product two value create environment handle running simulation env environmenttrajectorymultiplicationfilenamehdfexample_01hdf5 file_titleexample_01 comment first example get trajectory environment traj envtrajectory add parameter trajf_add_parameterx 10 commentim first dimension trajf_add_parametery 10 commentim second dimension explore parameter cartesian product trajf_explorecartesian_productx10203040 y607080 run simulation parameter combination envrunmultiply let go one one first job multiplying two value def multiplytraj example sophisticated simulation involves multiplying two value param traj trajectory containing parameter particular combination also serf container result ztrajx trajy trajf_add_resultzz commenti product two value simulation function multiply function us called trajectory container manages parameter access parameter simply natural naming seen via trajx trajy value z simply added result traj object definition job want simulate create environment run simulation create environment handle running simulation env environmenttrajectorymultiplicationfilenamehdfexample_01hdf5 file_titleexample_01 comment first example environment us parameter name new trajectory filename store trajectory title file comment added trajectory option available like number processor multiprocessing verbose final hdf5 file supposed check documentation httppypetreadthedocsorg want know environment automatically generate trajectory u access via get trajectory environment traj envtrajectory need populate trajectory parameter added default value xy10 add parameter trajf_add_parameterx 10 commentim first dimension trajf_add_parametery 10 commentim second dimension well calculating 10 10 quite boring want figure product result cartesian product set 10203040 x 607080 therefore use f_explore combination builder function cartesian_product explore parameter cartesian product trajf_explorecartesian_productx10203040 y607080 finally need tell environment run job multiply parameter combination run simulation parameter combination envrunmultiply thats environment evoke function multiply 12 time parameter combination every time pas traj container another one 12 combination different x value calculate value z moreover environment storage service taken care storage trajectory including result computed hdf5 file fun using tool cheer robert miscellaneous acknowledgement thanks robert prpper philipp meier answering python question might want check spykeviewer httpsgithubcomrproeppspykeviewer tool visualization mea recording neo httppythonhostedorgneo data thanks owen mackwood snep toolbox provided initial idea project thanks mehmet nevvaf timur work scoop integration netqueue feature thanks henri bunting work brian2 subpackage thanks bccn berlin httpwwwbccnberlinde research training group grk 15891 neural information processing group httpwwwnituberlinde support test test found pypettests note involve heavy file io need privilege write file temporary folder test suite make use tempfilegettempdir function create temporary folder test module run individually instance python trajectory_testpy run test python all_testspy also found pypettests pas additional argument python all_testspy k foldermyfolder k keep hdf5 log file created test want inspect otherwise deleted completed test folder specify folder store hdf5 file instead temporary one folder cannot created program default tempfilegettempdir running test take 20 minute test suite encompasses 1000 test code coverage 90 moreover pypet constantly tested python 37 38 linux using travisci testing window platform performed via appveyor source code available httpsgithubcomsmokincaterpillarpypet license bsd please read license file legal notice pypet created robert meyer neural information processing group tu berlin supported research training group grk 15891 contact robertmeyer alcemytech alcemy gmbh choriner str 83 10119 berlin germany
Python ,"ParamNoise
A comparison of parameter space noise methods for exploration in deep reinforcement learning
NOTE: This project is not maintained.  Reach out if you'd like to help reboot it.
Links to papers
Parameter Space Noise for Exploration : https://openreview.net/forum?id=ByBAl2eAZ&noteId=ByBAl2eAZ
Noisy Networks For Exploration : https://openreview.net/forum?id=rywHCPkAW&noteId=rywHCPkAW
Resources

OpenAI Baselines for useful Atari wrappers and replay buffer
bearpaw's pytorch-classification repo for utilities, logging, training framework
ikostrikov's PPO implementation for other utilities and PPO guidance
pytorch-rl for DQN help
PyTorch DQN tutorial for PyTorch tricks
Original DQN paper since both papers use the original hyperparameters, for the most part

TODOs

Implement PPO and MuJoCo env handling
Revisit logging; make sure everything is there to reproduce results in papers
Implement plotting (matplotlib is in Logger object; maybe try out visdom)
More tests (figure out different combinations of arguments to ensure everything's interacting well)
Begin experiments (start with Mujoco; it's cheaper)

Atari Games to Test

Alien: Adaptive helps a lot, learned shows no improvement
Enduro: Both methods improve
Seaquest: Adaptive helps, learned performs worse than baseline
Space Invaders: Adaptive helps, but learned helps more
WizardOfWor: Adaptive worse than baseline, but learned helps a lot

MuJoCo enviroments to test

Hopper
Walker2d
HalfCheetah
Sparse versions of these? (from rllab)

",paramnois a comparison of paramet space nois method for explor in deep reinforc learn note thi project is not maintain reach out if youd like to help reboot it link to paper paramet space nois for explor httpsopenreviewnetforumidbybal2eaznoteidbybal2eaz noisi network for explor httpsopenreviewnetforumidrywhcpkawnoteidrywhcpkaw resourc openai baselin for use atari wrapper and replay buffer bearpaw pytorchclassif repo for util log train framework ikostrikov ppo implement for other util and ppo guidanc pytorchrl for dqn help pytorch dqn tutori for pytorch trick origin dqn paper sinc both paper use the origin hyperparamet for the most part todo implement ppo and mujoco env handl revisit log make sure everyth is there to reproduc result in paper implement plot matplotlib is in logger object mayb tri out visdom more test figur out differ combin of argument to ensur everyth interact well begin experi start with mujoco it cheaper atari game to test alien adapt help a lot learn show no improv enduro both method improv seaquest adapt help learn perform wors than baselin space invad adapt help but learn help more wizardofwor adapt wors than baselin but learn help a lot mujoco enviro to test hopper walker2d halfcheetah spars version of these from rllab,paramnoise a comparison of parameter space noise method for exploration in deep reinforcement learning note this project is not maintained reach out if youd like to help reboot it link to paper parameter space noise for exploration httpsopenreviewnetforumidbybal2eaznoteidbybal2eaz noisy network for exploration httpsopenreviewnetforumidrywhcpkawnoteidrywhcpkaw resource openai baseline for useful atari wrapper and replay buffer bearpaws pytorchclassification repo for utility logging training framework ikostrikovs ppo implementation for other utility and ppo guidance pytorchrl for dqn help pytorch dqn tutorial for pytorch trick original dqn paper since both paper use the original hyperparameters for the most part todos implement ppo and mujoco env handling revisit logging make sure everything is there to reproduce result in paper implement plotting matplotlib is in logger object maybe try out visdom more test figure out different combination of argument to ensure everythings interacting well begin experiment start with mujoco it cheaper atari game to test alien adaptive help a lot learned show no improvement enduro both method improve seaquest adaptive help learned performs worse than baseline space invader adaptive help but learned help more wizardofwor adaptive worse than baseline but learned help a lot mujoco enviroments to test hopper walker2d halfcheetah sparse version of these from rllab,paramnoise comparison parameter space noise method exploration deep reinforcement learning note project maintained reach youd like help reboot link paper parameter space noise exploration httpsopenreviewnetforumidbybal2eaznoteidbybal2eaz noisy network exploration httpsopenreviewnetforumidrywhcpkawnoteidrywhcpkaw resource openai baseline useful atari wrapper replay buffer bearpaws pytorchclassification repo utility logging training framework ikostrikovs ppo implementation utility ppo guidance pytorchrl dqn help pytorch dqn tutorial pytorch trick original dqn paper since paper use original hyperparameters part todos implement ppo mujoco env handling revisit logging make sure everything reproduce result paper implement plotting matplotlib logger object maybe try visdom test figure different combination argument ensure everythings interacting well begin experiment start mujoco cheaper atari game test alien adaptive help lot learned show improvement enduro method improve seaquest adaptive help learned performs worse baseline space invader adaptive help learned help wizardofwor adaptive worse baseline learned help lot mujoco enviroments test hopper walker2d halfcheetah sparse version rllab
Python ,"PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
Code accompanying CVPR'20 paper of the same title. Paper link: https://arxiv.org/abs/2003.03808



Table of Contents

PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
Table of Contents

What does it do?
How do I use it?
Usage

Prereqs
Data
Applying PULSE





What does it do?
Given a low-resolution input image, PULSE searches the outputs of a generative model (here, StyleGAN) for high-resolution images that are perceptually realistic and downscale correctly.

How do I use it?
The easiest way to apply PULSE to your own images is with our interactive demo, found at https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true.
If you want to try using this codebase, continue on.
Usage
The main file of interest for applying PULSE is run.py. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.
Prereqs
You will need to install cmake first (required for dlib, which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux. For the full set of required Python packages, create a Conda environment from the provided YAML, e.g.
conda create -f pulse.yml

Finally, you will need an internet connection the first time you run the code as it will automatically download the relevant pretrained model from Google Drive (if it has already been downloaded, it will use the local copy).
Data
By default, input data for run.py should be placed in ./input/ (though this can be modified). However, this assumes faces have already been aligned and downscaled. If you have data that is not already in this form, place it in realpics and run align_face.py which will automatically do this for you. (Again, all directories can be changed by command line arguments if more convenient.) You will at this stage pic a downscaling factor.
Note that if your data begins at a low resolution already, downscaling it further will retain very little information. In this case, you may wish to bicubically upsample (usually, to 1024x1024) and allow align_face.py to downscale for you.
The dataset we evaluated on was CelebA-HQ, but in our experience PULSE works with any picture of a realistic face.
Applying PULSE
Once your data is appropriately formatted, all you need to do is
python run.py

Enjoy!
Contact both Sachit Menon and Alex Damian (sachit.menon@duke.edu and alexandru.damian@duke.edu) for questions regarding this work.
",puls selfsupervis photo upsampl via latent space explor of gener model code accompani cvpr20 paper of the same titl paper link httpsarxivorgabs200303808 tabl of content puls selfsupervis photo upsampl via latent space explor of gener model tabl of content what doe it do how do i use it usag prereq data appli puls what doe it do given a lowresolut input imag puls search the output of a gener model here stylegan for highresolut imag that are perceptu realist and downscal correctli how do i use it the easiest way to appli puls to your own imag is with our interact demo found at httpscolabresearchgooglecomdrive1cygv0fosrhcqsvq3gkoymgtmt0g63xcuspsharingsandboxmodetru if you want to tri use thi codebas continu on usag the main file of interest for appli puls is runpi a full list of argument with descript can be found in that file here we describ those relev to get start prereq you will need to instal cmake first requir for dlib which is use for face align current the code onli work with cuda instal and therefor requir an appropri gpu and ha been test on linux for the full set of requir python packag creat a conda environ from the provid yaml eg conda creat f pulseyml final you will need an internet connect the first time you run the code as it will automat download the relev pretrain model from googl drive if it ha alreadi been download it will use the local copi data by default input data for runpi should be place in input though thi can be modifi howev thi assum face have alreadi been align and downscal if you have data that is not alreadi in thi form place it in realpic and run align_facepi which will automat do thi for you again all directori can be chang by command line argument if more conveni you will at thi stage pic a downscal factor note that if your data begin at a low resolut alreadi downscal it further will retain veri littl inform in thi case you may wish to bicub upsampl usual to 1024x1024 and allow align_facepi to downscal for you the dataset we evalu on wa celebahq but in our experi puls work with ani pictur of a realist face appli puls onc your data is appropri format all you need to do is python runpi enjoy contact both sachit menon and alex damian sachitmenondukeedu and alexandrudamiandukeedu for question regard thi work,pulse selfsupervised photo upsampling via latent space exploration of generative model code accompanying cvpr20 paper of the same title paper link httpsarxivorgabs200303808 table of content pulse selfsupervised photo upsampling via latent space exploration of generative model table of content what doe it do how do i use it usage prereqs data applying pulse what doe it do given a lowresolution input image pulse search the output of a generative model here stylegan for highresolution image that are perceptually realistic and downscale correctly how do i use it the easiest way to apply pulse to your own image is with our interactive demo found at httpscolabresearchgooglecomdrive1cygv0fosrhcqsvq3gkoymgtmt0g63xcuspsharingsandboxmodetrue if you want to try using this codebase continue on usage the main file of interest for applying pulse is runpy a full list of argument with description can be found in that file here we describe those relevant to getting started prereqs you will need to install cmake first required for dlib which is used for face alignment currently the code only work with cuda installed and therefore requires an appropriate gpu and ha been tested on linux for the full set of required python package create a conda environment from the provided yaml eg conda create f pulseyml finally you will need an internet connection the first time you run the code a it will automatically download the relevant pretrained model from google drive if it ha already been downloaded it will use the local copy data by default input data for runpy should be placed in input though this can be modified however this assumes face have already been aligned and downscaled if you have data that is not already in this form place it in realpics and run align_facepy which will automatically do this for you again all directory can be changed by command line argument if more convenient you will at this stage pic a downscaling factor note that if your data begin at a low resolution already downscaling it further will retain very little information in this case you may wish to bicubically upsample usually to 1024x1024 and allow align_facepy to downscale for you the dataset we evaluated on wa celebahq but in our experience pulse work with any picture of a realistic face applying pulse once your data is appropriately formatted all you need to do is python runpy enjoy contact both sachit menon and alex damian sachitmenondukeedu and alexandrudamiandukeedu for question regarding this work,pulse selfsupervised photo upsampling via latent space exploration generative model code accompanying cvpr20 paper title paper link httpsarxivorgabs200303808 table content pulse selfsupervised photo upsampling via latent space exploration generative model table content use usage prereqs data applying pulse given lowresolution input image pulse search output generative model stylegan highresolution image perceptually realistic downscale correctly use easiest way apply pulse image interactive demo found httpscolabresearchgooglecomdrive1cygv0fosrhcqsvq3gkoymgtmt0g63xcuspsharingsandboxmodetrue want try using codebase continue usage main file interest applying pulse runpy full list argument description found file describe relevant getting started prereqs need install cmake first required dlib used face alignment currently code work cuda installed therefore requires appropriate gpu tested linux full set required python package create conda environment provided yaml eg conda create f pulseyml finally need internet connection first time run code automatically download relevant pretrained model google drive already downloaded use local copy data default input data runpy placed input though modified however assumes face already aligned downscaled data already form place realpics run align_facepy automatically directory changed command line argument convenient stage pic downscaling factor note data begin low resolution already downscaling retain little information case may wish bicubically upsample usually 1024x1024 allow align_facepy downscale dataset evaluated celebahq experience pulse work picture realistic face applying pulse data appropriately formatted need python runpy enjoy contact sachit menon alex damian sachitmenondukeedu alexandrudamiandukeedu question regarding work
Python ,"BOOM
An easy-to-use multi-process Configuration Space Exploration pipeline framework.
Features

Easy to use: you only need to write your configuration file and modules, we will handle everything else!
Flexible: we offer common modules for QA pipelines, and it is very easy to develop your own modules.
Parameter tuning: automatically run on all possible parameter combinations and saves the results.
High efficiency: we use multiple processes to run the whole pipeline.
Compatibility: only compatible with Python 2.

Installation
First, install RabbitMQ and pip.
Then, clone the repository, run
make install

It will install dependencies using pip, and install this framework to your PATH.
Run
We offer a command-line executable program boom.
When executing, it will load conf.yaml from the current directory.
You can also specify the configuration file to use by adding option -conf PATH_TO_CONF_FILE.
For more options, run boom --help.
Docker image
To build the docker image, run make docker.
Tutorials
Please check out the two tutorials in examples folder.
Quick tutorails
Configuation Space Exploration
A QA pipeline may be consisted with several modules, each module may have some parameters.
Each combination of parameters corresponds to a path in the parameter space.
BOOM exhaustively run the pipeline on every possible parameter combinations, saves all intermediate results and final results.
The following figure shows a pipeline which has several modules.
The execution path is a tree which every level corresponds to a module, and each node stands for a different parameter value.
The leaf nodes are metric values.
Red arrows belongs to the best parameter combination.

Components
There are two main components to a BOOM pipeline: the modules and the configuration file.
Each pipeline can have an arbitrary number of modules (n >= 1) but there is only one configuration file that defines the pipeline.
BOOM works by instantiating each module and passing data along from one module to the next, allowing each to process and transform the data along the way.
Modules
The building block of a BOOM pipeline is the Module class. Each module in the pipeline takes in the data in the exact format returned by the previous module and return the data for the next module in its process() method. At a minimum, each user-defined module must subclass Module and implement the __init__() and process() methods.
Configuration Files
The configuration file defines the structure and composition of the pipeline and allows the user to define a parameter space for the pipeline to be executed over. The configuration is written is a YAML file and contains two core components: pipeline, where pipeline metadata is declared, and modules, where the pipeline composition is defined.
Following is the pipeline section of the toy example's configuration file:
pipeline:
    name: toy_pipeline
    rabbitmq_host: 127.0.0.1
    clean_up: false
    use_mongodb: false
    mongodb_host: 127.0.0.1

Under the pipeline key, there are 5 key-value pairs that need to be declared:
name
rabbitmq_host
clean_up
use_mongodb
mongodb_host

name allows the user to declare a name for the pipeline. rabbitmq_host and mongodb_host are simply the host addresses for RabbitMQ and MongoDB, respectively. clean_up is a boolean value that will delete intermediate output files if declared true. use_mongodb is a boolean value that will write data to MongoDB instead of files if declared true.
Following is the modules section of the toy example's configuration file:
pipeline: # Pipeline section, defines pipeline's properties
    mode: docker # Running mode, local or docker, default local
    name: toy_pipeline # Name of the pipeline
    rabbitmq_host: 127.0.0.1 # RabbitMQ's host uri
    clean_up: false # Whether the pipeline cleans up after finished running, true or false
    use_mongodb: true # Whether to use MongoDB, true or false, default false
    mongodb_host: 127.0.0.1 # MongoDB's host

modules:
    -   name: module_1 # Name of the module
        type: Sample # Type of the module
        input_file: data.json # Input file's uri
        output_module: module_2 # The following module's name
        instances: 1 # Number of instances of this module
        params:
            -   name: p1
                type: collection # Type of the param, int, float or collection
                values: # Possible vaules for collection param
                    - val1
                    - val2
                    - val3

            -   name: p2
                type: int
                start: 0
                end: 20
                step_size: 20

    -   name: module_2
        type: Sample
        output_module: module_3
        instances: 1
        params:
            -   name: p
                type: float
                start: 0.0
                end: 80.0
                step_size: 40.0
        
    -   name: module_3
        type: Sample
        output_module: module_4
        instances: 1

    -   name: module_4
        type: CSVWriter
        output_file: results.csv 
        instances: 1

The modules section of the configuration file should contain a list of modules. Each module consists of a set of key-value pairs which  must include name, type, input_file (first module only), output_module (or output_file for the last module), instances, and (optionally) params. params is a list of parameters, defined by a name, type (float, int, or collection). If the parameter is a float or int, the param should also contain start, end, and step_size. If the parameter is of type collection, then it should contain a values list.
API documentation
You can find the API documentation here.
Warning
This framework is still under heavy development,
please be careful.
",boom an easytous multiprocess configur space explor pipelin framework featur easi to use you onli need to write your configur file and modul we will handl everyth els flexibl we offer common modul for qa pipelin and it is veri easi to develop your own modul paramet tune automat run on all possibl paramet combin and save the result high effici we use multipl process to run the whole pipelin compat onli compat with python 2 instal first instal rabbitmq and pip then clone the repositori run make instal it will instal depend use pip and instal thi framework to your path run we offer a commandlin execut program boom when execut it will load confyaml from the current directori you can also specifi the configur file to use by ad option conf path_to_conf_fil for more option run boom help docker imag to build the docker imag run make docker tutori pleas check out the two tutori in exampl folder quick tutorail configu space explor a qa pipelin may be consist with sever modul each modul may have some paramet each combin of paramet correspond to a path in the paramet space boom exhaust run the pipelin on everi possibl paramet combin save all intermedi result and final result the follow figur show a pipelin which ha sever modul the execut path is a tree which everi level correspond to a modul and each node stand for a differ paramet valu the leaf node are metric valu red arrow belong to the best paramet combin compon there are two main compon to a boom pipelin the modul and the configur file each pipelin can have an arbitrari number of modul n 1 but there is onli one configur file that defin the pipelin boom work by instanti each modul and pass data along from one modul to the next allow each to process and transform the data along the way modul the build block of a boom pipelin is the modul class each modul in the pipelin take in the data in the exact format return by the previou modul and return the data for the next modul in it process method at a minimum each userdefin modul must subclass modul and implement the __init__ and process method configur file the configur file defin the structur and composit of the pipelin and allow the user to defin a paramet space for the pipelin to be execut over the configur is written is a yaml file and contain two core compon pipelin where pipelin metadata is declar and modul where the pipelin composit is defin follow is the pipelin section of the toy exampl configur file pipelin name toy_pipelin rabbitmq_host 127001 clean_up fals use_mongodb fals mongodb_host 127001 under the pipelin key there are 5 keyvalu pair that need to be declar name rabbitmq_host clean_up use_mongodb mongodb_host name allow the user to declar a name for the pipelin rabbitmq_host and mongodb_host are simpli the host address for rabbitmq and mongodb respect clean_up is a boolean valu that will delet intermedi output file if declar true use_mongodb is a boolean valu that will write data to mongodb instead of file if declar true follow is the modul section of the toy exampl configur file pipelin pipelin section defin pipelin properti mode docker run mode local or docker default local name toy_pipelin name of the pipelin rabbitmq_host 127001 rabbitmq host uri clean_up fals whether the pipelin clean up after finish run true or fals use_mongodb true whether to use mongodb true or fals default fals mongodb_host 127001 mongodb host modul name module_1 name of the modul type sampl type of the modul input_fil datajson input file uri output_modul module_2 the follow modul name instanc 1 number of instanc of thi modul param name p1 type collect type of the param int float or collect valu possibl vaul for collect param val1 val2 val3 name p2 type int start 0 end 20 step_siz 20 name module_2 type sampl output_modul module_3 instanc 1 param name p type float start 00 end 800 step_siz 400 name module_3 type sampl output_modul module_4 instanc 1 name module_4 type csvwriter output_fil resultscsv instanc 1 the modul section of the configur file should contain a list of modul each modul consist of a set of keyvalu pair which must includ name type input_fil first modul onli output_modul or output_fil for the last modul instanc and option param param is a list of paramet defin by a name type float int or collect if the paramet is a float or int the param should also contain start end and step_siz if the paramet is of type collect then it should contain a valu list api document you can find the api document here warn thi framework is still under heavi develop pleas be care,boom an easytouse multiprocess configuration space exploration pipeline framework feature easy to use you only need to write your configuration file and module we will handle everything else flexible we offer common module for qa pipeline and it is very easy to develop your own module parameter tuning automatically run on all possible parameter combination and save the result high efficiency we use multiple process to run the whole pipeline compatibility only compatible with python 2 installation first install rabbitmq and pip then clone the repository run make install it will install dependency using pip and install this framework to your path run we offer a commandline executable program boom when executing it will load confyaml from the current directory you can also specify the configuration file to use by adding option conf path_to_conf_file for more option run boom help docker image to build the docker image run make docker tutorial please check out the two tutorial in example folder quick tutorails configuation space exploration a qa pipeline may be consisted with several module each module may have some parameter each combination of parameter corresponds to a path in the parameter space boom exhaustively run the pipeline on every possible parameter combination save all intermediate result and final result the following figure show a pipeline which ha several module the execution path is a tree which every level corresponds to a module and each node stand for a different parameter value the leaf node are metric value red arrow belongs to the best parameter combination component there are two main component to a boom pipeline the module and the configuration file each pipeline can have an arbitrary number of module n 1 but there is only one configuration file that defines the pipeline boom work by instantiating each module and passing data along from one module to the next allowing each to process and transform the data along the way module the building block of a boom pipeline is the module class each module in the pipeline take in the data in the exact format returned by the previous module and return the data for the next module in it process method at a minimum each userdefined module must subclass module and implement the __init__ and process method configuration file the configuration file defines the structure and composition of the pipeline and allows the user to define a parameter space for the pipeline to be executed over the configuration is written is a yaml file and contains two core component pipeline where pipeline metadata is declared and module where the pipeline composition is defined following is the pipeline section of the toy example configuration file pipeline name toy_pipeline rabbitmq_host 127001 clean_up false use_mongodb false mongodb_host 127001 under the pipeline key there are 5 keyvalue pair that need to be declared name rabbitmq_host clean_up use_mongodb mongodb_host name allows the user to declare a name for the pipeline rabbitmq_host and mongodb_host are simply the host address for rabbitmq and mongodb respectively clean_up is a boolean value that will delete intermediate output file if declared true use_mongodb is a boolean value that will write data to mongodb instead of file if declared true following is the module section of the toy example configuration file pipeline pipeline section defines pipeline property mode docker running mode local or docker default local name toy_pipeline name of the pipeline rabbitmq_host 127001 rabbitmqs host uri clean_up false whether the pipeline clean up after finished running true or false use_mongodb true whether to use mongodb true or false default false mongodb_host 127001 mongodbs host module name module_1 name of the module type sample type of the module input_file datajson input file uri output_module module_2 the following module name instance 1 number of instance of this module params name p1 type collection type of the param int float or collection value possible vaules for collection param val1 val2 val3 name p2 type int start 0 end 20 step_size 20 name module_2 type sample output_module module_3 instance 1 params name p type float start 00 end 800 step_size 400 name module_3 type sample output_module module_4 instance 1 name module_4 type csvwriter output_file resultscsv instance 1 the module section of the configuration file should contain a list of module each module consists of a set of keyvalue pair which must include name type input_file first module only output_module or output_file for the last module instance and optionally params params is a list of parameter defined by a name type float int or collection if the parameter is a float or int the param should also contain start end and step_size if the parameter is of type collection then it should contain a value list api documentation you can find the api documentation here warning this framework is still under heavy development please be careful,boom easytouse multiprocess configuration space exploration pipeline framework feature easy use need write configuration file module handle everything else flexible offer common module qa pipeline easy develop module parameter tuning automatically run possible parameter combination save result high efficiency use multiple process run whole pipeline compatibility compatible python 2 installation first install rabbitmq pip clone repository run make install install dependency using pip install framework path run offer commandline executable program boom executing load confyaml current directory also specify configuration file use adding option conf path_to_conf_file option run boom help docker image build docker image run make docker tutorial please check two tutorial example folder quick tutorails configuation space exploration qa pipeline may consisted several module module may parameter combination parameter corresponds path parameter space boom exhaustively run pipeline every possible parameter combination save intermediate result final result following figure show pipeline several module execution path tree every level corresponds module node stand different parameter value leaf node metric value red arrow belongs best parameter combination component two main component boom pipeline module configuration file pipeline arbitrary number module n 1 one configuration file defines pipeline boom work instantiating module passing data along one module next allowing process transform data along way module building block boom pipeline module class module pipeline take data exact format returned previous module return data next module process method minimum userdefined module must subclass module implement __init__ process method configuration file configuration file defines structure composition pipeline allows user define parameter space pipeline executed configuration written yaml file contains two core component pipeline pipeline metadata declared module pipeline composition defined following pipeline section toy example configuration file pipeline name toy_pipeline rabbitmq_host 127001 clean_up false use_mongodb false mongodb_host 127001 pipeline key 5 keyvalue pair need declared name rabbitmq_host clean_up use_mongodb mongodb_host name allows user declare name pipeline rabbitmq_host mongodb_host simply host address rabbitmq mongodb respectively clean_up boolean value delete intermediate output file declared true use_mongodb boolean value write data mongodb instead file declared true following module section toy example configuration file pipeline pipeline section defines pipeline property mode docker running mode local docker default local name toy_pipeline name pipeline rabbitmq_host 127001 rabbitmqs host uri clean_up false whether pipeline clean finished running true false use_mongodb true whether use mongodb true false default false mongodb_host 127001 mongodbs host module name module_1 name module type sample type module input_file datajson input file uri output_module module_2 following module name instance 1 number instance module params name p1 type collection type param int float collection value possible vaules collection param val1 val2 val3 name p2 type int start 0 end 20 step_size 20 name module_2 type sample output_module module_3 instance 1 params name p type float start 00 end 800 step_size 400 name module_3 type sample output_module module_4 instance 1 name module_4 type csvwriter output_file resultscsv instance 1 module section configuration file contain list module module consists set keyvalue pair must include name type input_file first module output_module output_file last module instance optionally params params list parameter defined name type float int collection parameter float int param also contain start end step_size parameter type collection contain value list api documentation find api documentation warning framework still heavy development please careful
Python ,"atiamML
Chemla - Latent representations for real-time synthesis space exploration


code/ contains project's code along with simple scripts that demonstrate the use of the developped methodologies - see code/README.md
report/ contains the report in PDF format along with the LaTeX source and eventual figures - see report/README.md
toy/ contains toy datasets, along with the procedural scripts to generate it - see toy/README.md

",atiamml chemla latent represent for realtim synthesi space explor code contain project code along with simpl script that demonstr the use of the develop methodolog see codereadmemd report contain the report in pdf format along with the latex sourc and eventu figur see reportreadmemd toy contain toy dataset along with the procedur script to gener it see toyreadmemd,atiamml chemla latent representation for realtime synthesis space exploration code contains project code along with simple script that demonstrate the use of the developped methodology see codereadmemd report contains the report in pdf format along with the latex source and eventual figure see reportreadmemd toy contains toy datasets along with the procedural script to generate it see toyreadmemd,atiamml chemla latent representation realtime synthesis space exploration code contains project code along simple script demonstrate use developped methodology see codereadmemd report contains report pdf format along latex source eventual figure see reportreadmemd toy contains toy datasets along procedural script generate see toyreadmemd
Python ,"Safe-Explorer
Introduction
This repository contains Pytorch implementation of paper ""Safe Exploration in Continuous Action Spaces"" [Dalal et al.] along with ""Continuous Control With Deep Reinforcement
Learning"" [Lillicrap et al.]. Dalal et al. present a closed form analytically optimal solution to ensure safety in continuous action space. The proposed ""safety layer"",
makes the smallest possible perturbation to the original action such that safety constraints are satisfied.

Dalal et al. also propose two new domains BallND and Spaceship which are governed by first and second order dynamics respectively. In Spaceship domain agent receives a reward only on task completion, while BallND has continuous reward based distance from the target. Implementation of both of these tasks extend OpenAI gym's environment interface (gym.Env).
Setup
The code requires Python 3.6+ and is tested with torch 1.1.0. To install dependencies run,
pip install -r requirements.txt
Training
To obtain list of parameters and their default values run,
python -m safe_explorer.main --help
Train the model by simply running,
BallND
python -m safe_explorer.main --main_trainer_task ballnd
Spaceship
python -m safe_explorer.main --main_trainer_task spaceship
Monitor training with Tensorboard,
tensorboard --logdir=runs
Results
To be updated.
Acknowledgement
Some modifications in DDPG implementation are based OpenAI Spinning Up implement.
References


Lillicrap, Timothy P., et al. ""Continuous control with deep reinforcement learning."" arXiv preprint arXiv:1509.02971 (2015).


Dalal, Gal, et al. ""Safe exploration in continuous action spaces."" arXiv preprint arXiv:1801.08757 (2018).


",safeexplor introduct thi repositori contain pytorch implement of paper safe explor in continu action space dalal et al along with continu control with deep reinforc learn lillicrap et al dalal et al present a close form analyt optim solut to ensur safeti in continu action space the propos safeti layer make the smallest possibl perturb to the origin action such that safeti constraint are satisfi dalal et al also propos two new domain ballnd and spaceship which are govern by first and second order dynam respect in spaceship domain agent receiv a reward onli on task complet while ballnd ha continu reward base distanc from the target implement of both of these task extend openai gym environ interfac gymenv setup the code requir python 36 and is test with torch 110 to instal depend run pip instal r requirementstxt train to obtain list of paramet and their default valu run python m safe_explorermain help train the model by simpli run ballnd python m safe_explorermain main_trainer_task ballnd spaceship python m safe_explorermain main_trainer_task spaceship monitor train with tensorboard tensorboard logdirrun result to be updat acknowledg some modif in ddpg implement are base openai spin up implement refer lillicrap timothi p et al continu control with deep reinforc learn arxiv preprint arxiv150902971 2015 dalal gal et al safe explor in continu action space arxiv preprint arxiv180108757 2018,safeexplorer introduction this repository contains pytorch implementation of paper safe exploration in continuous action space dalal et al along with continuous control with deep reinforcement learning lillicrap et al dalal et al present a closed form analytically optimal solution to ensure safety in continuous action space the proposed safety layer make the smallest possible perturbation to the original action such that safety constraint are satisfied dalal et al also propose two new domain ballnd and spaceship which are governed by first and second order dynamic respectively in spaceship domain agent receives a reward only on task completion while ballnd ha continuous reward based distance from the target implementation of both of these task extend openai gym environment interface gymenv setup the code requires python 36 and is tested with torch 110 to install dependency run pip install r requirementstxt training to obtain list of parameter and their default value run python m safe_explorermain help train the model by simply running ballnd python m safe_explorermain main_trainer_task ballnd spaceship python m safe_explorermain main_trainer_task spaceship monitor training with tensorboard tensorboard logdirruns result to be updated acknowledgement some modification in ddpg implementation are based openai spinning up implement reference lillicrap timothy p et al continuous control with deep reinforcement learning arxiv preprint arxiv150902971 2015 dalal gal et al safe exploration in continuous action space arxiv preprint arxiv180108757 2018,safeexplorer introduction repository contains pytorch implementation paper safe exploration continuous action space dalal et al along continuous control deep reinforcement learning lillicrap et al dalal et al present closed form analytically optimal solution ensure safety continuous action space proposed safety layer make smallest possible perturbation original action safety constraint satisfied dalal et al also propose two new domain ballnd spaceship governed first second order dynamic respectively spaceship domain agent receives reward task completion ballnd continuous reward based distance target implementation task extend openai gym environment interface gymenv setup code requires python 36 tested torch 110 install dependency run pip install r requirementstxt training obtain list parameter default value run python safe_explorermain help train model simply running ballnd python safe_explorermain main_trainer_task ballnd spaceship python safe_explorermain main_trainer_task spaceship monitor training tensorboard tensorboard logdirruns result updated acknowledgement modification ddpg implementation based openai spinning implement reference lillicrap timothy p et al continuous control deep reinforcement learning arxiv preprint arxiv150902971 2015 dalal gal et al safe exploration continuous action space arxiv preprint arxiv180108757 2018
Java ,"About
CyberBiology - life simulator on computer
Build
Linux
Build artifact:
make
You can run after build:
java ./build/world.jar

",about cyberbiolog life simul on comput build linux build artifact make you can run after build java buildworldjar,about cyberbiology life simulator on computer build linux build artifact make you can run after build java buildworldjar,cyberbiology life simulator computer build linux build artifact make run build java buildworldjar
Java ,"Caleydo - Visualization for Molecular Biology

Caleydo is a visualization framework for molecular biology data. It is targeted at analyzing multiple heterogeneous but related tabular datasets (e.g.,  mRNA expression, copy number status and clinical variables), stratifications or clusters in these datasets and their relationships to biological pathways.
For user documentation please refer to the Caleydo Help. For general information and downloads based on binaries please use the Caleydo Website. This guide assumes that you want to install Caleydo from source.
Installation
Caleydo uses Java, OpenGL and the Eclipse Rich Client Platform (RCP). Things you need to install before being able to run Caleydo:

Eclipse Kepler for RCP and RAP Developers, which you can get from the eclipse download page. Other Eclipse versions won't work.
Install EGit in Eclipse using software updates.
Java SDK >= 1.7

To install Caleydo use EGit within Eclipse and clone the repository. Each directory in the caleydo-dev folder corresponds to an Eclipse project. Here is a good tutorial on how to import Eclipse projects from git.
If you want to use ssh (instead of https) for communicating with github out of eclipse follow these instructions.
You will have to generate a new RSA key and save it to you ~/.ssh folder. Remeber to set a passphrase for you key. This will result in a file ida_rsa and ida_rsa.pub turning up in your ssh folder.
Save your public rsa key with your eclipse account folder.
When cloning the repository follow the above tutorial. Don't change the username ""git"" to your username!
Team
Caleydo is an academic project currently developed by members of

Institute for Computer Graphics and Vision at Graz University of Technology, Austria
Institute of Computer Graphics at Johannes Kepler University Linz, Austria
Pfister Lab at the School of Engineering and Applied Sciences, Harvard University, Cambridge, USA
Park Lab at Harvard Medical School, Boston, USA

Acknowledgements
Caleydo makes use of a range of open source tools, bioinformatics resources and pre-packages several datasets, which we gratefully acknowledge here.
Software Libraries

Bubble Sets - A Java implementation of the visualization technique.
CDK - The Chemistry Development Kit.
JGrahT - A graph library.
Jogl - Java bindings for OpenGL.
PathVisio - loading and parsing WikiPathways.
WordHoard - statistical utilities.

Bioinformatics Resources

David Bioinformatics Resources - Gene ID mapping.
KEGG - Pathways.
WikiPathways - Pathways.

Datasets

CCLE - BROAD Institute Cancer Cell Line Encyclopedia.
TCGA - The Cancer Genome Atlas.

Resources

GitHub - Hosting our source code.
WebStorm - Free developer licenses for our web projects from JetBrains.

",caleydo visual for molecular biolog caleydo is a visual framework for molecular biolog data it is target at analyz multipl heterogen but relat tabular dataset eg mrna express copi number statu and clinic variabl stratif or cluster in these dataset and their relationship to biolog pathway for user document pleas refer to the caleydo help for gener inform and download base on binari pleas use the caleydo websit thi guid assum that you want to instal caleydo from sourc instal caleydo use java opengl and the eclips rich client platform rcp thing you need to instal befor be abl to run caleydo eclips kepler for rcp and rap develop which you can get from the eclips download page other eclips version wont work instal egit in eclips use softwar updat java sdk 17 to instal caleydo use egit within eclips and clone the repositori each directori in the caleydodev folder correspond to an eclips project here is a good tutori on how to import eclips project from git if you want to use ssh instead of http for commun with github out of eclips follow these instruct you will have to gener a new rsa key and save it to you ssh folder remeb to set a passphras for you key thi will result in a file ida_rsa and ida_rsapub turn up in your ssh folder save your public rsa key with your eclips account folder when clone the repositori follow the abov tutori dont chang the usernam git to your usernam team caleydo is an academ project current develop by member of institut for comput graphic and vision at graz univers of technolog austria institut of comput graphic at johann kepler univers linz austria pfister lab at the school of engin and appli scienc harvard univers cambridg usa park lab at harvard medic school boston usa acknowledg caleydo make use of a rang of open sourc tool bioinformat resourc and prepackag sever dataset which we grate acknowledg here softwar librari bubbl set a java implement of the visual techniqu cdk the chemistri develop kit jgraht a graph librari jogl java bind for opengl pathvisio load and pars wikipathway wordhoard statist util bioinformat resourc david bioinformat resourc gene id map kegg pathway wikipathway pathway dataset ccle broad institut cancer cell line encyclopedia tcga the cancer genom atla resourc github host our sourc code webstorm free develop licens for our web project from jetbrain,caleydo visualization for molecular biology caleydo is a visualization framework for molecular biology data it is targeted at analyzing multiple heterogeneous but related tabular datasets eg mrna expression copy number status and clinical variable stratification or cluster in these datasets and their relationship to biological pathway for user documentation please refer to the caleydo help for general information and downloads based on binary please use the caleydo website this guide assumes that you want to install caleydo from source installation caleydo us java opengl and the eclipse rich client platform rcp thing you need to install before being able to run caleydo eclipse kepler for rcp and rap developer which you can get from the eclipse download page other eclipse version wont work install egit in eclipse using software update java sdk 17 to install caleydo use egit within eclipse and clone the repository each directory in the caleydodev folder corresponds to an eclipse project here is a good tutorial on how to import eclipse project from git if you want to use ssh instead of http for communicating with github out of eclipse follow these instruction you will have to generate a new rsa key and save it to you ssh folder remeber to set a passphrase for you key this will result in a file ida_rsa and ida_rsapub turning up in your ssh folder save your public rsa key with your eclipse account folder when cloning the repository follow the above tutorial dont change the username git to your username team caleydo is an academic project currently developed by member of institute for computer graphic and vision at graz university of technology austria institute of computer graphic at johannes kepler university linz austria pfister lab at the school of engineering and applied science harvard university cambridge usa park lab at harvard medical school boston usa acknowledgement caleydo make use of a range of open source tool bioinformatics resource and prepackages several datasets which we gratefully acknowledge here software library bubble set a java implementation of the visualization technique cdk the chemistry development kit jgraht a graph library jogl java binding for opengl pathvisio loading and parsing wikipathways wordhoard statistical utility bioinformatics resource david bioinformatics resource gene id mapping kegg pathway wikipathways pathway datasets ccle broad institute cancer cell line encyclopedia tcga the cancer genome atlas resource github hosting our source code webstorm free developer license for our web project from jetbrains,caleydo visualization molecular biology caleydo visualization framework molecular biology data targeted analyzing multiple heterogeneous related tabular datasets eg mrna expression copy number status clinical variable stratification cluster datasets relationship biological pathway user documentation please refer caleydo help general information downloads based binary please use caleydo website guide assumes want install caleydo source installation caleydo us java opengl eclipse rich client platform rcp thing need install able run caleydo eclipse kepler rcp rap developer get eclipse download page eclipse version wont work install egit eclipse using software update java sdk 17 install caleydo use egit within eclipse clone repository directory caleydodev folder corresponds eclipse project good tutorial import eclipse project git want use ssh instead http communicating github eclipse follow instruction generate new rsa key save ssh folder remeber set passphrase key result file ida_rsa ida_rsapub turning ssh folder save public rsa key eclipse account folder cloning repository follow tutorial dont change username git username team caleydo academic project currently developed member institute computer graphic vision graz university technology austria institute computer graphic johannes kepler university linz austria pfister lab school engineering applied science harvard university cambridge usa park lab harvard medical school boston usa acknowledgement caleydo make use range open source tool bioinformatics resource prepackages several datasets gratefully acknowledge software library bubble set java implementation visualization technique cdk chemistry development kit jgraht graph library jogl java binding opengl pathvisio loading parsing wikipathways wordhoard statistical utility bioinformatics resource david bioinformatics resource gene id mapping kegg pathway wikipathways pathway datasets ccle broad institute cancer cell line encyclopedia tcga cancer genome atlas resource github hosting source code webstorm free developer license web project jetbrains
Java ,"20n/act: An open source platform for bioengineering
20n/act is the data aggregation and prediction system for bioengineering. For a target molecule, 20n/act predicts DNA insertions into cells (usually a microbe such as E. coli or S. cerevisiae) that modify the cell. These modified cells make the target molecule by fermentation from sugar. We call these ""target molecules/chemicals"" the bioreachables. The system predicted/invented the first bio-route to Acetaminophen/Tylenol/APAP. Read more on our blog post. The technical details of the APAP work can be found in patents applications on coli and yeast fermentation.
Getting started
Live preview
See predicted DNA for 11 sample molecules at Bioreachables Preview (Login:Pass = public:preview). Due to limitations we can only make a preview version available. If you'd like the full version please contact us.
Building the project
Checkout the repo. Follow instructions to run to create the database and prediction corpus. If you'd rather get a pre-packaged DB without creating it yourself please contact us. The codebase is public to further the state-of-the-art in automating biological engineering/synthetic biology. Some modules are specific to microbes, but most of the predictive stack deals with host-agnostic enzymatic biochemistry.
Components of 20n/act
Predictor stack
Answers ""what DNA do I insert if I want to make my chemical?""




Module
Function
Code




1
Installer
Integrates heterogeneous raw data
Code:com.act.reachables.initdb Run:Instructions


2
Reaction operator (RO) inference
Mines rules of enzymatic catalysis
Code:biointerpretation module


2
Structure Activity Relationship (SAR) inference
Mines substrate specificities
Code:biointerpretation module


3
Biointerpretation
Mechanistic validation of enzymatic transforms (using ROs)
Code:com.act.biointerpretation.BiointerpretationDriver  Run:Instructions


4
Reachables computation
Exhaustively enumerates all biosynthesizable chemicals
Code:com.act.reachables.reachablesCode:com.act.reachables.postprocess_reachablesRun:Instructions


5
Cascades computation
Exhaustively enumerates all enzymatic routes from metabolic natives to bioreachable target
Code:com.act.reachables.cascadesRun:Instructions


6
DNA designer
Computes protein & DNA design (coli specific) for each non-natural enzymatic path
Code:org.twentyn.proteintodna.ProteinToDNADriverRun:Instructions


7
Application miner
Mines chemical applications using web searches [Bing]
Code:act.installer.bing.BingSearcherRun:Instructions


8
Enzymatic biochemistry NLP
Text -> Chemical tokens -> Biologically feasible reactions using ROs
Code:act.shared.TextToRxnsFrontend:TextToRxnsUI


9
Patent search
Chemical -> Patents
Code:act.installer.reachablesexplorer.PatentFinderRun:Instructions


10
Bioreachables wiki
Aggregates reachables, cascades, use cases, protein and DNA designs into a user friendly wiki interface
Documentation



  
Analytics
Answers ""Is my bio-engineered cell doing what I want it to?""




Module
Function
Code




1
LCMS: Untargeted metabolomics
Deep-learnt signal processing to identify all chemical [side]effects of DNA engineering on cell
Code:DeepLearningLcmsPeakCode:com.act.lcms.UntargetedMetabolomics


2
LCMS: Comparative visualization
Visualizing traces side-by-side from untargeted evaluation of over and underexpressed peaks
Doc:LCMSDataVisualisation



  
Unit economics of bioproduction
Answers ""Can I use bio-production to make this chemical at scale?""




Module
Function
Code




1
Cost model: Manufacturing unit economics for large scale production
It backcalculates cell efficiency (yield, titers, productivity) objectives based on given COGS ($ per ton) of target chemical. From cell efficiency objectives it guesstimates the R&D investment (money and time) and ROI expectations
Code:act.installer.bing.CostModelCode (viz server):costModelUISource model:XLS



License and Contributing
Code licensed under the GNU General Public License v3.0.
If an alternative license is desired, please contact 20n.
Original Authors

Saurabh Srivastava
J. Christopher Anderson
Mark T. Daly
Michael Lampe
Thomas Legrand
Vijay Ramakrishnan
Gil Goldshlager
Nishant Kakar

",20nact an open sourc platform for bioengin 20nact is the data aggreg and predict system for bioengin for a target molecul 20nact predict dna insert into cell usual a microb such as e coli or s cerevisia that modifi the cell these modifi cell make the target molecul by ferment from sugar we call these target moleculeschem the bioreach the system predictedinv the first biorout to acetaminophentylenolapap read more on our blog post the technic detail of the apap work can be found in patent applic on coli and yeast ferment get start live preview see predict dna for 11 sampl molecul at bioreach preview loginpass publicpreview due to limit we can onli make a preview version avail if youd like the full version pleas contact us build the project checkout the repo follow instruct to run to creat the databas and predict corpu if youd rather get a prepackag db without creat it yourself pleas contact us the codebas is public to further the stateoftheart in autom biolog engineeringsynthet biolog some modul are specif to microb but most of the predict stack deal with hostagnost enzymat biochemistri compon of 20nact predictor stack answer what dna do i insert if i want to make my chemic modul function code 1 instal integr heterogen raw data codecomactreachablesinitdb runinstruct 2 reaction oper ro infer mine rule of enzymat catalysi codebiointerpret modul 2 structur activ relationship sar infer mine substrat specif codebiointerpret modul 3 biointerpret mechanist valid of enzymat transform use ro codecomactbiointerpretationbiointerpretationdriv runinstruct 4 reachabl comput exhaust enumer all biosynthesiz chemic codecomactreachablesreachablescodecomactreachablespostprocess_reachablesruninstruct 5 cascad comput exhaust enumer all enzymat rout from metabol nativ to bioreach target codecomactreachablescascadesruninstruct 6 dna design comput protein dna design coli specif for each nonnatur enzymat path codeorgtwentynproteintodnaproteintodnadriverruninstruct 7 applic miner mine chemic applic use web search bing codeactinstallerbingbingsearcherruninstruct 8 enzymat biochemistri nlp text chemic token biolog feasibl reaction use ro codeactsharedtexttorxnsfrontendtexttorxnsui 9 patent search chemic patent codeactinstallerreachablesexplorerpatentfinderruninstruct 10 bioreach wiki aggreg reachabl cascad use case protein and dna design into a user friendli wiki interfac document analyt answer is my bioengin cell do what i want it to modul function code 1 lcm untarget metabolom deeplearnt signal process to identifi all chemic sideeffect of dna engin on cell codedeeplearninglcmspeakcodecomactlcmsuntargetedmetabolom 2 lcm compar visual visual trace sidebysid from untarget evalu of over and underexpress peak doclcmsdatavisualis unit econom of bioproduct answer can i use bioproduct to make thi chemic at scale modul function code 1 cost model manufactur unit econom for larg scale product it backcalcul cell effici yield titer product object base on given cog per ton of target chemic from cell effici object it guesstim the rd invest money and time and roi expect codeactinstallerbingcostmodelcod viz servercostmodeluisourc modelxl licens and contribut code licens under the gnu gener public licens v30 if an altern licens is desir pleas contact 20n origin author saurabh srivastava j christoph anderson mark t dali michael lamp thoma legrand vijay ramakrishnan gil goldshlag nishant kakar,20nact an open source platform for bioengineering 20nact is the data aggregation and prediction system for bioengineering for a target molecule 20nact predicts dna insertion into cell usually a microbe such a e coli or s cerevisiae that modify the cell these modified cell make the target molecule by fermentation from sugar we call these target moleculeschemicals the bioreachables the system predictedinvented the first bioroute to acetaminophentylenolapap read more on our blog post the technical detail of the apap work can be found in patent application on coli and yeast fermentation getting started live preview see predicted dna for 11 sample molecule at bioreachables preview loginpass publicpreview due to limitation we can only make a preview version available if youd like the full version please contact u building the project checkout the repo follow instruction to run to create the database and prediction corpus if youd rather get a prepackaged db without creating it yourself please contact u the codebase is public to further the stateoftheart in automating biological engineeringsynthetic biology some module are specific to microbe but most of the predictive stack deal with hostagnostic enzymatic biochemistry component of 20nact predictor stack answer what dna do i insert if i want to make my chemical module function code 1 installer integrates heterogeneous raw data codecomactreachablesinitdb runinstructions 2 reaction operator ro inference mine rule of enzymatic catalysis codebiointerpretation module 2 structure activity relationship sar inference mine substrate specificity codebiointerpretation module 3 biointerpretation mechanistic validation of enzymatic transforms using ro codecomactbiointerpretationbiointerpretationdriver runinstructions 4 reachables computation exhaustively enumerates all biosynthesizable chemical codecomactreachablesreachablescodecomactreachablespostprocess_reachablesruninstructions 5 cascade computation exhaustively enumerates all enzymatic route from metabolic native to bioreachable target codecomactreachablescascadesruninstructions 6 dna designer computes protein dna design coli specific for each nonnatural enzymatic path codeorgtwentynproteintodnaproteintodnadriverruninstructions 7 application miner mine chemical application using web search bing codeactinstallerbingbingsearcherruninstructions 8 enzymatic biochemistry nlp text chemical token biologically feasible reaction using ro codeactsharedtexttorxnsfrontendtexttorxnsui 9 patent search chemical patent codeactinstallerreachablesexplorerpatentfinderruninstructions 10 bioreachables wiki aggregate reachables cascade use case protein and dna design into a user friendly wiki interface documentation analytics answer is my bioengineered cell doing what i want it to module function code 1 lcm untargeted metabolomics deeplearnt signal processing to identify all chemical sideeffects of dna engineering on cell codedeeplearninglcmspeakcodecomactlcmsuntargetedmetabolomics 2 lcm comparative visualization visualizing trace sidebyside from untargeted evaluation of over and underexpressed peak doclcmsdatavisualisation unit economics of bioproduction answer can i use bioproduction to make this chemical at scale module function code 1 cost model manufacturing unit economics for large scale production it backcalculates cell efficiency yield titer productivity objective based on given cog per ton of target chemical from cell efficiency objective it guesstimate the rd investment money and time and roi expectation codeactinstallerbingcostmodelcode viz servercostmodeluisource modelxls license and contributing code licensed under the gnu general public license v30 if an alternative license is desired please contact 20n original author saurabh srivastava j christopher anderson mark t daly michael lampe thomas legrand vijay ramakrishnan gil goldshlager nishant kakar,20nact open source platform bioengineering 20nact data aggregation prediction system bioengineering target molecule 20nact predicts dna insertion cell usually microbe e coli cerevisiae modify cell modified cell make target molecule fermentation sugar call target moleculeschemicals bioreachables system predictedinvented first bioroute acetaminophentylenolapap read blog post technical detail apap work found patent application coli yeast fermentation getting started live preview see predicted dna 11 sample molecule bioreachables preview loginpass publicpreview due limitation make preview version available youd like full version please contact u building project checkout repo follow instruction run create database prediction corpus youd rather get prepackaged db without creating please contact u codebase public stateoftheart automating biological engineeringsynthetic biology module specific microbe predictive stack deal hostagnostic enzymatic biochemistry component 20nact predictor stack answer dna insert want make chemical module function code 1 installer integrates heterogeneous raw data codecomactreachablesinitdb runinstructions 2 reaction operator ro inference mine rule enzymatic catalysis codebiointerpretation module 2 structure activity relationship sar inference mine substrate specificity codebiointerpretation module 3 biointerpretation mechanistic validation enzymatic transforms using ro codecomactbiointerpretationbiointerpretationdriver runinstructions 4 reachables computation exhaustively enumerates biosynthesizable chemical codecomactreachablesreachablescodecomactreachablespostprocess_reachablesruninstructions 5 cascade computation exhaustively enumerates enzymatic route metabolic native bioreachable target codecomactreachablescascadesruninstructions 6 dna designer computes protein dna design coli specific nonnatural enzymatic path codeorgtwentynproteintodnaproteintodnadriverruninstructions 7 application miner mine chemical application using web search bing codeactinstallerbingbingsearcherruninstructions 8 enzymatic biochemistry nlp text chemical token biologically feasible reaction using ro codeactsharedtexttorxnsfrontendtexttorxnsui 9 patent search chemical patent codeactinstallerreachablesexplorerpatentfinderruninstructions 10 bioreachables wiki aggregate reachables cascade use case protein dna design user friendly wiki interface documentation analytics answer bioengineered cell want module function code 1 lcm untargeted metabolomics deeplearnt signal processing identify chemical sideeffects dna engineering cell codedeeplearninglcmspeakcodecomactlcmsuntargetedmetabolomics 2 lcm comparative visualization visualizing trace sidebyside untargeted evaluation underexpressed peak doclcmsdatavisualisation unit economics bioproduction answer use bioproduction make chemical scale module function code 1 cost model manufacturing unit economics large scale production backcalculates cell efficiency yield titer productivity objective based given cog per ton target chemical cell efficiency objective guesstimate rd investment money time roi expectation codeactinstallerbingcostmodelcode viz servercostmodeluisource modelxls license contributing code licensed gnu general public license v30 alternative license desired please contact 20n original author saurabh srivastava j christopher anderson mark daly michael lampe thomas legrand vijay ramakrishnan gil goldshlager nishant kakar
Java ,"This is the official University of Minnesota Populus Git Repository.
About
Populus is a package of educational software allowing students to manipulate ecological and evolutionary models, producing graphical representations of their dynamics.  It also contains an integrated help system discussing each of the models.
Please go to https://www.cbs.umn.edu/populus/ for more information about Populus.
Copyright
Don Alstad 
Department of Ecology, Evolution & Behavior 
University of Minnesota 
1987 Upper Buford Circle 
St. Paul, MN 55108-6097
How to run
Installers will be available on the main Populus page, https://www.cbs.umn.edu/populus/.
To build and run from source, use the gradle wrapper in the top directory:
$ ./gradlew build
$ ./gradlew run
If you are using JDK 14 or later, you can also create installer/packager:
$ ./gradlew jpackage
The images and installers will be in the build directory.
Note that for Windows, you'll need to run gradlew.bat instead, and need to install WiX to package.
Feedback
If you find bugs, irregularities, places for improvement, or have other comments, please email populus@umn.edu.
Language Support
Spanish translations of some of the more basic models are provided. We would be interested in corresponding with people who would be able to help with other translations.  If interested, please email populus@umn.edu.
Programming Credits
Java versions: Amos Anderson, Lars Roe, Sharareh Noorbaloochi 
DOS versions: Chris Bratteli
License

",thi is the offici univers of minnesota populu git repositori about populu is a packag of educ softwar allow student to manipul ecolog and evolutionari model produc graphic represent of their dynam it also contain an integr help system discuss each of the model pleas go to httpswwwcbsumnedupopulu for more inform about populu copyright don alstad depart of ecolog evolut behavior univers of minnesota 1987 upper buford circl st paul mn 551086097 how to run instal will be avail on the main populu page httpswwwcbsumnedupopulu to build and run from sourc use the gradl wrapper in the top directori gradlew build gradlew run if you are use jdk 14 or later you can also creat installerpackag gradlew jpackag the imag and instal will be in the build directori note that for window youll need to run gradlewbat instead and need to instal wix to packag feedback if you find bug irregular place for improv or have other comment pleas email populusumnedu languag support spanish translat of some of the more basic model are provid we would be interest in correspond with peopl who would be abl to help with other translat if interest pleas email populusumnedu program credit java version amo anderson lar roe sharareh noorbaloochi do version chri bratt licens,this is the official university of minnesota populus git repository about populus is a package of educational software allowing student to manipulate ecological and evolutionary model producing graphical representation of their dynamic it also contains an integrated help system discussing each of the model please go to httpswwwcbsumnedupopulus for more information about populus copyright don alstad department of ecology evolution behavior university of minnesota 1987 upper buford circle st paul mn 551086097 how to run installers will be available on the main populus page httpswwwcbsumnedupopulus to build and run from source use the gradle wrapper in the top directory gradlew build gradlew run if you are using jdk 14 or later you can also create installerpackager gradlew jpackage the image and installers will be in the build directory note that for window youll need to run gradlewbat instead and need to install wix to package feedback if you find bug irregularity place for improvement or have other comment please email populusumnedu language support spanish translation of some of the more basic model are provided we would be interested in corresponding with people who would be able to help with other translation if interested please email populusumnedu programming credit java version amos anderson lars roe sharareh noorbaloochi do version chris bratteli license,official university minnesota populus git repository populus package educational software allowing student manipulate ecological evolutionary model producing graphical representation dynamic also contains integrated help system discussing model please go httpswwwcbsumnedupopulus information populus copyright alstad department ecology evolution behavior university minnesota 1987 upper buford circle st paul mn 551086097 run installers available main populus page httpswwwcbsumnedupopulus build run source use gradle wrapper top directory gradlew build gradlew run using jdk 14 later also create installerpackager gradlew jpackage image installers build directory note window youll need run gradlewbat instead need install wix package feedback find bug irregularity place improvement comment please email populusumnedu language support spanish translation basic model provided would interested corresponding people would able help translation interested please email populusumnedu programming credit java version amos anderson lars roe sharareh noorbaloochi do version chris bratteli license
Java ,"
libSBOLj provides the core Java interfaces and their implementation for
the Synthetic Biology Open Language (SBOL). The library provides an API to
work with SBOL objects, the functionality to read and write SBOL documents as XML/RDF files, and a validator to check the
correctness of SBOL models.
Using the libSBOLj library
In a Maven project:
In a Maven project that utilizes the libSBOLj library, add a dependency in the Maven project's pom.xml file.
<dependency>
	<groupId>org.sbolstandard</groupId>
	<artifactId>libSBOLj</artifactId>
	<version>2.4.0</version>
</dependency>

In a non-Maven project:
Download libSBOLj-<version>-withDependencies.jar from the latest github release.
Running in the command line:
libSBOLj comes with a command-line interface (CLI) that can be used to validate SBOL files. You can execute
libSBOLj-<version>-withDependencies.jar to validate and convert files as follows.
java -jar libSBOLj-<version>-withDependencies.jar <inputFile> -l <language>

If validation/conversion is successful, the program will print the contents of the input file in the specified language (SBOL1, SBOL2, GenBank, and FASTA). You can also output the result to a file.
java -jar libSBOLj-<version>-withDependencies.jar <inputFile> -l <language> -o <outputFile>

One can also provide a URI using the -s flag for a TopLevel object, and only that object and all its dependencies will be output.
If validation fails with an error, there will be a message printed about the validation error.  The -f flag can be used to indicate that validation should continue after the first error, while the -d flag will provide a detailed error trace on a validation error.
In addition to checking all required validation rules, it will also check if the URIs are compliant and whether the SBOL document is complete (i.e., all referenced objects are contained within the file).  These validation checks can be turned off with the -n and -i flags, respectively.  It is also possible to turn-on best practices checking using the -b flag.
When the input file is being converted into SBOL 2.0, the conversion should be provided a default URI prefix.  It can also be provided a default version, if desired.  Finally, the -t flag will insert the type of top level objects into the URI during conversion, if desired.
java -jar libSBOLj-<version>-withDependencies.jar <inFile> -o <outFile> -p <URIprefix> -v <version>

Finally, it can be used to compare the equality of the contents of two SBOL files using the command below:
java -jar libSBOLj-<version>-withDependencies.jar <firstSBOLFile> -e <secondSBOLFile>

Using the latest libSBOLj SNAPSHOT
Getting the libSBOLj source

Create a GitHub account.
Setup Git on your machine.
Clone the libSBOLj GitHub repository to your machine.
Retrieve the SBOLTestSuite Submodule using the instructions below.

Retrieving SBOLTestSuite Submodule
git submodule update --init --recursive

Compiling and Packaging libSBOLj


Setup Apache Maven. A tutorial on using Apache Maven is provided here.


In the command line, change to the libSBOLj directory (e.g. cd /path/to/libSBOLj) and execute the following command


mvn package

This will compile the libSBOLj source files, package the compiled source into a libSBOLj JAR file (libSBOLj-<version>-SNAPSHOT-withDependencies.jar), and place the JAR file into the core2/target sub-directory.
",libsbolj provid the core java interfac and their implement for the synthet biolog open languag sbol the librari provid an api to work with sbol object the function to read and write sbol document as xmlrdf file and a valid to check the correct of sbol model use the libsbolj librari in a maven project in a maven project that util the libsbolj librari add a depend in the maven project pomxml file depend groupidorgsbolstandardgroupid artifactidlibsboljartifactid version240vers depend in a nonmaven project download libsboljversionwithdependenciesjar from the latest github releas run in the command line libsbolj come with a commandlin interfac cli that can be use to valid sbol file you can execut libsboljversionwithdependenciesjar to valid and convert file as follow java jar libsboljversionwithdependenciesjar inputfil l languag if validationconvers is success the program will print the content of the input file in the specifi languag sbol1 sbol2 genbank and fasta you can also output the result to a file java jar libsboljversionwithdependenciesjar inputfil l languag o outputfil one can also provid a uri use the s flag for a toplevel object and onli that object and all it depend will be output if valid fail with an error there will be a messag print about the valid error the f flag can be use to indic that valid should continu after the first error while the d flag will provid a detail error trace on a valid error in addit to check all requir valid rule it will also check if the uri are compliant and whether the sbol document is complet ie all referenc object are contain within the file these valid check can be turn off with the n and i flag respect it is also possibl to turnon best practic check use the b flag when the input file is be convert into sbol 20 the convers should be provid a default uri prefix it can also be provid a default version if desir final the t flag will insert the type of top level object into the uri dure convers if desir java jar libsboljversionwithdependenciesjar infil o outfil p uriprefix v version final it can be use to compar the equal of the content of two sbol file use the command below java jar libsboljversionwithdependenciesjar firstsbolfil e secondsbolfil use the latest libsbolj snapshot get the libsbolj sourc creat a github account setup git on your machin clone the libsbolj github repositori to your machin retriev the sboltestsuit submodul use the instruct below retriev sboltestsuit submodul git submodul updat init recurs compil and packag libsbolj setup apach maven a tutori on use apach maven is provid here in the command line chang to the libsbolj directori eg cd pathtolibsbolj and execut the follow command mvn packag thi will compil the libsbolj sourc file packag the compil sourc into a libsbolj jar file libsboljversionsnapshotwithdependenciesjar and place the jar file into the core2target subdirectori,libsbolj provides the core java interface and their implementation for the synthetic biology open language sbol the library provides an api to work with sbol object the functionality to read and write sbol document a xmlrdf file and a validator to check the correctness of sbol model using the libsbolj library in a maven project in a maven project that utilizes the libsbolj library add a dependency in the maven project pomxml file dependency groupidorgsbolstandardgroupid artifactidlibsboljartifactid version240version dependency in a nonmaven project download libsboljversionwithdependenciesjar from the latest github release running in the command line libsbolj come with a commandline interface cli that can be used to validate sbol file you can execute libsboljversionwithdependenciesjar to validate and convert file a follows java jar libsboljversionwithdependenciesjar inputfile l language if validationconversion is successful the program will print the content of the input file in the specified language sbol1 sbol2 genbank and fasta you can also output the result to a file java jar libsboljversionwithdependenciesjar inputfile l language o outputfile one can also provide a uri using the s flag for a toplevel object and only that object and all it dependency will be output if validation fails with an error there will be a message printed about the validation error the f flag can be used to indicate that validation should continue after the first error while the d flag will provide a detailed error trace on a validation error in addition to checking all required validation rule it will also check if the uris are compliant and whether the sbol document is complete ie all referenced object are contained within the file these validation check can be turned off with the n and i flag respectively it is also possible to turnon best practice checking using the b flag when the input file is being converted into sbol 20 the conversion should be provided a default uri prefix it can also be provided a default version if desired finally the t flag will insert the type of top level object into the uri during conversion if desired java jar libsboljversionwithdependenciesjar infile o outfile p uriprefix v version finally it can be used to compare the equality of the content of two sbol file using the command below java jar libsboljversionwithdependenciesjar firstsbolfile e secondsbolfile using the latest libsbolj snapshot getting the libsbolj source create a github account setup git on your machine clone the libsbolj github repository to your machine retrieve the sboltestsuite submodule using the instruction below retrieving sboltestsuite submodule git submodule update init recursive compiling and packaging libsbolj setup apache maven a tutorial on using apache maven is provided here in the command line change to the libsbolj directory eg cd pathtolibsbolj and execute the following command mvn package this will compile the libsbolj source file package the compiled source into a libsbolj jar file libsboljversionsnapshotwithdependenciesjar and place the jar file into the core2target subdirectory,libsbolj provides core java interface implementation synthetic biology open language sbol library provides api work sbol object functionality read write sbol document xmlrdf file validator check correctness sbol model using libsbolj library maven project maven project utilizes libsbolj library add dependency maven project pomxml file dependency &#9; groupidorgsbolstandardgroupid &#9; artifactidlibsboljartifactid &#9; version240version dependency nonmaven project download libsboljversionwithdependenciesjar latest github release running command line libsbolj come commandline interface cli used validate sbol file execute libsboljversionwithdependenciesjar validate convert file follows java jar libsboljversionwithdependenciesjar inputfile l language validationconversion successful program print content input file specified language sbol1 sbol2 genbank fasta also output result file java jar libsboljversionwithdependenciesjar inputfile l language outputfile one also provide uri using flag toplevel object object dependency output validation fails error message printed validation error f flag used indicate validation continue first error flag provide detailed error trace validation error addition checking required validation rule also check uris compliant whether sbol document complete ie referenced object contained within file validation check turned n flag respectively also possible turnon best practice checking using b flag input file converted sbol 20 conversion provided default uri prefix also provided default version desired finally flag insert type top level object uri conversion desired java jar libsboljversionwithdependenciesjar infile outfile p uriprefix v version finally used compare equality content two sbol file using command java jar libsboljversionwithdependenciesjar firstsbolfile e secondsbolfile using latest libsbolj snapshot getting libsbolj source create github account setup git machine clone libsbolj github repository machine retrieve sboltestsuite submodule using instruction retrieving sboltestsuite submodule git submodule update init recursive compiling packaging libsbolj setup apache maven tutorial using apache maven provided command line change libsbolj directory eg cd pathtolibsbolj execute following command mvn package compile libsbolj source file package compiled source libsbolj jar file libsboljversionsnapshotwithdependenciesjar place jar file core2target subdirectory
Java ,"JVARKIT
Java utilities for Bioinformatics


Documentation
Documentation is available at: http://lindenb.github.io/jvarkit/
Compilation
February 2019. I'm moving to java OpenJdk. See the [[NEWS]] file.
Each tool is compiled independently of each other.
See the documentation for each tool at http://lindenb.github.io/jvarkit/. All the pages should include a paragraph titled 'Download and Compile'
You shouldn't try to compile all the tools because some of them are not tested, deprecated, or just too specific to my lab.
Author
Pierre Lindenbaum PhD
http://plindenbaum.blogspot.com
@yokofakun
",jvarkit java util for bioinformat document document is avail at httplindenbgithubiojvarkit compil februari 2019 im move to java openjdk see the news file each tool is compil independ of each other see the document for each tool at httplindenbgithubiojvarkit all the page should includ a paragraph titl download and compil you shouldnt tri to compil all the tool becaus some of them are not test deprec or just too specif to my lab author pierr lindenbaum phd httpplindenbaumblogspotcom yokofakun,jvarkit java utility for bioinformatics documentation documentation is available at httplindenbgithubiojvarkit compilation february 2019 im moving to java openjdk see the news file each tool is compiled independently of each other see the documentation for each tool at httplindenbgithubiojvarkit all the page should include a paragraph titled download and compile you shouldnt try to compile all the tool because some of them are not tested deprecated or just too specific to my lab author pierre lindenbaum phd httpplindenbaumblogspotcom yokofakun,jvarkit java utility bioinformatics documentation documentation available httplindenbgithubiojvarkit compilation february 2019 im moving java openjdk see news file tool compiled independently see documentation tool httplindenbgithubiojvarkit page include paragraph titled download compile shouldnt try compile tool tested deprecated specific lab author pierre lindenbaum phd httpplindenbaumblogspotcom yokofakun
Java ,"This repo contains code samples, data and problem solutions for the
book ""Computational Biology with Java"".
See petergarst.com for more information.
",thi repo contain code sampl data and problem solut for the book comput biolog with java see petergarstcom for more inform,this repo contains code sample data and problem solution for the book computational biology with java see petergarstcom for more information,repo contains code sample data problem solution book computational biology java see petergarstcom information
Java ,"
BioLær er en native Android quiz-applikation med fokus på molekylærbiologi. I denne første udgave af app'en er det laboratoriumanalysen ELISA, der kan quizzes i. Dog er systemet konstrueret således, at det let senere kan udvides til andre fagområder. Applikationen er udviklet i forbindelse med førsteårsprojektet på datamatikeruddannelsen. I projektets Wiki-sektion kan du læse meget mere om tilblivelsen, den tilhørende rapport, rettigheder mv.
English description

This project includes a native Android Application about laboratory analysis ELISA.
The idea was to make a biology application to help students understand the term ELISA.
Developed in Java and XML with Android Studio in the context of a school project.

Authors

Sebastian Ougter Olsen
Mathias Elholm Blomgaard
Thomas Christensen
Daniel Lyck
Michael Trans


",biolr er en nativ android quizapplik med foku p molekylrbiolog i denn frste udgav af appen er det laboratoriumanalysen elisa der kan quizz i dog er systemet konstrueret slede at det let sener kan udvid til andr fagomrd applikationen er udviklet i forbindels med frstersprojektet p datamatikeruddannelsen i projektet wikisekt kan du lse meget mere om tilblivelsen den tilhrend rapport rettighed mv english descript thi project includ a nativ android applic about laboratori analysi elisa the idea wa to make a biolog applic to help student understand the term elisa develop in java and xml with android studio in the context of a school project author sebastian ougter olsen mathia elholm blomgaard thoma christensen daniel lyck michael tran,biolr er en native android quizapplikation med fokus p molekylrbiologi i denne frste udgave af appen er det laboratoriumanalysen elisa der kan quiz i dog er systemet konstrueret sledes at det let senere kan udvides til andre fagomrder applikationen er udviklet i forbindelse med frstersprojektet p datamatikeruddannelsen i projektets wikisektion kan du lse meget mere om tilblivelsen den tilhrende rapport rettigheder mv english description this project includes a native android application about laboratory analysis elisa the idea wa to make a biology application to help student understand the term elisa developed in java and xml with android studio in the context of a school project author sebastian ougter olsen mathias elholm blomgaard thomas christensen daniel lyck michael trans,biolr er en native android quizapplikation med fokus p molekylrbiologi denne frste udgave af appen er det laboratoriumanalysen elisa der kan quiz dog er systemet konstrueret sledes det let senere kan udvides til andre fagomrder applikationen er udviklet forbindelse med frstersprojektet p datamatikeruddannelsen projektets wikisektion kan du lse meget mere om tilblivelsen den tilhrende rapport rettigheder mv english description project includes native android application laboratory analysis elisa idea make biology application help student understand term elisa developed java xml android studio context school project author sebastian ougter olsen mathias elholm blomgaard thomas christensen daniel lyck michael trans
Java ,"BiologyAnalyze
蛋白质网络、基因测序相关数据分析
",biologyanalyz,biologyanalyze,biologyanalyze
Java ,"



iBioSim is a computer-aided design (CAD) tool aimed for the modeling, analysis, and design of genetic circuits.
While iBioSim primarily targets models of genetic circuits, models representing metabolic networks, cell-signaling pathways,
and other biological and chemical systems can also be analyzed.
iBioSim also includes modeling and visualization support for multi-cellular and spatial models as well.
It is capable of importing and exporting models specified using the Systems Biology Markup Language (SBML).
It can import all levels and versions of SBML and is able to export Level 3 Version 1.
It supports all core SBML modeling constructs except some types of fast reactions, and also has support for the
hierarchical model composition, layout, flux balance constraints, and arrays packages.
It has also been tested successfully on the stochastic benchmark suite and the curated models in the BioModels database.
iBioSim also supports the Synthetic Biology Open Language (SBOL), an emerging standard for information exchange in synthetic
biology.
Website: iBioSim
Video Demo: Tools Workflow
Contact: Chris Myers (@cjmyers) myers@ece.utah.edu
Contributor(s): Nathan Barker, Pedro Fontanarrosa, Scott Glass, Kevin Jones, Hiroyuki Kuwahara, Curtis Madsen, Nam Nguyen, Tramy Nguyen, Tyler Patterson, Nicholas Roehner, Jason Stevens, Leandro Watanabe, Michael Zhang, Zhen Zhang, and Zach Zundel.
Active Developer(s): Pedro Fontanarrosa, Chris Myers, Tramy Nguyen, Leandro Watanabe.
Running iBioSim

Download the iBioSim tool from the release page here:
After downloading the tool, run the corresponding start-up script:

Windows: iBioSim.bat
Mac OS X: iBioSim.mac64
Linux: iBioSim.linux64



[Optional] Installing iBioSim for Development
Pre-installation Requirements

Create a GitHub account.
Setup Git on your machine.
Install Maven plugin on your machine.
Install Eclipse IDE  for Java.
Install libSBML for validation and flattening.
Clone the iBioSim GitHub repository to your machine

Importing iBioSim to Eclipse

Clone the iBioSim (https://github.com/MyersResearchGroup/iBioSim.git) project (e.g. git clone https://github.com/MyersResearchGroup/iBioSim.git) to a location of your preference.
Open up your Eclipse workspace that you want to import your iBioSim project to.
Select Import from the File Menu.
When given the option to select which project import, select Existing Maven Projects under Maven

Set Maven Projects:

Root Directory: full path to your iBioSim project (i.e. path/to/iBioSim)
Once root directory is set, all the pom.xml should be displayed under Projects. Select all pom.xml files.
All installation should be complete so click Finish





Setting up iBioSim Configurations in Eclipse

Open up iBioSim Run Configurations window and create a new Java Application in your Eclipse workspace


Give the java application a name (i.e. iBioSim_GUI)
Set the Main tab to the following information:

Project: iBioSim-gui
Main class: edu.utah.ece.async.ibiosim.gui.Gui


Set the Environment tab to the following information:

Create variables with the corresponding value:

BIOSIM: full path to your iBioSim project (i.e. path/to/iBioSim)
PATH: append your copy of iBioSim bin directory to whatever existing PATH already supplied to the value of this variable (i.e. $PATH:path/to/iBioSim/bin).




Set Arguments tab to the following information:

Program arguments: -Xms2048 -Xms2048 -XX:+UseSerialGC -Djava.library.path=/path/to/lib/
Note: for the java library path, /path/to/lib/ is the location where libSBML is installed. The libSBML is installed by default in /usr/local/lib in Linux and Mac OS X machines and libSBML-5.17.0-win64 in Windows 64-bit machines.


If you are running on Mac OS X, also set the following:

VM arguments: -Dapple.laf.useScreenMenuBar=true -Xdock:name=""iBioSim"" -Xdock:icon=$BIOSIM/src/resources/icons/iBioSim.jpg


All run configurations are complete. Make sure to apply all your changes.

Building iBioSim

Go to the directory where the iBioSim is checked out and perform mvn clean install (NOTE: if you do not want to generate javadocs, use the flag -Dmaven.javadoc.skip=true).

[Optional] Building reb2sac and GeneNet dependencies

iBioSim incorporates tools that are not Java-based, and therefore, have to be installed separately.
The easiest way to install reb2sac and GeneNet is to simply download the pre-compiled binaries for your operating system below:

reb2sac
GeneNet


Another way to install them is to compile these tools on your machine following the instructions below:

reb2sac
GeneNet


After compiling or downloading reb2sac and GeneNet, copy the compiled binaries into the bin directory in the local copy of your iBioSim.

",ibiosim is a computeraid design cad tool aim for the model analysi and design of genet circuit while ibiosim primarili target model of genet circuit model repres metabol network cellsign pathway and other biolog and chemic system can also be analyz ibiosim also includ model and visual support for multicellular and spatial model as well it is capabl of import and export model specifi use the system biolog markup languag sbml it can import all level and version of sbml and is abl to export level 3 version 1 it support all core sbml model construct except some type of fast reaction and also ha support for the hierarch model composit layout flux balanc constraint and array packag it ha also been test success on the stochast benchmark suit and the curat model in the biomodel databas ibiosim also support the synthet biolog open languag sbol an emerg standard for inform exchang in synthet biolog websit ibiosim video demo tool workflow contact chri myer cjmyer myerseceutahedu contributor nathan barker pedro fontanarrosa scott glass kevin jone hiroyuki kuwahara curti madsen nam nguyen trami nguyen tyler patterson nichola roehner jason steven leandro watanab michael zhang zhen zhang and zach zundel activ develop pedro fontanarrosa chri myer trami nguyen leandro watanab run ibiosim download the ibiosim tool from the releas page here after download the tool run the correspond startup script window ibiosimbat mac os x ibiosimmac64 linux ibiosimlinux64 option instal ibiosim for develop preinstal requir creat a github account setup git on your machin instal maven plugin on your machin instal eclips ide for java instal libsbml for valid and flatten clone the ibiosim github repositori to your machin import ibiosim to eclips clone the ibiosim httpsgithubcommyersresearchgroupibiosimgit project eg git clone httpsgithubcommyersresearchgroupibiosimgit to a locat of your prefer open up your eclips workspac that you want to import your ibiosim project to select import from the file menu when given the option to select which project import select exist maven project under maven set maven project root directori full path to your ibiosim project ie pathtoibiosim onc root directori is set all the pomxml should be display under project select all pomxml file all instal should be complet so click finish set up ibiosim configur in eclips open up ibiosim run configur window and creat a new java applic in your eclips workspac give the java applic a name ie ibiosim_gui set the main tab to the follow inform project ibiosimgui main class eduutaheceasyncibiosimguigui set the environ tab to the follow inform creat variabl with the correspond valu biosim full path to your ibiosim project ie pathtoibiosim path append your copi of ibiosim bin directori to whatev exist path alreadi suppli to the valu of thi variabl ie pathpathtoibiosimbin set argument tab to the follow inform program argument xms2048 xms2048 xxuseserialgc djavalibrarypathpathtolib note for the java librari path pathtolib is the locat where libsbml is instal the libsbml is instal by default in usrlocallib in linux and mac os x machin and libsbml5170win64 in window 64bit machin if you are run on mac os x also set the follow vm argument dapplelafusescreenmenubartru xdocknameibiosim xdockiconbiosimsrcresourcesiconsibiosimjpg all run configur are complet make sure to appli all your chang build ibiosim go to the directori where the ibiosim is check out and perform mvn clean instal note if you do not want to gener javadoc use the flag dmavenjavadocskiptru option build reb2sac and genenet depend ibiosim incorpor tool that are not javabas and therefor have to be instal separ the easiest way to instal reb2sac and genenet is to simpli download the precompil binari for your oper system below reb2sac genenet anoth way to instal them is to compil these tool on your machin follow the instruct below reb2sac genenet after compil or download reb2sac and genenet copi the compil binari into the bin directori in the local copi of your ibiosim,ibiosim is a computeraided design cad tool aimed for the modeling analysis and design of genetic circuit while ibiosim primarily target model of genetic circuit model representing metabolic network cellsignaling pathway and other biological and chemical system can also be analyzed ibiosim also includes modeling and visualization support for multicellular and spatial model a well it is capable of importing and exporting model specified using the system biology markup language sbml it can import all level and version of sbml and is able to export level 3 version 1 it support all core sbml modeling construct except some type of fast reaction and also ha support for the hierarchical model composition layout flux balance constraint and array package it ha also been tested successfully on the stochastic benchmark suite and the curated model in the biomodels database ibiosim also support the synthetic biology open language sbol an emerging standard for information exchange in synthetic biology website ibiosim video demo tool workflow contact chris myers cjmyers myerseceutahedu contributor nathan barker pedro fontanarrosa scott glass kevin jones hiroyuki kuwahara curtis madsen nam nguyen tramy nguyen tyler patterson nicholas roehner jason stevens leandro watanabe michael zhang zhen zhang and zach zundel active developer pedro fontanarrosa chris myers tramy nguyen leandro watanabe running ibiosim download the ibiosim tool from the release page here after downloading the tool run the corresponding startup script window ibiosimbat mac o x ibiosimmac64 linux ibiosimlinux64 optional installing ibiosim for development preinstallation requirement create a github account setup git on your machine install maven plugin on your machine install eclipse ide for java install libsbml for validation and flattening clone the ibiosim github repository to your machine importing ibiosim to eclipse clone the ibiosim httpsgithubcommyersresearchgroupibiosimgit project eg git clone httpsgithubcommyersresearchgroupibiosimgit to a location of your preference open up your eclipse workspace that you want to import your ibiosim project to select import from the file menu when given the option to select which project import select existing maven project under maven set maven project root directory full path to your ibiosim project ie pathtoibiosim once root directory is set all the pomxml should be displayed under project select all pomxml file all installation should be complete so click finish setting up ibiosim configuration in eclipse open up ibiosim run configuration window and create a new java application in your eclipse workspace give the java application a name ie ibiosim_gui set the main tab to the following information project ibiosimgui main class eduutaheceasyncibiosimguigui set the environment tab to the following information create variable with the corresponding value biosim full path to your ibiosim project ie pathtoibiosim path append your copy of ibiosim bin directory to whatever existing path already supplied to the value of this variable ie pathpathtoibiosimbin set argument tab to the following information program argument xms2048 xms2048 xxuseserialgc djavalibrarypathpathtolib note for the java library path pathtolib is the location where libsbml is installed the libsbml is installed by default in usrlocallib in linux and mac o x machine and libsbml5170win64 in window 64bit machine if you are running on mac o x also set the following vm argument dapplelafusescreenmenubartrue xdocknameibiosim xdockiconbiosimsrcresourcesiconsibiosimjpg all run configuration are complete make sure to apply all your change building ibiosim go to the directory where the ibiosim is checked out and perform mvn clean install note if you do not want to generate javadocs use the flag dmavenjavadocskiptrue optional building reb2sac and genenet dependency ibiosim incorporates tool that are not javabased and therefore have to be installed separately the easiest way to install reb2sac and genenet is to simply download the precompiled binary for your operating system below reb2sac genenet another way to install them is to compile these tool on your machine following the instruction below reb2sac genenet after compiling or downloading reb2sac and genenet copy the compiled binary into the bin directory in the local copy of your ibiosim,ibiosim computeraided design cad tool aimed modeling analysis design genetic circuit ibiosim primarily target model genetic circuit model representing metabolic network cellsignaling pathway biological chemical system also analyzed ibiosim also includes modeling visualization support multicellular spatial model well capable importing exporting model specified using system biology markup language sbml import level version sbml able export level 3 version 1 support core sbml modeling construct except type fast reaction also support hierarchical model composition layout flux balance constraint array package also tested successfully stochastic benchmark suite curated model biomodels database ibiosim also support synthetic biology open language sbol emerging standard information exchange synthetic biology website ibiosim video demo tool workflow contact chris myers cjmyers myerseceutahedu contributor nathan barker pedro fontanarrosa scott glass kevin jones hiroyuki kuwahara curtis madsen nam nguyen tramy nguyen tyler patterson nicholas roehner jason stevens leandro watanabe michael zhang zhen zhang zach zundel active developer pedro fontanarrosa chris myers tramy nguyen leandro watanabe running ibiosim download ibiosim tool release page downloading tool run corresponding startup script window ibiosimbat mac o x ibiosimmac64 linux ibiosimlinux64 optional installing ibiosim development preinstallation requirement create github account setup git machine install maven plugin machine install eclipse ide java install libsbml validation flattening clone ibiosim github repository machine importing ibiosim eclipse clone ibiosim httpsgithubcommyersresearchgroupibiosimgit project eg git clone httpsgithubcommyersresearchgroupibiosimgit location preference open eclipse workspace want import ibiosim project select import file menu given option select project import select existing maven project maven set maven project root directory full path ibiosim project ie pathtoibiosim root directory set pomxml displayed project select pomxml file installation complete click finish setting ibiosim configuration eclipse open ibiosim run configuration window create new java application eclipse workspace give java application name ie ibiosim_gui set main tab following information project ibiosimgui main class eduutaheceasyncibiosimguigui set environment tab following information create variable corresponding value biosim full path ibiosim project ie pathtoibiosim path append copy ibiosim bin directory whatever existing path already supplied value variable ie pathpathtoibiosimbin set argument tab following information program argument xms2048 xms2048 xxuseserialgc djavalibrarypathpathtolib note java library path pathtolib location libsbml installed libsbml installed default usrlocallib linux mac o x machine libsbml5170win64 window 64bit machine running mac o x also set following vm argument dapplelafusescreenmenubartrue xdocknameibiosim xdockiconbiosimsrcresourcesiconsibiosimjpg run configuration complete make sure apply change building ibiosim go directory ibiosim checked perform mvn clean install note want generate javadocs use flag dmavenjavadocskiptrue optional building reb2sac genenet dependency ibiosim incorporates tool javabased therefore installed separately easiest way install reb2sac genenet simply download precompiled binary operating system reb2sac genenet another way install compile tool machine following instruction reb2sac genenet compiling downloading reb2sac genenet copy compiled binary bin directory local copy ibiosim
C++ ,"



iBioSim is a computer-aided design (CAD) tool aimed for the modeling, analysis, and design of genetic circuits.
While iBioSim primarily targets models of genetic circuits, models representing metabolic networks, cell-signaling pathways,
and other biological and chemical systems can also be analyzed.
iBioSim also includes modeling and visualization support for multi-cellular and spatial models as well.
It is capable of importing and exporting models specified using the Systems Biology Markup Language (SBML).
It can import all levels and versions of SBML and is able to export Level 3 Version 1.
It supports all core SBML modeling constructs except some types of fast reactions, and also has support for the
hierarchical model composition, layout, flux balance constraints, and arrays packages.
It has also been tested successfully on the stochastic benchmark suite and the curated models in the BioModels database.
iBioSim also supports the Synthetic Biology Open Language (SBOL), an emerging standard for information exchange in synthetic
biology.
Website: iBioSim
Video Demo: Tools Workflow
Contact: Chris Myers (@cjmyers) myers@ece.utah.edu
Contributor(s): Nathan Barker, Pedro Fontanarrosa, Scott Glass, Kevin Jones, Hiroyuki Kuwahara, Curtis Madsen, Nam Nguyen, Tramy Nguyen, Tyler Patterson, Nicholas Roehner, Jason Stevens, Leandro Watanabe, Michael Zhang, Zhen Zhang, and Zach Zundel.
Active Developer(s): Pedro Fontanarrosa, Chris Myers, Tramy Nguyen, Leandro Watanabe.
Running iBioSim

Download the iBioSim tool from the release page here:
After downloading the tool, run the corresponding start-up script:

Windows: iBioSim.bat
Mac OS X: iBioSim.mac64
Linux: iBioSim.linux64



[Optional] Installing iBioSim for Development
Pre-installation Requirements

Create a GitHub account.
Setup Git on your machine.
Install Maven plugin on your machine.
Install Eclipse IDE  for Java.
Install libSBML for validation and flattening.
Clone the iBioSim GitHub repository to your machine

Importing iBioSim to Eclipse

Clone the iBioSim (https://github.com/MyersResearchGroup/iBioSim.git) project (e.g. git clone https://github.com/MyersResearchGroup/iBioSim.git) to a location of your preference.
Open up your Eclipse workspace that you want to import your iBioSim project to.
Select Import from the File Menu.
When given the option to select which project import, select Existing Maven Projects under Maven

Set Maven Projects:

Root Directory: full path to your iBioSim project (i.e. path/to/iBioSim)
Once root directory is set, all the pom.xml should be displayed under Projects. Select all pom.xml files.
All installation should be complete so click Finish





Setting up iBioSim Configurations in Eclipse

Open up iBioSim Run Configurations window and create a new Java Application in your Eclipse workspace


Give the java application a name (i.e. iBioSim_GUI)
Set the Main tab to the following information:

Project: iBioSim-gui
Main class: edu.utah.ece.async.ibiosim.gui.Gui


Set the Environment tab to the following information:

Create variables with the corresponding value:

BIOSIM: full path to your iBioSim project (i.e. path/to/iBioSim)
PATH: append your copy of iBioSim bin directory to whatever existing PATH already supplied to the value of this variable (i.e. $PATH:path/to/iBioSim/bin).




Set Arguments tab to the following information:

Program arguments: -Xms2048 -Xms2048 -XX:+UseSerialGC -Djava.library.path=/path/to/lib/
Note: for the java library path, /path/to/lib/ is the location where libSBML is installed. The libSBML is installed by default in /usr/local/lib in Linux and Mac OS X machines and libSBML-5.17.0-win64 in Windows 64-bit machines.


If you are running on Mac OS X, also set the following:

VM arguments: -Dapple.laf.useScreenMenuBar=true -Xdock:name=""iBioSim"" -Xdock:icon=$BIOSIM/src/resources/icons/iBioSim.jpg


All run configurations are complete. Make sure to apply all your changes.

Building iBioSim

Go to the directory where the iBioSim is checked out and perform mvn clean install (NOTE: if you do not want to generate javadocs, use the flag -Dmaven.javadoc.skip=true).

[Optional] Building reb2sac and GeneNet dependencies

iBioSim incorporates tools that are not Java-based, and therefore, have to be installed separately.
The easiest way to install reb2sac and GeneNet is to simply download the pre-compiled binaries for your operating system below:

reb2sac
GeneNet


Another way to install them is to compile these tools on your machine following the instructions below:

reb2sac
GeneNet


After compiling or downloading reb2sac and GeneNet, copy the compiled binaries into the bin directory in the local copy of your iBioSim.

",ibiosim is a computeraid design cad tool aim for the model analysi and design of genet circuit while ibiosim primarili target model of genet circuit model repres metabol network cellsign pathway and other biolog and chemic system can also be analyz ibiosim also includ model and visual support for multicellular and spatial model as well it is capabl of import and export model specifi use the system biolog markup languag sbml it can import all level and version of sbml and is abl to export level 3 version 1 it support all core sbml model construct except some type of fast reaction and also ha support for the hierarch model composit layout flux balanc constraint and array packag it ha also been test success on the stochast benchmark suit and the curat model in the biomodel databas ibiosim also support the synthet biolog open languag sbol an emerg standard for inform exchang in synthet biolog websit ibiosim video demo tool workflow contact chri myer cjmyer myerseceutahedu contributor nathan barker pedro fontanarrosa scott glass kevin jone hiroyuki kuwahara curti madsen nam nguyen trami nguyen tyler patterson nichola roehner jason steven leandro watanab michael zhang zhen zhang and zach zundel activ develop pedro fontanarrosa chri myer trami nguyen leandro watanab run ibiosim download the ibiosim tool from the releas page here after download the tool run the correspond startup script window ibiosimbat mac os x ibiosimmac64 linux ibiosimlinux64 option instal ibiosim for develop preinstal requir creat a github account setup git on your machin instal maven plugin on your machin instal eclips ide for java instal libsbml for valid and flatten clone the ibiosim github repositori to your machin import ibiosim to eclips clone the ibiosim httpsgithubcommyersresearchgroupibiosimgit project eg git clone httpsgithubcommyersresearchgroupibiosimgit to a locat of your prefer open up your eclips workspac that you want to import your ibiosim project to select import from the file menu when given the option to select which project import select exist maven project under maven set maven project root directori full path to your ibiosim project ie pathtoibiosim onc root directori is set all the pomxml should be display under project select all pomxml file all instal should be complet so click finish set up ibiosim configur in eclips open up ibiosim run configur window and creat a new java applic in your eclips workspac give the java applic a name ie ibiosim_gui set the main tab to the follow inform project ibiosimgui main class eduutaheceasyncibiosimguigui set the environ tab to the follow inform creat variabl with the correspond valu biosim full path to your ibiosim project ie pathtoibiosim path append your copi of ibiosim bin directori to whatev exist path alreadi suppli to the valu of thi variabl ie pathpathtoibiosimbin set argument tab to the follow inform program argument xms2048 xms2048 xxuseserialgc djavalibrarypathpathtolib note for the java librari path pathtolib is the locat where libsbml is instal the libsbml is instal by default in usrlocallib in linux and mac os x machin and libsbml5170win64 in window 64bit machin if you are run on mac os x also set the follow vm argument dapplelafusescreenmenubartru xdocknameibiosim xdockiconbiosimsrcresourcesiconsibiosimjpg all run configur are complet make sure to appli all your chang build ibiosim go to the directori where the ibiosim is check out and perform mvn clean instal note if you do not want to gener javadoc use the flag dmavenjavadocskiptru option build reb2sac and genenet depend ibiosim incorpor tool that are not javabas and therefor have to be instal separ the easiest way to instal reb2sac and genenet is to simpli download the precompil binari for your oper system below reb2sac genenet anoth way to instal them is to compil these tool on your machin follow the instruct below reb2sac genenet after compil or download reb2sac and genenet copi the compil binari into the bin directori in the local copi of your ibiosim,ibiosim is a computeraided design cad tool aimed for the modeling analysis and design of genetic circuit while ibiosim primarily target model of genetic circuit model representing metabolic network cellsignaling pathway and other biological and chemical system can also be analyzed ibiosim also includes modeling and visualization support for multicellular and spatial model a well it is capable of importing and exporting model specified using the system biology markup language sbml it can import all level and version of sbml and is able to export level 3 version 1 it support all core sbml modeling construct except some type of fast reaction and also ha support for the hierarchical model composition layout flux balance constraint and array package it ha also been tested successfully on the stochastic benchmark suite and the curated model in the biomodels database ibiosim also support the synthetic biology open language sbol an emerging standard for information exchange in synthetic biology website ibiosim video demo tool workflow contact chris myers cjmyers myerseceutahedu contributor nathan barker pedro fontanarrosa scott glass kevin jones hiroyuki kuwahara curtis madsen nam nguyen tramy nguyen tyler patterson nicholas roehner jason stevens leandro watanabe michael zhang zhen zhang and zach zundel active developer pedro fontanarrosa chris myers tramy nguyen leandro watanabe running ibiosim download the ibiosim tool from the release page here after downloading the tool run the corresponding startup script window ibiosimbat mac o x ibiosimmac64 linux ibiosimlinux64 optional installing ibiosim for development preinstallation requirement create a github account setup git on your machine install maven plugin on your machine install eclipse ide for java install libsbml for validation and flattening clone the ibiosim github repository to your machine importing ibiosim to eclipse clone the ibiosim httpsgithubcommyersresearchgroupibiosimgit project eg git clone httpsgithubcommyersresearchgroupibiosimgit to a location of your preference open up your eclipse workspace that you want to import your ibiosim project to select import from the file menu when given the option to select which project import select existing maven project under maven set maven project root directory full path to your ibiosim project ie pathtoibiosim once root directory is set all the pomxml should be displayed under project select all pomxml file all installation should be complete so click finish setting up ibiosim configuration in eclipse open up ibiosim run configuration window and create a new java application in your eclipse workspace give the java application a name ie ibiosim_gui set the main tab to the following information project ibiosimgui main class eduutaheceasyncibiosimguigui set the environment tab to the following information create variable with the corresponding value biosim full path to your ibiosim project ie pathtoibiosim path append your copy of ibiosim bin directory to whatever existing path already supplied to the value of this variable ie pathpathtoibiosimbin set argument tab to the following information program argument xms2048 xms2048 xxuseserialgc djavalibrarypathpathtolib note for the java library path pathtolib is the location where libsbml is installed the libsbml is installed by default in usrlocallib in linux and mac o x machine and libsbml5170win64 in window 64bit machine if you are running on mac o x also set the following vm argument dapplelafusescreenmenubartrue xdocknameibiosim xdockiconbiosimsrcresourcesiconsibiosimjpg all run configuration are complete make sure to apply all your change building ibiosim go to the directory where the ibiosim is checked out and perform mvn clean install note if you do not want to generate javadocs use the flag dmavenjavadocskiptrue optional building reb2sac and genenet dependency ibiosim incorporates tool that are not javabased and therefore have to be installed separately the easiest way to install reb2sac and genenet is to simply download the precompiled binary for your operating system below reb2sac genenet another way to install them is to compile these tool on your machine following the instruction below reb2sac genenet after compiling or downloading reb2sac and genenet copy the compiled binary into the bin directory in the local copy of your ibiosim,ibiosim computeraided design cad tool aimed modeling analysis design genetic circuit ibiosim primarily target model genetic circuit model representing metabolic network cellsignaling pathway biological chemical system also analyzed ibiosim also includes modeling visualization support multicellular spatial model well capable importing exporting model specified using system biology markup language sbml import level version sbml able export level 3 version 1 support core sbml modeling construct except type fast reaction also support hierarchical model composition layout flux balance constraint array package also tested successfully stochastic benchmark suite curated model biomodels database ibiosim also support synthetic biology open language sbol emerging standard information exchange synthetic biology website ibiosim video demo tool workflow contact chris myers cjmyers myerseceutahedu contributor nathan barker pedro fontanarrosa scott glass kevin jones hiroyuki kuwahara curtis madsen nam nguyen tramy nguyen tyler patterson nicholas roehner jason stevens leandro watanabe michael zhang zhen zhang zach zundel active developer pedro fontanarrosa chris myers tramy nguyen leandro watanabe running ibiosim download ibiosim tool release page downloading tool run corresponding startup script window ibiosimbat mac o x ibiosimmac64 linux ibiosimlinux64 optional installing ibiosim development preinstallation requirement create github account setup git machine install maven plugin machine install eclipse ide java install libsbml validation flattening clone ibiosim github repository machine importing ibiosim eclipse clone ibiosim httpsgithubcommyersresearchgroupibiosimgit project eg git clone httpsgithubcommyersresearchgroupibiosimgit location preference open eclipse workspace want import ibiosim project select import file menu given option select project import select existing maven project maven set maven project root directory full path ibiosim project ie pathtoibiosim root directory set pomxml displayed project select pomxml file installation complete click finish setting ibiosim configuration eclipse open ibiosim run configuration window create new java application eclipse workspace give java application name ie ibiosim_gui set main tab following information project ibiosimgui main class eduutaheceasyncibiosimguigui set environment tab following information create variable corresponding value biosim full path ibiosim project ie pathtoibiosim path append copy ibiosim bin directory whatever existing path already supplied value variable ie pathpathtoibiosimbin set argument tab following information program argument xms2048 xms2048 xxuseserialgc djavalibrarypathpathtolib note java library path pathtolib location libsbml installed libsbml installed default usrlocallib linux mac o x machine libsbml5170win64 window 64bit machine running mac o x also set following vm argument dapplelafusescreenmenubartrue xdocknameibiosim xdockiconbiosimsrcresourcesiconsibiosimjpg run configuration complete make sure apply change building ibiosim go directory ibiosim checked perform mvn clean install note want generate javadocs use flag dmavenjavadocskiptrue optional building reb2sac genenet dependency ibiosim incorporates tool javabased therefore installed separately easiest way install reb2sac genenet simply download precompiled binary operating system reb2sac genenet another way install compile tool machine following instruction reb2sac genenet compiling downloading reb2sac genenet copy compiled binary bin directory local copy ibiosim
C++ ,"Introducing Scorum
Scorum platform has three core functions:

Blogging platform where authors and readers will be rewarded for creating and engaging with content
Statistical centers where fans can browse and authors can use Microsoft’s Power BI tool to integrate data-rich visuals into their content
Commission-free betting exchange where fans can place bets against each other using Scorum Coins (SCR)
Scorum’s blockchain protocol is built on the Graphene Framework and utilizes a delegated proof of stake consensus.

Public Announcement & Discussion
The Scorum team has been hard at work developing the blogging platform and the statistics center.
Find out more as we take the project public through the following channels:

Get the latest updates and chat with us on Telegram, Facebook, and Twitter
Read more about our vision on Steemit
Join our affiliate program to get Scorum Coins for free or apply for whitelist to get coins with a discount

No Support & No Warranty
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
IN THE SOFTWARE.
Blockchain consensus rules
Rather than attempt to describe the rules of the blockchain, it is up to
each individual to inspect the code to understand the consensus rules.
Quickstart
Just want to get up and running quickly?  Try deploying a prebuilt
dockerized container.
Dockerized Node
See this guide for detailed instructions including commands for Ubuntu  16.04 (LTS).
Seed Nodes
A list of some seed nodes to get you started can be found in
seed-nodes. This list is embedded into default config.ini.
Building
See doc/building.md for detailed build instructions, including
compile-time options, and specific commands for Ubuntu 16.04 (LTS).
System Requirements
For a full node, you need 10GB of space available. Scorumd uses a memory mapped file which currently holds 2GB of data and by default is set to use up to 10GB. It's highly recommended to run scorumd on a fast disk such as an SSD or by placing the shared memory files in a ramdisk and using the shared-file-dir config (or command line) option to specify where. Any CPU with decent single core performance should be sufficient.
Main net chain_id
genesis.json hash sum: db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1
Test net chain_id
genesis.testnet.json hash sum: d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2
",introduc scorum scorum platform ha three core function blog platform where author and reader will be reward for creat and engag with content statist center where fan can brows and author can use microsoft power bi tool to integr datarich visual into their content commissionfre bet exchang where fan can place bet against each other use scorum coin scr scorum blockchain protocol is built on the graphen framework and util a deleg proof of stake consensu public announc discuss the scorum team ha been hard at work develop the blog platform and the statist center find out more as we take the project public through the follow channel get the latest updat and chat with us on telegram facebook and twitter read more about our vision on steemit join our affili program to get scorum coin for free or appli for whitelist to get coin with a discount no support no warranti the softwar is provid as is without warranti of ani kind express or impli includ but not limit to the warranti of merchant fit for a particular purpos and noninfring in no event shall the author or copyright holder be liabl for ani claim damag or other liabil whether in an action of contract tort or otherwis aris from out of or in connect with the softwar or the use or other deal in the softwar blockchain consensu rule rather than attempt to describ the rule of the blockchain it is up to each individu to inspect the code to understand the consensu rule quickstart just want to get up and run quickli tri deploy a prebuilt docker contain docker node see thi guid for detail instruct includ command for ubuntu 1604 lt seed node a list of some seed node to get you start can be found in seednod thi list is embed into default configini build see docbuildingmd for detail build instruct includ compiletim option and specif command for ubuntu 1604 lt system requir for a full node you need 10gb of space avail scorumd use a memori map file which current hold 2gb of data and by default is set to use up to 10gb it highli recommend to run scorumd on a fast disk such as an ssd or by place the share memori file in a ramdisk and use the sharedfiledir config or command line option to specifi where ani cpu with decent singl core perform should be suffici main net chain_id genesisjson hash sum db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1 test net chain_id genesistestnetjson hash sum d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2,introducing scorum scorum platform ha three core function blogging platform where author and reader will be rewarded for creating and engaging with content statistical center where fan can browse and author can use microsofts power bi tool to integrate datarich visuals into their content commissionfree betting exchange where fan can place bet against each other using scorum coin scr scorums blockchain protocol is built on the graphene framework and utilizes a delegated proof of stake consensus public announcement discussion the scorum team ha been hard at work developing the blogging platform and the statistic center find out more a we take the project public through the following channel get the latest update and chat with u on telegram facebook and twitter read more about our vision on steemit join our affiliate program to get scorum coin for free or apply for whitelist to get coin with a discount no support no warranty the software is provided a is without warranty of any kind express or implied including but not limited to the warranty of merchantability fitness for a particular purpose and noninfringement in no event shall the author or copyright holder be liable for any claim damage or other liability whether in an action of contract tort or otherwise arising from out of or in connection with the software or the use or other dealing in the software blockchain consensus rule rather than attempt to describe the rule of the blockchain it is up to each individual to inspect the code to understand the consensus rule quickstart just want to get up and running quickly try deploying a prebuilt dockerized container dockerized node see this guide for detailed instruction including command for ubuntu 1604 lts seed node a list of some seed node to get you started can be found in seednodes this list is embedded into default configini building see docbuildingmd for detailed build instruction including compiletime option and specific command for ubuntu 1604 lts system requirement for a full node you need 10gb of space available scorumd us a memory mapped file which currently hold 2gb of data and by default is set to use up to 10gb it highly recommended to run scorumd on a fast disk such a an ssd or by placing the shared memory file in a ramdisk and using the sharedfiledir config or command line option to specify where any cpu with decent single core performance should be sufficient main net chain_id genesisjson hash sum db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1 test net chain_id genesistestnetjson hash sum d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2,introducing scorum scorum platform three core function blogging platform author reader rewarded creating engaging content statistical center fan browse author use microsofts power bi tool integrate datarich visuals content commissionfree betting exchange fan place bet using scorum coin scr scorums blockchain protocol built graphene framework utilizes delegated proof stake consensus public announcement discussion scorum team hard work developing blogging platform statistic center find take project public following channel get latest update chat u telegram facebook twitter read vision steemit join affiliate program get scorum coin free apply whitelist get coin discount support warranty software provided without warranty kind express implied including limited warranty merchantability fitness particular purpose noninfringement event shall author copyright holder liable claim damage liability whether action contract tort otherwise arising connection software use dealing software blockchain consensus rule rather attempt describe rule blockchain individual inspect code understand consensus rule quickstart want get running quickly try deploying prebuilt dockerized container dockerized node see guide detailed instruction including command ubuntu 1604 lts seed node list seed node get started found seednodes list embedded default configini building see docbuildingmd detailed build instruction including compiletime option specific command ubuntu 1604 lts system requirement full node need 10gb space available scorumd us memory mapped file currently hold 2gb data default set use 10gb highly recommended run scorumd fast disk ssd placing shared memory file ramdisk using sharedfiledir config command line option specify cpu decent single core performance sufficient main net chain_id genesisjson hash sum db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1 test net chain_id genesistestnetjson hash sum d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2
C++ ,"Introducing Scorum
Scorum platform has three core functions:

Blogging platform where authors and readers will be rewarded for creating and engaging with content
Statistical centers where fans can browse and authors can use Microsoft’s Power BI tool to integrate data-rich visuals into their content
Commission-free betting exchange where fans can place bets against each other using Scorum Coins (SCR)
Scorum’s blockchain protocol is built on the Graphene Framework and utilizes a delegated proof of stake consensus.

Public Announcement & Discussion
The Scorum team has been hard at work developing the blogging platform and the statistics center.
Find out more as we take the project public through the following channels:

Get the latest updates and chat with us on Telegram, Facebook, and Twitter
Read more about our vision on Steemit
Join our affiliate program to get Scorum Coins for free or apply for whitelist to get coins with a discount

No Support & No Warranty
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
IN THE SOFTWARE.
Blockchain consensus rules
Rather than attempt to describe the rules of the blockchain, it is up to
each individual to inspect the code to understand the consensus rules.
Quickstart
Just want to get up and running quickly?  Try deploying a prebuilt
dockerized container.
Dockerized Node
See this guide for detailed instructions including commands for Ubuntu  16.04 (LTS).
Seed Nodes
A list of some seed nodes to get you started can be found in
seed-nodes. This list is embedded into default config.ini.
Building
See doc/building.md for detailed build instructions, including
compile-time options, and specific commands for Ubuntu 16.04 (LTS).
System Requirements
For a full node, you need 10GB of space available. Scorumd uses a memory mapped file which currently holds 2GB of data and by default is set to use up to 10GB. It's highly recommended to run scorumd on a fast disk such as an SSD or by placing the shared memory files in a ramdisk and using the shared-file-dir config (or command line) option to specify where. Any CPU with decent single core performance should be sufficient.
Main net chain_id
genesis.json hash sum: db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1
Test net chain_id
genesis.testnet.json hash sum: d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2
",introduc scorum scorum platform ha three core function blog platform where author and reader will be reward for creat and engag with content statist center where fan can brows and author can use microsoft power bi tool to integr datarich visual into their content commissionfre bet exchang where fan can place bet against each other use scorum coin scr scorum blockchain protocol is built on the graphen framework and util a deleg proof of stake consensu public announc discuss the scorum team ha been hard at work develop the blog platform and the statist center find out more as we take the project public through the follow channel get the latest updat and chat with us on telegram facebook and twitter read more about our vision on steemit join our affili program to get scorum coin for free or appli for whitelist to get coin with a discount no support no warranti the softwar is provid as is without warranti of ani kind express or impli includ but not limit to the warranti of merchant fit for a particular purpos and noninfring in no event shall the author or copyright holder be liabl for ani claim damag or other liabil whether in an action of contract tort or otherwis aris from out of or in connect with the softwar or the use or other deal in the softwar blockchain consensu rule rather than attempt to describ the rule of the blockchain it is up to each individu to inspect the code to understand the consensu rule quickstart just want to get up and run quickli tri deploy a prebuilt docker contain docker node see thi guid for detail instruct includ command for ubuntu 1604 lt seed node a list of some seed node to get you start can be found in seednod thi list is embed into default configini build see docbuildingmd for detail build instruct includ compiletim option and specif command for ubuntu 1604 lt system requir for a full node you need 10gb of space avail scorumd use a memori map file which current hold 2gb of data and by default is set to use up to 10gb it highli recommend to run scorumd on a fast disk such as an ssd or by place the share memori file in a ramdisk and use the sharedfiledir config or command line option to specifi where ani cpu with decent singl core perform should be suffici main net chain_id genesisjson hash sum db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1 test net chain_id genesistestnetjson hash sum d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2,introducing scorum scorum platform ha three core function blogging platform where author and reader will be rewarded for creating and engaging with content statistical center where fan can browse and author can use microsofts power bi tool to integrate datarich visuals into their content commissionfree betting exchange where fan can place bet against each other using scorum coin scr scorums blockchain protocol is built on the graphene framework and utilizes a delegated proof of stake consensus public announcement discussion the scorum team ha been hard at work developing the blogging platform and the statistic center find out more a we take the project public through the following channel get the latest update and chat with u on telegram facebook and twitter read more about our vision on steemit join our affiliate program to get scorum coin for free or apply for whitelist to get coin with a discount no support no warranty the software is provided a is without warranty of any kind express or implied including but not limited to the warranty of merchantability fitness for a particular purpose and noninfringement in no event shall the author or copyright holder be liable for any claim damage or other liability whether in an action of contract tort or otherwise arising from out of or in connection with the software or the use or other dealing in the software blockchain consensus rule rather than attempt to describe the rule of the blockchain it is up to each individual to inspect the code to understand the consensus rule quickstart just want to get up and running quickly try deploying a prebuilt dockerized container dockerized node see this guide for detailed instruction including command for ubuntu 1604 lts seed node a list of some seed node to get you started can be found in seednodes this list is embedded into default configini building see docbuildingmd for detailed build instruction including compiletime option and specific command for ubuntu 1604 lts system requirement for a full node you need 10gb of space available scorumd us a memory mapped file which currently hold 2gb of data and by default is set to use up to 10gb it highly recommended to run scorumd on a fast disk such a an ssd or by placing the shared memory file in a ramdisk and using the sharedfiledir config or command line option to specify where any cpu with decent single core performance should be sufficient main net chain_id genesisjson hash sum db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1 test net chain_id genesistestnetjson hash sum d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2,introducing scorum scorum platform three core function blogging platform author reader rewarded creating engaging content statistical center fan browse author use microsofts power bi tool integrate datarich visuals content commissionfree betting exchange fan place bet using scorum coin scr scorums blockchain protocol built graphene framework utilizes delegated proof stake consensus public announcement discussion scorum team hard work developing blogging platform statistic center find take project public following channel get latest update chat u telegram facebook twitter read vision steemit join affiliate program get scorum coin free apply whitelist get coin discount support warranty software provided without warranty kind express implied including limited warranty merchantability fitness particular purpose noninfringement event shall author copyright holder liable claim damage liability whether action contract tort otherwise arising connection software use dealing software blockchain consensus rule rather attempt describe rule blockchain individual inspect code understand consensus rule quickstart want get running quickly try deploying prebuilt dockerized container dockerized node see guide detailed instruction including command ubuntu 1604 lts seed node list seed node get started found seednodes list embedded default configini building see docbuildingmd detailed build instruction including compiletime option specific command ubuntu 1604 lts system requirement full node need 10gb space available scorumd us memory mapped file currently hold 2gb data default set use 10gb highly recommended run scorumd fast disk ssd placing shared memory file ramdisk using sharedfiledir config command line option specify cpu decent single core performance sufficient main net chain_id genesisjson hash sum db4007d45f04c1403a7e66a5c66b5b1cdfc2dde8b5335d1d2f116d592ca3dbb1 test net chain_id genesistestnetjson hash sum d3c1f19a4947c296446583f988c43fd1a83818fabaf3454a0020198cb361ebd2
C++ ,"

Перейти на русский язык
This project is devoted to development of the electronic timing system for orienteering with inexpensive base stations and cheap tags.
It is also possible to use one on rogaining events, adventure races, trail running, wherever time keeping is required.
Here are hardware and firmware parts of the timing system.
Links to data processing software are placed below.
Download latest release
Manual
This project is open and free. Whoever is not afraid of difficulties can try doing it oneself. Just follow the instructions from the Manual.
The low cost of the components can be worth your efforts (about USD $10 for one base station and $0.2 per RFID tag).
This development is a hobby.
No guarantees are given, various kinds of problems are possible during reproduction.
Support is also not guaranteed. So, act at your own risk.
Version
The version consists of three numbers. The first number indicates the version of the hardware.
If any changes are made to the circuit or to the PCB, this number is incremented by 1.
The second and third numbers indicates the version of the firmware.
If any new function is added to the firmware, the second number is incremented by 1.
If the firmware just fixes bugs, the third number in the version is incremented by 1.
When a new version of the firmware is released with new functions, the third number is reset to 0.
The base station and the master station have their own versions. The release version is the largest of these two numbers.
The current release version is 3.7.0
The current base station version is 3.7.0
The current master station version is 1.7.0
Changelog
Build the firmware of the base station with #define HW_VERS 1 to install the firmware vX.6.X or greater on the PCB v1 or v2.
Reporting Issues and Asking for Help
Issues and suggested improvements can be posted on Issues page.
Please make sure you provide all relevant information about your problem or idea.
We also have our Telegram chat where you can ask any questions about the system.
Contributing
You can contribute by writing code.
We welcome software for working with the system on a PC via USB and on Android via Bluetooth or NFC.
The data transfer protocol and commands are described in the Manual.
With pleasure we will add a link to your developments working with Sportiduino.
Pull requests, creation of forks, developing any new ideas are welcome.
You can also help by improving documentation and its translation.
Parts of the system
Cards
The system uses cards NTAG213/215/216.
As stickers on Chinese web marketplaces they cost about $0.1, 0.2 and 0.4, respectively.
As key fobs the cost is doubled.
Memory of these cards can keep 32, 120 and 216 marks, respectively.
Also it is possible to use Mifare Classic 1K and 4K cards.
Cards 1K are also cheap and come bundled with the RC522 module.
The memory of these chips is enough for 42 marks. They work a little slower than NTAG.
The system automatically detects the type of used cards.
Read more
Base stations
The main components of the station are the ATmega328P microcontroller and the MFRC522 module,
which operates at a frequency of 13.56 MHz, real-time clock DS3231SN.
All powered by 3 AA batteries through the MCP1700T-33 stabilizer.
The capacity of the kit of three alkaline AA batteries should be enough for a year of active use.
Tested at ambient temperatures from -20 to +50 degrees Celcius.
Totally, the initial components for one base station and the consumables cost about $10 (in 2019).
Read more
Master station
With the master station you can read and write tags and configure base stations.
It is simpler than the base station.
It consists of Arduino Nano, RFID module, LED and buzzer.
It connects with a PC via USB.
Read more
There is also a wireless station with the Bluetooth module.
Data processing
SportiduinoPQ
Cards and stations are configured by SportiduinoPQ program.
The program is written on Python, based on PyQt5 and SportiduinoPython module.
SportOrg
Reading cards is implemented in the SportOrg program.

This system and its variants have been used in Russia at a number of events
up to approx. 1400 participants and approx. 70 check points.

Available from:  https://github.com/sportiduino/sportiduino
License:         GNU GPLv3
",thi project is devot to develop of the electron time system for orient with inexpens base station and cheap tag it is also possibl to use one on rogain event adventur race trail run wherev time keep is requir here are hardwar and firmwar part of the time system link to data process softwar are place below download latest releas manual thi project is open and free whoever is not afraid of difficulti can tri do it oneself just follow the instruct from the manual the low cost of the compon can be worth your effort about usd 10 for one base station and 02 per rfid tag thi develop is a hobbi no guarante are given variou kind of problem are possibl dure reproduct support is also not guarante so act at your own risk version the version consist of three number the first number indic the version of the hardwar if ani chang are made to the circuit or to the pcb thi number is increment by 1 the second and third number indic the version of the firmwar if ani new function is ad to the firmwar the second number is increment by 1 if the firmwar just fix bug the third number in the version is increment by 1 when a new version of the firmwar is releas with new function the third number is reset to 0 the base station and the master station have their own version the releas version is the largest of these two number the current releas version is 370 the current base station version is 370 the current master station version is 170 changelog build the firmwar of the base station with defin hw_ver 1 to instal the firmwar vx6x or greater on the pcb v1 or v2 report issu and ask for help issu and suggest improv can be post on issu page pleas make sure you provid all relev inform about your problem or idea we also have our telegram chat where you can ask ani question about the system contribut you can contribut by write code we welcom softwar for work with the system on a pc via usb and on android via bluetooth or nfc the data transfer protocol and command are describ in the manual with pleasur we will add a link to your develop work with sportiduino pull request creation of fork develop ani new idea are welcom you can also help by improv document and it translat part of the system card the system use card ntag213215216 as sticker on chines web marketplac they cost about 01 02 and 04 respect as key fob the cost is doubl memori of these card can keep 32 120 and 216 mark respect also it is possibl to use mifar classic 1k and 4k card card 1k are also cheap and come bundl with the rc522 modul the memori of these chip is enough for 42 mark they work a littl slower than ntag the system automat detect the type of use card read more base station the main compon of the station are the atmega328p microcontrol and the mfrc522 modul which oper at a frequenc of 1356 mhz realtim clock ds3231sn all power by 3 aa batteri through the mcp1700t33 stabil the capac of the kit of three alkalin aa batteri should be enough for a year of activ use test at ambient temperatur from 20 to 50 degre celciu total the initi compon for one base station and the consum cost about 10 in 2019 read more master station with the master station you can read and write tag and configur base station it is simpler than the base station it consist of arduino nano rfid modul led and buzzer it connect with a pc via usb read more there is also a wireless station with the bluetooth modul data process sportiduinopq card and station are configur by sportiduinopq program the program is written on python base on pyqt5 and sportiduinopython modul sportorg read card is implement in the sportorg program thi system and it variant have been use in russia at a number of event up to approx 1400 particip and approx 70 check point avail from httpsgithubcomsportiduinosportiduino licens gnu gplv3,this project is devoted to development of the electronic timing system for orienteering with inexpensive base station and cheap tag it is also possible to use one on rogaining event adventure race trail running wherever time keeping is required here are hardware and firmware part of the timing system link to data processing software are placed below download latest release manual this project is open and free whoever is not afraid of difficulty can try doing it oneself just follow the instruction from the manual the low cost of the component can be worth your effort about usd 10 for one base station and 02 per rfid tag this development is a hobby no guarantee are given various kind of problem are possible during reproduction support is also not guaranteed so act at your own risk version the version consists of three number the first number indicates the version of the hardware if any change are made to the circuit or to the pcb this number is incremented by 1 the second and third number indicates the version of the firmware if any new function is added to the firmware the second number is incremented by 1 if the firmware just fix bug the third number in the version is incremented by 1 when a new version of the firmware is released with new function the third number is reset to 0 the base station and the master station have their own version the release version is the largest of these two number the current release version is 370 the current base station version is 370 the current master station version is 170 changelog build the firmware of the base station with define hw_vers 1 to install the firmware vx6x or greater on the pcb v1 or v2 reporting issue and asking for help issue and suggested improvement can be posted on issue page please make sure you provide all relevant information about your problem or idea we also have our telegram chat where you can ask any question about the system contributing you can contribute by writing code we welcome software for working with the system on a pc via usb and on android via bluetooth or nfc the data transfer protocol and command are described in the manual with pleasure we will add a link to your development working with sportiduino pull request creation of fork developing any new idea are welcome you can also help by improving documentation and it translation part of the system card the system us card ntag213215216 a sticker on chinese web marketplace they cost about 01 02 and 04 respectively a key fob the cost is doubled memory of these card can keep 32 120 and 216 mark respectively also it is possible to use mifare classic 1k and 4k card card 1k are also cheap and come bundled with the rc522 module the memory of these chip is enough for 42 mark they work a little slower than ntag the system automatically detects the type of used card read more base station the main component of the station are the atmega328p microcontroller and the mfrc522 module which operates at a frequency of 1356 mhz realtime clock ds3231sn all powered by 3 aa battery through the mcp1700t33 stabilizer the capacity of the kit of three alkaline aa battery should be enough for a year of active use tested at ambient temperature from 20 to 50 degree celcius totally the initial component for one base station and the consumables cost about 10 in 2019 read more master station with the master station you can read and write tag and configure base station it is simpler than the base station it consists of arduino nano rfid module led and buzzer it connects with a pc via usb read more there is also a wireless station with the bluetooth module data processing sportiduinopq card and station are configured by sportiduinopq program the program is written on python based on pyqt5 and sportiduinopython module sportorg reading card is implemented in the sportorg program this system and it variant have been used in russia at a number of event up to approx 1400 participant and approx 70 check point available from httpsgithubcomsportiduinosportiduino license gnu gplv3,project devoted development electronic timing system orienteering inexpensive base station cheap tag also possible use one rogaining event adventure race trail running wherever time keeping required hardware firmware part timing system link data processing software placed download latest release manual project open free whoever afraid difficulty try oneself follow instruction manual low cost component worth effort usd 10 one base station 02 per rfid tag development hobby guarantee given various kind problem possible reproduction support also guaranteed act risk version version consists three number first number indicates version hardware change made circuit pcb number incremented 1 second third number indicates version firmware new function added firmware second number incremented 1 firmware fix bug third number version incremented 1 new version firmware released new function third number reset 0 base station master station version release version largest two number current release version 370 current base station version 370 current master station version 170 changelog build firmware base station define hw_vers 1 install firmware vx6x greater pcb v1 v2 reporting issue asking help issue suggested improvement posted issue page please make sure provide relevant information problem idea also telegram chat ask question system contributing contribute writing code welcome software working system pc via usb android via bluetooth nfc data transfer protocol command described manual pleasure add link development working sportiduino pull request creation fork developing new idea welcome also help improving documentation translation part system card system us card ntag213215216 sticker chinese web marketplace cost 01 02 04 respectively key fob cost doubled memory card keep 32 120 216 mark respectively also possible use mifare classic 1k 4k card card 1k also cheap come bundled rc522 module memory chip enough 42 mark work little slower ntag system automatically detects type used card read base station main component station atmega328p microcontroller mfrc522 module operates frequency 1356 mhz realtime clock ds3231sn powered 3 aa battery mcp1700t33 stabilizer capacity kit three alkaline aa battery enough year active use tested ambient temperature 20 50 degree celcius totally initial component one base station consumables cost 10 2019 read master station master station read write tag configure base station simpler base station consists arduino nano rfid module led buzzer connects pc via usb read also wireless station bluetooth module data processing sportiduinopq card station configured sportiduinopq program program written python based pyqt5 sportiduinopython module sportorg reading card implemented sportorg program system variant used russia number event approx 1400 participant approx 70 check point available httpsgithubcomsportiduinosportiduino license gnu gplv3
C++ ,"Sports Recognition in Videos
This repository builds a classifier to predict the sport being played using Dense Trajectory Features. It works
on the UCF Sports dataset and builds a multi-class classifier based on SVM using chi-squared kernel. The paper behind the project is:
Heng Wang, Alexander Kläser, Cordelia Schmid, Liu Cheng-Lin. Action Recognition by Dense Trajectories. CVPR 2011 - IEEE Conference on Computer Vision & Pattern Recognition, Jun 2011
Prerequisites

The UCF Sports Dataset, a
stripped down version (with no image dumps) can be found
here
Dense Trajectory Features,
this code has been included in dense_trajectory_release_v1.2 folder

Project Setup
Run setup.sh to install all dependencies. It also builds a DenseTrack executable
which gives out the features of all videos
Code Flow

The DenseTrack executable computes a large feature vector comprising of
HOG + HOF + MBH descriptors concatenated with each other
The data is split into train and test with a ratio test_size
A codebook of size k using k-means clustering is generated in attempts
A bag-of-visual-words representation is created for each video using the
histogram built using the above clustering
All the bag-of-visual-words are fed into the SVM using chi-squared kernel
and classified using a One-Vs-Rest Classifier

Execution
To run the code, run Driver.py which generates the One-Vs-Rest Classifier and
dumps in a model.p file and along with the codebook centers for bag-of-visual-words
in centers.p.
Evaluation
The code has been evaluated on accuracy of predictions after the test and train
split ratio of 0.3. Following classes have been used from the UCF Sports Dataset:

Diving-Side (7 videos)
Kicking-Front (10 videos)
Riding-Horse (12 videos)
Run-Side (13 videos)
SkateBoarding-Front (12 videos)
Swing-Bench (20 videos)
Swing-SideAngle (13 videos)
Walk-Front (22 videos)

After preliminary evaluation, we achieved a result of around 29%.
Contact
The repository has been made available at
https://github.com/ChinmayJindal/sports-recognition
",sport recognit in video thi repositori build a classifi to predict the sport be play use dens trajectori featur it work on the ucf sport dataset and build a multiclass classifi base on svm use chisquar kernel the paper behind the project is heng wang alexand klser cordelia schmid liu chenglin action recognit by dens trajectori cvpr 2011 ieee confer on comput vision pattern recognit jun 2011 prerequisit the ucf sport dataset a strip down version with no imag dump can be found here dens trajectori featur thi code ha been includ in dense_trajectory_release_v12 folder project setup run setupsh to instal all depend it also build a densetrack execut which give out the featur of all video code flow the densetrack execut comput a larg featur vector compris of hog hof mbh descriptor concaten with each other the data is split into train and test with a ratio test_siz a codebook of size k use kmean cluster is gener in attempt a bagofvisualword represent is creat for each video use the histogram built use the abov cluster all the bagofvisualword are fed into the svm use chisquar kernel and classifi use a onevsrest classifi execut to run the code run driverpi which gener the onevsrest classifi and dump in a modelp file and along with the codebook center for bagofvisualword in centersp evalu the code ha been evalu on accuraci of predict after the test and train split ratio of 03 follow class have been use from the ucf sport dataset divingsid 7 video kickingfront 10 video ridinghors 12 video runsid 13 video skateboardingfront 12 video swingbench 20 video swingsideangl 13 video walkfront 22 video after preliminari evalu we achiev a result of around 29 contact the repositori ha been made avail at httpsgithubcomchinmayjindalsportsrecognit,sport recognition in video this repository build a classifier to predict the sport being played using dense trajectory feature it work on the ucf sport dataset and build a multiclass classifier based on svm using chisquared kernel the paper behind the project is heng wang alexander klser cordelia schmid liu chenglin action recognition by dense trajectory cvpr 2011 ieee conference on computer vision pattern recognition jun 2011 prerequisite the ucf sport dataset a stripped down version with no image dump can be found here dense trajectory feature this code ha been included in dense_trajectory_release_v12 folder project setup run setupsh to install all dependency it also build a densetrack executable which give out the feature of all video code flow the densetrack executable computes a large feature vector comprising of hog hof mbh descriptor concatenated with each other the data is split into train and test with a ratio test_size a codebook of size k using kmeans clustering is generated in attempt a bagofvisualwords representation is created for each video using the histogram built using the above clustering all the bagofvisualwords are fed into the svm using chisquared kernel and classified using a onevsrest classifier execution to run the code run driverpy which generates the onevsrest classifier and dump in a modelp file and along with the codebook center for bagofvisualwords in centersp evaluation the code ha been evaluated on accuracy of prediction after the test and train split ratio of 03 following class have been used from the ucf sport dataset divingside 7 video kickingfront 10 video ridinghorse 12 video runside 13 video skateboardingfront 12 video swingbench 20 video swingsideangle 13 video walkfront 22 video after preliminary evaluation we achieved a result of around 29 contact the repository ha been made available at httpsgithubcomchinmayjindalsportsrecognition,sport recognition video repository build classifier predict sport played using dense trajectory feature work ucf sport dataset build multiclass classifier based svm using chisquared kernel paper behind project heng wang alexander klser cordelia schmid liu chenglin action recognition dense trajectory cvpr 2011 ieee conference computer vision pattern recognition jun 2011 prerequisite ucf sport dataset stripped version image dump found dense trajectory feature code included dense_trajectory_release_v12 folder project setup run setupsh install dependency also build densetrack executable give feature video code flow densetrack executable computes large feature vector comprising hog hof mbh descriptor concatenated data split train test ratio test_size codebook size k using kmeans clustering generated attempt bagofvisualwords representation created video using histogram built using clustering bagofvisualwords fed svm using chisquared kernel classified using onevsrest classifier execution run code run driverpy generates onevsrest classifier dump modelp file along codebook center bagofvisualwords centersp evaluation code evaluated accuracy prediction test train split ratio 03 following class used ucf sport dataset divingside 7 video kickingfront 10 video ridinghorse 12 video runside 13 video skateboardingfront 12 video swingbench 20 video swingsideangle 13 video walkfront 22 video preliminary evaluation achieved result around 29 contact repository made available httpsgithubcomchinmayjindalsportsrecognition
C++ ,"two_point_calib
This is an implementation of ""A Two-point Method for PTZ camera Calibration in Sports"" (WACV2018)
Dependences:

OpenCV 3.1 or later.
Eigen 3.2.6 or later.
flann 1.8.4 or later.
matio: https://github.com/tbeu/matio

The code is tested on Xcode 6.4 on a Mac 10.10.5 system. But the code has minimum dependence on compile and system, so it should work well on linux and windows as well.
File structure:
matlab: synthetic example of the two-point calibration method.
In the src/pan_tilt_forest folder
bt_dtr: a general implementation of regression forest with back tracking
cvx_gl and cvx_pgl: geometry and project geometry files
dt_util: decision tree utility
util: pan-tilt-zoom camera pose estimation given ray-3D correspondences
Todo: cmake file and training parameters
",two_point_calib thi is an implement of a twopoint method for ptz camera calibr in sport wacv2018 depend opencv 31 or later eigen 326 or later flann 184 or later matio httpsgithubcomtbeumatio the code is test on xcode 64 on a mac 10105 system but the code ha minimum depend on compil and system so it should work well on linux and window as well file structur matlab synthet exampl of the twopoint calibr method in the srcpan_tilt_forest folder bt_dtr a gener implement of regress forest with back track cvx_gl and cvx_pgl geometri and project geometri file dt_util decis tree util util pantiltzoom camera pose estim given ray3d correspond todo cmake file and train paramet,two_point_calib this is an implementation of a twopoint method for ptz camera calibration in sport wacv2018 dependence opencv 31 or later eigen 326 or later flann 184 or later matio httpsgithubcomtbeumatio the code is tested on xcode 64 on a mac 10105 system but the code ha minimum dependence on compile and system so it should work well on linux and window a well file structure matlab synthetic example of the twopoint calibration method in the srcpan_tilt_forest folder bt_dtr a general implementation of regression forest with back tracking cvx_gl and cvx_pgl geometry and project geometry file dt_util decision tree utility util pantiltzoom camera pose estimation given ray3d correspondence todo cmake file and training parameter,two_point_calib implementation twopoint method ptz camera calibration sport wacv2018 dependence opencv 31 later eigen 326 later flann 184 later matio httpsgithubcomtbeumatio code tested xcode 64 mac 10105 system code minimum dependence compile system work well linux window well file structure matlab synthetic example twopoint calibration method srcpan_tilt_forest folder bt_dtr general implementation regression forest back tracking cvx_gl cvx_pgl geometry project geometry file dt_util decision tree utility util pantiltzoom camera pose estimation given ray3d correspondence todo cmake file training parameter
C++ ,"Pekka Pirilän tulospalveluohjelma (sports time keeping program)
Briefly in Finnish / Lyhyesti suomeksi
Tässä ovat Pekka Pirilän (1945-2015) tulospalveluohjelmien lähdekoodit. Mukana
ovat kaikki muunnelmat: teksti- ja Windows-versio sekä henkilökohtaisesta että
viestiohjelmasta. Pekan perhe julkaisi lähdekoodin avoimen lähdekoodin
GPLv3-lisenssin alaiseksi. Lisenssi tarkoittaa karkeasti sitä, että kuka vain
voi muuttaa ja käyttää ohjelmaa vapaasti kaikissa tilanteissa. Ohjelman
käyttöön liittyviä palveluita ja jopa kopioita ohjelmasta saa myydä, mutta
muutetun version levittäjä sitoutuu julkaisemaan versiostaan myös lähdekoodin
ja ostaja saa jälleen tehdä kopiollaan mitä haluaa.
Valmiiksi käännetyt ja paketoidut versiot sekä ohjeet löytyvät edelleen
osoitteesta http://www.pirila.fi/ohj/index.html.
Pekan perheen toiveena on, että ohjelmasta olisi hyötyä urheiluyhteisölle vielä
vuosien ajan.
Description
This is the source code of a suite of sports time keeping programs Pekka Pirilä
(1945-2015) started developing in around 1986. The program originally
specialized in orienteering, but was later amended to support additional
sports. The user interface is in Finnish and there are Finnish language
variables and comments throughout the source code. The source code is released
under GPLv3. More information in Finnish at http://www.pirila.fi/ohj/index.html.
Console programs
Required tools
The console program's project files are for Visual Studio. They were
successfully compiled with Visual Studio Express 2013 for Windows Desktop,
but Visual Studio 2010 and anything newer can probably be made to work. The
language is probably compliant with C++03.
Compile

Open TPsource\V52\VS\Libs\tputilv2.sln
Compile by pressing F7

Two new folders will be created next to TPsource: vc10 and TPexe


Open TPsource\V52\VS\Hk\HkMaali520.sln
Compile by pressing F7

A stand-alone executable TPexe\Hk\V521\HkMaali.exe is built and ready
to use


To build the relay version, repeat with TPsource\V52\VS\V\JukMaali520.sln

Windows programs
Required tools
The Windows programs are made with
Embarcadero C++ Builder.
Version 10.1 Berlin was successfully used to compile and run the program.
SecureBridge 7.1 for RAD Studio 10.1 Berlin
is an add-on that is required to compile and run the program. NOTE: At least
in Windows 10 you must edit one of SecureBridge's header files to be able to
compile this program. Open
""Program Files (x86)\Devart\SecureBridge for RAD Studio 10\Include\Win32\ScSSHSocket.hpp""
and change Winapi::Winsock::PSockAddrIn to Winapi::Winsock2::PSockAddrIn.
Compile

Open TPsource\V52\RADStudio10\DBboxm-XE.cbproj
Right-click on DBboxm-XE.lib in Project Manager and select Make
Open TPsource\V52\RADStudio10\Tputil-XE.cbproj
Right-click on Tputil-XE.lib in Project Manager and select Make
Open TPsource\V52\RADStudio10\HkKisaWin.cbproj and Run to start the
program for individual competitions
Open TPsource\V52\RADStudio10\ViestiWin.cbproj and Run to start the
relay program

Heap errors during compilation
If you get heap errors with linking, you can try these things

http://stackoverflow.com/questions/28929516/c-builder-xe7-lme288-error

Run command prompt as Administrator.
Type (without quotes) ""bcdedit /set IncreaseUserVa 3072""
Reboot computer.


Empty %TEMP%, reboot, try again, repeat
Run C++ Builder as admin

",pekka piriln tulospalveluohjelma sport time keep program briefli in finnish lyhyesti suomeksi tss ovat pekka piriln 19452015 tulospalveluohjelmien lhdekoodit mukana ovat kaikki muunnelmat teksti ja windowsversio sek henkilkohtaisesta ett viestiohjelmasta pekan perh julkaisi lhdekoodin avoimen lhdekoodin gplv3lisenssin alaiseksi lisenssi tarkoittaa karkeasti sit ett kuka vain voi muuttaa ja kytt ohjelmaa vapaasti kaikissa tilanteissa ohjelman kyttn liittyvi palveluita ja jopa kopioita ohjelmasta saa myyd mutta muutetun version levittj sitoutuu julkaisemaan versiostaan my lhdekoodin ja ostaja saa jlleen tehd kopiollaan mit haluaa valmiiksi knnetyt ja paketoidut versiot sek ohjeet lytyvt edelleen osoitteesta httpwwwpirilafiohjindexhtml pekan perheen toiveena on ett ohjelmasta olisi hyti urheiluyhteisl viel vuosien ajan descript thi is the sourc code of a suit of sport time keep program pekka piril 19452015 start develop in around 1986 the program origin special in orient but wa later amend to support addit sport the user interfac is in finnish and there are finnish languag variabl and comment throughout the sourc code the sourc code is releas under gplv3 more inform in finnish at httpwwwpirilafiohjindexhtml consol program requir tool the consol program project file are for visual studio they were success compil with visual studio express 2013 for window desktop but visual studio 2010 and anyth newer can probabl be made to work the languag is probabl compliant with c03 compil open tpsourcev52vslibstputilv2sln compil by press f7 two new folder will be creat next to tpsourc vc10 and tpex open tpsourcev52vshkhkmaali520sln compil by press f7 a standalon execut tpexehkv521hkmaaliex is built and readi to use to build the relay version repeat with tpsourcev52vsvjukmaali520sln window program requir tool the window program are made with embarcadero c builder version 101 berlin wa success use to compil and run the program securebridg 71 for rad studio 101 berlin is an addon that is requir to compil and run the program note at least in window 10 you must edit one of securebridg header file to be abl to compil thi program open program file x86devartsecurebridg for rad studio 10includewin32scsshsockethpp and chang winapiwinsockpsockaddrin to winapiwinsock2psockaddrin compil open tpsourcev52radstudio10dbboxmxecbproj rightclick on dbboxmxelib in project manag and select make open tpsourcev52radstudio10tputilxecbproj rightclick on tputilxelib in project manag and select make open tpsourcev52radstudio10hkkisawincbproj and run to start the program for individu competit open tpsourcev52radstudio10viestiwincbproj and run to start the relay program heap error dure compil if you get heap error with link you can tri these thing httpstackoverflowcomquestions28929516cbuilderxe7lme288error run command prompt as administr type without quot bcdedit set increaseuserva 3072 reboot comput empti temp reboot tri again repeat run c builder as admin,pekka piriln tulospalveluohjelma sport time keeping program briefly in finnish lyhyesti suomeksi tss ovat pekka piriln 19452015 tulospalveluohjelmien lhdekoodit mukana ovat kaikki muunnelmat teksti ja windowsversio sek henkilkohtaisesta ett viestiohjelmasta pekan perhe julkaisi lhdekoodin avoimen lhdekoodin gplv3lisenssin alaiseksi lisenssi tarkoittaa karkeasti sit ett kuka vain voi muuttaa ja kytt ohjelmaa vapaasti kaikissa tilanteissa ohjelman kyttn liittyvi palveluita ja jopa kopioita ohjelmasta saa myyd mutta muutetun version levittj sitoutuu julkaisemaan versiostaan mys lhdekoodin ja ostaja saa jlleen tehd kopiollaan mit haluaa valmiiksi knnetyt ja paketoidut versiot sek ohjeet lytyvt edelleen osoitteesta httpwwwpirilafiohjindexhtml pekan perheen toiveena on ett ohjelmasta olisi hyty urheiluyhteislle viel vuosien ajan description this is the source code of a suite of sport time keeping program pekka piril 19452015 started developing in around 1986 the program originally specialized in orienteering but wa later amended to support additional sport the user interface is in finnish and there are finnish language variable and comment throughout the source code the source code is released under gplv3 more information in finnish at httpwwwpirilafiohjindexhtml console program required tool the console program project file are for visual studio they were successfully compiled with visual studio express 2013 for window desktop but visual studio 2010 and anything newer can probably be made to work the language is probably compliant with c03 compile open tpsourcev52vslibstputilv2sln compile by pressing f7 two new folder will be created next to tpsource vc10 and tpexe open tpsourcev52vshkhkmaali520sln compile by pressing f7 a standalone executable tpexehkv521hkmaaliexe is built and ready to use to build the relay version repeat with tpsourcev52vsvjukmaali520sln window program required tool the window program are made with embarcadero c builder version 101 berlin wa successfully used to compile and run the program securebridge 71 for rad studio 101 berlin is an addon that is required to compile and run the program note at least in window 10 you must edit one of securebridges header file to be able to compile this program open program file x86devartsecurebridge for rad studio 10includewin32scsshsockethpp and change winapiwinsockpsockaddrin to winapiwinsock2psockaddrin compile open tpsourcev52radstudio10dbboxmxecbproj rightclick on dbboxmxelib in project manager and select make open tpsourcev52radstudio10tputilxecbproj rightclick on tputilxelib in project manager and select make open tpsourcev52radstudio10hkkisawincbproj and run to start the program for individual competition open tpsourcev52radstudio10viestiwincbproj and run to start the relay program heap error during compilation if you get heap error with linking you can try these thing httpstackoverflowcomquestions28929516cbuilderxe7lme288error run command prompt a administrator type without quote bcdedit set increaseuserva 3072 reboot computer empty temp reboot try again repeat run c builder a admin,pekka piriln tulospalveluohjelma sport time keeping program briefly finnish lyhyesti suomeksi tss ovat pekka piriln 19452015 tulospalveluohjelmien lhdekoodit mukana ovat kaikki muunnelmat teksti ja windowsversio sek henkilkohtaisesta ett viestiohjelmasta pekan perhe julkaisi lhdekoodin avoimen lhdekoodin gplv3lisenssin alaiseksi lisenssi tarkoittaa karkeasti sit ett kuka vain voi muuttaa ja kytt ohjelmaa vapaasti kaikissa tilanteissa ohjelman kyttn liittyvi palveluita ja jopa kopioita ohjelmasta saa myyd mutta muutetun version levittj sitoutuu julkaisemaan versiostaan mys lhdekoodin ja ostaja saa jlleen tehd kopiollaan mit haluaa valmiiksi knnetyt ja paketoidut versiot sek ohjeet lytyvt edelleen osoitteesta httpwwwpirilafiohjindexhtml pekan perheen toiveena ett ohjelmasta olisi hyty urheiluyhteislle viel vuosien ajan description source code suite sport time keeping program pekka piril 19452015 started developing around 1986 program originally specialized orienteering later amended support additional sport user interface finnish finnish language variable comment throughout source code source code released gplv3 information finnish httpwwwpirilafiohjindexhtml console program required tool console program project file visual studio successfully compiled visual studio express 2013 window desktop visual studio 2010 anything newer probably made work language probably compliant c03 compile open tpsourcev52vslibstputilv2sln compile pressing f7 two new folder created next tpsource vc10 tpexe open tpsourcev52vshkhkmaali520sln compile pressing f7 standalone executable tpexehkv521hkmaaliexe built ready use build relay version repeat tpsourcev52vsvjukmaali520sln window program required tool window program made embarcadero c builder version 101 berlin successfully used compile run program securebridge 71 rad studio 101 berlin addon required compile run program note least window 10 must edit one securebridges header file able compile program open program file x86devartsecurebridge rad studio 10includewin32scsshsockethpp change winapiwinsockpsockaddrin winapiwinsock2psockaddrin compile open tpsourcev52radstudio10dbboxmxecbproj rightclick dbboxmxelib project manager select make open tpsourcev52radstudio10tputilxecbproj rightclick tputilxelib project manager select make open tpsourcev52radstudio10hkkisawincbproj run start program individual competition open tpsourcev52radstudio10viestiwincbproj run start relay program heap error compilation get heap error linking try thing httpstackoverflowcomquestions28929516cbuilderxe7lme288error run command prompt administrator type without quote bcdedit set increaseuserva 3072 reboot computer empty temp reboot try repeat run c builder admin
C++ ,"DFSCoin development tree
DFSCoin is a PoS-based cryptocurrency.
Development process
Developers work in their own trees, then submit pull requests when
they think their feature or bug fix is ready.
The patch will be accepted if there is broad consensus that it is a
good thing.  Developers should expect to rework and resubmit patches
if they don't match the project's coding conventions (see coding.txt)
or are controversial.
The master branch is regularly built and tested, but is not guaranteed
to be completely stable. Tags are regularly created to indicate new
stable release versions of DFSCoin.
Feature branches are created when there are major new features being
worked on by several people.
From time to time a pull request will become outdated. If this occurs, and
the pull is no longer automatically mergeable; a comment on the pull will
be used to issue a warning of closure. The pull will be closed 15 days
after the warning if action is not taken by the author. Pull requests closed
in this manner will have their corresponding issue labeled 'stagnant'.
Issues with no commits will be given a similar warning, and closed after
15 days from their last activity. Issues closed in this manner will be
labeled 'stale'.
",dfscoin develop tree dfscoin is a posbas cryptocurr develop process develop work in their own tree then submit pull request when they think their featur or bug fix is readi the patch will be accept if there is broad consensu that it is a good thing develop should expect to rework and resubmit patch if they dont match the project code convent see codingtxt or are controversi the master branch is regularli built and test but is not guarante to be complet stabl tag are regularli creat to indic new stabl releas version of dfscoin featur branch are creat when there are major new featur be work on by sever peopl from time to time a pull request will becom outdat if thi occur and the pull is no longer automat mergeabl a comment on the pull will be use to issu a warn of closur the pull will be close 15 day after the warn if action is not taken by the author pull request close in thi manner will have their correspond issu label stagnant issu with no commit will be given a similar warn and close after 15 day from their last activ issu close in thi manner will be label stale,dfscoin development tree dfscoin is a posbased cryptocurrency development process developer work in their own tree then submit pull request when they think their feature or bug fix is ready the patch will be accepted if there is broad consensus that it is a good thing developer should expect to rework and resubmit patch if they dont match the project coding convention see codingtxt or are controversial the master branch is regularly built and tested but is not guaranteed to be completely stable tag are regularly created to indicate new stable release version of dfscoin feature branch are created when there are major new feature being worked on by several people from time to time a pull request will become outdated if this occurs and the pull is no longer automatically mergeable a comment on the pull will be used to issue a warning of closure the pull will be closed 15 day after the warning if action is not taken by the author pull request closed in this manner will have their corresponding issue labeled stagnant issue with no commits will be given a similar warning and closed after 15 day from their last activity issue closed in this manner will be labeled stale,dfscoin development tree dfscoin posbased cryptocurrency development process developer work tree submit pull request think feature bug fix ready patch accepted broad consensus good thing developer expect rework resubmit patch dont match project coding convention see codingtxt controversial master branch regularly built tested guaranteed completely stable tag regularly created indicate new stable release version dfscoin feature branch created major new feature worked several people time time pull request become outdated occurs pull longer automatically mergeable comment pull used issue warning closure pull closed 15 day warning action taken author pull request closed manner corresponding issue labeled stagnant issue commits given similar warning closed 15 day last activity issue closed manner labeled stale
C++ ,"SportsCash Core integration/staging repository
Dash forked Bitcoin - PIVX Forked Dash - SPORTSCASH Forked PIVX
Project INFO

Bitcointalkhttps://bitcointalk.org/index.php?topic=4356707.msg38922386#msg38922386
Telegramhttps://t.me/joinchat/GzldMxHltxGz_ZqHSFlGPg
Discordhttps://discord.gg/GfuxwGB
Twitterhttps://twitter.com/SportsCashCoin
Reddithttps://www.reddit.com/r/SportsCashCoin/
Websitehttp://sportscash.co

Coin Specs

AlgoQuark
Block Time60 Seconds
Difficulty RetargetingEvery Block
Max Coin Supply (PoS)24,000,000 SCC
Premine1,000,000 SCC
Masternode Collateral10,000 SCC
Port Collateral33001
RPCPort Collateral33002

PoS Rewards Breakdown

Block HeightRewardMasternodesStakers
<= 2,0001 SCC0.9 SCC0.1 SCC
2001-1,144,8577 SCC6.3 SCC0.7 SCC
1,144,858-2,744,8575 SCC4.5 SCC0.5 SCC
>2744858-54115233 SCC2.7 SCC0.3 SCC
5411523- 0 SCC0 SCC0 SCC

",sportscash core integrationstag repositori dash fork bitcoin pivx fork dash sportscash fork pivx project info bitcointalkhttpsbitcointalkorgindexphptopic4356707msg38922386msg38922386 telegramhttpstmejoinchatgzldmxhltxgz_zqhsflgpg discordhttpsdiscordgggfuxwgb twitterhttpstwittercomsportscashcoin reddithttpswwwredditcomrsportscashcoin websitehttpsportscashco coin spec algoquark block time60 second difficulti retargetingeveri block max coin suppli pos24000000 scc premine1000000 scc masternod collateral10000 scc port collateral33001 rpcport collateral33002 po reward breakdown block heightrewardmasternodesstak 20001 scc09 scc01 scc 200111448577 scc63 scc07 scc 114485827448575 scc45 scc05 scc 274485854115233 scc27 scc03 scc 5411523 0 scc0 scc0 scc,sportscash core integrationstaging repository dash forked bitcoin pivx forked dash sportscash forked pivx project info bitcointalkhttpsbitcointalkorgindexphptopic4356707msg38922386msg38922386 telegramhttpstmejoinchatgzldmxhltxgz_zqhsflgpg discordhttpsdiscordgggfuxwgb twitterhttpstwittercomsportscashcoin reddithttpswwwredditcomrsportscashcoin websitehttpsportscashco coin spec algoquark block time60 second difficulty retargetingevery block max coin supply pos24000000 scc premine1000000 scc masternode collateral10000 scc port collateral33001 rpcport collateral33002 po reward breakdown block heightrewardmasternodesstakers 20001 scc09 scc01 scc 200111448577 scc63 scc07 scc 114485827448575 scc45 scc05 scc 274485854115233 scc27 scc03 scc 5411523 0 scc0 scc0 scc,sportscash core integrationstaging repository dash forked bitcoin pivx forked dash sportscash forked pivx project info bitcointalkhttpsbitcointalkorgindexphptopic4356707msg38922386msg38922386 telegramhttpstmejoinchatgzldmxhltxgz_zqhsflgpg discordhttpsdiscordgggfuxwgb twitterhttpstwittercomsportscashcoin reddithttpswwwredditcomrsportscashcoin websitehttpsportscashco coin spec algoquark block time60 second difficulty retargetingevery block max coin supply pos24000000 scc premine1000000 scc masternode collateral10000 scc port collateral33001 rpcport collateral33002 po reward breakdown block heightrewardmasternodesstakers 20001 scc09 scc01 scc 200111448577 scc63 scc07 scc 114485827448575 scc45 scc05 scc 274485854115233 scc27 scc03 scc 5411523 0 scc0 scc0 scc
C++ ,"sports
Analytics, tools and puzzles applicable to a variety of sports.
",sport analyt tool and puzzl applic to a varieti of sport,sport analytics tool and puzzle applicable to a variety of sport,sport analytics tool puzzle applicable variety sport
C++ ,"Coursera-Data Structures and Algorithms Specialization
This specialization is a mix of theory and practice: you will learn algorithmic techniques for solving various computational problems and will implement about 100 algorithmic coding problems in a programming language of your choice. No other online course in Algorithms even comes close to offering you a wealth of programming challenges that you may face at your next job interview. To prepare you, we invested over 3000 hours into designing our challenges as an alternative to multiple choice questions that you usually find in MOOCs. Sorry, we do not believe in multiple choice questions when it comes to learning algorithms...or anything else in computer science! For each algorithm you develop and implement, we designed multiple tests to check its correctness and running time — you will have to debug your programs without even knowing what these tests are! It may sound difficult, but we believe it is the only way to truly understand how the algorithms work and to master the art of programming. The specialization contains two real-world projects: Big Networks and Genome Assembly. You will analyze both road networks and social networks and will learn how to compute the shortest route between New York and San Francisco (1000 times faster than the standard shortest path algorithms!) Afterwards, you will learn how to assemble genomes from millions of short fragments of DNA and how assembly algorithms fuel recent developments in personalized medicine.
Data Structures and Algorithms Specialization
Skills Gained:
Algorithms Data Structure Debugging Graph Theory Software Testing Binary Search Tree Computer Programming
Course 1 - Algorithmic Toolbox
The course covers basic algorithmic techniques and ideas for computational problems arising frequently in practical applications: sorting and searching, divide and conquer, greedy algorithms, dynamic programming. We will learn a lot of theory: how to sort data and how it helps for searching; how to break a large problem into pieces and solve them recursively; when it makes sense to proceed greedily; how dynamic programming is used in genomic studies. You will practice solving computational problems, designing new algorithms, and implementing solutions efficiently (so that they run in less than a second).
Course 2 - Data Structures
A good algorithm usually comes together with a set of good data structures that allow the algorithm to manipulate the data efficiently. In this course, we consider the common data structures that are used in various computational problems. You will learn how these data structures are implemented in different programming languages and will practice implementing them in our programming assignments. This will help you to understand what is going on inside a particular built-in implementation of a data structure and what to expect from it. You will also learn typical use cases for these data structures.
A few examples of questions that we are going to cover in this class are the following:

What is a good strategy of resizing a dynamic array?
How priority queues are implemented in C++, Java, and Python?
How to implement a hash table so that the amortized running time of all operations is O(1) on average?
What are good strategies to keep a binary tree balanced?

Course 3 - Algorithms on Graphs
If you have ever used a navigation service to find optimal route and estimate time to destination, you've used algorithms on graphs. Graphs arise in various real-world situations as there are road networks, computer networks and, most recently, social networks! If you're looking for the fastest time to get to work, cheapest way to connect set of computers into a network or efficient algorithm to automatically find communities and opinion leaders in Facebook, you're going to work with graphs and algorithms on graphs.
In this course, you will first learn what a graph is and what are some of the most important properties. Then you'll learn several ways to traverse graphs and how you can do useful things while traversing the graph in some order. We will then talk about shortest paths algorithms — from the basic ones to those which open door for 1000000 times faster algorithms used in Google Maps and other navigational services. You will use these algorithms if you choose to work on our Fast Shortest Routes industrial capstone project. We will finish with minimum spanning trees which are used to plan road, telephone and computer networks and also find applications in clustering and approximate algorithms.
Course 4 - Algorithms on Strings
World and internet is full of textual information. We search for information using textual queries, we read websites, books, e-mails. All those are strings from the point of view of computer science. To make sense of all that information and make search efficient, search engines use many string algorithms. Moreover, the emerging field of personalized medicine uses many search algorithms to find disease-causing mutations in the human genome.
Course 5 - Advanced Algorithms and Complexity
You've learned the basic algorithms now and are ready to step into the area of more complex problems and algorithms to solve them. Advanced algorithms build upon basic ones and use new ideas. We will start with networks flows which are used in more typical applications such as optimal matchings, finding disjoint paths and flight scheduling as well as more surprising ones like image segmentation in computer vision. We then proceed to linear programming with applications in optimizing budget allocation, portfolio optimization, finding the cheapest diet satisfying all requirements and many others. Next we discuss inherently hard problems for which no exact good solutions are known (and not likely to be found) and how to solve them in practice. We finish with a soft introduction to streaming algorithms that are heavily used in Big Data processing. Such algorithms are usually designed to be able to process huge datasets without being able even to store a dataset.
Course 6 - Genome Assembly Programming Challenge
In Spring 2011, thousands of people in Germany were hospitalized with a deadly disease that started as food poisoning with bloody diarrhea and often led to kidney failure. It was the beginning of the deadliest outbreak in recent history, caused by a mysterious bacterial strain that we will refer to as E. coli X. Soon, German officials linked the outbreak to a restaurant in Lübeck, where nearly 20% of the patrons had developed bloody diarrhea in a single week. At this point, biologists knew that they were facing a previously unknown pathogen and that traditional methods would not suffice – computational biologists would be needed to assemble and analyze the genome of the newly emerged pathogen.
To investigate the evolutionary origin and pathogenic potential of the outbreak strain, researchers started a crowdsourced research program. They released bacterial DNA sequencing data from one of a patient, which elicited a burst of analyses carried out by computational biologists on four continents. They even used GitHub for the project: https://github.com/ehec-outbreak-crowdsourced/BGI-data-analysis/wiki
The 2011 German outbreak represented an early example of epidemiologists collaborating with computational biologists to stop an outbreak. In this Genome Assembly Programming Challenge, you will follow in the footsteps of the bioinformaticians investigating the outbreak by developing a program to assemble the genome of the E. coli X from millions of overlapping substrings of the E.coli X genome.
",courseradata structur and algorithm special thi special is a mix of theori and practic you will learn algorithm techniqu for solv variou comput problem and will implement about 100 algorithm code problem in a program languag of your choic no other onlin cours in algorithm even come close to offer you a wealth of program challeng that you may face at your next job interview to prepar you we invest over 3000 hour into design our challeng as an altern to multipl choic question that you usual find in mooc sorri we do not believ in multipl choic question when it come to learn algorithmsor anyth els in comput scienc for each algorithm you develop and implement we design multipl test to check it correct and run time you will have to debug your program without even know what these test are it may sound difficult but we believ it is the onli way to truli understand how the algorithm work and to master the art of program the special contain two realworld project big network and genom assembl you will analyz both road network and social network and will learn how to comput the shortest rout between new york and san francisco 1000 time faster than the standard shortest path algorithm afterward you will learn how to assembl genom from million of short fragment of dna and how assembl algorithm fuel recent develop in person medicin data structur and algorithm special skill gain algorithm data structur debug graph theori softwar test binari search tree comput program cours 1 algorithm toolbox the cours cover basic algorithm techniqu and idea for comput problem aris frequent in practic applic sort and search divid and conquer greedi algorithm dynam program we will learn a lot of theori how to sort data and how it help for search how to break a larg problem into piec and solv them recurs when it make sens to proceed greedili how dynam program is use in genom studi you will practic solv comput problem design new algorithm and implement solut effici so that they run in less than a second cours 2 data structur a good algorithm usual come togeth with a set of good data structur that allow the algorithm to manipul the data effici in thi cours we consid the common data structur that are use in variou comput problem you will learn how these data structur are implement in differ program languag and will practic implement them in our program assign thi will help you to understand what is go on insid a particular builtin implement of a data structur and what to expect from it you will also learn typic use case for these data structur a few exampl of question that we are go to cover in thi class are the follow what is a good strategi of resiz a dynam array how prioriti queue are implement in c java and python how to implement a hash tabl so that the amort run time of all oper is o1 on averag what are good strategi to keep a binari tree balanc cours 3 algorithm on graph if you have ever use a navig servic to find optim rout and estim time to destin youv use algorithm on graph graph aris in variou realworld situat as there are road network comput network and most recent social network if your look for the fastest time to get to work cheapest way to connect set of comput into a network or effici algorithm to automat find commun and opinion leader in facebook your go to work with graph and algorithm on graph in thi cours you will first learn what a graph is and what are some of the most import properti then youll learn sever way to travers graph and how you can do use thing while travers the graph in some order we will then talk about shortest path algorithm from the basic one to those which open door for 1000000 time faster algorithm use in googl map and other navig servic you will use these algorithm if you choos to work on our fast shortest rout industri capston project we will finish with minimum span tree which are use to plan road telephon and comput network and also find applic in cluster and approxim algorithm cours 4 algorithm on string world and internet is full of textual inform we search for inform use textual queri we read websit book email all those are string from the point of view of comput scienc to make sens of all that inform and make search effici search engin use mani string algorithm moreov the emerg field of person medicin use mani search algorithm to find diseasecaus mutat in the human genom cours 5 advanc algorithm and complex youv learn the basic algorithm now and are readi to step into the area of more complex problem and algorithm to solv them advanc algorithm build upon basic one and use new idea we will start with network flow which are use in more typic applic such as optim match find disjoint path and flight schedul as well as more surpris one like imag segment in comput vision we then proceed to linear program with applic in optim budget alloc portfolio optim find the cheapest diet satisfi all requir and mani other next we discuss inher hard problem for which no exact good solut are known and not like to be found and how to solv them in practic we finish with a soft introduct to stream algorithm that are heavili use in big data process such algorithm are usual design to be abl to process huge dataset without be abl even to store a dataset cours 6 genom assembl program challeng in spring 2011 thousand of peopl in germani were hospit with a deadli diseas that start as food poison with bloodi diarrhea and often led to kidney failur it wa the begin of the deadliest outbreak in recent histori caus by a mysteri bacteri strain that we will refer to as e coli x soon german offici link the outbreak to a restaur in lbeck where nearli 20 of the patron had develop bloodi diarrhea in a singl week at thi point biologist knew that they were face a previous unknown pathogen and that tradit method would not suffic comput biologist would be need to assembl and analyz the genom of the newli emerg pathogen to investig the evolutionari origin and pathogen potenti of the outbreak strain research start a crowdsourc research program they releas bacteri dna sequenc data from one of a patient which elicit a burst of analys carri out by comput biologist on four contin they even use github for the project httpsgithubcomehecoutbreakcrowdsourcedbgidataanalysiswiki the 2011 german outbreak repres an earli exampl of epidemiologist collabor with comput biologist to stop an outbreak in thi genom assembl program challeng you will follow in the footstep of the bioinformatician investig the outbreak by develop a program to assembl the genom of the e coli x from million of overlap substr of the ecoli x genom,courseradata structure and algorithm specialization this specialization is a mix of theory and practice you will learn algorithmic technique for solving various computational problem and will implement about 100 algorithmic coding problem in a programming language of your choice no other online course in algorithm even come close to offering you a wealth of programming challenge that you may face at your next job interview to prepare you we invested over 3000 hour into designing our challenge a an alternative to multiple choice question that you usually find in moocs sorry we do not believe in multiple choice question when it come to learning algorithmsor anything else in computer science for each algorithm you develop and implement we designed multiple test to check it correctness and running time you will have to debug your program without even knowing what these test are it may sound difficult but we believe it is the only way to truly understand how the algorithm work and to master the art of programming the specialization contains two realworld project big network and genome assembly you will analyze both road network and social network and will learn how to compute the shortest route between new york and san francisco 1000 time faster than the standard shortest path algorithm afterwards you will learn how to assemble genome from million of short fragment of dna and how assembly algorithm fuel recent development in personalized medicine data structure and algorithm specialization skill gained algorithm data structure debugging graph theory software testing binary search tree computer programming course 1 algorithmic toolbox the course cover basic algorithmic technique and idea for computational problem arising frequently in practical application sorting and searching divide and conquer greedy algorithm dynamic programming we will learn a lot of theory how to sort data and how it help for searching how to break a large problem into piece and solve them recursively when it make sense to proceed greedily how dynamic programming is used in genomic study you will practice solving computational problem designing new algorithm and implementing solution efficiently so that they run in le than a second course 2 data structure a good algorithm usually come together with a set of good data structure that allow the algorithm to manipulate the data efficiently in this course we consider the common data structure that are used in various computational problem you will learn how these data structure are implemented in different programming language and will practice implementing them in our programming assignment this will help you to understand what is going on inside a particular builtin implementation of a data structure and what to expect from it you will also learn typical use case for these data structure a few example of question that we are going to cover in this class are the following what is a good strategy of resizing a dynamic array how priority queue are implemented in c java and python how to implement a hash table so that the amortized running time of all operation is o1 on average what are good strategy to keep a binary tree balanced course 3 algorithm on graph if you have ever used a navigation service to find optimal route and estimate time to destination youve used algorithm on graph graph arise in various realworld situation a there are road network computer network and most recently social network if youre looking for the fastest time to get to work cheapest way to connect set of computer into a network or efficient algorithm to automatically find community and opinion leader in facebook youre going to work with graph and algorithm on graph in this course you will first learn what a graph is and what are some of the most important property then youll learn several way to traverse graph and how you can do useful thing while traversing the graph in some order we will then talk about shortest path algorithm from the basic one to those which open door for 1000000 time faster algorithm used in google map and other navigational service you will use these algorithm if you choose to work on our fast shortest route industrial capstone project we will finish with minimum spanning tree which are used to plan road telephone and computer network and also find application in clustering and approximate algorithm course 4 algorithm on string world and internet is full of textual information we search for information using textual query we read website book email all those are string from the point of view of computer science to make sense of all that information and make search efficient search engine use many string algorithm moreover the emerging field of personalized medicine us many search algorithm to find diseasecausing mutation in the human genome course 5 advanced algorithm and complexity youve learned the basic algorithm now and are ready to step into the area of more complex problem and algorithm to solve them advanced algorithm build upon basic one and use new idea we will start with network flow which are used in more typical application such a optimal matchings finding disjoint path and flight scheduling a well a more surprising one like image segmentation in computer vision we then proceed to linear programming with application in optimizing budget allocation portfolio optimization finding the cheapest diet satisfying all requirement and many others next we discus inherently hard problem for which no exact good solution are known and not likely to be found and how to solve them in practice we finish with a soft introduction to streaming algorithm that are heavily used in big data processing such algorithm are usually designed to be able to process huge datasets without being able even to store a dataset course 6 genome assembly programming challenge in spring 2011 thousand of people in germany were hospitalized with a deadly disease that started a food poisoning with bloody diarrhea and often led to kidney failure it wa the beginning of the deadliest outbreak in recent history caused by a mysterious bacterial strain that we will refer to a e coli x soon german official linked the outbreak to a restaurant in lbeck where nearly 20 of the patron had developed bloody diarrhea in a single week at this point biologist knew that they were facing a previously unknown pathogen and that traditional method would not suffice computational biologist would be needed to assemble and analyze the genome of the newly emerged pathogen to investigate the evolutionary origin and pathogenic potential of the outbreak strain researcher started a crowdsourced research program they released bacterial dna sequencing data from one of a patient which elicited a burst of analysis carried out by computational biologist on four continent they even used github for the project httpsgithubcomehecoutbreakcrowdsourcedbgidataanalysiswiki the 2011 german outbreak represented an early example of epidemiologist collaborating with computational biologist to stop an outbreak in this genome assembly programming challenge you will follow in the footstep of the bioinformaticians investigating the outbreak by developing a program to assemble the genome of the e coli x from million of overlapping substring of the ecoli x genome,courseradata structure algorithm specialization specialization mix theory practice learn algorithmic technique solving various computational problem implement 100 algorithmic coding problem programming language choice online course algorithm even come close offering wealth programming challenge may face next job interview prepare invested 3000 hour designing challenge alternative multiple choice question usually find moocs sorry believe multiple choice question come learning algorithmsor anything else computer science algorithm develop implement designed multiple test check correctness running time debug program without even knowing test may sound difficult believe way truly understand algorithm work master art programming specialization contains two realworld project big network genome assembly analyze road network social network learn compute shortest route new york san francisco 1000 time faster standard shortest path algorithm afterwards learn assemble genome million short fragment dna assembly algorithm fuel recent development personalized medicine data structure algorithm specialization skill gained algorithm data structure debugging graph theory software testing binary search tree computer programming course 1 algorithmic toolbox course cover basic algorithmic technique idea computational problem arising frequently practical application sorting searching divide conquer greedy algorithm dynamic programming learn lot theory sort data help searching break large problem piece solve recursively make sense proceed greedily dynamic programming used genomic study practice solving computational problem designing new algorithm implementing solution efficiently run le second course 2 data structure good algorithm usually come together set good data structure allow algorithm manipulate data efficiently course consider common data structure used various computational problem learn data structure implemented different programming language practice implementing programming assignment help understand going inside particular builtin implementation data structure expect also learn typical use case data structure example question going cover class following good strategy resizing dynamic array priority queue implemented c java python implement hash table amortized running time operation o1 average good strategy keep binary tree balanced course 3 algorithm graph ever used navigation service find optimal route estimate time destination youve used algorithm graph graph arise various realworld situation road network computer network recently social network youre looking fastest time get work cheapest way connect set computer network efficient algorithm automatically find community opinion leader facebook youre going work graph algorithm graph course first learn graph important property youll learn several way traverse graph useful thing traversing graph order talk shortest path algorithm basic one open door 1000000 time faster algorithm used google map navigational service use algorithm choose work fast shortest route industrial capstone project finish minimum spanning tree used plan road telephone computer network also find application clustering approximate algorithm course 4 algorithm string world internet full textual information search information using textual query read website book email string point view computer science make sense information make search efficient search engine use many string algorithm moreover emerging field personalized medicine us many search algorithm find diseasecausing mutation human genome course 5 advanced algorithm complexity youve learned basic algorithm ready step area complex problem algorithm solve advanced algorithm build upon basic one use new idea start network flow used typical application optimal matchings finding disjoint path flight scheduling well surprising one like image segmentation computer vision proceed linear programming application optimizing budget allocation portfolio optimization finding cheapest diet satisfying requirement many others next discus inherently hard problem exact good solution known likely found solve practice finish soft introduction streaming algorithm heavily used big data processing algorithm usually designed able process huge datasets without able even store dataset course 6 genome assembly programming challenge spring 2011 thousand people germany hospitalized deadly disease started food poisoning bloody diarrhea often led kidney failure beginning deadliest outbreak recent history caused mysterious bacterial strain refer e coli x soon german official linked outbreak restaurant lbeck nearly 20 patron developed bloody diarrhea single week point biologist knew facing previously unknown pathogen traditional method would suffice computational biologist would needed assemble analyze genome newly emerged pathogen investigate evolutionary origin pathogenic potential outbreak strain researcher started crowdsourced research program released bacterial dna sequencing data one patient elicited burst analysis carried computational biologist four continent even used github project httpsgithubcomehecoutbreakcrowdsourcedbgidataanalysiswiki 2011 german outbreak represented early example epidemiologist collaborating computational biologist stop outbreak genome assembly programming challenge follow footstep bioinformaticians investigating outbreak developing program assemble genome e coli x million overlapping substring ecoli x genome
C++ ,"NewQuant
NewQuant is a C++ library for data analysis and financial engineering computation. It is in building now, not a completed version.
01.ExceptionClass is finished，it is a self-defined exception class.
02.MathematicsExpression is finished，it helps users to build numerical functors in a convenient way.
03.MatrixComputation，the most important module of NewQuant，is finished mostly，it includes kinds of matrices and kinds of linear-equations solvers. Users can use it to do basic matrix computation，to solve linear-equation，to do matrix decomposition(such as LU decomposition)，to solve least square problem.
04.MonteCarlo，the majority part is finished，it includes kinds of SDEsolvers，users can use it to simulate kinds of SDEs，for example GBM. This module is very useful for financial engineering.
05.Regression is in building now，it is the base of econometrics.
06.SpecialFunction is in building now，it is the base of StatisticsComputation module.
07.StatisticsComputation，is finished partly，it includes kinds of computation about pdf，cdf and quantile now，it is also the base of econometrics.
NewQuant is released under BSD license.
",newquant newquant is a c librari for data analysi and financi engin comput it is in build now not a complet version 01exceptionclass is finishedit is a selfdefin except class 02mathematicsexpress is finishedit help user to build numer functor in a conveni way 03matrixcomputationth most import modul of newquanti finish mostlyit includ kind of matric and kind of linearequ solver user can use it to do basic matrix computationto solv linearequationto do matrix decompositionsuch as lu decompositionto solv least squar problem 04montecarloth major part is finishedit includ kind of sdesolversus can use it to simul kind of sdesfor exampl gbm thi modul is veri use for financi engin 05regress is in build nowit is the base of econometr 06specialfunct is in build nowit is the base of statisticscomput modul 07statisticscomputationi finish partlyit includ kind of comput about pdfcdf and quantil nowit is also the base of econometr newquant is releas under bsd licens,newquant newquant is a c library for data analysis and financial engineering computation it is in building now not a completed version 01exceptionclass is finishedit is a selfdefined exception class 02mathematicsexpression is finishedit help user to build numerical functors in a convenient way 03matrixcomputationthe most important module of newquantis finished mostlyit includes kind of matrix and kind of linearequations solver user can use it to do basic matrix computationto solve linearequationto do matrix decompositionsuch a lu decompositionto solve least square problem 04montecarlothe majority part is finishedit includes kind of sdesolversusers can use it to simulate kind of sdesfor example gbm this module is very useful for financial engineering 05regression is in building nowit is the base of econometrics 06specialfunction is in building nowit is the base of statisticscomputation module 07statisticscomputationis finished partlyit includes kind of computation about pdfcdf and quantile nowit is also the base of econometrics newquant is released under bsd license,newquant newquant c library data analysis financial engineering computation building completed version 01exceptionclass finishedit selfdefined exception class 02mathematicsexpression finishedit help user build numerical functors convenient way 03matrixcomputationthe important module newquantis finished mostlyit includes kind matrix kind linearequations solver user use basic matrix computationto solve linearequationto matrix decompositionsuch lu decompositionto solve least square problem 04montecarlothe majority part finishedit includes kind sdesolversusers use simulate kind sdesfor example gbm module useful financial engineering 05regression building nowit base econometrics 06specialfunction building nowit base statisticscomputation module 07statisticscomputationis finished partlyit includes kind computation pdfcdf quantile nowit also base econometrics newquant released bsd license
C++ ,"FEAT


FEAT is a feature engineering automation tool that learns new representations of raw data
to improve classifier and regressor performance. The underlying methods use Pareto
optimization and evolutionary computation to search the space of possible transformations.
FEAT wraps around a user-chosen ML method and provides a set of representations that give the best
performance for that method. Each individual in FEAT's population is its own data representation.
FEAT uses the Shogun C++ ML toolbox to fit models.
Check out the documentation for installation and examples.
Cite
La Cava, W., Singh, T. R., Taggart, J., Suri, S., & Moore, J. H.. Learning concise representations for regression by evolving networks of trees. ICLR 2019. arxiv:1807.0091
Bibtex:
@inproceedings{la_cava_learning_2019,
    series = {{ICLR}},
    title = {Learning concise representations for regression by evolving networks of trees},
    url = {https://arxiv.org/abs/1807.00981},
    language = {en},
    booktitle = {International {Conference} on {Learning} {Representations}},
    author = {La Cava, William and Singh, Tilak Raj and Taggart, James and Suri, Srinivas and Moore, Jason H.},
    year = {2019},
}

Contact
Maintained by William La Cava (lacava at upenn.edu)
Acknowledgments
This work is supported by grant K99-LM012926 from the National Library of Medicine.
FEAT is being developed to study human disease in the Epistasis Lab
at UPenn.
License
GNU GPLv3, see LICENSE
",feat feat is a featur engin autom tool that learn new represent of raw data to improv classifi and regressor perform the underli method use pareto optim and evolutionari comput to search the space of possibl transform feat wrap around a userchosen ml method and provid a set of represent that give the best perform for that method each individu in feat popul is it own data represent feat use the shogun c ml toolbox to fit model check out the document for instal and exampl cite la cava w singh t r taggart j suri s moor j h learn concis represent for regress by evolv network of tree iclr 2019 arxiv18070091 bibtex inproceedingsla_cava_learning_2019 seri iclr titl learn concis represent for regress by evolv network of tree url httpsarxivorgabs180700981 languag en booktitl intern confer on learn represent author la cava william and singh tilak raj and taggart jame and suri sriniva and moor jason h year 2019 contact maintain by william la cava lacava at upennedu acknowledg thi work is support by grant k99lm012926 from the nation librari of medicin feat is be develop to studi human diseas in the epistasi lab at upenn licens gnu gplv3 see licens,feat feat is a feature engineering automation tool that learns new representation of raw data to improve classifier and regressor performance the underlying method use pareto optimization and evolutionary computation to search the space of possible transformation feat wrap around a userchosen ml method and provides a set of representation that give the best performance for that method each individual in feat population is it own data representation feat us the shogun c ml toolbox to fit model check out the documentation for installation and example cite la cava w singh t r taggart j suri s moore j h learning concise representation for regression by evolving network of tree iclr 2019 arxiv18070091 bibtex inproceedingsla_cava_learning_2019 series iclr title learning concise representation for regression by evolving network of tree url httpsarxivorgabs180700981 language en booktitle international conference on learning representation author la cava william and singh tilak raj and taggart james and suri srinivas and moore jason h year 2019 contact maintained by william la cava lacava at upennedu acknowledgment this work is supported by grant k99lm012926 from the national library of medicine feat is being developed to study human disease in the epistasis lab at upenn license gnu gplv3 see license,feat feat feature engineering automation tool learns new representation raw data improve classifier regressor performance underlying method use pareto optimization evolutionary computation search space possible transformation feat wrap around userchosen ml method provides set representation give best performance method individual feat population data representation feat us shogun c ml toolbox fit model check documentation installation example cite la cava w singh r taggart j suri moore j h learning concise representation regression evolving network tree iclr 2019 arxiv18070091 bibtex inproceedingsla_cava_learning_2019 series iclr title learning concise representation regression evolving network tree url httpsarxivorgabs180700981 language en booktitle international conference learning representation author la cava william singh tilak raj taggart james suri srinivas moore jason h year 2019 contact maintained william la cava lacava upennedu acknowledgment work supported grant k99lm012926 national library medicine feat developed study human disease epistasis lab upenn license gnu gplv3 see license
C++ ,"FEAT


FEAT is a feature engineering automation tool that learns new representations of raw data
to improve classifier and regressor performance. The underlying methods use Pareto
optimization and evolutionary computation to search the space of possible transformations.
FEAT wraps around a user-chosen ML method and provides a set of representations that give the best
performance for that method. Each individual in FEAT's population is its own data representation.
FEAT uses the Shogun C++ ML toolbox to fit models.
Check out the documentation for installation and examples.
Cite
La Cava, W., Singh, T. R., Taggart, J., Suri, S., & Moore, J. H.. Learning concise representations for regression by evolving networks of trees. ICLR 2019. arxiv:1807.0091
Bibtex:
@inproceedings{la_cava_learning_2019,
    series = {{ICLR}},
    title = {Learning concise representations for regression by evolving networks of trees},
    url = {https://arxiv.org/abs/1807.00981},
    language = {en},
    booktitle = {International {Conference} on {Learning} {Representations}},
    author = {La Cava, William and Singh, Tilak Raj and Taggart, James and Suri, Srinivas and Moore, Jason H.},
    year = {2019},
}

Contact
Maintained by William La Cava (lacava at upenn.edu)
Acknowledgments
This work is supported by grant K99-LM012926 from the National Library of Medicine.
FEAT is being developed to study human disease in the Epistasis Lab
at UPenn.
License
GNU GPLv3, see LICENSE
",feat feat is a featur engin autom tool that learn new represent of raw data to improv classifi and regressor perform the underli method use pareto optim and evolutionari comput to search the space of possibl transform feat wrap around a userchosen ml method and provid a set of represent that give the best perform for that method each individu in feat popul is it own data represent feat use the shogun c ml toolbox to fit model check out the document for instal and exampl cite la cava w singh t r taggart j suri s moor j h learn concis represent for regress by evolv network of tree iclr 2019 arxiv18070091 bibtex inproceedingsla_cava_learning_2019 seri iclr titl learn concis represent for regress by evolv network of tree url httpsarxivorgabs180700981 languag en booktitl intern confer on learn represent author la cava william and singh tilak raj and taggart jame and suri sriniva and moor jason h year 2019 contact maintain by william la cava lacava at upennedu acknowledg thi work is support by grant k99lm012926 from the nation librari of medicin feat is be develop to studi human diseas in the epistasi lab at upenn licens gnu gplv3 see licens,feat feat is a feature engineering automation tool that learns new representation of raw data to improve classifier and regressor performance the underlying method use pareto optimization and evolutionary computation to search the space of possible transformation feat wrap around a userchosen ml method and provides a set of representation that give the best performance for that method each individual in feat population is it own data representation feat us the shogun c ml toolbox to fit model check out the documentation for installation and example cite la cava w singh t r taggart j suri s moore j h learning concise representation for regression by evolving network of tree iclr 2019 arxiv18070091 bibtex inproceedingsla_cava_learning_2019 series iclr title learning concise representation for regression by evolving network of tree url httpsarxivorgabs180700981 language en booktitle international conference on learning representation author la cava william and singh tilak raj and taggart james and suri srinivas and moore jason h year 2019 contact maintained by william la cava lacava at upennedu acknowledgment this work is supported by grant k99lm012926 from the national library of medicine feat is being developed to study human disease in the epistasis lab at upenn license gnu gplv3 see license,feat feat feature engineering automation tool learns new representation raw data improve classifier regressor performance underlying method use pareto optimization evolutionary computation search space possible transformation feat wrap around userchosen ml method provides set representation give best performance method individual feat population data representation feat us shogun c ml toolbox fit model check documentation installation example cite la cava w singh r taggart j suri moore j h learning concise representation regression evolving network tree iclr 2019 arxiv18070091 bibtex inproceedingsla_cava_learning_2019 series iclr title learning concise representation regression evolving network tree url httpsarxivorgabs180700981 language en booktitle international conference learning representation author la cava william singh tilak raj taggart james suri srinivas moore jason h year 2019 contact maintained william la cava lacava upennedu acknowledgment work supported grant k99lm012926 national library medicine feat developed study human disease epistasis lab upenn license gnu gplv3 see license
C++ ,"data-structures-and-algorithms
This project contains learning materials for the Data Structures and Algorithms course for students from Software Engineering program at Sofia University, Faculty of Mathematics and informatics
",datastructuresandalgorithm thi project contain learn materi for the data structur and algorithm cours for student from softwar engin program at sofia univers faculti of mathemat and informat,datastructuresandalgorithms this project contains learning material for the data structure and algorithm course for student from software engineering program at sofia university faculty of mathematics and informatics,datastructuresandalgorithms project contains learning material data structure algorithm course student software engineering program sofia university faculty mathematics informatics
C++ ,"TH10_DataReversing
Using binary reverse engineering techniques to extract the memory data of the Touhou 10th game, Mountain of Faith.
These are some AI projects about MoF:
twinject by Netdex
TH10AI by DREAMWORLDVOID
touhou10-dqn by actumn
",th10_datarevers use binari revers engin techniqu to extract the memori data of the touhou 10th game mountain of faith these are some ai project about mof twinject by netdex th10ai by dreamworldvoid touhou10dqn by actumn,th10_datareversing using binary reverse engineering technique to extract the memory data of the touhou 10th game mountain of faith these are some ai project about mof twinject by netdex th10ai by dreamworldvoid touhou10dqn by actumn,th10_datareversing using binary reverse engineering technique extract memory data touhou 10th game mountain faith ai project mof twinject netdex th10ai dreamworldvoid touhou10dqn actumn
C++ ,"game-data-reverse-engineering
Reverse engineering resources for data files from various video games I like
",gamedatareverseengin revers engin resourc for data file from variou video game i like,gamedatareverseengineering reverse engineering resource for data file from various video game i like,gamedatareverseengineering reverse engineering resource data file various video game like
C++ ,"Engineering
My academic adventures.
This repository contains all my projects during my graduation.
",engin my academ adventur thi repositori contain all my project dure my graduat,engineering my academic adventure this repository contains all my project during my graduation,engineering academic adventure repository contains project graduation
C++ ,"libSPRITE
#Installation
To install, type make install as root.
This will copy headers files to /usr/local/include/SPRITE/ and static library
to /usr/local/lib/SPRITE by default.
To uninstall, type make uninstall as root.
Lua Paths
libSPRITE assumes the Lua headers are install in /usr/local/include. Some
distributions place it elsewhere. You can change where the Makefile looks for
the Lua include files by specifying the LUA_INCLUDE variable in the arguments
to make.
ex.: make LUA_INCLUDE=/usr/include/lua5.2
You can also change the Lua library path by setting the LUA_LIB variable.
Testing
'make test' will build the unit tests for this package. You must have cppunit
installed to compile and run the unit tests. After compiling, run './run_test'
to execute the unit tests. You will have to run as root to execute all test cases.
NOTE: You must start from a clean systems ('make clean') before running 'make test'. Otherwise, some tests that depend on compile time assertions will fail.
Makefile Overrides
By default, libSPRITE sends output to stdout and stderr for info, warnings, and errors. To supress these messages, you can specifiy -DNO_PRINT_INFO -DNO_PRINT_WARNING -DNO_PRINT_ERROR. The best way to do that is by appending to these options to the USER_CFLAGS. For example:
make USER_CFLAGS='-DNO_PRINT_INFO -DNO_PRINT_WARNING'

There are also overides for CPPFLAGS (USER_CPPFLAGS) and LDFLAGS (USER_LDFLAGS).
Using CMake
To build with cmake, create a directory called build, cd to the build directory and type cmake ../.
To specify the build prefix, using the CMAKE_INSTALL_PREFIX macro. Example
cmake -DCMAKE_INSTALL_PREFIX=/usr/local/

By default, the build type is Release. To build unit tests us the CMAKE_BUILD_TYPE macro. Example:
cmake -DCMAKE_BUILD_TYPE=Test

After specifying cmake with this macro, make will create a run_test executable in each folder. Execute the run_test executable for the folder you wish to test.
Run cmake with the -DCMAKE_BUILD_TYPE=Release option to switch back to the normal build.
Documentation
Documentation can be found on the Wiki
Tutorial
A growing Tutorial for developing applications using libSPRITE can be found here
",libsprit instal to instal type make instal as root thi will copi header file to usrlocalincludesprit and static librari to usrlocallibsprit by default to uninstal type make uninstal as root lua path libsprit assum the lua header are instal in usrlocalinclud some distribut place it elsewher you can chang where the makefil look for the lua includ file by specifi the lua_includ variabl in the argument to make ex make lua_includeusrincludelua52 you can also chang the lua librari path by set the lua_lib variabl test make test will build the unit test for thi packag you must have cppunit instal to compil and run the unit test after compil run run_test to execut the unit test you will have to run as root to execut all test case note you must start from a clean system make clean befor run make test otherwis some test that depend on compil time assert will fail makefil overrid by default libsprit send output to stdout and stderr for info warn and error to supress these messag you can specifiy dno_print_info dno_print_warn dno_print_error the best way to do that is by append to these option to the user_cflag for exampl make user_cflagsdno_print_info dno_print_warn there are also overid for cppflag user_cppflag and ldflag user_ldflag use cmake to build with cmake creat a directori call build cd to the build directori and type cmake to specifi the build prefix use the cmake_install_prefix macro exampl cmake dcmake_install_prefixusrloc by default the build type is releas to build unit test us the cmake_build_typ macro exampl cmake dcmake_build_typetest after specifi cmake with thi macro make will creat a run_test execut in each folder execut the run_test execut for the folder you wish to test run cmake with the dcmake_build_typereleas option to switch back to the normal build document document can be found on the wiki tutori a grow tutori for develop applic use libsprit can be found here,libsprite installation to install type make install a root this will copy header file to usrlocalincludesprite and static library to usrlocallibsprite by default to uninstall type make uninstall a root lua path libsprite assumes the lua header are install in usrlocalinclude some distribution place it elsewhere you can change where the makefile look for the lua include file by specifying the lua_include variable in the argument to make ex make lua_includeusrincludelua52 you can also change the lua library path by setting the lua_lib variable testing make test will build the unit test for this package you must have cppunit installed to compile and run the unit test after compiling run run_test to execute the unit test you will have to run a root to execute all test case note you must start from a clean system make clean before running make test otherwise some test that depend on compile time assertion will fail makefile override by default libsprite sends output to stdout and stderr for info warning and error to supress these message you can specifiy dno_print_info dno_print_warning dno_print_error the best way to do that is by appending to these option to the user_cflags for example make user_cflagsdno_print_info dno_print_warning there are also overides for cppflags user_cppflags and ldflags user_ldflags using cmake to build with cmake create a directory called build cd to the build directory and type cmake to specify the build prefix using the cmake_install_prefix macro example cmake dcmake_install_prefixusrlocal by default the build type is release to build unit test u the cmake_build_type macro example cmake dcmake_build_typetest after specifying cmake with this macro make will create a run_test executable in each folder execute the run_test executable for the folder you wish to test run cmake with the dcmake_build_typerelease option to switch back to the normal build documentation documentation can be found on the wiki tutorial a growing tutorial for developing application using libsprite can be found here,libsprite installation install type make install root copy header file usrlocalincludesprite static library usrlocallibsprite default uninstall type make uninstall root lua path libsprite assumes lua header install usrlocalinclude distribution place elsewhere change makefile look lua include file specifying lua_include variable argument make ex make lua_includeusrincludelua52 also change lua library path setting lua_lib variable testing make test build unit test package must cppunit installed compile run unit test compiling run run_test execute unit test run root execute test case note must start clean system make clean running make test otherwise test depend compile time assertion fail makefile override default libsprite sends output stdout stderr info warning error supress message specifiy dno_print_info dno_print_warning dno_print_error best way appending option user_cflags example make user_cflagsdno_print_info dno_print_warning also overides cppflags user_cppflags ldflags user_ldflags using cmake build cmake create directory called build cd build directory type cmake specify build prefix using cmake_install_prefix macro example cmake dcmake_install_prefixusrlocal default build type release build unit test u cmake_build_type macro example cmake dcmake_build_typetest specifying cmake macro make create run_test executable folder execute run_test executable folder wish test run cmake dcmake_build_typerelease option switch back normal build documentation documentation found wiki tutorial growing tutorial developing application using libsprite found
C++ ,"Welcome to GitHub Pages
You can use the editor on GitHub to maintain and preview the content for your website in Markdown files.
Whenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.
Markdown
Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
For more details see GitHub Flavored Markdown.
Jekyll Themes
Your Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.
Support or Contact
Having trouble with Pages? Check out our documentation or contact support and we’ll help you sort it out.
",welcom to github page you can use the editor on github to maintain and preview the content for your websit in markdown file whenev you commit to thi repositori github page will run jekyl to rebuild the page in your site from the content in your markdown file markdown markdown is a lightweight and easytous syntax for style your write it includ convent for syntax highlight code block header 1 header 2 header 3 bullet list 1 number 2 list bold and _italic_ and code text linkurl and imagesrc for more detail see github flavor markdown jekyl theme your page site will use the layout and style from the jekyl theme you have select in your repositori set the name of thi theme is save in the jekyl _configyml configur file support or contact have troubl with page check out our document or contact support and well help you sort it out,welcome to github page you can use the editor on github to maintain and preview the content for your website in markdown file whenever you commit to this repository github page will run jekyll to rebuild the page in your site from the content in your markdown file markdown markdown is a lightweight and easytouse syntax for styling your writing it includes convention for syntax highlighted code block header 1 header 2 header 3 bulleted list 1 numbered 2 list bold and _italic_ and code text linkurl and imagesrc for more detail see github flavored markdown jekyll theme your page site will use the layout and style from the jekyll theme you have selected in your repository setting the name of this theme is saved in the jekyll _configyml configuration file support or contact having trouble with page check out our documentation or contact support and well help you sort it out,welcome github page use editor github maintain preview content website markdown file whenever commit repository github page run jekyll rebuild page site content markdown file markdown markdown lightweight easytouse syntax styling writing includes convention syntax highlighted code block header 1 header 2 header 3 bulleted list 1 numbered 2 list bold _italic_ code text linkurl imagesrc detail see github flavored markdown jekyll theme page site use layout style jekyll theme selected repository setting name theme saved jekyll _configyml configuration file support contact trouble page check documentation contact support well help sort
C++ ,"Artificial intelligence library
My C++ deep learning framework (with GPU support) & other machine learning algorithms implementations
DeepLearning Operations

Convolution
Dropout
Softmax
Recurrent
Linear
Sigmoid, Tanh, Relu, Selu activations
Layer normalization
Addition
Concatenation
Maxpooling
Averagepooling
ResidualBlock
Autoencoder
L1 e L2 regularizations
Gradient clipping

Neural network optimizers

Stochastic gradient descent with minibatch and momentum
Direct Feedback Alignment

Deep Reinforcement Learning agents

Deep Qlearning Agents

Deep Reinforcement Learning environments

Kbandits
TicTacToe

Optimization algorithms

Genetic algorithms (with multicore features for high performance)
Particle Swarm Optimization

Other machine learning algorithms

linear regression
logistic regression
genetic programming

Data Mining

K-means clustering

Visualization

Bitmap class for loading, saving and processing multiple image formats
Visualizaiton namespace for fast data visualization

License
The MIT License (MIT)
Copyright (c) 2015 Carlo Meroni
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
",artifici intellig librari my c deep learn framework with gpu support other machin learn algorithm implement deeplearn oper convolut dropout softmax recurr linear sigmoid tanh relu selu activ layer normal addit concaten maxpool averagepool residualblock autoencod l1 e l2 regular gradient clip neural network optim stochast gradient descent with minibatch and momentum direct feedback align deep reinforc learn agent deep qlearn agent deep reinforc learn environ kbandit tictacto optim algorithm genet algorithm with multicor featur for high perform particl swarm optim other machin learn algorithm linear regress logist regress genet program data mine kmean cluster visual bitmap class for load save and process multipl imag format visualizaiton namespac for fast data visual licens the mit licens mit copyright c 2015 carlo meroni permiss is herebi grant free of charg to ani person obtain a copi of thi softwar and associ document file the softwar to deal in the softwar without restrict includ without limit the right to use copi modifi merg publish distribut sublicens andor sell copi of the softwar and to permit person to whom the softwar is furnish to do so subject to the follow condit the abov copyright notic and thi permiss notic shall be includ in all copi or substanti portion of the softwar the softwar is provid as is without warranti of ani kind express or impli includ but not limit to the warranti of merchant fit for a particular purpos and noninfring in no event shall the author or copyright holder be liabl for ani claim damag or other liabil whether in an action of contract tort or otherwis aris from out of or in connect with the softwar or the use or other deal in the softwar,artificial intelligence library my c deep learning framework with gpu support other machine learning algorithm implementation deeplearning operation convolution dropout softmax recurrent linear sigmoid tanh relu selu activation layer normalization addition concatenation maxpooling averagepooling residualblock autoencoder l1 e l2 regularization gradient clipping neural network optimizers stochastic gradient descent with minibatch and momentum direct feedback alignment deep reinforcement learning agent deep qlearning agent deep reinforcement learning environment kbandits tictactoe optimization algorithm genetic algorithm with multicore feature for high performance particle swarm optimization other machine learning algorithm linear regression logistic regression genetic programming data mining kmeans clustering visualization bitmap class for loading saving and processing multiple image format visualizaiton namespace for fast data visualization license the mit license mit copyright c 2015 carlo meroni permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation file the software to deal in the software without restriction including without limitation the right to use copy modify merge publish distribute sublicense andor sell copy of the software and to permit person to whom the software is furnished to do so subject to the following condition the above copyright notice and this permission notice shall be included in all copy or substantial portion of the software the software is provided a is without warranty of any kind express or implied including but not limited to the warranty of merchantability fitness for a particular purpose and noninfringement in no event shall the author or copyright holder be liable for any claim damage or other liability whether in an action of contract tort or otherwise arising from out of or in connection with the software or the use or other dealing in the software,artificial intelligence library c deep learning framework gpu support machine learning algorithm implementation deeplearning operation convolution dropout softmax recurrent linear sigmoid tanh relu selu activation layer normalization addition concatenation maxpooling averagepooling residualblock autoencoder l1 e l2 regularization gradient clipping neural network optimizers stochastic gradient descent minibatch momentum direct feedback alignment deep reinforcement learning agent deep qlearning agent deep reinforcement learning environment kbandits tictactoe optimization algorithm genetic algorithm multicore feature high performance particle swarm optimization machine learning algorithm linear regression logistic regression genetic programming data mining kmeans clustering visualization bitmap class loading saving processing multiple image format visualizaiton namespace fast data visualization license mit license mit copyright c 2015 carlo meroni permission hereby granted free charge person obtaining copy software associated documentation file software deal software without restriction including without limitation right use copy modify merge publish distribute sublicense andor sell copy software permit person software furnished subject following condition copyright notice permission notice shall included copy substantial portion software software provided without warranty kind express implied including limited warranty merchantability fitness particular purpose noninfringement event shall author copyright holder liable claim damage liability whether action contract tort otherwise arising connection software use dealing software
C++ ,"About This Repository
This repository should now be considered a historical curiosity only.
The original version of this code was developed between 2002-2004 and included free with the book ""Artificial Intelligence for Games"". Over the intervening years, the code has become a less useful reference. The third edition of the textbook is considerably larger, and this code did not keep up with errata or improvements in the algorithms. I have not use the code in this repository for my consulting work in over a decade.
The third edition of the textbook does not mention this code, but includes considerably expanded and corrected listings in the text in a language neutral format. I would recommend that version.

Historical Information
The Artificial Intelligence for Games system.
Copyright (c) Ian Millington 2003-2009. All Rights Reserved.
This software is distributed under licence. Use of this software
implies agreement with all terms and conditions of the accompanying
software licence.
This code also contains portions of the AI Core engine.
Copyright (c) Icosagon Limited 2003-2007. All Rights Reserved.
Please see accompanying LICENSE file.
Installation
The code can be extracted to any directory.
Platform Compatibility
The software has been designed for platform indepedence as much as
possible. The only file that may need altering for your platform is
./src/timing.cpp which currently wraps the windows multimedia timer.
Building
Building with Scons
The code can be built using SCONS, available from
http://www.scons.org/. Simply cd (change directory) into the build
directory and type scons.

cd build
scons

To remove intermediate files after building (but leaving the library
and demos):

scons -c ../src

Building with Microsoft Visual Studio 8 Professional
Solution and project files are included for use with Microsoft visual
studio 8 professional. They may also work with Express edition, but
I've not tested that.
Building with CMake on Linux
The code can be built on Linux (and possibly on other platforms)
using CMake:
cd build
cmake .
make
Documentation
To build the documentation (see below) you must have doxygen
installed (it is available from http://www.stack.nl/~dimitri/doxygen/
Simply cd into the ./doc/build/doxygen directory, then type:

doxygen aicore.config

to build the documentation.
Layout
The build process creates a statically linked library in ./lib which
can be used with the include headers in ./include. The demo programs
are built and placed in the ./bin directory.
Source code is contained in the ./src directory, and documentation
is in the ./doc directory, in particular the reference documentation
is in the ./doc/ref directory.
Documentation
The source code is heavily documented, and the contents correspond to
the discussion in the ""Artificial Intelligence for Games"" book.
It is possible to create 'doxygen' documentation with the tags in the
source code files, and a configuration for building the documentation
is provided in the ./doc/build/doxygen directory. The doxygen
configuration supplied provides only html output, since other output
formats depend on how your machine is configured.
This is not currently targeted from the scons configuration, because
scons suport for doxygen depends on where doxygen is installed on your
machine.
Demos
To run the demos you will require OpenGL and GLUT installed on your
machine, and the relevant DLLs or shared objects on the path.
",about thi repositori thi repositori should now be consid a histor curios onli the origin version of thi code wa develop between 20022004 and includ free with the book artifici intellig for game over the interven year the code ha becom a less use refer the third edit of the textbook is consider larger and thi code did not keep up with errata or improv in the algorithm i have not use the code in thi repositori for my consult work in over a decad the third edit of the textbook doe not mention thi code but includ consider expand and correct list in the text in a languag neutral format i would recommend that version histor inform the artifici intellig for game system copyright c ian millington 20032009 all right reserv thi softwar is distribut under licenc use of thi softwar impli agreement with all term and condit of the accompani softwar licenc thi code also contain portion of the ai core engin copyright c icosagon limit 20032007 all right reserv pleas see accompani licens file instal the code can be extract to ani directori platform compat the softwar ha been design for platform indeped as much as possibl the onli file that may need alter for your platform is srctimingcpp which current wrap the window multimedia timer build build with scon the code can be built use scon avail from httpwwwsconsorg simpli cd chang directori into the build directori and type scon cd build scon to remov intermedi file after build but leav the librari and demo scon c src build with microsoft visual studio 8 profession solut and project file are includ for use with microsoft visual studio 8 profession they may also work with express edit but ive not test that build with cmake on linux the code can be built on linux and possibl on other platform use cmake cd build cmake make document to build the document see below you must have doxygen instal it is avail from httpwwwstacknldimitridoxygen simpli cd into the docbuilddoxygen directori then type doxygen aicoreconfig to build the document layout the build process creat a static link librari in lib which can be use with the includ header in includ the demo program are built and place in the bin directori sourc code is contain in the src directori and document is in the doc directori in particular the refer document is in the docref directori document the sourc code is heavili document and the content correspond to the discuss in the artifici intellig for game book it is possibl to creat doxygen document with the tag in the sourc code file and a configur for build the document is provid in the docbuilddoxygen directori the doxygen configur suppli provid onli html output sinc other output format depend on how your machin is configur thi is not current target from the scon configur becaus scon suport for doxygen depend on where doxygen is instal on your machin demo to run the demo you will requir opengl and glut instal on your machin and the relev dll or share object on the path,about this repository this repository should now be considered a historical curiosity only the original version of this code wa developed between 20022004 and included free with the book artificial intelligence for game over the intervening year the code ha become a le useful reference the third edition of the textbook is considerably larger and this code did not keep up with erratum or improvement in the algorithm i have not use the code in this repository for my consulting work in over a decade the third edition of the textbook doe not mention this code but includes considerably expanded and corrected listing in the text in a language neutral format i would recommend that version historical information the artificial intelligence for game system copyright c ian millington 20032009 all right reserved this software is distributed under licence use of this software implies agreement with all term and condition of the accompanying software licence this code also contains portion of the ai core engine copyright c icosagon limited 20032007 all right reserved please see accompanying license file installation the code can be extracted to any directory platform compatibility the software ha been designed for platform indepedence a much a possible the only file that may need altering for your platform is srctimingcpp which currently wrap the window multimedia timer building building with scons the code can be built using scons available from httpwwwsconsorg simply cd change directory into the build directory and type scons cd build scons to remove intermediate file after building but leaving the library and demo scons c src building with microsoft visual studio 8 professional solution and project file are included for use with microsoft visual studio 8 professional they may also work with express edition but ive not tested that building with cmake on linux the code can be built on linux and possibly on other platform using cmake cd build cmake make documentation to build the documentation see below you must have doxygen installed it is available from httpwwwstacknldimitridoxygen simply cd into the docbuilddoxygen directory then type doxygen aicoreconfig to build the documentation layout the build process creates a statically linked library in lib which can be used with the include header in include the demo program are built and placed in the bin directory source code is contained in the src directory and documentation is in the doc directory in particular the reference documentation is in the docref directory documentation the source code is heavily documented and the content correspond to the discussion in the artificial intelligence for game book it is possible to create doxygen documentation with the tag in the source code file and a configuration for building the documentation is provided in the docbuilddoxygen directory the doxygen configuration supplied provides only html output since other output format depend on how your machine is configured this is not currently targeted from the scons configuration because scons suport for doxygen depends on where doxygen is installed on your machine demo to run the demo you will require opengl and glut installed on your machine and the relevant dlls or shared object on the path,repository repository considered historical curiosity original version code developed 20022004 included free book artificial intelligence game intervening year code become le useful reference third edition textbook considerably larger code keep erratum improvement algorithm use code repository consulting work decade third edition textbook mention code includes considerably expanded corrected listing text language neutral format would recommend version historical information artificial intelligence game system copyright c ian millington 20032009 right reserved software distributed licence use software implies agreement term condition accompanying software licence code also contains portion ai core engine copyright c icosagon limited 20032007 right reserved please see accompanying license file installation code extracted directory platform compatibility software designed platform indepedence much possible file may need altering platform srctimingcpp currently wrap window multimedia timer building building scons code built using scons available httpwwwsconsorg simply cd change directory build directory type scons cd build scons remove intermediate file building leaving library demo scons c src building microsoft visual studio 8 professional solution project file included use microsoft visual studio 8 professional may also work express edition ive tested building cmake linux code built linux possibly platform using cmake cd build cmake make documentation build documentation see must doxygen installed available httpwwwstacknldimitridoxygen simply cd docbuilddoxygen directory type doxygen aicoreconfig build documentation layout build process creates statically linked library lib used include header include demo program built placed bin directory source code contained src directory documentation doc directory particular reference documentation docref directory documentation source code heavily documented content correspond discussion artificial intelligence game book possible create doxygen documentation tag source code file configuration building documentation provided docbuilddoxygen directory doxygen configuration supplied provides html output since output format depend machine configured currently targeted scons configuration scons suport doxygen depends doxygen installed machine demo run demo require opengl glut installed machine relevant dlls shared object path
C++ ,"OpenNERO
   
OpenNERO is an open source software platform designed for
research and education in Artificial Intelligence. The project is based on the
Neuro-Evolving Robotic Operatives (NERO) game developed by graduate
and undergraduate students at the  Neural Networks Research Group and
Department of Computer Science at the
University of Texas at Austin.
In particular, OpenNERO has been used to implement several demos and exercises for Russell
and Norvig's textbook Artificial Intelligence: A Modern Approach.  These
demos and exercises illustrate AI methods such as brute-force search, heuristic search, scripting,
reinforcement learning, and evolutionary computation, and AI problems such as maze running,
vacuuming, and robotic battle. The methods and problems are implemented in several different
environments (or ""mods""), as described below.
More environments, problems, and methods, as well as demos and exercises illustrating them, will
be added in the future. The current ones are intended to serve as a starting point on which new
ones can be built, by us, but also by the community at large. If you have questions or would like to contribute, check out the OpenNERO Google Group.
Get Started

Get OpenNERO

OpenNERO Dependencies
Download a Binary

Linux
Windows
Mac OS X


Build from Source

Linux
Windows
Mac OS X




Demonstrations

Search

The Maze Environment
First Person Search
Uninformed Search
Heuristic Search


Planning

Tower of Hanoi Environment
Symbolic Planning


Natural Language Processing

Tower of Hanoi Environment
Natural Language Processing


Reinforcement Learning

The Maze Environment
Q-learning


Evolutionary Computation

The Roomba Environment
Neuroevolution


Multi-Agent Systems

NERO Environment
NERO Machine Learning Game


Vision

The Vision Environment
Vision: Edge Detection




Exercises

OpenNero Setup
Adding Stuff
Create Roomba Agent
Maze Generator
Maze Solver
AI Exercises
Heuristic Search
Planning
NLPExercise
QLearningExercise
Maze Learner
Advanced Maze
Nero Tournament
Sample Tournament Results
Object Recognition


System Documentation
Headless Mode
System Overview

Contributors
Many people have contributed to OpenNERO, including Igor V. Karpov, John B. Sheblak, Adam Dziuk, Minh Phan, Dan Lessin, Wes Tansey, Reza Mahjourian, Risto Miikkulainen, members of the Neural Networks Research Group at UT Austin, students and alumni of the Computational Intelligence and Game Design stream of the Freshman Research Initiative at UT Austin.

NOTE: as with any active project, OpenNERO is a work in progress and many updates are frequently being made. If you have trouble with OpenNERO, check the discussion group and then consider submitting an issue. And of course, if you would like to contribute, let us know!
",opennero opennero is an open sourc softwar platform design for research and educ in artifici intellig the project is base on the neuroevolv robot oper nero game develop by graduat and undergradu student at the neural network research group and depart of comput scienc at the univers of texa at austin in particular opennero ha been use to implement sever demo and exercis for russel and norvig textbook artifici intellig a modern approach these demo and exercis illustr ai method such as bruteforc search heurist search script reinforc learn and evolutionari comput and ai problem such as maze run vacuum and robot battl the method and problem are implement in sever differ environ or mod as describ below more environ problem and method as well as demo and exercis illustr them will be ad in the futur the current one are intend to serv as a start point on which new one can be built by us but also by the commun at larg if you have question or would like to contribut check out the opennero googl group get start get opennero opennero depend download a binari linux window mac os x build from sourc linux window mac os x demonstr search the maze environ first person search uninform search heurist search plan tower of hanoi environ symbol plan natur languag process tower of hanoi environ natur languag process reinforc learn the maze environ qlearn evolutionari comput the roomba environ neuroevolut multiag system nero environ nero machin learn game vision the vision environ vision edg detect exercis opennero setup ad stuff creat roomba agent maze gener maze solver ai exercis heurist search plan nlpexercis qlearningexercis maze learner advanc maze nero tournament sampl tournament result object recognit system document headless mode system overview contributor mani peopl have contribut to opennero includ igor v karpov john b sheblak adam dziuk minh phan dan lessin we tansey reza mahjourian risto miikkulainen member of the neural network research group at ut austin student and alumni of the comput intellig and game design stream of the freshman research initi at ut austin note as with ani activ project opennero is a work in progress and mani updat are frequent be made if you have troubl with opennero check the discuss group and then consid submit an issu and of cours if you would like to contribut let us know,opennero opennero is an open source software platform designed for research and education in artificial intelligence the project is based on the neuroevolving robotic operative nero game developed by graduate and undergraduate student at the neural network research group and department of computer science at the university of texas at austin in particular opennero ha been used to implement several demo and exercise for russell and norvigs textbook artificial intelligence a modern approach these demo and exercise illustrate ai method such a bruteforce search heuristic search scripting reinforcement learning and evolutionary computation and ai problem such a maze running vacuuming and robotic battle the method and problem are implemented in several different environment or mod a described below more environment problem and method a well a demo and exercise illustrating them will be added in the future the current one are intended to serve a a starting point on which new one can be built by u but also by the community at large if you have question or would like to contribute check out the opennero google group get started get opennero opennero dependency download a binary linux window mac o x build from source linux window mac o x demonstration search the maze environment first person search uninformed search heuristic search planning tower of hanoi environment symbolic planning natural language processing tower of hanoi environment natural language processing reinforcement learning the maze environment qlearning evolutionary computation the roomba environment neuroevolution multiagent system nero environment nero machine learning game vision the vision environment vision edge detection exercise opennero setup adding stuff create roomba agent maze generator maze solver ai exercise heuristic search planning nlpexercise qlearningexercise maze learner advanced maze nero tournament sample tournament result object recognition system documentation headless mode system overview contributor many people have contributed to opennero including igor v karpov john b sheblak adam dziuk minh phan dan lessin wes tansey reza mahjourian risto miikkulainen member of the neural network research group at ut austin student and alumnus of the computational intelligence and game design stream of the freshman research initiative at ut austin note a with any active project opennero is a work in progress and many update are frequently being made if you have trouble with opennero check the discussion group and then consider submitting an issue and of course if you would like to contribute let u know,opennero opennero open source software platform designed research education artificial intelligence project based neuroevolving robotic operative nero game developed graduate undergraduate student neural network research group department computer science university texas austin particular opennero used implement several demo exercise russell norvigs textbook artificial intelligence modern approach demo exercise illustrate ai method bruteforce search heuristic search scripting reinforcement learning evolutionary computation ai problem maze running vacuuming robotic battle method problem implemented several different environment mod described environment problem method well demo exercise illustrating added future current one intended serve starting point new one built u also community large question would like contribute check opennero google group get started get opennero opennero dependency download binary linux window mac o x build source linux window mac o x demonstration search maze environment first person search uninformed search heuristic search planning tower hanoi environment symbolic planning natural language processing tower hanoi environment natural language processing reinforcement learning maze environment qlearning evolutionary computation roomba environment neuroevolution multiagent system nero environment nero machine learning game vision vision environment vision edge detection exercise opennero setup adding stuff create roomba agent maze generator maze solver ai exercise heuristic search planning nlpexercise qlearningexercise maze learner advanced maze nero tournament sample tournament result object recognition system documentation headless mode system overview contributor many people contributed opennero including igor v karpov john b sheblak adam dziuk minh phan dan lessin wes tansey reza mahjourian risto miikkulainen member neural network research group ut austin student alumnus computational intelligence game design stream freshman research initiative ut austin note active project opennero work progress many update frequently made trouble opennero check discussion group consider submitting issue course would like contribute let u know
C++ ,"AIKIDO - AI for KIDO   

⚠️ Warning: AIKIDO is under heavy development. These instructions are
primarily for reference by the developers.

AIKIDO is a C++ library, complete with Python bindings, for solving robotic motion
planning and decision making problems. This library is tightly integrated with
DART for kinematic/dynamics calculations and OMPL for motion planning. AIKIDO
optionally integrates with ROS, through the suite of aikido_ros packages, for
execution on real robots.
Installation
On Ubuntu Trusty using apt-get
AIKIDO depends on ROS. You should install ROS by adding the ROS repository to your sources.list as follows. We encourage users to install indigo.
$ sudo sh -c 'echo ""deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main"" > /etc/apt/sources.list.d/ros-latest.list'
$ sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654
$ sudo apt-get update
$ sudo apt-get install ros-indigo-actionlib ros-indigo-geometry-msgs ros-indigo-interactive-markers ros-indigo-roscpp ros-indigo-std-msgs ros-indigo-tf ros-indigo-trajectory-msgs ros-indigo-visualization-msgs
Once ROS is installed, you can install AIKIDO from the Personal Robotics Lab PPA:
$ sudo add-apt-repository ppa:libccd-debs/ppa
$ sudo add-apt-repository ppa:fcl-debs/ppa
$ sudo add-apt-repository ppa:dartsim/ppa
$ sudo add-apt-repository ppa:personalrobotics/ppa
$ sudo apt-get update
$ sudo apt-get install libaikido-all-dev
On macOS using Homebrew
# Install the Homebrew package manager
$ /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""
# Add Homebrew tap for Personal Robotics Lab software
$ brew tap personalrobotics/tap
# Install AIKIDO
$ brew install aikido

Note: While ROS seems to be available on macOS, we haven't tested it with AIKIDO. For now, brew install aikido installs AIKIDO without the ROS-dependent components.

Building from Source
Dependencies
AIKIDO depends on CMake, Boost, DART (version 6.3 or above), OMPL, yaml-cpp, tinyxml2, pr-control-msgs, libmicrohttpd, and the
Python development headers (python-dev on Debian systems). DART and AIKIDO both
make heavy use of C++14 and require a modern compiler.
On Ubuntu Trusty using CMake
You should install the ROS packages as described above to build all the ROS-dependent AIKIDO components (e.g., aikido-control-ros).
Install the other dependencies:
$ sudo add-apt-repository ppa:libccd-debs/ppa
$ sudo add-apt-repository ppa:fcl-debs/ppa
$ sudo add-apt-repository ppa:dartsim/ppa
$ sudo add-apt-repository ppa:personalrobotics/ppa
$ sudo apt-get update
$ sudo apt-get install cmake build-essential libboost-filesystem-dev libdart6-optimizer-nlopt-dev libdart6-utils-dev libdart6-utils-urdf-dev libmicrohttpd-dev libompl-dev libtinyxml2-dev libyaml-cpp-dev pr-control-msgs
Once the dependencies are installed, you can build and install AIKIDO using CMake:
$ mkdir build
$ cd build
$ cmake ..
$ make  # you may want to build AIKIDO using multi-core by executing `make -j4`
$ sudo make install
AIKIDO includes several optional components that depend on ROS. While we
suggest building AIKIDO in a Catkin workspace (see below) to enable the ROS
components, it is also possible to build those components in a standalone
build. To do so, source the setup.bash file in your Catkin workspace before
running the above commands, e.g.:
$ . /path/to/my/workspace/setup.bash
On Ubuntu Trusty using Catkin
It is also possible to build AIKIDO as a third-party package inside a
Catkin workspace. To do so, clone AIKIDO into your Catkin
workspace and use the catkin build command like normal.
If you are using the older catkin_make command, then you must build your workspace
with catkin_make_isolated. This may dramatically increase your build time, so we
strongly recommend that you use catkin build, which is provided by the
catkin_tools package, if possible.
On macOS using CMake
Please install Homebrew as described above, then you can easily install the dependencies as follows:
$ cd <aikido_directory>
$ brew bundle
Once the dependencies are installed, you can build and install AIKIDO using CMake:
$ cd <aikido_directory>
$ mkdir build
$ cd build
$ cmake ..
$ make  # you may want to build AIKIDO using multi-core by executing `make -j4`
$ sudo make install
Code Style
Please follow the AIKIDO style guidelines when making a contribution.
License
AIKIDO is licensed under a BSD license. See LICENSE for more
information.
Authors
AIKIDO is developed by the
Personal Robotics Lab in the
Paul G. Allen School of Computer Science and Engineering at
the University of Washington.
The library was started by
Michael Koval (@mkoval)
and Pras Velagapudi (@psigen).
It has received major contributions from
Shushman Choudhury (@Shushman),
Brian Hou (@brianhou),
Aaron Johnson (@aaronjoh),
Jennifer King (@jeking),
Gilwoo Lee (@gilwoolee),
Jeongseok Lee (@jslee02),
and Clint Liddick (@ClintLiddick).
We also would like to thank
Michael Grey (@mxgrey)
and Jeongseok Lee (@jslee02)
for making changes to DART to better support AIKIDO.
",aikido ai for kido warn aikido is under heavi develop these instruct are primarili for refer by the develop aikido is a c librari complet with python bind for solv robot motion plan and decis make problem thi librari is tightli integr with dart for kinematicdynam calcul and ompl for motion plan aikido option integr with ro through the suit of aikido_ro packag for execut on real robot instal on ubuntu trusti use aptget aikido depend on ro you should instal ro by ad the ro repositori to your sourceslist as follow we encourag user to instal indigo sudo sh c echo deb httppackagesrosorgrosubuntu lsb_releas sc main etcaptsourceslistdroslatestlist sudo aptkey adv keyserv hkpkeyserverubuntucom80 recvkey c1cf6e31e6bade8868b172b4f42ed6fbab17c654 sudo aptget updat sudo aptget instal rosindigoactionlib rosindigogeometrymsg rosindigointeractivemark rosindigoroscpp rosindigostdmsg rosindigotf rosindigotrajectorymsg rosindigovisualizationmsg onc ro is instal you can instal aikido from the person robot lab ppa sudo addaptrepositori ppalibccddebsppa sudo addaptrepositori ppafcldebsppa sudo addaptrepositori ppadartsimppa sudo addaptrepositori ppapersonalroboticsppa sudo aptget updat sudo aptget instal libaikidoalldev on maco use homebrew instal the homebrew packag manag usrbinrubi e curl fssl httpsrawgithubusercontentcomhomebrewinstallmasterinstal add homebrew tap for person robot lab softwar brew tap personalroboticstap instal aikido brew instal aikido note while ro seem to be avail on maco we havent test it with aikido for now brew instal aikido instal aikido without the rosdepend compon build from sourc depend aikido depend on cmake boost dart version 63 or abov ompl yamlcpp tinyxml2 prcontrolmsg libmicrohttpd and the python develop header pythondev on debian system dart and aikido both make heavi use of c14 and requir a modern compil on ubuntu trusti use cmake you should instal the ro packag as describ abov to build all the rosdepend aikido compon eg aikidocontrolro instal the other depend sudo addaptrepositori ppalibccddebsppa sudo addaptrepositori ppafcldebsppa sudo addaptrepositori ppadartsimppa sudo addaptrepositori ppapersonalroboticsppa sudo aptget updat sudo aptget instal cmake buildessenti libboostfilesystemdev libdart6optimizernloptdev libdart6utilsdev libdart6utilsurdfdev libmicrohttpddev libompldev libtinyxml2dev libyamlcppdev prcontrolmsg onc the depend are instal you can build and instal aikido use cmake mkdir build cd build cmake make you may want to build aikido use multicor by execut make j4 sudo make instal aikido includ sever option compon that depend on ro while we suggest build aikido in a catkin workspac see below to enabl the ro compon it is also possibl to build those compon in a standalon build to do so sourc the setupbash file in your catkin workspac befor run the abov command eg pathtomyworkspacesetupbash on ubuntu trusti use catkin it is also possibl to build aikido as a thirdparti packag insid a catkin workspac to do so clone aikido into your catkin workspac and use the catkin build command like normal if you are use the older catkin_mak command then you must build your workspac with catkin_make_isol thi may dramat increas your build time so we strongli recommend that you use catkin build which is provid by the catkin_tool packag if possibl on maco use cmake pleas instal homebrew as describ abov then you can easili instal the depend as follow cd aikido_directori brew bundl onc the depend are instal you can build and instal aikido use cmake cd aikido_directori mkdir build cd build cmake make you may want to build aikido use multicor by execut make j4 sudo make instal code style pleas follow the aikido style guidelin when make a contribut licens aikido is licens under a bsd licens see licens for more inform author aikido is develop by the person robot lab in the paul g allen school of comput scienc and engin at the univers of washington the librari wa start by michael koval mkoval and pra velagapudi psigen it ha receiv major contribut from shushman choudhuri shushman brian hou brianhou aaron johnson aaronjoh jennif king jeke gilwoo lee gilwoole jeongseok lee jslee02 and clint liddick clintliddick we also would like to thank michael grey mxgrey and jeongseok lee jslee02 for make chang to dart to better support aikido,aikido ai for kido warning aikido is under heavy development these instruction are primarily for reference by the developer aikido is a c library complete with python binding for solving robotic motion planning and decision making problem this library is tightly integrated with dart for kinematicdynamics calculation and ompl for motion planning aikido optionally integrates with ro through the suite of aikido_ros package for execution on real robot installation on ubuntu trusty using aptget aikido depends on ro you should install ro by adding the ro repository to your sourceslist a follows we encourage user to install indigo sudo sh c echo deb httppackagesrosorgrosubuntu lsb_release sc main etcaptsourceslistdroslatestlist sudo aptkey adv keyserver hkpkeyserverubuntucom80 recvkey c1cf6e31e6bade8868b172b4f42ed6fbab17c654 sudo aptget update sudo aptget install rosindigoactionlib rosindigogeometrymsgs rosindigointeractivemarkers rosindigoroscpp rosindigostdmsgs rosindigotf rosindigotrajectorymsgs rosindigovisualizationmsgs once ro is installed you can install aikido from the personal robotics lab ppa sudo addaptrepository ppalibccddebsppa sudo addaptrepository ppafcldebsppa sudo addaptrepository ppadartsimppa sudo addaptrepository ppapersonalroboticsppa sudo aptget update sudo aptget install libaikidoalldev on macos using homebrew install the homebrew package manager usrbinruby e curl fssl httpsrawgithubusercontentcomhomebrewinstallmasterinstall add homebrew tap for personal robotics lab software brew tap personalroboticstap install aikido brew install aikido note while ro seems to be available on macos we havent tested it with aikido for now brew install aikido installs aikido without the rosdependent component building from source dependency aikido depends on cmake boost dart version 63 or above ompl yamlcpp tinyxml2 prcontrolmsgs libmicrohttpd and the python development header pythondev on debian system dart and aikido both make heavy use of c14 and require a modern compiler on ubuntu trusty using cmake you should install the ro package a described above to build all the rosdependent aikido component eg aikidocontrolros install the other dependency sudo addaptrepository ppalibccddebsppa sudo addaptrepository ppafcldebsppa sudo addaptrepository ppadartsimppa sudo addaptrepository ppapersonalroboticsppa sudo aptget update sudo aptget install cmake buildessential libboostfilesystemdev libdart6optimizernloptdev libdart6utilsdev libdart6utilsurdfdev libmicrohttpddev libompldev libtinyxml2dev libyamlcppdev prcontrolmsgs once the dependency are installed you can build and install aikido using cmake mkdir build cd build cmake make you may want to build aikido using multicore by executing make j4 sudo make install aikido includes several optional component that depend on ro while we suggest building aikido in a catkin workspace see below to enable the ro component it is also possible to build those component in a standalone build to do so source the setupbash file in your catkin workspace before running the above command eg pathtomyworkspacesetupbash on ubuntu trusty using catkin it is also possible to build aikido a a thirdparty package inside a catkin workspace to do so clone aikido into your catkin workspace and use the catkin build command like normal if you are using the older catkin_make command then you must build your workspace with catkin_make_isolated this may dramatically increase your build time so we strongly recommend that you use catkin build which is provided by the catkin_tools package if possible on macos using cmake please install homebrew a described above then you can easily install the dependency a follows cd aikido_directory brew bundle once the dependency are installed you can build and install aikido using cmake cd aikido_directory mkdir build cd build cmake make you may want to build aikido using multicore by executing make j4 sudo make install code style please follow the aikido style guideline when making a contribution license aikido is licensed under a bsd license see license for more information author aikido is developed by the personal robotics lab in the paul g allen school of computer science and engineering at the university of washington the library wa started by michael koval mkoval and pras velagapudi psigen it ha received major contribution from shushman choudhury shushman brian hou brianhou aaron johnson aaronjoh jennifer king jeking gilwoo lee gilwoolee jeongseok lee jslee02 and clint liddick clintliddick we also would like to thank michael grey mxgrey and jeongseok lee jslee02 for making change to dart to better support aikido,aikido ai kido warning aikido heavy development instruction primarily reference developer aikido c library complete python binding solving robotic motion planning decision making problem library tightly integrated dart kinematicdynamics calculation ompl motion planning aikido optionally integrates ro suite aikido_ros package execution real robot installation ubuntu trusty using aptget aikido depends ro install ro adding ro repository sourceslist follows encourage user install indigo sudo sh c echo deb httppackagesrosorgrosubuntu lsb_release sc main etcaptsourceslistdroslatestlist sudo aptkey adv keyserver hkpkeyserverubuntucom80 recvkey c1cf6e31e6bade8868b172b4f42ed6fbab17c654 sudo aptget update sudo aptget install rosindigoactionlib rosindigogeometrymsgs rosindigointeractivemarkers rosindigoroscpp rosindigostdmsgs rosindigotf rosindigotrajectorymsgs rosindigovisualizationmsgs ro installed install aikido personal robotics lab ppa sudo addaptrepository ppalibccddebsppa sudo addaptrepository ppafcldebsppa sudo addaptrepository ppadartsimppa sudo addaptrepository ppapersonalroboticsppa sudo aptget update sudo aptget install libaikidoalldev macos using homebrew install homebrew package manager usrbinruby e curl fssl httpsrawgithubusercontentcomhomebrewinstallmasterinstall add homebrew tap personal robotics lab software brew tap personalroboticstap install aikido brew install aikido note ro seems available macos havent tested aikido brew install aikido installs aikido without rosdependent component building source dependency aikido depends cmake boost dart version 63 ompl yamlcpp tinyxml2 prcontrolmsgs libmicrohttpd python development header pythondev debian system dart aikido make heavy use c14 require modern compiler ubuntu trusty using cmake install ro package described build rosdependent aikido component eg aikidocontrolros install dependency sudo addaptrepository ppalibccddebsppa sudo addaptrepository ppafcldebsppa sudo addaptrepository ppadartsimppa sudo addaptrepository ppapersonalroboticsppa sudo aptget update sudo aptget install cmake buildessential libboostfilesystemdev libdart6optimizernloptdev libdart6utilsdev libdart6utilsurdfdev libmicrohttpddev libompldev libtinyxml2dev libyamlcppdev prcontrolmsgs dependency installed build install aikido using cmake mkdir build cd build cmake make may want build aikido using multicore executing make j4 sudo make install aikido includes several optional component depend ro suggest building aikido catkin workspace see enable ro component also possible build component standalone build source setupbash file catkin workspace running command eg pathtomyworkspacesetupbash ubuntu trusty using catkin also possible build aikido thirdparty package inside catkin workspace clone aikido catkin workspace use catkin build command like normal using older catkin_make command must build workspace catkin_make_isolated may dramatically increase build time strongly recommend use catkin build provided catkin_tools package possible macos using cmake please install homebrew described easily install dependency follows cd aikido_directory brew bundle dependency installed build install aikido using cmake cd aikido_directory mkdir build cd build cmake make may want build aikido using multicore executing make j4 sudo make install code style please follow aikido style guideline making contribution license aikido licensed bsd license see license information author aikido developed personal robotics lab paul g allen school computer science engineering university washington library started michael koval mkoval pras velagapudi psigen received major contribution shushman choudhury shushman brian hou brianhou aaron johnson aaronjoh jennifer king jeking gilwoo lee gilwoolee jeongseok lee jslee02 clint liddick clintliddick also would like thank michael grey mxgrey jeongseok lee jslee02 making change dart better support aikido
C++ ,"⚡电磁AI智能车






采集数据发送至上位机、运行训练好的模型
✨FEATURE

🚀纯电磁四轮
🚑基于状态机的出轨、堵转保护
🎨1.3吋OLED与6个按键的中文GUI
💥一键切换AI模式与PID模式！
🗃️可在运行时切换AI模型
🔧在Keil中通过GUI配置全车参数
⚡PID采用CMSIS-DSP库实现
⚡兼顾效率与灵活性的滑动平均滤波器
🔒运行时错误检查：CAR_ERROR_CHECK
⤴️极简调度器
🕒基于平衡二叉树的软件定时器
🔋低电量自动关机
♻️std::set_new_handler

👀PREVIEW
控制台

复位后打印的内容


开始

图中柱子是电池电量，数字是“CPU占用率”


主页

右上角数字代表“CPU占用率”的位数


模型选择


预存了3个model.nncu.c，可在运行时切换。

控制面板


电机设置


方向控制

数字电位器

电磁传感器



单击切换显示方式

显示波形


配置参数

LIBRARY

致敬开源！


nncu
SimpleGUI
逐飞科技RT1064开源库
StateMachineCompact
MCUXpressoSDK
DSP

📌DEPENDENCE

开发环境：Keil 5.31
编译器：ArmClang V6.14
使用MCUXpresso Config Tool初始化时钟和外设
请使用Keil打开car_config.h

十五届必胜 🎉 🎉 🎉
",ai featur 13oled6gui aipid ai keilgui pidcmsisdsp car_error_check stdset_new_handl preview cpu cpu 3modelnncuc librari nncu simplegui rt1064 statemachinecompact mcuxpressosdk dsp depend keil 531 armclang v614 mcuxpresso config tool keilcar_configh,ai feature 13oled6gui aipid ai keilgui pidcmsisdsp car_error_check stdset_new_handler preview cpu cpu 3modelnncuc library nncu simplegui rt1064 statemachinecompact mcuxpressosdk dsp dependence keil 531 armclang v614 mcuxpresso config tool keilcar_configh,ai feature 13oled6gui aipid ai keilgui pidcmsisdsp car_error_check stdset_new_handler preview cpu cpu 3modelnncuc library nncu simplegui rt1064 statemachinecompact mcuxpressosdk dsp dependence keil 531 armclang v614 mcuxpresso config tool keilcar_configh
C++ ,"Welcome to the modern version of Polyworld, an Artificial Life
system designed as an approach to Artificial Intelligence.
Documentation is hosted on our wiki: https://github.com/polyworld/polyworld/wiki
For installation instructions, please refer to the wiki page for your OS:

Linux Installation
Mac Installation

Note that the Github repository at https://github.com/polyworld/polyworld is the official
home of the Polyworld project, which was formerly hosted at
http://sourceforge.net/projects/polyworld.
",welcom to the modern version of polyworld an artifici life system design as an approach to artifici intellig document is host on our wiki httpsgithubcompolyworldpolyworldwiki for instal instruct pleas refer to the wiki page for your os linux instal mac instal note that the github repositori at httpsgithubcompolyworldpolyworld is the offici home of the polyworld project which wa formerli host at httpsourceforgenetprojectspolyworld,welcome to the modern version of polyworld an artificial life system designed a an approach to artificial intelligence documentation is hosted on our wiki httpsgithubcompolyworldpolyworldwiki for installation instruction please refer to the wiki page for your o linux installation mac installation note that the github repository at httpsgithubcompolyworldpolyworld is the official home of the polyworld project which wa formerly hosted at httpsourceforgenetprojectspolyworld,welcome modern version polyworld artificial life system designed approach artificial intelligence documentation hosted wiki httpsgithubcompolyworldpolyworldwiki installation instruction please refer wiki page o linux installation mac installation note github repository httpsgithubcompolyworldpolyworld official home polyworld project formerly hosted httpsourceforgenetprojectspolyworld
C++ ,"Quackle   


Crossword game artificial intelligence and analysis tool.
See LICENSE in this directory.
Building Quackle:
Quackle is built and tested with the latest releases of Qt 5.12 and 5.13.
See README.MacOS and README.Windows for platform-specific instructions.  Generally:
Clone the repo or download the tarball and untar.  Use qmake to build quackle.pro and quackleio/quackleio.pro:
qmake quackle.pro && make
cd quackleio && qmake && make && cd ..

Finally, build the main binary.
cd quacker && qmake && make

The binary will build as 'Quackle'.  It might be found in the quacker directory or in the release subdirectory.
File organization:

quackle/ - libquackle sources.  libquackle is the engine, and can be linked to any convenient interface.  It does not use Qt.
quackle/quackleio/ - I/O library for Quackle.  Implements stuff for accessing dictionaries, serializing GCG files, etc.  Also, command-line option handling.  This does have some modest dependencies on Qt.
quackle/quacker/ - code for full Quackle UI.  Written in Qt, and requires libquackleio and libquackle.
quackle/makeminidawg/ - standalone console program for building Quackle dictionaries.
quackle/makegaddag/ - standalone console program for building gaddag files.
quackle/data/ - lexicons, strategy files, and alphabet resources for Quackle.
In this directory is libquackle. Run qmake and then run make in this directory. Then cd to quackle/quackleio/, run qmake, and then run make.

olaughlin@gmail.com
jasonkatzbrown@gmail.edu
jfultz@wolfram.com
matt.liberty@gmail.com
",quackl crossword game artifici intellig and analysi tool see licens in thi directori build quackl quackl is built and test with the latest releas of qt 512 and 513 see readmemaco and readmewindow for platformspecif instruct gener clone the repo or download the tarbal and untar use qmake to build quacklepro and quackleioquackleiopro qmake quacklepro make cd quackleio qmake make cd final build the main binari cd quacker qmake make the binari will build as quackl it might be found in the quacker directori or in the releas subdirectori file organ quackl libquackl sourc libquackl is the engin and can be link to ani conveni interfac it doe not use qt quacklequackleio io librari for quackl implement stuff for access dictionari serial gcg file etc also commandlin option handl thi doe have some modest depend on qt quacklequack code for full quackl ui written in qt and requir libquackleio and libquackl quacklemakeminidawg standalon consol program for build quackl dictionari quacklemakegaddag standalon consol program for build gaddag file quackledata lexicon strategi file and alphabet resourc for quackl in thi directori is libquackl run qmake and then run make in thi directori then cd to quacklequackleio run qmake and then run make olaughlingmailcom jasonkatzbrowngmailedu jfultzwolframcom mattlibertygmailcom,quackle crossword game artificial intelligence and analysis tool see license in this directory building quackle quackle is built and tested with the latest release of qt 512 and 513 see readmemacos and readmewindows for platformspecific instruction generally clone the repo or download the tarball and untar use qmake to build quacklepro and quackleioquackleiopro qmake quacklepro make cd quackleio qmake make cd finally build the main binary cd quacker qmake make the binary will build a quackle it might be found in the quacker directory or in the release subdirectory file organization quackle libquackle source libquackle is the engine and can be linked to any convenient interface it doe not use qt quacklequackleio io library for quackle implement stuff for accessing dictionary serializing gcg file etc also commandline option handling this doe have some modest dependency on qt quacklequacker code for full quackle ui written in qt and requires libquackleio and libquackle quacklemakeminidawg standalone console program for building quackle dictionary quacklemakegaddag standalone console program for building gaddag file quackledata lexicon strategy file and alphabet resource for quackle in this directory is libquackle run qmake and then run make in this directory then cd to quacklequackleio run qmake and then run make olaughlingmailcom jasonkatzbrowngmailedu jfultzwolframcom mattlibertygmailcom,quackle crossword game artificial intelligence analysis tool see license directory building quackle quackle built tested latest release qt 512 513 see readmemacos readmewindows platformspecific instruction generally clone repo download tarball untar use qmake build quacklepro quackleioquackleiopro qmake quacklepro make cd quackleio qmake make cd finally build main binary cd quacker qmake make binary build quackle might found quacker directory release subdirectory file organization quackle libquackle source libquackle engine linked convenient interface use qt quacklequackleio io library quackle implement stuff accessing dictionary serializing gcg file etc also commandline option handling modest dependency qt quacklequacker code full quackle ui written qt requires libquackleio libquackle quacklemakeminidawg standalone console program building quackle dictionary quacklemakegaddag standalone console program building gaddag file quackledata lexicon strategy file alphabet resource quackle directory libquackle run qmake run make directory cd quacklequackleio run qmake run make olaughlingmailcom jasonkatzbrowngmailedu jfultzwolframcom mattlibertygmailcom
C++ ,"Physis Shard
Physis Shard is a framework for developing learning systems.
It defines basically two simple interfaces that allows easy addition of agents and problems.
In other words, to add agents or problems they only need to implement these interfaces.
Contents
Agents:

Spectrum-diverse Unified Neuron Evolution Architecture (SUNA)
Random agent (named Mysterious_Agent)
Dummy agent (the user define its output every iteration)

Environments (i.e., Problems):

Mountain Car
Double Cart Pole (with and without velocities)
Function Approximation
Multiplexer
Single Cart Pole

Install
This library depends on the zweifel library.
You can install it from the git address:
https://github.com/zweifel/zweifel
After installing the zweifel library, its full path need to be defined in the Physis Shard's Makefile.
In other words, change the following variable in the Makefile to point to zweifel library's correct installing location:
PATH_TO_ZWEIFEL_LIBRARY=/home/user/zweifel

Run make afterwards:
make

It should produce two executables: rl and rl_live.
To run tests run:
./rl

At the end of the test, the best solution's dna will be saved with the name dna_best_individual
To test this final solution run:
./rl_live dna_best_individual

Changing Environments
Environment can be changed in main.cpp.
For example commenting out where the Reinforcement_Environment is defined and
uncommenting the line with:
Reinforcement_Environment* env= new Double_Cart_Pole(random);

If the environment should be terminated when the maximum steps is reached
uncomment the following in parameters.h:
#define TERMINATE_IF_MAX_STEPS_REACHED		

Do not forget to comment it out when surpassing the maximum number of steps is
not a termination condition! For example, montain car does not need it while
double cart pole does.
Changing Parameters
Many parameters of the environment as well as of the agent can be changed by modifying some definitions
in parameters.h
Running Experiments
To run a trial until its maximum number of trials defined in main.cpp, run:
./rl

To test the best individual, run:
./rl_live dna_best_individual

A series of trials can be run by using the script mean_curve.sh
Adding Agents or Problems
An agent needs to implement the Reinforcement_Agent.h while a problem needs to implement the Reinforcement_Environment.h.
There are simple examples of agents and problems inside respectively the agents/ and environments/ directories.
Most of the examples were built with the general reinforcement learning in mind, however they can be applied to supervised learning as well as unsupervised learning (e.g., consider the reward from the system as an error).
License
Apache License Version 2.0
",physi shard physi shard is a framework for develop learn system it defin basic two simpl interfac that allow easi addit of agent and problem in other word to add agent or problem they onli need to implement these interfac content agent spectrumdivers unifi neuron evolut architectur suna random agent name mysterious_ag dummi agent the user defin it output everi iter environ ie problem mountain car doubl cart pole with and without veloc function approxim multiplex singl cart pole instal thi librari depend on the zweifel librari you can instal it from the git address httpsgithubcomzweifelzweifel after instal the zweifel librari it full path need to be defin in the physi shard makefil in other word chang the follow variabl in the makefil to point to zweifel librari correct instal locat path_to_zweifel_libraryhomeuserzweifel run make afterward make it should produc two execut rl and rl_live to run test run rl at the end of the test the best solut dna will be save with the name dna_best_individu to test thi final solut run rl_live dna_best_individu chang environ environ can be chang in maincpp for exampl comment out where the reinforcement_environ is defin and uncom the line with reinforcement_environ env new double_cart_polerandom if the environ should be termin when the maximum step is reach uncom the follow in parametersh defin terminate_if_max_steps_reach do not forget to comment it out when surpass the maximum number of step is not a termin condit for exampl montain car doe not need it while doubl cart pole doe chang paramet mani paramet of the environ as well as of the agent can be chang by modifi some definit in parametersh run experi to run a trial until it maximum number of trial defin in maincpp run rl to test the best individu run rl_live dna_best_individu a seri of trial can be run by use the script mean_curvesh ad agent or problem an agent need to implement the reinforcement_agenth while a problem need to implement the reinforcement_environmenth there are simpl exampl of agent and problem insid respect the agent and environ directori most of the exampl were built with the gener reinforc learn in mind howev they can be appli to supervis learn as well as unsupervis learn eg consid the reward from the system as an error licens apach licens version 20,physis shard physis shard is a framework for developing learning system it defines basically two simple interface that allows easy addition of agent and problem in other word to add agent or problem they only need to implement these interface content agent spectrumdiverse unified neuron evolution architecture suna random agent named mysterious_agent dummy agent the user define it output every iteration environment ie problem mountain car double cart pole with and without velocity function approximation multiplexer single cart pole install this library depends on the zweifel library you can install it from the git address httpsgithubcomzweifelzweifel after installing the zweifel library it full path need to be defined in the physis shard makefile in other word change the following variable in the makefile to point to zweifel library correct installing location path_to_zweifel_libraryhomeuserzweifel run make afterwards make it should produce two executables rl and rl_live to run test run rl at the end of the test the best solution dna will be saved with the name dna_best_individual to test this final solution run rl_live dna_best_individual changing environment environment can be changed in maincpp for example commenting out where the reinforcement_environment is defined and uncommenting the line with reinforcement_environment env new double_cart_polerandom if the environment should be terminated when the maximum step is reached uncomment the following in parametersh define terminate_if_max_steps_reached do not forget to comment it out when surpassing the maximum number of step is not a termination condition for example montain car doe not need it while double cart pole doe changing parameter many parameter of the environment a well a of the agent can be changed by modifying some definition in parametersh running experiment to run a trial until it maximum number of trial defined in maincpp run rl to test the best individual run rl_live dna_best_individual a series of trial can be run by using the script mean_curvesh adding agent or problem an agent need to implement the reinforcement_agenth while a problem need to implement the reinforcement_environmenth there are simple example of agent and problem inside respectively the agent and environment directory most of the example were built with the general reinforcement learning in mind however they can be applied to supervised learning a well a unsupervised learning eg consider the reward from the system a an error license apache license version 20,physis shard physis shard framework developing learning system defines basically two simple interface allows easy addition agent problem word add agent problem need implement interface content agent spectrumdiverse unified neuron evolution architecture suna random agent named mysterious_agent dummy agent user define output every iteration environment ie problem mountain car double cart pole without velocity function approximation multiplexer single cart pole install library depends zweifel library install git address httpsgithubcomzweifelzweifel installing zweifel library full path need defined physis shard makefile word change following variable makefile point zweifel library correct installing location path_to_zweifel_libraryhomeuserzweifel run make afterwards make produce two executables rl rl_live run test run rl end test best solution dna saved name dna_best_individual test final solution run rl_live dna_best_individual changing environment environment changed maincpp example commenting reinforcement_environment defined uncommenting line reinforcement_environment env new double_cart_polerandom environment terminated maximum step reached uncomment following parametersh define terminate_if_max_steps_reached &#9; &#9; forget comment surpassing maximum number step termination condition example montain car need double cart pole changing parameter many parameter environment well agent changed modifying definition parametersh running experiment run trial maximum number trial defined maincpp run rl test best individual run rl_live dna_best_individual series trial run using script mean_curvesh adding agent problem agent need implement reinforcement_agenth problem need implement reinforcement_environmenth simple example agent problem inside respectively agent environment directory example built general reinforcement learning mind however applied supervised learning well unsupervised learning eg consider reward system error license apache license version 20
C++ ,"Russian AI Cup (not only Russian!)
Artificial intelligence programming contest. Official website: http://russianaicup.ru
Russian AI Cup — intitiative of the company Mail.Ru Group within the IT-oriented competitions.In this championship participants compete in skills of creating an artificial intelligence on example of game stratefies. Organizers are Mail.Ru Group and Codeforces. Best participants will receive special prizes.
Russian AI Cup — largest annual artificial intelligence programming contest in Russia, and third open competition for talented IT-specialists that is part of Mail.Ru Group strategy of forming and developing competitive Russian IT-industry on a global scale.
RAIC 2018: CodeBall
We are pleased to welcome all the participants of the Russian AI Cup: CodeBall 2018 open beta!
The beta test will run until 21:00 UTC on December 23. Please note that at this time we can make a significant changes in the rules, scoring system and any other aspects of the championship. This week we will try to fix possible mistakes, optimize performance and make any other necessary improvements. The ratings will be reset after this week of beta.
Some links on the website are not working. We will fix it within a few days.
Good luck and have fun!
UPDATE 19.12.18
Here you can read about updates.
Useful links

About CodeBall. News, Notifications;
Quick start instructions. Other sources will be published soon on this repository and here;
Play CodeBall in the browser.

Community for English speakers (Discord)
We are HERE. Join us!
Community for Russian speakers (Telegram)
HERE. Join us!
Official Contacts
Also, we've email cups@corp.mail.ru. If you have any private questions, you can ask us. We always check our inbox and reply to all.
",russian ai cup not onli russian artifici intellig program contest offici websit httprussianaicupru russian ai cup intiti of the compani mailru group within the itori competitionsin thi championship particip compet in skill of creat an artifici intellig on exampl of game stratefi organ are mailru group and codeforc best particip will receiv special prize russian ai cup largest annual artifici intellig program contest in russia and third open competit for talent itspecialist that is part of mailru group strategi of form and develop competit russian itindustri on a global scale raic 2018 codebal we are pleas to welcom all the particip of the russian ai cup codebal 2018 open beta the beta test will run until 2100 utc on decemb 23 pleas note that at thi time we can make a signific chang in the rule score system and ani other aspect of the championship thi week we will tri to fix possibl mistak optim perform and make ani other necessari improv the rate will be reset after thi week of beta some link on the websit are not work we will fix it within a few day good luck and have fun updat 191218 here you can read about updat use link about codebal news notif quick start instruct other sourc will be publish soon on thi repositori and here play codebal in the browser commun for english speaker discord we are here join us commun for russian speaker telegram here join us offici contact also weve email cupscorpmailru if you have ani privat question you can ask us we alway check our inbox and repli to all,russian ai cup not only russian artificial intelligence programming contest official website httprussianaicupru russian ai cup intitiative of the company mailru group within the itoriented competitionsin this championship participant compete in skill of creating an artificial intelligence on example of game stratefies organizer are mailru group and codeforces best participant will receive special prize russian ai cup largest annual artificial intelligence programming contest in russia and third open competition for talented itspecialists that is part of mailru group strategy of forming and developing competitive russian itindustry on a global scale raic 2018 codeball we are pleased to welcome all the participant of the russian ai cup codeball 2018 open beta the beta test will run until 2100 utc on december 23 please note that at this time we can make a significant change in the rule scoring system and any other aspect of the championship this week we will try to fix possible mistake optimize performance and make any other necessary improvement the rating will be reset after this week of beta some link on the website are not working we will fix it within a few day good luck and have fun update 191218 here you can read about update useful link about codeball news notification quick start instruction other source will be published soon on this repository and here play codeball in the browser community for english speaker discord we are here join u community for russian speaker telegram here join u official contact also weve email cupscorpmailru if you have any private question you can ask u we always check our inbox and reply to all,russian ai cup russian artificial intelligence programming contest official website httprussianaicupru russian ai cup intitiative company mailru group within itoriented competitionsin championship participant compete skill creating artificial intelligence example game stratefies organizer mailru group codeforces best participant receive special prize russian ai cup largest annual artificial intelligence programming contest russia third open competition talented itspecialists part mailru group strategy forming developing competitive russian itindustry global scale raic 2018 codeball pleased welcome participant russian ai cup codeball 2018 open beta beta test run 2100 utc december 23 please note time make significant change rule scoring system aspect championship week try fix possible mistake optimize performance make necessary improvement rating reset week beta link website working fix within day good luck fun update 191218 read update useful link codeball news notification quick start instruction source published soon repository play codeball browser community english speaker discord join u community russian speaker telegram join u official contact also weve email cupscorpmailru private question ask u always check inbox reply
C++ ,"Russian AI Cup (not only Russian!)
Artificial intelligence programming contest. Official website: http://russianaicup.ru
Russian AI Cup — intitiative of the company Mail.Ru Group within the IT-oriented competitions.In this championship participants compete in skills of creating an artificial intelligence on example of game stratefies. Organizers are Mail.Ru Group and Codeforces. Best participants will receive special prizes.
Russian AI Cup — largest annual artificial intelligence programming contest in Russia, and third open competition for talented IT-specialists that is part of Mail.Ru Group strategy of forming and developing competitive Russian IT-industry on a global scale.
RAIC 2018: CodeBall
We are pleased to welcome all the participants of the Russian AI Cup: CodeBall 2018 open beta!
The beta test will run until 21:00 UTC on December 23. Please note that at this time we can make a significant changes in the rules, scoring system and any other aspects of the championship. This week we will try to fix possible mistakes, optimize performance and make any other necessary improvements. The ratings will be reset after this week of beta.
Some links on the website are not working. We will fix it within a few days.
Good luck and have fun!
UPDATE 19.12.18
Here you can read about updates.
Useful links

About CodeBall. News, Notifications;
Quick start instructions. Other sources will be published soon on this repository and here;
Play CodeBall in the browser.

Community for English speakers (Discord)
We are HERE. Join us!
Community for Russian speakers (Telegram)
HERE. Join us!
Official Contacts
Also, we've email cups@corp.mail.ru. If you have any private questions, you can ask us. We always check our inbox and reply to all.
",russian ai cup not onli russian artifici intellig program contest offici websit httprussianaicupru russian ai cup intiti of the compani mailru group within the itori competitionsin thi championship particip compet in skill of creat an artifici intellig on exampl of game stratefi organ are mailru group and codeforc best particip will receiv special prize russian ai cup largest annual artifici intellig program contest in russia and third open competit for talent itspecialist that is part of mailru group strategi of form and develop competit russian itindustri on a global scale raic 2018 codebal we are pleas to welcom all the particip of the russian ai cup codebal 2018 open beta the beta test will run until 2100 utc on decemb 23 pleas note that at thi time we can make a signific chang in the rule score system and ani other aspect of the championship thi week we will tri to fix possibl mistak optim perform and make ani other necessari improv the rate will be reset after thi week of beta some link on the websit are not work we will fix it within a few day good luck and have fun updat 191218 here you can read about updat use link about codebal news notif quick start instruct other sourc will be publish soon on thi repositori and here play codebal in the browser commun for english speaker discord we are here join us commun for russian speaker telegram here join us offici contact also weve email cupscorpmailru if you have ani privat question you can ask us we alway check our inbox and repli to all,russian ai cup not only russian artificial intelligence programming contest official website httprussianaicupru russian ai cup intitiative of the company mailru group within the itoriented competitionsin this championship participant compete in skill of creating an artificial intelligence on example of game stratefies organizer are mailru group and codeforces best participant will receive special prize russian ai cup largest annual artificial intelligence programming contest in russia and third open competition for talented itspecialists that is part of mailru group strategy of forming and developing competitive russian itindustry on a global scale raic 2018 codeball we are pleased to welcome all the participant of the russian ai cup codeball 2018 open beta the beta test will run until 2100 utc on december 23 please note that at this time we can make a significant change in the rule scoring system and any other aspect of the championship this week we will try to fix possible mistake optimize performance and make any other necessary improvement the rating will be reset after this week of beta some link on the website are not working we will fix it within a few day good luck and have fun update 191218 here you can read about update useful link about codeball news notification quick start instruction other source will be published soon on this repository and here play codeball in the browser community for english speaker discord we are here join u community for russian speaker telegram here join u official contact also weve email cupscorpmailru if you have any private question you can ask u we always check our inbox and reply to all,russian ai cup russian artificial intelligence programming contest official website httprussianaicupru russian ai cup intitiative company mailru group within itoriented competitionsin championship participant compete skill creating artificial intelligence example game stratefies organizer mailru group codeforces best participant receive special prize russian ai cup largest annual artificial intelligence programming contest russia third open competition talented itspecialists part mailru group strategy forming developing competitive russian itindustry global scale raic 2018 codeball pleased welcome participant russian ai cup codeball 2018 open beta beta test run 2100 utc december 23 please note time make significant change rule scoring system aspect championship week try fix possible mistake optimize performance make necessary improvement rating reset week beta link website working fix within day good luck fun update 191218 read update useful link codeball news notification quick start instruction source published soon repository play codeball browser community english speaker discord join u community russian speaker telegram join u official contact also weve email cupscorpmailru private question ask u always check inbox reply
C++ ,"Endless Sky
Explore other star systems. Earn money by trading, carrying passengers, or completing missions. Use your earnings to buy a better ship or to upgrade the weapons and engines on your current one. Blow up pirates. Take sides in a civil war. Or leave human space behind and hope to find some friendly aliens whose culture is more civilized than your own...

Endless Sky is a sandbox-style space exploration game similar to Elite, Escape Velocity, or Star Control. You start out as the captain of a tiny space ship and can choose what to do from there. The game includes a major plot line and many minor missions, but you can choose whether you want to play through the plot or strike out on your own as a merchant or bounty hunter or explorer.
See the player's manual for more information, or the home page for screenshots and the occasional blog post.
Installing the game
Official releases of Endless Sky are available on Steam and as direct downloads from GitHub. A PPA is available for Ubuntu and for Debian. Other package managers may also include the game, though the specific version provided may not be up-to-date.
System Requirements
Endless Sky has very minimal system requirements, meaning most systems should be able to run the game. The most restrictive requirement is likely that your device must support at least OpenGL 3.




Minimum
Recommended




RAM
350 MB
750 MB


Graphics
OpenGL 3.0
OpenGL 3.3


Storage Free
120 MB
300 MB



Building from source
While most development is done on Linux using the SCons build tool to compile the project, IDE-specific files are provided for XCode and Code::Blocks to simplify the installation on Mac OS and Windows. It is possible to use other IDEs or build systems to compile the game, but support is not provided.
For full installation instructions, consult the Build Instructions wiki page.
Contributing
As a free and open source game, Endless Sky is the product of many peoples' work. Contributions of artwork, storylines, and other writing are most in-demand, while there is a loosely defined development roadmap. Those who wish to contribute are encouraged to review the wiki, and to post in the discussion forum.
Licensing
Endless Sky is a free, open source game. The source code is available under the GPL v3 license, and all the artwork is either public domain or released under a variety of Creative Commons licenses. (To determine the copyright status of any of the artwork, consult the copyright file.)
",endless sky explor other star system earn money by trade carri passeng or complet mission use your earn to buy a better ship or to upgrad the weapon and engin on your current one blow up pirat take side in a civil war or leav human space behind and hope to find some friendli alien whose cultur is more civil than your own endless sky is a sandboxstyl space explor game similar to elit escap veloc or star control you start out as the captain of a tini space ship and can choos what to do from there the game includ a major plot line and mani minor mission but you can choos whether you want to play through the plot or strike out on your own as a merchant or bounti hunter or explor see the player manual for more inform or the home page for screenshot and the occasion blog post instal the game offici releas of endless sky are avail on steam and as direct download from github a ppa is avail for ubuntu and for debian other packag manag may also includ the game though the specif version provid may not be uptod system requir endless sky ha veri minim system requir mean most system should be abl to run the game the most restrict requir is like that your devic must support at least opengl 3 minimum recommend ram 350 mb 750 mb graphic opengl 30 opengl 33 storag free 120 mb 300 mb build from sourc while most develop is done on linux use the scon build tool to compil the project idespecif file are provid for xcode and codeblock to simplifi the instal on mac os and window it is possibl to use other ide or build system to compil the game but support is not provid for full instal instruct consult the build instruct wiki page contribut as a free and open sourc game endless sky is the product of mani peopl work contribut of artwork storylin and other write are most indemand while there is a loos defin develop roadmap those who wish to contribut are encourag to review the wiki and to post in the discuss forum licens endless sky is a free open sourc game the sourc code is avail under the gpl v3 licens and all the artwork is either public domain or releas under a varieti of creativ common licens to determin the copyright statu of ani of the artwork consult the copyright file,endless sky explore other star system earn money by trading carrying passenger or completing mission use your earnings to buy a better ship or to upgrade the weapon and engine on your current one blow up pirate take side in a civil war or leave human space behind and hope to find some friendly alien whose culture is more civilized than your own endless sky is a sandboxstyle space exploration game similar to elite escape velocity or star control you start out a the captain of a tiny space ship and can choose what to do from there the game includes a major plot line and many minor mission but you can choose whether you want to play through the plot or strike out on your own a a merchant or bounty hunter or explorer see the player manual for more information or the home page for screenshots and the occasional blog post installing the game official release of endless sky are available on steam and a direct downloads from github a ppa is available for ubuntu and for debian other package manager may also include the game though the specific version provided may not be uptodate system requirement endless sky ha very minimal system requirement meaning most system should be able to run the game the most restrictive requirement is likely that your device must support at least opengl 3 minimum recommended ram 350 mb 750 mb graphic opengl 30 opengl 33 storage free 120 mb 300 mb building from source while most development is done on linux using the scons build tool to compile the project idespecific file are provided for xcode and codeblocks to simplify the installation on mac o and window it is possible to use other ides or build system to compile the game but support is not provided for full installation instruction consult the build instruction wiki page contributing a a free and open source game endless sky is the product of many people work contribution of artwork storyline and other writing are most indemand while there is a loosely defined development roadmap those who wish to contribute are encouraged to review the wiki and to post in the discussion forum licensing endless sky is a free open source game the source code is available under the gpl v3 license and all the artwork is either public domain or released under a variety of creative common license to determine the copyright status of any of the artwork consult the copyright file,endless sky explore star system earn money trading carrying passenger completing mission use earnings buy better ship upgrade weapon engine current one blow pirate take side civil war leave human space behind hope find friendly alien whose culture civilized endless sky sandboxstyle space exploration game similar elite escape velocity star control start captain tiny space ship choose game includes major plot line many minor mission choose whether want play plot strike merchant bounty hunter explorer see player manual information home page screenshots occasional blog post installing game official release endless sky available steam direct downloads github ppa available ubuntu debian package manager may also include game though specific version provided may uptodate system requirement endless sky minimal system requirement meaning system able run game restrictive requirement likely device must support least opengl 3 minimum recommended ram 350 mb 750 mb graphic opengl 30 opengl 33 storage free 120 mb 300 mb building source development done linux using scons build tool compile project idespecific file provided xcode codeblocks simplify installation mac o window possible use ides build system compile game support provided full installation instruction consult build instruction wiki page contributing free open source game endless sky product many people work contribution artwork storyline writing indemand loosely defined development roadmap wish contribute encouraged review wiki post discussion forum licensing endless sky free open source game source code available gpl v3 license artwork either public domain released variety creative common license determine copyright status artwork consult copyright file
C++ ,"



Pioneer Space Simulator

Pioneer is a space adventure game set in the Milky Way galaxy at the turn of
the 31st century.
The game is open-ended, and you are free to explore the millions of star
systems in the game. You can land on planets, slingshot past gas giants, and
burn yourself to a crisp flying between binary star systems. You can try your
hand at piracy, make your fortune trading between systems, or do missions for
the various factions fighting for power, freedom or self-determination.
For more information, see:
http://pioneerspacesim.net/
Community
Come by #pioneer at irc.freenode.net and say hi to the team:
http://pioneerspacesim.net/irc
Bugs? Please log an issue:
http://pioneerspacesim.net/issues
Follow Pioneer on Twitter:
https://twitter.com/pioneerspacesim/
Pioneer wiki
http://pioneerwiki.com/wiki/Pioneer_Wiki
Join the player's forum:
http://spacesimcentral.com/community/pioneer/
Join the development forum:
http://pioneerspacesim.net/forum
Manual
Manual can be found at:
http://pioneerwiki.com/wiki/Manual
Basic flight:
https://pioneerwiki.com/wiki/Basic_flight
Keyboard and mouse control is found at:
http://pioneerwiki.com/wiki/Keyboard_and_mouse_controls
FAQ
For frequently asked questions, please see
http://pioneerwiki.com/wiki/FAQ
BUG Reporting
Please see the section of the FAQ pertaining to bugs, crashs and reporting other issues: Bug Reporting FAQs.
Please do your best to fill out the issue template as completely as possible, especially when you're reporting a crash bug or a graphical issue. Having system information including graphics drivers and the method you used to install Pioneer helps immensely to diagnose and fix these kinds of issues.
Contributing
If you are hungry to contribute, more information can be found here:
http://pioneerwiki.com/wiki/How_you_can_contribute
If you have a contribution you want to share, and want to learn how to make a
pull request, see:
http://pioneerwiki.com/wiki/Using_git_and_GitHub
Localization
Localization for Pioneer is handled trough Transifex, and pulled to the source from there automatically. Because of this please don't make pull requests for translations. You can find the localization project here.
You need to register at transifex to be able to access the translations.
If you want a new language introduced, please request it on the Freenode IRC channel of Pioneer, or here by making an issue for it.
Getting Pioneer
Latest build is available at
https://pioneerspacesim.net/page/download/
For compiling from source, please see COMPILING.txt
Changelog
Please see Changelog.txt
",pioneer space simul pioneer is a space adventur game set in the milki way galaxi at the turn of the 31st centuri the game is openend and you are free to explor the million of star system in the game you can land on planet slingshot past ga giant and burn yourself to a crisp fli between binari star system you can tri your hand at piraci make your fortun trade between system or do mission for the variou faction fight for power freedom or selfdetermin for more inform see httppioneerspacesimnet commun come by pioneer at ircfreenodenet and say hi to the team httppioneerspacesimnetirc bug pleas log an issu httppioneerspacesimnetissu follow pioneer on twitter httpstwittercompioneerspacesim pioneer wiki httppioneerwikicomwikipioneer_wiki join the player forum httpspacesimcentralcomcommunitypion join the develop forum httppioneerspacesimnetforum manual manual can be found at httppioneerwikicomwikimanu basic flight httpspioneerwikicomwikibasic_flight keyboard and mous control is found at httppioneerwikicomwikikeyboard_and_mouse_control faq for frequent ask question pleas see httppioneerwikicomwikifaq bug report pleas see the section of the faq pertain to bug crash and report other issu bug report faq pleas do your best to fill out the issu templat as complet as possibl especi when your report a crash bug or a graphic issu have system inform includ graphic driver and the method you use to instal pioneer help immens to diagnos and fix these kind of issu contribut if you are hungri to contribut more inform can be found here httppioneerwikicomwikihow_you_can_contribut if you have a contribut you want to share and want to learn how to make a pull request see httppioneerwikicomwikiusing_git_and_github local local for pioneer is handl trough transifex and pull to the sourc from there automat becaus of thi pleas dont make pull request for translat you can find the local project here you need to regist at transifex to be abl to access the translat if you want a new languag introduc pleas request it on the freenod irc channel of pioneer or here by make an issu for it get pioneer latest build is avail at httpspioneerspacesimnetpagedownload for compil from sourc pleas see compilingtxt changelog pleas see changelogtxt,pioneer space simulator pioneer is a space adventure game set in the milky way galaxy at the turn of the 31st century the game is openended and you are free to explore the million of star system in the game you can land on planet slingshot past gas giant and burn yourself to a crisp flying between binary star system you can try your hand at piracy make your fortune trading between system or do mission for the various faction fighting for power freedom or selfdetermination for more information see httppioneerspacesimnet community come by pioneer at ircfreenodenet and say hi to the team httppioneerspacesimnetirc bug please log an issue httppioneerspacesimnetissues follow pioneer on twitter httpstwittercompioneerspacesim pioneer wiki httppioneerwikicomwikipioneer_wiki join the player forum httpspacesimcentralcomcommunitypioneer join the development forum httppioneerspacesimnetforum manual manual can be found at httppioneerwikicomwikimanual basic flight httpspioneerwikicomwikibasic_flight keyboard and mouse control is found at httppioneerwikicomwikikeyboard_and_mouse_controls faq for frequently asked question please see httppioneerwikicomwikifaq bug reporting please see the section of the faq pertaining to bug crash and reporting other issue bug reporting faq please do your best to fill out the issue template a completely a possible especially when youre reporting a crash bug or a graphical issue having system information including graphic driver and the method you used to install pioneer help immensely to diagnose and fix these kind of issue contributing if you are hungry to contribute more information can be found here httppioneerwikicomwikihow_you_can_contribute if you have a contribution you want to share and want to learn how to make a pull request see httppioneerwikicomwikiusing_git_and_github localization localization for pioneer is handled trough transifex and pulled to the source from there automatically because of this please dont make pull request for translation you can find the localization project here you need to register at transifex to be able to access the translation if you want a new language introduced please request it on the freenode irc channel of pioneer or here by making an issue for it getting pioneer latest build is available at httpspioneerspacesimnetpagedownload for compiling from source please see compilingtxt changelog please see changelogtxt,pioneer space simulator pioneer space adventure game set milky way galaxy turn 31st century game openended free explore million star system game land planet slingshot past gas giant burn crisp flying binary star system try hand piracy make fortune trading system mission various faction fighting power freedom selfdetermination information see httppioneerspacesimnet community come pioneer ircfreenodenet say hi team httppioneerspacesimnetirc bug please log issue httppioneerspacesimnetissues follow pioneer twitter httpstwittercompioneerspacesim pioneer wiki httppioneerwikicomwikipioneer_wiki join player forum httpspacesimcentralcomcommunitypioneer join development forum httppioneerspacesimnetforum manual manual found httppioneerwikicomwikimanual basic flight httpspioneerwikicomwikibasic_flight keyboard mouse control found httppioneerwikicomwikikeyboard_and_mouse_controls faq frequently asked question please see httppioneerwikicomwikifaq bug reporting please see section faq pertaining bug crash reporting issue bug reporting faq please best fill issue template completely possible especially youre reporting crash bug graphical issue system information including graphic driver method used install pioneer help immensely diagnose fix kind issue contributing hungry contribute information found httppioneerwikicomwikihow_you_can_contribute contribution want share want learn make pull request see httppioneerwikicomwikiusing_git_and_github localization localization pioneer handled trough transifex pulled source automatically please dont make pull request translation find localization project need register transifex able access translation want new language introduced please request freenode irc channel pioneer making issue getting pioneer latest build available httpspioneerspacesimnetpagedownload compiling source please see compilingtxt changelog please see changelogtxt
C++ ,"Woozoolike
A simple space exploration roguelike for 7DRL 2017.
Screenshots


Builds (Windows)

7DRL Version(English) - Download
Latest Version(English) - Download
Latest Version(Korean) - Download

Contact

Discord: https://discord.gg/RhH3vyn

",woozoolik a simpl space explor roguelik for 7drl 2017 screenshot build window 7drl versionenglish download latest versionenglish download latest versionkorean download contact discord httpsdiscordggrhh3vyn,woozoolike a simple space exploration roguelike for 7drl 2017 screenshots build window 7drl versionenglish download latest versionenglish download latest versionkorean download contact discord httpsdiscordggrhh3vyn,woozoolike simple space exploration roguelike 7drl 2017 screenshots build window 7drl versionenglish download latest versionenglish download latest versionkorean download contact discord httpsdiscordggrhh3vyn
C++ ,"OSP (OpenGL + C++ Experiments)
This repo hopes to become a space exploration game along the lines of Squad's Kerbal Space Program; as of today it's a place to experiment implementing different features in C++/OpenGL.
Features
(Those marked with a ✔️ are already implemented)
(Those marked with a ♻️ are ongoing or started)
(The list may be expanded at any time)
✔️ Keplerian Orbit Simulator for on-rails solar system

✔️ Simulating elliptic orbits
♻️ Simulating parabolic and hyperbolic orbits (Not really neccesary for now)

Vessel Building and Controlling

Vessel assembly from a list of parts

♻️ Procedural parts

♻️ Procedural Engines

✔️ Nozzle simulator
Liquid fueled engines
Solid fueled engines


Procedural structures (tanks, fuselage...)


Pre-made parts


Vessel controlling

♻️ Navball

✔️ Navball aligns with vessel
♻️ Navball aligns to reference frames


S.A.S



♻️ Planetary Surfaces

Either very big view distance or joining together multiple scaled cameras
Rendering of planets from far away

Simple billboard shader


♻️ Rendering of planets from the surface and near space

♻️ Near space rendering

♻️ Rocky bodies

♻️ Cubesphere rendering

Very complex cubespheres for asteroids and weird-shaped bodies
Not spheric planets (see above, could be related)
Very big scale shadow rendering (Per vertex?)


LOD

Seamless LOD transitions
Removing seams between quads


Atmospheres

Atmospheric shader
Seamless transition from space to ground
Clouds / Cloud shadows




Other bodies

Gas body rendering

Clouds and animations?









In-Vessel physics

Rigidbody physics simulator
Extraction of acceleration and rotation from the simulation
Applying forces to the vessel from outside (Gravitational gradient?)
Seamless transition from in-vessel physics to the orbit simulator

Maybe they don't need to be separated, but perfomance could suffer


Interaction with the terrain system

♻️ Newtonian Orbit Simulator for vessels

✔️ Orbit propagation
✔️ Orbit predictor (threaded)
♻️ Maneouver planning
Improving perfomance so many vessels can be simulated at high warp speeds

Modding
The engine is data-driven, so pretty much no hardcoded stuff.
Custom behaviour may be implemented using a scripting language (lua, squirrel...), or even a bigger language like C# (Mono)
Screenshots (may be outdated)



",osp opengl c experi thi repo hope to becom a space explor game along the line of squad kerbal space program as of today it a place to experi implement differ featur in copengl featur those mark with a are alreadi implement those mark with a are ongo or start the list may be expand at ani time keplerian orbit simul for onrail solar system simul ellipt orbit simul parabol and hyperbol orbit not realli neccesari for now vessel build and control vessel assembl from a list of part procedur part procedur engin nozzl simul liquid fuel engin solid fuel engin procedur structur tank fuselag premad part vessel control navbal navbal align with vessel navbal align to refer frame sa planetari surfac either veri big view distanc or join togeth multipl scale camera render of planet from far away simpl billboard shader render of planet from the surfac and near space near space render rocki bodi cubespher render veri complex cubespher for asteroid and weirdshap bodi not spheric planet see abov could be relat veri big scale shadow render per vertex lod seamless lod transit remov seam between quad atmospher atmospher shader seamless transit from space to ground cloud cloud shadow other bodi ga bodi render cloud and anim invessel physic rigidbodi physic simul extract of acceler and rotat from the simul appli forc to the vessel from outsid gravit gradient seamless transit from invessel physic to the orbit simul mayb they dont need to be separ but perfom could suffer interact with the terrain system newtonian orbit simul for vessel orbit propag orbit predictor thread maneouv plan improv perfom so mani vessel can be simul at high warp speed mod the engin is datadriven so pretti much no hardcod stuff custom behaviour may be implement use a script languag lua squirrel or even a bigger languag like c mono screenshot may be outdat,osp opengl c experiment this repo hope to become a space exploration game along the line of squad kerbal space program a of today it a place to experiment implementing different feature in copengl feature those marked with a are already implemented those marked with a are ongoing or started the list may be expanded at any time keplerian orbit simulator for onrails solar system simulating elliptic orbit simulating parabolic and hyperbolic orbit not really neccesary for now vessel building and controlling vessel assembly from a list of part procedural part procedural engine nozzle simulator liquid fueled engine solid fueled engine procedural structure tank fuselage premade part vessel controlling navball navball aligns with vessel navball aligns to reference frame sa planetary surface either very big view distance or joining together multiple scaled camera rendering of planet from far away simple billboard shader rendering of planet from the surface and near space near space rendering rocky body cubesphere rendering very complex cubespheres for asteroid and weirdshaped body not spheric planet see above could be related very big scale shadow rendering per vertex lod seamless lod transition removing seam between quad atmosphere atmospheric shader seamless transition from space to ground cloud cloud shadow other body gas body rendering cloud and animation invessel physic rigidbody physic simulator extraction of acceleration and rotation from the simulation applying force to the vessel from outside gravitational gradient seamless transition from invessel physic to the orbit simulator maybe they dont need to be separated but perfomance could suffer interaction with the terrain system newtonian orbit simulator for vessel orbit propagation orbit predictor threaded maneouver planning improving perfomance so many vessel can be simulated at high warp speed modding the engine is datadriven so pretty much no hardcoded stuff custom behaviour may be implemented using a scripting language lua squirrel or even a bigger language like c mono screenshots may be outdated,osp opengl c experiment repo hope become space exploration game along line squad kerbal space program today place experiment implementing different feature copengl feature marked already implemented marked ongoing started list may expanded time keplerian orbit simulator onrails solar system simulating elliptic orbit simulating parabolic hyperbolic orbit really neccesary vessel building controlling vessel assembly list part procedural part procedural engine nozzle simulator liquid fueled engine solid fueled engine procedural structure tank fuselage premade part vessel controlling navball navball aligns vessel navball aligns reference frame sa planetary surface either big view distance joining together multiple scaled camera rendering planet far away simple billboard shader rendering planet surface near space near space rendering rocky body cubesphere rendering complex cubespheres asteroid weirdshaped body spheric planet see could related big scale shadow rendering per vertex lod seamless lod transition removing seam quad atmosphere atmospheric shader seamless transition space ground cloud cloud shadow body gas body rendering cloud animation invessel physic rigidbody physic simulator extraction acceleration rotation simulation applying force vessel outside gravitational gradient seamless transition invessel physic orbit simulator maybe dont need separated perfomance could suffer interaction terrain system newtonian orbit simulator vessel orbit propagation orbit predictor threaded maneouver planning improving perfomance many vessel simulated high warp speed modding engine datadriven pretty much hardcoded stuff custom behaviour may implemented using scripting language lua squirrel even bigger language like c mono screenshots may outdated
C++ ,"Light-HLS: Fast, Accurate and Convenient
Let's try to make HLS developemnt easier for everyone~ ^_^.
Light-HLS is a light weight high-level synthesis (HLS) framework for academic exploration and evaluation, which can be called to perform various design space exploration (DSE) for FPGA-based HLS design. It covers the abilities of previous works, overcomes the existing limitations and brings more practical features. Light-HLS is modularized and portable so designers can use the components of Light-HLS to conduct various DSE procedures.  Light-HLS gets rid of RTL code generation so it will not suffer from the time-consuming synthesis of commercial HLS tools like VivadoHLS, which involves many detailed operations in both its frond-end and back-end, but can accurately estimate timing, resource and some other results of commercial tools for applications.
If Light-HLS helps for your works, please cite our paper in ICCAD 2019 ^_^:
T. Liang, J. Zhao, L. Feng, S. Sinha and W. Zhang, ""Hi-ClockFlow: Multi-Clock Dataflow Automation and Throughput Optimization in High-Level Synthesis,"" 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), Westminster, CO, USA, 2019, pp. 1-6. doi: 10.1109/ICCAD45719.2019.8942136

A well-organzied Wiki can be find here. Since we are still developing this project and there could be some bugs and issues, if you have any problems, PLEASE feel free to let us know: ( tliang@connect.ust.hk ), for which we will sincerely appreciate ^_^. We strongly recommand to you send us an email so we can add you into our maillist for the latest information of Light-HLS, because Light-HLS is a young tool and continuously updated to add features and fix bugs. This is a young project and if you want to join us, we are happy to make it a better one togather! \^_^/ Here are some known issues raised by our users, which we are handling one by one.

Light-HLS Frond-End
The goal of Light-HLS frond-end is to generate IR code close enough to those generated via commercial tools, like VivadoHLS, for DSE purpose. In the front-end of Light-HLS, initial IR codes generated via Clang will be processed by HLS optimization passes consisted of three different levels: (a) At instruction level, Light-HLS modifies, removes or reorders the instructions, e.g. reducing bitwidth,  removing redundant instruction  and reordering computation. (b) At loop/function level, functions will be instantiated and loops may be extracted into sub-functions. (c) As for memory access level, redundant load/store instructions will be removed based on dependency analysis.
Light-HLS Back-End
The back-end of Light-HLS is developed to schedule and bind for the optimized IR codes, so it can predict the resultant performance and resource cost accurately based on the given settings. The IR instructions can be automatically characterized by Light-HLS and a corresponding library, which records the timing and resource of different types of operations, will be generated. For scheduling, based on the generated library, Light-HLS maps most operations to corresponding cycles based on as-soon-as-possible (ASAP) strategy. For some pipelined loops, the constraints of the port number of the BRAMs and loop-carried dependencies are considered. Moreover, some operations might be scheduled as late as possible (ALAP) to lower the II. 	As for resource binding, Light-HLS accumulates the resource cost by each operation and the chaining of operations is considered. The reusing of hardware resource is an important feature in HLS and Light-HLS reuses resources based on more detailed rules, e.g. the source and type of input.
Light-HLS Application Scenariors
Let's first see what we can do with Light-HLS in the research about HLS.


HLS designs can be set with various configurations, leading to different results of performance and resource. To find the optmial solution, designers can determine the configuration and call Light-HLS to predict the result in tens milliseconds, which will be close to the result in VivadoHLS. An example is Hi-ClockFlow, a tool which searches for the configuration of clock settings and HLS directives for the multi-clock dataflow.


HLS designs are sensitive to the source codes, some of which are friendly to FPGA while the others are not. If researchers want to analyze and optimize the design at source code level, Light-HLS have accomplished the back-tracing from back-end, to front-end, to source code, so researchers can find out which part of source code have some interesting behaviors causing problems. In the example Hi-ClockFlow, Light-HLS helps to partition the source code and map the performance and resource to the different parts of the source code.


In the front-end of HLS, source code will be processed by a series of LLVM Passes for analysis and optimization. However, for most of researchers, even if they come up with an idea for the front-end processing, they can hardly estimate the exact outcome of the solution if it can be applied to the commercial HLS tools. Currently, Light-HLS can generate IR code similar to the one generated by VivadoHLS and provide accurate scheduling and resource binding in back-end. Therefore, researchers might implement a Pass, plug it into the front-end of Light-HLS (Yes, just plug ^_^), and evaluate it with the back-end of Light-HLS to see the effect.


In the back-end of HLS, the IR instructions/blocks/loops/functions are scheduled and binded to specific hardware resource on FPGA. Based on the IR code similar to the one generated by commercial tools, how to properly schedule the source code and bind the resource can be tested and analyzed with Light-HLS. Currently, Light-HLS can provide the estimated performance and resource cost close to those from VivadoHLS 2018.2. (We will catch up the version of 2019.2 recently.) Light-HLS can generate the library of the timing and resource cost of all the IR instructions, e.g. add, fmul, MAC, fptoui and etc, for a specified devive, like Zedboard. Researchers can change the original scheme of Light-HLS's scheduling and binding to see the effect.


Category:
Installation of Light-HLS
Usage of Light-HLS
Implementation of Light-HLS and Further development
Notes for You to Create Your Own HLS Tools with LLVM
Good Good Study Day Day Up (^o^)/~
",lighthl fast accur and conveni let tri to make hl developemnt easier for everyon _ lighthl is a light weight highlevel synthesi hl framework for academ explor and evalu which can be call to perform variou design space explor dse for fpgabas hl design it cover the abil of previou work overcom the exist limit and bring more practic featur lighthl is modular and portabl so design can use the compon of lighthl to conduct variou dse procedur lighthl get rid of rtl code gener so it will not suffer from the timeconsum synthesi of commerci hl tool like vivadohl which involv mani detail oper in both it frondend and backend but can accur estim time resourc and some other result of commerci tool for applic if lighthl help for your work pleas cite our paper in iccad 2019 _ t liang j zhao l feng s sinha and w zhang hiclockflow multiclock dataflow autom and throughput optim in highlevel synthesi 2019 ieeeacm intern confer on computeraid design iccad westminst co usa 2019 pp 16 doi 101109iccad4571920198942136 a wellorganzi wiki can be find here sinc we are still develop thi project and there could be some bug and issu if you have ani problem pleas feel free to let us know tliangconnectusthk for which we will sincer appreci _ we strongli recommand to you send us an email so we can add you into our maillist for the latest inform of lighthl becaus lighthl is a young tool and continu updat to add featur and fix bug thi is a young project and if you want to join us we are happi to make it a better one togath _ here are some known issu rais by our user which we are handl one by one lighthl frondend the goal of lighthl frondend is to gener ir code close enough to those gener via commerci tool like vivadohl for dse purpos in the frontend of lighthl initi ir code gener via clang will be process by hl optim pass consist of three differ level a at instruct level lighthl modifi remov or reorder the instruct eg reduc bitwidth remov redund instruct and reorder comput b at loopfunct level function will be instanti and loop may be extract into subfunct c as for memori access level redund loadstor instruct will be remov base on depend analysi lighthl backend the backend of lighthl is develop to schedul and bind for the optim ir code so it can predict the result perform and resourc cost accur base on the given set the ir instruct can be automat character by lighthl and a correspond librari which record the time and resourc of differ type of oper will be gener for schedul base on the gener librari lighthl map most oper to correspond cycl base on assoonasposs asap strategi for some pipelin loop the constraint of the port number of the bram and loopcarri depend are consid moreov some oper might be schedul as late as possibl alap to lower the ii as for resourc bind lighthl accumul the resourc cost by each oper and the chain of oper is consid the reus of hardwar resourc is an import featur in hl and lighthl reus resourc base on more detail rule eg the sourc and type of input lighthl applic scenarior let first see what we can do with lighthl in the research about hl hl design can be set with variou configur lead to differ result of perform and resourc to find the optmial solut design can determin the configur and call lighthl to predict the result in ten millisecond which will be close to the result in vivadohl an exampl is hiclockflow a tool which search for the configur of clock set and hl direct for the multiclock dataflow hl design are sensit to the sourc code some of which are friendli to fpga while the other are not if research want to analyz and optim the design at sourc code level lighthl have accomplish the backtrac from backend to frontend to sourc code so research can find out which part of sourc code have some interest behavior caus problem in the exampl hiclockflow lighthl help to partit the sourc code and map the perform and resourc to the differ part of the sourc code in the frontend of hl sourc code will be process by a seri of llvm pass for analysi and optim howev for most of research even if they come up with an idea for the frontend process they can hardli estim the exact outcom of the solut if it can be appli to the commerci hl tool current lighthl can gener ir code similar to the one gener by vivadohl and provid accur schedul and resourc bind in backend therefor research might implement a pass plug it into the frontend of lighthl ye just plug _ and evalu it with the backend of lighthl to see the effect in the backend of hl the ir instructionsblocksloopsfunct are schedul and bind to specif hardwar resourc on fpga base on the ir code similar to the one gener by commerci tool how to properli schedul the sourc code and bind the resourc can be test and analyz with lighthl current lighthl can provid the estim perform and resourc cost close to those from vivadohl 20182 we will catch up the version of 20192 recent lighthl can gener the librari of the time and resourc cost of all the ir instruct eg add fmul mac fptoui and etc for a specifi deviv like zedboard research can chang the origin scheme of lighthlss schedul and bind to see the effect categori instal of lighthl usag of lighthl implement of lighthl and further develop note for you to creat your own hl tool with llvm good good studi day day up o,lighthls fast accurate and convenient let try to make hl developemnt easier for everyone _ lighthls is a light weight highlevel synthesis hl framework for academic exploration and evaluation which can be called to perform various design space exploration dse for fpgabased hl design it cover the ability of previous work overcomes the existing limitation and brings more practical feature lighthls is modularized and portable so designer can use the component of lighthls to conduct various dse procedure lighthls get rid of rtl code generation so it will not suffer from the timeconsuming synthesis of commercial hl tool like vivadohls which involves many detailed operation in both it frondend and backend but can accurately estimate timing resource and some other result of commercial tool for application if lighthls help for your work please cite our paper in iccad 2019 _ t liang j zhao l feng s sinha and w zhang hiclockflow multiclock dataflow automation and throughput optimization in highlevel synthesis 2019 ieeeacm international conference on computeraided design iccad westminster co usa 2019 pp 16 doi 101109iccad4571920198942136 a wellorganzied wiki can be find here since we are still developing this project and there could be some bug and issue if you have any problem please feel free to let u know tliangconnectusthk for which we will sincerely appreciate _ we strongly recommand to you send u an email so we can add you into our maillist for the latest information of lighthls because lighthls is a young tool and continuously updated to add feature and fix bug this is a young project and if you want to join u we are happy to make it a better one togather _ here are some known issue raised by our user which we are handling one by one lighthls frondend the goal of lighthls frondend is to generate ir code close enough to those generated via commercial tool like vivadohls for dse purpose in the frontend of lighthls initial ir code generated via clang will be processed by hl optimization pass consisted of three different level a at instruction level lighthls modifies remove or reorder the instruction eg reducing bitwidth removing redundant instruction and reordering computation b at loopfunction level function will be instantiated and loop may be extracted into subfunctions c a for memory access level redundant loadstore instruction will be removed based on dependency analysis lighthls backend the backend of lighthls is developed to schedule and bind for the optimized ir code so it can predict the resultant performance and resource cost accurately based on the given setting the ir instruction can be automatically characterized by lighthls and a corresponding library which record the timing and resource of different type of operation will be generated for scheduling based on the generated library lighthls map most operation to corresponding cycle based on assoonaspossible asap strategy for some pipelined loop the constraint of the port number of the brams and loopcarried dependency are considered moreover some operation might be scheduled a late a possible alap to lower the ii a for resource binding lighthls accumulates the resource cost by each operation and the chaining of operation is considered the reusing of hardware resource is an important feature in hl and lighthls reuses resource based on more detailed rule eg the source and type of input lighthls application scenariors let first see what we can do with lighthls in the research about hl hl design can be set with various configuration leading to different result of performance and resource to find the optmial solution designer can determine the configuration and call lighthls to predict the result in ten millisecond which will be close to the result in vivadohls an example is hiclockflow a tool which search for the configuration of clock setting and hl directive for the multiclock dataflow hl design are sensitive to the source code some of which are friendly to fpga while the others are not if researcher want to analyze and optimize the design at source code level lighthls have accomplished the backtracing from backend to frontend to source code so researcher can find out which part of source code have some interesting behavior causing problem in the example hiclockflow lighthls help to partition the source code and map the performance and resource to the different part of the source code in the frontend of hl source code will be processed by a series of llvm pass for analysis and optimization however for most of researcher even if they come up with an idea for the frontend processing they can hardly estimate the exact outcome of the solution if it can be applied to the commercial hl tool currently lighthls can generate ir code similar to the one generated by vivadohls and provide accurate scheduling and resource binding in backend therefore researcher might implement a pas plug it into the frontend of lighthls yes just plug _ and evaluate it with the backend of lighthls to see the effect in the backend of hl the ir instructionsblocksloopsfunctions are scheduled and binded to specific hardware resource on fpga based on the ir code similar to the one generated by commercial tool how to properly schedule the source code and bind the resource can be tested and analyzed with lighthls currently lighthls can provide the estimated performance and resource cost close to those from vivadohls 20182 we will catch up the version of 20192 recently lighthls can generate the library of the timing and resource cost of all the ir instruction eg add fmul mac fptoui and etc for a specified devive like zedboard researcher can change the original scheme of lighthlss scheduling and binding to see the effect category installation of lighthls usage of lighthls implementation of lighthls and further development note for you to create your own hl tool with llvm good good study day day up o,lighthls fast accurate convenient let try make hl developemnt easier everyone _ lighthls light weight highlevel synthesis hl framework academic exploration evaluation called perform various design space exploration dse fpgabased hl design cover ability previous work overcomes existing limitation brings practical feature lighthls modularized portable designer use component lighthls conduct various dse procedure lighthls get rid rtl code generation suffer timeconsuming synthesis commercial hl tool like vivadohls involves many detailed operation frondend backend accurately estimate timing resource result commercial tool application lighthls help work please cite paper iccad 2019 _ liang j zhao l feng sinha w zhang hiclockflow multiclock dataflow automation throughput optimization highlevel synthesis 2019 ieeeacm international conference computeraided design iccad westminster co usa 2019 pp 16 doi 101109iccad4571920198942136 wellorganzied wiki find since still developing project could bug issue problem please feel free let u know tliangconnectusthk sincerely appreciate _ strongly recommand send u email add maillist latest information lighthls lighthls young tool continuously updated add feature fix bug young project want join u happy make better one togather _ known issue raised user handling one one lighthls frondend goal lighthls frondend generate ir code close enough generated via commercial tool like vivadohls dse purpose frontend lighthls initial ir code generated via clang processed hl optimization pass consisted three different level instruction level lighthls modifies remove reorder instruction eg reducing bitwidth removing redundant instruction reordering computation b loopfunction level function instantiated loop may extracted subfunctions c memory access level redundant loadstore instruction removed based dependency analysis lighthls backend backend lighthls developed schedule bind optimized ir code predict resultant performance resource cost accurately based given setting ir instruction automatically characterized lighthls corresponding library record timing resource different type operation generated scheduling based generated library lighthls map operation corresponding cycle based assoonaspossible asap strategy pipelined loop constraint port number brams loopcarried dependency considered moreover operation might scheduled late possible alap lower ii &#9; resource binding lighthls accumulates resource cost operation chaining operation considered reusing hardware resource important feature hl lighthls reuses resource based detailed rule eg source type input lighthls application scenariors let first see lighthls research hl hl design set various configuration leading different result performance resource find optmial solution designer determine configuration call lighthls predict result ten millisecond close result vivadohls example hiclockflow tool search configuration clock setting hl directive multiclock dataflow hl design sensitive source code friendly fpga others researcher want analyze optimize design source code level lighthls accomplished backtracing backend frontend source code researcher find part source code interesting behavior causing problem example hiclockflow lighthls help partition source code map performance resource different part source code frontend hl source code processed series llvm pass analysis optimization however researcher even come idea frontend processing hardly estimate exact outcome solution applied commercial hl tool currently lighthls generate ir code similar one generated vivadohls provide accurate scheduling resource binding backend therefore researcher might implement pas plug frontend lighthls yes plug _ evaluate backend lighthls see effect backend hl ir instructionsblocksloopsfunctions scheduled binded specific hardware resource fpga based ir code similar one generated commercial tool properly schedule source code bind resource tested analyzed lighthls currently lighthls provide estimated performance resource cost close vivadohls 20182 catch version 20192 recently lighthls generate library timing resource cost ir instruction eg add fmul mac fptoui etc specified devive like zedboard researcher change original scheme lighthlss scheduling binding see effect category installation lighthls usage lighthls implementation lighthls development note create hl tool llvm good good study day day
C++ ,"Molpher-lib: Introduction
This C++/Python library is a chemical space exploration software. It is based on the Molpher program which introduced a chemical space exploration method called molecular morphing. The original Molpher method uses stochastic optimization to traverse chemical space between two existing molecules. The main promise of this algorithm is that a virtual library enriched in compounds with improved biological activity could be generated in this way.
The purpose of Molpher-lib is to bring molecular morphing closer to the cheminformatics community, but also offer new features that go beyond the capabilities of the original Molpher program. Molpher-lib makes it possible to roam the chemical universe freely and with little constraints on the inputs. For example, we could just use a carbon atom as a starting point and have Molpher-lib autonomously evolve it into a complete molecular structure. To ensure that the generated molecules have required properties, Molpher-lib also helps with implementation of custom rules and constraints. If you want to know more about Molpher-lib and its usage, make sure to check out some examples on the website. We also have some Jupyter notebooks with examples
that you can explore.
If you would like to participate in the development or just check out the current features of the library, there is extensive documentation which can help you. A big part of the documentation is dedicated to a detailed tutorial that should introduce the philosophy of Molpher-lib in more detail and give you a good idea of what it is currently capable of.
The library is actively developed and many new features are planned to be added. The long-term goal is to make Molpher-lib a universal and easy-to-use de novo drug design framework. Ideas, comments and feature requests are more than welcome and can be submitted to the issue tracker. You can also subscribe to the RSS feed of the dev branch for development updates. If you want to know what is new in the current version, you can look at the changelog.
Installation
Supported platforms:

Linux 64-bit

At the moment, the library binaries are only compiled for 64-bit Linux systems. However, development for other platforms is also planned. If you manage to compile the library on a different platform, consider making a pull request or comment on the issue tracker. Any help is much appreciated.
Installation with Anaconda
Molpher-lib is distributed as a conda package. At the moment, this is the preferred way to install and use the library. All you need to do is get the full Anaconda distribution or its lightweight variant, Miniconda. It is essentially a Python distribution, package manager and virtual environment in one and makes setting up a development environment for any project very easy. After installing Anaconda/Miniconda you can run the following in the Linux terminal:
conda install -c rdkit -c lich molpher-lib
This will automatically download the latest version of the library and install everything to the currently active environment (for more information on environments and the conda command see Conda Test Drive). The library depends on the popular cheminformatics toolkit RDKit so do not forget to add the rdkit channel.
If you are interested in the development snapshots of the library
(most up to date code, but can contain bugs)
, you can use the dev channel instead:
conda install -c rdkit -c lich/label/dev molpher-lib
After that the library should import in your environment and you should be able to successfully run the integrated unit tests:
from molpher.tests import run

run()
You can also check out the Jupyter notebooks with examples from the documentation.
Compiling from Source
Compiling and installing from source is a little bit more elaborate. This process is described in detail in the documentation, but in the simplest case the following should work:
# get dependencies
sudo apt-get install git build-essential python3-dev python3-numpy cmake python3-setuptools

# clone the repo
git clone https://github.com/lich-uct/molpher-lib.git
git checkout dev # or the branch/tag/commit you want
REPOSITORY_ROOT=`pwd`/molpher-lib

# this might take a while, but you if you are lucky, 
# cmake might be able to find dependencies 
# if you already have them somewhere on your system
# so you can skip this step if you have TBB, Boost and RDKit
# installed at standard locations
cd ${REPOSITORY_ROOT}/deps
./build_deps.sh --all

# finally, build the library itself
cd ${REPOSITORY_ROOT}
mkdir cmake-build
cd cmake-build
cmake .. -DCMAKE_BUILD_TYPE=Debug -DPYTHON_EXECUTABLE=python3
make molpher_install_python
After setting the appropriate variables:
export CMAKE_INSTALL_PREFIX=""${REPOSITORY_ROOT}/dist""
export DEPS_DIR=${CMAKE_INSTALL_PREFIX}/../deps
export PYTHONPATH=${DEPS_DIR}/rdkit/:${CMAKE_INSTALL_PREFIX}/lib/python3.5/site-packages
export LD_LIBRARY_PATH=${DEPS_DIR}/tbb/lib/intel64/gcc4.7:${DEPS_DIR}/rdkit/lib/:${DEPS_DIR}/boost/stage/lib:${CMAKE_INSTALL_PREFIX}/lib
you should be good to go:
python3
from molpher.tests import run

run()
This will run the integrated unit tests. They should all pass without problems.
If you want to explore some example code from the documentations, there are
a few Jupyter notebooks located under doc/notebooks. You can create
the needed conda environment
and launch your Jupyter server as follows:
cd ${REPOSITORY_ROOT}
conda env create -f ""environment.yml""
. source_2_activate
python setup.py build_ext --inplace
cd doc/notebooks/
jupyter-notebook
Note that you will need to have the library already compiled and installed in the standard
${REPOSITORY_ROOT}/dist directory.
This installation process has been tested on common Debian-based systems so experience on other Linux flavors may differ. If you run into problems, report them to the issue tracker and hopefully someone will be able to help.
",molpherlib introduct thi cpython librari is a chemic space explor softwar it is base on the molpher program which introduc a chemic space explor method call molecular morph the origin molpher method use stochast optim to travers chemic space between two exist molecul the main promis of thi algorithm is that a virtual librari enrich in compound with improv biolog activ could be gener in thi way the purpos of molpherlib is to bring molecular morph closer to the cheminformat commun but also offer new featur that go beyond the capabl of the origin molpher program molpherlib make it possibl to roam the chemic univers freeli and with littl constraint on the input for exampl we could just use a carbon atom as a start point and have molpherlib autonom evolv it into a complet molecular structur to ensur that the gener molecul have requir properti molpherlib also help with implement of custom rule and constraint if you want to know more about molpherlib and it usag make sure to check out some exampl on the websit we also have some jupyt notebook with exampl that you can explor if you would like to particip in the develop or just check out the current featur of the librari there is extens document which can help you a big part of the document is dedic to a detail tutori that should introduc the philosophi of molpherlib in more detail and give you a good idea of what it is current capabl of the librari is activ develop and mani new featur are plan to be ad the longterm goal is to make molpherlib a univers and easytous de novo drug design framework idea comment and featur request are more than welcom and can be submit to the issu tracker you can also subscrib to the rss feed of the dev branch for develop updat if you want to know what is new in the current version you can look at the changelog instal support platform linux 64bit at the moment the librari binari are onli compil for 64bit linux system howev develop for other platform is also plan if you manag to compil the librari on a differ platform consid make a pull request or comment on the issu tracker ani help is much appreci instal with anaconda molpherlib is distribut as a conda packag at the moment thi is the prefer way to instal and use the librari all you need to do is get the full anaconda distribut or it lightweight variant miniconda it is essenti a python distribut packag manag and virtual environ in one and make set up a develop environ for ani project veri easi after instal anacondaminiconda you can run the follow in the linux termin conda instal c rdkit c lich molpherlib thi will automat download the latest version of the librari and instal everyth to the current activ environ for more inform on environ and the conda command see conda test drive the librari depend on the popular cheminformat toolkit rdkit so do not forget to add the rdkit channel if you are interest in the develop snapshot of the librari most up to date code but can contain bug you can use the dev channel instead conda instal c rdkit c lichlabeldev molpherlib after that the librari should import in your environ and you should be abl to success run the integr unit test from molphertest import run run you can also check out the jupyt notebook with exampl from the document compil from sourc compil and instal from sourc is a littl bit more elabor thi process is describ in detail in the document but in the simplest case the follow should work get depend sudo aptget instal git buildessenti python3dev python3numpi cmake python3setuptool clone the repo git clone httpsgithubcomlichuctmolpherlibgit git checkout dev or the branchtagcommit you want repository_rootpwdmolpherlib thi might take a while but you if you are lucki cmake might be abl to find depend if you alreadi have them somewher on your system so you can skip thi step if you have tbb boost and rdkit instal at standard locat cd repository_rootdep build_depssh all final build the librari itself cd repository_root mkdir cmakebuild cd cmakebuild cmake dcmake_build_typedebug dpython_executablepython3 make molpher_install_python after set the appropri variabl export cmake_install_prefixrepository_rootdist export deps_dircmake_install_prefixdep export pythonpathdeps_dirrdkitcmake_install_prefixlibpython35sitepackag export ld_library_pathdeps_dirtbblibintel64gcc47deps_dirrdkitlibdeps_dirbooststagelibcmake_install_prefixlib you should be good to go python3 from molphertest import run run thi will run the integr unit test they should all pass without problem if you want to explor some exampl code from the document there are a few jupyt notebook locat under docnotebook you can creat the need conda environ and launch your jupyt server as follow cd repository_root conda env creat f environmentyml source_2_activ python setuppi build_ext inplac cd docnotebook jupyternotebook note that you will need to have the librari alreadi compil and instal in the standard repository_rootdist directori thi instal process ha been test on common debianbas system so experi on other linux flavor may differ if you run into problem report them to the issu tracker and hope someon will be abl to help,molpherlib introduction this cpython library is a chemical space exploration software it is based on the molpher program which introduced a chemical space exploration method called molecular morphing the original molpher method us stochastic optimization to traverse chemical space between two existing molecule the main promise of this algorithm is that a virtual library enriched in compound with improved biological activity could be generated in this way the purpose of molpherlib is to bring molecular morphing closer to the cheminformatics community but also offer new feature that go beyond the capability of the original molpher program molpherlib make it possible to roam the chemical universe freely and with little constraint on the input for example we could just use a carbon atom a a starting point and have molpherlib autonomously evolve it into a complete molecular structure to ensure that the generated molecule have required property molpherlib also help with implementation of custom rule and constraint if you want to know more about molpherlib and it usage make sure to check out some example on the website we also have some jupyter notebook with example that you can explore if you would like to participate in the development or just check out the current feature of the library there is extensive documentation which can help you a big part of the documentation is dedicated to a detailed tutorial that should introduce the philosophy of molpherlib in more detail and give you a good idea of what it is currently capable of the library is actively developed and many new feature are planned to be added the longterm goal is to make molpherlib a universal and easytouse de novo drug design framework idea comment and feature request are more than welcome and can be submitted to the issue tracker you can also subscribe to the r feed of the dev branch for development update if you want to know what is new in the current version you can look at the changelog installation supported platform linux 64bit at the moment the library binary are only compiled for 64bit linux system however development for other platform is also planned if you manage to compile the library on a different platform consider making a pull request or comment on the issue tracker any help is much appreciated installation with anaconda molpherlib is distributed a a conda package at the moment this is the preferred way to install and use the library all you need to do is get the full anaconda distribution or it lightweight variant miniconda it is essentially a python distribution package manager and virtual environment in one and make setting up a development environment for any project very easy after installing anacondaminiconda you can run the following in the linux terminal conda install c rdkit c lich molpherlib this will automatically download the latest version of the library and install everything to the currently active environment for more information on environment and the conda command see conda test drive the library depends on the popular cheminformatics toolkit rdkit so do not forget to add the rdkit channel if you are interested in the development snapshot of the library most up to date code but can contain bug you can use the dev channel instead conda install c rdkit c lichlabeldev molpherlib after that the library should import in your environment and you should be able to successfully run the integrated unit test from molphertests import run run you can also check out the jupyter notebook with example from the documentation compiling from source compiling and installing from source is a little bit more elaborate this process is described in detail in the documentation but in the simplest case the following should work get dependency sudo aptget install git buildessential python3dev python3numpy cmake python3setuptools clone the repo git clone httpsgithubcomlichuctmolpherlibgit git checkout dev or the branchtagcommit you want repository_rootpwdmolpherlib this might take a while but you if you are lucky cmake might be able to find dependency if you already have them somewhere on your system so you can skip this step if you have tbb boost and rdkit installed at standard location cd repository_rootdeps build_depssh all finally build the library itself cd repository_root mkdir cmakebuild cd cmakebuild cmake dcmake_build_typedebug dpython_executablepython3 make molpher_install_python after setting the appropriate variable export cmake_install_prefixrepository_rootdist export deps_dircmake_install_prefixdeps export pythonpathdeps_dirrdkitcmake_install_prefixlibpython35sitepackages export ld_library_pathdeps_dirtbblibintel64gcc47deps_dirrdkitlibdeps_dirbooststagelibcmake_install_prefixlib you should be good to go python3 from molphertests import run run this will run the integrated unit test they should all pas without problem if you want to explore some example code from the documentation there are a few jupyter notebook located under docnotebooks you can create the needed conda environment and launch your jupyter server a follows cd repository_root conda env create f environmentyml source_2_activate python setuppy build_ext inplace cd docnotebooks jupyternotebook note that you will need to have the library already compiled and installed in the standard repository_rootdist directory this installation process ha been tested on common debianbased system so experience on other linux flavor may differ if you run into problem report them to the issue tracker and hopefully someone will be able to help,molpherlib introduction cpython library chemical space exploration software based molpher program introduced chemical space exploration method called molecular morphing original molpher method us stochastic optimization traverse chemical space two existing molecule main promise algorithm virtual library enriched compound improved biological activity could generated way purpose molpherlib bring molecular morphing closer cheminformatics community also offer new feature go beyond capability original molpher program molpherlib make possible roam chemical universe freely little constraint input example could use carbon atom starting point molpherlib autonomously evolve complete molecular structure ensure generated molecule required property molpherlib also help implementation custom rule constraint want know molpherlib usage make sure check example website also jupyter notebook example explore would like participate development check current feature library extensive documentation help big part documentation dedicated detailed tutorial introduce philosophy molpherlib detail give good idea currently capable library actively developed many new feature planned added longterm goal make molpherlib universal easytouse de novo drug design framework idea comment feature request welcome submitted issue tracker also subscribe r feed dev branch development update want know new current version look changelog installation supported platform linux 64bit moment library binary compiled 64bit linux system however development platform also planned manage compile library different platform consider making pull request comment issue tracker help much appreciated installation anaconda molpherlib distributed conda package moment preferred way install use library need get full anaconda distribution lightweight variant miniconda essentially python distribution package manager virtual environment one make setting development environment project easy installing anacondaminiconda run following linux terminal conda install c rdkit c lich molpherlib automatically download latest version library install everything currently active environment information environment conda command see conda test drive library depends popular cheminformatics toolkit rdkit forget add rdkit channel interested development snapshot library date code contain bug use dev channel instead conda install c rdkit c lichlabeldev molpherlib library import environment able successfully run integrated unit test molphertests import run run also check jupyter notebook example documentation compiling source compiling installing source little bit elaborate process described detail documentation simplest case following work get dependency sudo aptget install git buildessential python3dev python3numpy cmake python3setuptools clone repo git clone httpsgithubcomlichuctmolpherlibgit git checkout dev branchtagcommit want repository_rootpwdmolpherlib might take lucky cmake might able find dependency already somewhere system skip step tbb boost rdkit installed standard location cd repository_rootdeps build_depssh finally build library cd repository_root mkdir cmakebuild cd cmakebuild cmake dcmake_build_typedebug dpython_executablepython3 make molpher_install_python setting appropriate variable export cmake_install_prefixrepository_rootdist export deps_dircmake_install_prefixdeps export pythonpathdeps_dirrdkitcmake_install_prefixlibpython35sitepackages export ld_library_pathdeps_dirtbblibintel64gcc47deps_dirrdkitlibdeps_dirbooststagelibcmake_install_prefixlib good go python3 molphertests import run run run integrated unit test pas without problem want explore example code documentation jupyter notebook located docnotebooks create needed conda environment launch jupyter server follows cd repository_root conda env create f environmentyml source_2_activate python setuppy build_ext inplace cd docnotebooks jupyternotebook note need library already compiled installed standard repository_rootdist directory installation process tested common debianbased system experience linux flavor may differ run problem report issue tracker hopefully someone able help
C++ ,"#SYNCHROTRACE (Now deprecated. Please use https://github.com/dpac-vlsi/SynchroTrace-gem5 )
There are two tools which together form the prototype SynchroTrace simulation flow built into Gem5.
1) Sigil - Multi-threaded Trace Capture Tool
2) Replay - Event-Trace Replay Framework
This code base includes (2) Replay.
Currently, the Sigil version required to generate traces is provided separately from the same github user ""dpac-vlsi"".
The logical steps to using this simulation environment for design space exploration or CMP simulation is as follows:
1)
a) Generate Multi-threaded Event Traces for the program binary you are testing (See Sigil documentation for further information):
-Use the Sigil wrapping script (runsigil_and_gz_newbranch.py) with necessary options on your binary
OR
b) Use previously generated or sample traces
2) Compile SynchroTrace (For a list of dependencies, please look in the Additional Notes section below)
3) Run SynchroTrace with necessary options on the generated traces
#####Installing SynchroTrace and Running the 8 Thread FFT Example


Check Necessary Dependencies:
gcc-4.4.7
gmp-5.1.1
mpc-1.0
mpfr-3.1.2
swig-2.0.1
python-2.7.6
scons-2.3.0
zlib-dev or zlib1g-dev
m4

Please note:
a) Runtime problems have been encountered when using later versions of swig and gcc.

b) Please ensure /usr/bin/gcc-4.4 is symbolically linked to your defaultgcc (/usr/bin/gcc)
c) gmp, mpc, and mpfr are packaged with gcc when using package installer such as apt.
d) swig-2.0.1 can be found at http://sourceforge.net/projects/swig/files/swig/swig-2.0.1/
e) Currently we do not provide any means to automatically install the missing packages.


Build SynchroTrace


a) If not done so, clone the SynchroTrace repo from GitHub:
$ git clone https://github.com/dpac-vlsi/SynchroTrace
b) Go to the base SynchroTrace directory
c) Run the following command (Note that the number of jobs refers to the number of cores available for compilation):
     scons build/X86_MESI_CMP_directory/gem5.opt --jobs=6
At this point, the gem5 executable should be built with integrated Trace Replay in the location specified in the command above.
gem5 is usually run with a configuration script that hooks up the various architecture models packaged with the gem5 framework.
We have written a SynchroTrace configuration script to which arguments can be passed to configure the desired system.
This script can be found at <SYNCHROTRACE_FOLDER>/configs/synchrotrace/synchrotrace.py
The run_synchrotrace_fft.pl run script can be used to run the FFT example simply as follows:
$ ./run_synchrotrace_fft.pl
This script can be modified and emulated to run your own configuration.
A different trace location by changing the $eventDir variable.
This design being simulated is set by changing the arguments provided in the $synchrotracecmd variable.
For a list of valid arguments, run the following from the main SynchroTrace folder:
$ ./build/X86_MESI_CMP_directory/gem5.opt ./configs/synchrotrace/synchrotrace.py --help
d) Once the run is completed, simulated metrics can be found in m5out/stats.txt and m5out/ruby.stats
####################################################################################################################################
#####Additional Notes:


Sample Sigil Traces are located in $BASESYNCHROTRACEDIR/sample_sigil_traces


SynchroTrace configurations are located in $BASESYNCHROTRACEDIR/configs/synchrotrace/synchrotrace.py and $BASESYNCHROTRACEDIR/configs/ruby/Ruby.py


The run_synchrotrace_fft.pl run script has a section for debug flags. The following is a list of the available debug flags used by SynchroTrace with brief descriptions.
-DebugFlag('mutexLogger') - Prints order of threads obtaining mutex lock
-DebugFlag('printEvent') - Prints EventID# for specific thread before/after event started/completed. This debug flag makes the simulation time very slow.
-DebugFlag('printEventFull') - Prints EventIDs for Threads, Threads on what Cores every 50k cycles
-DebugFlag('cacheMiss') - Prints out cache misses as they happen and address
-DebugFlag('memoryInBarrier') - prints memory reads, writes, read bytes, write bytes every barrier
-DebugFlag('flitsInBarrier') - prints flits generated every barrier
-DebugFlag('l1MissesInBarrier') - prints l1 misses per thread every barrier
-DebugFlag('latencyInBarrier') - prints 3 lines. # packets in barrier, Accumulated queueing delay in barrier, Accumulated network latency in Barrier.
-DebugFlag('powerStatsInBarrier') - prints the total router power specifically for that barrier, i.e. not a rolling average.
-DebugFlag('roi') - Prints out the cycle when we reach the parallel region in Debate. Prints out when the threads all join up.
-DebugFlag('netMessages') - Prints the network packet messages out at 10k cycle buckets.
-DebugFlag('amTrace') - Original default debug flag.


An example of this command with a debug flag is as follows:


./build/X86_MESI_CMP_directory/gem5.opt --debug-flags=printEventFull ./configs/synchrotrace/synchrotrace.py --garnet-network=fixed --topology=Mesh --mesh-rows=8 --eventDir=$eventDir --outputDir=$outputDir --num-cpus=8 --num_threads=8 --num-dirs=8 --num-l2caches=8 --l1d_size=8kB --l1d_assoc=16 --l1i_size=8kB --l1i_assoc=2 --l2_size=128kB --l2_assoc=4 --cpi_iops=1 --cpi_flops=2 --bandwidth_factor=4 --l1_latency=3 --masterFreq=1 2> fft.err"";
where the $eventDir points to the directory of the traces and $outputDir points to the desired output directory path.
",synchrotrac now deprec pleas use httpsgithubcomdpacvlsisynchrotracegem5 there are two tool which togeth form the prototyp synchrotrac simul flow built into gem5 1 sigil multithread trace captur tool 2 replay eventtrac replay framework thi code base includ 2 replay current the sigil version requir to gener trace is provid separ from the same github user dpacvlsi the logic step to use thi simul environ for design space explor or cmp simul is as follow 1 a gener multithread event trace for the program binari you are test see sigil document for further inform use the sigil wrap script runsigil_and_gz_newbranchpi with necessari option on your binari or b use previous gener or sampl trace 2 compil synchrotrac for a list of depend pleas look in the addit note section below 3 run synchrotrac with necessari option on the gener trace instal synchrotrac and run the 8 thread fft exampl check necessari depend gcc447 gmp511 mpc10 mpfr312 swig201 python276 scons230 zlibdev or zlib1gdev m4 pleas note a runtim problem have been encount when use later version of swig and gcc b pleas ensur usrbingcc44 is symbol link to your defaultgcc usrbingcc c gmp mpc and mpfr are packag with gcc when use packag instal such as apt d swig201 can be found at httpsourceforgenetprojectsswigfilesswigswig201 e current we do not provid ani mean to automat instal the miss packag build synchrotrac a if not done so clone the synchrotrac repo from github git clone httpsgithubcomdpacvlsisynchrotrac b go to the base synchrotrac directori c run the follow command note that the number of job refer to the number of core avail for compil scon buildx86_mesi_cmp_directorygem5opt jobs6 at thi point the gem5 execut should be built with integr trace replay in the locat specifi in the command abov gem5 is usual run with a configur script that hook up the variou architectur model packag with the gem5 framework we have written a synchrotrac configur script to which argument can be pass to configur the desir system thi script can be found at synchrotrace_folderconfigssynchrotracesynchrotracepi the run_synchrotrace_fftpl run script can be use to run the fft exampl simpli as follow run_synchrotrace_fftpl thi script can be modifi and emul to run your own configur a differ trace locat by chang the eventdir variabl thi design be simul is set by chang the argument provid in the synchrotracecmd variabl for a list of valid argument run the follow from the main synchrotrac folder buildx86_mesi_cmp_directorygem5opt configssynchrotracesynchrotracepi help d onc the run is complet simul metric can be found in m5outstatstxt and m5outrubystat addit note sampl sigil trace are locat in basesynchrotracedirsample_sigil_trac synchrotrac configur are locat in basesynchrotracedirconfigssynchrotracesynchrotracepi and basesynchrotracedirconfigsrubyrubypi the run_synchrotrace_fftpl run script ha a section for debug flag the follow is a list of the avail debug flag use by synchrotrac with brief descript debugflagmutexlogg print order of thread obtain mutex lock debugflagprintev print eventid for specif thread beforeaft event startedcomplet thi debug flag make the simul time veri slow debugflagprinteventful print eventid for thread thread on what core everi 50k cycl debugflagcachemiss print out cach miss as they happen and address debugflagmemoryinbarri print memori read write read byte write byte everi barrier debugflagflitsinbarri print flit gener everi barrier debugflagl1missesinbarri print l1 miss per thread everi barrier debugflaglatencyinbarri print 3 line packet in barrier accumul queue delay in barrier accumul network latenc in barrier debugflagpowerstatsinbarri print the total router power specif for that barrier ie not a roll averag debugflagroi print out the cycl when we reach the parallel region in debat print out when the thread all join up debugflagnetmessag print the network packet messag out at 10k cycl bucket debugflagamtrac origin default debug flag an exampl of thi command with a debug flag is as follow buildx86_mesi_cmp_directorygem5opt debugflagsprinteventful configssynchrotracesynchrotracepi garnetnetworkfix topologymesh meshrows8 eventdireventdir outputdiroutputdir numcpus8 num_threads8 numdirs8 numl2caches8 l1d_size8kb l1d_assoc16 l1i_size8kb l1i_assoc2 l2_size128kb l2_assoc4 cpi_iops1 cpi_flops2 bandwidth_factor4 l1_latency3 masterfreq1 2 ffterr where the eventdir point to the directori of the trace and outputdir point to the desir output directori path,synchrotrace now deprecated please use httpsgithubcomdpacvlsisynchrotracegem5 there are two tool which together form the prototype synchrotrace simulation flow built into gem5 1 sigil multithreaded trace capture tool 2 replay eventtrace replay framework this code base includes 2 replay currently the sigil version required to generate trace is provided separately from the same github user dpacvlsi the logical step to using this simulation environment for design space exploration or cmp simulation is a follows 1 a generate multithreaded event trace for the program binary you are testing see sigil documentation for further information use the sigil wrapping script runsigil_and_gz_newbranchpy with necessary option on your binary or b use previously generated or sample trace 2 compile synchrotrace for a list of dependency please look in the additional note section below 3 run synchrotrace with necessary option on the generated trace installing synchrotrace and running the 8 thread fft example check necessary dependency gcc447 gmp511 mpc10 mpfr312 swig201 python276 scons230 zlibdev or zlib1gdev m4 please note a runtime problem have been encountered when using later version of swig and gcc b please ensure usrbingcc44 is symbolically linked to your defaultgcc usrbingcc c gmp mpc and mpfr are packaged with gcc when using package installer such a apt d swig201 can be found at httpsourceforgenetprojectsswigfilesswigswig201 e currently we do not provide any mean to automatically install the missing package build synchrotrace a if not done so clone the synchrotrace repo from github git clone httpsgithubcomdpacvlsisynchrotrace b go to the base synchrotrace directory c run the following command note that the number of job refers to the number of core available for compilation scons buildx86_mesi_cmp_directorygem5opt jobs6 at this point the gem5 executable should be built with integrated trace replay in the location specified in the command above gem5 is usually run with a configuration script that hook up the various architecture model packaged with the gem5 framework we have written a synchrotrace configuration script to which argument can be passed to configure the desired system this script can be found at synchrotrace_folderconfigssynchrotracesynchrotracepy the run_synchrotrace_fftpl run script can be used to run the fft example simply a follows run_synchrotrace_fftpl this script can be modified and emulated to run your own configuration a different trace location by changing the eventdir variable this design being simulated is set by changing the argument provided in the synchrotracecmd variable for a list of valid argument run the following from the main synchrotrace folder buildx86_mesi_cmp_directorygem5opt configssynchrotracesynchrotracepy help d once the run is completed simulated metric can be found in m5outstatstxt and m5outrubystats additional note sample sigil trace are located in basesynchrotracedirsample_sigil_traces synchrotrace configuration are located in basesynchrotracedirconfigssynchrotracesynchrotracepy and basesynchrotracedirconfigsrubyrubypy the run_synchrotrace_fftpl run script ha a section for debug flag the following is a list of the available debug flag used by synchrotrace with brief description debugflagmutexlogger print order of thread obtaining mutex lock debugflagprintevent print eventid for specific thread beforeafter event startedcompleted this debug flag make the simulation time very slow debugflagprinteventfull print eventids for thread thread on what core every 50k cycle debugflagcachemiss print out cache miss a they happen and address debugflagmemoryinbarrier print memory read writes read byte write byte every barrier debugflagflitsinbarrier print flit generated every barrier debugflagl1missesinbarrier print l1 miss per thread every barrier debugflaglatencyinbarrier print 3 line packet in barrier accumulated queueing delay in barrier accumulated network latency in barrier debugflagpowerstatsinbarrier print the total router power specifically for that barrier ie not a rolling average debugflagroi print out the cycle when we reach the parallel region in debate print out when the thread all join up debugflagnetmessages print the network packet message out at 10k cycle bucket debugflagamtrace original default debug flag an example of this command with a debug flag is a follows buildx86_mesi_cmp_directorygem5opt debugflagsprinteventfull configssynchrotracesynchrotracepy garnetnetworkfixed topologymesh meshrows8 eventdireventdir outputdiroutputdir numcpus8 num_threads8 numdirs8 numl2caches8 l1d_size8kb l1d_assoc16 l1i_size8kb l1i_assoc2 l2_size128kb l2_assoc4 cpi_iops1 cpi_flops2 bandwidth_factor4 l1_latency3 masterfreq1 2 ffterr where the eventdir point to the directory of the trace and outputdir point to the desired output directory path,synchrotrace deprecated please use httpsgithubcomdpacvlsisynchrotracegem5 two tool together form prototype synchrotrace simulation flow built gem5 1 sigil multithreaded trace capture tool 2 replay eventtrace replay framework code base includes 2 replay currently sigil version required generate trace provided separately github user dpacvlsi logical step using simulation environment design space exploration cmp simulation follows 1 generate multithreaded event trace program binary testing see sigil documentation information use sigil wrapping script runsigil_and_gz_newbranchpy necessary option binary b use previously generated sample trace 2 compile synchrotrace list dependency please look additional note section 3 run synchrotrace necessary option generated trace installing synchrotrace running 8 thread fft example check necessary dependency gcc447 gmp511 mpc10 mpfr312 swig201 python276 scons230 zlibdev zlib1gdev m4 please note runtime problem encountered using later version swig gcc b please ensure usrbingcc44 symbolically linked defaultgcc usrbingcc c gmp mpc mpfr packaged gcc using package installer apt swig201 found httpsourceforgenetprojectsswigfilesswigswig201 e currently provide mean automatically install missing package build synchrotrace done clone synchrotrace repo github git clone httpsgithubcomdpacvlsisynchrotrace b go base synchrotrace directory c run following command note number job refers number core available compilation scons buildx86_mesi_cmp_directorygem5opt jobs6 point gem5 executable built integrated trace replay location specified command gem5 usually run configuration script hook various architecture model packaged gem5 framework written synchrotrace configuration script argument passed configure desired system script found synchrotrace_folderconfigssynchrotracesynchrotracepy run_synchrotrace_fftpl run script used run fft example simply follows run_synchrotrace_fftpl script modified emulated run configuration different trace location changing eventdir variable design simulated set changing argument provided synchrotracecmd variable list valid argument run following main synchrotrace folder buildx86_mesi_cmp_directorygem5opt configssynchrotracesynchrotracepy help run completed simulated metric found m5outstatstxt m5outrubystats additional note sample sigil trace located basesynchrotracedirsample_sigil_traces synchrotrace configuration located basesynchrotracedirconfigssynchrotracesynchrotracepy basesynchrotracedirconfigsrubyrubypy run_synchrotrace_fftpl run script section debug flag following list available debug flag used synchrotrace brief description debugflagmutexlogger print order thread obtaining mutex lock debugflagprintevent print eventid specific thread beforeafter event startedcompleted debug flag make simulation time slow debugflagprinteventfull print eventids thread thread core every 50k cycle debugflagcachemiss print cache miss happen address debugflagmemoryinbarrier print memory read writes read byte write byte every barrier debugflagflitsinbarrier print flit generated every barrier debugflagl1missesinbarrier print l1 miss per thread every barrier debugflaglatencyinbarrier print 3 line packet barrier accumulated queueing delay barrier accumulated network latency barrier debugflagpowerstatsinbarrier print total router power specifically barrier ie rolling average debugflagroi print cycle reach parallel region debate print thread join debugflagnetmessages print network packet message 10k cycle bucket debugflagamtrace original default debug flag example command debug flag follows buildx86_mesi_cmp_directorygem5opt debugflagsprinteventfull configssynchrotracesynchrotracepy garnetnetworkfixed topologymesh meshrows8 eventdireventdir outputdiroutputdir numcpus8 num_threads8 numdirs8 numl2caches8 l1d_size8kb l1d_assoc16 l1i_size8kb l1i_assoc2 l2_size128kb l2_assoc4 cpi_iops1 cpi_flops2 bandwidth_factor4 l1_latency3 masterfreq1 2 ffterr eventdir point directory trace outputdir point desired output directory path
C++ ,"Finite Galaxy
Finite Galaxy is a free and open source space exploration game; the repository containing all files is located at https://github.com/finite-galaxy/finite-galaxy/
It is derived from Endless Sky, a game created by Michael Zahniser, which is located at https://github.com/endless-sky/endless-sky/
Both games can be installed alongside and played independently of each other. Although derived from the same source code and basically using the same content, Finite Galaxy and Endless Sky have diverged and are no longer compatible. If you transpose a save game from one to the other, you are likely to encounter hundreds of errors.
Table of contents

Installation

GNU/Linux
Apple/Mac OS X
Microsoft Windows


Introduction
Changes

Major changes
Minor changes
Not yet implemented ideas


Contributing

Help wanted
Posting issues
Posting pull requests


Reveal entire map

Installation
GNU Linux
Open your terminal and enter:

to install dependencies:

on ArchLinux: pacman -S --needed git gcc scons sdl2 libpng libjpeg-turbo mesa glew openal libmad pango ttf-linux-libertine
on Debian/Ubuntu: sudo apt-get install git g++ scons libsdl2-dev libpng-dev libjpeg-dev libgl1-mesa-dev libglew-dev libopenal-dev libmad0-dev libpango fonts-linuxlibertine
on Fedora/RHEL/CentOS: sudo dnf install git gcc-c++ scons SDL2-devel libpng-devel libjpeg-turbo-devel mesa-libGL-devel glew-devel openal-soft-devel libmad-devel pango linux-libertine-fonts (replace dnf with yum on some versions).


git clone https://github.com/finite-galaxy/finite-galaxy.git to get a local copy of the repository.
cd finite-galaxy/ to open the directory.
git pull to update the game.
scons to compile the game.
./finite-galaxy to run the game.

For more help, consult the man page (the finite-galaxy.6 file).
(return to top)
Apple Mac OS X
If you have trouble compiling or encounter errors, please post here.
To build Finite Galaxy, you will first need to download Xcode from the App Store.
Next, install Homebrew (from http://brew.sh).
Once Homebrew is installed, use it to install the libraries you will need:
brew install libpng
brew install libjpeg-turbo
brew install libmad
brew install sdl2
brew install pango

If the versions of those libraries are different from the ones that the Xcode project is set up for, you will need to modify the file paths in the “Frameworks” section in Xcode.
It is possible that you will also need to modify the “Header Search Paths” and “Library Search Paths” in “Build Settings” to point to wherever Homebrew installed those libraries.
Library paths
To create a Mac OS X binary that will work on systems other than your own, you may also need to use install_name_tool to modify the libraries so that their location is relative to the @rpath.
sudo install_name_tool -id ""@rpath/libpng16.16.dylib"" /usr/local/lib/libpng16.16.dylib
sudo install_name_tool -id ""@rpath/libmad.0.2.1.dylib"" /usr/local/lib/libmad.0.2.1.dylib
sudo install_name_tool -id ""@rpath/libturbojpeg.0.dylib"" /usr/local/opt/libjpeg-turbo/lib/libturbojpeg.0.dylib
sudo install_name_tool -id ""@rpath/libSDL2-2.0.0.dylib"" /usr/local/lib/libSDL2-2.0.0.dylib
sudo install_name_tool -id ""@rpath/pango-1.44.7.dylib"" /usr/local/lib/pango-1.44.7.dylib

(return to top)
Microsoft Windows
If you have trouble compiling or encounter errors, please post here.

Acquire the files with git clone https://github.com/finite-galaxy/finite-galaxy.git or click “Download ZIP” and extract it.
If you don't have it already, open finite-galaxy/fonts/LinLibertine_DRah.ttf and install the font (or copy it to the appropiate location).
For building the game, see https://github.com/endless-sky/endless-sky/wiki/BuildInstructions#Windows

(return to top)
Introduction
Why did I start this project? Why not contribute to Endless Sky instead?

Although I like Endless Sky as a whole, it also contains things I don't like. Conversation scenes, news and portraits, tribute, and plundering of installed outfits are just a few examples.
The original creator, Michael Zahniser, seemed to disappear and pace of development appeared to slow down: there were only nine commits in September 2018 and zero in October. (I started Finite Galaxy on October 18.)
Numerous pull requests over there have been open for over a year, reviews are haphazard, there are many lengthy discussions on unimportant things, while useful proposals were often ignored.
The direction and vision is not always clear.
Support for plug-ins is rather limited.

In short, I consider it a better use of time to work on a project where I can incorporate most of my ideas, where I can remove things I dislike, and where I can contribute whenever I like, without having to wait weeks or months for a review or wasting my time on something that won't be included.
Finite Galaxy is very much a work in progress. Nevertheless, it can be compiled and played without errors. Feel free to try it out yourself!
(return to top)
Changes
Major changes

Hyperjump fuel is based on your ship's effective mass (including cargo and carried ships)
Hyperjump fuel is no longer free, its price depends on the planet (when landing) or the government (when hailing ships in space)
Ships continuously consume energy, based upon the number of bunks, to represent life-support
Ship categories are based on total mass

ship mass = hull mass + outfit space + cargo space


Introduced core space, reserved for energy generators, shields and hull systems, and hyperdrives

outfit space = core space + engine space + weapon space


Installed outfits can no longer be plundered by default; outfits in cargo still can
Minimum depreciation value raised to 50%, time lowered to one year
Weapon projectile damage is a random number between damage and damage + random damage
Guns fire in parallel by default, i.e. no harmonized angle convergence.
Ship info display shows more stats
Redistributed most human ships and many outfits to have more regional specialization
Removed tribute from planets (relevant code is still present for plug-ins)
Removed news and portraits (relevant code is still present for plug-ins)
Distances from planets to the sytem's centre are trebled; as a result space feels larger, thrusters are more desirable, and players won't always land immediately in the middle of a space fight
Non-missile weapons have their weapon range increased by about a third
Exploding ships are significantly more dangerous
Add support for Unicode and different writing directions

(return to top)
Minor changes
See changelog.txt, ship_overview.txt, and https://github.com/finite-galaxy/finite-galaxy/commits/master
(return to top)
Not yet implemented ideas

Add quotation mark preference
Add “Licences” tab in player info panel
Add “Tribute” tab in player info panel
Add “Manufacturer” to ships
Add functionality to deposit credits at the bank for a fixed time (e.g. one year), receiving either the sum plus interest when it expires, or the sum minus a penalty when you claim it beforehand
Add planet landing fees support
Allow friendly fire
Allow sorting available jobs (by e.g. cargo size, distance, name, payment, etc.)
Allow sorting outfits by cost, mass, name
Allow sorting ships by cost, mass, name, outfit space, shields, etc.
Display flagship speed by default and display target's speed with tactical scanner
De-hardcode jump radius
Increase jump radius if you have multiple jump drives installed, perhaps 100*(jump drive)^0.5
Limit the commodities for sale on specific planets
Make ship explosion ‘weapon’ automatically proportional to mass (base, empty, or total mass)
Separate fleet overview column in outfitter and shipyard from ship info display
Separate slots for guns and missile launchers
Ships entering a system from hyperspace should be positioned near the system's centre, instead of near the first inhabited planet
https://github.com/endless-sky/endless-sky/wiki/DevelopmentRoadmap

(return to top)
Contributing
Contributions are welcome; anyone can contribute; feel free to open issues or make pull requests.
Help wanted
Code:

De-hardcode Drone/Fighter classes (currently a boolean is used), to allow for multiple, customizable fighter/bay types (e.g. small, medium, large).
De-hardcode hardpoint slots (currently a boolean is used), to allow for multiple, customizable hardpoint types (e.g. gun slots, missile bays, turret mounts).
Implementing the ideas listed above.
Other new mechanics that make the game more enjoyable.
Update code to C++17.

Art:
Because my Blender skills are non-existent, I could use help from people who are capable and willing to:

Make new outfit sprites and turret hardpoints (e.g. six-gun blaster turret).
Modify existing ship sprites and thumbnails.
Produce ship thumbnails for ships that only have sprites.
Create new ships.
See open issues.

Miscellaneous:
Unfortunately I'm unable to test things on platforms other than my own (Fedora Linux). BSD, MacOS X, and Windows users could help by trying compiling and running the game, and if necessary, correct the appropiate files accordingly.
(return to top)
Posting issues
The issues page on GitHub is for tracking bugs and for art, content, and feature requests. When posting a new issue, please:

Be polite and always assume good faith.
Check to make sure it's not a duplicate of an existing issue.
Create a separate “issue” for each bug, problem, question, or request.

If requesting a new feature, first ask yourself: will this make the game more fun or interesting? Remember that this is a game, not a simulator. Changes will not be made purely for the sake of realism, especially if they introduce needless complexity or aggravation.
If you believe your issue has been resolved, you can close the issue yourself.
(return to top)
Posting pull requests
If you are posting a pull request, please:

Do not combine multiple unrelated changes into a single pull.
Check the diff and make sure the pull request does not contain unintended changes.
If proposing a major pull request, start by posting an issue and discussing the best way to implement it. Often the first strategy that occurs to you will not be the cleanest or most effective way to implement a new feature.
code/:

follow the coding standard.
C++14
do not use tabs; use two spaces instead
make numbers with many digits easier to read for humans by inserting '

(decimal numbers) at intervals of three digits if there are more than four in a row
(hexadecimal numbers) at intervals of four digits if there are more than six in a row


use Oxford English


data/:

do not use tabs; use two spaces instead
use Oxford spelling (the variant of English used by many scientific journals and international organizations such as the United Nations), instead of American, British, Canadian, or other national varieties.
no diacritics in English:

á, à, â → a; same for other vowels
å → aa
æ, ä → ae
œ, ø, ö → oe
ü → ue
İ/ı → I/i
ç → c
č, ć, ċ → ch
š, ś, ş → sh
ñ → nh
ß → ss


text strings (conversations, descriptions, mission dialogues, tooltips, etc.):

avoid abbreviations (e.g., i.e., etc.); contractions (isn't) are fine
use an Oxford comma when giving more than two items (e.g. one, two, and three; not one, two and three)
use U+2013 – en-dash for number ranges (e.g. 10–12) and for parenthetical expressions – like this – instead of parentheses, em-dashes, or hyphens
use U+2026 … horizontal ellipsis instead of three full stops
use U+202F   narrow non-breaking space as a thousands separator for numbers with five or more digits (e.g. 12 345)
use U+2212 − minus sign for negative numbers, subtractions, and deductions


use the "" quote for direct speech and ' apostrophe within direct speech; the source code replaces these with proper “primary” and ‘secondary’ opening and closing quotation marks; surround such strings with ` backticks


repeatedly check and double check any new or changed strings to avoid unnecessary typos; e.g. mind the difference between it's (cf. he's, she's) and its (cf. his, her).


images/:

file names are lower case and use underscores instead of spaces
add both normal and @2x versions

for ships, also create thumbnails
for turrets, also create hardpoints


insert yourself in the copyright.txt file
include all assets (Blender, GIMP, other files) in the opening post


sounds/:

file names are lower case and use underscores instead of spaces
insert yourself in the copyright.txt file



(return to top)
Reveal entire map
Part of the fun of the game is travelling around and exploring. However, if you don't have time for that and simply want to reveal everything in the entire galaxy, then open your save game, find # What you know: and insert the following lines directly afterwards:
visited ""1 Axis""
visited ""10 Pole""
visited ""11 Autumn Above""
visited ""11 Spring Below""
visited ""12 Autumn Above""
visited ""14 Pole""
visited ""14 Summer Above""
visited ""14 Winter Below""
visited ""16 Autumn Rising""
visited ""3 Axis""
visited ""3 Pole""
visited ""3 Spring Rising""
visited ""4 Axis""
visited ""4 Spring Rising""
visited ""4 Summer Rising""
visited ""4 Winter Rising""
visited ""5 Axis""
visited ""5 Spring Below""
visited ""5 Summer Above""
visited ""5 Winter Above""
visited ""7 Autumn Rising""
visited ""8 Winter Below""
visited ""9 Spring Above""
visited ""Ablodab""
visited ""Ablub""
visited ""Acamar""
visited ""Achernar""
visited ""Acrux""
visited ""Adhara""
visited ""Aescolanus""
visited ""Al Dhanab""
visited ""Albaldah""
visited ""Albireo""
visited ""Alcyone""
visited ""Aldebaran""
visited ""Alderamin""
visited ""Aldhibain""
visited ""Algedi""
visited ""Algenib""
visited ""Algenubi""
visited ""Algieba""
visited ""Algol""
visited ""Algorel""
visited ""Alheka""
visited ""Alhena""
visited ""Alioth""
visited ""Alkaid""
visited ""Almaaz""
visited ""Almach""
visited ""Alnair""
visited ""Alnasl""
visited ""Alnilam""
visited ""Alnitak""
visited ""Alniyat""
visited ""Alpha Arae""
visited ""Alpha Centauri""
visited ""Alpha Hydri""
visited ""Alphard""
visited ""Alphecca""
visited ""Alpheratz""
visited ""Altair""
visited ""Aludra""
visited ""Ancient Hope""
visited ""Ankaa""
visited ""Answer""
visited ""Antares""
visited ""Antevorta""
visited ""Ap'arak""
visited ""Arcturus""
visited ""Arculus""
visited ""Arneb""
visited ""Ascella""
visited ""Asikafarnut""
visited ""Aspidiske""
visited ""Atria""
visited ""Avior""
visited ""Aya'k'k""
visited ""Beginning""
visited ""Bellatrix""
visited ""Belonging""
visited ""Belug""
visited ""Belugt""
visited ""Beta Lupi""
visited ""Betelgeuse""
visited ""Bloptab""
visited ""Blubipad""
visited ""Blugtad""
visited ""Boral""
visited ""Bore Fah""
visited ""Bote Asu""
visited ""Bright Void""
visited ""Broken Bowl""
visited ""Caeculus""
visited ""Canopus""
visited ""Capella""
visited ""Caph""
visited ""Cardax""
visited ""Cardea""
visited ""Castor""
visited ""Cebalrai""
visited ""Celeborim""
visited ""Chalawan""
visited ""Charm""
visited ""Chikatip""
visited ""Chimitarp""
visited ""Chirr'ay'akai""
visited ""Chornifath""
visited ""Chy'chra""
visited ""Cinxia""
visited ""Coluber""
visited ""Companion""
visited ""Convector""
visited ""Cor Caroli""
visited ""Da Ent""
visited ""Da Lest""
visited ""Dabih""
visited ""Danoa""
visited ""Dark Hills""
visited ""Debrugt""
visited ""Delta Capricorni""
visited ""Delta Sagittarii""
visited ""Delta Velorum""
visited ""Deneb""
visited ""Denebola""
visited ""Diphda""
visited ""Dokdobaru""
visited ""Dschubba""
visited ""Dubhe""
visited ""Due Yoot""
visited ""Durax""
visited ""Eber""
visited ""Eblumab""
visited ""Edusa""
visited ""Ehma Ti""
visited ""Ek'kek'ru""
visited ""Ekuarik""
visited ""Elnath""
visited ""Eltanin""
visited ""Eneremprukt""
visited ""Enif""
visited ""Es'sprak'ai""
visited ""Eshkoshtar""
visited ""Eteron""
visited ""Fah Root""
visited ""Fah Soom""
visited ""Fala""
visited ""Fallen Leaf""
visited ""Far Horizon""
visited ""Farbutero""
visited ""Farinus""
visited ""Faronektu""
visited ""Fasitopfar""
visited ""Fell Omen""
visited ""Feroteri""
visited ""Ferukistek""
visited ""Fingol""
visited ""Flugbu""
visited ""Fomalhaut""
visited ""Fornarep""
visited ""Four Pillars""
visited ""Furmeliki""
visited ""Gacrux""
visited ""Gamma Cassiopeiae""
visited ""Gamma Corvi""
visited ""Gienah""
visited ""Girtab""
visited ""Glubatub""
visited ""Gomeisa""
visited ""Good Omen""
visited ""Gorvi""
visited ""Graffias""
visited ""Gupta""
visited ""Hadar""
visited ""Hamal""
visited ""Han""
visited ""Hassaleh""
visited ""Hatysa""
visited ""Heia Due""
visited ""Hesselpost""
visited ""Hevru Hai""
visited ""Hi Yahr""
visited ""Hintar""
visited ""Holeb""
visited ""Homeward""
visited ""Host""
visited ""Hunter""
visited ""Ik'kara'ka""
visited ""Ildaria""
visited ""Imo Dep""
visited ""Insitor""
visited ""Io Lowe""
visited ""Io Mann""
visited ""Ipsing""
visited ""Iyech'yek""
visited ""Izar""
visited ""Ka'ch'chrai""
visited ""Ka'pru""
visited ""Kaliptari""
visited ""Kappa Centauri""
visited ""Kashikt""
visited ""Kasikfar""
visited ""Kaus Australis""
visited ""Kaus Borealis""
visited ""Ki War Ek""
visited ""Kiro'ku""
visited ""Kiru'kichi""
visited ""Kochab""
visited ""Kor Ak'Mari""
visited ""Kor En'lakfar""
visited ""Kor Fel'tar""
visited ""Kor Men""
visited ""Kor Nor'peli""
visited ""Kor Tar'bei""
visited ""Kor Zena'i""
visited ""Kornephoros""
visited ""Korsmanath""
visited ""Kraz""
visited ""Kugel""
visited ""Kursa""
visited ""Last Word""
visited ""Lesath""
visited ""Levana""
visited ""Limen""
visited ""Lolami""
visited ""Lom Tahr""
visited ""Lone Cloud""
visited ""Lucina""
visited ""Lurata""
visited ""Makferuti""
visited ""Markab""
visited ""Markeb""
visited ""Matar""
visited ""Mebla""
visited ""Mebsuta""
visited ""Meftarkata""
visited ""Mei Yohn""
visited ""Mekislepti""
visited ""Membulem""
visited ""Men""
visited ""Menkalinan""
visited ""Menkar""
visited ""Menkent""
visited ""Merak""
visited ""Mesuket""
visited ""Miaplacidus""
visited ""Miblulub""
visited ""Mimosa""
visited ""Minkar""
visited ""Mintaka""
visited ""Mirach""
visited ""Mirfak""
visited ""Mirzam""
visited ""Mizar""
visited ""Moktar""
visited ""Mora""
visited ""Muhlifain""
visited ""Muphrid""
visited ""Naos""
visited ""Naper""
visited ""Nashira""
visited ""Nenia""
visited ""Nihal""
visited ""Nocte""
visited ""Nunki""
visited ""Oblate""
visited ""Orbona""
visited ""Orvala""
visited ""Ossipago""
visited ""Over the Rainbow""
visited ""Pantica""
visited ""Parca""
visited ""Peacock""
visited ""Pelubta""
visited ""Peragenor""
visited ""Peresedersi""
visited ""Perfica""
visited ""Persian""
visited ""Persitar""
visited ""Phact""
visited ""Phecda""
visited ""Pherkad""
visited ""Phurad""
visited ""Pik'ro'iyak""
visited ""Plort""
visited ""Polaris""
visited ""Pollux""
visited ""Porrima""
visited ""Prakacha'a""
visited ""Procyon""
visited ""Pug Iyik""
visited ""Quaru""
visited ""Rajak""
visited ""Rasalhague""
visited ""Rastaban""
visited ""Rati Cal""
visited ""Regor""
visited ""Regulus""
visited ""Remembrance""
visited ""Rigel""
visited ""Ruchbah""
visited ""Rutilicus""
visited ""Ruwarku""
visited ""Sabik""
visited ""Sabriset""
visited ""Sadachbia""
visited ""Sadalmelik""
visited ""Sadalsuud""
visited ""Sadr""
visited ""Sagittarius A*""
visited ""Saiph""
visited ""Salipastart""
visited ""Salm""
visited ""Sargas""
visited ""Sarin""
visited ""Sayaiban""
visited ""Scheat""
visited ""Schedar""
visited ""Segesta""
visited ""Seginus""
visited ""Seketra""
visited ""Sepetrosk""
visited ""Sepriaptu""
visited ""Sevrelect""
visited ""Shaula""
visited ""Sheratan""
visited ""Si'yak'ku""
visited ""Sich'ka'ara""
visited ""Silikatakfar""
visited ""Silver Bell""
visited ""Silver String""
visited ""Similisti""
visited ""Sirius""
visited ""Situla""
visited ""Skeruto""
visited ""Sko'karak""
visited ""Sobarati""
visited ""Sol""
visited ""Sol Arachi""
visited ""Sol Kimek""
visited ""Sol Saryds""
visited ""Solifar""
visited ""Sospi""
visited ""Speloog""
visited ""Spica""
visited ""Steep Roof""
visited ""Stercutus""
visited ""Suhail""
visited ""Sumar""
visited ""Sumprast""
visited ""Tais""
visited ""Talita""
visited ""Tania Australis""
visited ""Tarazed""
visited ""Tarf""
visited ""Tebuteb""
visited ""Tejat""
visited ""Terminus""
visited ""Terra Incognita""
visited ""Torbab""
visited ""Tortor""
visited ""Turais""
visited ""Ula Mon""
visited ""Ultima Thule""
visited ""Umbral""
visited ""Unagi""
visited ""Unukalhai""
visited ""Uwa Fahn""
visited ""Vega""
visited ""Vindemiatrix""
visited ""Volax""
visited ""Wah Ki""
visited ""Wah Oh""
visited ""Wah Yoot""
visited ""Waypoint""
visited ""Wazn""
visited ""Wei""
visited ""Wezen""
visited ""World's End""
visited ""Ya Hai""
visited ""Yed Prior""
visited ""Zaurak""
visited ""Zeta Aquilae""
visited ""Zeta Centauri""
visited ""Zosma""
visited ""Zuba Zub""
visited ""Zubenelgenubi""
visited ""Zubenelhakrabi""
visited ""Zubeneschamali""
""visited planet"" ""Ember Reaches""
""visited planet"" ""Ember Threshold""
""visited planet"" ""Ember Wormhole""
""visited planet"" ""Hai Wormhole""
""visited planet"" ""Pirate Wormhole""
""visited planet"" ""Pug Wormhole""
""visited planet"" ""Quarg Wormhole""
""visited planet"" ""Remnant Wormhole""
""visited planet"" ""Rim Wormhole""
""visited planet"" ""The Eye""

(return to top)
",finit galaxi finit galaxi is a free and open sourc space explor game the repositori contain all file is locat at httpsgithubcomfinitegalaxyfinitegalaxi it is deriv from endless sky a game creat by michael zahnis which is locat at httpsgithubcomendlessskyendlessski both game can be instal alongsid and play independ of each other although deriv from the same sourc code and basic use the same content finit galaxi and endless sky have diverg and are no longer compat if you transpos a save game from one to the other you are like to encount hundr of error tabl of content instal gnulinux applemac os x microsoft window introduct chang major chang minor chang not yet implement idea contribut help want post issu post pull request reveal entir map instal gnu linux open your termin and enter to instal depend on archlinux pacman s need git gcc scon sdl2 libpng libjpegturbo mesa glew open libmad pango ttflinuxlibertin on debianubuntu sudo aptget instal git g scon libsdl2dev libpngdev libjpegdev libgl1mesadev libglewdev libopenaldev libmad0dev libpango fontslinuxlibertin on fedorarhelcento sudo dnf instal git gccc scon sdl2devel libpngdevel libjpegturbodevel mesalibgldevel glewdevel openalsoftdevel libmaddevel pango linuxlibertinefont replac dnf with yum on some version git clone httpsgithubcomfinitegalaxyfinitegalaxygit to get a local copi of the repositori cd finitegalaxi to open the directori git pull to updat the game scon to compil the game finitegalaxi to run the game for more help consult the man page the finitegalaxy6 file return to top appl mac os x if you have troubl compil or encount error pleas post here to build finit galaxi you will first need to download xcode from the app store next instal homebrew from httpbrewsh onc homebrew is instal use it to instal the librari you will need brew instal libpng brew instal libjpegturbo brew instal libmad brew instal sdl2 brew instal pango if the version of those librari are differ from the one that the xcode project is set up for you will need to modifi the file path in the framework section in xcode it is possibl that you will also need to modifi the header search path and librari search path in build set to point to wherev homebrew instal those librari librari path to creat a mac os x binari that will work on system other than your own you may also need to use install_name_tool to modifi the librari so that their locat is rel to the rpath sudo install_name_tool id rpathlibpng1616dylib usrlocalliblibpng1616dylib sudo install_name_tool id rpathlibmad021dylib usrlocalliblibmad021dylib sudo install_name_tool id rpathlibturbojpeg0dylib usrlocaloptlibjpegturboliblibturbojpeg0dylib sudo install_name_tool id rpathlibsdl2200dylib usrlocalliblibsdl2200dylib sudo install_name_tool id rpathpango1447dylib usrlocallibpango1447dylib return to top microsoft window if you have troubl compil or encount error pleas post here acquir the file with git clone httpsgithubcomfinitegalaxyfinitegalaxygit or click download zip and extract it if you dont have it alreadi open finitegalaxyfontslinlibertine_drahttf and instal the font or copi it to the appropi locat for build the game see httpsgithubcomendlessskyendlessskywikibuildinstructionswindow return to top introduct whi did i start thi project whi not contribut to endless sky instead although i like endless sky as a whole it also contain thing i dont like convers scene news and portrait tribut and plunder of instal outfit are just a few exampl the origin creator michael zahnis seem to disappear and pace of develop appear to slow down there were onli nine commit in septemb 2018 and zero in octob i start finit galaxi on octob 18 numer pull request over there have been open for over a year review are haphazard there are mani lengthi discuss on unimport thing while use propos were often ignor the direct and vision is not alway clear support for plugin is rather limit in short i consid it a better use of time to work on a project where i can incorpor most of my idea where i can remov thing i dislik and where i can contribut whenev i like without have to wait week or month for a review or wast my time on someth that wont be includ finit galaxi is veri much a work in progress nevertheless it can be compil and play without error feel free to tri it out yourself return to top chang major chang hyperjump fuel is base on your ship effect mass includ cargo and carri ship hyperjump fuel is no longer free it price depend on the planet when land or the govern when hail ship in space ship continu consum energi base upon the number of bunk to repres lifesupport ship categori are base on total mass ship mass hull mass outfit space cargo space introduc core space reserv for energi gener shield and hull system and hyperdr outfit space core space engin space weapon space instal outfit can no longer be plunder by default outfit in cargo still can minimum depreci valu rais to 50 time lower to one year weapon projectil damag is a random number between damag and damag random damag gun fire in parallel by default ie no harmon angl converg ship info display show more stat redistribut most human ship and mani outfit to have more region special remov tribut from planet relev code is still present for plugin remov news and portrait relev code is still present for plugin distanc from planet to the sytem centr are trebl as a result space feel larger thruster are more desir and player wont alway land immedi in the middl of a space fight nonmissil weapon have their weapon rang increas by about a third explod ship are significantli more danger add support for unicod and differ write direct return to top minor chang see changelogtxt ship_overviewtxt and httpsgithubcomfinitegalaxyfinitegalaxycommitsmast return to top not yet implement idea add quotat mark prefer add licenc tab in player info panel add tribut tab in player info panel add manufactur to ship add function to deposit credit at the bank for a fix time eg one year receiv either the sum plu interest when it expir or the sum minu a penalti when you claim it beforehand add planet land fee support allow friendli fire allow sort avail job by eg cargo size distanc name payment etc allow sort outfit by cost mass name allow sort ship by cost mass name outfit space shield etc display flagship speed by default and display target speed with tactic scanner dehardcod jump radiu increas jump radiu if you have multipl jump drive instal perhap 100jump drive05 limit the commod for sale on specif planet make ship explos weapon automat proport to mass base empti or total mass separ fleet overview column in outfitt and shipyard from ship info display separ slot for gun and missil launcher ship enter a system from hyperspac should be posit near the system centr instead of near the first inhabit planet httpsgithubcomendlessskyendlessskywikidevelopmentroadmap return to top contribut contribut are welcom anyon can contribut feel free to open issu or make pull request help want code dehardcod dronefight class current a boolean is use to allow for multipl customiz fighterbay type eg small medium larg dehardcod hardpoint slot current a boolean is use to allow for multipl customiz hardpoint type eg gun slot missil bay turret mount implement the idea list abov other new mechan that make the game more enjoy updat code to c17 art becaus my blender skill are nonexist i could use help from peopl who are capabl and will to make new outfit sprite and turret hardpoint eg sixgun blaster turret modifi exist ship sprite and thumbnail produc ship thumbnail for ship that onli have sprite creat new ship see open issu miscellan unfortun im unabl to test thing on platform other than my own fedora linux bsd maco x and window user could help by tri compil and run the game and if necessari correct the appropi file accordingli return to top post issu the issu page on github is for track bug and for art content and featur request when post a new issu pleas be polit and alway assum good faith check to make sure it not a duplic of an exist issu creat a separ issu for each bug problem question or request if request a new featur first ask yourself will thi make the game more fun or interest rememb that thi is a game not a simul chang will not be made pure for the sake of realism especi if they introduc needless complex or aggrav if you believ your issu ha been resolv you can close the issu yourself return to top post pull request if you are post a pull request pleas do not combin multipl unrel chang into a singl pull check the diff and make sure the pull request doe not contain unintend chang if propos a major pull request start by post an issu and discuss the best way to implement it often the first strategi that occur to you will not be the cleanest or most effect way to implement a new featur code follow the code standard c14 do not use tab use two space instead make number with mani digit easier to read for human by insert decim number at interv of three digit if there are more than four in a row hexadecim number at interv of four digit if there are more than six in a row use oxford english data do not use tab use two space instead use oxford spell the variant of english use by mani scientif journal and intern organ such as the unit nation instead of american british canadian or other nation varieti no diacrit in english a same for other vowel aa ae oe ue ii c ch sh nh ss text string convers descript mission dialogu tooltip etc avoid abbrevi eg ie etc contract isnt are fine use an oxford comma when give more than two item eg one two and three not one two and three use u2013 endash for number rang eg 1012 and for parenthet express like thi instead of parenthes emdash or hyphen use u2026 horizont ellipsi instead of three full stop use u202f narrow nonbreak space as a thousand separ for number with five or more digit eg 12 345 use u2212 minu sign for neg number subtract and deduct use the quot for direct speech and apostroph within direct speech the sourc code replac these with proper primari and secondari open and close quotat mark surround such string with backtick repeatedli check and doubl check ani new or chang string to avoid unnecessari typo eg mind the differ between it cf he she and it cf hi her imag file name are lower case and use underscor instead of space add both normal and 2x version for ship also creat thumbnail for turret also creat hardpoint insert yourself in the copyrighttxt file includ all asset blender gimp other file in the open post sound file name are lower case and use underscor instead of space insert yourself in the copyrighttxt file return to top reveal entir map part of the fun of the game is travel around and explor howev if you dont have time for that and simpli want to reveal everyth in the entir galaxi then open your save game find what you know and insert the follow line directli afterward visit 1 axi visit 10 pole visit 11 autumn abov visit 11 spring below visit 12 autumn abov visit 14 pole visit 14 summer abov visit 14 winter below visit 16 autumn rise visit 3 axi visit 3 pole visit 3 spring rise visit 4 axi visit 4 spring rise visit 4 summer rise visit 4 winter rise visit 5 axi visit 5 spring below visit 5 summer abov visit 5 winter abov visit 7 autumn rise visit 8 winter below visit 9 spring abov visit ablodab visit ablub visit acamar visit achernar visit acrux visit adhara visit aescolanu visit al dhanab visit albaldah visit albireo visit alcyon visit aldebaran visit alderamin visit aldhibain visit algedi visit algenib visit algenubi visit algieba visit algol visit algorel visit alheka visit alhena visit alioth visit alkaid visit almaaz visit almach visit alnair visit alnasl visit alnilam visit alnitak visit alniyat visit alpha ara visit alpha centauri visit alpha hydri visit alphard visit alphecca visit alpheratz visit altair visit aludra visit ancient hope visit ankaa visit answer visit antar visit antevorta visit aparak visit arcturu visit arculu visit arneb visit ascella visit asikafarnut visit aspidisk visit atria visit avior visit ayakk visit begin visit bellatrix visit belong visit belug visit belugt visit beta lupi visit betelgeus visit bloptab visit blubipad visit blugtad visit boral visit bore fah visit bote asu visit bright void visit broken bowl visit caeculu visit canopu visit capella visit caph visit cardax visit cardea visit castor visit cebalrai visit celeborim visit chalawan visit charm visit chikatip visit chimitarp visit chirrayakai visit chornifath visit chychra visit cinxia visit colub visit companion visit convector visit cor caroli visit da ent visit da lest visit dabih visit danoa visit dark hill visit debrugt visit delta capricorni visit delta sagittarii visit delta velorum visit deneb visit denebola visit diphda visit dokdobaru visit dschubba visit dubh visit due yoot visit durax visit eber visit eblumab visit edusa visit ehma ti visit ekkekru visit ekuarik visit elnath visit eltanin visit eneremprukt visit enif visit essprakai visit eshkoshtar visit eteron visit fah root visit fah soom visit fala visit fallen leaf visit far horizon visit farbutero visit farinu visit faronektu visit fasitopfar visit fell omen visit feroteri visit ferukistek visit fingol visit flugbu visit fomalhaut visit fornarep visit four pillar visit furmeliki visit gacrux visit gamma cassiopeia visit gamma corvi visit gienah visit girtab visit glubatub visit gomeisa visit good omen visit gorvi visit graffia visit gupta visit hadar visit hamal visit han visit hassaleh visit hatysa visit heia due visit hesselpost visit hevru hai visit hi yahr visit hintar visit holeb visit homeward visit host visit hunter visit ikkaraka visit ildaria visit imo dep visit insitor visit io low visit io mann visit ips visit iyechyek visit izar visit kachchrai visit kapru visit kaliptari visit kappa centauri visit kashikt visit kasikfar visit kau australi visit kau boreali visit ki war ek visit kiroku visit kirukichi visit kochab visit kor akmari visit kor enlakfar visit kor feltar visit kor men visit kor norp visit kor tarbei visit kor zenai visit kornephoro visit korsmanath visit kraz visit kugel visit kursa visit last word visit lesath visit levana visit limen visit lolami visit lom tahr visit lone cloud visit lucina visit lurata visit makferuti visit markab visit markeb visit matar visit mebla visit mebsuta visit meftarkata visit mei yohn visit mekislepti visit membulem visit men visit menkalinan visit menkar visit menkent visit merak visit mesuket visit miaplacidu visit miblulub visit mimosa visit minkar visit mintaka visit mirach visit mirfak visit mirzam visit mizar visit moktar visit mora visit muhlifain visit muphrid visit nao visit naper visit nashira visit nenia visit nihal visit noct visit nunki visit oblat visit orbona visit orvala visit ossipago visit over the rainbow visit pantica visit parca visit peacock visit pelubta visit peragenor visit peresedersi visit perfica visit persian visit persitar visit phact visit phecda visit pherkad visit phurad visit pikroiyak visit plort visit polari visit pollux visit porrima visit prakachaa visit procyon visit pug iyik visit quaru visit rajak visit rasalhagu visit rastaban visit rati cal visit regor visit regulu visit remembr visit rigel visit ruchbah visit rutilicu visit ruwarku visit sabik visit sabriset visit sadachbia visit sadalmelik visit sadalsuud visit sadr visit sagittariu a visit saiph visit salipastart visit salm visit sarga visit sarin visit sayaiban visit scheat visit schedar visit segesta visit seginu visit seketra visit sepetrosk visit sepriaptu visit sevrelect visit shaula visit sheratan visit siyakku visit sichkaara visit silikatakfar visit silver bell visit silver string visit similisti visit siriu visit situla visit skeruto visit skokarak visit sobarati visit sol visit sol arachi visit sol kimek visit sol saryd visit solifar visit sospi visit speloog visit spica visit steep roof visit stercutu visit suhail visit sumar visit sumprast visit tai visit talita visit tania australi visit taraz visit tarf visit tebuteb visit tejat visit terminu visit terra incognita visit torbab visit tortor visit turai visit ula mon visit ultima thule visit umbral visit unagi visit unukalhai visit uwa fahn visit vega visit vindemiatrix visit volax visit wah ki visit wah oh visit wah yoot visit waypoint visit wazn visit wei visit wezen visit world end visit ya hai visit yed prior visit zaurak visit zeta aquila visit zeta centauri visit zosma visit zuba zub visit zubenelgenubi visit zubenelhakrabi visit zubeneschamali visit planet ember reach visit planet ember threshold visit planet ember wormhol visit planet hai wormhol visit planet pirat wormhol visit planet pug wormhol visit planet quarg wormhol visit planet remnant wormhol visit planet rim wormhol visit planet the eye return to top,finite galaxy finite galaxy is a free and open source space exploration game the repository containing all file is located at httpsgithubcomfinitegalaxyfinitegalaxy it is derived from endless sky a game created by michael zahniser which is located at httpsgithubcomendlessskyendlesssky both game can be installed alongside and played independently of each other although derived from the same source code and basically using the same content finite galaxy and endless sky have diverged and are no longer compatible if you transpose a save game from one to the other you are likely to encounter hundred of error table of content installation gnulinux applemac o x microsoft window introduction change major change minor change not yet implemented idea contributing help wanted posting issue posting pull request reveal entire map installation gnu linux open your terminal and enter to install dependency on archlinux pacman s needed git gcc scons sdl2 libpng libjpegturbo mesa glew openal libmad pango ttflinuxlibertine on debianubuntu sudo aptget install git g scons libsdl2dev libpngdev libjpegdev libgl1mesadev libglewdev libopenaldev libmad0dev libpango fontslinuxlibertine on fedorarhelcentos sudo dnf install git gccc scons sdl2devel libpngdevel libjpegturbodevel mesalibgldevel glewdevel openalsoftdevel libmaddevel pango linuxlibertinefonts replace dnf with yum on some version git clone httpsgithubcomfinitegalaxyfinitegalaxygit to get a local copy of the repository cd finitegalaxy to open the directory git pull to update the game scons to compile the game finitegalaxy to run the game for more help consult the man page the finitegalaxy6 file return to top apple mac o x if you have trouble compiling or encounter error please post here to build finite galaxy you will first need to download xcode from the app store next install homebrew from httpbrewsh once homebrew is installed use it to install the library you will need brew install libpng brew install libjpegturbo brew install libmad brew install sdl2 brew install pango if the version of those library are different from the one that the xcode project is set up for you will need to modify the file path in the framework section in xcode it is possible that you will also need to modify the header search path and library search path in build setting to point to wherever homebrew installed those library library path to create a mac o x binary that will work on system other than your own you may also need to use install_name_tool to modify the library so that their location is relative to the rpath sudo install_name_tool id rpathlibpng1616dylib usrlocalliblibpng1616dylib sudo install_name_tool id rpathlibmad021dylib usrlocalliblibmad021dylib sudo install_name_tool id rpathlibturbojpeg0dylib usrlocaloptlibjpegturboliblibturbojpeg0dylib sudo install_name_tool id rpathlibsdl2200dylib usrlocalliblibsdl2200dylib sudo install_name_tool id rpathpango1447dylib usrlocallibpango1447dylib return to top microsoft window if you have trouble compiling or encounter error please post here acquire the file with git clone httpsgithubcomfinitegalaxyfinitegalaxygit or click download zip and extract it if you dont have it already open finitegalaxyfontslinlibertine_drahttf and install the font or copy it to the appropiate location for building the game see httpsgithubcomendlessskyendlessskywikibuildinstructionswindows return to top introduction why did i start this project why not contribute to endless sky instead although i like endless sky a a whole it also contains thing i dont like conversation scene news and portrait tribute and plundering of installed outfit are just a few example the original creator michael zahniser seemed to disappear and pace of development appeared to slow down there were only nine commits in september 2018 and zero in october i started finite galaxy on october 18 numerous pull request over there have been open for over a year review are haphazard there are many lengthy discussion on unimportant thing while useful proposal were often ignored the direction and vision is not always clear support for plugins is rather limited in short i consider it a better use of time to work on a project where i can incorporate most of my idea where i can remove thing i dislike and where i can contribute whenever i like without having to wait week or month for a review or wasting my time on something that wont be included finite galaxy is very much a work in progress nevertheless it can be compiled and played without error feel free to try it out yourself return to top change major change hyperjump fuel is based on your ship effective mass including cargo and carried ship hyperjump fuel is no longer free it price depends on the planet when landing or the government when hailing ship in space ship continuously consume energy based upon the number of bunk to represent lifesupport ship category are based on total mass ship mass hull mass outfit space cargo space introduced core space reserved for energy generator shield and hull system and hyperdrives outfit space core space engine space weapon space installed outfit can no longer be plundered by default outfit in cargo still can minimum depreciation value raised to 50 time lowered to one year weapon projectile damage is a random number between damage and damage random damage gun fire in parallel by default ie no harmonized angle convergence ship info display show more stats redistributed most human ship and many outfit to have more regional specialization removed tribute from planet relevant code is still present for plugins removed news and portrait relevant code is still present for plugins distance from planet to the sytems centre are trebled a a result space feel larger thruster are more desirable and player wont always land immediately in the middle of a space fight nonmissile weapon have their weapon range increased by about a third exploding ship are significantly more dangerous add support for unicode and different writing direction return to top minor change see changelogtxt ship_overviewtxt and httpsgithubcomfinitegalaxyfinitegalaxycommitsmaster return to top not yet implemented idea add quotation mark preference add licence tab in player info panel add tribute tab in player info panel add manufacturer to ship add functionality to deposit credit at the bank for a fixed time eg one year receiving either the sum plus interest when it expires or the sum minus a penalty when you claim it beforehand add planet landing fee support allow friendly fire allow sorting available job by eg cargo size distance name payment etc allow sorting outfit by cost mass name allow sorting ship by cost mass name outfit space shield etc display flagship speed by default and display target speed with tactical scanner dehardcode jump radius increase jump radius if you have multiple jump drive installed perhaps 100jump drive05 limit the commodity for sale on specific planet make ship explosion weapon automatically proportional to mass base empty or total mass separate fleet overview column in outfitter and shipyard from ship info display separate slot for gun and missile launcher ship entering a system from hyperspace should be positioned near the system centre instead of near the first inhabited planet httpsgithubcomendlessskyendlessskywikidevelopmentroadmap return to top contributing contribution are welcome anyone can contribute feel free to open issue or make pull request help wanted code dehardcode dronefighter class currently a boolean is used to allow for multiple customizable fighterbay type eg small medium large dehardcode hardpoint slot currently a boolean is used to allow for multiple customizable hardpoint type eg gun slot missile bay turret mount implementing the idea listed above other new mechanic that make the game more enjoyable update code to c17 art because my blender skill are nonexistent i could use help from people who are capable and willing to make new outfit sprite and turret hardpoints eg sixgun blaster turret modify existing ship sprite and thumbnail produce ship thumbnail for ship that only have sprite create new ship see open issue miscellaneous unfortunately im unable to test thing on platform other than my own fedora linux bsd macos x and window user could help by trying compiling and running the game and if necessary correct the appropiate file accordingly return to top posting issue the issue page on github is for tracking bug and for art content and feature request when posting a new issue please be polite and always assume good faith check to make sure it not a duplicate of an existing issue create a separate issue for each bug problem question or request if requesting a new feature first ask yourself will this make the game more fun or interesting remember that this is a game not a simulator change will not be made purely for the sake of realism especially if they introduce needle complexity or aggravation if you believe your issue ha been resolved you can close the issue yourself return to top posting pull request if you are posting a pull request please do not combine multiple unrelated change into a single pull check the diff and make sure the pull request doe not contain unintended change if proposing a major pull request start by posting an issue and discussing the best way to implement it often the first strategy that occurs to you will not be the cleanest or most effective way to implement a new feature code follow the coding standard c14 do not use tab use two space instead make number with many digit easier to read for human by inserting decimal number at interval of three digit if there are more than four in a row hexadecimal number at interval of four digit if there are more than six in a row use oxford english data do not use tab use two space instead use oxford spelling the variant of english used by many scientific journal and international organization such a the united nation instead of american british canadian or other national variety no diacritic in english a same for other vowel aa ae oe ue ii c ch sh nh s text string conversation description mission dialogue tooltips etc avoid abbreviation eg ie etc contraction isnt are fine use an oxford comma when giving more than two item eg one two and three not one two and three use u2013 endash for number range eg 1012 and for parenthetical expression like this instead of parenthesis emdashes or hyphen use u2026 horizontal ellipsis instead of three full stop use u202f narrow nonbreaking space a a thousand separator for number with five or more digit eg 12 345 use u2212 minus sign for negative number subtraction and deduction use the quote for direct speech and apostrophe within direct speech the source code replaces these with proper primary and secondary opening and closing quotation mark surround such string with backticks repeatedly check and double check any new or changed string to avoid unnecessary typo eg mind the difference between it cf he shes and it cf his her image file name are lower case and use underscore instead of space add both normal and 2x version for ship also create thumbnail for turret also create hardpoints insert yourself in the copyrighttxt file include all asset blender gimp other file in the opening post sound file name are lower case and use underscore instead of space insert yourself in the copyrighttxt file return to top reveal entire map part of the fun of the game is travelling around and exploring however if you dont have time for that and simply want to reveal everything in the entire galaxy then open your save game find what you know and insert the following line directly afterwards visited 1 axis visited 10 pole visited 11 autumn above visited 11 spring below visited 12 autumn above visited 14 pole visited 14 summer above visited 14 winter below visited 16 autumn rising visited 3 axis visited 3 pole visited 3 spring rising visited 4 axis visited 4 spring rising visited 4 summer rising visited 4 winter rising visited 5 axis visited 5 spring below visited 5 summer above visited 5 winter above visited 7 autumn rising visited 8 winter below visited 9 spring above visited ablodab visited ablub visited acamar visited achernar visited acrux visited adhara visited aescolanus visited al dhanab visited albaldah visited albireo visited alcyone visited aldebaran visited alderamin visited aldhibain visited algedi visited algenib visited algenubi visited algieba visited algol visited algorel visited alheka visited alhena visited alioth visited alkaid visited almaaz visited almach visited alnair visited alnasl visited alnilam visited alnitak visited alniyat visited alpha arae visited alpha centauri visited alpha hydri visited alphard visited alphecca visited alpheratz visited altair visited aludra visited ancient hope visited ankaa visited answer visited antares visited antevorta visited aparak visited arcturus visited arculus visited arneb visited ascella visited asikafarnut visited aspidiske visited atrium visited avior visited ayakk visited beginning visited bellatrix visited belonging visited belug visited belugt visited beta lupi visited betelgeuse visited bloptab visited blubipad visited blugtad visited boral visited bore fah visited bote asu visited bright void visited broken bowl visited caeculus visited canopus visited capella visited caph visited cardax visited cardea visited castor visited cebalrai visited celeborim visited chalawan visited charm visited chikatip visited chimitarp visited chirrayakai visited chornifath visited chychra visited cinxia visited coluber visited companion visited convector visited cor carolus visited da ent visited da lest visited dabih visited danoa visited dark hill visited debrugt visited delta capricorni visited delta sagittarii visited delta velorum visited deneb visited denebola visited diphda visited dokdobaru visited dschubba visited dubhe visited due yoot visited durax visited eber visited eblumab visited edusa visited ehma ti visited ekkekru visited ekuarik visited elnath visited eltanin visited eneremprukt visited enif visited essprakai visited eshkoshtar visited eteron visited fah root visited fah soom visited fala visited fallen leaf visited far horizon visited farbutero visited farinus visited faronektu visited fasitopfar visited fell omen visited feroteri visited ferukistek visited fingol visited flugbu visited fomalhaut visited fornarep visited four pillar visited furmeliki visited gacrux visited gamma cassiopeiae visited gamma corvi visited gienah visited girtab visited glubatub visited gomeisa visited good omen visited gorvi visited graffias visited gupta visited hadar visited hamal visited han visited hassaleh visited hatysa visited heia due visited hesselpost visited hevru hai visited hi yahr visited hintar visited holeb visited homeward visited host visited hunter visited ikkaraka visited ildaria visited imo dep visited insitor visited io lowe visited io mann visited ipsing visited iyechyek visited izar visited kachchrai visited kapru visited kaliptari visited kappa centauri visited kashikt visited kasikfar visited kaus australis visited kaus borealis visited ki war ek visited kiroku visited kirukichi visited kochab visited kor akmari visited kor enlakfar visited kor feltar visited kor men visited kor norpeli visited kor tarbei visited kor zenai visited kornephoros visited korsmanath visited kraz visited kugel visited kursa visited last word visited lesath visited levana visited limen visited lolami visited lom tahr visited lone cloud visited lucina visited lurata visited makferuti visited markab visited markeb visited matar visited mebla visited mebsuta visited meftarkata visited mei yohn visited mekislepti visited membulem visited men visited menkalinan visited menkar visited menkent visited merak visited mesuket visited miaplacidus visited miblulub visited mimosa visited minkar visited mintaka visited mirach visited mirfak visited mirzam visited mizar visited moktar visited mora visited muhlifain visited muphrid visited naos visited naper visited nashira visited nenia visited nihal visited nocte visited nunki visited oblate visited orbona visited orvala visited ossipago visited over the rainbow visited pantica visited parca visited peacock visited pelubta visited peragenor visited peresedersi visited perfica visited persian visited persitar visited phact visited phecda visited pherkad visited phurad visited pikroiyak visited plort visited polaris visited pollux visited porrima visited prakachaa visited procyon visited pug iyik visited quaru visited rajak visited rasalhague visited rastaban visited rati cal visited regor visited regulus visited remembrance visited rigel visited ruchbah visited rutilicus visited ruwarku visited sabik visited sabriset visited sadachbia visited sadalmelik visited sadalsuud visited sadr visited sagittarius a visited saiph visited salipastart visited salm visited sargas visited sarin visited sayaiban visited scheat visited schedar visited segesta visited seginus visited seketra visited sepetrosk visited sepriaptu visited sevrelect visited shaula visited sheratan visited siyakku visited sichkaara visited silikatakfar visited silver bell visited silver string visited similisti visited sirius visited situla visited skeruto visited skokarak visited sobarati visited sol visited sol arachi visited sol kimek visited sol saryds visited solifar visited sospi visited speloog visited spica visited steep roof visited stercutus visited suhail visited sumar visited sumprast visited tai visited talita visited tania australis visited tarazed visited tarf visited tebuteb visited tejat visited terminus visited terra incognita visited torbab visited tortor visited turais visited ula mon visited ultima thule visited umbral visited unagi visited unukalhai visited uwa fahn visited vega visited vindemiatrix visited volax visited wah ki visited wah oh visited wah yoot visited waypoint visited wazn visited wei visited wezen visited world end visited ya hai visited yed prior visited zaurak visited zeta aquilae visited zeta centauri visited zosma visited zuba zub visited zubenelgenubi visited zubenelhakrabi visited zubeneschamali visited planet ember reach visited planet ember threshold visited planet ember wormhole visited planet hai wormhole visited planet pirate wormhole visited planet pug wormhole visited planet quarg wormhole visited planet remnant wormhole visited planet rim wormhole visited planet the eye return to top,finite galaxy finite galaxy free open source space exploration game repository containing file located httpsgithubcomfinitegalaxyfinitegalaxy derived endless sky game created michael zahniser located httpsgithubcomendlessskyendlesssky game installed alongside played independently although derived source code basically using content finite galaxy endless sky diverged longer compatible transpose save game one likely encounter hundred error table content installation gnulinux applemac o x microsoft window introduction change major change minor change yet implemented idea contributing help wanted posting issue posting pull request reveal entire map installation gnu linux open terminal enter install dependency archlinux pacman needed git gcc scons sdl2 libpng libjpegturbo mesa glew openal libmad pango ttflinuxlibertine debianubuntu sudo aptget install git g scons libsdl2dev libpngdev libjpegdev libgl1mesadev libglewdev libopenaldev libmad0dev libpango fontslinuxlibertine fedorarhelcentos sudo dnf install git gccc scons sdl2devel libpngdevel libjpegturbodevel mesalibgldevel glewdevel openalsoftdevel libmaddevel pango linuxlibertinefonts replace dnf yum version git clone httpsgithubcomfinitegalaxyfinitegalaxygit get local copy repository cd finitegalaxy open directory git pull update game scons compile game finitegalaxy run game help consult man page finitegalaxy6 file return top apple mac o x trouble compiling encounter error please post build finite galaxy first need download xcode app store next install homebrew httpbrewsh homebrew installed use install library need brew install libpng brew install libjpegturbo brew install libmad brew install sdl2 brew install pango version library different one xcode project set need modify file path framework section xcode possible also need modify header search path library search path build setting point wherever homebrew installed library library path create mac o x binary work system may also need use install_name_tool modify library location relative rpath sudo install_name_tool id rpathlibpng1616dylib usrlocalliblibpng1616dylib sudo install_name_tool id rpathlibmad021dylib usrlocalliblibmad021dylib sudo install_name_tool id rpathlibturbojpeg0dylib usrlocaloptlibjpegturboliblibturbojpeg0dylib sudo install_name_tool id rpathlibsdl2200dylib usrlocalliblibsdl2200dylib sudo install_name_tool id rpathpango1447dylib usrlocallibpango1447dylib return top microsoft window trouble compiling encounter error please post acquire file git clone httpsgithubcomfinitegalaxyfinitegalaxygit click download zip extract dont already open finitegalaxyfontslinlibertine_drahttf install font copy appropiate location building game see httpsgithubcomendlessskyendlessskywikibuildinstructionswindows return top introduction start project contribute endless sky instead although like endless sky whole also contains thing dont like conversation scene news portrait tribute plundering installed outfit example original creator michael zahniser seemed disappear pace development appeared slow nine commits september 2018 zero october started finite galaxy october 18 numerous pull request open year review haphazard many lengthy discussion unimportant thing useful proposal often ignored direction vision always clear support plugins rather limited short consider better use time work project incorporate idea remove thing dislike contribute whenever like without wait week month review wasting time something wont included finite galaxy much work progress nevertheless compiled played without error feel free try return top change major change hyperjump fuel based ship effective mass including cargo carried ship hyperjump fuel longer free price depends planet landing government hailing ship space ship continuously consume energy based upon number bunk represent lifesupport ship category based total mass ship mass hull mass outfit space cargo space introduced core space reserved energy generator shield hull system hyperdrives outfit space core space engine space weapon space installed outfit longer plundered default outfit cargo still minimum depreciation value raised 50 time lowered one year weapon projectile damage random number damage damage random damage gun fire parallel default ie harmonized angle convergence ship info display show stats redistributed human ship many outfit regional specialization removed tribute planet relevant code still present plugins removed news portrait relevant code still present plugins distance planet sytems centre trebled result space feel larger thruster desirable player wont always land immediately middle space fight nonmissile weapon weapon range increased third exploding ship significantly dangerous add support unicode different writing direction return top minor change see changelogtxt ship_overviewtxt httpsgithubcomfinitegalaxyfinitegalaxycommitsmaster return top yet implemented idea add quotation mark preference add licence tab player info panel add tribute tab player info panel add manufacturer ship add functionality deposit credit bank fixed time eg one year receiving either sum plus interest expires sum minus penalty claim beforehand add planet landing fee support allow friendly fire allow sorting available job eg cargo size distance name payment etc allow sorting outfit cost mass name allow sorting ship cost mass name outfit space shield etc display flagship speed default display target speed tactical scanner dehardcode jump radius increase jump radius multiple jump drive installed perhaps 100jump drive05 limit commodity sale specific planet make ship explosion weapon automatically proportional mass base empty total mass separate fleet overview column outfitter shipyard ship info display separate slot gun missile launcher ship entering system hyperspace positioned near system centre instead near first inhabited planet httpsgithubcomendlessskyendlessskywikidevelopmentroadmap return top contributing contribution welcome anyone contribute feel free open issue make pull request help wanted code dehardcode dronefighter class currently boolean used allow multiple customizable fighterbay type eg small medium large dehardcode hardpoint slot currently boolean used allow multiple customizable hardpoint type eg gun slot missile bay turret mount implementing idea listed new mechanic make game enjoyable update code c17 art blender skill nonexistent could use help people capable willing make new outfit sprite turret hardpoints eg sixgun blaster turret modify existing ship sprite thumbnail produce ship thumbnail ship sprite create new ship see open issue miscellaneous unfortunately im unable test thing platform fedora linux bsd macos x window user could help trying compiling running game necessary correct appropiate file accordingly return top posting issue issue page github tracking bug art content feature request posting new issue please polite always assume good faith check make sure duplicate existing issue create separate issue bug problem question request requesting new feature first ask make game fun interesting remember game simulator change made purely sake realism especially introduce needle complexity aggravation believe issue resolved close issue return top posting pull request posting pull request please combine multiple unrelated change single pull check diff make sure pull request contain unintended change proposing major pull request start posting issue discussing best way implement often first strategy occurs cleanest effective way implement new feature code follow coding standard c14 use tab use two space instead make number many digit easier read human inserting decimal number interval three digit four row hexadecimal number interval four digit six row use oxford english data use tab use two space instead use oxford spelling variant english used many scientific journal international organization united nation instead american british canadian national variety diacritic english vowel aa ae oe ue ii c ch sh nh s text string conversation description mission dialogue tooltips etc avoid abbreviation eg ie etc contraction isnt fine use oxford comma giving two item eg one two three one two three use u2013 endash number range eg 1012 parenthetical expression like instead parenthesis emdashes hyphen use u2026 horizontal ellipsis instead three full stop use u202f narrow nonbreaking space thousand separator number five digit eg 12 345 use u2212 minus sign negative number subtraction deduction use quote direct speech apostrophe within direct speech source code replaces proper primary secondary opening closing quotation mark surround string backticks repeatedly check double check new changed string avoid unnecessary typo eg mind difference cf he shes cf image file name lower case use underscore instead space add normal 2x version ship also create thumbnail turret also create hardpoints insert copyrighttxt file include asset blender gimp file opening post sound file name lower case use underscore instead space insert copyrighttxt file return top reveal entire map part fun game travelling around exploring however dont time simply want reveal everything entire galaxy open save game find know insert following line directly afterwards visited 1 axis visited 10 pole visited 11 autumn visited 11 spring visited 12 autumn visited 14 pole visited 14 summer visited 14 winter visited 16 autumn rising visited 3 axis visited 3 pole visited 3 spring rising visited 4 axis visited 4 spring rising visited 4 summer rising visited 4 winter rising visited 5 axis visited 5 spring visited 5 summer visited 5 winter visited 7 autumn rising visited 8 winter visited 9 spring visited ablodab visited ablub visited acamar visited achernar visited acrux visited adhara visited aescolanus visited al dhanab visited albaldah visited albireo visited alcyone visited aldebaran visited alderamin visited aldhibain visited algedi visited algenib visited algenubi visited algieba visited algol visited algorel visited alheka visited alhena visited alioth visited alkaid visited almaaz visited almach visited alnair visited alnasl visited alnilam visited alnitak visited alniyat visited alpha arae visited alpha centauri visited alpha hydri visited alphard visited alphecca visited alpheratz visited altair visited aludra visited ancient hope visited ankaa visited answer visited antares visited antevorta visited aparak visited arcturus visited arculus visited arneb visited ascella visited asikafarnut visited aspidiske visited atrium visited avior visited ayakk visited beginning visited bellatrix visited belonging visited belug visited belugt visited beta lupi visited betelgeuse visited bloptab visited blubipad visited blugtad visited boral visited bore fah visited bote asu visited bright void visited broken bowl visited caeculus visited canopus visited capella visited caph visited cardax visited cardea visited castor visited cebalrai visited celeborim visited chalawan visited charm visited chikatip visited chimitarp visited chirrayakai visited chornifath visited chychra visited cinxia visited coluber visited companion visited convector visited cor carolus visited da ent visited da lest visited dabih visited danoa visited dark hill visited debrugt visited delta capricorni visited delta sagittarii visited delta velorum visited deneb visited denebola visited diphda visited dokdobaru visited dschubba visited dubhe visited due yoot visited durax visited eber visited eblumab visited edusa visited ehma ti visited ekkekru visited ekuarik visited elnath visited eltanin visited eneremprukt visited enif visited essprakai visited eshkoshtar visited eteron visited fah root visited fah soom visited fala visited fallen leaf visited far horizon visited farbutero visited farinus visited faronektu visited fasitopfar visited fell omen visited feroteri visited ferukistek visited fingol visited flugbu visited fomalhaut visited fornarep visited four pillar visited furmeliki visited gacrux visited gamma cassiopeiae visited gamma corvi visited gienah visited girtab visited glubatub visited gomeisa visited good omen visited gorvi visited graffias visited gupta visited hadar visited hamal visited han visited hassaleh visited hatysa visited heia due visited hesselpost visited hevru hai visited hi yahr visited hintar visited holeb visited homeward visited host visited hunter visited ikkaraka visited ildaria visited imo dep visited insitor visited io lowe visited io mann visited ipsing visited iyechyek visited izar visited kachchrai visited kapru visited kaliptari visited kappa centauri visited kashikt visited kasikfar visited kaus australis visited kaus borealis visited ki war ek visited kiroku visited kirukichi visited kochab visited kor akmari visited kor enlakfar visited kor feltar visited kor men visited kor norpeli visited kor tarbei visited kor zenai visited kornephoros visited korsmanath visited kraz visited kugel visited kursa visited last word visited lesath visited levana visited limen visited lolami visited lom tahr visited lone cloud visited lucina visited lurata visited makferuti visited markab visited markeb visited matar visited mebla visited mebsuta visited meftarkata visited mei yohn visited mekislepti visited membulem visited men visited menkalinan visited menkar visited menkent visited merak visited mesuket visited miaplacidus visited miblulub visited mimosa visited minkar visited mintaka visited mirach visited mirfak visited mirzam visited mizar visited moktar visited mora visited muhlifain visited muphrid visited naos visited naper visited nashira visited nenia visited nihal visited nocte visited nunki visited oblate visited orbona visited orvala visited ossipago visited rainbow visited pantica visited parca visited peacock visited pelubta visited peragenor visited peresedersi visited perfica visited persian visited persitar visited phact visited phecda visited pherkad visited phurad visited pikroiyak visited plort visited polaris visited pollux visited porrima visited prakachaa visited procyon visited pug iyik visited quaru visited rajak visited rasalhague visited rastaban visited rati cal visited regor visited regulus visited remembrance visited rigel visited ruchbah visited rutilicus visited ruwarku visited sabik visited sabriset visited sadachbia visited sadalmelik visited sadalsuud visited sadr visited sagittarius visited saiph visited salipastart visited salm visited sargas visited sarin visited sayaiban visited scheat visited schedar visited segesta visited seginus visited seketra visited sepetrosk visited sepriaptu visited sevrelect visited shaula visited sheratan visited siyakku visited sichkaara visited silikatakfar visited silver bell visited silver string visited similisti visited sirius visited situla visited skeruto visited skokarak visited sobarati visited sol visited sol arachi visited sol kimek visited sol saryds visited solifar visited sospi visited speloog visited spica visited steep roof visited stercutus visited suhail visited sumar visited sumprast visited tai visited talita visited tania australis visited tarazed visited tarf visited tebuteb visited tejat visited terminus visited terra incognita visited torbab visited tortor visited turais visited ula mon visited ultima thule visited umbral visited unagi visited unukalhai visited uwa fahn visited vega visited vindemiatrix visited volax visited wah ki visited wah oh visited wah yoot visited waypoint visited wazn visited wei visited wezen visited world end visited ya hai visited yed prior visited zaurak visited zeta aquilae visited zeta centauri visited zosma visited zuba zub visited zubenelgenubi visited zubenelhakrabi visited zubeneschamali visited planet ember reach visited planet ember threshold visited planet ember wormhole visited planet hai wormhole visited planet pirate wormhole visited planet pug wormhole visited planet quarg wormhole visited planet remnant wormhole visited planet rim wormhole visited planet eye return top
C++ ,"DeSyDe
DeSyDe is a design space exploration tool developed at KTH (ForSyDe research group).
Releases:


latest:

Release for our DSD'18 publication + user tutorial



previous:

Release for our TODAES article
Release for our RAPIDO'17 publication



(Almost) hassle-free installation
You need to install DeSyDe via the automated build scripts. We have
tried assuring an (almost) fully-automated installation process,
especially for Linux machines. The idea is quite simple: the script
downloads almost everything necessary so that nothing on your system
is touched and then proceeds to compile everything. This sandboxing
comes with the cost of added compilation time, but since this should
be a one-time process, the larger time frame is a good trade-off for
flexibility.
The only dependency that is not cloned directly from its repo and
compiled alongside DeSyDe is Qt, as DeSyDe currently does not make
Gecode's Gist optional. Please ensure that you have the basic
development files for Qt installed and reachable in your machine. In
future releases this necessity will be removed.
If you are on any debian-based distro with reasonably updated
packages, you should be good to go by issuing the following install
command (do not forget to prepend sudo if necessary):
apt install automake libtool qt5-default

As of 2019-06-05, it seems from user feedback that on ubuntu and other
derived distros not all dependencies are pulled with these commands, so
it may be necessary to install qtcreator to be able to compile DeSyDe
(do not forget to prepend sudo if necessary):
apt install qtcreator

Then, a make followed by make install should do the trick. Tested
on Linux Mint 18.3 and Debian 10.
Usage
Please follow the tutorial for more details on how
to use the tool and how to interpret its output.
Running the Experiments
The experiments provided in the examples folder represent those that are still functional and were
used as proof of concepts into previous papers this project was involved. For a step-by-step tutorial
on how to setup your own experiment, check out the tutorial provided in this repo.
Included examples

DSD18: experiments from our DSD'18 dealing with TDN NoCs exploration that optimize power while respecting real time constraints.
ScalAnalysis: folder containing scripts that generates experiments for different sized NoCs platforms based on a template extracted from DSD18.
tutorial: the files used for the user tutorial.

Publications
Kathrin Rosvall, Tage Mohammadat, George Ungureanu, Johnny Öberg, and Ingo Sander. “Exploring Power and Throughput for Dataflow Applications on Predictable NoC Multiprocessors,” 719–26, 2018.
Kathrin Rosvall, Nima Khalilzad, George Ungureanu, and Ingo Sander. Throughput propagation in constraint-based design space exploration for mixed-criticality systems. In Proceedings of the 2017 Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools (RAPIDO '17), Stockholm, Sweden. ACM, January 2017.
Nima Khalilzad, Kathrin Rosvall, and Ingo Sander. A modular design space exploration framework for multiprocessor real-time systems. In Forum on specification & Design Languages (FDL '16), Bremen, Germany. IEEE, September 2016.
Kathrin Rosvall and Ingo Sander. A constraint-based design space exploration framework for real-time applications on MPSoCs. In Design Automation and Test in Europe (DATE '14), Dresden, Germany, Mar. 2014.
",desyd desyd is a design space explor tool develop at kth forsyd research group releas latest releas for our dsd18 public user tutori previou releas for our toda articl releas for our rapido17 public almost hasslefre instal you need to instal desyd via the autom build script we have tri assur an almost fullyautom instal process especi for linux machin the idea is quit simpl the script download almost everyth necessari so that noth on your system is touch and then proce to compil everyth thi sandbox come with the cost of ad compil time but sinc thi should be a onetim process the larger time frame is a good tradeoff for flexibl the onli depend that is not clone directli from it repo and compil alongsid desyd is qt as desyd current doe not make gecod gist option pleas ensur that you have the basic develop file for qt instal and reachabl in your machin in futur releas thi necess will be remov if you are on ani debianbas distro with reason updat packag you should be good to go by issu the follow instal command do not forget to prepend sudo if necessari apt instal automak libtool qt5default as of 20190605 it seem from user feedback that on ubuntu and other deriv distro not all depend are pull with these command so it may be necessari to instal qtcreator to be abl to compil desyd do not forget to prepend sudo if necessari apt instal qtcreator then a make follow by make instal should do the trick test on linux mint 183 and debian 10 usag pleas follow the tutori for more detail on how to use the tool and how to interpret it output run the experi the experi provid in the exampl folder repres those that are still function and were use as proof of concept into previou paper thi project wa involv for a stepbystep tutori on how to setup your own experi check out the tutori provid in thi repo includ exampl dsd18 experi from our dsd18 deal with tdn noc explor that optim power while respect real time constraint scalanalysi folder contain script that gener experi for differ size noc platform base on a templat extract from dsd18 tutori the file use for the user tutori public kathrin rosval tage mohammadat georg ungureanu johnni berg and ingo sander explor power and throughput for dataflow applic on predict noc multiprocessor 71926 2018 kathrin rosval nima khalilzad georg ungureanu and ingo sander throughput propag in constraintbas design space explor for mixedcrit system in proceed of the 2017 workshop on rapid simul and perform evalu method and tool rapido 17 stockholm sweden acm januari 2017 nima khalilzad kathrin rosval and ingo sander a modular design space explor framework for multiprocessor realtim system in forum on specif design languag fdl 16 bremen germani ieee septemb 2016 kathrin rosval and ingo sander a constraintbas design space explor framework for realtim applic on mpsoc in design autom and test in europ date 14 dresden germani mar 2014,desyde desyde is a design space exploration tool developed at kth forsyde research group release latest release for our dsd18 publication user tutorial previous release for our todaes article release for our rapido17 publication almost hasslefree installation you need to install desyde via the automated build script we have tried assuring an almost fullyautomated installation process especially for linux machine the idea is quite simple the script downloads almost everything necessary so that nothing on your system is touched and then proceeds to compile everything this sandboxing come with the cost of added compilation time but since this should be a onetime process the larger time frame is a good tradeoff for flexibility the only dependency that is not cloned directly from it repo and compiled alongside desyde is qt a desyde currently doe not make gecodes gist optional please ensure that you have the basic development file for qt installed and reachable in your machine in future release this necessity will be removed if you are on any debianbased distro with reasonably updated package you should be good to go by issuing the following install command do not forget to prepend sudo if necessary apt install automake libtool qt5default a of 20190605 it seems from user feedback that on ubuntu and other derived distros not all dependency are pulled with these command so it may be necessary to install qtcreator to be able to compile desyde do not forget to prepend sudo if necessary apt install qtcreator then a make followed by make install should do the trick tested on linux mint 183 and debian 10 usage please follow the tutorial for more detail on how to use the tool and how to interpret it output running the experiment the experiment provided in the example folder represent those that are still functional and were used a proof of concept into previous paper this project wa involved for a stepbystep tutorial on how to setup your own experiment check out the tutorial provided in this repo included example dsd18 experiment from our dsd18 dealing with tdn noc exploration that optimize power while respecting real time constraint scalanalysis folder containing script that generates experiment for different sized noc platform based on a template extracted from dsd18 tutorial the file used for the user tutorial publication kathrin rosvall tage mohammadat george ungureanu johnny berg and ingo sander exploring power and throughput for dataflow application on predictable noc multiprocessor 71926 2018 kathrin rosvall nima khalilzad george ungureanu and ingo sander throughput propagation in constraintbased design space exploration for mixedcriticality system in proceeding of the 2017 workshop on rapid simulation and performance evaluation method and tool rapido 17 stockholm sweden acm january 2017 nima khalilzad kathrin rosvall and ingo sander a modular design space exploration framework for multiprocessor realtime system in forum on specification design language fdl 16 bremen germany ieee september 2016 kathrin rosvall and ingo sander a constraintbased design space exploration framework for realtime application on mpsocs in design automation and test in europe date 14 dresden germany mar 2014,desyde desyde design space exploration tool developed kth forsyde research group release latest release dsd18 publication user tutorial previous release todaes article release rapido17 publication almost hasslefree installation need install desyde via automated build script tried assuring almost fullyautomated installation process especially linux machine idea quite simple script downloads almost everything necessary nothing system touched proceeds compile everything sandboxing come cost added compilation time since onetime process larger time frame good tradeoff flexibility dependency cloned directly repo compiled alongside desyde qt desyde currently make gecodes gist optional please ensure basic development file qt installed reachable machine future release necessity removed debianbased distro reasonably updated package good go issuing following install command forget prepend sudo necessary apt install automake libtool qt5default 20190605 seems user feedback ubuntu derived distros dependency pulled command may necessary install qtcreator able compile desyde forget prepend sudo necessary apt install qtcreator make followed make install trick tested linux mint 183 debian 10 usage please follow tutorial detail use tool interpret output running experiment experiment provided example folder represent still functional used proof concept previous paper project involved stepbystep tutorial setup experiment check tutorial provided repo included example dsd18 experiment dsd18 dealing tdn noc exploration optimize power respecting real time constraint scalanalysis folder containing script generates experiment different sized noc platform based template extracted dsd18 tutorial file used user tutorial publication kathrin rosvall tage mohammadat george ungureanu johnny berg ingo sander exploring power throughput dataflow application predictable noc multiprocessor 71926 2018 kathrin rosvall nima khalilzad george ungureanu ingo sander throughput propagation constraintbased design space exploration mixedcriticality system proceeding 2017 workshop rapid simulation performance evaluation method tool rapido 17 stockholm sweden acm january 2017 nima khalilzad kathrin rosvall ingo sander modular design space exploration framework multiprocessor realtime system forum specification design language fdl 16 bremen germany ieee september 2016 kathrin rosvall ingo sander constraintbased design space exploration framework realtime application mpsocs design automation test europe date 14 dresden germany mar 2014
C++ ,"Tux in Space
Tux in Space: space exploration game
This program is a simulation game. The program simulates the moving, under
the phisic's laws, of planets, spaceships, suns, and everything else in
deep space.
Highlighted features:

A wide gerarchic collection of various object types with different
behaviours
A phisic engine that simulates the gravity force and different types of
impacts between objects
Space Monsters with basic AI

test/use the program:
There is an executable file, compiled in a 64 bit Linux; Maybe can run
in other computers as well, but you can easily compile the program with
the makefile (just run 'make' in the master directory).
The program is written for linux only, but with very little work or maybe no work at all could also
run on other operative systems. (See information file)
Officially supported compiler is gcc >= 6.3.
For developers is suggested to use the latest version in the
master branch, for normal users the latest release.
Status
The program is far far away from being complete.
License
Tux in Space - space exploration game
Copyright (C) 2016-2017 emanuele.sorce@hotmail.com
This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, version 3 or compatible.
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty or
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc.
",tux in space tux in space space explor game thi program is a simul game the program simul the move under the phisic law of planet spaceship sun and everyth els in deep space highlight featur a wide gerarch collect of variou object type with differ behaviour a phisic engin that simul the graviti forc and differ type of impact between object space monster with basic ai testus the program there is an execut file compil in a 64 bit linux mayb can run in other comput as well but you can easili compil the program with the makefil just run make in the master directori the program is written for linux onli but with veri littl work or mayb no work at all could also run on other oper system see inform file offici support compil is gcc 63 for develop is suggest to use the latest version in the master branch for normal user the latest releas statu the program is far far away from be complet licens tux in space space explor game copyright c 20162017 emanuelesorcehotmailcom thi program is free softwar you can redistribut it andor modifi it under the term of the gnu gener public licens as publish by the free softwar foundat version 3 or compat thi program is distribut in the hope that it will be use but without ani warranti without even the impli warranti or merchant or fit for a particular purpos see the gnu gener public licens for more detail you should have receiv a copi of the gnu gener public licens along with thi program if not write to the free softwar foundat inc,tux in space tux in space space exploration game this program is a simulation game the program simulates the moving under the phisics law of planet spaceship sun and everything else in deep space highlighted feature a wide gerarchic collection of various object type with different behaviour a phisic engine that simulates the gravity force and different type of impact between object space monster with basic ai testuse the program there is an executable file compiled in a 64 bit linux maybe can run in other computer a well but you can easily compile the program with the makefile just run make in the master directory the program is written for linux only but with very little work or maybe no work at all could also run on other operative system see information file officially supported compiler is gcc 63 for developer is suggested to use the latest version in the master branch for normal user the latest release status the program is far far away from being complete license tux in space space exploration game copyright c 20162017 emanuelesorcehotmailcom this program is free software you can redistribute it andor modify it under the term of the gnu general public license a published by the free software foundation version 3 or compatible this program is distributed in the hope that it will be useful but without any warranty without even the implied warranty or merchantability or fitness for a particular purpose see the gnu general public license for more detail you should have received a copy of the gnu general public license along with this program if not write to the free software foundation inc,tux space tux space space exploration game program simulation game program simulates moving phisics law planet spaceship sun everything else deep space highlighted feature wide gerarchic collection various object type different behaviour phisic engine simulates gravity force different type impact object space monster basic ai testuse program executable file compiled 64 bit linux maybe run computer well easily compile program makefile run make master directory program written linux little work maybe work could also run operative system see information file officially supported compiler gcc 63 developer suggested use latest version master branch normal user latest release status program far far away complete license tux space space exploration game copyright c 20162017 emanuelesorcehotmailcom program free software redistribute andor modify term gnu general public license published free software foundation version 3 compatible program distributed hope useful without warranty without even implied warranty merchantability fitness particular purpose see gnu general public license detail received copy gnu general public license along program write free software foundation inc
C++ ,"turbo-ros-pkg
ROS software repository, Robotics & Biology Laboratory, TU Berlin
",turborospkg ro softwar repositori robot biolog laboratori tu berlin,turborospkg ro software repository robotics biology laboratory tu berlin,turborospkg ro software repository robotics biology laboratory tu berlin
C++ ,"DIY spectrophotometer
How to use it ?
Please check the userguide to obtain all the useful information about the user interface.
Goal of the project
This project was done at the Hackuarium association, a DIY biology hacking-space and open laboratory located in Ecublens (Switzerland). Feel free to contact us if you would like additional information or would like to buy a fully built and tested spectrophotometer.
Recently, the open-source and open-hardware community has been involved in the creation of open scientific tools. In this context, a few spectrophotometer projects emerged. However, none of them was sufficiently evolved to be casually used in a laboratory. It is for this reason that we started this project.
Our goal was to create a simple spectrophotometer that measures the
absorbance of a sample for 3 different light colors (red, green, blue), that is
self-contained (battery powered) and displays the result on a LCD screen. In addition, the tool had to be reliable, precise and cheap.
The initial idea was that you could teach spectrophotometry without
having to buy an expensive instrument, since you can find experiments
that do not require a specific light wavelength. This includes
optical density for bacteria culture, determination of a pigment's concentration in
a solution, determination of the kinetic of a reaction and many others.
The device is rather cheap, for you can buy all the components on AliExpress
(but one) and either 3D print or laser cut the case (MDF or acrylic glass). When constructing 10, the price
per spectrophotometer is around $30. However if you want to have a final
product with an aluminium case and PCBs already assembled, the cost would rather be $90 each if you order 20 of them.

Can we do science with this ?
Before trying to explain how it is done more in detail maybe the first
question that we should answer is : can we do science with this tool?
When thinking about the design of the spectro, we found an light sensor that converts the light energy to frequency and that is linear on a range of nearly 10^6. This means that the luminosity range that we can measure precisely is really big. Only the precision of the component is therefore really promising and shows that we could do something that gives good results.
And indeed, the final product gives really accurate results in fields like:

Chemistry
Biology

And other, which you can see on our lab notebook.
Some of our results
Patent blue V
To begin we did a very simple experiment that allows to measure the absorbance of solutions containing different known concentrations of patent blue V (E131), a blue pigment that is used in various blue candies.

The result is rather good knowing that the solutions were simply prepared by adding various volumes (0 to 2 mL) of a concentrated pigmented solution with a 1mL seringe to 100mL of water. Not the most accurate method...
Escherichia coli
Another feature of the simple-spectrophotometer is that it works on a rechargeable battery (autonomy of 48 hours). Consequently, you can measure a kinetic by placing the device directly inside an incubator, which is not feasible with a regular commercial spectrophotometer.
Optical density (OD) at 600nm is often used in biology to determine the biomass increase versus time. In this experiment we have placed Escherichia coli with cell growing media directly in the spectrophotometer cuvette and placed the spectrophotometer itself on a shaker in the incubator.
We can then program it to measure the absorbance every X seconds. In our case we measured the absorbance every 30 minutes (up to 40 measures). The following curve was obtained.

This experiment clearly shows the log phase and the stationary phase. But the growth was faster than expected. An acquisition every 15 minutes would have been a better choice.
Using the simple spectrophotometer to teach science
The use of the spectrophotometer is rather simple. Just put a blank, press the button, wait 10s and put the sample. The result is displayed on the screen. In addition, it could be open so that students could see how it is inside and could really understand how the tool works (avoiding the ""black box"" effect). Also, since it is inexpensive, having ten of these for a classroom would be possible, which would allow students to have one instrument per pair.
Using the spectrophotometer
For all this reason, we also consider our instrument as a pedagogical tool. This allowed us to use it to teach some basic scientific concepts to groups of children and teenagers.
School in Mondomo (Colombia)
How to make it ?
The electronic design is compatible with the Arduino platform. We use an
ATMEGA32U4 microcontroller which is directly connected to the peripherals we need for this spectrophotometer.
We prefer to design the full PCB rather than to make a shield for an existing Arduino board. Indeed, the extra work to add the microcontroller is rather limited, so it does not make sense from our point of view to create a shield. Also, the resulting board is smaller and more reliable.
All the components we use but one are mainstream and can be found on eBay or AliExpress. The only special component is the light to frequency converter TSL235R
that we buy on Mouser website.

For more details about the construction process, follow one of the links underneath.

The PCB (using eagle)
The case (using FreeCAD and OpenSCAD)
The software (using Arduino)

Experiments

Bacteria growth
Pigment concentration
Measure of the fluorescence
Kinetic of a reaction

Tests
Underneath, you will find some of the tests we have run to verify the quality of our spectrophotometer.

Battery
Reproducibility of results

Workshop
We regularly organize one day workshops ""Build your own spectrophotometer"" during which the participants learn all the process we went through to obtain the final product and build their own spectrophotometer. Please contact us if you are interested by this kind of workshop.
Here are the slides of the presentation we make during the workshop.

Slides

Cloning the project
This project uses SUBMODULES.
In order to clone it use:
git clone --recurse-submodules https://github.com/hackuarium/simple-spectro
To update the submodules:
git submodule update --recursive --remote
",diy spectrophotomet how to use it pleas check the userguid to obtain all the use inform about the user interfac goal of the project thi project wa done at the hackuarium associ a diy biolog hackingspac and open laboratori locat in ecublen switzerland feel free to contact us if you would like addit inform or would like to buy a fulli built and test spectrophotomet recent the opensourc and openhardwar commun ha been involv in the creation of open scientif tool in thi context a few spectrophotomet project emerg howev none of them wa suffici evolv to be casual use in a laboratori it is for thi reason that we start thi project our goal wa to creat a simpl spectrophotomet that measur the absorb of a sampl for 3 differ light color red green blue that is selfcontain batteri power and display the result on a lcd screen in addit the tool had to be reliabl precis and cheap the initi idea wa that you could teach spectrophotometri without have to buy an expens instrument sinc you can find experi that do not requir a specif light wavelength thi includ optic densiti for bacteria cultur determin of a pigment concentr in a solut determin of the kinet of a reaction and mani other the devic is rather cheap for you can buy all the compon on aliexpress but one and either 3d print or laser cut the case mdf or acryl glass when construct 10 the price per spectrophotomet is around 30 howev if you want to have a final product with an aluminium case and pcb alreadi assembl the cost would rather be 90 each if you order 20 of them can we do scienc with thi befor tri to explain how it is done more in detail mayb the first question that we should answer is can we do scienc with thi tool when think about the design of the spectro we found an light sensor that convert the light energi to frequenc and that is linear on a rang of nearli 106 thi mean that the luminos rang that we can measur precis is realli big onli the precis of the compon is therefor realli promis and show that we could do someth that give good result and inde the final product give realli accur result in field like chemistri biolog and other which you can see on our lab notebook some of our result patent blue v to begin we did a veri simpl experi that allow to measur the absorb of solut contain differ known concentr of patent blue v e131 a blue pigment that is use in variou blue candi the result is rather good know that the solut were simpli prepar by ad variou volum 0 to 2 ml of a concentr pigment solut with a 1ml sering to 100ml of water not the most accur method escherichia coli anoth featur of the simplespectrophotomet is that it work on a recharg batteri autonomi of 48 hour consequ you can measur a kinet by place the devic directli insid an incub which is not feasibl with a regular commerci spectrophotomet optic densiti od at 600nm is often use in biolog to determin the biomass increas versu time in thi experi we have place escherichia coli with cell grow media directli in the spectrophotomet cuvett and place the spectrophotomet itself on a shaker in the incub we can then program it to measur the absorb everi x second in our case we measur the absorb everi 30 minut up to 40 measur the follow curv wa obtain thi experi clearli show the log phase and the stationari phase but the growth wa faster than expect an acquisit everi 15 minut would have been a better choic use the simpl spectrophotomet to teach scienc the use of the spectrophotomet is rather simpl just put a blank press the button wait 10 and put the sampl the result is display on the screen in addit it could be open so that student could see how it is insid and could realli understand how the tool work avoid the black box effect also sinc it is inexpens have ten of these for a classroom would be possibl which would allow student to have one instrument per pair use the spectrophotomet for all thi reason we also consid our instrument as a pedagog tool thi allow us to use it to teach some basic scientif concept to group of children and teenag school in mondomo colombia how to make it the electron design is compat with the arduino platform we use an atmega32u4 microcontrol which is directli connect to the peripher we need for thi spectrophotomet we prefer to design the full pcb rather than to make a shield for an exist arduino board inde the extra work to add the microcontrol is rather limit so it doe not make sens from our point of view to creat a shield also the result board is smaller and more reliabl all the compon we use but one are mainstream and can be found on ebay or aliexpress the onli special compon is the light to frequenc convert tsl235r that we buy on mouser websit for more detail about the construct process follow one of the link underneath the pcb use eagl the case use freecad and openscad the softwar use arduino experi bacteria growth pigment concentr measur of the fluoresc kinet of a reaction test underneath you will find some of the test we have run to verifi the qualiti of our spectrophotomet batteri reproduc of result workshop we regularli organ one day workshop build your own spectrophotomet dure which the particip learn all the process we went through to obtain the final product and build their own spectrophotomet pleas contact us if you are interest by thi kind of workshop here are the slide of the present we make dure the workshop slide clone the project thi project use submodul in order to clone it use git clone recursesubmodul httpsgithubcomhackuariumsimplespectro to updat the submodul git submodul updat recurs remot,diy spectrophotometer how to use it please check the userguide to obtain all the useful information about the user interface goal of the project this project wa done at the hackuarium association a diy biology hackingspace and open laboratory located in ecublens switzerland feel free to contact u if you would like additional information or would like to buy a fully built and tested spectrophotometer recently the opensource and openhardware community ha been involved in the creation of open scientific tool in this context a few spectrophotometer project emerged however none of them wa sufficiently evolved to be casually used in a laboratory it is for this reason that we started this project our goal wa to create a simple spectrophotometer that measure the absorbance of a sample for 3 different light color red green blue that is selfcontained battery powered and display the result on a lcd screen in addition the tool had to be reliable precise and cheap the initial idea wa that you could teach spectrophotometry without having to buy an expensive instrument since you can find experiment that do not require a specific light wavelength this includes optical density for bacteria culture determination of a pigment concentration in a solution determination of the kinetic of a reaction and many others the device is rather cheap for you can buy all the component on aliexpress but one and either 3d print or laser cut the case mdf or acrylic glass when constructing 10 the price per spectrophotometer is around 30 however if you want to have a final product with an aluminium case and pcbs already assembled the cost would rather be 90 each if you order 20 of them can we do science with this before trying to explain how it is done more in detail maybe the first question that we should answer is can we do science with this tool when thinking about the design of the spectro we found an light sensor that convert the light energy to frequency and that is linear on a range of nearly 106 this mean that the luminosity range that we can measure precisely is really big only the precision of the component is therefore really promising and show that we could do something that give good result and indeed the final product give really accurate result in field like chemistry biology and other which you can see on our lab notebook some of our result patent blue v to begin we did a very simple experiment that allows to measure the absorbance of solution containing different known concentration of patent blue v e131 a blue pigment that is used in various blue candy the result is rather good knowing that the solution were simply prepared by adding various volume 0 to 2 ml of a concentrated pigmented solution with a 1ml seringe to 100ml of water not the most accurate method escherichia coli another feature of the simplespectrophotometer is that it work on a rechargeable battery autonomy of 48 hour consequently you can measure a kinetic by placing the device directly inside an incubator which is not feasible with a regular commercial spectrophotometer optical density od at 600nm is often used in biology to determine the biomass increase versus time in this experiment we have placed escherichia coli with cell growing medium directly in the spectrophotometer cuvette and placed the spectrophotometer itself on a shaker in the incubator we can then program it to measure the absorbance every x second in our case we measured the absorbance every 30 minute up to 40 measure the following curve wa obtained this experiment clearly show the log phase and the stationary phase but the growth wa faster than expected an acquisition every 15 minute would have been a better choice using the simple spectrophotometer to teach science the use of the spectrophotometer is rather simple just put a blank press the button wait 10 and put the sample the result is displayed on the screen in addition it could be open so that student could see how it is inside and could really understand how the tool work avoiding the black box effect also since it is inexpensive having ten of these for a classroom would be possible which would allow student to have one instrument per pair using the spectrophotometer for all this reason we also consider our instrument a a pedagogical tool this allowed u to use it to teach some basic scientific concept to group of child and teenager school in mondomo colombia how to make it the electronic design is compatible with the arduino platform we use an atmega32u4 microcontroller which is directly connected to the peripheral we need for this spectrophotometer we prefer to design the full pcb rather than to make a shield for an existing arduino board indeed the extra work to add the microcontroller is rather limited so it doe not make sense from our point of view to create a shield also the resulting board is smaller and more reliable all the component we use but one are mainstream and can be found on ebay or aliexpress the only special component is the light to frequency converter tsl235r that we buy on mouser website for more detail about the construction process follow one of the link underneath the pcb using eagle the case using freecad and openscad the software using arduino experiment bacteria growth pigment concentration measure of the fluorescence kinetic of a reaction test underneath you will find some of the test we have run to verify the quality of our spectrophotometer battery reproducibility of result workshop we regularly organize one day workshop build your own spectrophotometer during which the participant learn all the process we went through to obtain the final product and build their own spectrophotometer please contact u if you are interested by this kind of workshop here are the slide of the presentation we make during the workshop slide cloning the project this project us submodules in order to clone it use git clone recursesubmodules httpsgithubcomhackuariumsimplespectro to update the submodules git submodule update recursive remote,diy spectrophotometer use please check userguide obtain useful information user interface goal project project done hackuarium association diy biology hackingspace open laboratory located ecublens switzerland feel free contact u would like additional information would like buy fully built tested spectrophotometer recently opensource openhardware community involved creation open scientific tool context spectrophotometer project emerged however none sufficiently evolved casually used laboratory reason started project goal create simple spectrophotometer measure absorbance sample 3 different light color red green blue selfcontained battery powered display result lcd screen addition tool reliable precise cheap initial idea could teach spectrophotometry without buy expensive instrument since find experiment require specific light wavelength includes optical density bacteria culture determination pigment concentration solution determination kinetic reaction many others device rather cheap buy component aliexpress one either 3d print laser cut case mdf acrylic glass constructing 10 price per spectrophotometer around 30 however want final product aluminium case pcbs already assembled cost would rather 90 order 20 science trying explain done detail maybe first question answer science tool thinking design spectro found light sensor convert light energy frequency linear range nearly 106 mean luminosity range measure precisely really big precision component therefore really promising show could something give good result indeed final product give really accurate result field like chemistry biology see lab notebook result patent blue v begin simple experiment allows measure absorbance solution containing different known concentration patent blue v e131 blue pigment used various blue candy result rather good knowing solution simply prepared adding various volume 0 2 ml concentrated pigmented solution 1ml seringe 100ml water accurate method escherichia coli another feature simplespectrophotometer work rechargeable battery autonomy 48 hour consequently measure kinetic placing device directly inside incubator feasible regular commercial spectrophotometer optical density od 600nm often used biology determine biomass increase versus time experiment placed escherichia coli cell growing medium directly spectrophotometer cuvette placed spectrophotometer shaker incubator program measure absorbance every x second case measured absorbance every 30 minute 40 measure following curve obtained experiment clearly show log phase stationary phase growth faster expected acquisition every 15 minute would better choice using simple spectrophotometer teach science use spectrophotometer rather simple put blank press button wait 10 put sample result displayed screen addition could open student could see inside could really understand tool work avoiding black box effect also since inexpensive ten classroom would possible would allow student one instrument per pair using spectrophotometer reason also consider instrument pedagogical tool allowed u use teach basic scientific concept group child teenager school mondomo colombia make electronic design compatible arduino platform use atmega32u4 microcontroller directly connected peripheral need spectrophotometer prefer design full pcb rather make shield existing arduino board indeed extra work add microcontroller rather limited make sense point view create shield also resulting board smaller reliable component use one mainstream found ebay aliexpress special component light frequency converter tsl235r buy mouser website detail construction process follow one link underneath pcb using eagle case using freecad openscad software using arduino experiment bacteria growth pigment concentration measure fluorescence kinetic reaction test underneath find test run verify quality spectrophotometer battery reproducibility result workshop regularly organize one day workshop build spectrophotometer participant learn process went obtain final product build spectrophotometer please contact u interested kind workshop slide presentation make workshop slide cloning project project us submodules order clone use git clone recursesubmodules httpsgithubcomhackuariumsimplespectro update submodules git submodule update recursive remote
C++ ,"LibAPR - The Adaptive Particle Representation Library
Library for producing and processing on the Adaptive Particle Representation (APR) (For article see: https://www.nature.com/articles/s41467-018-07390-9).

Labeled Zebrafish nuclei: Gopi Shah, Huisken Lab (MPI-CBG, Dresden and Morgridge Institute for Research, Madison); see also Schmid et al., Nature Communications 2017


Dependencies

HDF5 1.8.20 or higher
OpenMP > 3.0 (optional, but suggested)
CMake 3.6 or higher
LibTIFF 4.0 or higher

NB: This update to 2.0 introduces changes to IO and iteration that are not compatable with old versions.
Building
The repository requires sub-modules, so the repository needs to be cloned recursively:
git clone --recursive https://github.com/cheesema/LibAPR

If you need to update your clone at any point later, run
git pull
git submodule update

Building on Linux
On Ubuntu, install the cmake, build-essential, libhdf5-dev and libtiff5-dev packages (on other distributions, refer to the documentation there, the package names will be similar). OpenMP support is provided by the GCC compiler installed as part of the build-essential package.
In the directory of the cloned repository, run
mkdir build
cd build
cmake ..
make

This will create the libapr.so library in the build directory, as well as all of the examples.
Docker build
We provide a working Dockerfile that install the library within the image on a separate repo.
Building on OSX
On OSX, install the cmake, hdf5 and libtiff homebrew packages and have the Xcode command line tools installed.
If you want to compile with OpenMP support, also install the llvm package (this can also be done using homebrew), as the clang version shipped by Apple currently does not support OpenMP.
In the directory of the cloned repository, run
mkdir build
cd build
cmake ..
make

This will create the libapr.dylib library in the build directory, as well as all of the examples.
In case you want to use the homebrew-installed clang (OpenMP support), modify the call to cmake above to
CC=""/usr/local/opt/llvm/bin/clang"" CXX=""/usr/local/opt/llvm/bin/clang++"" LDFLAGS=""-L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib"" CPPFLAGS=""-I/usr/local/opt/llvm/include"" cmake ..

Building on Windows
The simplest way to utilise the library from Windows 10 is through using the Windows Subsystem for Linux; see: https://docs.microsoft.com/en-us/windows/wsl/install-win10 then follow linux instructions.
Compilation only works with mingw64/clang or the Intel C++ Compiler, with Intel C++ being the recommended way
The below instructions for VS can be attempted; however they have not been reproduced.
You need to have Visual Studio 2017 installed, with the community edition being sufficient. LibAPR does not compile correctly with the default Visual Studio compiler, so you also need to have the Intel C++ Compiler, 18.0 or higher installed. cmake is also a requirement.
Furthermore, you need to have HDF5 installed (binary distribution download at The HDF Group and LibTIFF (source download from SimpleSystems. LibTIFF needs to be compiled via cmake. LibTIFF's install target will then install the library into C:\Program Files\tiff.
In the directory of the cloned repository, run:
mkdir build
cd build
cmake -G ""Visual Studio 15 2017 Win64"" -DTIFF_INCLUDE_DIR=""C:/Program Files/tiff/include"" -DTIFF_LIBRARY=""C:/Program Files/tiff/lib/tiff.lib "" -DHDF5_ROOT=""C:/Program Files/HDF_Group/HDF5/1.8.17""  -T ""Intel C++ Compiler 18.0"" ..
cmake --build . --config Debug

This will set the appropriate hints for Visual Studio to find both LibTIFF and HDF5. This will create the apr.dll library in the build/Debug directory, as well as all of the examples. If you need a Release build, run cmake --build . --config Release from the build directory.
Examples and Documentation
These examples can be turned on by adding -DAPR_BUILD_EXAMPLES=ON to the cmake command.
There are nine basic examples, that show how to generate and compute with the APR:



Example
How to ...




Example_get_apr
create an APR from a TIFF and store as hdf5.


Example_apr_iterate
iterate through a given APR.


Example_neighbour_access
access particle and face neighbours.


Example_compress_apr
additionally compress the intensities stored in an APR.


Example_random_access
perform random access operations on particles.


Example_ray_cast
perform a maximum intensity projection ray cast directly on the APR data structures read from an APR.


Example_reconstruct_image
reconstruct a pixel image from an APR.



All examples except Example_get_apr require an already produced APR, such as those created by Example_get_apr.
For tutorial on how to use the examples, and explanation of data-structures see the library guide.
LibAPR Tests
The testing framework can be turned on by adding -DAPR_TESTS=ON to the cmake command. All tests can then be run by executing on the command line your build folder.
ctest

Please let us know by creating an issue, if any of these tests are failing on your machine.
Python support
Note: These have been updated and externalised, and will be released shortly.
Java wrappers
Basic Java wrappers can be found at LibAPR-java-wrapper
Coming soon

more examples for APR-based filtering and segmentation
deployment of the Java wrappers to Maven Central so they can be used in your project directly
support for loading the APR in Fiji, including scenery-based 3D rendering
improved java wrapper support
CUDA GPU-accelerated APR generation and processing
Block based decomposition for extremely large images.
Time series support.

Contact us
If anything is not working as you think it should, or would like it to, please get in touch with us!! Further, if you have a project, or algorithm, you would like to try using the APR for also please get in contact we would be glad to help!

Citing this work
If you use this library in an academic context, please cite the following paper:

Cheeseman, Günther, Gonciarz, Susik, Sbalzarini: Adaptive Particle Representation of Fluorescence Microscopy Images (Nature Communications, 2018) https://doi.org/10.1038/s41467-018-07390-9

",libapr the adapt particl represent librari librari for produc and process on the adapt particl represent apr for articl see httpswwwnaturecomarticless41467018073909 label zebrafish nuclei gopi shah huisken lab mpicbg dresden and morgridg institut for research madison see also schmid et al natur commun 2017 depend hdf5 1820 or higher openmp 30 option but suggest cmake 36 or higher libtiff 40 or higher nb thi updat to 20 introduc chang to io and iter that are not compat with old version build the repositori requir submodul so the repositori need to be clone recurs git clone recurs httpsgithubcomcheesemalibapr if you need to updat your clone at ani point later run git pull git submodul updat build on linux on ubuntu instal the cmake buildessenti libhdf5dev and libtiff5dev packag on other distribut refer to the document there the packag name will be similar openmp support is provid by the gcc compil instal as part of the buildessenti packag in the directori of the clone repositori run mkdir build cd build cmake make thi will creat the libaprso librari in the build directori as well as all of the exampl docker build we provid a work dockerfil that instal the librari within the imag on a separ repo build on osx on osx instal the cmake hdf5 and libtiff homebrew packag and have the xcode command line tool instal if you want to compil with openmp support also instal the llvm packag thi can also be done use homebrew as the clang version ship by appl current doe not support openmp in the directori of the clone repositori run mkdir build cd build cmake make thi will creat the libaprdylib librari in the build directori as well as all of the exampl in case you want to use the homebrewinstal clang openmp support modifi the call to cmake abov to ccusrlocaloptllvmbinclang cxxusrlocaloptllvmbinclang ldflagslusrlocaloptllvmlib wlrpathusrlocaloptllvmlib cppflagsiusrlocaloptllvminclud cmake build on window the simplest way to utilis the librari from window 10 is through use the window subsystem for linux see httpsdocsmicrosoftcomenuswindowswslinstallwin10 then follow linux instruct compil onli work with mingw64clang or the intel c compil with intel c be the recommend way the below instruct for vs can be attempt howev they have not been reproduc you need to have visual studio 2017 instal with the commun edit be suffici libapr doe not compil correctli with the default visual studio compil so you also need to have the intel c compil 180 or higher instal cmake is also a requir furthermor you need to have hdf5 instal binari distribut download at the hdf group and libtiff sourc download from simplesystem libtiff need to be compil via cmake libtiff instal target will then instal the librari into cprogram filestiff in the directori of the clone repositori run mkdir build cd build cmake g visual studio 15 2017 win64 dtiff_include_dircprogram filestiffinclud dtiff_librarycprogram filestifflibtifflib dhdf5_rootcprogram fileshdf_grouphdf51817 t intel c compil 180 cmake build config debug thi will set the appropri hint for visual studio to find both libtiff and hdf5 thi will creat the aprdll librari in the builddebug directori as well as all of the exampl if you need a releas build run cmake build config releas from the build directori exampl and document these exampl can be turn on by ad dapr_build_exampleson to the cmake command there are nine basic exampl that show how to gener and comput with the apr exampl how to example_get_apr creat an apr from a tiff and store as hdf5 example_apr_iter iter through a given apr example_neighbour_access access particl and face neighbour example_compress_apr addit compress the intens store in an apr example_random_access perform random access oper on particl example_ray_cast perform a maximum intens project ray cast directli on the apr data structur read from an apr example_reconstruct_imag reconstruct a pixel imag from an apr all exampl except example_get_apr requir an alreadi produc apr such as those creat by example_get_apr for tutori on how to use the exampl and explan of datastructur see the librari guid libapr test the test framework can be turn on by ad dapr_testson to the cmake command all test can then be run by execut on the command line your build folder ctest pleas let us know by creat an issu if ani of these test are fail on your machin python support note these have been updat and externalis and will be releas shortli java wrapper basic java wrapper can be found at libaprjavawrapp come soon more exampl for aprbas filter and segment deploy of the java wrapper to maven central so they can be use in your project directli support for load the apr in fiji includ scenerybas 3d render improv java wrapper support cuda gpuacceler apr gener and process block base decomposit for extrem larg imag time seri support contact us if anyth is not work as you think it should or would like it to pleas get in touch with us further if you have a project or algorithm you would like to tri use the apr for also pleas get in contact we would be glad to help cite thi work if you use thi librari in an academ context pleas cite the follow paper cheeseman gnther gonciarz susik sbalzarini adapt particl represent of fluoresc microscopi imag natur commun 2018 httpsdoiorg101038s41467018073909,libapr the adaptive particle representation library library for producing and processing on the adaptive particle representation apr for article see httpswwwnaturecomarticless41467018073909 labeled zebrafish nucleus gopi shah huisken lab mpicbg dresden and morgridge institute for research madison see also schmid et al nature communication 2017 dependency hdf5 1820 or higher openmp 30 optional but suggested cmake 36 or higher libtiff 40 or higher nb this update to 20 introduces change to io and iteration that are not compatable with old version building the repository requires submodules so the repository need to be cloned recursively git clone recursive httpsgithubcomcheesemalibapr if you need to update your clone at any point later run git pull git submodule update building on linux on ubuntu install the cmake buildessential libhdf5dev and libtiff5dev package on other distribution refer to the documentation there the package name will be similar openmp support is provided by the gcc compiler installed a part of the buildessential package in the directory of the cloned repository run mkdir build cd build cmake make this will create the libaprso library in the build directory a well a all of the example docker build we provide a working dockerfile that install the library within the image on a separate repo building on osx on osx install the cmake hdf5 and libtiff homebrew package and have the xcode command line tool installed if you want to compile with openmp support also install the llvm package this can also be done using homebrew a the clang version shipped by apple currently doe not support openmp in the directory of the cloned repository run mkdir build cd build cmake make this will create the libaprdylib library in the build directory a well a all of the example in case you want to use the homebrewinstalled clang openmp support modify the call to cmake above to ccusrlocaloptllvmbinclang cxxusrlocaloptllvmbinclang ldflagslusrlocaloptllvmlib wlrpathusrlocaloptllvmlib cppflagsiusrlocaloptllvminclude cmake building on window the simplest way to utilise the library from window 10 is through using the window subsystem for linux see httpsdocsmicrosoftcomenuswindowswslinstallwin10 then follow linux instruction compilation only work with mingw64clang or the intel c compiler with intel c being the recommended way the below instruction for v can be attempted however they have not been reproduced you need to have visual studio 2017 installed with the community edition being sufficient libapr doe not compile correctly with the default visual studio compiler so you also need to have the intel c compiler 180 or higher installed cmake is also a requirement furthermore you need to have hdf5 installed binary distribution download at the hdf group and libtiff source download from simplesystems libtiff need to be compiled via cmake libtiffs install target will then install the library into cprogram filestiff in the directory of the cloned repository run mkdir build cd build cmake g visual studio 15 2017 win64 dtiff_include_dircprogram filestiffinclude dtiff_librarycprogram filestifflibtifflib dhdf5_rootcprogram fileshdf_grouphdf51817 t intel c compiler 180 cmake build config debug this will set the appropriate hint for visual studio to find both libtiff and hdf5 this will create the aprdll library in the builddebug directory a well a all of the example if you need a release build run cmake build config release from the build directory example and documentation these example can be turned on by adding dapr_build_exampleson to the cmake command there are nine basic example that show how to generate and compute with the apr example how to example_get_apr create an apr from a tiff and store a hdf5 example_apr_iterate iterate through a given apr example_neighbour_access access particle and face neighbour example_compress_apr additionally compress the intensity stored in an apr example_random_access perform random access operation on particle example_ray_cast perform a maximum intensity projection ray cast directly on the apr data structure read from an apr example_reconstruct_image reconstruct a pixel image from an apr all example except example_get_apr require an already produced apr such a those created by example_get_apr for tutorial on how to use the example and explanation of datastructures see the library guide libapr test the testing framework can be turned on by adding dapr_testson to the cmake command all test can then be run by executing on the command line your build folder ctest please let u know by creating an issue if any of these test are failing on your machine python support note these have been updated and externalised and will be released shortly java wrapper basic java wrapper can be found at libaprjavawrapper coming soon more example for aprbased filtering and segmentation deployment of the java wrapper to maven central so they can be used in your project directly support for loading the apr in fiji including scenerybased 3d rendering improved java wrapper support cuda gpuaccelerated apr generation and processing block based decomposition for extremely large image time series support contact u if anything is not working a you think it should or would like it to please get in touch with u further if you have a project or algorithm you would like to try using the apr for also please get in contact we would be glad to help citing this work if you use this library in an academic context please cite the following paper cheeseman gnther gonciarz susik sbalzarini adaptive particle representation of fluorescence microscopy image nature communication 2018 httpsdoiorg101038s41467018073909,libapr adaptive particle representation library library producing processing adaptive particle representation apr article see httpswwwnaturecomarticless41467018073909 labeled zebrafish nucleus gopi shah huisken lab mpicbg dresden morgridge institute research madison see also schmid et al nature communication 2017 dependency hdf5 1820 higher openmp 30 optional suggested cmake 36 higher libtiff 40 higher nb update 20 introduces change io iteration compatable old version building repository requires submodules repository need cloned recursively git clone recursive httpsgithubcomcheesemalibapr need update clone point later run git pull git submodule update building linux ubuntu install cmake buildessential libhdf5dev libtiff5dev package distribution refer documentation package name similar openmp support provided gcc compiler installed part buildessential package directory cloned repository run mkdir build cd build cmake make create libaprso library build directory well example docker build provide working dockerfile install library within image separate repo building osx osx install cmake hdf5 libtiff homebrew package xcode command line tool installed want compile openmp support also install llvm package also done using homebrew clang version shipped apple currently support openmp directory cloned repository run mkdir build cd build cmake make create libaprdylib library build directory well example case want use homebrewinstalled clang openmp support modify call cmake ccusrlocaloptllvmbinclang cxxusrlocaloptllvmbinclang ldflagslusrlocaloptllvmlib wlrpathusrlocaloptllvmlib cppflagsiusrlocaloptllvminclude cmake building window simplest way utilise library window 10 using window subsystem linux see httpsdocsmicrosoftcomenuswindowswslinstallwin10 follow linux instruction compilation work mingw64clang intel c compiler intel c recommended way instruction v attempted however reproduced need visual studio 2017 installed community edition sufficient libapr compile correctly default visual studio compiler also need intel c compiler 180 higher installed cmake also requirement furthermore need hdf5 installed binary distribution download hdf group libtiff source download simplesystems libtiff need compiled via cmake libtiffs install target install library cprogram filestiff directory cloned repository run mkdir build cd build cmake g visual studio 15 2017 win64 dtiff_include_dircprogram filestiffinclude dtiff_librarycprogram filestifflibtifflib dhdf5_rootcprogram fileshdf_grouphdf51817 intel c compiler 180 cmake build config debug set appropriate hint visual studio find libtiff hdf5 create aprdll library builddebug directory well example need release build run cmake build config release build directory example documentation example turned adding dapr_build_exampleson cmake command nine basic example show generate compute apr example example_get_apr create apr tiff store hdf5 example_apr_iterate iterate given apr example_neighbour_access access particle face neighbour example_compress_apr additionally compress intensity stored apr example_random_access perform random access operation particle example_ray_cast perform maximum intensity projection ray cast directly apr data structure read apr example_reconstruct_image reconstruct pixel image apr example except example_get_apr require already produced apr created example_get_apr tutorial use example explanation datastructures see library guide libapr test testing framework turned adding dapr_testson cmake command test run executing command line build folder ctest please let u know creating issue test failing machine python support note updated externalised released shortly java wrapper basic java wrapper found libaprjavawrapper coming soon example aprbased filtering segmentation deployment java wrapper maven central used project directly support loading apr fiji including scenerybased 3d rendering improved java wrapper support cuda gpuaccelerated apr generation processing block based decomposition extremely large image time series support contact u anything working think would like please get touch u project algorithm would like try using apr also please get contact would glad help citing work use library academic context please cite following paper cheeseman gnther gonciarz susik sbalzarini adaptive particle representation fluorescence microscopy image nature communication 2018 httpsdoiorg101038s41467018073909
C++ ,"All the C program code are translated to C++ STL. Smart pointers are used to avoid danger pointers and memory leak.
It is rewritten to support MVC design pattern required by MFC under Visual Studio 2017. It would be a good code reference for C++ programmers. The ""model"" folder in source code should be portable to other C++ compilers. As mentioned above, to understand the code to display the Windows UI required the fundamental knowledge of Microsoft Foundation Class (MFC) Library. C++ Beginners might find it difficult to understand the code of some essential UI controls. Many thanks to the MFC experts provided the source code of such advanced controls.
Welcome for C++ experts for further improvement or contribution of coding enhancement.
",all the c program code are translat to c stl smart pointer are use to avoid danger pointer and memori leak it is rewritten to support mvc design pattern requir by mfc under visual studio 2017 it would be a good code refer for c programm the model folder in sourc code should be portabl to other c compil as mention abov to understand the code to display the window ui requir the fundament knowledg of microsoft foundat class mfc librari c beginn might find it difficult to understand the code of some essenti ui control mani thank to the mfc expert provid the sourc code of such advanc control welcom for c expert for further improv or contribut of code enhanc,all the c program code are translated to c stl smart pointer are used to avoid danger pointer and memory leak it is rewritten to support mvc design pattern required by mfc under visual studio 2017 it would be a good code reference for c programmer the model folder in source code should be portable to other c compiler a mentioned above to understand the code to display the window ui required the fundamental knowledge of microsoft foundation class mfc library c beginner might find it difficult to understand the code of some essential ui control many thanks to the mfc expert provided the source code of such advanced control welcome for c expert for further improvement or contribution of coding enhancement,c program code translated c stl smart pointer used avoid danger pointer memory leak rewritten support mvc design pattern required mfc visual studio 2017 would good code reference c programmer model folder source code portable c compiler mentioned understand code display window ui required fundamental knowledge microsoft foundation class mfc library c beginner might find difficult understand code essential ui control many thanks mfc expert provided source code advanced control welcome c expert improvement contribution coding enhancement
C++ ,"Cyclops LED Driver
Precision, wide-bandwidth current source with optional optical feedback mode
for driving high-power LEDs and laser diodes. Good for sneaking optogenetic
stimuli between fast things (e.g. galvo flyback on a 2P system). Good for
really controlling the amount of light you deliver during 1P imaging or
optogenetic stimulation.
If you have questions or comments, please come talk on the open-ephys slack
in the #cyclops channel.
Features

Ultra-precise
High power
Up to 1.5A per LED
Wide bandwidth

~2.5 MHz -3 dB bandwidth
Maximum 100 ns 1.0A rise and fall times


Current and optical feedback modes
Built-in waveform generation
Over-current protection
Modular

Arduino compatible: internal waveform generation
Also, accepts external analog, gate, or trigger inputs



Stimulus generation options

External stimulus sequencer
External digital trigger

TTL logic level


External analog waveform generator

0-5V analog signals


Internal 12-bit DAC

Synchronized across up to 4 drivers
Arduino library
Programmable triggering logic
Respond to USB input




Buying one
You can purchase a fully assembled cyclops driver from the open-ephys
store. All profits go towards continued
operation of open-ephys.
Documentation
Documentation and usage information are here: MANUAL.pdf. If you
have questions concerning usage, performance, etc., please direct them toward
the Open Ephys forum.
Hardware Licensing
Copyright Jonathan P. Newman 2020.
This work is licensed under CC BY-NC-SA 4.0. To view a copy of this license,
visit https://creativecommons.org/licenses/by-nc-sa/4.0
Note: This license applies to hardware designs and documentation which reside
in the 'device', 'experimental', 'resources' folders of this repository along
with information in 'MANUAL.md' and 'MANUAL.pdf'
Software Licensing
Copyright (c) Jonathan P. Newman 2017. All right reserved.
The code associated with the Cyclops project is free software: you can
redistribute it and/or modify it under the terms of the GNU General Public
License as published by the Free Software Foundation, either version 3 of the
License, or (at your option) any later version.
The code associated with the Cyclops project is distributed in the hope that it
will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
Public License for more details.
You should have received a copy of the GNU General Public License along with
this code.  If not, see http://www.gnu.org/licenses/.
Note: This license applies to software/fireware source code which resides in in
the 'lib' folder of this repository
",cyclop led driver precis widebandwidth current sourc with option optic feedback mode for drive highpow led and laser diod good for sneak optogenet stimuli between fast thing eg galvo flyback on a 2p system good for realli control the amount of light you deliv dure 1p imag or optogenet stimul if you have question or comment pleas come talk on the openephi slack in the cyclop channel featur ultraprecis high power up to 15a per led wide bandwidth 25 mhz 3 db bandwidth maximum 100 ns 10a rise and fall time current and optic feedback mode builtin waveform gener overcurr protect modular arduino compat intern waveform gener also accept extern analog gate or trigger input stimulu gener option extern stimulu sequenc extern digit trigger ttl logic level extern analog waveform gener 05v analog signal intern 12bit dac synchron across up to 4 driver arduino librari programm trigger logic respond to usb input buy one you can purchas a fulli assembl cyclop driver from the openephi store all profit go toward continu oper of openephi document document and usag inform are here manualpdf if you have question concern usag perform etc pleas direct them toward the open ephi forum hardwar licens copyright jonathan p newman 2020 thi work is licens under cc byncsa 40 to view a copi of thi licens visit httpscreativecommonsorglicensesbyncsa40 note thi licens appli to hardwar design and document which resid in the devic experiment resourc folder of thi repositori along with inform in manualmd and manualpdf softwar licens copyright c jonathan p newman 2017 all right reserv the code associ with the cyclop project is free softwar you can redistribut it andor modifi it under the term of the gnu gener public licens as publish by the free softwar foundat either version 3 of the licens or at your option ani later version the code associ with the cyclop project is distribut in the hope that it will be use but without ani warranti without even the impli warranti of merchant or fit for a particular purpos see the gnu gener public licens for more detail you should have receiv a copi of the gnu gener public licens along with thi code if not see httpwwwgnuorglicens note thi licens appli to softwarefirewar sourc code which resid in in the lib folder of thi repositori,cyclops led driver precision widebandwidth current source with optional optical feedback mode for driving highpower led and laser diode good for sneaking optogenetic stimulus between fast thing eg galvo flyback on a 2p system good for really controlling the amount of light you deliver during 1p imaging or optogenetic stimulation if you have question or comment please come talk on the openephys slack in the cyclops channel feature ultraprecise high power up to 15a per led wide bandwidth 25 mhz 3 db bandwidth maximum 100 n 10a rise and fall time current and optical feedback mode builtin waveform generation overcurrent protection modular arduino compatible internal waveform generation also accepts external analog gate or trigger input stimulus generation option external stimulus sequencer external digital trigger ttl logic level external analog waveform generator 05v analog signal internal 12bit dac synchronized across up to 4 driver arduino library programmable triggering logic respond to usb input buying one you can purchase a fully assembled cyclops driver from the openephys store all profit go towards continued operation of openephys documentation documentation and usage information are here manualpdf if you have question concerning usage performance etc please direct them toward the open ephys forum hardware licensing copyright jonathan p newman 2020 this work is licensed under cc byncsa 40 to view a copy of this license visit httpscreativecommonsorglicensesbyncsa40 note this license applies to hardware design and documentation which reside in the device experimental resource folder of this repository along with information in manualmd and manualpdf software licensing copyright c jonathan p newman 2017 all right reserved the code associated with the cyclops project is free software you can redistribute it andor modify it under the term of the gnu general public license a published by the free software foundation either version 3 of the license or at your option any later version the code associated with the cyclops project is distributed in the hope that it will be useful but without any warranty without even the implied warranty of merchantability or fitness for a particular purpose see the gnu general public license for more detail you should have received a copy of the gnu general public license along with this code if not see httpwwwgnuorglicenses note this license applies to softwarefireware source code which resides in in the lib folder of this repository,cyclops led driver precision widebandwidth current source optional optical feedback mode driving highpower led laser diode good sneaking optogenetic stimulus fast thing eg galvo flyback 2p system good really controlling amount light deliver 1p imaging optogenetic stimulation question comment please come talk openephys slack cyclops channel feature ultraprecise high power 15a per led wide bandwidth 25 mhz 3 db bandwidth maximum 100 n 10a rise fall time current optical feedback mode builtin waveform generation overcurrent protection modular arduino compatible internal waveform generation also accepts external analog gate trigger input stimulus generation option external stimulus sequencer external digital trigger ttl logic level external analog waveform generator 05v analog signal internal 12bit dac synchronized across 4 driver arduino library programmable triggering logic respond usb input buying one purchase fully assembled cyclops driver openephys store profit go towards continued operation openephys documentation documentation usage information manualpdf question concerning usage performance etc please direct toward open ephys forum hardware licensing copyright jonathan p newman 2020 work licensed cc byncsa 40 view copy license visit httpscreativecommonsorglicensesbyncsa40 note license applies hardware design documentation reside device experimental resource folder repository along information manualmd manualpdf software licensing copyright c jonathan p newman 2017 right reserved code associated cyclops project free software redistribute andor modify term gnu general public license published free software foundation either version 3 license option later version code associated cyclops project distributed hope useful without warranty without even implied warranty merchantability fitness particular purpose see gnu general public license detail received copy gnu general public license along code see httpwwwgnuorglicenses note license applies softwarefireware source code resides lib folder repository
C++ ,"PhysiBoSS
Multiscale simulation of multi-cellular system
Overview:

Presentation
Usage
Documentation
References
Remarks

Presentation
PhysiBoSS (PhysiCell-MaBoSS) is C++ software for multiscale simulation of heterogeneous multi-cellular system. It integrates together cell's internal signalling pathway model (boolean formalism), physical representation of cell (agent-based) and extra-cellular matrix diffusing or fixed entities.
It is adapted from PhysiCell sources, with the boolean network computation inside each cell from MaBoSS software.

Usage
Compiling PhysiBoSS
PhysiBoSS should run and be easily installed on Linux and MacOS system.
It requires moderatly recent version of C++ (at least c++11) and OpenMP support. Compilation of MaBoSS library requires flex and bison library, usually already present (and can be easily installed on e.g. Linux ubuntu with sudo apt-get install bison flex). We also provide a Docker image of PhysiBoSS that can be used if it cannot be installed in your machine. It can also be used without any installation via a Web interface for specific simulations on nanohub.
To install it on Linux system, from a Terminal:
Clone the repository on your local machine, and go inside the main directory. Type make install, which will install and compile MaBoSS then PhysiBoSS. The executables will be created in the 'bin' directory if all goes well.
It can be compiled in 'Debug', 'Release' or 'Proliling' modes, to set in the 'Makefile' file. Default is 'Release' mode (fastest).
You might also have to change your c++ compiler in the Makefile according to your operating system.
Commands list:
git clone https://github.com/gletort/PhysiBoSS.git
cd PhysiBoSS
make install
If errors happened during the compilation, please refer to the installation page.
Running one simulation
To run a simulation, you need (at least) a XML parameter file indicating the conditions of the simulation, and the networks file (you can find some on MaBoSS website and on our logical modelling pipeline repository).
Other options are possible, cf the code-documentation or this repository wiki for more informations.
Example of a parameter file (with only few parameters shown):
  <?xml version=""1.0"" encoding=""UTF-8"" ?>
 
  <simulation>
 		<time_step> 0.2 </time_step>
 		<mechanics_time_step> 0.1 </mechanics_time_step>
 		....
  </simulation>
 
  <cell_properties>
 		<mode_motility> 1 </mode_motility>
 		<polarity_coefficient> 0.5 </polarity_coefficient>
 		...
  </cell_properties>
 
  <network>
 		<network_update_step> 10 </network_update_step>
 		...
  </network>
 
  <initial_configuration>
 		<load_cells_from_file> init.txt </load_cells_from_file>
 		...
  </initial_configuration>
Image and analyse a simulation
To visualize graphically the result of a simulation, with use the software Paraview (or you can also generate a .svg snapshot of the simulation). Analysis of the result files were done with python scripts proposed in this directory. For documentation on how to use Paraview to set-up the rendering of PhysiBoSS outputs, see here, with the explication on how to draw spheres from a set of points (x, y, z, radius).
Nanohub
PhysiBoSS can be directly used via a Web interface on nanohub. This allows to run it without any installation and running directly on the server and can be used without any coding skills. The parameters of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server. It just required a nanohub account.
Available simulations tools of PhysiBoSS can be found on https://nanohub.org/resources/tools, under the keywords PhysiBoSS or PhysiBoSSa.
A model of tumors cell spheroid growing and invading into the surrounding extra-cellular matrix (ECM) is currently available PhysiBoSSa_ECM. Various parameters as the density of the extracellular matrix, the cell motility, ECM degradation from the cells, TGF-beta production... can be tuned by the user.
Documentation
Code-oriented documentation can be generated with Doxygen:
make doc
in the main directory.
It can be configured in the Doxyfile file present in this directory.
It will generate the html documentation in the doc/html directory.
You can visualize it in a browser, e.g.:
firefox doc/html/index.html &
You can also refer to (future) publications with PhysiBoSS for scientific applications of this software and description of the models.
Step-by-step examples with the necessary files to run them are also proposed in the 'examples' directory and on the Wiki of this repository.
References
 For PhysiBoSS: 

PhysiBoSS publication: Letort G, Montagud A, Stoll G, Heiland R, Barillot E, Macklin P, Zinovyev A, Calzone L .  PhysiBoSS: a multi-scale agent-based modelling framework integrating physical dimension and cell signalling.  Bioinformatics, bty766, doi:10.1093/bioinformatics/bty766


For PhysiCell: 
Paul Macklin's lab website  
PhysiCell publication: A. Ghaffarizadeh, S.H. Friedman, S.M. Mumenthaler, and P. Macklin, PhysiCell: an Open Source Physics-Based Cell Simulator for 3-D Multicellular Systems, bioRxiv 088773, 2016. DOI: 10.1101/088773. 
BioFVM website  
BioFVM publication: A. Ghaffarizadeh, S.H. Friedman, and P. Macklin. BioFVM: an efficient, parallelized diffusive transport solver for 3-D biological simulations. Bioinformatics, 2015. 


For MaBoSS:
MaBoSS website  
MaBoSS publication: Stoll G, Viara E, Barillot E, Calzone L. Continuous time Boolean modeling for biological signaling: application of Gillespie algorithm. BMC Syst Biol. 2012 Aug 29;6:116. doi: 10.1186/1752-0509-6-116. 


Remarks
Please, refer to the Wiki of this repository for a much more extended documentation, with step by step examples instructions.
PhysiCell is developed in Paul Macklin's lab.
MaBoSS and PhysiBoSS are developed in the Computational Systems Biology of Cancer group at Institut Curie (Paris, France).
We invite you to use PhysiBoSS for you research and give feedbacks to us. Any help in developing it further is more than welcome.
Do not hesitate to contact us for any comments or difficulties in using PhysiBoSS: physiboss@gmail.com.
Wishing you to enjoy using PhysiBoSS,
PhysiBoSS's team.
",physiboss multiscal simul of multicellular system overview present usag document refer remark present physiboss physicellmaboss is c softwar for multiscal simul of heterogen multicellular system it integr togeth cell intern signal pathway model boolean formal physic represent of cell agentbas and extracellular matrix diffus or fix entiti it is adapt from physicel sourc with the boolean network comput insid each cell from maboss softwar usag compil physiboss physiboss should run and be easili instal on linux and maco system it requir moderatli recent version of c at least c11 and openmp support compil of maboss librari requir flex and bison librari usual alreadi present and can be easili instal on eg linux ubuntu with sudo aptget instal bison flex we also provid a docker imag of physiboss that can be use if it cannot be instal in your machin it can also be use without ani instal via a web interfac for specif simul on nanohub to instal it on linux system from a termin clone the repositori on your local machin and go insid the main directori type make instal which will instal and compil maboss then physiboss the execut will be creat in the bin directori if all goe well it can be compil in debug releas or prolil mode to set in the makefil file default is releas mode fastest you might also have to chang your c compil in the makefil accord to your oper system command list git clone httpsgithubcomgletortphysibossgit cd physiboss make instal if error happen dure the compil pleas refer to the instal page run one simul to run a simul you need at least a xml paramet file indic the condit of the simul and the network file you can find some on maboss websit and on our logic model pipelin repositori other option are possibl cf the codedocument or thi repositori wiki for more inform exampl of a paramet file with onli few paramet shown xml version10 encodingutf8 simul time_step 02 time_step mechanics_time_step 01 mechanics_time_step simul cell_properti mode_motil 1 mode_motil polarity_coeffici 05 polarity_coeffici cell_properti network network_update_step 10 network_update_step network initial_configur load_cells_from_fil inittxt load_cells_from_fil initial_configur imag and analys a simul to visual graphic the result of a simul with use the softwar paraview or you can also gener a svg snapshot of the simul analysi of the result file were done with python script propos in thi directori for document on how to use paraview to setup the render of physiboss output see here with the explic on how to draw sphere from a set of point x y z radiu nanohub physiboss can be directli use via a web interfac on nanohub thi allow to run it without ani instal and run directli on the server and can be use without ani code skill the paramet of the simul can be enter in the interfac and then the simul will be run on the nanohub server it just requir a nanohub account avail simul tool of physiboss can be found on httpsnanohuborgresourcestool under the keyword physiboss or physibossa a model of tumor cell spheroid grow and invad into the surround extracellular matrix ecm is current avail physibossa_ecm variou paramet as the densiti of the extracellular matrix the cell motil ecm degrad from the cell tgfbeta product can be tune by the user document codeori document can be gener with doxygen make doc in the main directori it can be configur in the doxyfil file present in thi directori it will gener the html document in the dochtml directori you can visual it in a browser eg firefox dochtmlindexhtml you can also refer to futur public with physiboss for scientif applic of thi softwar and descript of the model stepbystep exampl with the necessari file to run them are also propos in the exampl directori and on the wiki of thi repositori refer for physiboss physiboss public letort g montagud a stoll g heiland r barillot e macklin p zinovyev a calzon l physiboss a multiscal agentbas model framework integr physic dimens and cell signal bioinformat bty766 doi101093bioinformaticsbty766 for physicel paul macklin lab websit physicel public a ghaffarizadeh sh friedman sm mumenthal and p macklin physicel an open sourc physicsbas cell simul for 3d multicellular system biorxiv 088773 2016 doi 101101088773 biofvm websit biofvm public a ghaffarizadeh sh friedman and p macklin biofvm an effici parallel diffus transport solver for 3d biolog simul bioinformat 2015 for maboss maboss websit maboss public stoll g viara e barillot e calzon l continu time boolean model for biolog signal applic of gillespi algorithm bmc syst biol 2012 aug 296116 doi 101186175205096116 remark pleas refer to the wiki of thi repositori for a much more extend document with step by step exampl instruct physicel is develop in paul macklin lab maboss and physiboss are develop in the comput system biolog of cancer group at institut curi pari franc we invit you to use physiboss for you research and give feedback to us ani help in develop it further is more than welcom do not hesit to contact us for ani comment or difficulti in use physiboss physibossgmailcom wish you to enjoy use physiboss physibosss team,physiboss multiscale simulation of multicellular system overview presentation usage documentation reference remark presentation physiboss physicellmaboss is c software for multiscale simulation of heterogeneous multicellular system it integrates together cell internal signalling pathway model boolean formalism physical representation of cell agentbased and extracellular matrix diffusing or fixed entity it is adapted from physicell source with the boolean network computation inside each cell from maboss software usage compiling physiboss physiboss should run and be easily installed on linux and macos system it requires moderatly recent version of c at least c11 and openmp support compilation of maboss library requires flex and bison library usually already present and can be easily installed on eg linux ubuntu with sudo aptget install bison flex we also provide a docker image of physiboss that can be used if it cannot be installed in your machine it can also be used without any installation via a web interface for specific simulation on nanohub to install it on linux system from a terminal clone the repository on your local machine and go inside the main directory type make install which will install and compile maboss then physiboss the executables will be created in the bin directory if all go well it can be compiled in debug release or proliling mode to set in the makefile file default is release mode fastest you might also have to change your c compiler in the makefile according to your operating system command list git clone httpsgithubcomgletortphysibossgit cd physiboss make install if error happened during the compilation please refer to the installation page running one simulation to run a simulation you need at least a xml parameter file indicating the condition of the simulation and the network file you can find some on maboss website and on our logical modelling pipeline repository other option are possible cf the codedocumentation or this repository wiki for more information example of a parameter file with only few parameter shown xml version10 encodingutf8 simulation time_step 02 time_step mechanics_time_step 01 mechanics_time_step simulation cell_properties mode_motility 1 mode_motility polarity_coefficient 05 polarity_coefficient cell_properties network network_update_step 10 network_update_step network initial_configuration load_cells_from_file inittxt load_cells_from_file initial_configuration image and analyse a simulation to visualize graphically the result of a simulation with use the software paraview or you can also generate a svg snapshot of the simulation analysis of the result file were done with python script proposed in this directory for documentation on how to use paraview to setup the rendering of physiboss output see here with the explication on how to draw sphere from a set of point x y z radius nanohub physiboss can be directly used via a web interface on nanohub this allows to run it without any installation and running directly on the server and can be used without any coding skill the parameter of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server it just required a nanohub account available simulation tool of physiboss can be found on httpsnanohuborgresourcestools under the keywords physiboss or physibossa a model of tumor cell spheroid growing and invading into the surrounding extracellular matrix ecm is currently available physibossa_ecm various parameter a the density of the extracellular matrix the cell motility ecm degradation from the cell tgfbeta production can be tuned by the user documentation codeoriented documentation can be generated with doxygen make doc in the main directory it can be configured in the doxyfile file present in this directory it will generate the html documentation in the dochtml directory you can visualize it in a browser eg firefox dochtmlindexhtml you can also refer to future publication with physiboss for scientific application of this software and description of the model stepbystep example with the necessary file to run them are also proposed in the example directory and on the wiki of this repository reference for physiboss physiboss publication letort g montagud a stoll g heiland r barillot e macklin p zinovyev a calzone l physiboss a multiscale agentbased modelling framework integrating physical dimension and cell signalling bioinformatics bty766 doi101093bioinformaticsbty766 for physicell paul macklins lab website physicell publication a ghaffarizadeh sh friedman sm mumenthaler and p macklin physicell an open source physicsbased cell simulator for 3d multicellular system biorxiv 088773 2016 doi 101101088773 biofvm website biofvm publication a ghaffarizadeh sh friedman and p macklin biofvm an efficient parallelized diffusive transport solver for 3d biological simulation bioinformatics 2015 for maboss maboss website maboss publication stoll g viara e barillot e calzone l continuous time boolean modeling for biological signaling application of gillespie algorithm bmc syst biol 2012 aug 296116 doi 101186175205096116 remark please refer to the wiki of this repository for a much more extended documentation with step by step example instruction physicell is developed in paul macklins lab maboss and physiboss are developed in the computational system biology of cancer group at institut curie paris france we invite you to use physiboss for you research and give feedback to u any help in developing it further is more than welcome do not hesitate to contact u for any comment or difficulty in using physiboss physibossgmailcom wishing you to enjoy using physiboss physibosss team,physiboss multiscale simulation multicellular system overview presentation usage documentation reference remark presentation physiboss physicellmaboss c software multiscale simulation heterogeneous multicellular system integrates together cell internal signalling pathway model boolean formalism physical representation cell agentbased extracellular matrix diffusing fixed entity adapted physicell source boolean network computation inside cell maboss software usage compiling physiboss physiboss run easily installed linux macos system requires moderatly recent version c least c11 openmp support compilation maboss library requires flex bison library usually already present easily installed eg linux ubuntu sudo aptget install bison flex also provide docker image physiboss used cannot installed machine also used without installation via web interface specific simulation nanohub install linux system terminal clone repository local machine go inside main directory type make install install compile maboss physiboss executables created bin directory go well compiled debug release proliling mode set makefile file default release mode fastest might also change c compiler makefile according operating system command list git clone httpsgithubcomgletortphysibossgit cd physiboss make install error happened compilation please refer installation page running one simulation run simulation need least xml parameter file indicating condition simulation network file find maboss website logical modelling pipeline repository option possible cf codedocumentation repository wiki information example parameter file parameter shown xml version10 encodingutf8 simulation &#9; &#9; time_step 02 time_step &#9; &#9; mechanics_time_step 01 mechanics_time_step &#9; &#9; simulation cell_properties &#9; &#9; mode_motility 1 mode_motility &#9; &#9; polarity_coefficient 05 polarity_coefficient &#9; &#9; cell_properties network &#9; &#9; network_update_step 10 network_update_step &#9; &#9; network initial_configuration &#9; &#9; load_cells_from_file inittxt load_cells_from_file &#9; &#9; initial_configuration image analyse simulation visualize graphically result simulation use software paraview also generate svg snapshot simulation analysis result file done python script proposed directory documentation use paraview setup rendering physiboss output see explication draw sphere set point x z radius nanohub physiboss directly used via web interface nanohub allows run without installation running directly server used without coding skill parameter simulation entered interface simulation runned nanohub server required nanohub account available simulation tool physiboss found httpsnanohuborgresourcestools keywords physiboss physibossa model tumor cell spheroid growing invading surrounding extracellular matrix ecm currently available physibossa_ecm various parameter density extracellular matrix cell motility ecm degradation cell tgfbeta production tuned user documentation codeoriented documentation generated doxygen make doc main directory configured doxyfile file present directory generate html documentation dochtml directory visualize browser eg firefox dochtmlindexhtml also refer future publication physiboss scientific application software description model stepbystep example necessary file run also proposed example directory wiki repository reference physiboss physiboss publication letort g montagud stoll g heiland r barillot e macklin p zinovyev calzone l physiboss multiscale agentbased modelling framework integrating physical dimension cell signalling bioinformatics bty766 doi101093bioinformaticsbty766 physicell paul macklins lab website physicell publication ghaffarizadeh sh friedman sm mumenthaler p macklin physicell open source physicsbased cell simulator 3d multicellular system biorxiv 088773 2016 doi 101101088773 biofvm website biofvm publication ghaffarizadeh sh friedman p macklin biofvm efficient parallelized diffusive transport solver 3d biological simulation bioinformatics 2015 maboss maboss website maboss publication stoll g viara e barillot e calzone l continuous time boolean modeling biological signaling application gillespie algorithm bmc syst biol 2012 aug 296116 doi 101186175205096116 remark please refer wiki repository much extended documentation step step example instruction physicell developed paul macklins lab maboss physiboss developed computational system biology cancer group institut curie paris france invite use physiboss research give feedback u help developing welcome hesitate contact u comment difficulty using physiboss physibossgmailcom wishing enjoy using physiboss physibosss team
C++ ,"PhysiBoSS
Multiscale simulation of multi-cellular system
Overview:

Presentation
Usage
Documentation
References
Remarks

Presentation
PhysiBoSS (PhysiCell-MaBoSS) is C++ software for multiscale simulation of heterogeneous multi-cellular system. It integrates together cell's internal signalling pathway model (boolean formalism), physical representation of cell (agent-based) and extra-cellular matrix diffusing or fixed entities.
It is adapted from PhysiCell sources, with the boolean network computation inside each cell from MaBoSS software.

Usage
Compiling PhysiBoSS
PhysiBoSS should run and be easily installed on Linux and MacOS system.
It requires moderatly recent version of C++ (at least c++11) and OpenMP support. Compilation of MaBoSS library requires flex and bison library, usually already present (and can be easily installed on e.g. Linux ubuntu with sudo apt-get install bison flex). We also provide a Docker image of PhysiBoSS that can be used if it cannot be installed in your machine. It can also be used without any installation via a Web interface for specific simulations on nanohub.
To install it on Linux system, from a Terminal:
Clone the repository on your local machine, and go inside the main directory. Type make install, which will install and compile MaBoSS then PhysiBoSS. The executables will be created in the 'bin' directory if all goes well.
It can be compiled in 'Debug', 'Release' or 'Proliling' modes, to set in the 'Makefile' file. Default is 'Release' mode (fastest).
You might also have to change your c++ compiler in the Makefile according to your operating system.
Commands list:
git clone https://github.com/gletort/PhysiBoSS.git
cd PhysiBoSS
make install
If errors happened during the compilation, please refer to the installation page.
Running one simulation
To run a simulation, you need (at least) a XML parameter file indicating the conditions of the simulation, and the networks file (you can find some on MaBoSS website and on our logical modelling pipeline repository).
Other options are possible, cf the code-documentation or this repository wiki for more informations.
Example of a parameter file (with only few parameters shown):
  <?xml version=""1.0"" encoding=""UTF-8"" ?>
 
  <simulation>
 		<time_step> 0.2 </time_step>
 		<mechanics_time_step> 0.1 </mechanics_time_step>
 		....
  </simulation>
 
  <cell_properties>
 		<mode_motility> 1 </mode_motility>
 		<polarity_coefficient> 0.5 </polarity_coefficient>
 		...
  </cell_properties>
 
  <network>
 		<network_update_step> 10 </network_update_step>
 		...
  </network>
 
  <initial_configuration>
 		<load_cells_from_file> init.txt </load_cells_from_file>
 		...
  </initial_configuration>
Image and analyse a simulation
To visualize graphically the result of a simulation, with use the software Paraview (or you can also generate a .svg snapshot of the simulation). Analysis of the result files were done with python scripts proposed in this directory. For documentation on how to use Paraview to set-up the rendering of PhysiBoSS outputs, see here, with the explication on how to draw spheres from a set of points (x, y, z, radius).
Nanohub
PhysiBoSS can be directly used via a Web interface on nanohub. This allows to run it without any installation and running directly on the server and can be used without any coding skills. The parameters of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server. It just required a nanohub account.
Available simulations tools of PhysiBoSS can be found on https://nanohub.org/resources/tools, under the keywords PhysiBoSS or PhysiBoSSa.
A model of tumors cell spheroid growing and invading into the surrounding extra-cellular matrix (ECM) is currently available PhysiBoSSa_ECM. Various parameters as the density of the extracellular matrix, the cell motility, ECM degradation from the cells, TGF-beta production... can be tuned by the user.
Documentation
Code-oriented documentation can be generated with Doxygen:
make doc
in the main directory.
It can be configured in the Doxyfile file present in this directory.
It will generate the html documentation in the doc/html directory.
You can visualize it in a browser, e.g.:
firefox doc/html/index.html &
You can also refer to (future) publications with PhysiBoSS for scientific applications of this software and description of the models.
Step-by-step examples with the necessary files to run them are also proposed in the 'examples' directory and on the Wiki of this repository.
References
 For PhysiBoSS: 

PhysiBoSS publication: Letort G, Montagud A, Stoll G, Heiland R, Barillot E, Macklin P, Zinovyev A, Calzone L .  PhysiBoSS: a multi-scale agent-based modelling framework integrating physical dimension and cell signalling.  Bioinformatics, bty766, doi:10.1093/bioinformatics/bty766


For PhysiCell: 
Paul Macklin's lab website  
PhysiCell publication: A. Ghaffarizadeh, S.H. Friedman, S.M. Mumenthaler, and P. Macklin, PhysiCell: an Open Source Physics-Based Cell Simulator for 3-D Multicellular Systems, bioRxiv 088773, 2016. DOI: 10.1101/088773. 
BioFVM website  
BioFVM publication: A. Ghaffarizadeh, S.H. Friedman, and P. Macklin. BioFVM: an efficient, parallelized diffusive transport solver for 3-D biological simulations. Bioinformatics, 2015. 


For MaBoSS:
MaBoSS website  
MaBoSS publication: Stoll G, Viara E, Barillot E, Calzone L. Continuous time Boolean modeling for biological signaling: application of Gillespie algorithm. BMC Syst Biol. 2012 Aug 29;6:116. doi: 10.1186/1752-0509-6-116. 


Remarks
Please, refer to the Wiki of this repository for a much more extended documentation, with step by step examples instructions.
PhysiCell is developed in Paul Macklin's lab.
MaBoSS and PhysiBoSS are developed in the Computational Systems Biology of Cancer group at Institut Curie (Paris, France).
We invite you to use PhysiBoSS for you research and give feedbacks to us. Any help in developing it further is more than welcome.
Do not hesitate to contact us for any comments or difficulties in using PhysiBoSS: physiboss@gmail.com.
Wishing you to enjoy using PhysiBoSS,
PhysiBoSS's team.
",physiboss multiscal simul of multicellular system overview present usag document refer remark present physiboss physicellmaboss is c softwar for multiscal simul of heterogen multicellular system it integr togeth cell intern signal pathway model boolean formal physic represent of cell agentbas and extracellular matrix diffus or fix entiti it is adapt from physicel sourc with the boolean network comput insid each cell from maboss softwar usag compil physiboss physiboss should run and be easili instal on linux and maco system it requir moderatli recent version of c at least c11 and openmp support compil of maboss librari requir flex and bison librari usual alreadi present and can be easili instal on eg linux ubuntu with sudo aptget instal bison flex we also provid a docker imag of physiboss that can be use if it cannot be instal in your machin it can also be use without ani instal via a web interfac for specif simul on nanohub to instal it on linux system from a termin clone the repositori on your local machin and go insid the main directori type make instal which will instal and compil maboss then physiboss the execut will be creat in the bin directori if all goe well it can be compil in debug releas or prolil mode to set in the makefil file default is releas mode fastest you might also have to chang your c compil in the makefil accord to your oper system command list git clone httpsgithubcomgletortphysibossgit cd physiboss make instal if error happen dure the compil pleas refer to the instal page run one simul to run a simul you need at least a xml paramet file indic the condit of the simul and the network file you can find some on maboss websit and on our logic model pipelin repositori other option are possibl cf the codedocument or thi repositori wiki for more inform exampl of a paramet file with onli few paramet shown xml version10 encodingutf8 simul time_step 02 time_step mechanics_time_step 01 mechanics_time_step simul cell_properti mode_motil 1 mode_motil polarity_coeffici 05 polarity_coeffici cell_properti network network_update_step 10 network_update_step network initial_configur load_cells_from_fil inittxt load_cells_from_fil initial_configur imag and analys a simul to visual graphic the result of a simul with use the softwar paraview or you can also gener a svg snapshot of the simul analysi of the result file were done with python script propos in thi directori for document on how to use paraview to setup the render of physiboss output see here with the explic on how to draw sphere from a set of point x y z radiu nanohub physiboss can be directli use via a web interfac on nanohub thi allow to run it without ani instal and run directli on the server and can be use without ani code skill the paramet of the simul can be enter in the interfac and then the simul will be run on the nanohub server it just requir a nanohub account avail simul tool of physiboss can be found on httpsnanohuborgresourcestool under the keyword physiboss or physibossa a model of tumor cell spheroid grow and invad into the surround extracellular matrix ecm is current avail physibossa_ecm variou paramet as the densiti of the extracellular matrix the cell motil ecm degrad from the cell tgfbeta product can be tune by the user document codeori document can be gener with doxygen make doc in the main directori it can be configur in the doxyfil file present in thi directori it will gener the html document in the dochtml directori you can visual it in a browser eg firefox dochtmlindexhtml you can also refer to futur public with physiboss for scientif applic of thi softwar and descript of the model stepbystep exampl with the necessari file to run them are also propos in the exampl directori and on the wiki of thi repositori refer for physiboss physiboss public letort g montagud a stoll g heiland r barillot e macklin p zinovyev a calzon l physiboss a multiscal agentbas model framework integr physic dimens and cell signal bioinformat bty766 doi101093bioinformaticsbty766 for physicel paul macklin lab websit physicel public a ghaffarizadeh sh friedman sm mumenthal and p macklin physicel an open sourc physicsbas cell simul for 3d multicellular system biorxiv 088773 2016 doi 101101088773 biofvm websit biofvm public a ghaffarizadeh sh friedman and p macklin biofvm an effici parallel diffus transport solver for 3d biolog simul bioinformat 2015 for maboss maboss websit maboss public stoll g viara e barillot e calzon l continu time boolean model for biolog signal applic of gillespi algorithm bmc syst biol 2012 aug 296116 doi 101186175205096116 remark pleas refer to the wiki of thi repositori for a much more extend document with step by step exampl instruct physicel is develop in paul macklin lab maboss and physiboss are develop in the comput system biolog of cancer group at institut curi pari franc we invit you to use physiboss for you research and give feedback to us ani help in develop it further is more than welcom do not hesit to contact us for ani comment or difficulti in use physiboss physibossgmailcom wish you to enjoy use physiboss physibosss team,physiboss multiscale simulation of multicellular system overview presentation usage documentation reference remark presentation physiboss physicellmaboss is c software for multiscale simulation of heterogeneous multicellular system it integrates together cell internal signalling pathway model boolean formalism physical representation of cell agentbased and extracellular matrix diffusing or fixed entity it is adapted from physicell source with the boolean network computation inside each cell from maboss software usage compiling physiboss physiboss should run and be easily installed on linux and macos system it requires moderatly recent version of c at least c11 and openmp support compilation of maboss library requires flex and bison library usually already present and can be easily installed on eg linux ubuntu with sudo aptget install bison flex we also provide a docker image of physiboss that can be used if it cannot be installed in your machine it can also be used without any installation via a web interface for specific simulation on nanohub to install it on linux system from a terminal clone the repository on your local machine and go inside the main directory type make install which will install and compile maboss then physiboss the executables will be created in the bin directory if all go well it can be compiled in debug release or proliling mode to set in the makefile file default is release mode fastest you might also have to change your c compiler in the makefile according to your operating system command list git clone httpsgithubcomgletortphysibossgit cd physiboss make install if error happened during the compilation please refer to the installation page running one simulation to run a simulation you need at least a xml parameter file indicating the condition of the simulation and the network file you can find some on maboss website and on our logical modelling pipeline repository other option are possible cf the codedocumentation or this repository wiki for more information example of a parameter file with only few parameter shown xml version10 encodingutf8 simulation time_step 02 time_step mechanics_time_step 01 mechanics_time_step simulation cell_properties mode_motility 1 mode_motility polarity_coefficient 05 polarity_coefficient cell_properties network network_update_step 10 network_update_step network initial_configuration load_cells_from_file inittxt load_cells_from_file initial_configuration image and analyse a simulation to visualize graphically the result of a simulation with use the software paraview or you can also generate a svg snapshot of the simulation analysis of the result file were done with python script proposed in this directory for documentation on how to use paraview to setup the rendering of physiboss output see here with the explication on how to draw sphere from a set of point x y z radius nanohub physiboss can be directly used via a web interface on nanohub this allows to run it without any installation and running directly on the server and can be used without any coding skill the parameter of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server it just required a nanohub account available simulation tool of physiboss can be found on httpsnanohuborgresourcestools under the keywords physiboss or physibossa a model of tumor cell spheroid growing and invading into the surrounding extracellular matrix ecm is currently available physibossa_ecm various parameter a the density of the extracellular matrix the cell motility ecm degradation from the cell tgfbeta production can be tuned by the user documentation codeoriented documentation can be generated with doxygen make doc in the main directory it can be configured in the doxyfile file present in this directory it will generate the html documentation in the dochtml directory you can visualize it in a browser eg firefox dochtmlindexhtml you can also refer to future publication with physiboss for scientific application of this software and description of the model stepbystep example with the necessary file to run them are also proposed in the example directory and on the wiki of this repository reference for physiboss physiboss publication letort g montagud a stoll g heiland r barillot e macklin p zinovyev a calzone l physiboss a multiscale agentbased modelling framework integrating physical dimension and cell signalling bioinformatics bty766 doi101093bioinformaticsbty766 for physicell paul macklins lab website physicell publication a ghaffarizadeh sh friedman sm mumenthaler and p macklin physicell an open source physicsbased cell simulator for 3d multicellular system biorxiv 088773 2016 doi 101101088773 biofvm website biofvm publication a ghaffarizadeh sh friedman and p macklin biofvm an efficient parallelized diffusive transport solver for 3d biological simulation bioinformatics 2015 for maboss maboss website maboss publication stoll g viara e barillot e calzone l continuous time boolean modeling for biological signaling application of gillespie algorithm bmc syst biol 2012 aug 296116 doi 101186175205096116 remark please refer to the wiki of this repository for a much more extended documentation with step by step example instruction physicell is developed in paul macklins lab maboss and physiboss are developed in the computational system biology of cancer group at institut curie paris france we invite you to use physiboss for you research and give feedback to u any help in developing it further is more than welcome do not hesitate to contact u for any comment or difficulty in using physiboss physibossgmailcom wishing you to enjoy using physiboss physibosss team,physiboss multiscale simulation multicellular system overview presentation usage documentation reference remark presentation physiboss physicellmaboss c software multiscale simulation heterogeneous multicellular system integrates together cell internal signalling pathway model boolean formalism physical representation cell agentbased extracellular matrix diffusing fixed entity adapted physicell source boolean network computation inside cell maboss software usage compiling physiboss physiboss run easily installed linux macos system requires moderatly recent version c least c11 openmp support compilation maboss library requires flex bison library usually already present easily installed eg linux ubuntu sudo aptget install bison flex also provide docker image physiboss used cannot installed machine also used without installation via web interface specific simulation nanohub install linux system terminal clone repository local machine go inside main directory type make install install compile maboss physiboss executables created bin directory go well compiled debug release proliling mode set makefile file default release mode fastest might also change c compiler makefile according operating system command list git clone httpsgithubcomgletortphysibossgit cd physiboss make install error happened compilation please refer installation page running one simulation run simulation need least xml parameter file indicating condition simulation network file find maboss website logical modelling pipeline repository option possible cf codedocumentation repository wiki information example parameter file parameter shown xml version10 encodingutf8 simulation &#9; &#9; time_step 02 time_step &#9; &#9; mechanics_time_step 01 mechanics_time_step &#9; &#9; simulation cell_properties &#9; &#9; mode_motility 1 mode_motility &#9; &#9; polarity_coefficient 05 polarity_coefficient &#9; &#9; cell_properties network &#9; &#9; network_update_step 10 network_update_step &#9; &#9; network initial_configuration &#9; &#9; load_cells_from_file inittxt load_cells_from_file &#9; &#9; initial_configuration image analyse simulation visualize graphically result simulation use software paraview also generate svg snapshot simulation analysis result file done python script proposed directory documentation use paraview setup rendering physiboss output see explication draw sphere set point x z radius nanohub physiboss directly used via web interface nanohub allows run without installation running directly server used without coding skill parameter simulation entered interface simulation runned nanohub server required nanohub account available simulation tool physiboss found httpsnanohuborgresourcestools keywords physiboss physibossa model tumor cell spheroid growing invading surrounding extracellular matrix ecm currently available physibossa_ecm various parameter density extracellular matrix cell motility ecm degradation cell tgfbeta production tuned user documentation codeoriented documentation generated doxygen make doc main directory configured doxyfile file present directory generate html documentation dochtml directory visualize browser eg firefox dochtmlindexhtml also refer future publication physiboss scientific application software description model stepbystep example necessary file run also proposed example directory wiki repository reference physiboss physiboss publication letort g montagud stoll g heiland r barillot e macklin p zinovyev calzone l physiboss multiscale agentbased modelling framework integrating physical dimension cell signalling bioinformatics bty766 doi101093bioinformaticsbty766 physicell paul macklins lab website physicell publication ghaffarizadeh sh friedman sm mumenthaler p macklin physicell open source physicsbased cell simulator 3d multicellular system biorxiv 088773 2016 doi 101101088773 biofvm website biofvm publication ghaffarizadeh sh friedman p macklin biofvm efficient parallelized diffusive transport solver 3d biological simulation bioinformatics 2015 maboss maboss website maboss publication stoll g viara e barillot e calzone l continuous time boolean modeling biological signaling application gillespie algorithm bmc syst biol 2012 aug 296116 doi 101186175205096116 remark please refer wiki repository much extended documentation step step example instruction physicell developed paul macklins lab maboss physiboss developed computational system biology cancer group institut curie paris france invite use physiboss research give feedback u help developing welcome hesitate contact u comment difficulty using physiboss physibossgmailcom wishing enjoy using physiboss physibosss team
C++ ,"


Core Principles
These are the core principles of object-oriented approach to the current state of artificial neural networks that is inspired by synaptic plasticity between biological neurons:

Unlike the current ANN implementations, neurons must be objects not tensors between matrices.
Just the current ANN implementations, neurons should be GPU accelerated (ideally) to provide the necessary parallelism.
While the current ANN implementations can only create special cases, a Plexus Network must be architecture-free (i.e. adaptive) to create a generalized solution of all machine learning problems.
Instead of dealing with decision of choosing an ANN layer combination(such as Convolution, Pooling or Recurrent layers), the network must have a layerless design.
There must be fundamentally two types of neurons: sensory neuron, interneuron.
Input of the network must be made of sensory neurons. Any interneuron can be picked as a motor neuron (an element of the output). There are literally no difference between an interneuron and a motor neuron except the intervene of the network for igniting the wick of learning process through the motor neurons. Any non-motor interneuron can be assumed as a cognitive neuron which collectively forms the cognition of network.
There can be arbitrary amount of I/O groups in a single network.
Instead of batch size, iteration, and epoch concepts, training examples must be fed on time basis with a manner like; learn first sample for X seconds, OK done? then learn second sample for Y seconds. By this approach, you can assign importance factors to your samples with maximum flexibility.
Network must be retrainable.
Network must be modular. In other words: You must be able to train a small network and then plug that network into a bigger network (we are talking about some kind of self-fusing here).
Neurons must exhibit the characteristics of cellular automata just like Conway's Game of Life.
Number of neurons in the network can be increased or decreased (scalability).
There must be no need for a network-wide oscillation. Yet the execution of neurons should follow a path very similar to flow of electric current nevertheless.
Network should use randomness and/or uncertainty principle flawlessly. Consciousness is an emergent property from cellular level to macro scale, the network. But it's also an emergent property for the neuron from quantum level uncertainty to cellular mechanisms. In such a way that randomness is the cause of the illusion of consciousness.
Most importantly, the network must and can not iterate through the whole dataset. Besides that, it's also generally impossible to iterate the whole dataset on real life situations if the system is continuous like in robotics. Because of that; the network must be designed to handle such a continuous data stream that literally endless and must be designed to handle that data stream chunk by chunk. Therefore, when you are feeding the network, use a diverse feed but not a grouped feed (like 123123123123123123 but not like 111111222222333333).

Activation function
The activation function that used by Plexus is Sigmoid:



and the derivative of the Sigmoid function:



Implementation of this algorithm in Python programming language is publicly accessible through this link: https://github.com/mertyildiran/Plexus/blob/master/plexus/plexus.py
You can directly skip to Application part if you are not willing to understand the mathematical and algorithmic background.
Basics
Plexus Network has only two classes; Network and Neuron. In a Plexus Network, there are many instances of Neuron class but there is only one instance of Network class.
When you crate a new Plexus Network you give these five parameters to the Network class: size of the network, input dimension, output dimension, connectivity rate, precision. The network accordingly builds itself.




size is literally equal to total number of neurons in the network. All neurons are referenced in an instance variable called Network.neurons
input dimension specifies the number of sensory neurons. Sensory neurons are randomly selected from neurons.
output dimension specifies the number of motor neurons. Motor neurons are randomly selected from non-sensory neurons.
number of neurons multiplied by connectivity rate gives the average number of subscriptions made by a single neuron.





precision simply defines the precision of the all calculations will be made by neurons (how many digits after the decimal point).

After the network has been successfully created. It will ignite itself automatically. Ignition in simple terms, no matter if you have plugged in some data or not, it will fire the neurons with using some mechanism very similar to flow of electric current (will be explained later on this paper).
Anatomy of a Single Neuron
A single neuron in a Plexus Network, takes the network as the only parameter and stores these seven very important information (in it's instance variables): subscriptions, publications, potential, desired_potential, loss and type



There are eventually there types of neurons:

Neuron.type = 1 means it's a sensory neuron.
Neuron.type = 2 means it's a motor neuron.
Neuron.type = 0 means it's neither a sensory nor a motor neuron. It means it's an cognitive interneuron (or just cognitive neuron).

Functionality of a neuron is relative to its type.
subscriptions is neuron's indirect data feed. Each non-sensory neuron subscribes to some other neurons of any type. For sensory neurons, subscriptions are completely meaningless and empty by default because it gets its data feed from outside world by assignments of the network. Subscriptions are literally the Plexus Network equivalent of Dendrites in biological neurons. subscriptions is a dictionary that holds Neuron(reference) as key and Weight as value.
publications holds literally the mirror data of subscriptions in the target neurons. In other words; any subscription creates also a publication reference in the target neuron. Similarly, publications is the Plexus Network equivalent of Axons in biological neurons.
potential p is the overall total potential value of all subscriptions multiplied by the corresponding weights. Only in sensory neurons, it is directly assigned by the network. Value of potential may only be updated by the neuron's itself and its being calculated by this simple formula each time when the neuron is fired:






desired_potential p' is the ideal value of the neuron's potential that is desired to eventually reach. For sensory neurons, it is meaningless. For motor neurons, it is assigned by the network. If it's None then the neuron does not learn anything and just calculates potential when it's fired.
loss l is calculated not just at the output but in every neuron except sensory ones and it is equal to absolute difference (distance) between desired potential and current potential.



All numerical values inside a neuron are floating point numbers and all the calculations obey to the precision that given at start.
Sensory and Motor Neurons
Input Layer in classical neural networks renamed as Sensory Neurons in Plexus networks, and Target/Output Layer renamed as Motor Neurons. This naming convention is necessary cause the built of the relevance of artificial neural networks with biological neural networks and Neuroscience.
The difference of sensory neurons from the cognitive neurons (that neither sensory nor motor ones) is, they do not actually fire. They just stand still for the data load. They do not have any subscriptions to the other neurons (literally no subscriptions). But they can be subscribed by the other neurons, including motor ones. They do not learn, they do not consume any CPU resources. They just stored in the memory. You can assign an image, a frame of a video, or a chunk of an audio to a group of sensory neurons.
The difference of motor neurons form the other neurons is, they are only responsible to the network. They act as the fuse of the learning and calculation of the loss. The network dictates a desired potential on each motor neuron. The motor neuron calculates its potential, compares it with desired potential, calculates the loss then tries to update its weights randomly many times and if it fails, it blames its subscriptions. So just like the network, motor neurons are also able to dictate a desired potential on the other non-motor neurons. This is why any neuron holds an additional potential variable called desired_potential.
Partially Subscribe
On the second phase of the network initiation, any non-sensory neurons are forced to subscribe to some non-motor neurons which are selected by random sampling. Length of this sample is also selected by random sampling (rounds to nearest integer) is done from a normal distribution. Such a normal distribution that, the average number of subscriptions is the mean, and square root of the mean is the standard deviation. (e.g. if neurons on average has 100 subscriptions then the mean is 100 and the standard deviation is 10)



Algorithm
Even so the Python implementation of Plexus Network is easy to understand, it will be helpful for readers to explain the algorithm in pseudocode;
Initiation
procedure initiate the network is
    connectivity ← size * connectivity_rate;
    connectivity_sqrt ← sqrt(connectivity);
    connectivity_sqrt_sqrt ← sqrt(connectivity_sqrt);
    for item in size, do
        create neuron;
    end
    pick sensory neurons randomly;
    pick motor neurons randomly;
    determine non-sensory neurons;
    determine non-motor neurons;
    initiate subscriptions;
    initiate instance variables;
    ignite the network;
Initiation is nothing more than a make the assignments for once phase until the ignition. The final step (ignition) never stops but can be paused (if user wants).
Initiate Subscriptions
procedure initiate subscriptions is
    for neuron in neurons, do
        if neuron is not a sensory neuron, then
            call neuron.partially_subscribe();
        end
    end
    return True;
Partially Subscribe
procedure partially subscribe is
    sample ← randomly sample approximately ""connectivity"" units of a neuron from within all non-motor neurons;
    for neuron in sample, do
        if neuron is not self, then
            establish a subscription;    // weight is randomly assigned
            establish a publication;
        end
    end
    return True;
The time complexity of the procedure initiate subscriptions is O(n2), so this may take a while if the size of the network and connectivity is big.
Ignite
procedure ignite subscriptions is
    create an empty ban_list;
    while network is not frozen, do
        if next_queue is empty, then
            get the output of network and print it;
            increase the wave_counter;
            if first_queue is empty, then
                for neuron in sensory neurons, do
                    for target_neuron in neuron.publications, do
                        append target_neuron to first_queue;
                    end
                end
                copy first_queue to next_queue;
            end
        end
        copy next_queue to current_queue;
        empty next_queue;
        for neuron in ban_list, do
            if neuron.ban_counter > connectivity_sqrt_sqrt, then
                remove the neuron from current_queue;
            end
        end
        while current_queue is not empty, do
            neuron ← select a random neuron from current_queue;
            remove the neuron from current_queue;
            if neuron.ban_counter <= connectivity_sqrt_sqrt, then
                call neuron.fire();
                append the neuron to ban_list;
                increase neuron.ban_counter;
                for target_neuron in neuron.publications, do
                    append target_neuron to next_queue;
                end
            end
        end
    end
Procedure ignite regulates the firing order of neurons and creates an effect very similar to flow of electric current, network wide. It continuously runs until the network frozen, nothing else can stop it. It fires the neurons step by step through adding them to a queue.
It generates its first queue from the publications of sensory neurons. Time complexity of if next_queue is empty, then block is O(n2) but it can be ignored (unless there are too many sensory neurons) because it runs once per wave.
It eliminates banned neurons with for neuron in ban_list, do block. Function of ban_counter is giving neurons connectivity_sqrt_sqrt amount of chance after they added to ban_list. Then it fires the neurons inside current_queue, one by one, choosing them randomly.
After a neuron fired, it adds the fired neuron to ban_list and lastly copies the publications of that neuron to next_queue so execution(firing process) can follow the path through the connections.
Each execution from first sensory neuron to last motor neuron symbolizes one wave. Every time a wave finished, procedure falls into if next_queue is empty, then block so wave starts over from the sensory neurons.
ban_counter and connectivity_sqrt_sqrt comparison creates execution loops inside cognitive neurons and these loops act like memory units which is a pretty important concept. Because loops create the relation between currently fed data and previously learned data. Without these loops the network fails on both classification and regression problems.
Because neuron.fire() has a time complexity of O(n2), each turn inside while network is not frozen, do block, has a time complexity of O(n4). But don't worry because it will approximate to O(n3) because of the probabilistic nature of fire function and the network will fire more than a million of neurons per minute. By the way, while network is not frozen, do block is ignored because it's an endless loop under normal conditions.
Fire
procedure fire is
    if self is not a sensory neuron, then
        potential ← calculate potential;
        increase fire counter;
        if desired_potential is not None, then

            loss ← calculate loss;
            if loss = 0, then
                desired_potential ← None;
                return True;
            end
            if blame_lock is not empty, then
                if (wave_counter - blame_lock) < connectivity, then
                    return True;
                else
                    blame_lock ← None;
            end

            try connectivity times:
                generate new weights randomly;
                calculate new potential and new loss according to these weights;
                if loss_new < loss_current, then return True;
            end

            try sqrt(connectivity) times:
                generate hypothetical potentials for neurons in subscriptions randomly;
                calculate new potential and new loss according to these hypothetical potentials;
                if loss_new < loss_current, then
                    apply these hypothetical potentials as ""desired_potential""s;
                    return True;
                end
            end

            if (still) not improved, then
                either create some new subscriptions;
                or break some of the subscriptions;
                return True;
            end

        end
    end
Procedure fire handles all feedforwarding, backpropagation and learning process by itself. fire function is an instance method of Neuron class. This procedure is by far the most important one in the Plexus Network. It's basically the core function and CPU spends most of its time to execute fire functions again and again.
If desired_potential is not assigned to a value, then it just calculates the potential and finishes.
If desired_potential is assigned to a value, then first it calculates the loss. If loss is equal to zero, then the current state of the neuron is perfectly well and there is nothing to learn.
If blame_lock is not empty, then it will pass this function connectivity times with this control statement: if blame_lock is not empty, then.



It tries to improve the current state of the neuron by updating its weights randomly, connectivity times. If it's improved, then break.
It tries to improve the current state of the neuron by dictating randomly generated hypothetical potentials over the subscriptions, square root of connectivity times. If it's improved, then break.
If it still is not improved, then it either creates some new subscriptions or breaks some of the subscriptions it currently has and hopes it will lead the neuron to new improvements in the future.
On the first wave, the fire function is only meaningful for motor neurons but after the first wave desired_potential dictation will spread throughout the cognitive neurons.
Load
procedure load (input, output) is
    if output is None, then
        for neuron in motor neurons, do
            neuron.desired_potential ← None;
        end
    end
    if (number of sensory neurons is not equal to input length), then
        raise an error but do not interrupt the network;
    else
        for neuron in sensory neurons, do
            neuron.potential ← load from the input;
        end
    end
    if (number of motor neurons is not equal to output length), then
        raise an error but do not interrupt the network;
    else
        for neuron in motor neurons, do
            neuron.desired_potential ← load from the output;
        end
    end
Procedure load is the only method that you can feed your data to the network. You should call that function and load your data in real time. Also you should do it periodically and continuously, like every 3 seconds. If you leave second parameter empty then this procedure will automatically assume that you are testing the network, so it will replace desired_potential values of motor neurons with None. Otherwise, it means you are training the network so it will load the input data to sensory neurons and it will load the output data to desired_potential values of motor neurons.
Application
Installation of the Python Package
pip install plexus
If you want to install Plexus on development mode:
git clone https://github.com/mertyildiran/Plexus.git
cd Plexus/
pip install -e .
or alternatively:
make dev
and test the installation with:
make cpp
Examples
Binary Classification Example
(you can alternatively run this example with python3 examples/classification_binary.py command using a pre-written script version of below commands)
Suppose you need to train the network to figure out that the elements of given arrays are bigger than 0.5 or not (like [0.9, 0.6, 1.0, 0.8] or [0.1, 0.3, 0.0, 0.4]) and suppose it's a 4-element array. So let's create a network according to your needs:
import cplexus as plexus

SIZE = 14
INPUT_SIZE = 4
OUTPUT_SIZE = 2
CONNECTIVITY = 1
PRECISION = 2
RANDOMLY_FIRE = False
DYNAMIC_OUTPUT = True
VISUALIZATION = False
net = plexus.Network(
    SIZE,
    INPUT_SIZE,
    OUTPUT_SIZE,
    CONNECTIVITY,
    PRECISION,
    RANDOMLY_FIRE,
    DYNAMIC_OUTPUT,
    VISUALIZATION
)
If you want to visualize the network using PyQtGraph enable VISUALIZATION = False. Because our network is automatically initiated and ignited, now all we have to do is training the network. So let's train our network with 80 samples:
PRECISION = 2
TRAINING_SAMPLE_SIZE = 20
for i in range(1, TRAINING_SAMPLE_SIZE):
    if (i % 2) == 0:
        output = [1.0, 0.0]
        generated_list = generate_list_bigger()
        notify_the_load(generated_list, output, TRAINING_DURATION)
        net.load(generated_list, output)
    else:
        output = [0.0, 1.0]
        generated_list = generate_list_smaller()
        notify_the_load(generated_list, output, TRAINING_DURATION)
        net.load(generated_list, output)
    time.sleep(TRAINING_DURATION)
You should load your data one by one from each kind, respectively. Because it will prevent over fitting to one specific kind. You must wait a short time like TRAINING_DURATION = 0.01 seconds (which is a reasonable duration in such a case), after each load.
output[0] will converge to detect bigger than 0.5 inputs.
output[1] will converge to detect smaller than 0.5 inputs.
Before the testing you should define a criteria called DOMINANCE_THRESHOLD so you can catch the decision making. Now let's test the network:
error = 0
error_divisor = 0
for i in repeat(None, TESTING_SAMPLE_SIZE):
    binary_random = random.randint(0, 1)
    if binary_random == 0:
        generated_list = generate_list_bigger()
        expected = [1.0, 0.0]
    else:
        generated_list = generate_list_smaller()
        expected = [0.0, 1.0]

    net.load(generated_list)
    time.sleep(TRAINING_DURATION)

    output = net.output
    error += abs(expected[0] - output[0])
    error += abs(expected[1] - output[1])
    error_divisor += 2
With the while loop given above, you will be able to check the output by giving enough time to propagate your input throught the network. By giving net.load() only one parameter here, you automatically disable the training.
Now freeze your network and calculate the overall error:
net.freeze()
error = error / error_divisor
which outputs:
Overall error: 0.010249996604397894

Classifying Prime Numbers Example
This example is quite simple to the previous example but this time we are teaching the network to understand if the given number is prime or not. Which is a relatively complex problem.
Run python3 examples/classification_prime.py 1 -l cpp to see the result. You will observe that the network is able to learn the solution for such a complex problem in the matter of seconds.
Sequence Basic Example
In this example, instead of classification, we will train the network to detect a pattern in given sequence. The magic here is; without even changing anything related to network, just by changing logic we feed the data into the network, the network automatically turns into a Recurrent Neural Network.
Run python3 examples/sequence_basic.py to see the output. This is the output you should see:
___ PLEXUS NETWORK BASIC SEQUENCE RECOGNITION EXAMPLE ___

Create a Plexus network with 14 neurons, 4 of them sensory, 1 of them motor, 1 connectivity rate, 2 digit precision

Precision of the network will be 0.01
Each individual non-sensory neuron will subscribe to 14 different neurons
14 neurons created
4 neuron picked as sensory neuron
1 neuron picked as motor neuron
Network has been ignited

*** LEARNING ***

Generate The Dataset (20 Items Long) To Recognize a Sequence & Learn for 0.1 Seconds Each
Load Input: [1.0, 0.0, 0.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 1.0, 0.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 0.0, 1.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 0.0, 0.0, 1.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [1.0, 0.0, 0.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 1.0, 0.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 0.0, 1.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 0.0, 0.0, 1.0]	Output: [1.0]	and wait 0.1 seconds
Load Input: [1.0, 0.0, 0.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 1.0, 0.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 0.0, 1.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 0.0, 0.0, 1.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [1.0, 0.0, 0.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 1.0, 0.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 0.0, 1.0, 0.0]	Output: [0.0]	and wait 0.1 seconds
Load Input: [0.0, 0.0, 0.0, 1.0]	Output: [1.0]	and wait 0.1 seconds
...

by looking at this output, you should be able to see the pattern. Now on testing stage you can see how successful the network is on detecting the pattern:
*** TESTING ***

Test the network with random data (20 times)
Load Input: ([1.0, 0.0, 0.0, 0.0], [0.0])	RESULT: 0.019999999552965164
Load Input: ([0.0, 1.0, 0.0, 0.0], [0.0])	RESULT: 0.0
Load Input: ([0.0, 0.0, 1.0, 0.0], [0.0])	RESULT: 0.019999999552965164
Load Input: ([0.0, 0.0, 0.0, 1.0], [0.0])	RESULT: 0.019999999552965164
Load Input: ([1.0, 0.0, 0.0, 0.0], [0.0])	RESULT: 0.0
Load Input: ([0.0, 1.0, 0.0, 0.0], [0.0])	RESULT: 0.0
Load Input: ([0.0, 0.0, 1.0, 0.0], [0.0])	RESULT: 0.05999999865889549
Load Input: ([0.0, 0.0, 0.0, 1.0], [1.0])	RESULT: 0.6600000262260437
Load Input: ([1.0, 0.0, 0.0, 0.0], [0.0])	RESULT: 0.019999999552965164
Load Input: ([0.0, 1.0, 0.0, 0.0], [0.0])	RESULT: 0.0
Load Input: ([0.0, 0.0, 1.0, 0.0], [0.0])	RESULT: 0.05999999865889549
Load Input: ([0.0, 0.0, 0.0, 1.0], [0.0])	RESULT: 0.6600000262260437
Load Input: ([1.0, 0.0, 0.0, 0.0], [0.0])	RESULT: 0.019999999552965164
Load Input: ([0.0, 1.0, 0.0, 0.0], [0.0])	RESULT: 0.0
Load Input: ([0.0, 0.0, 1.0, 0.0], [0.0])	RESULT: 0.0
Load Input: ([0.0, 0.0, 0.0, 1.0], [1.0])	RESULT: 0.9800000190734863
...

and the overall error:
Network is now frozen

1786760 waves are executed throughout the network

In total: 66110093 times a random non-sensory neuron is fired


Overall error: 0.040124996623490006

CatDog Example
(you can alternatively run this example with python3 examples/catdog.py command using a pre-written script version of below commands)
Suppose you need to train the network to figure out that the given image (32x32 RGB) is an image of a cat or a dog and map them to blue and red respectively. So let's create a network according to your needs:
SIZE = 32 * 32 * 3 + 3 + 256
INPUT_SIZE = 32 * 32 * 3
OUTPUT_SIZE = 3
CONNECTIVITY = 0.005
PRECISION = 3
TRAINING_DURATION = 3
RANDOMLY_FIRE = False
DYNAMIC_OUTPUT = False
VISUALIZATION = False
net = plexus.Network(
    SIZE,
    INPUT_SIZE,
    OUTPUT_SIZE,
    CONNECTIVITY,
    PRECISION,
    RANDOMLY_FIRE,
    DYNAMIC_OUTPUT,
    VISUALIZATION
)
We will plug in 32x32 RGB to the network so we need 3072 sensory neurons. 3 motor neurons for see how RGB our result is and 256 cognitive neurons to train. We need 3 digits precision because we need to store 255 different values between 0.0 and 1.0 range.
Explaining the answer of How to load CIFAR-10 dataset and use it is out of the scope of this paper but you can easily understand it by reading the code: examples/catdog.py Once you get the numpy array of CIFAR-10 (or any other image data) just normalize it and load:
TRAINING_SAMPLE_SIZE = 20
for i in range(1, TRAINING_SAMPLE_SIZE):
    if (i % 2) == 0:
        cat = random.sample(cats, 1)[0]
        cat_normalized = np.true_divide(cat, 255).flatten()
        blue_normalized = np.true_divide(blue, 255).flatten()
        cv2.imshow(""Input"", cat)
        net.load(cat_normalized, blue_normalized)
    else:
        dog = random.sample(dogs, 1)[0]
        dog_normalized = np.true_divide(dog, 255).flatten()
        red_normalized = np.true_divide(red, 255).flatten()
        cv2.imshow(""Input"", dog)
        net.load(dog_normalized, red_normalized)
    show_output(net)
You will get an Overall error as the result very similar to examples above although this time the input length was 768 times bigger. This is because Plexus Network amalgamates the problems from all levels of difficulty on a single medium. It makes easy problems relatively hard, and hard problems relatively easy.
When you run this example, you will get a slightly better result when compared to flipping a coin. You will most likely get an Overall error between 0.35 - 0.45 which is the proof that the network is able to learn something.
By the way, don't forget that; Plexus Network does not iterate over the dataset and furthermore it runs in real-time. Also you have trained the network just for 4-5 minutes. Now let's see what happens if we train our network for a long period of time:
Note
Implementation of GPU acceleration and saving the trained network to disk are in work-in-progress (WIP) state. Therefore some parts of the implementation are subject to change.
",core principl these are the core principl of objectori approach to the current state of artifici neural network that is inspir by synapt plastic between biolog neuron unlik the current ann implement neuron must be object not tensor between matric just the current ann implement neuron should be gpu acceler ideal to provid the necessari parallel while the current ann implement can onli creat special case a plexu network must be architecturefre ie adapt to creat a gener solut of all machin learn problem instead of deal with decis of choos an ann layer combinationsuch as convolut pool or recurr layer the network must have a layerless design there must be fundament two type of neuron sensori neuron interneuron input of the network must be made of sensori neuron ani interneuron can be pick as a motor neuron an element of the output there are liter no differ between an interneuron and a motor neuron except the interven of the network for ignit the wick of learn process through the motor neuron ani nonmotor interneuron can be assum as a cognit neuron which collect form the cognit of network there can be arbitrari amount of io group in a singl network instead of batch size iter and epoch concept train exampl must be fed on time basi with a manner like learn first sampl for x second ok done then learn second sampl for y second by thi approach you can assign import factor to your sampl with maximum flexibl network must be retrain network must be modular in other word you must be abl to train a small network and then plug that network into a bigger network we are talk about some kind of selffus here neuron must exhibit the characterist of cellular automata just like conway game of life number of neuron in the network can be increas or decreas scalabl there must be no need for a networkwid oscil yet the execut of neuron should follow a path veri similar to flow of electr current nevertheless network should use random andor uncertainti principl flawlessli conscious is an emerg properti from cellular level to macro scale the network but it also an emerg properti for the neuron from quantum level uncertainti to cellular mechan in such a way that random is the caus of the illus of conscious most importantli the network must and can not iter through the whole dataset besid that it also gener imposs to iter the whole dataset on real life situat if the system is continu like in robot becaus of that the network must be design to handl such a continu data stream that liter endless and must be design to handl that data stream chunk by chunk therefor when you are feed the network use a divers feed but not a group feed like 123123123123123123 but not like 111111222222333333 activ function the activ function that use by plexu is sigmoid and the deriv of the sigmoid function implement of thi algorithm in python program languag is publicli access through thi link httpsgithubcommertyildiranplexusblobmasterplexusplexuspi you can directli skip to applic part if you are not will to understand the mathemat and algorithm background basic plexu network ha onli two class network and neuron in a plexu network there are mani instanc of neuron class but there is onli one instanc of network class when you crate a new plexu network you give these five paramet to the network class size of the network input dimens output dimens connect rate precis the network accordingli build itself size is liter equal to total number of neuron in the network all neuron are referenc in an instanc variabl call networkneuron input dimens specifi the number of sensori neuron sensori neuron are randomli select from neuron output dimens specifi the number of motor neuron motor neuron are randomli select from nonsensori neuron number of neuron multipli by connect rate give the averag number of subscript made by a singl neuron precis simpli defin the precis of the all calcul will be made by neuron how mani digit after the decim point after the network ha been success creat it will ignit itself automat ignit in simpl term no matter if you have plug in some data or not it will fire the neuron with use some mechan veri similar to flow of electr current will be explain later on thi paper anatomi of a singl neuron a singl neuron in a plexu network take the network as the onli paramet and store these seven veri import inform in it instanc variabl subscript public potenti desired_potenti loss and type there are eventu there type of neuron neurontyp 1 mean it a sensori neuron neurontyp 2 mean it a motor neuron neurontyp 0 mean it neither a sensori nor a motor neuron it mean it an cognit interneuron or just cognit neuron function of a neuron is rel to it type subscript is neuron indirect data feed each nonsensori neuron subscrib to some other neuron of ani type for sensori neuron subscript are complet meaningless and empti by default becaus it get it data feed from outsid world by assign of the network subscript are liter the plexu network equival of dendrit in biolog neuron subscript is a dictionari that hold neuronrefer as key and weight as valu public hold liter the mirror data of subscript in the target neuron in other word ani subscript creat also a public refer in the target neuron similarli public is the plexu network equival of axon in biolog neuron potenti p is the overal total potenti valu of all subscript multipli by the correspond weight onli in sensori neuron it is directli assign by the network valu of potenti may onli be updat by the neuron itself and it be calcul by thi simpl formula each time when the neuron is fire desired_potenti p is the ideal valu of the neuron potenti that is desir to eventu reach for sensori neuron it is meaningless for motor neuron it is assign by the network if it none then the neuron doe not learn anyth and just calcul potenti when it fire loss l is calcul not just at the output but in everi neuron except sensori one and it is equal to absolut differ distanc between desir potenti and current potenti all numer valu insid a neuron are float point number and all the calcul obey to the precis that given at start sensori and motor neuron input layer in classic neural network renam as sensori neuron in plexu network and targetoutput layer renam as motor neuron thi name convent is necessari caus the built of the relev of artifici neural network with biolog neural network and neurosci the differ of sensori neuron from the cognit neuron that neither sensori nor motor one is they do not actual fire they just stand still for the data load they do not have ani subscript to the other neuron liter no subscript but they can be subscrib by the other neuron includ motor one they do not learn they do not consum ani cpu resourc they just store in the memori you can assign an imag a frame of a video or a chunk of an audio to a group of sensori neuron the differ of motor neuron form the other neuron is they are onli respons to the network they act as the fuse of the learn and calcul of the loss the network dictat a desir potenti on each motor neuron the motor neuron calcul it potenti compar it with desir potenti calcul the loss then tri to updat it weight randomli mani time and if it fail it blame it subscript so just like the network motor neuron are also abl to dictat a desir potenti on the other nonmotor neuron thi is whi ani neuron hold an addit potenti variabl call desired_potenti partial subscrib on the second phase of the network initi ani nonsensori neuron are forc to subscrib to some nonmotor neuron which are select by random sampl length of thi sampl is also select by random sampl round to nearest integ is done from a normal distribut such a normal distribut that the averag number of subscript is the mean and squar root of the mean is the standard deviat eg if neuron on averag ha 100 subscript then the mean is 100 and the standard deviat is 10 algorithm even so the python implement of plexu network is easi to understand it will be help for reader to explain the algorithm in pseudocod initi procedur initi the network is connect size connectivity_r connectivity_sqrt sqrtconnect connectivity_sqrt_sqrt sqrtconnectivity_sqrt for item in size do creat neuron end pick sensori neuron randomli pick motor neuron randomli determin nonsensori neuron determin nonmotor neuron initi subscript initi instanc variabl ignit the network initi is noth more than a make the assign for onc phase until the ignit the final step ignit never stop but can be paus if user want initi subscript procedur initi subscript is for neuron in neuron do if neuron is not a sensori neuron then call neuronpartially_subscrib end end return true partial subscrib procedur partial subscrib is sampl randomli sampl approxim connect unit of a neuron from within all nonmotor neuron for neuron in sampl do if neuron is not self then establish a subscript weight is randomli assign establish a public end end return true the time complex of the procedur initi subscript is on2 so thi may take a while if the size of the network and connect is big ignit procedur ignit subscript is creat an empti ban_list while network is not frozen do if next_queu is empti then get the output of network and print it increas the wave_count if first_queu is empti then for neuron in sensori neuron do for target_neuron in neuronpubl do append target_neuron to first_queu end end copi first_queu to next_queu end end copi next_queu to current_queu empti next_queu for neuron in ban_list do if neuronban_count connectivity_sqrt_sqrt then remov the neuron from current_queu end end while current_queu is not empti do neuron select a random neuron from current_queu remov the neuron from current_queu if neuronban_count connectivity_sqrt_sqrt then call neuronfir append the neuron to ban_list increas neuronban_count for target_neuron in neuronpubl do append target_neuron to next_queu end end end end procedur ignit regul the fire order of neuron and creat an effect veri similar to flow of electr current network wide it continu run until the network frozen noth els can stop it it fire the neuron step by step through ad them to a queue it gener it first queue from the public of sensori neuron time complex of if next_queu is empti then block is on2 but it can be ignor unless there are too mani sensori neuron becaus it run onc per wave it elimin ban neuron with for neuron in ban_list do block function of ban_count is give neuron connectivity_sqrt_sqrt amount of chanc after they ad to ban_list then it fire the neuron insid current_queu one by one choos them randomli after a neuron fire it add the fire neuron to ban_list and lastli copi the public of that neuron to next_queu so executionfir process can follow the path through the connect each execut from first sensori neuron to last motor neuron symbol one wave everi time a wave finish procedur fall into if next_queu is empti then block so wave start over from the sensori neuron ban_count and connectivity_sqrt_sqrt comparison creat execut loop insid cognit neuron and these loop act like memori unit which is a pretti import concept becaus loop creat the relat between current fed data and previous learn data without these loop the network fail on both classif and regress problem becaus neuronfir ha a time complex of on2 each turn insid while network is not frozen do block ha a time complex of on4 but dont worri becaus it will approxim to on3 becaus of the probabilist natur of fire function and the network will fire more than a million of neuron per minut by the way while network is not frozen do block is ignor becaus it an endless loop under normal condit fire procedur fire is if self is not a sensori neuron then potenti calcul potenti increas fire counter if desired_potenti is not none then loss calcul loss if loss 0 then desired_potenti none return true end if blame_lock is not empti then if wave_count blame_lock connect then return true els blame_lock none end tri connect time gener new weight randomli calcul new potenti and new loss accord to these weight if loss_new loss_curr then return true end tri sqrtconnect time gener hypothet potenti for neuron in subscript randomli calcul new potenti and new loss accord to these hypothet potenti if loss_new loss_curr then appli these hypothet potenti as desired_potenti return true end end if still not improv then either creat some new subscript or break some of the subscript return true end end end procedur fire handl all feedforward backpropag and learn process by itself fire function is an instanc method of neuron class thi procedur is by far the most import one in the plexu network it basic the core function and cpu spend most of it time to execut fire function again and again if desired_potenti is not assign to a valu then it just calcul the potenti and finish if desired_potenti is assign to a valu then first it calcul the loss if loss is equal to zero then the current state of the neuron is perfectli well and there is noth to learn if blame_lock is not empti then it will pass thi function connect time with thi control statement if blame_lock is not empti then it tri to improv the current state of the neuron by updat it weight randomli connect time if it improv then break it tri to improv the current state of the neuron by dictat randomli gener hypothet potenti over the subscript squar root of connect time if it improv then break if it still is not improv then it either creat some new subscript or break some of the subscript it current ha and hope it will lead the neuron to new improv in the futur on the first wave the fire function is onli meaning for motor neuron but after the first wave desired_potenti dictat will spread throughout the cognit neuron load procedur load input output is if output is none then for neuron in motor neuron do neurondesired_potenti none end end if number of sensori neuron is not equal to input length then rais an error but do not interrupt the network els for neuron in sensori neuron do neuronpotenti load from the input end end if number of motor neuron is not equal to output length then rais an error but do not interrupt the network els for neuron in motor neuron do neurondesired_potenti load from the output end end procedur load is the onli method that you can feed your data to the network you should call that function and load your data in real time also you should do it period and continu like everi 3 second if you leav second paramet empti then thi procedur will automat assum that you are test the network so it will replac desired_potenti valu of motor neuron with none otherwis it mean you are train the network so it will load the input data to sensori neuron and it will load the output data to desired_potenti valu of motor neuron applic instal of the python packag pip instal plexu if you want to instal plexu on develop mode git clone httpsgithubcommertyildiranplexusgit cd plexu pip instal e or altern make dev and test the instal with make cpp exampl binari classif exampl you can altern run thi exampl with python3 examplesclassification_binarypi command use a prewritten script version of below command suppos you need to train the network to figur out that the element of given array are bigger than 05 or not like 09 06 10 08 or 01 03 00 04 and suppos it a 4element array so let creat a network accord to your need import cplexu as plexu size 14 input_s 4 output_s 2 connect 1 precis 2 randomly_fir fals dynamic_output true visual fals net plexusnetwork size input_s output_s connect precis randomly_fir dynamic_output visual if you want to visual the network use pyqtgraph enabl visual fals becaus our network is automat initi and ignit now all we have to do is train the network so let train our network with 80 sampl precis 2 training_sample_s 20 for i in range1 training_sample_s if i 2 0 output 10 00 generated_list generate_list_bigg notify_the_loadgenerated_list output training_dur netloadgenerated_list output els output 00 10 generated_list generate_list_smal notify_the_loadgenerated_list output training_dur netloadgenerated_list output timesleeptraining_dur you should load your data one by one from each kind respect becaus it will prevent over fit to one specif kind you must wait a short time like training_dur 001 second which is a reason durat in such a case after each load output0 will converg to detect bigger than 05 input output1 will converg to detect smaller than 05 input befor the test you should defin a criteria call dominance_threshold so you can catch the decis make now let test the network error 0 error_divisor 0 for i in repeatnon testing_sample_s binary_random randomrandint0 1 if binary_random 0 generated_list generate_list_bigg expect 10 00 els generated_list generate_list_smal expect 00 10 netloadgenerated_list timesleeptraining_dur output netoutput error absexpected0 output0 error absexpected1 output1 error_divisor 2 with the while loop given abov you will be abl to check the output by give enough time to propag your input throught the network by give netload onli one paramet here you automat disabl the train now freez your network and calcul the overal error netfreez error error error_divisor which output overal error 0010249996604397894 classifi prime number exampl thi exampl is quit simpl to the previou exampl but thi time we are teach the network to understand if the given number is prime or not which is a rel complex problem run python3 examplesclassification_primepi 1 l cpp to see the result you will observ that the network is abl to learn the solut for such a complex problem in the matter of second sequenc basic exampl in thi exampl instead of classif we will train the network to detect a pattern in given sequenc the magic here is without even chang anyth relat to network just by chang logic we feed the data into the network the network automat turn into a recurr neural network run python3 examplessequence_basicpi to see the output thi is the output you should see ___ plexu network basic sequenc recognit exampl ___ creat a plexu network with 14 neuron 4 of them sensori 1 of them motor 1 connect rate 2 digit precis precis of the network will be 001 each individu nonsensori neuron will subscrib to 14 differ neuron 14 neuron creat 4 neuron pick as sensori neuron 1 neuron pick as motor neuron network ha been ignit learn gener the dataset 20 item long to recogn a sequenc learn for 01 second each load input 10 00 00 00 output 00 and wait 01 second load input 00 10 00 00 output 00 and wait 01 second load input 00 00 10 00 output 00 and wait 01 second load input 00 00 00 10 output 00 and wait 01 second load input 10 00 00 00 output 00 and wait 01 second load input 00 10 00 00 output 00 and wait 01 second load input 00 00 10 00 output 00 and wait 01 second load input 00 00 00 10 output 10 and wait 01 second load input 10 00 00 00 output 00 and wait 01 second load input 00 10 00 00 output 00 and wait 01 second load input 00 00 10 00 output 00 and wait 01 second load input 00 00 00 10 output 00 and wait 01 second load input 10 00 00 00 output 00 and wait 01 second load input 00 10 00 00 output 00 and wait 01 second load input 00 00 10 00 output 00 and wait 01 second load input 00 00 00 10 output 10 and wait 01 second by look at thi output you should be abl to see the pattern now on test stage you can see how success the network is on detect the pattern test test the network with random data 20 time load input 10 00 00 00 00 result 0019999999552965164 load input 00 10 00 00 00 result 00 load input 00 00 10 00 00 result 0019999999552965164 load input 00 00 00 10 00 result 0019999999552965164 load input 10 00 00 00 00 result 00 load input 00 10 00 00 00 result 00 load input 00 00 10 00 00 result 005999999865889549 load input 00 00 00 10 10 result 06600000262260437 load input 10 00 00 00 00 result 0019999999552965164 load input 00 10 00 00 00 result 00 load input 00 00 10 00 00 result 005999999865889549 load input 00 00 00 10 00 result 06600000262260437 load input 10 00 00 00 00 result 0019999999552965164 load input 00 10 00 00 00 result 00 load input 00 00 10 00 00 result 00 load input 00 00 00 10 10 result 09800000190734863 and the overal error network is now frozen 1786760 wave are execut throughout the network in total 66110093 time a random nonsensori neuron is fire overal error 0040124996623490006 catdog exampl you can altern run thi exampl with python3 examplescatdogpi command use a prewritten script version of below command suppos you need to train the network to figur out that the given imag 32x32 rgb is an imag of a cat or a dog and map them to blue and red respect so let creat a network accord to your need size 32 32 3 3 256 input_s 32 32 3 output_s 3 connect 0005 precis 3 training_dur 3 randomly_fir fals dynamic_output fals visual fals net plexusnetwork size input_s output_s connect precis randomly_fir dynamic_output visual we will plug in 32x32 rgb to the network so we need 3072 sensori neuron 3 motor neuron for see how rgb our result is and 256 cognit neuron to train we need 3 digit precis becaus we need to store 255 differ valu between 00 and 10 rang explain the answer of how to load cifar10 dataset and use it is out of the scope of thi paper but you can easili understand it by read the code examplescatdogpi onc you get the numpi array of cifar10 or ani other imag data just normal it and load training_sample_s 20 for i in range1 training_sample_s if i 2 0 cat randomsamplecat 10 cat_norm nptrue_dividecat 255flatten blue_norm nptrue_divideblu 255flatten cv2imshowinput cat netloadcat_norm blue_norm els dog randomsampledog 10 dog_norm nptrue_dividedog 255flatten red_norm nptrue_divid 255flatten cv2imshowinput dog netloaddog_norm red_norm show_outputnet you will get an overal error as the result veri similar to exampl abov although thi time the input length wa 768 time bigger thi is becaus plexu network amalgam the problem from all level of difficulti on a singl medium it make easi problem rel hard and hard problem rel easi when you run thi exampl you will get a slightli better result when compar to flip a coin you will most like get an overal error between 035 045 which is the proof that the network is abl to learn someth by the way dont forget that plexu network doe not iter over the dataset and furthermor it run in realtim also you have train the network just for 45 minut now let see what happen if we train our network for a long period of time note implement of gpu acceler and save the train network to disk are in workinprogress wip state therefor some part of the implement are subject to chang,core principle these are the core principle of objectoriented approach to the current state of artificial neural network that is inspired by synaptic plasticity between biological neuron unlike the current ann implementation neuron must be object not tensor between matrix just the current ann implementation neuron should be gpu accelerated ideally to provide the necessary parallelism while the current ann implementation can only create special case a plexus network must be architecturefree ie adaptive to create a generalized solution of all machine learning problem instead of dealing with decision of choosing an ann layer combinationsuch a convolution pooling or recurrent layer the network must have a layerless design there must be fundamentally two type of neuron sensory neuron interneuron input of the network must be made of sensory neuron any interneuron can be picked a a motor neuron an element of the output there are literally no difference between an interneuron and a motor neuron except the intervene of the network for igniting the wick of learning process through the motor neuron any nonmotor interneuron can be assumed a a cognitive neuron which collectively form the cognition of network there can be arbitrary amount of io group in a single network instead of batch size iteration and epoch concept training example must be fed on time basis with a manner like learn first sample for x second ok done then learn second sample for y second by this approach you can assign importance factor to your sample with maximum flexibility network must be retrainable network must be modular in other word you must be able to train a small network and then plug that network into a bigger network we are talking about some kind of selffusing here neuron must exhibit the characteristic of cellular automaton just like conways game of life number of neuron in the network can be increased or decreased scalability there must be no need for a networkwide oscillation yet the execution of neuron should follow a path very similar to flow of electric current nevertheless network should use randomness andor uncertainty principle flawlessly consciousness is an emergent property from cellular level to macro scale the network but it also an emergent property for the neuron from quantum level uncertainty to cellular mechanism in such a way that randomness is the cause of the illusion of consciousness most importantly the network must and can not iterate through the whole dataset besides that it also generally impossible to iterate the whole dataset on real life situation if the system is continuous like in robotics because of that the network must be designed to handle such a continuous data stream that literally endless and must be designed to handle that data stream chunk by chunk therefore when you are feeding the network use a diverse feed but not a grouped feed like 123123123123123123 but not like 111111222222333333 activation function the activation function that used by plexus is sigmoid and the derivative of the sigmoid function implementation of this algorithm in python programming language is publicly accessible through this link httpsgithubcommertyildiranplexusblobmasterplexusplexuspy you can directly skip to application part if you are not willing to understand the mathematical and algorithmic background basic plexus network ha only two class network and neuron in a plexus network there are many instance of neuron class but there is only one instance of network class when you crate a new plexus network you give these five parameter to the network class size of the network input dimension output dimension connectivity rate precision the network accordingly build itself size is literally equal to total number of neuron in the network all neuron are referenced in an instance variable called networkneurons input dimension specifies the number of sensory neuron sensory neuron are randomly selected from neuron output dimension specifies the number of motor neuron motor neuron are randomly selected from nonsensory neuron number of neuron multiplied by connectivity rate give the average number of subscription made by a single neuron precision simply defines the precision of the all calculation will be made by neuron how many digit after the decimal point after the network ha been successfully created it will ignite itself automatically ignition in simple term no matter if you have plugged in some data or not it will fire the neuron with using some mechanism very similar to flow of electric current will be explained later on this paper anatomy of a single neuron a single neuron in a plexus network take the network a the only parameter and store these seven very important information in it instance variable subscription publication potential desired_potential loss and type there are eventually there type of neuron neurontype 1 mean it a sensory neuron neurontype 2 mean it a motor neuron neurontype 0 mean it neither a sensory nor a motor neuron it mean it an cognitive interneuron or just cognitive neuron functionality of a neuron is relative to it type subscription is neuron indirect data feed each nonsensory neuron subscribes to some other neuron of any type for sensory neuron subscription are completely meaningless and empty by default because it get it data feed from outside world by assignment of the network subscription are literally the plexus network equivalent of dendrite in biological neuron subscription is a dictionary that hold neuronreference a key and weight a value publication hold literally the mirror data of subscription in the target neuron in other word any subscription creates also a publication reference in the target neuron similarly publication is the plexus network equivalent of axon in biological neuron potential p is the overall total potential value of all subscription multiplied by the corresponding weight only in sensory neuron it is directly assigned by the network value of potential may only be updated by the neuron itself and it being calculated by this simple formula each time when the neuron is fired desired_potential p is the ideal value of the neuron potential that is desired to eventually reach for sensory neuron it is meaningless for motor neuron it is assigned by the network if it none then the neuron doe not learn anything and just calculates potential when it fired loss l is calculated not just at the output but in every neuron except sensory one and it is equal to absolute difference distance between desired potential and current potential all numerical value inside a neuron are floating point number and all the calculation obey to the precision that given at start sensory and motor neuron input layer in classical neural network renamed a sensory neuron in plexus network and targetoutput layer renamed a motor neuron this naming convention is necessary cause the built of the relevance of artificial neural network with biological neural network and neuroscience the difference of sensory neuron from the cognitive neuron that neither sensory nor motor one is they do not actually fire they just stand still for the data load they do not have any subscription to the other neuron literally no subscription but they can be subscribed by the other neuron including motor one they do not learn they do not consume any cpu resource they just stored in the memory you can assign an image a frame of a video or a chunk of an audio to a group of sensory neuron the difference of motor neuron form the other neuron is they are only responsible to the network they act a the fuse of the learning and calculation of the loss the network dictate a desired potential on each motor neuron the motor neuron calculates it potential compare it with desired potential calculates the loss then try to update it weight randomly many time and if it fails it blame it subscription so just like the network motor neuron are also able to dictate a desired potential on the other nonmotor neuron this is why any neuron hold an additional potential variable called desired_potential partially subscribe on the second phase of the network initiation any nonsensory neuron are forced to subscribe to some nonmotor neuron which are selected by random sampling length of this sample is also selected by random sampling round to nearest integer is done from a normal distribution such a normal distribution that the average number of subscription is the mean and square root of the mean is the standard deviation eg if neuron on average ha 100 subscription then the mean is 100 and the standard deviation is 10 algorithm even so the python implementation of plexus network is easy to understand it will be helpful for reader to explain the algorithm in pseudocode initiation procedure initiate the network is connectivity size connectivity_rate connectivity_sqrt sqrtconnectivity connectivity_sqrt_sqrt sqrtconnectivity_sqrt for item in size do create neuron end pick sensory neuron randomly pick motor neuron randomly determine nonsensory neuron determine nonmotor neuron initiate subscription initiate instance variable ignite the network initiation is nothing more than a make the assignment for once phase until the ignition the final step ignition never stop but can be paused if user want initiate subscription procedure initiate subscription is for neuron in neuron do if neuron is not a sensory neuron then call neuronpartially_subscribe end end return true partially subscribe procedure partially subscribe is sample randomly sample approximately connectivity unit of a neuron from within all nonmotor neuron for neuron in sample do if neuron is not self then establish a subscription weight is randomly assigned establish a publication end end return true the time complexity of the procedure initiate subscription is on2 so this may take a while if the size of the network and connectivity is big ignite procedure ignite subscription is create an empty ban_list while network is not frozen do if next_queue is empty then get the output of network and print it increase the wave_counter if first_queue is empty then for neuron in sensory neuron do for target_neuron in neuronpublications do append target_neuron to first_queue end end copy first_queue to next_queue end end copy next_queue to current_queue empty next_queue for neuron in ban_list do if neuronban_counter connectivity_sqrt_sqrt then remove the neuron from current_queue end end while current_queue is not empty do neuron select a random neuron from current_queue remove the neuron from current_queue if neuronban_counter connectivity_sqrt_sqrt then call neuronfire append the neuron to ban_list increase neuronban_counter for target_neuron in neuronpublications do append target_neuron to next_queue end end end end procedure ignite regulates the firing order of neuron and creates an effect very similar to flow of electric current network wide it continuously run until the network frozen nothing else can stop it it fire the neuron step by step through adding them to a queue it generates it first queue from the publication of sensory neuron time complexity of if next_queue is empty then block is on2 but it can be ignored unless there are too many sensory neuron because it run once per wave it eliminates banned neuron with for neuron in ban_list do block function of ban_counter is giving neuron connectivity_sqrt_sqrt amount of chance after they added to ban_list then it fire the neuron inside current_queue one by one choosing them randomly after a neuron fired it add the fired neuron to ban_list and lastly copy the publication of that neuron to next_queue so executionfiring process can follow the path through the connection each execution from first sensory neuron to last motor neuron symbolizes one wave every time a wave finished procedure fall into if next_queue is empty then block so wave start over from the sensory neuron ban_counter and connectivity_sqrt_sqrt comparison creates execution loop inside cognitive neuron and these loop act like memory unit which is a pretty important concept because loop create the relation between currently fed data and previously learned data without these loop the network fails on both classification and regression problem because neuronfire ha a time complexity of on2 each turn inside while network is not frozen do block ha a time complexity of on4 but dont worry because it will approximate to on3 because of the probabilistic nature of fire function and the network will fire more than a million of neuron per minute by the way while network is not frozen do block is ignored because it an endless loop under normal condition fire procedure fire is if self is not a sensory neuron then potential calculate potential increase fire counter if desired_potential is not none then loss calculate loss if loss 0 then desired_potential none return true end if blame_lock is not empty then if wave_counter blame_lock connectivity then return true else blame_lock none end try connectivity time generate new weight randomly calculate new potential and new loss according to these weight if loss_new loss_current then return true end try sqrtconnectivity time generate hypothetical potential for neuron in subscription randomly calculate new potential and new loss according to these hypothetical potential if loss_new loss_current then apply these hypothetical potential a desired_potentials return true end end if still not improved then either create some new subscription or break some of the subscription return true end end end procedure fire handle all feedforwarding backpropagation and learning process by itself fire function is an instance method of neuron class this procedure is by far the most important one in the plexus network it basically the core function and cpu spends most of it time to execute fire function again and again if desired_potential is not assigned to a value then it just calculates the potential and finish if desired_potential is assigned to a value then first it calculates the loss if loss is equal to zero then the current state of the neuron is perfectly well and there is nothing to learn if blame_lock is not empty then it will pas this function connectivity time with this control statement if blame_lock is not empty then it try to improve the current state of the neuron by updating it weight randomly connectivity time if it improved then break it try to improve the current state of the neuron by dictating randomly generated hypothetical potential over the subscription square root of connectivity time if it improved then break if it still is not improved then it either creates some new subscription or break some of the subscription it currently ha and hope it will lead the neuron to new improvement in the future on the first wave the fire function is only meaningful for motor neuron but after the first wave desired_potential dictation will spread throughout the cognitive neuron load procedure load input output is if output is none then for neuron in motor neuron do neurondesired_potential none end end if number of sensory neuron is not equal to input length then raise an error but do not interrupt the network else for neuron in sensory neuron do neuronpotential load from the input end end if number of motor neuron is not equal to output length then raise an error but do not interrupt the network else for neuron in motor neuron do neurondesired_potential load from the output end end procedure load is the only method that you can feed your data to the network you should call that function and load your data in real time also you should do it periodically and continuously like every 3 second if you leave second parameter empty then this procedure will automatically assume that you are testing the network so it will replace desired_potential value of motor neuron with none otherwise it mean you are training the network so it will load the input data to sensory neuron and it will load the output data to desired_potential value of motor neuron application installation of the python package pip install plexus if you want to install plexus on development mode git clone httpsgithubcommertyildiranplexusgit cd plexus pip install e or alternatively make dev and test the installation with make cpp example binary classification example you can alternatively run this example with python3 examplesclassification_binarypy command using a prewritten script version of below command suppose you need to train the network to figure out that the element of given array are bigger than 05 or not like 09 06 10 08 or 01 03 00 04 and suppose it a 4element array so let create a network according to your need import cplexus a plexus size 14 input_size 4 output_size 2 connectivity 1 precision 2 randomly_fire false dynamic_output true visualization false net plexusnetwork size input_size output_size connectivity precision randomly_fire dynamic_output visualization if you want to visualize the network using pyqtgraph enable visualization false because our network is automatically initiated and ignited now all we have to do is training the network so let train our network with 80 sample precision 2 training_sample_size 20 for i in range1 training_sample_size if i 2 0 output 10 00 generated_list generate_list_bigger notify_the_loadgenerated_list output training_duration netloadgenerated_list output else output 00 10 generated_list generate_list_smaller notify_the_loadgenerated_list output training_duration netloadgenerated_list output timesleeptraining_duration you should load your data one by one from each kind respectively because it will prevent over fitting to one specific kind you must wait a short time like training_duration 001 second which is a reasonable duration in such a case after each load output0 will converge to detect bigger than 05 input output1 will converge to detect smaller than 05 input before the testing you should define a criterion called dominance_threshold so you can catch the decision making now let test the network error 0 error_divisor 0 for i in repeatnone testing_sample_size binary_random randomrandint0 1 if binary_random 0 generated_list generate_list_bigger expected 10 00 else generated_list generate_list_smaller expected 00 10 netloadgenerated_list timesleeptraining_duration output netoutput error absexpected0 output0 error absexpected1 output1 error_divisor 2 with the while loop given above you will be able to check the output by giving enough time to propagate your input throught the network by giving netload only one parameter here you automatically disable the training now freeze your network and calculate the overall error netfreeze error error error_divisor which output overall error 0010249996604397894 classifying prime number example this example is quite simple to the previous example but this time we are teaching the network to understand if the given number is prime or not which is a relatively complex problem run python3 examplesclassification_primepy 1 l cpp to see the result you will observe that the network is able to learn the solution for such a complex problem in the matter of second sequence basic example in this example instead of classification we will train the network to detect a pattern in given sequence the magic here is without even changing anything related to network just by changing logic we feed the data into the network the network automatically turn into a recurrent neural network run python3 examplessequence_basicpy to see the output this is the output you should see ___ plexus network basic sequence recognition example ___ create a plexus network with 14 neuron 4 of them sensory 1 of them motor 1 connectivity rate 2 digit precision precision of the network will be 001 each individual nonsensory neuron will subscribe to 14 different neuron 14 neuron created 4 neuron picked a sensory neuron 1 neuron picked a motor neuron network ha been ignited learning generate the dataset 20 item long to recognize a sequence learn for 01 second each load input 10 00 00 00 output 00 and wait 01 second load input 00 10 00 00 output 00 and wait 01 second load input 00 00 10 00 output 00 and wait 01 second load input 00 00 00 10 output 00 and wait 01 second load input 10 00 00 00 output 00 and wait 01 second load input 00 10 00 00 output 00 and wait 01 second load input 00 00 10 00 output 00 and wait 01 second load input 00 00 00 10 output 10 and wait 01 second load input 10 00 00 00 output 00 and wait 01 second load input 00 10 00 00 output 00 and wait 01 second load input 00 00 10 00 output 00 and wait 01 second load input 00 00 00 10 output 00 and wait 01 second load input 10 00 00 00 output 00 and wait 01 second load input 00 10 00 00 output 00 and wait 01 second load input 00 00 10 00 output 00 and wait 01 second load input 00 00 00 10 output 10 and wait 01 second by looking at this output you should be able to see the pattern now on testing stage you can see how successful the network is on detecting the pattern testing test the network with random data 20 time load input 10 00 00 00 00 result 0019999999552965164 load input 00 10 00 00 00 result 00 load input 00 00 10 00 00 result 0019999999552965164 load input 00 00 00 10 00 result 0019999999552965164 load input 10 00 00 00 00 result 00 load input 00 10 00 00 00 result 00 load input 00 00 10 00 00 result 005999999865889549 load input 00 00 00 10 10 result 06600000262260437 load input 10 00 00 00 00 result 0019999999552965164 load input 00 10 00 00 00 result 00 load input 00 00 10 00 00 result 005999999865889549 load input 00 00 00 10 00 result 06600000262260437 load input 10 00 00 00 00 result 0019999999552965164 load input 00 10 00 00 00 result 00 load input 00 00 10 00 00 result 00 load input 00 00 00 10 10 result 09800000190734863 and the overall error network is now frozen 1786760 wave are executed throughout the network in total 66110093 time a random nonsensory neuron is fired overall error 0040124996623490006 catdog example you can alternatively run this example with python3 examplescatdogpy command using a prewritten script version of below command suppose you need to train the network to figure out that the given image 32x32 rgb is an image of a cat or a dog and map them to blue and red respectively so let create a network according to your need size 32 32 3 3 256 input_size 32 32 3 output_size 3 connectivity 0005 precision 3 training_duration 3 randomly_fire false dynamic_output false visualization false net plexusnetwork size input_size output_size connectivity precision randomly_fire dynamic_output visualization we will plug in 32x32 rgb to the network so we need 3072 sensory neuron 3 motor neuron for see how rgb our result is and 256 cognitive neuron to train we need 3 digit precision because we need to store 255 different value between 00 and 10 range explaining the answer of how to load cifar10 dataset and use it is out of the scope of this paper but you can easily understand it by reading the code examplescatdogpy once you get the numpy array of cifar10 or any other image data just normalize it and load training_sample_size 20 for i in range1 training_sample_size if i 2 0 cat randomsamplecats 10 cat_normalized nptrue_dividecat 255flatten blue_normalized nptrue_divideblue 255flatten cv2imshowinput cat netloadcat_normalized blue_normalized else dog randomsampledogs 10 dog_normalized nptrue_dividedog 255flatten red_normalized nptrue_dividered 255flatten cv2imshowinput dog netloaddog_normalized red_normalized show_outputnet you will get an overall error a the result very similar to example above although this time the input length wa 768 time bigger this is because plexus network amalgamates the problem from all level of difficulty on a single medium it make easy problem relatively hard and hard problem relatively easy when you run this example you will get a slightly better result when compared to flipping a coin you will most likely get an overall error between 035 045 which is the proof that the network is able to learn something by the way dont forget that plexus network doe not iterate over the dataset and furthermore it run in realtime also you have trained the network just for 45 minute now let see what happens if we train our network for a long period of time note implementation of gpu acceleration and saving the trained network to disk are in workinprogress wip state therefore some part of the implementation are subject to change,core principle core principle objectoriented approach current state artificial neural network inspired synaptic plasticity biological neuron unlike current ann implementation neuron must object tensor matrix current ann implementation neuron gpu accelerated ideally provide necessary parallelism current ann implementation create special case plexus network must architecturefree ie adaptive create generalized solution machine learning problem instead dealing decision choosing ann layer combinationsuch convolution pooling recurrent layer network must layerless design must fundamentally two type neuron sensory neuron interneuron input network must made sensory neuron interneuron picked motor neuron element output literally difference interneuron motor neuron except intervene network igniting wick learning process motor neuron nonmotor interneuron assumed cognitive neuron collectively form cognition network arbitrary amount io group single network instead batch size iteration epoch concept training example must fed time basis manner like learn first sample x second ok done learn second sample second approach assign importance factor sample maximum flexibility network must retrainable network must modular word must able train small network plug network bigger network talking kind selffusing neuron must exhibit characteristic cellular automaton like conways game life number neuron network increased decreased scalability must need networkwide oscillation yet execution neuron follow path similar flow electric current nevertheless network use randomness andor uncertainty principle flawlessly consciousness emergent property cellular level macro scale network also emergent property neuron quantum level uncertainty cellular mechanism way randomness cause illusion consciousness importantly network must iterate whole dataset besides also generally impossible iterate whole dataset real life situation system continuous like robotics network must designed handle continuous data stream literally endless must designed handle data stream chunk chunk therefore feeding network use diverse feed grouped feed like 123123123123123123 like 111111222222333333 activation function activation function used plexus sigmoid derivative sigmoid function implementation algorithm python programming language publicly accessible link httpsgithubcommertyildiranplexusblobmasterplexusplexuspy directly skip application part willing understand mathematical algorithmic background basic plexus network two class network neuron plexus network many instance neuron class one instance network class crate new plexus network give five parameter network class size network input dimension output dimension connectivity rate precision network accordingly build size literally equal total number neuron network neuron referenced instance variable called networkneurons input dimension specifies number sensory neuron sensory neuron randomly selected neuron output dimension specifies number motor neuron motor neuron randomly selected nonsensory neuron number neuron multiplied connectivity rate give average number subscription made single neuron precision simply defines precision calculation made neuron many digit decimal point network successfully created ignite automatically ignition simple term matter plugged data fire neuron using mechanism similar flow electric current explained later paper anatomy single neuron single neuron plexus network take network parameter store seven important information instance variable subscription publication potential desired_potential loss type eventually type neuron neurontype 1 mean sensory neuron neurontype 2 mean motor neuron neurontype 0 mean neither sensory motor neuron mean cognitive interneuron cognitive neuron functionality neuron relative type subscription neuron indirect data feed nonsensory neuron subscribes neuron type sensory neuron subscription completely meaningless empty default get data feed outside world assignment network subscription literally plexus network equivalent dendrite biological neuron subscription dictionary hold neuronreference key weight value publication hold literally mirror data subscription target neuron word subscription creates also publication reference target neuron similarly publication plexus network equivalent axon biological neuron potential p overall total potential value subscription multiplied corresponding weight sensory neuron directly assigned network value potential may updated neuron calculated simple formula time neuron fired desired_potential p ideal value neuron potential desired eventually reach sensory neuron meaningless motor neuron assigned network none neuron learn anything calculates potential fired loss l calculated output every neuron except sensory one equal absolute difference distance desired potential current potential numerical value inside neuron floating point number calculation obey precision given start sensory motor neuron input layer classical neural network renamed sensory neuron plexus network targetoutput layer renamed motor neuron naming convention necessary cause built relevance artificial neural network biological neural network neuroscience difference sensory neuron cognitive neuron neither sensory motor one actually fire stand still data load subscription neuron literally subscription subscribed neuron including motor one learn consume cpu resource stored memory assign image frame video chunk audio group sensory neuron difference motor neuron form neuron responsible network act fuse learning calculation loss network dictate desired potential motor neuron motor neuron calculates potential compare desired potential calculates loss try update weight randomly many time fails blame subscription like network motor neuron also able dictate desired potential nonmotor neuron neuron hold additional potential variable called desired_potential partially subscribe second phase network initiation nonsensory neuron forced subscribe nonmotor neuron selected random sampling length sample also selected random sampling round nearest integer done normal distribution normal distribution average number subscription mean square root mean standard deviation eg neuron average 100 subscription mean 100 standard deviation 10 algorithm even python implementation plexus network easy understand helpful reader explain algorithm pseudocode initiation procedure initiate network connectivity size connectivity_rate connectivity_sqrt sqrtconnectivity connectivity_sqrt_sqrt sqrtconnectivity_sqrt item size create neuron end pick sensory neuron randomly pick motor neuron randomly determine nonsensory neuron determine nonmotor neuron initiate subscription initiate instance variable ignite network initiation nothing make assignment phase ignition final step ignition never stop paused user want initiate subscription procedure initiate subscription neuron neuron neuron sensory neuron call neuronpartially_subscribe end end return true partially subscribe procedure partially subscribe sample randomly sample approximately connectivity unit neuron within nonmotor neuron neuron sample neuron self establish subscription weight randomly assigned establish publication end end return true time complexity procedure initiate subscription on2 may take size network connectivity big ignite procedure ignite subscription create empty ban_list network frozen next_queue empty get output network print increase wave_counter first_queue empty neuron sensory neuron target_neuron neuronpublications append target_neuron first_queue end end copy first_queue next_queue end end copy next_queue current_queue empty next_queue neuron ban_list neuronban_counter connectivity_sqrt_sqrt remove neuron current_queue end end current_queue empty neuron select random neuron current_queue remove neuron current_queue neuronban_counter connectivity_sqrt_sqrt call neuronfire append neuron ban_list increase neuronban_counter target_neuron neuronpublications append target_neuron next_queue end end end end procedure ignite regulates firing order neuron creates effect similar flow electric current network wide continuously run network frozen nothing else stop fire neuron step step adding queue generates first queue publication sensory neuron time complexity next_queue empty block on2 ignored unless many sensory neuron run per wave eliminates banned neuron neuron ban_list block function ban_counter giving neuron connectivity_sqrt_sqrt amount chance added ban_list fire neuron inside current_queue one one choosing randomly neuron fired add fired neuron ban_list lastly copy publication neuron next_queue executionfiring process follow path connection execution first sensory neuron last motor neuron symbolizes one wave every time wave finished procedure fall next_queue empty block wave start sensory neuron ban_counter connectivity_sqrt_sqrt comparison creates execution loop inside cognitive neuron loop act like memory unit pretty important concept loop create relation currently fed data previously learned data without loop network fails classification regression problem neuronfire time complexity on2 turn inside network frozen block time complexity on4 dont worry approximate on3 probabilistic nature fire function network fire million neuron per minute way network frozen block ignored endless loop normal condition fire procedure fire self sensory neuron potential calculate potential increase fire counter desired_potential none loss calculate loss loss 0 desired_potential none return true end blame_lock empty wave_counter blame_lock connectivity return true else blame_lock none end try connectivity time generate new weight randomly calculate new potential new loss according weight loss_new loss_current return true end try sqrtconnectivity time generate hypothetical potential neuron subscription randomly calculate new potential new loss according hypothetical potential loss_new loss_current apply hypothetical potential desired_potentials return true end end still improved either create new subscription break subscription return true end end end procedure fire handle feedforwarding backpropagation learning process fire function instance method neuron class procedure far important one plexus network basically core function cpu spends time execute fire function desired_potential assigned value calculates potential finish desired_potential assigned value first calculates loss loss equal zero current state neuron perfectly well nothing learn blame_lock empty pas function connectivity time control statement blame_lock empty try improve current state neuron updating weight randomly connectivity time improved break try improve current state neuron dictating randomly generated hypothetical potential subscription square root connectivity time improved break still improved either creates new subscription break subscription currently hope lead neuron new improvement future first wave fire function meaningful motor neuron first wave desired_potential dictation spread throughout cognitive neuron load procedure load input output output none neuron motor neuron neurondesired_potential none end end number sensory neuron equal input length raise error interrupt network else neuron sensory neuron neuronpotential load input end end number motor neuron equal output length raise error interrupt network else neuron motor neuron neurondesired_potential load output end end procedure load method feed data network call function load data real time also periodically continuously like every 3 second leave second parameter empty procedure automatically assume testing network replace desired_potential value motor neuron none otherwise mean training network load input data sensory neuron load output data desired_potential value motor neuron application installation python package pip install plexus want install plexus development mode git clone httpsgithubcommertyildiranplexusgit cd plexus pip install e alternatively make dev test installation make cpp example binary classification example alternatively run example python3 examplesclassification_binarypy command using prewritten script version command suppose need train network figure element given array bigger 05 like 09 06 10 08 01 03 00 04 suppose 4element array let create network according need import cplexus plexus size 14 input_size 4 output_size 2 connectivity 1 precision 2 randomly_fire false dynamic_output true visualization false net plexusnetwork size input_size output_size connectivity precision randomly_fire dynamic_output visualization want visualize network using pyqtgraph enable visualization false network automatically initiated ignited training network let train network 80 sample precision 2 training_sample_size 20 range1 training_sample_size 2 0 output 10 00 generated_list generate_list_bigger notify_the_loadgenerated_list output training_duration netloadgenerated_list output else output 00 10 generated_list generate_list_smaller notify_the_loadgenerated_list output training_duration netloadgenerated_list output timesleeptraining_duration load data one one kind respectively prevent fitting one specific kind must wait short time like training_duration 001 second reasonable duration case load output0 converge detect bigger 05 input output1 converge detect smaller 05 input testing define criterion called dominance_threshold catch decision making let test network error 0 error_divisor 0 repeatnone testing_sample_size binary_random randomrandint0 1 binary_random 0 generated_list generate_list_bigger expected 10 00 else generated_list generate_list_smaller expected 00 10 netloadgenerated_list timesleeptraining_duration output netoutput error absexpected0 output0 error absexpected1 output1 error_divisor 2 loop given able check output giving enough time propagate input throught network giving netload one parameter automatically disable training freeze network calculate overall error netfreeze error error error_divisor output overall error 0010249996604397894 classifying prime number example example quite simple previous example time teaching network understand given number prime relatively complex problem run python3 examplesclassification_primepy 1 l cpp see result observe network able learn solution complex problem matter second sequence basic example example instead classification train network detect pattern given sequence magic without even changing anything related network changing logic feed data network network automatically turn recurrent neural network run python3 examplessequence_basicpy see output output see ___ plexus network basic sequence recognition example ___ create plexus network 14 neuron 4 sensory 1 motor 1 connectivity rate 2 digit precision precision network 001 individual nonsensory neuron subscribe 14 different neuron 14 neuron created 4 neuron picked sensory neuron 1 neuron picked motor neuron network ignited learning generate dataset 20 item long recognize sequence learn 01 second load input 10 00 00 00 &#9; output 00 &#9; wait 01 second load input 00 10 00 00 &#9; output 00 &#9; wait 01 second load input 00 00 10 00 &#9; output 00 &#9; wait 01 second load input 00 00 00 10 &#9; output 00 &#9; wait 01 second load input 10 00 00 00 &#9; output 00 &#9; wait 01 second load input 00 10 00 00 &#9; output 00 &#9; wait 01 second load input 00 00 10 00 &#9; output 00 &#9; wait 01 second load input 00 00 00 10 &#9; output 10 &#9; wait 01 second load input 10 00 00 00 &#9; output 00 &#9; wait 01 second load input 00 10 00 00 &#9; output 00 &#9; wait 01 second load input 00 00 10 00 &#9; output 00 &#9; wait 01 second load input 00 00 00 10 &#9; output 00 &#9; wait 01 second load input 10 00 00 00 &#9; output 00 &#9; wait 01 second load input 00 10 00 00 &#9; output 00 &#9; wait 01 second load input 00 00 10 00 &#9; output 00 &#9; wait 01 second load input 00 00 00 10 &#9; output 10 &#9; wait 01 second looking output able see pattern testing stage see successful network detecting pattern testing test network random data 20 time load input 10 00 00 00 00 &#9; result 0019999999552965164 load input 00 10 00 00 00 &#9; result 00 load input 00 00 10 00 00 &#9; result 0019999999552965164 load input 00 00 00 10 00 &#9; result 0019999999552965164 load input 10 00 00 00 00 &#9; result 00 load input 00 10 00 00 00 &#9; result 00 load input 00 00 10 00 00 &#9; result 005999999865889549 load input 00 00 00 10 10 &#9; result 06600000262260437 load input 10 00 00 00 00 &#9; result 0019999999552965164 load input 00 10 00 00 00 &#9; result 00 load input 00 00 10 00 00 &#9; result 005999999865889549 load input 00 00 00 10 00 &#9; result 06600000262260437 load input 10 00 00 00 00 &#9; result 0019999999552965164 load input 00 10 00 00 00 &#9; result 00 load input 00 00 10 00 00 &#9; result 00 load input 00 00 00 10 10 &#9; result 09800000190734863 overall error network frozen 1786760 wave executed throughout network total 66110093 time random nonsensory neuron fired overall error 0040124996623490006 catdog example alternatively run example python3 examplescatdogpy command using prewritten script version command suppose need train network figure given image 32x32 rgb image cat dog map blue red respectively let create network according need size 32 32 3 3 256 input_size 32 32 3 output_size 3 connectivity 0005 precision 3 training_duration 3 randomly_fire false dynamic_output false visualization false net plexusnetwork size input_size output_size connectivity precision randomly_fire dynamic_output visualization plug 32x32 rgb network need 3072 sensory neuron 3 motor neuron see rgb result 256 cognitive neuron train need 3 digit precision need store 255 different value 00 10 range explaining answer load cifar10 dataset use scope paper easily understand reading code examplescatdogpy get numpy array cifar10 image data normalize load training_sample_size 20 range1 training_sample_size 2 0 cat randomsamplecats 10 cat_normalized nptrue_dividecat 255flatten blue_normalized nptrue_divideblue 255flatten cv2imshowinput cat netloadcat_normalized blue_normalized else dog randomsampledogs 10 dog_normalized nptrue_dividedog 255flatten red_normalized nptrue_dividered 255flatten cv2imshowinput dog netloaddog_normalized red_normalized show_outputnet get overall error result similar example although time input length 768 time bigger plexus network amalgamates problem level difficulty single medium make easy problem relatively hard hard problem relatively easy run example get slightly better result compared flipping coin likely get overall error 035 045 proof network able learn something way dont forget plexus network iterate dataset furthermore run realtime also trained network 45 minute let see happens train network long period time note implementation gpu acceleration saving trained network disk workinprogress wip state therefore part implementation subject change
C++ ,"CATH Tools  
Protein structure comparison tools such as SSAP, as used by the Orengo Group in curating CATH.












Executable DOWNLOADS   (for Linux/Mac; chmod them to be executable)
Docs     
Code     
Builds     
Extras repo     



Tools










 cath-cluster   Complete-linkage cluster arbitrary data.



 cath-map-clusters   Map names from previous clusters to new clusters based on (the overlaps between) their members (which may be specified as regions within a parent sequence). Renumber any clusters with no equivalents.



 cath-resolve-hits   Collapse a list of domain matches to your query sequence(s) down to the non-overlapping subset (ie domain architecture) that maximises the sum of the hits' scores.



 cath-ssap   Structurally align a pair of proteins.



 cath-superpose   Superpose two or more protein structures using an existing alignment.



Extra Tools

build-test          Perform the cath-tools tests (which should all pass, albeit with a few warnings)
cath-assign-domains Use an SVM model on SSAP+PRC data to form a plan for assigning the domains to CATH superfamilies/folds
cath-refine-align   Iteratively refine an existing alignment by attempting to optimise SSAP score
cath-score-align    Score an existing alignment using structural data

Authors
The SSAP algorithm (cath-ssap) was devised by Christine A Orengo and William R Taylor.
Please cite: Protein Structure Alignment, Taylor and Orengo, Journal of Molecular Biology 208, 1-22, PMID: 2769748. (PubMed, Elsevier)
Since then, many people have contributed to this code, most notably:

Tony E Lewis             (2011–…)
Oliver C Redfern                                          (~2003–2011)
James E Bray, Ian Sillitoe (~2000–2003)
Andrew C R Martin    (considerable edits around 2001)

Acknowledgements
cath-ssap typically uses DSSP, either by reading DSSP files or via its own implementation of the DSSP algorithms.
cath-cluster uses Fionn Murtagh's reciprocal-nearest-neighbour algorithm (see Multidimensional clustering algorithms, volume 4 of Compstat Lectures.
Physica-Verlag, Würzburg/ Wien, 1985. ISBN 3-7051-0008-4) as described and refined in Daniel Müllner's Modern hierarchical, agglomerative clustering algorithms (2011, arXiv:1109.2378).
Feedback
Please tell us about your cath-tools bugs/suggestions here.
If you find this software useful, please spread the word and star the GitHub repo.
",cath tool protein structur comparison tool such as ssap as use by the orengo group in curat cath execut download for linuxmac chmod them to be execut doc code build extra repo tool cathclust completelinkag cluster arbitrari data cathmapclust map name from previou cluster to new cluster base on the overlap between their member which may be specifi as region within a parent sequenc renumb ani cluster with no equival cathresolvehit collaps a list of domain match to your queri sequenc down to the nonoverlap subset ie domain architectur that maximis the sum of the hit score cathssap structur align a pair of protein cathsuperpos superpos two or more protein structur use an exist align extra tool buildtest perform the cathtool test which should all pass albeit with a few warn cathassigndomain use an svm model on ssapprc data to form a plan for assign the domain to cath superfamiliesfold cathrefinealign iter refin an exist align by attempt to optimis ssap score cathscorealign score an exist align use structur data author the ssap algorithm cathssap wa devis by christin a orengo and william r taylor pleas cite protein structur align taylor and orengo journal of molecular biolog 208 122 pmid 2769748 pubm elsevi sinc then mani peopl have contribut to thi code most notabl toni e lewi 2011 oliv c redfern 20032011 jame e bray ian sillito 20002003 andrew c r martin consider edit around 2001 acknowledg cathssap typic use dssp either by read dssp file or via it own implement of the dssp algorithm cathclust use fionn murtagh reciprocalnearestneighbour algorithm see multidimension cluster algorithm volum 4 of compstat lectur physicaverlag wrzburg wien 1985 isbn 3705100084 as describ and refin in daniel mllner modern hierarch agglom cluster algorithm 2011 arxiv11092378 feedback pleas tell us about your cathtool bugssuggest here if you find thi softwar use pleas spread the word and star the github repo,cath tool protein structure comparison tool such a ssap a used by the orengo group in curating cath executable downloads for linuxmac chmod them to be executable doc code build extra repo tool cathcluster completelinkage cluster arbitrary data cathmapclusters map name from previous cluster to new cluster based on the overlap between their member which may be specified a region within a parent sequence renumber any cluster with no equivalent cathresolvehits collapse a list of domain match to your query sequence down to the nonoverlapping subset ie domain architecture that maximises the sum of the hit score cathssap structurally align a pair of protein cathsuperpose superpose two or more protein structure using an existing alignment extra tool buildtest perform the cathtools test which should all pas albeit with a few warning cathassigndomains use an svm model on ssapprc data to form a plan for assigning the domain to cath superfamiliesfolds cathrefinealign iteratively refine an existing alignment by attempting to optimise ssap score cathscorealign score an existing alignment using structural data author the ssap algorithm cathssap wa devised by christine a orengo and william r taylor please cite protein structure alignment taylor and orengo journal of molecular biology 208 122 pmid 2769748 pubmed elsevier since then many people have contributed to this code most notably tony e lewis 2011 oliver c redfern 20032011 james e bray ian sillitoe 20002003 andrew c r martin considerable edits around 2001 acknowledgement cathssap typically us dssp either by reading dssp file or via it own implementation of the dssp algorithm cathcluster us fionn murtaghs reciprocalnearestneighbour algorithm see multidimensional clustering algorithm volume 4 of compstat lecture physicaverlag wrzburg wien 1985 isbn 3705100084 a described and refined in daniel mllners modern hierarchical agglomerative clustering algorithm 2011 arxiv11092378 feedback please tell u about your cathtools bugssuggestions here if you find this software useful please spread the word and star the github repo,cath tool protein structure comparison tool ssap used orengo group curating cath executable downloads linuxmac chmod executable doc code build extra repo tool cathcluster completelinkage cluster arbitrary data cathmapclusters map name previous cluster new cluster based overlap member may specified region within parent sequence renumber cluster equivalent cathresolvehits collapse list domain match query sequence nonoverlapping subset ie domain architecture maximises sum hit score cathssap structurally align pair protein cathsuperpose superpose two protein structure using existing alignment extra tool buildtest perform cathtools test pas albeit warning cathassigndomains use svm model ssapprc data form plan assigning domain cath superfamiliesfolds cathrefinealign iteratively refine existing alignment attempting optimise ssap score cathscorealign score existing alignment using structural data author ssap algorithm cathssap devised christine orengo william r taylor please cite protein structure alignment taylor orengo journal molecular biology 208 122 pmid 2769748 pubmed elsevier since many people contributed code notably tony e lewis 2011 oliver c redfern 20032011 james e bray ian sillitoe 20002003 andrew c r martin considerable edits around 2001 acknowledgement cathssap typically us dssp either reading dssp file via implementation dssp algorithm cathcluster us fionn murtaghs reciprocalnearestneighbour algorithm see multidimensional clustering algorithm volume 4 compstat lecture physicaverlag wrzburg wien 1985 isbn 3705100084 described refined daniel mllners modern hierarchical agglomerative clustering algorithm 2011 arxiv11092378 feedback please tell u cathtools bugssuggestions find software useful please spread word star github repo
C++ ,"skew-biology
Software for my DIY spectrometers, sensors, pcr, and incubators.
Spectrometer Software
Software for reading a spectrum from a DIY spectrometer using OpenCV,
calibrating it using non-linear regression to the function with the gnu scientific library.
Example DIY Lego Spectrometer


1k diffraction grating
Sony IMX179 8MP CCD
Velcro, aluminum foil, and a lot of black legos
CCD at m=0

Usage:
spec [roi x y w h] [cal C0 C1 C2]
The ROI and constants for calibration can be specified on the command line. The calculated intensity and wavelength can be saved to csv alongside the calibration file for analysis in other programs.
Calibration Details
This software currently uses a second-order polynomial for calibration off a minimum of three points. There may be value in moving to a cubic function.
nm = C0 + C1p + C2p^2
Calibration References

Calibrating the Wavelength of Your Spectrometer
OceanOptics Cubic Calibration
PublicLab Linear Calibration

Selecting a Region of Interest
On startup the region of interest can be selected with a mouse. The pixels in this region of interest are summed to calculate intensity.
Commands:



Command
Note




top
lists the top wavelengths, intensities, and pixel indicies


cal
enters calibration mode


save
saves to csv along with the calibration



Building:
cmake .
make

Example usage:
Calibrating a CFL using the mercury peaks terbium peaks. Appears to accurately predict europium peaks at 612nm.
CFL Spectrum Reference

Incubator
PID controled incubator w/ IR
RGB
simple rgb sensor control for recording the rgb values of a sample
being incubated by a connected computer
",skewbiolog softwar for my diy spectromet sensor pcr and incub spectromet softwar softwar for read a spectrum from a diy spectromet use opencv calibr it use nonlinear regress to the function with the gnu scientif librari exampl diy lego spectromet 1k diffract grate soni imx179 8mp ccd velcro aluminum foil and a lot of black lego ccd at m0 usag spec roi x y w h cal c0 c1 c2 the roi and constant for calibr can be specifi on the command line the calcul intens and wavelength can be save to csv alongsid the calibr file for analysi in other program calibr detail thi softwar current use a secondord polynomi for calibr off a minimum of three point there may be valu in move to a cubic function nm c0 c1p c2p2 calibr refer calibr the wavelength of your spectromet oceanopt cubic calibr publiclab linear calibr select a region of interest on startup the region of interest can be select with a mous the pixel in thi region of interest are sum to calcul intens command command note top list the top wavelength intens and pixel indici cal enter calibr mode save save to csv along with the calibr build cmake make exampl usag calibr a cfl use the mercuri peak terbium peak appear to accur predict europium peak at 612nm cfl spectrum refer incub pid control incub w ir rgb simpl rgb sensor control for record the rgb valu of a sampl be incub by a connect comput,skewbiology software for my diy spectrometer sensor pcr and incubator spectrometer software software for reading a spectrum from a diy spectrometer using opencv calibrating it using nonlinear regression to the function with the gnu scientific library example diy lego spectrometer 1k diffraction grating sony imx179 8mp ccd velcro aluminum foil and a lot of black lego ccd at m0 usage spec roi x y w h cal c0 c1 c2 the roi and constant for calibration can be specified on the command line the calculated intensity and wavelength can be saved to csv alongside the calibration file for analysis in other program calibration detail this software currently us a secondorder polynomial for calibration off a minimum of three point there may be value in moving to a cubic function nm c0 c1p c2p2 calibration reference calibrating the wavelength of your spectrometer oceanoptics cubic calibration publiclab linear calibration selecting a region of interest on startup the region of interest can be selected with a mouse the pixel in this region of interest are summed to calculate intensity command command note top list the top wavelength intensity and pixel indicies cal enters calibration mode save save to csv along with the calibration building cmake make example usage calibrating a cfl using the mercury peak terbium peak appears to accurately predict europium peak at 612nm cfl spectrum reference incubator pid controled incubator w ir rgb simple rgb sensor control for recording the rgb value of a sample being incubated by a connected computer,skewbiology software diy spectrometer sensor pcr incubator spectrometer software software reading spectrum diy spectrometer using opencv calibrating using nonlinear regression function gnu scientific library example diy lego spectrometer 1k diffraction grating sony imx179 8mp ccd velcro aluminum foil lot black lego ccd m0 usage spec roi x w h cal c0 c1 c2 roi constant calibration specified command line calculated intensity wavelength saved csv alongside calibration file analysis program calibration detail software currently us secondorder polynomial calibration minimum three point may value moving cubic function nm c0 c1p c2p2 calibration reference calibrating wavelength spectrometer oceanoptics cubic calibration publiclab linear calibration selecting region interest startup region interest selected mouse pixel region interest summed calculate intensity command command note top list top wavelength intensity pixel indicies cal enters calibration mode save save csv along calibration building cmake make example usage calibrating cfl using mercury peak terbium peak appears accurately predict europium peak 612nm cfl spectrum reference incubator pid controled incubator w ir rgb simple rgb sensor control recording rgb value sample incubated connected computer
